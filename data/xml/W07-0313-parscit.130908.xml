<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.981311">
Different measurements metrics to evaluate a chatbot system
</title>
<author confidence="0.698359">
Bayan Abu Shawar
</author>
<affiliation confidence="0.56968">
IT department
Arab Open University
[add]
</affiliation>
<email confidence="0.655727">
bshawar@arabou-jo.edu.jo
</email>
<author confidence="0.987374">
Eric Atwell
</author>
<affiliation confidence="0.9981125">
School of Computing
University of Leeds
</affiliation>
<address confidence="0.956323">
LS2 9JT, Leeds-UK
</address>
<email confidence="0.932476">
eric@comp .leeds.ac.uk
</email>
<sectionHeader confidence="0.995924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943227272727">
A chatbot is a software system, which can
interact or “chat” with a human user in
natural language such as English. For the
annual Loebner Prize contest, rival chat-
bots have been assessed in terms of ability
to fool a judge in a restricted chat session.
We are investigating methods to train and
adapt a chatbot to a specific user’s lan-
guage use or application, via a user-
supplied training corpus. We advocate
open-ended trials by real users, such as an
example Afrikaans chatbot for Afrikaans-
speaking researchers and students in
South Africa. This is evaluated in terms of
“glass box” dialogue efficiency metrics,
and “black box” dialogue quality metrics
and user satisfaction feedback. The other
examples presented in this paper are the
Qur&apos;an and the FAQchat prototypes. Our
general conclusion is that evaluation
should be adapted to the application and
to user needs.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956416666667">
“Before there were computers, we could distin-
guish persons from non-persons on the basis of an
ability to participate in conversations. But now, we
have hybrids operating between person and non
persons with whom we can talk in ordinary lan-
guage.” (Colby 1999a). Human machine conversa-
tion as a technology integrates different areas
where the core is the language, and the computa-
tional methodologies facilitate communication be-
tween users and computers using natural language.
A related term to machine conversation is the
chatbot, a conversational agent that interacts with
</bodyText>
<page confidence="0.999101">
89
</page>
<bodyText confidence="0.980107315789474">
users turn by turn using natural language. Different
chatbots or human-computer dialogue systems
have been developed using text communication
such as Eliza (Weizenbaum 1966), PARRY (Colby
1999b), CONVERSE (Batacharia etc 1999),
ALICE1. Chatbots have been used in different do-
mains such as: customer service, education, web
site help, and for fun.
Different mechanisms are used to evaluate
Spoken Dialogue Systems (SLDs), ranging from
glass box evaluation that evaluates individual
components, to black box evaluation that evaluates
the system as a whole McTear (2002). For exam-
ple, glass box evaluation was applied on the
(Hirschman 1995) ARPA Spoken Language sys-
tem, and it shows that the error rate for sentence
understanding was much lower than that for sen-
tence recognition. On the other hand black box
evaluation evaluates the system as a whole based
on user satisfaction and acceptance. The black box
approach evaluates the performance of the system
in terms of achieving its task, the cost of achieving
the task in terms of time taken and number of
turns, and measures the quality of the interaction,
normally summarised by the term ‘user satisfac-
tion’, which indicates whether the user “ gets the
information s/he wants, is s/he comfortable with
the system, and gets the information within accept-
able elapsed time, etc.” (Maier et al 1996).
The Loebner prize2 competition has been used
to evaluate machine conversation chatbots. The
Loebner Prize is a Turing test, which evaluates the
ability of the machine to fool people that they are
talking to human. In essence, judges are allowed a
short chat (10 to 15 minutes) with each chatbot,
and asked to rank them in terms of “naturalness”.
ALICE (Abu Shawar and Atwell 2003) is the
Artificial Linguistic Internet Computer Entity, first
</bodyText>
<footnote confidence="0.99922">
1 http://www.alicebot.org/
2 http://www.loebner.net/Prizef/loebner-prize.html
</footnote>
<note confidence="0.6256435">
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 89–96,
NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998708307692308">
implemented by Wallace in 1995. ALICE knowl-
edge about English conversation patterns is stored
in AIML files. AIML, or Artificial Intelligence
Mark-up Language, is a derivative of Extensible
Mark-up Language (XML). It was developed by
Wallace and the Alicebot free software community
during 1995-2000 to enable people to input dia-
logue pattern knowledge into chatbots based on the
A.L.I.C.E. open-source software technology.
In this paper we present other methods to
evaluate the chatbot systems. ALICE chtabot sys-
tem was used for this purpose, where a Java pro-
gram has been developed to read from a corpus
and convert the text to the AIML format. The Cor-
pus of Spoken Afrikaans (Korpus Gesproke Afri-
kaans, KGA), the corpus of the holy book of Islam
(Qur’an), and the FAQ of the School of Computing
at University of Leeds3 were used to produce two
KGA prototype, the Qur’an prototype and the
FAQchat one consequently.
Section 2 presents Loebner Prize contest, sec-
tion 3 illustrates the ALICE/AIMLE architecture.
The evaluation techniques of the KGA prototype,
the Qur’an prototype, and the FAQchat prototype
are discussed in sections 4, 5, and 6 consequently.
The conclusion is presented in section 7.
</bodyText>
<sectionHeader confidence="0.875024" genericHeader="method">
2 The Loebner Prize Competition
</sectionHeader>
<bodyText confidence="0.99981535">
The story began with the “imitation game” which
was presented in Alan Turing’s paper “Can Ma-
chine think?” (Turing 1950). The imitation game
has a human observer who tries to guess the sex of
two players, one of which is a man and the other is
a woman, but while screened from being able to
tell which is which by voice, or appearance. Turing
suggested putting a machine in the place of one of
the humans and essentially playing the same game.
If the observer can not tell which is the machine
and which is the human, this can be taken as strong
evidence that the machine can think.
Turing’s proposal provided the inspiration for
the Loebner Prize competition, which was an at-
tempt to implement the Turing test. The first con-
test organized by Dr. Robert Epstein was held on
1991, in Boston’s Computer Museum. In this in-
carnation the test was known as the Loebner con-
test, as Dr. Hugh Loebner pledged a $100,000
grand prize for the first computer program to pass
</bodyText>
<footnote confidence="0.726938">
3 http://www.comp.leeds.ac.uk
</footnote>
<bodyText confidence="0.998163418604651">
the test. At the beginning it was decided to limit
the topic, in order to limit the amount of language
the contestant programs must be able to cope with,
and to limit the tenor. Ten agents were used, 6
were computer programs. Ten judges would con-
verse with the agents for fifteen minutes and rank
the terminals in order from the apparently least
human to most human. The computer with the
highest median rank wins that year’s prize. Joseph
Weintraub won the first, second and third Loebner
Prize in 1991, 1992, and 1993 for his chatbots, PC
Therapist, PC Professor, which discusses men ver-
sus women, and PC Politician, which discusses
Liberals versus Conservatives. In 1994 Thomas
Whalen (Whalen 2003) won the prize for his pro-
gram TIPS, which provides information on a par-
ticular topic. TIPS provides ways to store,
organize, and search the important parts of sen-
tences collected and analysed during system tests.
However there are sceptics who doubt the ef-
fectiveness of the Turing Test and/or the Loebner
Competition. Block, who thought that “the Turing
test is a sorely inadequate test of intelligence be-
cause it relies solely on the ability to fool people”;
and Shieber (1994), who argued that intelligence is
not determinable simply by surface behavior.
Shieber claimed the reason that Turing chose natu-
ral language as the behavioral definition of human
intelligence is “exactly its open-ended, free-
wheeling nature”, which was lost when the topic
was restricted during the Loebner Prize. Epstein
(1992) admitted that they have trouble with the
topic restriction, and they agreed “every fifth year
or so ... we would hold an open-ended test - one
with no topic restriction.” They decided that the
winner of a restricted test would receive a small
cash prize while the one who wins the unrestricted
test would receive the full $100,000.
Loebner in his responses to these arguments be-
lieved that unrestricted test is simpler, less expen-
sive and the best way to conduct the Turing Test.
Loebner presented three goals when constructing
the Loebner Prize (Loebner 1994):
</bodyText>
<listItem confidence="0.999452142857143">
• “No one was doing anything about the
Turing Test, not AI.” The initial Loebner
Prize contest was the first time that the
Turing Test had ever been formally tried.
• Increasing the public understanding of AI
is a laudable goal of Loebner Prize. “I be-
lieve that this contest will advance AI and
</listItem>
<page confidence="0.986437">
90
</page>
<bodyText confidence="0.928712">
serve as a tool to measure the state of the
art.”
</bodyText>
<listItem confidence="0.99682">
• Performing a social experiment.
</listItem>
<bodyText confidence="0.999797769230769">
The first open-ended implementation of the
Turing Test was applied in the 1995 contest, and
the prize was granted to Weintraub for the fourth
time. For more details to see other winners over
years are found in the Loebner Webpage4.
In this paper, we advocate alternative evalua-
tion methods, more appropriate to practical infor-
mation systems applications. We have investigated
methods to train and adapt ALICE to a specific
user’s language use or application, via a user-
supplied training corpus. Our evaluation takes ac-
count of open-ended trials by real users, rather than
controlled 10-minute trials.
</bodyText>
<sectionHeader confidence="0.997672" genericHeader="method">
3 The ALICE/AIML chatbot architecture
</sectionHeader>
<bodyText confidence="0.913593458333333">
AIML consists of data objects called AIML ob-
jects, which are made up of units called topics and
categories. The topic is an optional top-level ele-
ment; it has a name attribute and a set of categories
related to that topic. Categories are the basic units
of knowledge in AIML. Each category is a rule for
matching an input and converting to an output, and
consists of a pattern, which matches against the
user input, and a template, which is used in gener-
ating the Alice chatbot answer. The format struc-
ture of AIML is shown in figure 1.
&lt; aiml version=”1.0” &gt;
&lt; topic name=” the topic” &gt;
&lt;category&gt;
&lt;pattern&gt;PATTERN&lt;/pattern&gt;
&lt;that&gt;THAT&lt;/that&gt;
&lt;template&gt;Template&lt;/template&gt;
&lt;/category&gt;
..
..
&lt;/topic&gt;
&lt;/aiml&gt;
The &lt;that&gt; tag is optional and means that the cur-
rent pattern depends on a previous bot output.
</bodyText>
<figureCaption confidence="0.978279">
Figure 1. AIML format
</figureCaption>
<footnote confidence="0.712317">
4 http://www.loebner.net/Prizef/loebner-prize.html
</footnote>
<bodyText confidence="0.99994730952381">
The AIML pattern is simple, consisting only of
words, spaces, and the wildcard symbols _ and *.
The words may consist of letters and numerals, but
no other characters. Words are separated by a sin-
gle space, and the wildcard characters function like
words. The pattern language is case invariant. The
idea of the pattern matching technique is based on
finding the best, longest, pattern match. Three
types of AIML categories are used: atomic cate-
gory, are those with patterns that do not have wild-
card symbols, _ and *; default categories are
those with patterns having wildcard symbols * or
_. The wildcard symbols match any input but can
differ in their alphabetical order. For example,
given input ‘hello robot’, if ALICE does not find a
category with exact matching atomic pattern, then
it will try to find a category with a default pattern;
The third type, recursive categories are those with
templates having &lt;srai&gt; and &lt;sr&gt; tags, which refer
to simply recursive artificial intelligence and sym-
bolic reduction. Recursive categories have many
applications: symbolic reduction that reduces com-
plex grammatical forms to simpler ones; divide
and conquer that splits an input into two or more
subparts, and combines the responses to each; and
dealing with synonyms by mapping different ways
of saying the same thing to the same reply.
The knowledge bases of almost all chatbots are
edited manually which restricts users to specific
languages and domains. We developed a Java pro-
gram to read a text from a machine readable text
(corpus) and convert it to AIML format. The chat-
bot-training-program was built to be general, the
generality in this respect implies, no restrictions on
specific language, domain, or structure. Different
languages were tested: English, Arabic, Afrikaans,
French, and Spanish. We also trained with a range
of different corpus genres and structures, includ-
ing: dialogue, monologue, and structured text
found in the Qur’an, and FAQ websites.
The chatbot-training-program is composed of
four phases as follows:
</bodyText>
<listItem confidence="0.959016555555556">
• Reading module which reads the dialogue
text from the basic corpus and inserts it
into a list.
• Text reprocessing module, where all cor-
pus and linguistic annotations such as
overlapping, fillers and others are filtered.
• Converter module, where the pre-
processed text is passed to the converter to
consider the first turn as a pattern and the
</listItem>
<page confidence="0.995067">
91
</page>
<bodyText confidence="0.947092">
second as a template. All punctuation is
removed from the patterns, and the pat-
terns are transformed to upper case.
</bodyText>
<listItem confidence="0.993916333333333">
• Producing the AIML files by copying the
generated categories from the list to the
AIML file.
</listItem>
<bodyText confidence="0.528227">
An example of a sequence of two utter-
ances from an English spoken corpus is:
</bodyText>
<figure confidence="0.716939352941176">
&lt;u who=F72PS002&gt;
&lt;s n=&amp;quot;32&amp;quot;&gt;&lt;w ITJ&gt;Hello&lt;c PUN&gt;.
&lt;/u&gt;
&lt;u who=PS000&gt;
&lt;s n=&amp;quot;33&amp;quot;&gt;&lt;w ITJ&gt;Hello &lt;w NP0&gt;Donald&lt;c
PUN&gt;.
&lt;/u&gt;
After the reading and the text processing
phase, the text becomes:
F72PS002: Hello
PS000: Hello Donald
The corresponding AIML atomic category that
is generated from the converter modules looks like:
&lt;category&gt;
&lt;pattern&gt;HELLO&lt;/pattern&gt;
&lt;template&gt;Hello Donald&lt;/template&gt;
&lt;/category&gt;
</figure>
<bodyText confidence="0.986239764705882">
As a result different prototypes were developed,
in each prototype, different machine-learning tech-
niques were used and a new chatbot was tested.
The machine learning techniques ranged from a
primitive simple technique like single word match-
ing to more complicated ones like matching the
least frequent words. Building atomic categories
and comparing the input with all atomic patterns to
find a match is an instance based learning tech-
nique. However, the learning approach does not
stop at this level, but it improved the matching
process by using the most significant words (least
frequent word). This increases the ability of find-
ing a nearest match by extending the knowledge
base which is used during the matching process.
Three prototypes will be discussed in this paper as
listed below:
</bodyText>
<listItem confidence="0.955511176470588">
• The KGA prototype that is trained by a
corpus of spoken Afrikaans. In this proto-
type two learning approaches were
adopted. The first word and the most sig-
nificant word (least frequent word) ap-
proach;
• The Qur’an prototype that is trained by the
holy book of Islam (Qur’an): where in ad-
dition to the first word approach, two sig-
nificant word approaches (least frequent
words) were used, and the system was
adapted to deal with the Arabic language
and the non-conversational nature of
Qur’an as shown in section 5;
• The FAQchat prototype that is used in the
FAQ of the School of Computing at Uni-
versity of Leeds. The same learning tech-
</listItem>
<bodyText confidence="0.867223461538462">
niques were used, where the question
represents the pattern and the answer rep-
resents the template. Instead of chatting for
just 10 minutes as suggested by the Loeb-
ner Prize, we advocate alternative evalua-
tion methods more attuned to and
appropriate to practical information sys-
tems applications. Our evaluation takes ac-
count of open-ended trials by real users,
rather than artificial 10-minute trials as il-
lustrated in the following sections.
The aim of the different evaluations method-
ologies is as follows:
</bodyText>
<listItem confidence="0.983858">
• Evaluate the success of the learning tech-
niques in giving answers, based on dia-
logue efficiency, quality and users’
satisfaction applied on the KGA.
• Evaluate the ability to use the chatbot as a
tool to access an information source, and a
useful application for this, which was ap-
plied on the Qur&apos;an corpus.
• Evaluate the ability of using the chatbot as
an information retrieval system by com-
paring it with a search engine, which was
applied on FAQchat.
</listItem>
<sectionHeader confidence="0.794943" genericHeader="method">
4 Evaluation of the KGA prototype
</sectionHeader>
<bodyText confidence="0.999881333333333">
We developed two versions of the ALICE that
speaks Afrikaans language, Afrikaana that speaks
only Afrikaans and AVRA that speaks English and
Afrikaans; this was inspired by our observation
that the Korpus Gesproke Afrikaans actually in-
cludes some English, as Afrikaans speakers are
generally bilingual and “code-switch” comfortably.
We mounted prototypes of the chatbots on web-
sites using Pandorabot service5, and encouraged
</bodyText>
<footnote confidence="0.914722">
5 http://www.pandorabots.com/pandora
</footnote>
<page confidence="0.995234">
92
</page>
<bodyText confidence="0.99984525">
open-ended testing and feedback from remote us-
ers in South Africa; this allowed us to refine the
system more effectively.
We adopted three evaluation metrics:
</bodyText>
<listItem confidence="0.999123166666667">
• Dialogue efficiency in terms of matching
type.
• Dialogue quality metrics based on re-
sponse type.
• Users&apos; satisfaction assessment based on an
open-ended request for feedback.
</listItem>
<subsectionHeader confidence="0.993819">
4.1 Dialogue efficiency metric
</subsectionHeader>
<bodyText confidence="0.996521142857143">
We measured the efficiency of 4 sample dia-
logues in terms of atomic match, first word match,
most significant match, and no match. We wanted
to measure the efficiency of the adopted learning
mechanisms to see if they increase the ability to
find answers to general user input as shown in ta-
ble 1.
</bodyText>
<table confidence="0.9997745">
Matching Type D1 D2 D3 D4
Atomic 1 3 6 3
First word 9 15 23 4
Most significant 13 2 19 9
No match 0 1 3 1
Number of turns 23 21 51 17
</table>
<tableCaption confidence="0.999585">
Table 1. Response type frequency
</tableCaption>
<bodyText confidence="0.998018444444445">
The frequency of each type in each dialogue
generated between the user and the Afrikaans
chatbot was calculated; in Figure 2, these absolute
frequencies are normalised to relative probabilities.
No significant test was applied, this approach to
evaluation via dialogue efficiency metrics illus-
trates that the first word and the most significant
approach increase the ability to generate answers
to users and let the conversation continue.
</bodyText>
<figureCaption confidence="0.8988175">
Figure 2. Dialogue efficiency: Response Type
Relative Frequencies
</figureCaption>
<subsectionHeader confidence="0.976621">
4.2 Dialogue quality metric
</subsectionHeader>
<bodyText confidence="0.999994285714286">
In order to measure the quality of each re-
sponse, we wanted to classify responses according
to an independent human evaluation of “reason-
ableness”: reasonable reply, weird but understand-
able, or nonsensical reply. We gave the transcript
to an Afrikaans-speaking teacher and asked her to
mark each response according to these classes. The
number of turns in each dialogue and the frequen-
cies of each response type were estimated. Figure 3
shows the frequencies normalised to relative prob-
abilities of each of the three categories for each
sample dialogue. For this evaluator, it seems that
“nonsensical” responses are more likely than rea-
sonable or understandable but weird answers.
</bodyText>
<subsectionHeader confidence="0.994637">
4.3 Users&apos; satisfaction
</subsectionHeader>
<bodyText confidence="0.989195125">
The first prototypes were based only on literal
pattern matching against corpus utterances: we had
not implemented the first word approach and least-
frequent word approach to add “wildcard” default
categories. Our Afrikaans-speaking evaluators
found these first prototypes disappointing and frus-
trating: it turned out that few of their attempts at
conversation found exact matches in the training
corpus, so Afrikaana replied with a default “ja”
most of the time. However, expanding the AIML
pattern matching using the first-word and least-
frequent-word approaches yielded more favorable
feedback. Our evaluators found the conversations
less repetitive and more interesting. We measure
user satisfaction based on this kind of informal
user feed back.
</bodyText>
<figureCaption confidence="0.9829655">
Figure 3. The quality of the Dialogue: Response
type relative probabilities
</figureCaption>
<figure confidence="0.999349958333333">
repetition (%)
0.8
0.6
0.4
0.2
0
Matching Types
Atomic
Most
significant
Match
nothing
First word
Repetion (%)
0.80
0.60
0.40
0.20
0.00
1.00
Response Types
reasonable
Weird
Non sensical
</figure>
<page confidence="0.983917">
93
</page>
<sectionHeader confidence="0.857052" genericHeader="method">
5 Evaluation of the Qur&apos;an prototype
</sectionHeader>
<bodyText confidence="0.9999756875">
In this prototype a parallel corpus of Eng-
lish/Arabic of the holy book of Islam was used, the
aim of the Qur’an prototype is to explore the prob-
lem of using the Arabic language and of using a
text which is not conversational in its nature like
the Qur’an. The Qur’an is composed of 114 soora
(chapters), and each soora is composed of different
number of verses. The same learning technique as
the KGA prototype were applied, where in this
case if an input was a whole verse, the response
will be the next verse of the same soora; or if an
input was a question or a statement, the output will
be all verses which seems appropriate based on the
significant word. To measure the quality of the
answers of the Qur’an chatbot version, the follow-
ing approach was applied:
</bodyText>
<listItem confidence="0.992003307692308">
1. Random sentences from Islamic sites were
selected and used as inputs of the Eng-
lish/Arabic version of the Qur’an.
2. The resulting transcripts which have 67
turns were given to 5 Muslims and 6 non-
Muslims students, who were asked to label
each turn in terms of:
• Related (R), in case the answer was correct
and in the same topic as the input.
• Partially related (PR), in case the answer
was not correct, but in the same topic.
• Not related (NR), in case the answer was
not correct and in a different topic.
</listItem>
<bodyText confidence="0.86691905">
Proportions of each label and each class of us-
ers (Muslims and non-Muslims) were calculated as
the total number over number of users times num-
ber of turns. Four out of the 67 turns returned no
answers, therefore actually 63 turns were used as
presented in figure 4.
In the transcripts used, more than half of the re-
sults were not related to their inputs. A small dif-
ference can be noticed between Muslims and non-
Muslims proportions. Approximately one half of
answers in the sample were not related from non-
Muslims’ point of view, whereas this figure is 58%
from the Muslims’ perspective. Explanation for
this includes:
• The different interpretation of the answers.
The Qur’an uses traditional Arabic lan-
guage, which is sometimes difficult to un-
derstand without knowing the meaning of
some words, and the historical story be-
hind each verse.
</bodyText>
<listItem confidence="0.9808188">
• The English translation of the Qur’an is
not enough to judge if the verse is related
or not, especially given that non-Muslims
do not have the background knowledge of
the Qur’an.
</listItem>
<bodyText confidence="0.999832">
Using chatting to access the Qur’an looks like
the use of a standard Qur’an search tool. In fact it
is totally different; a searching tool usually
matches words not statements. For example, if the
input is: “How shall I pray?” using chatting: the
robot will give you all ayyas where the word
“pray” is found because it is the most significant
word. However, using a search tool6 will not give
you any match. If the input was just the word
“pray”, using chatting will give you the same an-
swer as the previous, and the searching tool will
provide all ayyas that have “pray” as a string or
substring, so words such as: ”praying, prayed, etc.”
will match.
Another important difference is that in the
search tool there is a link between any word and
the document it is in, but in the chatting system
there is a link just for the most significant words,
so if it happened that the input statement involves a
significant word(s), a match will be found, other-
wise the chatbot answer will be: “I have no answer
for that”.
Figure4. The Qur’an proportion of each answer
type denoted by users
</bodyText>
<sectionHeader confidence="0.812749" genericHeader="evaluation">
6 Evaluation of the FAQchat prototype
</sectionHeader>
<bodyText confidence="0.996289">
To evaluate FAQchat, an interface was built,
which has a box to accept the user input, and a but-
ton to send this to the system. The outcomes ap-
</bodyText>
<figure confidence="0.955134384615385">
6 http://www.islamicity.com/QuranSearch/
Answer types
Proportion 70% Muslims
60% Non Muslims
50% Overall
40%
30%
20%
10%
0%
Related Partialy Not related
Related
Answers
</figure>
<page confidence="0.996886">
94
</page>
<bodyText confidence="0.999987194444444">
pear in two columns: one holds the FAQchat an-
swers, and the other holds the Google answers af-
ter filtering Google to the FAQ database only.
Google allows search to be restricted to a given
URL, but this still yields all matches from the
whole SoC website (http://www.comp.leeds.ac.uk)
so a Perl script was required to exclude matches
not from the FAQ sub-pages.
An evaluation sheet was prepared which con-
tains 15 information-seeking tasks or questions on
a range of different topics related to the FAQ data-
base. The tasks were suggested by a range of users
including SoC staff and research students to cover
the three possibilities where the FAQchat could
find a direct answer, links to more than one possi-
ble answer, and where the FAQchat could not find
any answer. In order not to restrict users to these
tasks, and not to be biased to specific topics, the
evaluation sheet included spaces for users to try 5
additional tasks or questions of their own choosing.
Users were free to decide exactly what input-string
to give to FAQchat to find an answer: they were
not required to type questions verbatim; users were
free to try more than once: if no appropriate an-
swer was found; users could reformulate the query.
The evaluation sheet was distributed among 21
members of the staff and students. Users were
asked to try using the system, and state whether
they were able to find answers using the FAQchat
responses, or using the Google responses; and
which of the two they preferred and why.
Twenty-one users tried the system; nine mem-
bers of the staff and the rest were postgraduates.
The analysis was tackled in two directions: the
preference and the number of matches found per
question and per user.
</bodyText>
<subsectionHeader confidence="0.999452">
6.1 Number of matches per question
</subsectionHeader>
<bodyText confidence="0.999682727272727">
The number of evaluators who managed to find
answers by FAQchat and Google was counted, for
each question.
Results in table 2 shows that 68% overall of our
sample of users managed to find answers using the
FAQchat while 46% found it by Google. Since
there is no specific format to ask the question,
there are cases where some users could find an-
swers while others could not. The success in find-
ing answers is based on the way the questions were
presented to FAQchat.
</bodyText>
<table confidence="0.97989">
Users Mean of users find- Proportion of find-
/Tool ing answers ing answers
FAQchat Google FAQchat Google
Staff 5.53 3.87 61% 43%
Student 8.8 5.87 73% 49%
Overall 14.3 9.73 68% 46%
</table>
<tableCaption confidence="0.999744">
Table 2: Proportion of users finding answers
</tableCaption>
<bodyText confidence="0.999957">
Of the overall sample, the staff outcome shows
that 61% were able to find answers by FAQchat
where 73% of students managed to do so; students
were more successful than staff.
</bodyText>
<subsectionHeader confidence="0.997421">
6.2 The preferred tool per each question
</subsectionHeader>
<bodyText confidence="0.93051175">
For each question, users were asked to state
which tool they preferred to use to find the answer.
The proportion of users who preferred each tool
was calculated. Results in figure 5 shows that 51%
of the staff, 41% of the students, and 47% overall
preferred using FAQchat against 11% who pre-
ferred the Google.
Figure5. Proportion of preferred tool
</bodyText>
<subsectionHeader confidence="0.961631">
6.3 Number of matches and preference found
per user
</subsectionHeader>
<bodyText confidence="0.999911">
The number of answers each user had found
was counted. The proportions found were the
same. The evaluation sheet ended with an open
section inviting general feedback. The following is
a summary of the feedback we obtained:
</bodyText>
<listItem confidence="0.913153">
• Both staff and students preferred using the
FAQchat for two main reasons:
1. The ability to give direct answers some-
times while Google only gives links.
2. The number of links returned by the
FAQchat is less than those returned by
Google for some questions, which saves
time browsing/searching.
</listItem>
<figure confidence="0.9927963">
Which tool do you prefer?
Avearge percentage 60% Staff
number 50% Student
40% Total
30%
20%
10%
0%
FAQchat Google
Tool
</figure>
<page confidence="0.967151">
95
</page>
<listItem confidence="0.91031825">
• Users who preferred Google justified their
preference for two reasons:
1. Prior familiarity with using Google.
2. FAQchat seemed harder to steer with care-
</listItem>
<bodyText confidence="0.975280363636364">
fully chosen keywords, but more often did
well on the first try. This happens because
FAQchat gives answers if the keyword
matches a significant word. The same will
occur if you reformulate the question and
the FAQchat matches the same word.
However Google may give different an-
swers in this case.
To test reliability of these results, the t=Test
were applied, the outcomes ensure the previous
results.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999903941176471">
The Loebner Prize Competition has been used
to evaluate the ability of chatbots to fool people
that they are speaking to humans. Comparing the
dialogues generated from ALICE, which won the
Loebner Prize with real human dialogues, shows
that ALICE tries to use explicit dialogue-act lin-
guistic expressions more than usual to re enforce
the impression that users are speaking to human.
Our general conclusion is that we should NOT
adopt an evaluation methodology just because a
standard has been established, such as the Loebner
Prize evaluation methodology adopted by most
chatbot developers. Instead, evaluation should be
adapted to the application and to user needs. If the
chatbot is meant to be adapted to provide a specific
service for users, then the best evaluation is based
on whether it achieves that service or task
</bodyText>
<sectionHeader confidence="0.999065" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997587615384615">
Abu Shawar B and Atwell E. 2003. Using dialogue
corpora to retrain a chatbot system. In Proceedings of
the Corpus Linguistics 2003 conference, Lancaster
University, UK, pp681-690.
Batacharia, B., Levy, D., Catizone R., Krotov A. and
Wilks, Y. 1999. CONVERSE: a conversational com-
panion. In Wilks, Y. (ed.), Machine Conversations.
Kluwer, Boston/Drdrecht/London, pp. 205-215.
Colby, K. 1999a. Comments on human-computer con-
versation. In Wilks, Y. (ed.), Machine Conversations.
Kluwer, Boston/Drdrecht/London, pp. 5-8.
Colby, K. 1999b. Human-computer conversation in a
cognitive therapy program. In Wilks, Y. (ed.), Ma-
chine Conversations. Kluwer, Bos-
ton/Drdrecht/London, pp. 9-19.
Epstein R. 1992. Can Machines Think?. AI magazine,
Vol 13, No. 2, pp80-95
Garner R. 1994. The idea of RED, [Online],
http://www.alma.gq.nu/docs/ideafred_garner.htm
Hirschman L. 1995. The Roles of language processing
in a spoken language interface. In Voice Communi-
cation Between Humans and Machines, D. Roe and J.
Wilpon (Eds), National Academy Press Washinton,
DC, pp217-237.
Hutchens, J. 1996. How to pass the Turing test by
cheating. [Onlin], http://ciips.ee.uwa.edu.au/Papers/,
1996
Hutchens, T., Alder, M. 1998. Introducing MegaHAL.
[Online],
http://cnts.uia.ac.be/conll98/pdf/271274hu.pdf
Loebner H. 1994. In Response to lessons from a re-
stricted Turing Test. [Online],
http://www.loebner.net/Prizef/In-response.html
Maier E, Mast M, and LuperFoy S. 1996. Overview.
In Elisabeth Maier, Marion Mast, and Susan Luper-
Foy (Eds), Dialogue Processing in Spoken Language
Systems, , Springer, Berlin, pp1-13.
McTear M. 2002. Spoken dialogue technology: ena-
bling the conversational user interface. ACM Com-
puting Surveys. Vol. 34, No. 1, pp. 90-169.
Shieber S. 1994. Lessons from a Restricted Turing
Test. Communications of the Association for Com-
puting Machinery, Vol 37, No. 6, pp70-78
Turing A. 1950. Computing Machinery and intelli-
gence. Mind 59, 236, 433-460.
Weizenbaum, J. 1966. ELIZA-A computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM. Vol. 10, No. 8, pp. 36-45.
Whalen T. 2003. My experience with 1994 Loebner
competition, [Online],
http://hps.elte.hu/~gk/Loebner/story94.htm
</reference>
<page confidence="0.998344">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439216">
<title confidence="0.997638">Different measurements metrics to evaluate a chatbot system</title>
<author confidence="0.996841">Bayan Abu</author>
<affiliation confidence="0.927044">IT</affiliation>
<author confidence="0.72771">Arab Open</author>
<email confidence="0.8244135">[add]bshawar@arabou-jo.edu.jo</email>
<author confidence="0.99979">Eric Atwell</author>
<affiliation confidence="0.999655">School of University of</affiliation>
<address confidence="0.935884">LS2 9JT, Leeds-UK</address>
<email confidence="0.961973">eric@comp.leeds.ac.uk</email>
<abstract confidence="0.998575391304348">A chatbot is a software system, which can interact or “chat” with a human user in natural language such as English. For the annual Loebner Prize contest, rival chatbots have been assessed in terms of ability to fool a judge in a restricted chat session. We are investigating methods to train and adapt a chatbot to a specific user’s language use or application, via a usersupplied training corpus. We advocate open-ended trials by real users, such as an example Afrikaans chatbot for Afrikaansspeaking researchers and students in South Africa. This is evaluated in terms of “glass box” dialogue efficiency metrics, and “black box” dialogue quality metrics and user satisfaction feedback. The other examples presented in this paper are the Qur&apos;an and the FAQchat prototypes. Our general conclusion is that evaluation should be adapted to the application and to user needs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abu Shawar B</author>
<author>E Atwell</author>
</authors>
<title>Using dialogue corpora to retrain a chatbot system.</title>
<date>2003</date>
<booktitle>In Proceedings of the Corpus Linguistics</booktitle>
<pages>681--690</pages>
<location>Lancaster University, UK,</location>
<marker>B, Atwell, 2003</marker>
<rawString>Abu Shawar B and Atwell E. 2003. Using dialogue corpora to retrain a chatbot system. In Proceedings of the Corpus Linguistics 2003 conference, Lancaster University, UK, pp681-690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Batacharia</author>
<author>D Levy</author>
<author>R Catizone</author>
<author>A Krotov</author>
<author>Y Wilks</author>
</authors>
<title>CONVERSE: a conversational companion.</title>
<date>1999</date>
<booktitle>Machine Conversations. Kluwer, Boston/Drdrecht/London,</booktitle>
<pages>205--215</pages>
<editor>In Wilks, Y. (ed.),</editor>
<marker>Batacharia, Levy, Catizone, Krotov, Wilks, 1999</marker>
<rawString>Batacharia, B., Levy, D., Catizone R., Krotov A. and Wilks, Y. 1999. CONVERSE: a conversational companion. In Wilks, Y. (ed.), Machine Conversations. Kluwer, Boston/Drdrecht/London, pp. 205-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Colby</author>
</authors>
<title>Comments on human-computer conversation.</title>
<date>1999</date>
<booktitle>Machine Conversations. Kluwer, Boston/Drdrecht/London,</booktitle>
<pages>5--8</pages>
<editor>In Wilks, Y. (ed.),</editor>
<contexts>
<context position="1390" citStr="Colby 1999" startWordPosition="219" endWordPosition="220"> Africa. This is evaluated in terms of “glass box” dialogue efficiency metrics, and “black box” dialogue quality metrics and user satisfaction feedback. The other examples presented in this paper are the Qur&apos;an and the FAQchat prototypes. Our general conclusion is that evaluation should be adapted to the application and to user needs. 1 Introduction “Before there were computers, we could distinguish persons from non-persons on the basis of an ability to participate in conversations. But now, we have hybrids operating between person and non persons with whom we can talk in ordinary language.” (Colby 1999a). Human machine conversation as a technology integrates different areas where the core is the language, and the computational methodologies facilitate communication between users and computers using natural language. A related term to machine conversation is the chatbot, a conversational agent that interacts with 89 users turn by turn using natural language. Different chatbots or human-computer dialogue systems have been developed using text communication such as Eliza (Weizenbaum 1966), PARRY (Colby 1999b), CONVERSE (Batacharia etc 1999), ALICE1. Chatbots have been used in different domains</context>
</contexts>
<marker>Colby, 1999</marker>
<rawString>Colby, K. 1999a. Comments on human-computer conversation. In Wilks, Y. (ed.), Machine Conversations. Kluwer, Boston/Drdrecht/London, pp. 5-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Colby</author>
</authors>
<title>Human-computer conversation in a cognitive therapy program.</title>
<date>1999</date>
<booktitle>Machine Conversations. Kluwer, Boston/Drdrecht/London,</booktitle>
<pages>9--19</pages>
<editor>In Wilks, Y. (ed.),</editor>
<contexts>
<context position="1390" citStr="Colby 1999" startWordPosition="219" endWordPosition="220"> Africa. This is evaluated in terms of “glass box” dialogue efficiency metrics, and “black box” dialogue quality metrics and user satisfaction feedback. The other examples presented in this paper are the Qur&apos;an and the FAQchat prototypes. Our general conclusion is that evaluation should be adapted to the application and to user needs. 1 Introduction “Before there were computers, we could distinguish persons from non-persons on the basis of an ability to participate in conversations. But now, we have hybrids operating between person and non persons with whom we can talk in ordinary language.” (Colby 1999a). Human machine conversation as a technology integrates different areas where the core is the language, and the computational methodologies facilitate communication between users and computers using natural language. A related term to machine conversation is the chatbot, a conversational agent that interacts with 89 users turn by turn using natural language. Different chatbots or human-computer dialogue systems have been developed using text communication such as Eliza (Weizenbaum 1966), PARRY (Colby 1999b), CONVERSE (Batacharia etc 1999), ALICE1. Chatbots have been used in different domains</context>
</contexts>
<marker>Colby, 1999</marker>
<rawString>Colby, K. 1999b. Human-computer conversation in a cognitive therapy program. In Wilks, Y. (ed.), Machine Conversations. Kluwer, Boston/Drdrecht/London, pp. 9-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Epstein</author>
</authors>
<title>Can Machines Think?.</title>
<date>1992</date>
<journal>AI magazine, Vol</journal>
<volume>13</volume>
<pages>80--95</pages>
<contexts>
<context position="7507" citStr="Epstein (1992)" startWordPosition="1208" endWordPosition="1209">during system tests. However there are sceptics who doubt the effectiveness of the Turing Test and/or the Loebner Competition. Block, who thought that “the Turing test is a sorely inadequate test of intelligence because it relies solely on the ability to fool people”; and Shieber (1994), who argued that intelligence is not determinable simply by surface behavior. Shieber claimed the reason that Turing chose natural language as the behavioral definition of human intelligence is “exactly its open-ended, freewheeling nature”, which was lost when the topic was restricted during the Loebner Prize. Epstein (1992) admitted that they have trouble with the topic restriction, and they agreed “every fifth year or so ... we would hold an open-ended test - one with no topic restriction.” They decided that the winner of a restricted test would receive a small cash prize while the one who wins the unrestricted test would receive the full $100,000. Loebner in his responses to these arguments believed that unrestricted test is simpler, less expensive and the best way to conduct the Turing Test. Loebner presented three goals when constructing the Loebner Prize (Loebner 1994): • “No one was doing anything about th</context>
</contexts>
<marker>Epstein, 1992</marker>
<rawString>Epstein R. 1992. Can Machines Think?. AI magazine, Vol 13, No. 2, pp80-95</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garner</author>
</authors>
<title>The idea of RED,</title>
<date>1994</date>
<note>[Online], http://www.alma.gq.nu/docs/ideafred_garner.htm</note>
<marker>Garner, 1994</marker>
<rawString>Garner R. 1994. The idea of RED, [Online], http://www.alma.gq.nu/docs/ideafred_garner.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>The Roles of language processing in a spoken language interface.</title>
<date>1995</date>
<booktitle>In Voice Communication Between Humans</booktitle>
<pages>217--237</pages>
<publisher>National Academy Press</publisher>
<location>Washinton, DC,</location>
<contexts>
<context position="2348" citStr="Hirschman 1995" startWordPosition="361" endWordPosition="362">g natural language. Different chatbots or human-computer dialogue systems have been developed using text communication such as Eliza (Weizenbaum 1966), PARRY (Colby 1999b), CONVERSE (Batacharia etc 1999), ALICE1. Chatbots have been used in different domains such as: customer service, education, web site help, and for fun. Different mechanisms are used to evaluate Spoken Dialogue Systems (SLDs), ranging from glass box evaluation that evaluates individual components, to black box evaluation that evaluates the system as a whole McTear (2002). For example, glass box evaluation was applied on the (Hirschman 1995) ARPA Spoken Language system, and it shows that the error rate for sentence understanding was much lower than that for sentence recognition. On the other hand black box evaluation evaluates the system as a whole based on user satisfaction and acceptance. The black box approach evaluates the performance of the system in terms of achieving its task, the cost of achieving the task in terms of time taken and number of turns, and measures the quality of the interaction, normally summarised by the term ‘user satisfaction’, which indicates whether the user “ gets the information s/he wants, is s/he c</context>
</contexts>
<marker>Hirschman, 1995</marker>
<rawString>Hirschman L. 1995. The Roles of language processing in a spoken language interface. In Voice Communication Between Humans and Machines, D. Roe and J. Wilpon (Eds), National Academy Press Washinton, DC, pp217-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hutchens</author>
</authors>
<title>How to pass the Turing test by cheating. [Onlin],</title>
<date>1996</date>
<location>http://ciips.ee.uwa.edu.au/Papers/,</location>
<marker>Hutchens, 1996</marker>
<rawString>Hutchens, J. 1996. How to pass the Turing test by cheating. [Onlin], http://ciips.ee.uwa.edu.au/Papers/, 1996</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hutchens</author>
<author>M Alder</author>
</authors>
<title>Introducing MegaHAL.</title>
<date>1998</date>
<publisher>[Online],</publisher>
<marker>Hutchens, Alder, 1998</marker>
<rawString>Hutchens, T., Alder, M. 1998. Introducing MegaHAL. [Online],</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Loebner</author>
</authors>
<title>In Response to lessons from a restricted Turing Test.</title>
<date>1994</date>
<note>[Online], http://www.loebner.net/Prizef/In-response.html</note>
<contexts>
<context position="8068" citStr="Loebner 1994" startWordPosition="1303" endWordPosition="1304"> restricted during the Loebner Prize. Epstein (1992) admitted that they have trouble with the topic restriction, and they agreed “every fifth year or so ... we would hold an open-ended test - one with no topic restriction.” They decided that the winner of a restricted test would receive a small cash prize while the one who wins the unrestricted test would receive the full $100,000. Loebner in his responses to these arguments believed that unrestricted test is simpler, less expensive and the best way to conduct the Turing Test. Loebner presented three goals when constructing the Loebner Prize (Loebner 1994): • “No one was doing anything about the Turing Test, not AI.” The initial Loebner Prize contest was the first time that the Turing Test had ever been formally tried. • Increasing the public understanding of AI is a laudable goal of Loebner Prize. “I believe that this contest will advance AI and 90 serve as a tool to measure the state of the art.” • Performing a social experiment. The first open-ended implementation of the Turing Test was applied in the 1995 contest, and the prize was granted to Weintraub for the fourth time. For more details to see other winners over years are found in the Lo</context>
</contexts>
<marker>Loebner, 1994</marker>
<rawString>Loebner H. 1994. In Response to lessons from a restricted Turing Test. [Online], http://www.loebner.net/Prizef/In-response.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Maier</author>
<author>M Mast</author>
<author>S LuperFoy</author>
</authors>
<date>1996</date>
<booktitle>Overview. In Elisabeth Maier, Marion Mast, and Susan LuperFoy (Eds), Dialogue Processing in Spoken Language Systems, ,</booktitle>
<pages>1--13</pages>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="3057" citStr="Maier et al 1996" startWordPosition="479" endWordPosition="482">s much lower than that for sentence recognition. On the other hand black box evaluation evaluates the system as a whole based on user satisfaction and acceptance. The black box approach evaluates the performance of the system in terms of achieving its task, the cost of achieving the task in terms of time taken and number of turns, and measures the quality of the interaction, normally summarised by the term ‘user satisfaction’, which indicates whether the user “ gets the information s/he wants, is s/he comfortable with the system, and gets the information within acceptable elapsed time, etc.” (Maier et al 1996). The Loebner prize2 competition has been used to evaluate machine conversation chatbots. The Loebner Prize is a Turing test, which evaluates the ability of the machine to fool people that they are talking to human. In essence, judges are allowed a short chat (10 to 15 minutes) with each chatbot, and asked to rank them in terms of “naturalness”. ALICE (Abu Shawar and Atwell 2003) is the Artificial Linguistic Internet Computer Entity, first 1 http://www.alicebot.org/ 2 http://www.loebner.net/Prizef/loebner-prize.html Bridging the Gap: Academic and Industrial Research in Dialog Technologies Work</context>
</contexts>
<marker>Maier, Mast, LuperFoy, 1996</marker>
<rawString>Maier E, Mast M, and LuperFoy S. 1996. Overview. In Elisabeth Maier, Marion Mast, and Susan LuperFoy (Eds), Dialogue Processing in Spoken Language Systems, , Springer, Berlin, pp1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M McTear</author>
</authors>
<title>Spoken dialogue technology: enabling the conversational user interface.</title>
<date>2002</date>
<journal>ACM Computing Surveys.</journal>
<volume>34</volume>
<pages>90--169</pages>
<contexts>
<context position="2277" citStr="McTear (2002)" startWordPosition="349" endWordPosition="350">a conversational agent that interacts with 89 users turn by turn using natural language. Different chatbots or human-computer dialogue systems have been developed using text communication such as Eliza (Weizenbaum 1966), PARRY (Colby 1999b), CONVERSE (Batacharia etc 1999), ALICE1. Chatbots have been used in different domains such as: customer service, education, web site help, and for fun. Different mechanisms are used to evaluate Spoken Dialogue Systems (SLDs), ranging from glass box evaluation that evaluates individual components, to black box evaluation that evaluates the system as a whole McTear (2002). For example, glass box evaluation was applied on the (Hirschman 1995) ARPA Spoken Language system, and it shows that the error rate for sentence understanding was much lower than that for sentence recognition. On the other hand black box evaluation evaluates the system as a whole based on user satisfaction and acceptance. The black box approach evaluates the performance of the system in terms of achieving its task, the cost of achieving the task in terms of time taken and number of turns, and measures the quality of the interaction, normally summarised by the term ‘user satisfaction’, which </context>
</contexts>
<marker>McTear, 2002</marker>
<rawString>McTear M. 2002. Spoken dialogue technology: enabling the conversational user interface. ACM Computing Surveys. Vol. 34, No. 1, pp. 90-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Lessons from a Restricted Turing Test.</title>
<date>1994</date>
<journal>Communications of the Association for Computing Machinery, Vol</journal>
<volume>37</volume>
<pages>70--78</pages>
<contexts>
<context position="7180" citStr="Shieber (1994)" startWordPosition="1159" endWordPosition="1160">hich discusses men versus women, and PC Politician, which discusses Liberals versus Conservatives. In 1994 Thomas Whalen (Whalen 2003) won the prize for his program TIPS, which provides information on a particular topic. TIPS provides ways to store, organize, and search the important parts of sentences collected and analysed during system tests. However there are sceptics who doubt the effectiveness of the Turing Test and/or the Loebner Competition. Block, who thought that “the Turing test is a sorely inadequate test of intelligence because it relies solely on the ability to fool people”; and Shieber (1994), who argued that intelligence is not determinable simply by surface behavior. Shieber claimed the reason that Turing chose natural language as the behavioral definition of human intelligence is “exactly its open-ended, freewheeling nature”, which was lost when the topic was restricted during the Loebner Prize. Epstein (1992) admitted that they have trouble with the topic restriction, and they agreed “every fifth year or so ... we would hold an open-ended test - one with no topic restriction.” They decided that the winner of a restricted test would receive a small cash prize while the one who </context>
</contexts>
<marker>Shieber, 1994</marker>
<rawString>Shieber S. 1994. Lessons from a Restricted Turing Test. Communications of the Association for Computing Machinery, Vol 37, No. 6, pp70-78</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Turing</author>
</authors>
<title>Computing Machinery and intelligence.</title>
<date>1950</date>
<journal>Mind</journal>
<volume>59</volume>
<pages>433--460</pages>
<contexts>
<context position="5127" citStr="Turing 1950" startWordPosition="801" endWordPosition="802">of Islam (Qur’an), and the FAQ of the School of Computing at University of Leeds3 were used to produce two KGA prototype, the Qur’an prototype and the FAQchat one consequently. Section 2 presents Loebner Prize contest, section 3 illustrates the ALICE/AIMLE architecture. The evaluation techniques of the KGA prototype, the Qur’an prototype, and the FAQchat prototype are discussed in sections 4, 5, and 6 consequently. The conclusion is presented in section 7. 2 The Loebner Prize Competition The story began with the “imitation game” which was presented in Alan Turing’s paper “Can Machine think?” (Turing 1950). The imitation game has a human observer who tries to guess the sex of two players, one of which is a man and the other is a woman, but while screened from being able to tell which is which by voice, or appearance. Turing suggested putting a machine in the place of one of the humans and essentially playing the same game. If the observer can not tell which is the machine and which is the human, this can be taken as strong evidence that the machine can think. Turing’s proposal provided the inspiration for the Loebner Prize competition, which was an attempt to implement the Turing test. The firs</context>
</contexts>
<marker>Turing, 1950</marker>
<rawString>Turing A. 1950. Computing Machinery and intelligence. Mind 59, 236, 433-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weizenbaum</author>
</authors>
<title>ELIZA-A computer program for the study of natural language communication between man and machine.</title>
<date>1966</date>
<journal>Communications of the ACM.</journal>
<volume>10</volume>
<pages>36--45</pages>
<contexts>
<context position="1883" citStr="Weizenbaum 1966" startWordPosition="290" endWordPosition="291"> But now, we have hybrids operating between person and non persons with whom we can talk in ordinary language.” (Colby 1999a). Human machine conversation as a technology integrates different areas where the core is the language, and the computational methodologies facilitate communication between users and computers using natural language. A related term to machine conversation is the chatbot, a conversational agent that interacts with 89 users turn by turn using natural language. Different chatbots or human-computer dialogue systems have been developed using text communication such as Eliza (Weizenbaum 1966), PARRY (Colby 1999b), CONVERSE (Batacharia etc 1999), ALICE1. Chatbots have been used in different domains such as: customer service, education, web site help, and for fun. Different mechanisms are used to evaluate Spoken Dialogue Systems (SLDs), ranging from glass box evaluation that evaluates individual components, to black box evaluation that evaluates the system as a whole McTear (2002). For example, glass box evaluation was applied on the (Hirschman 1995) ARPA Spoken Language system, and it shows that the error rate for sentence understanding was much lower than that for sentence recogni</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Weizenbaum, J. 1966. ELIZA-A computer program for the study of natural language communication between man and machine. Communications of the ACM. Vol. 10, No. 8, pp. 36-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Whalen</author>
</authors>
<date>2003</date>
<booktitle>My experience with 1994 Loebner competition, [Online], http://hps.elte.hu/~gk/Loebner/story94.htm</booktitle>
<contexts>
<context position="6700" citStr="Whalen 2003" startWordPosition="1078" endWordPosition="1079">t of language the contestant programs must be able to cope with, and to limit the tenor. Ten agents were used, 6 were computer programs. Ten judges would converse with the agents for fifteen minutes and rank the terminals in order from the apparently least human to most human. The computer with the highest median rank wins that year’s prize. Joseph Weintraub won the first, second and third Loebner Prize in 1991, 1992, and 1993 for his chatbots, PC Therapist, PC Professor, which discusses men versus women, and PC Politician, which discusses Liberals versus Conservatives. In 1994 Thomas Whalen (Whalen 2003) won the prize for his program TIPS, which provides information on a particular topic. TIPS provides ways to store, organize, and search the important parts of sentences collected and analysed during system tests. However there are sceptics who doubt the effectiveness of the Turing Test and/or the Loebner Competition. Block, who thought that “the Turing test is a sorely inadequate test of intelligence because it relies solely on the ability to fool people”; and Shieber (1994), who argued that intelligence is not determinable simply by surface behavior. Shieber claimed the reason that Turing ch</context>
</contexts>
<marker>Whalen, 2003</marker>
<rawString>Whalen T. 2003. My experience with 1994 Loebner competition, [Online], http://hps.elte.hu/~gk/Loebner/story94.htm</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>