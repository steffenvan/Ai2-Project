<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000649">
<title confidence="0.981954">
UMichigan: A Conditional Random Field Model for Resolving the Scope of
Negation
</title>
<author confidence="0.808429">
Amjad Abu-Jbara
</author>
<affiliation confidence="0.884876">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.855569">
Ann Arbor, MI, USA
</address>
<email confidence="0.998983">
amjbara@umich.edu
</email>
<author confidence="0.908177">
Dragomir Radev
</author>
<affiliation confidence="0.9329415">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.85606">
Ann Arbor, MI, USA
</address>
<email confidence="0.99942">
radev@umich.edu
</email>
<sectionHeader confidence="0.995653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999211222222222">
In this paper, we present a system for de-
tecting negation in English text. We address
three tasks: negation cue detection, negation
scope resolution and negated event identifi-
cation. We pose these tasks as sequence la-
beling problems. For each task, we train a
Conditional Random Field (CRF) model on
lexical, structural, and syntactic features ex-
tracted from labeled data. The models are
trained and tested using the dataset distributed
with the *sem Shared Task 2012 on resolving
the scope and focus of negation. The system
detects negation cues with 90.98% F1 mea-
sure (94.3% and 87.88% recall). It identifies
negation scope with 82.70% F1 on token-by-
token level and 64.78% F1 on full scope level.
Negated events are detected with 51.10% F1
measure.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896804878049">
Negation is a linguistic phenomenon present in all
languages (Tottie, 1993; Horn, 1989). The seman-
tic function of negation is to transform an affirma-
tive statement into its opposite meaning. The auto-
matic detection of negation and its scope is a prob-
lem encountered in a wide range of natural language
processing applications including, but not limited to,
data mining, relation extraction, question answering,
and sentiment analysis. For example, failing to ac-
count for negation may result in giving wrong an-
swers in question answering systems or in the pre-
diction of opposite sentiment in sentiment analysis
systems.
The occurrence of negation in a sentence is deter-
mined by the presence of a negation cue. A nega-
tion cue is a word, a phrase, a prefix, or a postfix
that triggers negation. Scope of negation is the part
of the meaning that is negated (Huddleston and Pul-
lum, 2002). The negated event is the event or the en-
tity that the negation indicates its absence or denies
its occurrence. For example, in the sentence below
never is the negation cue. The scope is enclosed in
square brackets. The negated event is underlined.
[Andrew had] never [liked smart phones],
but he received one as a gift last week and
started to use it.
Negation cues and scopes may be discontinuous.
For example, the negation cue neither ... nor is dis-
continuous.
In this chapter, we present a system for automat-
ically detecting negation cues, negated events, and
negation scopes in English text. The system uses
conditional random field (CRF) models trained on
labeled sentences extracted from two classical En-
glish novels. The CRF models are trained using lex-
ical, structural, and syntactic features. The experi-
ments show promising results.
This paper is organized as follows. Section 2 re-
views previous work. Section 3 describes the data.
Section 4 describes the CRFs models. Section 5
presents evaluation, results, and discussion.
</bodyText>
<sectionHeader confidence="0.99446" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.973978">
Most research on negation has been done in the
biomedical domain (Chapman et al., 2001; Mutalik
et al., 2001; Kim and Park, 2006; Morante et al.,
</bodyText>
<page confidence="0.975509">
328
</page>
<note confidence="0.7507965">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328–334,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<author confidence="0.246311">
Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2
</author>
<equation confidence="0.760636">
She She PRP (S(NP*) - She - - - -
would would MD (VP* - would - - - -
not not RB * not - - - - -
have have VB (VP* - have - - - -
said say VBD (VP* - said - - - -
‘ ‘ “ (SBAR(S(NP* - ’ - - - -
Godspeed Godspeed NNP * - Godspeed - - - -
’ ’ ” *) - ’ - - - -
had have VBD (VP* - had - - had -
it it PRP (ADVP* - it - - it -
not not RB *) - not - not - -
been be VBN (VP* - been - - been -
so so RB (ADVP*)))))))) - so - - so -
. . . *) - - - - - -
</equation>
<tableCaption confidence="0.997482">
Table 1: Example sentence annotated for negation following sem shared task 2012 format
</tableCaption>
<bodyText confidence="0.999678258064516">
2008a; Morante and Daelemans, 2009; Agarwal and
Yu, 2010; Morante, 2010; Read et al., 2011), mostly
on clinical reports. The reason is that most NLP re-
search in the biomedical domain is interested in au-
tomatically extracting factual relations and pieces of
information from unstructured data. Negation detec-
tion is important here because information that falls
in the scope of a negation cue cannot be treated as
facts.
Chapman et al. (2001) proposed a rule-based al-
gorithm called NegEx for determining whether a
finding or disease mentioned within narrative med-
ical reports is present or absent. The algorithm
uses regular-expression-based rules. Mutalik et
al. (2001) developed another rule based system
called Negfinder that recognizes negation patterns
in biomedical text. It consists of two components:
a lexical scanner, lexer that uses regular expres-
sion rules to generate a finite state machine, and a
parser. Morante (2008b) proposed a supervised ap-
proach for detecting negation cues and their scopes
in biomedical text. Their system consists of two
memory-based engines, one that decides if the to-
kens in a sentence are negation signals, and another
one that finds the full scope of these negation sig-
nals.
Negation has been also studied in the context of
sentiment analysis (Wilson et al., 2005; Jia et al.,
2009; Councill et al., 2010; Heerschop et al., 2011;
Hogenboom et al., 2011). Wiegand et al. (2010) sur-
veyed the recent work on negation scope detection
for sentiment analysis. Wilson et al. (2005) studied
the contextual features that affect text polarity. They
used a machine learning approach in which nega-
tion is encoded using several features. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar ex-
pression. Another feature accounts for a polar pred-
icate having a negated subject. They also have dis-
ambiguation features to handle negation words that
do not function as negation cues in certain contexts,
e.g. not to mention and not just.
Jia et al. (2009) proposed a rule based method to
determine the polarity of sentiments when one or
more occurrences of a negation term such as not ap-
pear in a sentence. The hand-crafted rules are ap-
plied to syntactic and dependency parse tree repre-
sentations of the sentence.
Hogenboom et al. (2011) found that applying a
simple rule that considers two words, following a
negation keyword, to be negated by that keyword,
to be effective in improving the accuracy of senti-
ment analysis in movie reviews. This simple method
yields a significant increase in overall sentiment
classification accuracy and macro-level F1 of 5.5%
and 6.2%, respectively, compared to not accounting
for negation.
This work is characterized by addressing three
tasks at once: negation cue detection, negated
event identification, and negation scope resolution.
Our proposed approach uses a supervised graphical
probabilistic model trained using labeled data.
</bodyText>
<page confidence="0.998304">
329
</page>
<sectionHeader confidence="0.996088" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.997178157894737">
We use the dataset distributed by the organizers of
the *sem Shared Task 2012 on resolving the scope
and focus of negation. This dataset includes two sto-
ries by Conan Doyle, The Hound of the Baskervilles,
The Adventures of Wisteria Lodge. All occur-
rences of negation are annotated accounting for
negation expressed by nouns, pronouns, verbs, ad-
verbs, determiners, conjunctions and prepositions.
For each negation cue, the negation cue and scope
are marked, as well as the negated event (if any ex-
ists). The annotation guidelines follow the proposal
of Morante et al. (2011)1. The data is split into three
sets: a training set containing 3,644 sentences, a de-
velopment set containing 787 sentences, and a test-
ing set containing 1,089 sentences. The data is pro-
vided in CoNLL format. Each line corresponds to a
token and each annotation is provided in a column;
empty lines indicate end of sentences. The provided
annotations are:
</bodyText>
<listItem confidence="0.951819909090909">
• Column 1: chapter name
• Column 2: sentence number within chapter
• Column 3: token number within sentence
• Column 4: word
• Column 5: lemma
• Column 6: part-of-speech
• Column 7: syntax
• Columns 8 to last:
– If the sentence has no negations, column
8 has a ”***” value and there are no more
columns.
</listItem>
<bodyText confidence="0.979377333333333">
– If the sentence has negations, the annota-
tion for each negation is provided in three
columns. The first column contains the
word or part of the word (e.g., morpheme
”un”), that belongs to the negation cue.
The second contains the word or part of
the word that belongs to the scope of the
negation cue. The third column contains
the word or part of the word that is the
</bodyText>
<footnote confidence="0.859888">
1http://www.clips.ua.ac.be/sites/default/files/ctrs-n3.pdf
</footnote>
<bodyText confidence="0.947718857142857">
Token Lemma Punc. Cat. POS Label
Since Since 0 OTH IN O
we we 0 PRO PRP O
have have 0 VB VBP O
been be 0 VB VBN O
so so 0 ADVB RB O
unfortunate unfortunate 0 ADJ JJ PRE
as as 0 ADVB RB O
to to 0 OTH TO O
miss miss 0 VB VB O
him him 0 PRO PRP O
and and 0 OTH CC O
have have 0 VB VBP O
no no 0 OTH DT NEG
notion notion 0 NOUN NN O
of of 0 OTH IN O
his his 0 PRO PRP$ O
errand errand 0 NOUN NN O
, , 1 OTH , O
this this 0 OTH DT O
accidental accidental 0 ADJ JJ O
</bodyText>
<table confidence="0.7975144">
souvenir souvenir 0 NOUN NN O
becomes become 0 VB VBZ O
of of 0 OTH IN O
importance importance 0 NOUN NN O
. . 1 OTH . O
</table>
<tableCaption confidence="0.9743095">
Table 2: Example sentence labeled for negation cue de-
tection
</tableCaption>
<bodyText confidence="0.99981">
negated event or property. It can be the
case that no negated event or property are
marked as negated.
Table 1 shows an example of an annotated sen-
tence that contains two negation cues.
</bodyText>
<sectionHeader confidence="0.998506" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999968909090909">
The problem that we are trying to solve can be split
into three tasks. The first task is to detect negation
cues. The second task is to identify the scope of each
detected negation cue. The third task is to identify
the negated event. We use a machine learning ap-
proach to address these tasks. We train a Condi-
tional Random Field (CRF) (Lafferty et al., 2001)
model on lexical, structural, and syntactic features
extracted from the training dataset. In the following
subsections, we describe the CRF model that we use
for each task.
</bodyText>
<subsectionHeader confidence="0.977955">
4.1 Negation Cue Detection
</subsectionHeader>
<bodyText confidence="0.996735">
Negation cues are lexical elements that indicate the
existence of negation in a sentence. From lexical
</bodyText>
<page confidence="0.917597">
330
</page>
<bodyText confidence="0.778148">
point of view, negation cues can be divided into four • Ends with negation postfix: This feature takes
categories: the value 1 if the token is a word that ends with
-less and 0 otherwise.
</bodyText>
<listItem confidence="0.9841485">
1. Prefix (i.e. in-, un-, im-, il-, dis-). For example,
un- in unsuitable) is a prefix negation cue.
2. Postfix (i.e. -less). for example, -less in
careless.
3. Multi-word negation cues such as neither...nor,
rather than, by no means, etc.
4. Single word negation cues such as not, no,
none, nobody, etc.
</listItem>
<bodyText confidence="0.999939428571429">
The goal of this task is to detect negation cues.
We pose this problem as a sequence labeling task.
The reason for this choice is that some negation cues
may not indicate negation in some contexts. For
example, the negation cue not in the phrase not to
mention does not indicate negation. Also, as we saw
above, some negation cues may consist of multiple
words, some of them are continuous and others are
discontinuous. Treating the task as a sequence label-
ing problem help model the contextual factors that
affect the function of negation cues. We train a CRF
model using features extracted from the sentences of
the training dataset. The token level features that we
train the model on are:
</bodyText>
<listItem confidence="0.9738422">
• Token: The word or the punctuation mark as it
appears in the sentence.
• Lemma: The lemmatized form of the token.
• Part-Of-Speech tag: The part of speech tag of
the token.
• Part-Of-Speech tag category: Part-of-speech
tags reduced into 5 categories: Adjec-
tive (ADJ), Verb (VB), Noun (NN), Adverb
(ADVB), Pronoun (PRO), and other (OTH).
• Is punctuation mark: This feature takes the
value 1 if the token is a punctuation mark and 0
otherwise.
• Starts with negation prefix: This feature takes
the value 1 if the token is a word that starts with
un-, in-, im-, il-, or dis- and 0 otherwise.
</listItem>
<bodyText confidence="0.999879176470588">
The CRF model that we use considers at each to-
ken the features of the current token, the two pre-
ceding tokens, and the two proceeding tokens. The
model also uses token bigrams and trigrams, and
part-of-speech tag bigrams and trigrams as features.
The labels are 5 types: ”O” for tokens that are
not part of any negation cue; ”NEG” for single
word negation cues; ”PRE” for prefix negation cue;
”POST” for postfix negation cue; and ”MULTI-
NEG” for multi-word negation cues. Table 2 shows
an example labeled sentence.
At testing time, if a token is labeled ”NEG” or
”MULTI-NEG” the whole token is treated as a nega-
tion cue or part of a negation cue respectively. If a
token is labeled as ”PRE” or ”POST”, a regular ex-
pression is used to determine the prefix/postfix that
trigged the negation.
</bodyText>
<subsectionHeader confidence="0.994517">
4.2 Negation Scope Detection
</subsectionHeader>
<bodyText confidence="0.999934333333333">
Scope of negation is the sequence of tokens (can
be discontinuous) that expresses the meaning that
is meant to be negated by a negation cue. A sen-
tence may contain zero or more negation cues. Each
negation cue has its own scope. It is possible that
the scope of two negation cues overlap. We use
each negation instance (i.e. each negation cue and
its scope) as one training example. Therefore, a
sentence that contains two negation cues provides
two training examples. We train a CRF model on
features extracted from all negation instances in the
training dataset. The features that we use are:
</bodyText>
<listItem confidence="0.906157777777778">
• Token: The word or the punctuation mark as it
appears in the sentence.
• Lemma: The lemmatized form of the token.
• Part-Of-Speech tag: The part of speech tag of
the token.
• Part-Of-Speech tag category: Part-of-speech
tags reduced into 5 categories: Adjec-
tive (ADJ), Verb (VB), Noun (NN), Adverb
(ADVB), Pronoun (PRO), and other (OTH).
</listItem>
<page confidence="0.950933">
331
</page>
<listItem confidence="0.990589621621622">
• Is punctuation mark: This feature takes the
value 1 if the token is a punctuation mark and 0
otherwise.
• Type of negation cue: Possible types are:
”NEG” for single word negation cues; ”PRE”
for prefix negation cue; ”POST” for postfix
negation cue; and ”MULTI” for multi-word
negation cues.
• Relative position: This feature takes the value
1 if the token position in the sentence is be-
fore the position of the negation cue, 2 if the
token position is after the position of the nega-
tion cue, and 3 if the token is the negation cue
itself.
• Distance: The number of tokens between the
current token and the negation cue.
• Same segment: This feature takes the value 1
if this token and the negation cue fall in the
segment in the sentence. The sentence is seg-
mented by punctuation marks.
• Chunk: This feature takes the value NP-B (VP-
B) if this token is the first token of a noun (verb)
phrase, NP-I (VP-I) if it is inside a noun (verb)
phrase, NP-E (VP-E) if it is the last token of a
noun (verb) phrase.
• Same chunk: This feature takes the value 1 if
this token and the negation cue fall in the same
chunk (noun phrase or verb phrase).
• Is negation: This feature takes the value 1 if
this token is a negation cue, and 0 otherwise.
• Syntactic distance: The number of edges in the
shortest path that connects the token and the
negation in the syntactic parse tree.
• Common ancestor node: The type of the node
in the syntactic parse tree that is the least com-
mon ancestor of this token and the negation cue
token.
</listItem>
<bodyText confidence="0.9988775">
The CRF model considers the features of 4 tokens
to the left and to the right at each position. It also
uses bigram and trigram combinations of some of
the features.
At testing time a few postprocessing rules are
used to fix sure labels if they were labeled incor-
rectly. For example, if a word starts with a prefix
negation cue, the word itself (without the prefix) is
always part of the scope and it is also the negated
event.
</bodyText>
<subsectionHeader confidence="0.997741">
4.3 Negated Event Identification
</subsectionHeader>
<bodyText confidence="0.999977636363636">
It is possible that a negation cue comes associated
with an event. A negation has an event if it oc-
curs in a factual context. The dataset that we use
was labeled for negated events whenever one exists.
We used the same features described in the previous
subsection to train a CRF model for negated event
identification. We have also tried to use one CRF
model for both scope resolution and negated event
identification, but we noticed that using two sepa-
rate models results in significantly better results for
both tasks.
</bodyText>
<sectionHeader confidence="0.994925" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999864666666667">
We use the testing set described in Section 3 to eval-
uate the system. The testing set contains 1089 sen-
tences 235 of which contains at least one negation.
We use the standard precision, recall, and f-
measure metrics to evaluate the system. We perform
the evaluation on different levels:
</bodyText>
<listItem confidence="0.9972050625">
1. Cues: the metrics are computed only for cue
detection.
2. Scope (tokens): the metrics are calculated at to-
ken level. If a sentence has 2 scopes, one with
5 tokens and another with 4, the total number
of scope tokens is 9.
3. Scope (full): the metrics are calculated at the
full scope level. Both the negation cue and
the whole scope should be correctly identified.
If a sentence contains 2 negation cues, then 2
scopes are checked. We report two values here
one the requires the cue match correctly and
one that does not.
4. Negated Events: the metrics are computed only
for negated events identification (apart from
negation cue and scope).
</listItem>
<page confidence="0.989094">
332
</page>
<table confidence="0.9998839">
Variant A
gold system tp fp fn precision recall F1
Cues 264 250 232 14 32 94.31 87.88 90.98
Scope (cue match) 249 227 126 14 123 90.00 50.60 64.78
Scope (no cue match) 249 227 126 14 123 90.00 50.60 64.78
Scope (tokens - no cue match) 1805 1716 1456 260 349 84.85 80.66 82.70
Negated (no cue match) 173 183 70 70 64 50.00 52.24 51.10
Full negation: 264 250 75 14 189 84.27 28.41 42.49
Variant B
gold system tp fp fn precision recall F1
Cues : 264 250 232 14 32 92.80 87.88 90.27
Scope (cue match): 249 227 126 14 123 55.51 50.60 52.94
Scope (no cue match): 249 227 126 14 123 55.51 50.60 52.94
Negated (no cue match): 173 183 70 70 64 38.25 52.24 44.16
Full negation: 264 250 75 14 189 30.00 28.41 29.18
# Sentences 1089
# Negation sentences 235
# Negation sentences with errors 171
% Correct sentences 83.47
% Correct negation sentences 27.23
</table>
<tableCaption confidence="0.999896">
Table 3: Results of negation cue, negated event, and negation scope detection
</tableCaption>
<bodyText confidence="0.954412368421053">
5. Full negation: the metrics are computed for all
the three tasks at once and requiring everything
to match correctly.
For cue, scope and negated event to be correct,
both the tokens and the words or parts of words have
to be correctly identified. The final periods in abbre-
viations are disregarded. If gold has value ”Mr.” and
system ”Mr”, system is counted as correct. Also,
punctuation tokens are *not* taken into account for
evaluation.
Two variants of the metrics are computed. In the
first variant (A), precision is calculated as tp / (tp +
fp) and recall is calculated as tp / (tp + fn) where tp
is the count of true positive labels, fp is the count
of false positive labels, and fn is the count of false
negative labels. In variant B, the precision is calcu-
lated differently, using the formula precision = tp /
system.
Table 3 shows the results of our system.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.999973772727273">
The system used no external resources outside the
training data. This means that the system recognizes
only negation cues that appeared in the training set.
This was the first source of error. For example, the
word unacquainted that starts with the negation pre-
fix un has never been seen in the training data. In-
tuitively, if no negation cue is detected, the system
does not attempt to produce scope levels. This prob-
lem can be overcome by using a lexicon of negation
words and those words that can be negated by adding
a negation prefix to them.
We noticed in several occasions that scope detec-
tion accuracy can be improved if some simple rules
can be imposed after doing the initial labeling us-
ing the CRF model (but we have not actually imple-
mented any such rules in the system). For example,
the system can require all the tokens that belong to
the same chunk (noun group, verb group, etc.) all
have the same label (e.g. the majority vote label).
The same thing could be also applied on the segment
rather than the chunk level where the boundaries of
segments are determined by punctuation marks.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999896777777778">
We presented a supervised system for identifying
negation in English sentences. The system uses
three CRF trained models. One model is trained for
negation cue detection. Another model is trained
for negated event identification. A third one is
trained for negation scope identification. The mod-
els are trained using features extracted from a la-
beled dataset. Our experiments show that the system
achieves promising results.
</bodyText>
<page confidence="0.998912">
333
</page>
<sectionHeader confidence="0.989929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999645757894737">
Shashank Agarwal and Hong Yu. 2010. Biomedi-
cal negation scope detection with conditional random
fields. Journal of the American Medical Informatics
Association, 17(6):696–701.
Wendy Webber Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of Biomedi-
cal Informatics, pages 301–310.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s great and what’s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ’10, pages 51–59, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Bas Heerschop, Paul van Iterson, Alexander Hogenboom,
Flavius Frasincar, and Uzay Kaymak. 2011. Analyz-
ing sentiment in a large set of web data while account-
ing for negation. In AWIC, pages 195–205.
Alexander Hogenboom, Paul van Iterson, Bas Heerschop,
Flavius Frasincar, and Uzay Kaymak. 2011. Deter-
mining negation scope and strength in sentiment anal-
ysis. In SMC, pages 2589–2594.
Laurence R. Horn. 1989. A natural history of nega-
tion /Laurence R. Horn. University of Chicago Press,
Chicago :.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The
effect of negation on sentiment analysis and retrieval
effectiveness. In Proceedings of the 18th ACM con-
ference on Information and knowledge management,
CIKM ’09, pages 1827–1830, New York, NY, USA.
ACM.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. 5(1):44–60, March.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Roser Morante and Walter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. Pro-
ceedings of the Workshop on BioNLP BioNLP 09,
(June):28.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008a. Learning the scope of negation in
biomedical texts. Proceedings of the Conference on
Empirical Methods in Natural Language Processing
EMNLP 08, (October):715–724.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008b. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715–724, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope. Technical report.
Roser Morante. 2010. Descriptive analysis of negation
cues in biomedical texts. Language Resources And
Evaluation, pages 1–8.
P. G. Mutalik, A. Deshpande, and P. M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents: a quantitative
study using the UMLS. Journal of the American Med-
ical Informatics Association: JAMIA, 8(6):598–609.
Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja
vrelid. 2011. Resolving speculation and negation
scope in biomedical articles with a syntactic con-
stituent ranker. In Proceedings of the Fourth Inter-
national Symposium on Languages in Biology and
Medicine, Singapore.
Gunnel Tottie. 1993. Negation in English Speech and
Writing: A Study in Variation. Language, 69(3):590–
593.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, NeSp-NLP
’10, pages 60–68, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ’05,
pages 347–354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.999059">
334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031009">
<title confidence="0.997502">UMichigan: A Conditional Random Field Model for Resolving the Scope of Negation</title>
<author confidence="0.678198">Amjad</author>
<affiliation confidence="0.948481">EECS University of</affiliation>
<author confidence="0.547011">Ann Arbor</author>
<author confidence="0.547011">MI</author>
<email confidence="0.998733">amjbara@umich.edu</email>
<author confidence="0.533863">Dragomir</author>
<affiliation confidence="0.9825275">EECS University of</affiliation>
<author confidence="0.445121">Ann Arbor</author>
<author confidence="0.445121">MI</author>
<email confidence="0.999616">radev@umich.edu</email>
<abstract confidence="0.938131421052632">In this paper, we present a system for detecting negation in English text. We address three tasks: negation cue detection, negation scope resolution and negated event identification. We pose these tasks as sequence labeling problems. For each task, we train a Conditional Random Field (CRF) model on lexical, structural, and syntactic features extracted from labeled data. The models are trained and tested using the dataset distributed with the *sem Shared Task 2012 on resolving the scope and focus of negation. The system detects negation cues with 90.98% F1 measure (94.3% and 87.88% recall). It identifies negation scope with 82.70% F1 on token-bytoken level and 64.78% F1 on full scope level. Negated events are detected with 51.10% F1 measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shashank Agarwal</author>
<author>Hong Yu</author>
</authors>
<title>Biomedical negation scope detection with conditional random fields.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>6</issue>
<contexts>
<context position="3945" citStr="Agarwal and Yu, 2010" startWordPosition="706" endWordPosition="709"> Event 1 Cue 2 Scope 2 Event 2 She She PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP (ADVP* - it - - it - not not RB *) - not - not - - been be VBN (VP* - been - - been - so so RB (ADVP*)))))))) - so - - so - . . . *) - - - - - - Table 1: Example sentence annotated for negation following sem shared task 2012 format 2008a; Morante and Daelemans, 2009; Agarwal and Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression-based rules. Mutalik </context>
</contexts>
<marker>Agarwal, Yu, 2010</marker>
<rawString>Shashank Agarwal and Hong Yu. 2010. Biomedical negation scope detection with conditional random fields. Journal of the American Medical Informatics Association, 17(6):696–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Webber Chapman</author>
<author>Will Bridewell</author>
<author>Paul Hanbury</author>
<author>Gregory F Cooper</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>301--310</pages>
<contexts>
<context position="3056" citStr="Chapman et al., 2001" startWordPosition="495" endWordPosition="498">y detecting negation cues, negated events, and negation scopes in English text. The system uses conditional random field (CRF) models trained on labeled sentences extracted from two classical English novels. The CRF models are trained using lexical, structural, and syntactic features. The experiments show promising results. This paper is organized as follows. Section 2 reviews previous work. Section 3 describes the data. Section 4 describes the CRFs models. Section 5 presents evaluation, results, and discussion. 2 Previous Work Most research on negation has been done in the biomedical domain (Chapman et al., 2001; Mutalik et al., 2001; Kim and Park, 2006; Morante et al., 328 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328–334, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2 She She PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP</context>
<context position="4330" citStr="Chapman et al. (2001)" startWordPosition="769" endWordPosition="772">een be VBN (VP* - been - - been - so so RB (ADVP*)))))))) - so - - so - . . . *) - - - - - - Table 1: Example sentence annotated for negation following sem shared task 2012 format 2008a; Morante and Daelemans, 2009; Agarwal and Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression-based rules. Mutalik et al. (2001) developed another rule based system called Negfinder that recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their syste</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Wendy Webber Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, pages 301–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10,</booktitle>
<pages>51--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5235" citStr="Councill et al., 2010" startWordPosition="913" endWordPosition="916">at recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to handle negation words that do not function a</context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Isaac G. Councill, Ryan McDonald, and Leonid Velikovich. 2010. What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10, pages 51–59, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bas Heerschop</author>
<author>Paul van Iterson</author>
<author>Alexander Hogenboom</author>
<author>Flavius Frasincar</author>
<author>Uzay Kaymak</author>
</authors>
<title>Analyzing sentiment in a large set of web data while accounting for negation. In AWIC,</title>
<date>2011</date>
<pages>195--205</pages>
<marker>Heerschop, van Iterson, Hogenboom, Frasincar, Kaymak, 2011</marker>
<rawString>Bas Heerschop, Paul van Iterson, Alexander Hogenboom, Flavius Frasincar, and Uzay Kaymak. 2011. Analyzing sentiment in a large set of web data while accounting for negation. In AWIC, pages 195–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Hogenboom</author>
<author>Paul van Iterson</author>
</authors>
<title>Bas Heerschop, Flavius Frasincar, and Uzay Kaymak.</title>
<date>2011</date>
<booktitle>In SMC,</booktitle>
<pages>2589--2594</pages>
<marker>Hogenboom, van Iterson, 2011</marker>
<rawString>Alexander Hogenboom, Paul van Iterson, Bas Heerschop, Flavius Frasincar, and Uzay Kaymak. 2011. Determining negation scope and strength in sentiment analysis. In SMC, pages 2589–2594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence R Horn</author>
</authors>
<title>A natural history of negation</title>
<date>1989</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago :.</location>
<contexts>
<context position="1123" citStr="Horn, 1989" startWordPosition="174" endWordPosition="175"> task, we train a Conditional Random Field (CRF) model on lexical, structural, and syntactic features extracted from labeled data. The models are trained and tested using the dataset distributed with the *sem Shared Task 2012 on resolving the scope and focus of negation. The system detects negation cues with 90.98% F1 measure (94.3% and 87.88% recall). It identifies negation scope with 82.70% F1 on token-bytoken level and 64.78% F1 on full scope level. Negated events are detected with 51.10% F1 measure. 1 Introduction Negation is a linguistic phenomenon present in all languages (Tottie, 1993; Horn, 1989). The semantic function of negation is to transform an affirmative statement into its opposite meaning. The automatic detection of negation and its scope is a problem encountered in a wide range of natural language processing applications including, but not limited to, data mining, relation extraction, question answering, and sentiment analysis. For example, failing to account for negation may result in giving wrong answers in question answering systems or in the prediction of opposite sentiment in sentiment analysis systems. The occurrence of negation in a sentence is determined by the presen</context>
</contexts>
<marker>Horn, 1989</marker>
<rawString>Laurence R. Horn. 1989. A natural history of negation /Laurence R. Horn. University of Chicago Press, Chicago :.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Huddleston</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="1918" citStr="Huddleston and Pullum, 2002" startWordPosition="307" endWordPosition="311">encountered in a wide range of natural language processing applications including, but not limited to, data mining, relation extraction, question answering, and sentiment analysis. For example, failing to account for negation may result in giving wrong answers in question answering systems or in the prediction of opposite sentiment in sentiment analysis systems. The occurrence of negation in a sentence is determined by the presence of a negation cue. A negation cue is a word, a phrase, a prefix, or a postfix that triggers negation. Scope of negation is the part of the meaning that is negated (Huddleston and Pullum, 2002). The negated event is the event or the entity that the negation indicates its absence or denies its occurrence. For example, in the sentence below never is the negation cue. The scope is enclosed in square brackets. The negated event is underlined. [Andrew had] never [liked smart phones], but he received one as a gift last week and started to use it. Negation cues and scopes may be discontinuous. For example, the negation cue neither ... nor is discontinuous. In this chapter, we present a system for automatically detecting negation cues, negated events, and negation scopes in English text. Th</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney D. Huddleston and Geoffrey K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lifeng Jia</author>
<author>Clement Yu</author>
<author>Weiyi Meng</author>
</authors>
<title>The effect of negation on sentiment analysis and retrieval effectiveness.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09,</booktitle>
<pages>1827--1830</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5212" citStr="Jia et al., 2009" startWordPosition="909" endWordPosition="912">alled Negfinder that recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to handle negation words</context>
</contexts>
<marker>Jia, Yu, Meng, 2009</marker>
<rawString>Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The effect of negation on sentiment analysis and retrieval effectiveness. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 1827–1830, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Jae Kim</author>
<author>Jong C Park</author>
</authors>
<title>Extracting contrastive information from negation patterns in biomedical literature.</title>
<date>2006</date>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="3098" citStr="Kim and Park, 2006" startWordPosition="503" endWordPosition="506">nd negation scopes in English text. The system uses conditional random field (CRF) models trained on labeled sentences extracted from two classical English novels. The CRF models are trained using lexical, structural, and syntactic features. The experiments show promising results. This paper is organized as follows. Section 2 reviews previous work. Section 3 describes the data. Section 4 describes the CRFs models. Section 5 presents evaluation, results, and discussion. 2 Previous Work Most research on negation has been done in the biomedical domain (Chapman et al., 2001; Mutalik et al., 2001; Kim and Park, 2006; Morante et al., 328 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328–334, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2 She She PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP (ADVP* - it - - it - not not RB *) - not </context>
</contexts>
<marker>Kim, Park, 2006</marker>
<rawString>Jung-Jae Kim and Jong C. Park. 2006. Extracting contrastive information from negation patterns in biomedical literature. 5(1):44–60, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9714" citStr="Lafferty et al., 2001" startWordPosition="1725" endWordPosition="1728"> . O Table 2: Example sentence labeled for negation cue detection negated event or property. It can be the case that no negated event or property are marked as negated. Table 1 shows an example of an annotated sentence that contains two negation cues. 4 Approach The problem that we are trying to solve can be split into three tasks. The first task is to detect negation cues. The second task is to identify the scope of each detected negation cue. The third task is to identify the negated event. We use a machine learning approach to address these tasks. We train a Conditional Random Field (CRF) (Lafferty et al., 2001) model on lexical, structural, and syntactic features extracted from the training dataset. In the following subsections, we describe the CRF model that we use for each task. 4.1 Negation Cue Detection Negation cues are lexical elements that indicate the existence of negation in a sentence. From lexical 330 point of view, negation cues can be divided into four • Ends with negation postfix: This feature takes categories: the value 1 if the token is a word that ends with -less and 0 otherwise. 1. Prefix (i.e. in-, un-, im-, il-, dis-). For example, un- in unsuitable) is a prefix negation cue. 2. </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>Proceedings of the Workshop on BioNLP BioNLP 09,</booktitle>
<location>(June):28.</location>
<contexts>
<context position="3923" citStr="Morante and Daelemans, 2009" startWordPosition="702" endWordPosition="705">emma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2 She She PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP (ADVP* - it - - it - not not RB *) - not - not - - been be VBN (VP* - been - - been - so so RB (ADVP*)))))))) - so - - so - . . . *) - - - - - - Table 1: Example sentence annotated for negation following sem shared task 2012 format 2008a; Morante and Daelemans, 2009; Agarwal and Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. Proceedings of the Workshop on BioNLP BioNLP 09, (June):28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Anthony Liekens</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of negation in biomedical texts.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP 08,</booktitle>
<location>(October):715–724.</location>
<marker>Morante, Liekens, Daelemans, 2008</marker>
<rawString>Roser Morante, Anthony Liekens, and Walter Daelemans. 2008a. Learning the scope of negation in biomedical texts. Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP 08, (October):715–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Anthony Liekens</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of negation in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>715--724</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>Morante, Liekens, Daelemans, 2008</marker>
<rawString>Roser Morante, Anthony Liekens, and Walter Daelemans. 2008b. Learning the scope of negation in biomedical texts. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 715–724, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Sarah Schrauwen</author>
<author>Walter Daelemans</author>
</authors>
<title>Annotation of negation cues and their scope.</title>
<date>2011</date>
<tech>Technical report.</tech>
<contexts>
<context position="7424" citStr="Morante et al. (2011)" startWordPosition="1266" endWordPosition="1269">ic model trained using labeled data. 329 3 Data We use the dataset distributed by the organizers of the *sem Shared Task 2012 on resolving the scope and focus of negation. This dataset includes two stories by Conan Doyle, The Hound of the Baskervilles, The Adventures of Wisteria Lodge. All occurrences of negation are annotated accounting for negation expressed by nouns, pronouns, verbs, adverbs, determiners, conjunctions and prepositions. For each negation cue, the negation cue and scope are marked, as well as the negated event (if any exists). The annotation guidelines follow the proposal of Morante et al. (2011)1. The data is split into three sets: a training set containing 3,644 sentences, a development set containing 787 sentences, and a testing set containing 1,089 sentences. The data is provided in CoNLL format. Each line corresponds to a token and each annotation is provided in a column; empty lines indicate end of sentences. The provided annotations are: • Column 1: chapter name • Column 2: sentence number within chapter • Column 3: token number within sentence • Column 4: word • Column 5: lemma • Column 6: part-of-speech • Column 7: syntax • Columns 8 to last: – If the sentence has no negation</context>
</contexts>
<marker>Morante, Schrauwen, Daelemans, 2011</marker>
<rawString>Roser Morante, Sarah Schrauwen, and Walter Daelemans. 2011. Annotation of negation cues and their scope. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
</authors>
<title>Descriptive analysis of negation cues in biomedical texts. Language Resources And Evaluation,</title>
<date>2010</date>
<pages>1--8</pages>
<contexts>
<context position="3960" citStr="Morante, 2010" startWordPosition="710" endWordPosition="711"> Event 2 She She PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP (ADVP* - it - - it - not not RB *) - not - not - - been be VBN (VP* - been - - been - so so RB (ADVP*)))))))) - so - - so - . . . *) - - - - - - Table 1: Example sentence annotated for negation following sem shared task 2012 format 2008a; Morante and Daelemans, 2009; Agarwal and Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression-based rules. Mutalik et al. (2001) d</context>
</contexts>
<marker>Morante, 2010</marker>
<rawString>Roser Morante. 2010. Descriptive analysis of negation cues in biomedical texts. Language Resources And Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P G Mutalik</author>
<author>A Deshpande</author>
<author>P M Nadkarni</author>
</authors>
<title>Use of general-purpose negation detection to augment concept indexing of medical documents: a quantitative study using the UMLS.</title>
<date>2001</date>
<journal>Journal of the American Medical Informatics Association: JAMIA,</journal>
<volume>8</volume>
<issue>6</issue>
<contexts>
<context position="3078" citStr="Mutalik et al., 2001" startWordPosition="499" endWordPosition="502">ues, negated events, and negation scopes in English text. The system uses conditional random field (CRF) models trained on labeled sentences extracted from two classical English novels. The CRF models are trained using lexical, structural, and syntactic features. The experiments show promising results. This paper is organized as follows. Section 2 reviews previous work. Section 3 describes the data. Section 4 describes the CRFs models. Section 5 presents evaluation, results, and discussion. 2 Previous Work Most research on negation has been done in the biomedical domain (Chapman et al., 2001; Mutalik et al., 2001; Kim and Park, 2006; Morante et al., 328 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328–334, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2 She She PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP (ADVP* - it - - it - </context>
<context position="4558" citStr="Mutalik et al. (2001)" startWordPosition="802" endWordPosition="805">Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression-based rules. Mutalik et al. (2001) developed another rule based system called Negfinder that recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sen</context>
</contexts>
<marker>Mutalik, Deshpande, Nadkarni, 2001</marker>
<rawString>P. G. Mutalik, A. Deshpande, and P. M. Nadkarni. 2001. Use of general-purpose negation detection to augment concept indexing of medical documents: a quantitative study using the UMLS. Journal of the American Medical Informatics Association: JAMIA, 8(6):598–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
<author>Lilja vrelid</author>
</authors>
<title>Resolving speculation and negation scope in biomedical articles with a syntactic constituent ranker.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourth International Symposium on Languages in Biology and Medicine,</booktitle>
<contexts>
<context position="3980" citStr="Read et al., 2011" startWordPosition="712" endWordPosition="715">e PRP (S(NP*) - She - - - - would would MD (VP* - would - - - - not not RB * not - - - - - have have VB (VP* - have - - - - said say VBD (VP* - said - - - - ‘ ‘ “ (SBAR(S(NP* - ’ - - - - Godspeed Godspeed NNP * - Godspeed - - - - ’ ’ ” *) - ’ - - - - had have VBD (VP* - had - - had - it it PRP (ADVP* - it - - it - not not RB *) - not - not - - been be VBN (VP* - been - - been - so so RB (ADVP*)))))))) - so - - so - . . . *) - - - - - - Table 1: Example sentence annotated for negation following sem shared task 2012 format 2008a; Morante and Daelemans, 2009; Agarwal and Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression-based rules. Mutalik et al. (2001) developed another rul</context>
</contexts>
<marker>Read, Velldal, Oepen, vrelid, 2011</marker>
<rawString>Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja vrelid. 2011. Resolving speculation and negation scope in biomedical articles with a syntactic constituent ranker. In Proceedings of the Fourth International Symposium on Languages in Biology and Medicine, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunnel Tottie</author>
</authors>
<title>Negation in English Speech and Writing: A Study in Variation.</title>
<date>1993</date>
<journal>Language,</journal>
<volume>69</volume>
<issue>3</issue>
<pages>593</pages>
<contexts>
<context position="1110" citStr="Tottie, 1993" startWordPosition="172" endWordPosition="173">lems. For each task, we train a Conditional Random Field (CRF) model on lexical, structural, and syntactic features extracted from labeled data. The models are trained and tested using the dataset distributed with the *sem Shared Task 2012 on resolving the scope and focus of negation. The system detects negation cues with 90.98% F1 measure (94.3% and 87.88% recall). It identifies negation scope with 82.70% F1 on token-bytoken level and 64.78% F1 on full scope level. Negated events are detected with 51.10% F1 measure. 1 Introduction Negation is a linguistic phenomenon present in all languages (Tottie, 1993; Horn, 1989). The semantic function of negation is to transform an affirmative statement into its opposite meaning. The automatic detection of negation and its scope is a problem encountered in a wide range of natural language processing applications including, but not limited to, data mining, relation extraction, question answering, and sentiment analysis. For example, failing to account for negation may result in giving wrong answers in question answering systems or in the prediction of opposite sentiment in sentiment analysis systems. The occurrence of negation in a sentence is determined </context>
</contexts>
<marker>Tottie, 1993</marker>
<rawString>Gunnel Tottie. 1993. Negation in English Speech and Writing: A Study in Variation. Language, 69(3):590– 593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Alexandra Balahur</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
<author>Andr´es Montoyo</author>
</authors>
<title>A survey on the role of negation in sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5307" citStr="Wiegand et al. (2010)" startWordPosition="925" endWordPosition="928">omponents: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to handle negation words that do not function as negation cues in certain contexts, e.g. not to mention and not just. J</context>
</contexts>
<marker>Wiegand, Balahur, Roth, Klakow, Montoyo, 2010</marker>
<rawString>Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow, and Andr´es Montoyo. 2010. A survey on the role of negation in sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10, pages 60–68, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5194" citStr="Wilson et al., 2005" startWordPosition="905" endWordPosition="908">r rule based system called Negfinder that recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to han</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>