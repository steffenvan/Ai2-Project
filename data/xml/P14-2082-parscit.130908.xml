<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000282">
<title confidence="0.997348">
An Annotation Framework for Dense Event Ordering
</title>
<author confidence="0.963804">
Taylor Cassidy
</author>
<affiliation confidence="0.867469">
IBM Research
</affiliation>
<email confidence="0.983837">
taylor.cassidy.ctr@mail.mil
</email>
<author confidence="0.911176">
Nathanael Chambers
</author>
<affiliation confidence="0.620653">
US Naval Academy
</affiliation>
<email confidence="0.996814">
nchamber@usna.edu
</email>
<author confidence="0.994329">
Bill McDowell
</author>
<affiliation confidence="0.991919">
Carnegie Mellon University
</affiliation>
<email confidence="0.969497">
forkunited@gmail.com
</email>
<author confidence="0.998989">
Steven Bethard
</author>
<affiliation confidence="0.995896">
Univ. of Alabama at Birmingham
</affiliation>
<email confidence="0.998315">
bethard@cis.uab.edu
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968260869565">
Today’s event ordering research is heav-
ily dependent on annotated corpora. Cur-
rent corpora influence shared evaluations
and drive algorithm development. Partly
due to this dependence, most research fo-
cuses on partial orderings of a document’s
events. For instance, the TempEval com-
petitions and the TimeBank only annotate
small portions of the event graph, focusing
on the most salient events or on specific
types of event pairs (e.g., only events in the
same sentence). Deeper temporal reason-
ers struggle with this sparsity because the
entire temporal picture is not represented.
This paper proposes a new annotation pro-
cess with a mechanism to force annotators
to label connected graphs. It generates 10
times more relations per document than the
TimeBank, and our TimeBank-Dense cor-
pus is larger than all current corpora. We
hope this process and its dense corpus en-
courages research on new global models
with deeper reasoning.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931204081633">
The TimeBank Corpus (Pustejovsky et al., 2003)
ushered in a wave of data-driven event ordering
research. It provided for a common dataset of re-
lations between events and time expressions that
allowed the community to compare approaches.
Later corpora and competitions have based their
tasks on the TimeBank setup. This paper ad-
dresses one of its shortcomings: sparse annotation.
We describe a new annotation framework (and a
TimeBank-Dense corpus) that we believe is needed
to fulfill the data needs of deeper reasoners.
The TimeBank includes a small subset of all
possible relations in its documents. The annota-
tors were instructed to label relations critical to the
document’s understanding. The result is a sparse la-
beling that leaves much of the document unlabeled.
The TempEval contests have largely followed suit
and focused on specific types of event pairs. For
instance, TempEval (Verhagen et al., 2007) only
labeled relations between events that syntactically
dominated each other. This paper is the first attempt
to annotate a document’s entire temporal graph.
A consequence of focusing on all relations is a
shift from the traditional classification task, where
the system is given a pair of events and asked only
to label the type of relation, to an identification task,
where the system must determine for itself which
events in the document to pair up. For example, in
TempEval-1 and 2 (Verhagen et al., 2007; Verha-
gen et al., 2010), systems were given event pairs
in specific syntactic positions: events and times in
the same noun phrase, main events in consecutive
sentences, etc. We now aim for a shift in the com-
munity wherein all pairs are considered candidates
for temporal ordering, allowing researchers to ask
questions such as: how must algorithms adapt to
label the complete graph of pairs, and if the more
difficult and ambiguous event pairs are included,
how must feature-based learners change?
We are not the first to propose these questions,
but this paper is the first to directly propose the
means by which they can be addressed. The stated
goal of TempEval-3 (UzZaman et al., 2013) was to
focus on relation identification instead of classifica-
tion, but the training and evaluation data followed
the TimeBank approach where only a subset of
event pairs were labeled. As a result, many systems
focused on classification, with the top system clas-
sifying pairs in only three syntactic constructions
</bodyText>
<page confidence="0.961579">
501
</page>
<bodyText confidence="0.2577775">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501–506,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.97281">
Current Systems &amp; Evaluations This Proposal
</subsectionHeader>
<bodyText confidence="0.997467142857143">
There were four or five people inside,
and they just started firing
There were four or five people inside,
and they just started firing
Ms. Sanders was hit several times and
was pronounced dead at the scene.
The other customers fled, and the
police said it did not appear that anyone
else was injured.
Ms. Sanders was hit several times and
was pronounced dead at the scene.
The other customers fled, and the
police said it did not appear that anyone
else was injured.
</bodyText>
<figureCaption confidence="0.991178">
Figure 1: A TimeBank annotated document is on the left, and this paper’s TimeBank-Dense annotation is
</figureCaption>
<bodyText confidence="0.94328375">
on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations.
(Bethard, 2013). We describe the first annotation
framework that forces annotators to annotate all
pairs1. With this new process, we created a dense
ordering of document events that can properly eval-
uate both relation identification and relation anno-
tation. Figure 1 illustrates one document before
and after our new annotations.
</bodyText>
<sectionHeader confidence="0.954871" genericHeader="method">
2 Previous Annotation Work
</sectionHeader>
<bodyText confidence="0.999195117647059">
The majority of corpora and competitions for event
ordering contain sparse annotations. Annotators for
the original TimeBank (Pustejovsky et al., 2003)
only annotated relations judged to be salient by
the annotator. Subsequent TempEval competitions
(Verhagen et al., 2007; Verhagen et al., 2010; Uz-
Zaman et al., 2013) mostly relied on the TimeBank,
but also aimed to improve coverage by annotating
relations between all events and times in the same
sentence. However, event tokens that were men-
tioned fewer than 20 times were excluded and only
one TempEval task considered relations between
events in different sentences. In practical terms, the
resulting evaluations remained sparse.
A major dilemma underlying these sparse tasks
is that the unlabeled event/time pairs are ambigu-
ous. Each unlabeled pair holds 3 possibilities:
</bodyText>
<listItem confidence="0.991844">
1. The annotator looked at the pair of events and
decided that no temporal relation exists.
2. The annotator did not look at the pair of
events, so a relation may or may not exist.
3. The annotator failed to look at the pair of
events, so a single relation may exist.
</listItem>
<bodyText confidence="0.435665">
Training and evaluation of temporal reasoners is
hampered by this ambiguity. To combat this, our
</bodyText>
<footnote confidence="0.864453">
1As discussed below, all pairs in a given window size.
</footnote>
<table confidence="0.999489777777778">
Events Times Rels R
TimeBank 7935 1414 6418 0.7
Bramsen 2006 627 – 615 1.0
TempEval-07 6832 1249 5790 0.7
TempEval-10 5688 2117 4907 0.6
TempEval-13 11145 2078 11098 0.8
Kolomiyets-12 1233 – 1139 0.9
Do 20122 324 232 3132 5.6
This work 1729 289 12715 6.3
</table>
<tableCaption confidence="0.965956">
Table 1: Events, times, relations and the ratio of
relations to events + times (R) in various corpora.
</tableCaption>
<bodyText confidence="0.999919142857143">
annotation adopts the VAGUE relation introduced
by TempEval 2007, and our approach forces anno-
tators to use it. This is the only work that includes
such a mechanism.
This paper is not the first to look into more dense
annotations. Bramsen et al. (2006) annotated multi-
sentence segments of text to build directed acyclic
graphs. Kolomiyets et al. (2012) annotated “tem-
poral dependency structures”, though they only
focused on relations between pairs of events. Do
et al. (2012) produced the densest annotation, but
“the annotator was not required to annotate all pairs
of event mentions, but as many as possible”. The
current paper takes a different tack to annotation
by requiring annotators to label every possible pair
of events/times in a given window. Thus this work
is the first annotation effort that can guarantee its
event/time graph to be strongly connected.
Table 1 compares the size and density of our
corpus to others. Ours is the densest and it contains
the largest number of temporal relations.
</bodyText>
<footnote confidence="0.584571">
2Do et al. (2012) reports 6264 relations, but this includes
both the relations and their inverses. We thus halve the count
</footnote>
<page confidence="0.995091">
502
</page>
<sectionHeader confidence="0.982491" genericHeader="method">
3 A Framework for Dense Annotation
</sectionHeader>
<bodyText confidence="0.974272511627907">
Frameworks for annotating text typically have two
independent facets: (1) the practical means of how
to label the text, and (2) the higher-level rules about
when something should be labeled. The first is
often accomplished through a markup language,
and we follow prior work in adopting TimeML here.
The second facet is the focus of this paper: when
should an annotator label an ordering relation?
Our proposal starts with documents that have al-
ready been annotated with events, time expressions,
and document creation times (DCT). The following
sentence serves as our motivating example:
Police confirmed Friday that the body
found along a highway in San Juan be-
longed to Jorge Hernandez.
This sentence is represented by a 4 node graph (3
events and 1 time). In a completely annotated graph
it would have 6 edges (relations) connecting the
nodes. In the TimeBank, from which this sentence
is drawn, only 3 of the 6 edges are labeled.
The impact of these annotation decisions (i.e.,
when to annotate a relation) can be significant. In
this example, a learner must somehow deal with
the 3 unlabeled edges. One option is to assume that
they are vague or ambiguous. However, all 6 edges
have clear well-defined ordering relations:
belonged BEFORE confirmed
belonged BEFORE found
found BEFORE confirmed
belonged BEFORE Friday
confirmed IS INCLUDED IN Friday
found IS INCLUDED IN Friday3
Learning algorithms handle these unlabeled
edges by making incorrect assumptions, or by ig-
noring large parts of the temporal graph. Sev-
eral models with rich temporal reasoners have
been published, but since they require more con-
nected graphs, improvement over pairwise classi-
fiers have been minimal (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009). This paper thus
proposes an annotation process that builds denser
graphs with formal properties that learners can rely
on, such as locally complete subgraphs.
</bodyText>
<subsectionHeader confidence="0.996765">
3.1 Ensuring Dense Graphs
</subsectionHeader>
<bodyText confidence="0.944710333333333">
While the ideal goal is to create a complete graph,
the time it would take to hand-label n(n − 1)/2
for accurate comparison to other corpora.
</bodyText>
<footnote confidence="0.895864">
3Revealed by the previous sentence (not shown here).
</footnote>
<bodyText confidence="0.9508064">
edges is prohibitive. We approximate completeness
by creating locally complete graphs over neigh-
boring sentences. The resulting event graph for a
document is strongly connected, but not complete.
Specifically, the following edge types are included:
</bodyText>
<listItem confidence="0.990901142857143">
1. Event-Event, Event-Time, and Time-Time
pairs in the same sentence
2. Event-Event, Event-Time, and Time-Time
pairs between the current and next sentence
3. Event-DCT pairs for every event in the text
4. Time-DCT pairs for every time expression in
the text
</listItem>
<bodyText confidence="0.999818">
Our process requires annotators to annotate the
above edge types, enforced via an annotation tool.
We describe the relation set and this tool next.
</bodyText>
<subsectionHeader confidence="0.676479">
3.1.1 Temporal Relations
</subsectionHeader>
<bodyText confidence="0.998620736842105">
The TimeBank corpus uses 14 relations based on
the Allen interval relations. The TempEval contests
have used a small set of relations (TempEval-1) and
the larger set of 14 relations (TempEval-3). Pub-
lished work has mirrored this trend, and different
groups focus on different aspects of the semantics.
We chose a middle ground between coarse and
fine-grained distinctions for annotation, settling on
6 relations: before, after, includes, is included, si-
multaneous, and vague. We do not adopt a more
fine-grained set because we annotate pairs that are
far more ambiguous than those considered in previ-
ous efforts. Decisions between relations like before
and immediately before can complicate an already
difficult task. The added benefit of a corpus (or
working system) that makes fine-grained distinc-
tions is also not clear. We lean toward higher an-
notator agreement with relations that have greater
separation between their semantics4.
</bodyText>
<subsectionHeader confidence="0.822435">
3.1.2 Enforcing Annotation
</subsectionHeader>
<bodyText confidence="0.9999465">
Imposing the above rules on annotators requires
automated assistance. We built a new tool that
reads TimeML formatted text, and computes the
set of required edges. Annotators are prompted to
assign a label for each edge, and skipping edges is
prohibited.5 The tool is unique in that it includes
a transitive reasoner that infers relations based on
the annotator’s latest annotations. For example,
</bodyText>
<footnote confidence="0.978935">
4For instance, a relation like starts is a special case of in-
cludes if events are viewed as open intervals, and immediately
before is a special case of before. We avoid this overlap and
only use includes and before
5Note that annotators are presented with pairs in order from
document start to finish, starting with the first two events.
</footnote>
<page confidence="0.998569">
503
</page>
<bodyText confidence="0.999964727272727">
if event e1 IS INCLUDED in t1, and t1 BEFORE
e2, the tool automatically labels e1 BEFORE e2.
The transitivity inference is run after each input
label, and the human annotator cannot override
the inferences. This prohibits the annotator from
entering edges that break transitivity. As a result,
several properties are ensured through this process:
the graph (1) is a strongly connected graph, (2) is
consistent with no contradictions, and (3) has all
required edges labeled. These 3 properties are new
to all current ordering corpora.
</bodyText>
<subsectionHeader confidence="0.99979">
3.2 Annotation Guidelines
</subsectionHeader>
<bodyText confidence="0.999573246575343">
Since the annotation tool frees the annotators from
the decision of when to label an edge, the focus is
now what to label each edge. This section describes
the guidelines for dense annotation.
The 80% confidence rule: The decision to label
an edge as VAGUE instead of a defined temporal
relation is critical. We adopted an 80% rule that in-
structed annotators to choose a specific non-vague
relation if they are 80% confident that it was the
writer’s intent that a reader infer that relation. By
not requiring 100% confidence, we allow for alter-
native interpretations that conflict with the chosen
edge label as long as that alternative is sufficiently
unlikely. In practice, annotators had different inter-
pretations of what constitutes 80% certainty, and
this generated much discussion. We mitigated these
disagreements with the following rule.
Majority annotator agreement: An edge’s la-
bel is the relation that received a majority of an-
notator votes, otherwise it is marked VAGUE. If a
document has 2 annotators, both have to agree on
the relation or it is labeled VAGUE. A document
with 3 annotators requires 2 to agree. This agree-
ment rule acts as a check to our 80% confidence
rule, backing off to VAGUE when decisions are un-
certain (arguably, this is the definition of VAGUE).
We also encouraged consistent labelings with
guidelines inspired by Bethard and Martin (2008).
Modal and conditional events: interpreted with
a possible worlds analysis. The core event was
treated as having occurred, whether or not the text
implied that it had occurred. For example,
They [EVENT expect] him to [EVENT
cut] costs throughout the organization.
This event pair is ordered (expect before cut) since
the expectation occurs before the cutting (in the
possible world where the cutting occurs). Negated
events and hypotheticals are treated similarly. One
assumes the event does occur, and all other events
are ordered accordingly. Negated states like “is not
anticipating” are interpreted as though the antici-
pation occurs, and surrounding events are ordered
with regard to its presumed temporal span.
Aspectual Events: annotated as IS INCLUDED
in their event arguments. For instance, events that
describe the manner in which another event is per-
formed are considered encompassed by the broader
event. Consider the following example:
The move may [EVENT help] [EVENT
prevent] Martin Ackerman from making
a run at the computer-services concern.
This event pair is assigned the relation (help IS IN-
CLUDED in prevent) because the help event is not
meaningful on its own. It describes the proportion
of the preventing accounted for by the move. In
TimeBank, the intentional action class is used in-
stead of the aspectual class in this case, but we still
consider it covered by this guideline.
Events that attribute a property: to a person
or event are interpreted to end when the entity ends.
For instance, ‘the talk is nonsense’ evokes a non-
sense event with an end point that coincides with
the end of the talk.
Time Expressions: the words now and today
were given “long now” interpretations if the words
could be replaced with nowadays and not change
the meaning of their sentences. The time’s dura-
tion starts sometime in the past and INCLUDES the
DCT. If nowadays is not suitable, then the now was
INCLUDED IN the DCT.
Generic Events: can be ordered with respect to
each other, but must be VAGUE with respect to
nearby non-generic events.
</bodyText>
<sectionHeader confidence="0.987704" genericHeader="method">
4 TimeBank-Dense: corpus statistics
</sectionHeader>
<bodyText confidence="0.9998634">
We chose a subset of TimeBank documents for our
new corpus: TimeBank-Dense. This provided an
initial labeling of events and time expressions. Us-
ing the tool described above, we annotated 36 ran-
dom documents with at least two annotators each.
These 36 were annotated with 4 times as many
relations as the entire 183 document TimeBank.
The four authors of this paper were the four an-
notators. All four annotated the same initial docu-
ment, conflicts and disagreements were discussed,
</bodyText>
<page confidence="0.995812">
504
</page>
<table confidence="0.9553508">
Annotated Relation Count
BEFORE 2590 INCLUDES 836
AFTER 2104 INCLUDED IN 1060
SIMULTAN. 215 VAGUE 5910
Total Relations: 12715
</table>
<tableCaption confidence="0.999335">
Table 2: Relation counts in TimeBank-Dense.
</tableCaption>
<bodyText confidence="0.999878023255814">
and guidelines were updated accordingly. The rest
of the documents were then annotated indepen-
dently. Document annotation was not random, but
we mixed pairs of authors where time constraints al-
lowed. Table 2 shows the relation counts in the final
corpus, and Table 3 gives the annotator agreement.
We show precision (holding one annotation as gold)
and kappa computed on the 4 types of pairs from
section 3.1. Micro-averaged precision was 65.1%,
compared to TimeBank’s 77%. Kappa ranged from
.56-.64, a slight drop from TimeBank’s .71.
The vague relation makes up 46% of the rela-
tions. This is the first empirical count of how many
temporal relations in news articles are truly vague.
Our lower agreement is likely due to the more
difficult task. Table 5 breaks down the individual
disagreements. The most frequent pertained to the
VAGUE relation. Practically speaking, VAGUE was
applied to the final graph if either annotator chose
it. This seems appropriate since a disagreement be-
tween annotators implies that the relation is vague.
The following example illustrates the difficulty
of labeling edges with a VAGUE relation:
No one was hurt, but firefighters or-
dered the evacuation of nearby homes
and said they’ll monitor the ground.
Both annotators chose VAGUE to label ordered and
said because the order is unclear. However, they
disagreed on evacuation with monitor. One chose
VAGUE, but the other chose IS INCLUDED. There is
a valid interpretation where a monitoring process
has already begun, and continues after the evacua-
tion. This interpretation reached 80% confidence
for one annotator, but not the other. In the face of
such a disagreement, the pair is labeled VAGUE.
How often do these disagreements occur? Ta-
ble 4 shows the 3 sources: (1) mutual vague: anno-
tators agree it is vague, (2) partial vague: one anno-
tator chooses vague, but the other does not, and (3)
no vague: annotators choose conflicting non-vague
relations. Only 17% of these disagreements are due
to hard conflicts (no vague). The released corpus
includes these 3 fine-grained VAGUE relations.
</bodyText>
<table confidence="0.9982526">
Annotators # Links Prec Kappa
A and B 9282 .65 .56
A and D 1605 .72 .63
B and D 279 .70 .64
C and D 1549 .65 .57
</table>
<tableCaption confidence="0.991492">
Table 3: Agreement between different annotators.
</tableCaption>
<table confidence="0.999805">
# Vague
Mutual VAGUE 1657 (28%)
Partial VAGUE 3234 (55%)
No VAGUE 1019 (17%)
</table>
<tableCaption confidence="0.937368666666667">
Table 4: VAGUE relation origins. Partial vague:
one annotator does not choose vague. No vague:
neither annotator chooses vague.
</tableCaption>
<figure confidence="0.746945">
b a i ii s v
b 1776 22 88 37 21 192
a 17 1444 32 102 9 155
</figure>
<table confidence="0.83313375">
i 71 34 642 45 23 191
ii 81 76 40 826 31 230
s 12 8 25 28 147 29
v 500 441 289 356 64 1197
</table>
<tableCaption confidence="0.980682">
Table 5: Relation agreement between the two main
annotators. Most disagreements involved VAGUE.
</tableCaption>
<sectionHeader confidence="0.997161" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999974">
We described our annotation framework that pro-
duces corpora with formal guarantees about the an-
notated graph’s structure. Both the annotation tool
and the new TimeBank-Dense corpus are publicly
available.6 This is the first corpus with guarantees
of connectedness, consistency, and a semantics for
unlabeled edges. We hope to encourage a shift in
the temporal ordering community to consider the
entire document when making local decisions. Fur-
ther work is needed to handle difficult pairs with
the VAGUE relation. We look forward to evaluating
new algorithms on this dense corpus.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998758833333333">
This work was supported, in part, by the Johns
Hopkins Human Language Technology Center of
Excellence. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors. We also give thanks
to Benjamin Van Durme for assistance and insight.
</bodyText>
<footnote confidence="0.980454">
6http://www.usna.edu/Users/cs/nchamber/caevo/
</footnote>
<page confidence="0.977062">
505
</page>
<note confidence="0.6872796">
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57–62. As-
sociation for Computational Linguistics.
</note>
<sectionHeader confidence="0.772184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997786803278689">
Steven Bethard, William J Corvey, Sara Klingenstein,
and James H Martin. 2008. Building a corpus of
temporal-causal structure. In LREC.
Steven Bethard. 2013. Cleartk-timeml: A minimal-
ist approach to tempeval 2013. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), pages 10–14, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
P. Bramsen, P. Deshpande, Y.K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In Proceedings of
the 2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 189–198. ACL.
N. Chambers and D. Jurafsky. 2008. Jointly com-
bining implicit constraints improves temporal order-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
698–706. ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 677–687, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 88–97, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. In Corpus
linguistics, volume 2003, page 40.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1–9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
75–80. Association for Computational Linguistics.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal rela-
tions with Markov Logic. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
405–413. ACL.
</reference>
<page confidence="0.998385">
506
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453589">
<title confidence="0.999882">An Annotation Framework for Dense Event Ordering</title>
<author confidence="0.937507">Taylor</author>
<affiliation confidence="0.996412">IBM Research</affiliation>
<email confidence="0.979616">taylor.cassidy.ctr@mail.mil</email>
<author confidence="0.680006">Nathanael</author>
<affiliation confidence="0.765711">US Naval</affiliation>
<email confidence="0.998244">nchamber@usna.edu</email>
<author confidence="0.993698">Bill</author>
<affiliation confidence="0.911944">Carnegie Mellon</affiliation>
<email confidence="0.998423">forkunited@gmail.com</email>
<author confidence="0.998973">Steven Bethard</author>
<affiliation confidence="0.986668">Univ. of Alabama at</affiliation>
<email confidence="0.997938">bethard@cis.uab.edu</email>
<abstract confidence="0.999168583333333">Today’s event ordering research is heavily dependent on annotated corpora. Current corpora influence shared evaluations and drive algorithm development. Partly due to this dependence, most research foon orderings a document’s events. For instance, the TempEval competitions and the TimeBank only annotate small portions of the event graph, focusing on the most salient events or on specific types of event pairs (e.g., only events in the same sentence). Deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented. This paper proposes a new annotation process with a mechanism to force annotators to label connected graphs. It generates 10 times more relations per document than the and our corpus is larger than all current corpora. We hope this process and its dense corpus encourages research on new global models with deeper reasoning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>William J Corvey</author>
<author>Sara Klingenstein</author>
<author>James H Martin</author>
</authors>
<title>Building a corpus of temporal-causal structure.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<marker>Bethard, Corvey, Klingenstein, Martin, 2008</marker>
<rawString>Steven Bethard, William J Corvey, Sara Klingenstein, and James H Martin. 2008. Building a corpus of temporal-causal structure. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
</authors>
<title>Cleartk-timeml: A minimalist approach to tempeval 2013.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>10--14</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="4615" citStr="Bethard, 2013" startWordPosition="726" endWordPosition="727">were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the police said it did not appear that anyone else was injured. Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: A TimeBank annotated document is on the left, and this paper’s TimeBank-Dense annotation is on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations. (Bethard, 2013). We describe the first annotation framework that forces annotators to annotate all pairs1. With this new process, we created a dense ordering of document events that can properly evaluate both relation identification and relation annotation. Figure 1 illustrates one document before and after our new annotations. 2 Previous Annotation Work The majority of corpora and competitions for event ordering contain sparse annotations. Annotators for the original TimeBank (Pustejovsky et al., 2003) only annotated relations judged to be salient by the annotator. Subsequent TempEval competitions (Verhagen</context>
</contexts>
<marker>Bethard, 2013</marker>
<rawString>Steven Bethard. 2013. Cleartk-timeml: A minimalist approach to tempeval 2013. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 10–14, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bramsen</author>
<author>P Deshpande</author>
<author>Y K Lee</author>
<author>R Barzilay</author>
</authors>
<title>Inducing temporal graphs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>189--198</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6815" citStr="Bramsen et al. (2006)" startWordPosition="1087" endWordPosition="1090">ndow size. Events Times Rels R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval-07 6832 1249 5790 0.7 TempEval-10 5688 2117 4907 0.6 TempEval-13 11145 2078 11098 0.8 Kolomiyets-12 1233 – 1139 0.9 Do 20122 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be stro</context>
</contexts>
<marker>Bramsen, Deshpande, Lee, Barzilay, 2006</marker>
<rawString>P. Bramsen, P. Deshpande, Y.K. Lee, and R. Barzilay. 2006. Inducing temporal graphs. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 189–198. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Jointly combining implicit constraints improves temporal ordering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>698--706</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9444" citStr="Chambers and Jurafsky, 2008" startWordPosition="1513" endWordPosition="1516">he 3 unlabeled edges. One option is to assume that they are vague or ambiguous. However, all 6 edges have clear well-defined ordering relations: belonged BEFORE confirmed belonged BEFORE found found BEFORE confirmed belonged BEFORE Friday confirmed IS INCLUDED IN Friday found IS INCLUDED IN Friday3 Learning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph. Several models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs. 3.1 Ensuring Dense Graphs While the ideal goal is to create a complete graph, the time it would take to hand-label n(n − 1)/2 for accurate comparison to other corpora. 3Revealed by the previous sentence (not shown here). edges is prohibitive. We approximate completeness by creating locally complete graphs over neighboring sentences. The resulting event graph for a document is strongly connected, but not complet</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>N. Chambers and D. Jurafsky. 2008. Jointly combining implicit constraints improves temporal ordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 698–706. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Do</author>
<author>Wei Lu</author>
<author>Dan Roth</author>
</authors>
<title>Joint inference for event timeline construction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>677--687</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="7039" citStr="Do et al. (2012)" startWordPosition="1121" endWordPosition="1124">132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected. Table 1 compares the size and density of our corpus to others. Ours is the densest and it contains the largest number of temporal relations. 2Do et al. (2012) reports 6264 relations, but this includes both th</context>
</contexts>
<marker>Do, Lu, Roth, 2012</marker>
<rawString>Quang Do, Wei Lu, and Dan Roth. 2012. Joint inference for event timeline construction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 677–687, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Extracting narrative timelines as temporal dependency structures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>88--97</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6915" citStr="Kolomiyets et al. (2012)" startWordPosition="1102" endWordPosition="1105"> 6832 1249 5790 0.7 TempEval-10 5688 2117 4907 0.6 TempEval-13 11145 2078 11098 0.8 Kolomiyets-12 1233 – 1139 0.9 Do 20122 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected. Table 1 compares the size and density of our corpus to others. Ours is the densest a</context>
</contexts>
<marker>Kolomiyets, Bethard, Moens, 2012</marker>
<rawString>Oleksandr Kolomiyets, Steven Bethard, and MarieFrancine Moens. 2012. Extracting narrative timelines as temporal dependency structures. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 88–97, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
</authors>
<title>The timebank corpus.</title>
<date>2003</date>
<booktitle>In Corpus linguistics,</booktitle>
<volume>volume</volume>
<pages>40</pages>
<contexts>
<context position="1288" citStr="Pustejovsky et al., 2003" startWordPosition="186" endWordPosition="189">n the most salient events or on specific types of event pairs (e.g., only events in the same sentence). Deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented. This paper proposes a new annotation process with a mechanism to force annotators to label connected graphs. It generates 10 times more relations per document than the TimeBank, and our TimeBank-Dense corpus is larger than all current corpora. We hope this process and its dense corpus encourages research on new global models with deeper reasoning. 1 Introduction The TimeBank Corpus (Pustejovsky et al., 2003) ushered in a wave of data-driven event ordering research. It provided for a common dataset of relations between events and time expressions that allowed the community to compare approaches. Later corpora and competitions have based their tasks on the TimeBank setup. This paper addresses one of its shortcomings: sparse annotation. We describe a new annotation framework (and a TimeBank-Dense corpus) that we believe is needed to fulfill the data needs of deeper reasoners. The TimeBank includes a small subset of all possible relations in its documents. The annotators were instructed to label rela</context>
<context position="5108" citStr="Pustejovsky et al., 2003" startWordPosition="797" endWordPosition="800">annotation is on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations. (Bethard, 2013). We describe the first annotation framework that forces annotators to annotate all pairs1. With this new process, we created a dense ordering of document events that can properly evaluate both relation identification and relation annotation. Figure 1 illustrates one document before and after our new annotations. 2 Previous Annotation Work The majority of corpora and competitions for event ordering contain sparse annotations. Annotators for the original TimeBank (Pustejovsky et al., 2003) only annotated relations judged to be salient by the annotator. Subsequent TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) mostly relied on the TimeBank, but also aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. In practical terms, the resulting evaluations remained sparse. A major dilemma underlying these sparse tasks is that the unlabele</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003. The timebank corpus. In Corpus linguistics, volume 2003, page 40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>James Allen</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3354" citStr="UzZaman et al., 2013" startWordPosition="524" endWordPosition="527">syntactic positions: events and times in the same noun phrase, main events in consecutive sentences, etc. We now aim for a shift in the community wherein all pairs are considered candidates for temporal ordering, allowing researchers to ask questions such as: how must algorithms adapt to label the complete graph of pairs, and if the more difficult and ambiguous event pairs are included, how must feature-based learners change? We are not the first to propose these questions, but this paper is the first to directly propose the means by which they can be addressed. The stated goal of TempEval-3 (UzZaman et al., 2013) was to focus on relation identification instead of classification, but the training and evaluation data followed the TimeBank approach where only a subset of event pairs were labeled. As a result, many systems focused on classification, with the top system classifying pairs in only three syntactic constructions 501 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501–506, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Current Systems &amp; Evaluations This Proposal There were four or five peo</context>
<context position="5274" citStr="UzZaman et al., 2013" startWordPosition="822" endWordPosition="826">mework that forces annotators to annotate all pairs1. With this new process, we created a dense ordering of document events that can properly evaluate both relation identification and relation annotation. Figure 1 illustrates one document before and after our new annotations. 2 Previous Annotation Work The majority of corpora and competitions for event ordering contain sparse annotations. Annotators for the original TimeBank (Pustejovsky et al., 2003) only annotated relations judged to be salient by the annotator. Subsequent TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) mostly relied on the TimeBank, but also aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. In practical terms, the resulting evaluations remained sparse. A major dilemma underlying these sparse tasks is that the unlabeled event/time pairs are ambiguous. Each unlabeled pair holds 3 possibilities: 1. The annotator looked at the pair of events and decided that no temporal relation exist</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Allen, Verhagen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 1–9, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>75--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2153" citStr="Verhagen et al., 2007" startWordPosition="323" endWordPosition="326">s on the TimeBank setup. This paper addresses one of its shortcomings: sparse annotation. We describe a new annotation framework (and a TimeBank-Dense corpus) that we believe is needed to fulfill the data needs of deeper reasoners. The TimeBank includes a small subset of all possible relations in its documents. The annotators were instructed to label relations critical to the document’s understanding. The result is a sparse labeling that leaves much of the document unlabeled. The TempEval contests have largely followed suit and focused on specific types of event pairs. For instance, TempEval (Verhagen et al., 2007) only labeled relations between events that syntactically dominated each other. This paper is the first attempt to annotate a document’s entire temporal graph. A consequence of focusing on all relations is a shift from the traditional classification task, where the system is given a pair of events and asked only to label the type of relation, to an identification task, where the system must determine for itself which events in the document to pair up. For example, in TempEval-1 and 2 (Verhagen et al., 2007; Verhagen et al., 2010), systems were given event pairs in specific syntactic positions:</context>
<context position="5228" citStr="Verhagen et al., 2007" startWordPosition="814" endWordPosition="817">d, 2013). We describe the first annotation framework that forces annotators to annotate all pairs1. With this new process, we created a dense ordering of document events that can properly evaluate both relation identification and relation annotation. Figure 1 illustrates one document before and after our new annotations. 2 Previous Annotation Work The majority of corpora and competitions for event ordering contain sparse annotations. Annotators for the original TimeBank (Pustejovsky et al., 2003) only annotated relations judged to be salient by the annotator. Subsequent TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) mostly relied on the TimeBank, but also aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. In practical terms, the resulting evaluations remained sparse. A major dilemma underlying these sparse tasks is that the unlabeled event/time pairs are ambiguous. Each unlabeled pair holds 3 possibilities: 1. The annotator looked at the pair of even</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75–80. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yoshikawa</author>
<author>S Riedel</author>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>Jointly identifying temporal relations with Markov Logic.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>405--413</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9469" citStr="Yoshikawa et al., 2009" startWordPosition="1517" endWordPosition="1520">ion is to assume that they are vague or ambiguous. However, all 6 edges have clear well-defined ordering relations: belonged BEFORE confirmed belonged BEFORE found found BEFORE confirmed belonged BEFORE Friday confirmed IS INCLUDED IN Friday found IS INCLUDED IN Friday3 Learning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph. Several models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs. 3.1 Ensuring Dense Graphs While the ideal goal is to create a complete graph, the time it would take to hand-label n(n − 1)/2 for accurate comparison to other corpora. 3Revealed by the previous sentence (not shown here). edges is prohibitive. We approximate completeness by creating locally complete graphs over neighboring sentences. The resulting event graph for a document is strongly connected, but not complete. Specifically, the foll</context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsumoto. 2009. Jointly identifying temporal relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 405–413. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>