<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.074467">
<title confidence="0.998322">
The Wisdom of the Crowd’s Ear: Speech Accent Rating and Annotation
with Amazon Mechanical Turk
</title>
<author confidence="0.941226">
Stephen A. Kunath Steven H. Weinberger
</author>
<affiliation confidence="0.9054415">
Linguistics Department Program in Linguistics
Georgetown University 3e4 George Mason University
</affiliation>
<address confidence="0.829283">
Washington, D.C. 20057 Fairfax, VA 22030
</address>
<email confidence="0.996581">
sak68@georgetown.edu weinberg@gmu.edu
</email>
<sectionHeader confidence="0.998553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996810545454545">
Human listeners can almost instantaneously
judge whether or not another speaker is part of
their speech community. The basis of this
judgment is the speaker’s accent. Even though
humans judge speech accents with ease, it has
been tremendously difficult to automatically
evaluate and rate accents in any consistent
manner. This paper describes an experiment
using the Amazon Mechanical Turk to de-
velop an automatic speech accent rating
dataset.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980325">
In linguistics literature and especially in second
language acquisition research, the evaluation of
human speech accents relies on human judges.
Whenever humans listen to the speech of others
they are almost instantly able to determine whether
the speaker is from the same language community.
Indeed, much of the research in accent evaluation
relies on native speakers to listen to samples of
accented speech and rate the accent severity (An-
derson-Hsieh,et. al., 1992; Cunningham-Anderson
and Engstrand 1989; Gut, 2007; Koster and Koet
1993; Magen, 1998, Flege, 1995; Munro, 1995,
2001). Two problems arise from the use of this
methodology. One is that the purely linguistic
judgments may be infiltrated by certain biases. So
for example, all other things being equal, some
native English judges may interpret certain Viet-
namese accents as being more severe than say, Ital-
ian accents when listening to the English uttered by
speakers from these language backgrounds. The
second, and more theoretically interesting problem,
is that human judges make these ratings based
upon some hidden, abstract knowledge of phonol-
ogy. The mystery of what this knowledge is and
contains is real, for as Gut (2007) remarks, “Éno
exact, comprehensive and universally accepted
definition of foreign accent exists” (p75). The task
of this linguistic and computational study is to aid
in defining and uncovering this knowledge.
This study aims to develop a method for integrat-
ing accent ratings and judgments from a large
number of human listeners, provided through
Amazon Mechanical Turk(MTurk), to construct a
set of training data for an automated speaker accent
evaluation system. This data and methodology will
be a resource that accent researchers can utilize. It
reflects the wisdom of the crowd’s ear to help de-
termine the components of speech that different
listeners use to rate the accentedness of non-native
speakers.
</bodyText>
<sectionHeader confidence="0.940171" genericHeader="method">
2 Source Data
</sectionHeader>
<bodyText confidence="0.999644285714286">
This task required HIT workers to listen to and rate
a selection of non-native English speech samples.
The source of all the speech samples for this effort
was George Mason University’s Speech Accent
Archive (http://accent.gmu.edu). The Speech Ac-
cent Archive was chosen because of the high qual-
ity of samples as well as the fact that each speech
</bodyText>
<page confidence="0.939508">
168
</page>
<note confidence="0.6340565">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 168–171,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965533333333">
sample had readings of the same elicitation para-
graph. This elicitation paragraph was designed to
include all of the phonological features considered
part of native English speech. Additionally, narrow
phonetic transcriptions and phonological generali-
zations are available for each sample. Each
speaker’s information record contains demographic
information and language background information.
Three native language groups were selected for
this study: Arabic, Mandarin, and Russian. The
motivation for this particular selection comes from
the fact that each of these languages represents a
different language family. These languages contain
different phonetic inventories as well as
phonological patterns.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="method">
3 HIT Description
</sectionHeader>
<bodyText confidence="0.9999848">
Our HIT consisted of three sections. The first sec-
tion asked the worker to describe their own native
language background and any foreign language
knowledge or experience. Asking about native and
foreign language experience allowed us to estimate
possible rating bias arising from experience with
second language phonology. The second section of
the HIT included two rating tasks for use as a base-
line and to help the workers get acclimated to the
task. Each worker was asked to listen to two audio
samples of speakers reading the same elicitation
paragraph, one of a native English speaker and one
of a native Spanish speaker who started learning
English late in life. The rating scale used was a
five point Likert scale. After completing the base-
line question, workers began the third section and
were then asked to listen to fifteen samples of non-
native English speakers read the same elicitation
paragraph. After listening to each sample the
workers were asked to rate the accentedness of the
speech on the five point Likert scale. The five-
point scale rates native accent as a 1 and heavy
accent as a 5. Workers were additionally asked to
group each speech sample into different native
language categories. For this question they were
presented with 3 language family groups: A, B,
and C. Based on their perception of each speech
sample they would attempt to categorize the fifteen
speakers into distinct groups native language
groups.
</bodyText>
<sectionHeader confidence="0.933806" genericHeader="method">
4 Worker Requirements and Cost
</sectionHeader>
<bodyText confidence="0.999632111111111">
Due to the type of questions contained in our HIT
we came up with several worker requirements for
the HIT. The first and most important requirement
was that HIT workers be located inside of the USA
so as to limit the number of non-native English
speakers. This requirement also helped to increase
the likelihood that the listener would be familiar
with varieties of English speech accents common
in America. Additionally, due to the size of the
</bodyText>
<figureCaption confidence="0.99676">
Figure 1. Mechanical Turk workers ratings of 2
baseline samples: English 1 and Spanish 11. The
numbers on the horizontal axis represent the how
native-like the speaker was rated. A (1) indicates
that the speaker sounds like a native English
speaker. A (5) indicates the presence of a heavy
accent.
</figureCaption>
<bodyText confidence="0.999984166666667">
task we had a requirement that any worker must
have at least a 65% approval record for previous
HITs on other MTurk tasks. After looking at other-
comparably difficult tasks we decided to offer our
first HIT at $0.75. Subsequent HITs decreased the
offered price to $0.50 for the task.
</bodyText>
<sectionHeader confidence="0.994854" genericHeader="method">
5 HIT Results
</sectionHeader>
<bodyText confidence="0.999924">
Two HITs were issued for this task. Each HIT had
25 workers. Average time for each worker on this
task was approximately 12.5 minutes. Initial data
analysis showed that users correctly carried out the
tasks. Baseline question results, shown in figure 1,
indicated that virtually every worker agreed that
the native English speaker sample was a native
speaker of English. The ratings of the baseline
Spanish showed that workers generally agreed that
it was heavily accented speech. In addition to the
</bodyText>
<page confidence="0.997453">
169
</page>
<bodyText confidence="0.999839545454546">
high quality of baseline evaluations, workers con-
sistently provided their own native and foreign
language information.
Ratings of the speech samples in each question, as
seen in Figure 2, showed relatively consistent
evaluations across workers. A more detailed statis-
tical analysis of inter-worker ratings and groupings
is currently underway, but the initial statistical tests
show that there was a consistent correlation be-
tween certain phonological speech patterns and
ratings of accentedness.
</bodyText>
<sectionHeader confidence="0.99818" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999443636363636">
This experiment has already provided a wealth of
information on how human’s rate accents and how
consistent those ratings are across a large number
of listeners. Currently, we are integrating the ac-
cent ratings with the phonetic transcriptions and
the list of identified phonological speech processes
to construct a set of features that are correlated
with accent ratings. We have begun to capitalize
on the Mechanical Turk paradigm and are con-
structing a qualification test to help us better un-
derstand inter-worker agreement on accent rating.
</bodyText>
<figureCaption confidence="0.59183725">
Figure 2. Workers accent ratings for all speech samples. The horizontal axis indicates the accentedness rating: (1) is a
native English accent and (5) is heavily accented. The vertical axis indicates the number of HIT workers that provided
the same rating for the sample. The numbers at the end of each language name represent the Speech Accent Archive
sample id for the language, e.g. Mandarin.1 indicates that the sample was the Mandarin 1 speaker on the Archive.
</figureCaption>
<page confidence="0.989669">
170
</page>
<bodyText confidence="0.999916411764706">
This qualification test will include a larger sample
of Native English speech data as well as a broader
selection of foreign accents. In this new qualifica-
tion test workers will be presented with a scale to
rate the speakers accent from native-like to heavily
accented. Additionally, the user will be asked to
group the samples into native language families.
Once the user passes this qualification test they
will then be able to work on HITs that are consid-
erably shorter than the original long-form HIT de-
scribed in this paper. In the new HITs workers will
listen to one or more speech samples at a time and
both rate and, if required, attempt to group the
sample relative to other speech samples. The selec-
tion criteria for these new samples will be based on
the presence of phonological speech processes that
have the highest correlation with accent ratings.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999477666666667">
The authors would like to thank Amazon.com and
the workshop organizers for providing MTurk
credits to perform this research.
</bodyText>
<sectionHeader confidence="0.9909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.952278342857143">
Anderson-Hsieh, J., Johnson, R., &amp; Kohler, K. 1992.
The relationship between native speaker judgments
of non-native pronunciation and deviance in segmen-
tals, prosody, and syllable structure. Language
Learning, 42, 529-555.
Cunningham-Anderson, U., and Engstrand, E., 1989.
Perceived strength and identity of foreign accent in
Swedish. Phonetica, 46, 138-154.
Flege, James E., Murray J. Munro, and Ian R.A.
MacKay (1995). Factors Affecting Strength of Per-
ceived Foreign Accent in a Second Language. Jour-
nal of the Acoustical Society of America, 97, 5, pp
3125-3134.
Gut, U. 2007. Foreign Accent. In C. Muller, (ed.),
Speaker Classification I. Berlin: Springer.
Koster, C., and Koet, T. 1993. The evaluation of accent
in the English of Dutchmen. Language Learning, 43,
1, 69-92.
Lippi-Green, R. 1997. English with an Accent. New
York: Routledge.
Magen, H. 1998. The perception of foreign-accented
speech. Journal of Phonetics, 26, 381-400.
Munro, Murray J. (1995). Nonsegmental Factors in For-
eign Accent: Ratings of Filtered Speech. Studies in
Second Language Acquisition, 17, pp 17-34.
Munro, Murray J. and Tracey M. Derwing (2001).
Modeling Perceptions of the Accentedness and Com-
prehensibility of L2 Speech: The Role of Speaking
Rate. Studies in Second Language Acquisition, 23, pp
451-468.
Scovel, T. 1995. Differentiation, recognition, and identi-
fication in the discrimination of foreign accents. In
J.Archibald (ed), Phonological Acquisition and
Phonological Theory. Hillsdale, NJ: Lawrence Erl-
baum.
</reference>
<page confidence="0.998192">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924682">
<title confidence="0.9981825">The Wisdom of the Crowd’s Ear: Speech Accent Rating and with Amazon Mechanical Turk</title>
<author confidence="0.999928">Stephen A Kunath Steven H Weinberger</author>
<affiliation confidence="0.9910755">Linguistics Department Program in Linguistics Georgetown University 3e4 George Mason University</affiliation>
<address confidence="0.999664">Washington, D.C. 20057 Fairfax, VA 22030</address>
<email confidence="0.998252">sak68@georgetown.eduweinberg@gmu.edu</email>
<abstract confidence="0.995500666666667">Human listeners can almost instantaneously judge whether or not another speaker is part of their speech community. The basis of this judgment is the speaker’s accent. Even though humans judge speech accents with ease, it has been tremendously difficult to automatically evaluate and rate accents in any consistent manner. This paper describes an experiment using the Amazon Mechanical Turk to develop an automatic speech accent rating dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Anderson-Hsieh</author>
<author>R Johnson</author>
<author>K Kohler</author>
</authors>
<title>The relationship between native speaker judgments of non-native pronunciation and deviance in segmentals, prosody, and syllable structure.</title>
<date>1992</date>
<journal>Language Learning,</journal>
<volume>42</volume>
<pages>529--555</pages>
<marker>Anderson-Hsieh, Johnson, Kohler, 1992</marker>
<rawString>Anderson-Hsieh, J., Johnson, R., &amp; Kohler, K. 1992. The relationship between native speaker judgments of non-native pronunciation and deviance in segmentals, prosody, and syllable structure. Language Learning, 42, 529-555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Cunningham-Anderson</author>
<author>E Engstrand</author>
</authors>
<title>Perceived strength and identity of foreign accent in Swedish.</title>
<date>1989</date>
<journal>Phonetica,</journal>
<volume>46</volume>
<pages>138--154</pages>
<contexts>
<context position="1284" citStr="Cunningham-Anderson and Engstrand 1989" startWordPosition="183" endWordPosition="186">aper describes an experiment using the Amazon Mechanical Turk to develop an automatic speech accent rating dataset. 1 Introduction In linguistics literature and especially in second language acquisition research, the evaluation of human speech accents relies on human judges. Whenever humans listen to the speech of others they are almost instantly able to determine whether the speaker is from the same language community. Indeed, much of the research in accent evaluation relies on native speakers to listen to samples of accented speech and rate the accent severity (Anderson-Hsieh,et. al., 1992; Cunningham-Anderson and Engstrand 1989; Gut, 2007; Koster and Koet 1993; Magen, 1998, Flege, 1995; Munro, 1995, 2001). Two problems arise from the use of this methodology. One is that the purely linguistic judgments may be infiltrated by certain biases. So for example, all other things being equal, some native English judges may interpret certain Vietnamese accents as being more severe than say, Italian accents when listening to the English uttered by speakers from these language backgrounds. The second, and more theoretically interesting problem, is that human judges make these ratings based upon some hidden, abstract knowledge o</context>
</contexts>
<marker>Cunningham-Anderson, Engstrand, 1989</marker>
<rawString>Cunningham-Anderson, U., and Engstrand, E., 1989. Perceived strength and identity of foreign accent in Swedish. Phonetica, 46, 138-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James E Flege</author>
<author>Murray J Munro</author>
<author>Ian R A MacKay</author>
</authors>
<title>Factors Affecting Strength of Perceived Foreign Accent in a Second Language.</title>
<date>1995</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>97</volume>
<pages>3125--3134</pages>
<marker>Flege, Munro, MacKay, 1995</marker>
<rawString>Flege, James E., Murray J. Munro, and Ian R.A. MacKay (1995). Factors Affecting Strength of Perceived Foreign Accent in a Second Language. Journal of the Acoustical Society of America, 97, 5, pp 3125-3134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Gut</author>
</authors>
<title>Foreign Accent.</title>
<date>2007</date>
<booktitle>Speaker Classification I.</booktitle>
<editor>In C. Muller, (ed.),</editor>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="1295" citStr="Gut, 2007" startWordPosition="187" endWordPosition="188">mazon Mechanical Turk to develop an automatic speech accent rating dataset. 1 Introduction In linguistics literature and especially in second language acquisition research, the evaluation of human speech accents relies on human judges. Whenever humans listen to the speech of others they are almost instantly able to determine whether the speaker is from the same language community. Indeed, much of the research in accent evaluation relies on native speakers to listen to samples of accented speech and rate the accent severity (Anderson-Hsieh,et. al., 1992; Cunningham-Anderson and Engstrand 1989; Gut, 2007; Koster and Koet 1993; Magen, 1998, Flege, 1995; Munro, 1995, 2001). Two problems arise from the use of this methodology. One is that the purely linguistic judgments may be infiltrated by certain biases. So for example, all other things being equal, some native English judges may interpret certain Vietnamese accents as being more severe than say, Italian accents when listening to the English uttered by speakers from these language backgrounds. The second, and more theoretically interesting problem, is that human judges make these ratings based upon some hidden, abstract knowledge of phonology</context>
</contexts>
<marker>Gut, 2007</marker>
<rawString>Gut, U. 2007. Foreign Accent. In C. Muller, (ed.), Speaker Classification I. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Koster</author>
<author>T Koet</author>
</authors>
<title>The evaluation of accent in</title>
<date>1993</date>
<journal>the English of Dutchmen. Language Learning,</journal>
<volume>43</volume>
<pages>69--92</pages>
<contexts>
<context position="1317" citStr="Koster and Koet 1993" startWordPosition="189" endWordPosition="192">nical Turk to develop an automatic speech accent rating dataset. 1 Introduction In linguistics literature and especially in second language acquisition research, the evaluation of human speech accents relies on human judges. Whenever humans listen to the speech of others they are almost instantly able to determine whether the speaker is from the same language community. Indeed, much of the research in accent evaluation relies on native speakers to listen to samples of accented speech and rate the accent severity (Anderson-Hsieh,et. al., 1992; Cunningham-Anderson and Engstrand 1989; Gut, 2007; Koster and Koet 1993; Magen, 1998, Flege, 1995; Munro, 1995, 2001). Two problems arise from the use of this methodology. One is that the purely linguistic judgments may be infiltrated by certain biases. So for example, all other things being equal, some native English judges may interpret certain Vietnamese accents as being more severe than say, Italian accents when listening to the English uttered by speakers from these language backgrounds. The second, and more theoretically interesting problem, is that human judges make these ratings based upon some hidden, abstract knowledge of phonology. The mystery of what </context>
</contexts>
<marker>Koster, Koet, 1993</marker>
<rawString>Koster, C., and Koet, T. 1993. The evaluation of accent in the English of Dutchmen. Language Learning, 43, 1, 69-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lippi-Green</author>
</authors>
<title>English with an Accent.</title>
<date>1997</date>
<location>New York: Routledge.</location>
<marker>Lippi-Green, 1997</marker>
<rawString>Lippi-Green, R. 1997. English with an Accent. New York: Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Magen</author>
</authors>
<title>The perception of foreign-accented speech.</title>
<date>1998</date>
<journal>Journal of Phonetics,</journal>
<volume>26</volume>
<pages>381--400</pages>
<contexts>
<context position="1330" citStr="Magen, 1998" startWordPosition="193" endWordPosition="194">an automatic speech accent rating dataset. 1 Introduction In linguistics literature and especially in second language acquisition research, the evaluation of human speech accents relies on human judges. Whenever humans listen to the speech of others they are almost instantly able to determine whether the speaker is from the same language community. Indeed, much of the research in accent evaluation relies on native speakers to listen to samples of accented speech and rate the accent severity (Anderson-Hsieh,et. al., 1992; Cunningham-Anderson and Engstrand 1989; Gut, 2007; Koster and Koet 1993; Magen, 1998, Flege, 1995; Munro, 1995, 2001). Two problems arise from the use of this methodology. One is that the purely linguistic judgments may be infiltrated by certain biases. So for example, all other things being equal, some native English judges may interpret certain Vietnamese accents as being more severe than say, Italian accents when listening to the English uttered by speakers from these language backgrounds. The second, and more theoretically interesting problem, is that human judges make these ratings based upon some hidden, abstract knowledge of phonology. The mystery of what this knowledg</context>
</contexts>
<marker>Magen, 1998</marker>
<rawString>Magen, H. 1998. The perception of foreign-accented speech. Journal of Phonetics, 26, 381-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murray J Munro</author>
</authors>
<title>Nonsegmental Factors in Foreign Accent: Ratings of Filtered Speech.</title>
<date>1995</date>
<journal>Studies in Second Language Acquisition,</journal>
<volume>17</volume>
<pages>17--34</pages>
<contexts>
<context position="1356" citStr="Munro, 1995" startWordPosition="197" endWordPosition="198"> rating dataset. 1 Introduction In linguistics literature and especially in second language acquisition research, the evaluation of human speech accents relies on human judges. Whenever humans listen to the speech of others they are almost instantly able to determine whether the speaker is from the same language community. Indeed, much of the research in accent evaluation relies on native speakers to listen to samples of accented speech and rate the accent severity (Anderson-Hsieh,et. al., 1992; Cunningham-Anderson and Engstrand 1989; Gut, 2007; Koster and Koet 1993; Magen, 1998, Flege, 1995; Munro, 1995, 2001). Two problems arise from the use of this methodology. One is that the purely linguistic judgments may be infiltrated by certain biases. So for example, all other things being equal, some native English judges may interpret certain Vietnamese accents as being more severe than say, Italian accents when listening to the English uttered by speakers from these language backgrounds. The second, and more theoretically interesting problem, is that human judges make these ratings based upon some hidden, abstract knowledge of phonology. The mystery of what this knowledge is and contains is real,</context>
</contexts>
<marker>Munro, 1995</marker>
<rawString>Munro, Murray J. (1995). Nonsegmental Factors in Foreign Accent: Ratings of Filtered Speech. Studies in Second Language Acquisition, 17, pp 17-34.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Murray J Munro</author>
<author>M Tracey</author>
</authors>
<title>Derwing (2001). Modeling Perceptions of the Accentedness and Comprehensibility of L2 Speech: The Role of Speaking Rate.</title>
<journal>Studies in Second Language Acquisition,</journal>
<volume>23</volume>
<pages>451--468</pages>
<marker>Munro, Tracey, </marker>
<rawString>Munro, Murray J. and Tracey M. Derwing (2001). Modeling Perceptions of the Accentedness and Comprehensibility of L2 Speech: The Role of Speaking Rate. Studies in Second Language Acquisition, 23, pp 451-468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Scovel</author>
</authors>
<title>Differentiation, recognition, and identification in the discrimination of foreign accents.</title>
<date>1995</date>
<booktitle>In J.Archibald (ed), Phonological Acquisition and Phonological Theory.</booktitle>
<location>Hillsdale, NJ: Lawrence Erlbaum.</location>
<marker>Scovel, 1995</marker>
<rawString>Scovel, T. 1995. Differentiation, recognition, and identification in the discrimination of foreign accents. In J.Archibald (ed), Phonological Acquisition and Phonological Theory. Hillsdale, NJ: Lawrence Erlbaum.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>