<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039886">
<title confidence="0.921792">
Lexical Disambiguation: Sources of Information and their Statistical
Realization
</title>
<author confidence="0.847748">
Ido Dagan *
</author>
<affiliation confidence="0.784727">
Computer Science Department, Technion, Haifa, Israel
</affiliation>
<sectionHeader confidence="0.671361666666667" genericHeader="abstract">
and
IBM Scientific Center, Technion City, Haifa, Israel
Abstract
</sectionHeader>
<bodyText confidence="0.999882181818182">
Lexical disambiguation can be achieved using differ-
ent sources of information. Aiming at high perfor-
mance of automatic disambiguation it is important
to know the relative importance and applicability of
the various sources. In this paper we classify sev-
eral sources of information and show how some of
them can be achieved using statistical data. First
evaluations indicate the extreme importance of local
information, which mainly represents lexical associ-
ations and selectional restrictions for syntactically
related words.
</bodyText>
<sectionHeader confidence="0.956262" genericHeader="method">
1 Disambiguation Sources
</sectionHeader>
<bodyText confidence="0.999939583333334">
The resolution of lexical ambiguities in unrestricted
text is one of the most difficult tasks of natural lan-
guage processing. In machine translation we are
confronted with the related task of target word se-
lectionâ€” the task of deciding which target language
word is the most appropriate equivalent of a source
language word in context. In contrast to compu-
tational systems, humans seem to select the correct
sense of an ambiguous word without much effort and
usually without even being aware to the existence
of an ambiguous situation. This fact naturally led
researches to point out various sources of informa-
tion which may provide the necessary cues for dis-
ambiguation, either for humans or machines. The
following paragraphs classify these sources into two
major types, based on either understanding of the
text or frequency characteristics of it.
One kind of information relates to the under-
standing of the meaning of the text, using semantic
and pragmatic knowledge and applying reasoning
mechanisms. The following sentences, taken from
foreign news sections in the Israeli Hebrew press,
demonstrate how different levels of understanding
can provide the disambiguating information.
</bodyText>
<listItem confidence="0.793016">
(1) haver ha-bayit ha-&apos;elyon shel ha-parlament ha-
</listItem>
<bodyText confidence="0.956224578947368">
&apos;This research was partially supported by grant number
120-741 of the Israel Council for Research and Development
sovieti zaka be-monitin ke-hoker shel ha-shhitut
be-kazahst an.
This sentence translates into English as:
(2) The member of the upper house of the soviet
parliament acquired a reputation as an investi-
gator of the corruption in Kazakhstan.
The two most frequent senses of the ambiguous noun
&apos;haver&apos; correspond to the English words &apos;friend&apos; and
&apos;member&apos;. In the above example, the information for
selecting the correct sense is provided by the seman-
tic knowledge that &apos;a house of parliament&apos; typically
has members but not friends. Computationally this
kind of information is usually captured by a shal-
low semantic model of selectional restrictions. In
other cases, such as example (3), it is necessary to
use deeper understanding of the text, which involves
some level of reasoning:
</bodyText>
<listItem confidence="0.762878666666667">
(3) be-het&apos;em le-hoq ha-hagira ha-hadash tihye le-
kol ezrah sovieti ha-zkut ha-otomatit lekabel
darkon bar tokef le-hamesh shanim.
</listItem>
<bodyText confidence="0.95322665">
This sentence translates into English as:
(4) According to the new emigration bill every so-
viet citizen will have the automatic right to re-
ceive a passport valid for five years.
The Hebrew word `hagira&apos; is used for the two sub-
senses &apos;emigration&apos; and &apos;immigration&apos;. In order to
make the correct selection it is necessary to reason
that since the soviet bill relates to soviet citizens
then it concerns with leaving the country rather
than entering it.
Another kind of information source, which was
originally raised in the psycholinguistic literature,
relates to the relative frequencies of word senses
and associations between word senses. These fac-
tors were shown to play an important role in lexical
retrieval, and were suggested as relevant for lexical
disambiguation [4, 3]. Hanks [1], for example, lists
different words associated with the two senses of the
word &apos;bank&apos;, such as money, notes, account, invest-
ment etc. versus river, swim, boat etc.
</bodyText>
<page confidence="0.979424">
341
</page>
<bodyText confidence="0.988501210526316">
Aiming for high performance in automatic disam- by counting the number of times in which the two
biguation, it is important to know (a) what is the words cooccurred within a limited distance [1]. For
portion of ambiguous cases in running text which instance, the words &apos;member&apos; and &apos;acquire&apos; cooc-
can be resolved by each source of information and curred 81 times in the corpus within a maximal dis-
(b) how to set preferences among these sources when tance of 7 words. This statistic is partly correlated
they provide contradicting evidence. with the first statistic, capturing also cases that were
2 Statistical Information missed by the parser, but it also reflects lexical as-
A tempting starting point for answering the above sociations between words that tend to cooccur ad-
questions is to use various types of statistical data jacently without having a specific syntactic relation
about word senses and evaluate their contribution between them. For instance, in one of our examples
to disambiguation. In recent years, statistical data the word `hatsba&apos;ah&apos;, which means either &apos;voting&apos; or
were used successfully for other linguistic tasks. &apos;indication&apos;, cooccurred in the same sentence with
The process of acquiring statistical data is usually the word `bhirot&apos; (elections). We expect that the
faster and more standard and objective than manual adjacency statistic will indicate the strong associa-
construction of knowledge. This makes such data tion between &apos;voting&apos; and &apos;elections&apos;, and thus would
suitable for the evaluation task we are confronted prefer &apos;voting&apos; as the appropriate sense.
with. The following paragraphs describe the kinds The results reported in [2] together with further
of statistics we use and explain how they reflect dif- examination of our data have clearly indicated some
ferent types of disambiguating information. interesting facts. In the vast majority of cases
In another paper [2] we describe a new multilin- enough disambiguating information is provided by
gual approach in which we gather statistics about the immediate context, especially by syntactically
senses of ambiguous words of one language using related words. The absolute frequency of a word
a corpus of a different language. For example, sense does not seem very useful, since it usually can
the different word associations for the two senses be overridden successfully by the local context. An
of &apos;bank&apos; will be identified in a Hebrew corpus, encouraging fact is that deep understanding of the
where a distinct word is used for each of the senses. text is rarely necessary, and seems to be required
This method enabled us to collect statistics from only for very delicate distinctions such as in exam-
very large corpora without manually tagging the oc- ple (3). In future work we intend to further analyze
currences of the ambiguous words with their word our data and test more examples, so that we can
senses. In our first experiment we have examined reach more decisive and quantitive conclusions. We
about one hundred examples of ambiguous Hebrew believe that such conclusions will contribute to im-
words which were selected randomly from foreign prove lexical disambiguation in broad coverage sys-
news sections in the Israeli press. For each sense tems.
of a Hebrew word we have collected statistics (in References
an English corpus) on its absolute frequency and its [1] Church, K. W., and Hanks, P., Word associa-
cooccurrences with other words that were syntacti- tion norms, mutual information, and Lexicog-
cally related with it in the example sentence. raphy, Computational Linguistics, vol. 16(1),
Two kinds of statistics were maintained. One 22-29 (1990).
statistic was the number of times in which the re- [2] Dagan, Ido, Alon Itai and Ulrike Schwa11, Two
lated words were identified in the corpus having the languages are more informative than one, sub-
same syntactic relation as in the example sentence. mitted to ACL-91.
This kind of statistic reflects both selectional restric- [3] Meyer, D., Schvaneveldt, R. and Ruddy, M.,
tions, like the relation between &apos;member&apos; (versus Loci of contextual effects on visual word-
&apos;friend&apos;) and &apos;a house of parliament&apos;, and also word recognition, in P. Rabbitt and S. Dornic (eds.),
associations, like the association between &apos;member&apos; Attention and Performance V, Academic Press,
and &apos;reputation&apos;, which is stronger than the associ- New-York, 1975.
ation between &apos;friend&apos; and &apos;reputation&apos;. In the first [4] Simpson, Greg B. and Curt Burgess, Implica-
case we expect a null frequency for the semantically tions of lexical ambiguity resolution for word
illegal alternative, while in the second case we ex- recognition, in Small, S. L., G. W. Cotrell and
pect the difference in frequencies to represent the M. K. Tanenhaus, (eds.) Lexical Ambiguity Res-
different degrees of association between the compet- olution, Morgan Kaufman Publishers, 1988.
ing alternatives and their surrounding context. In 342
getting this syntactically based statistic we are of
course limited by the coverage and the accuracy of
the parser, thus getting smaller and somewhat noisy
counts relative to the real counts in the corpus.
A second and more robust statistic is obtained
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.299345">
<title confidence="0.9985935">Lexical Disambiguation: Sources of Information and their Statistical Realization</title>
<author confidence="0.99427">Ido Dagan</author>
<affiliation confidence="0.915337333333333">Computer Science Department, Technion, Haifa, Israel and IBM Scientific Center, Technion City, Haifa, Israel</affiliation>
<abstract confidence="0.996127843373494">Lexical disambiguation can be achieved using different sources of information. Aiming at high performance of automatic disambiguation it is important to know the relative importance and applicability of the various sources. In this paper we classify several sources of information and show how some of them can be achieved using statistical data. First evaluations indicate the extreme importance of local information, which mainly represents lexical associations and selectional restrictions for syntactically related words. Disambiguation The resolution of lexical ambiguities in unrestricted text is one of the most difficult tasks of natural language processing. In machine translation we are with the related task of word setask of deciding which target language word is the most appropriate equivalent of a source language word in context. In contrast to computational systems, humans seem to select the correct sense of an ambiguous word without much effort and usually without even being aware to the existence of an ambiguous situation. This fact naturally led researches to point out various sources of information which may provide the necessary cues for disambiguation, either for humans or machines. The following paragraphs classify these sources into two types, based on either the or of it. One kind of information relates to the understanding of the meaning of the text, using semantic and pragmatic knowledge and applying reasoning mechanisms. The following sentences, taken from foreign news sections in the Israeli Hebrew press, demonstrate how different levels of understanding can provide the disambiguating information. ha-bayit ha-&apos;elyon shel ha-parlament ha- &apos;This research was partially supported by grant number the Israel Council for Research and Development be-monitin ke-hoker shel ha-shhitut be-kazahst an. This sentence translates into English as: (2) The member of the upper house of the soviet parliament acquired a reputation as an investigator of the corruption in Kazakhstan. The two most frequent senses of the ambiguous noun &apos;haver&apos; correspond to the English words &apos;friend&apos; and &apos;member&apos;. In the above example, the information for selecting the correct sense is provided by the semantic knowledge that &apos;a house of parliament&apos; typically has members but not friends. Computationally this kind of information is usually captured by a shallow semantic model of selectional restrictions. In other cases, such as example (3), it is necessary to use deeper understanding of the text, which involves some level of reasoning: (3) be-het&apos;em le-hoq ha-hagira ha-hadash tihye lekol ezrah sovieti ha-zkut ha-otomatit lekabel darkon bar tokef le-hamesh shanim. This sentence translates into English as: (4) According to the new emigration bill every soviet citizen will have the automatic right to receive a passport valid for five years. The Hebrew word `hagira&apos; is used for the two subsenses &apos;emigration&apos; and &apos;immigration&apos;. In order to make the correct selection it is necessary to reason that since the soviet bill relates to soviet citizens then it concerns with leaving the country rather than entering it. Another kind of information source, which was originally raised in the psycholinguistic literature, relates to the relative frequencies of word senses and associations between word senses. These factors were shown to play an important role in lexical retrieval, and were suggested as relevant for lexical disambiguation [4, 3]. Hanks [1], for example, lists different words associated with the two senses of the &apos;bank&apos;, such as money, account, investversus river, swim, 341 Aiming for high performance in automatic disam-biguation, it is important to know (a) what is the portion of ambiguous cases in running text which can be resolved by each source of information and (b) how to set preferences among these sources when they provide contradicting evidence. by counting the number of times in which the two words cooccurred within a limited distance [1]. For instance, the words &apos;member&apos; and &apos;acquire&apos; cooc-curred 81 times in the corpus within a maximal dis-tance of 7 words. This statistic is partly correlated with the first statistic, capturing also cases that were missed by the parser, but it also reflects lexical as-sociations between words that tend to cooccur ad-jacently without having a specific syntactic relation between them. For instance, in one of our examples the word `hatsba&apos;ah&apos;, which means either &apos;voting&apos; or &apos;indication&apos;, cooccurred in the same sentence with the word `bhirot&apos; (elections). We expect that the adjacency statistic will indicate the strong associa-tion between &apos;voting&apos; and &apos;elections&apos;, and thus would prefer &apos;voting&apos; as the appropriate sense. 2 Statistical Information The results reported in [2] together with further examination of our data have clearly indicated some interesting facts. In the vast majority of cases enough disambiguating information is provided by the immediate context, especially by syntactically related words. The absolute frequency of a word sense does not seem very useful, since it usually can be overridden successfully by the local context. An encouraging fact is that deep understanding of the text is rarely necessary, and seems to be required only for very delicate distinctions such as in exam-ple (3). In future work we intend to further analyze our data and test more examples, so that we can reach more decisive and quantitive conclusions. We believe that such conclusions will contribute to im-prove lexical disambiguation in broad coverage sys-tems.</abstract>
<note confidence="0.895007666666667">A tempting starting point for answering the above questions is to use various types of statistical data about word senses and evaluate their contribution to disambiguation. In recent years, statistical data were used successfully for other linguistic tasks. The process of acquiring statistical data is usually faster and more standard and objective than manual construction of knowledge. This makes such data suitable for the evaluation task we are confronted with. The following paragraphs describe the kinds of statistics we use and explain how they reflect dif-ferent types of disambiguating information. References In another paper [2] we describe a new multilin-gual approach in which we gather statistics about senses of ambiguous words of one language using a corpus of a different language. For example, the different word associations for the two senses of &apos;bank&apos; will be identified in a Hebrew corpus, where a distinct word is used for each of the senses. This method enabled us to collect statistics from very large corpora without manually tagging the oc-currences of the ambiguous words with their word senses. In our first experiment we have examined about one hundred examples of ambiguous Hebrew words which were selected randomly from foreign news sections in the Israeli press. For each sense of a Hebrew word we have collected statistics (in an English corpus) on its absolute frequency and its cooccurrences with other words that were syntacti-cally related with it in the example sentence. [1] Church, K. W., and Hanks, P., Word associa-tion norms, mutual information, and Lexicog- Computational 16(1), 22-29 (1990). Two kinds of statistics were maintained. One statistic was the number of times in which the re-lated words were identified in the corpus having the same syntactic relation as in the example sentence. This kind of statistic reflects both selectional restric-tions, like the relation between &apos;member&apos; (versus &apos;friend&apos;) and &apos;a house of parliament&apos;, and also word associations, like the association between &apos;member&apos; and &apos;reputation&apos;, which is stronger than the associ-ation between &apos;friend&apos; and &apos;reputation&apos;. In the first case we expect a null frequency for the semantically illegal alternative, while in the second case we ex-pect the difference in frequencies to represent the different degrees of association between the compet-ing alternatives and their surrounding context. In getting this syntactically based statistic we are of course limited by the coverage and the accuracy of the parser, thus getting smaller and somewhat noisy counts relative to the real counts in the corpus. [2] Dagan, Ido, Alon Itai and Ulrike Schwa11, Two languages are more informative than one, sub-mitted to ACL-91. A second and more robust statistic is obtained [3] Meyer, D., Schvaneveldt, R. and Ruddy, M., Loci of contextual effects on visual word-recognition, in P. Rabbitt and S. Dornic (eds.), and Performance V, Press, New-York, 1975. [4] Simpson, Greg B. and Curt Burgess, Implica-tions of lexical ambiguity resolution for word in Small, S. L., G. W. Cotrell Tanenhaus, (eds.) Ambiguity Res- Kaufman Publishers, 1988. 342</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>