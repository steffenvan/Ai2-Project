<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.985824">
Multilingual Dependency-based Syntactic and Semantic Parsing
</title>
<author confidence="0.992898">
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting Liu
</author>
<affiliation confidence="0.985304333333333">
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
</affiliation>
<email confidence="0.924578">
{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.989485" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.933569615384615">
Our CoNLL 2009 Shared Task system in-
cludes three cascaded components: syntactic
parsing, predicate classification, and semantic
role labeling. A pseudo-projective high-order
graph-based model is used in our syntactic de-
pendency parser. A support vector machine
(SVM) model is used to classify predicate
senses. Semantic role labeling is achieved us-
ing maximum entropy (MaxEnt) model based
semantic role classification and integer linear
programming (ILP) based post inference. Fi-
nally, we win the first place in the joint task,
including both the closed and open challenges.
</figureCaption>
<sectionHeader confidence="0.906743" genericHeader="method">
1 System Architecture
</sectionHeader>
<bodyText confidence="0.9991662">
Our CoNLL 2009 Shared Task (Hajiˇc et al., 2009):
multilingual syntactic and semantic dependencies
system includes three cascaded components: syn-
tactic parsing, predicate classification, and semantic
role labeling.
</bodyText>
<sectionHeader confidence="0.986944" genericHeader="method">
2 Syntactic Dependency Parsing
</sectionHeader>
<bodyText confidence="0.9981145">
We extend our CoNLL 2008 graph-based
model (Che et al., 2008) in four ways:
</bodyText>
<listItem confidence="0.984894714285714">
1. We use bigram features to choose multiple pos-
sible syntactic labels for one arc, and decide the op-
timal label during decoding.
2. We extend the model with sibling features (Mc-
Donald, 2006).
3. We extend the model with grandchildren fea-
tures. Rather than only using the left-most and right-
most grandchildren as Carreras (2007) and Johans-
son and Nugues (2008) did, we use all left and right
grandchildren in our model.
4. We adopt the pseudo-projective approach in-
troduced in (Nivre and Nilsson, 2005) to handle the
non-projective languages including Czech, German
and English.
</listItem>
<subsectionHeader confidence="0.965363">
2.1 Syntactic Label Determining
</subsectionHeader>
<bodyText confidence="0.943554777777778">
The model of (Che et al., 2008) decided one la-
bel for each arc before decoding according to uni-
gram features, which caused lower labeled attach-
ment score (LAS). On the other hand, keeping all
possible labels for each arc made the decoding in-
efficient. Therefore, in the system of this year, we
adopt approximate techniques to compromise, as
shown in the following formulas.
flbl
</bodyText>
<equation confidence="0.959766933333333">
uni(h, c, l) = flbl
1 (h,1, (d, l) ∪ filbl(c, 0, d, l)
L1 (h, c) = arg max &apos; (w · flbl
uni(h, c, l))
flbl
bi (h, c, l) = flbl
2 (h, c, l)
L2(h, c) = arg maxK�
l�L1(h,c)(w · {flbl
uni ∪ flbl
bi })
For each arc, we firstly use unigram features to
choose the K1-best labels. The second parameter of
flbl
1 (·) indicates whether the node is the head of the
</equation>
<bodyText confidence="0.9952264">
arc, and the third parameter indicates the direction.
L denotes the whole label set. Then we re-rank the
labels by combining the bigram features, and choose
K2-best labels. During decoding, we only use the
K2 labels chosen for each arc (K2 ¿ K1 &lt; |L|).
</bodyText>
<subsectionHeader confidence="0.995695">
2.2 High-order Model and Algorithm
</subsectionHeader>
<bodyText confidence="0.999927">
Following the Eisner (2000) algorithm, we use spans
as the basic unit. A span is defined as a substring
of the input sentence whose sub-tree is already pro-
duced. Only the start or end words of a span can link
with other spans. In this way, the algorithm parses
the left and the right dependence of a word indepen-
dently, and combines them in the later stage.
We follow McDonald (2006)’s implementation of
first-order Eisner parsing algorithm by modifying its
scoring method to incorporate high-order features.
Our extended algorithm is shown in Algorithm 1.
There are four different span-combining opera-
tions. Here we explain two of them that correspond
to right-arc (s &lt; t), as shown in Figure 1 and 2. We
</bodyText>
<page confidence="0.995024">
49
</page>
<note confidence="0.843554">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49–54,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<construct confidence="0.338728">
Algorithm 1 High-order Eisner Parsing Algorithm
</construct>
<listItem confidence="0.9552902">
1: C[s][s][c] = 0, 0 &lt; s &lt; N, c E cp, icp # cp: complete; icp: incomplete
2: for j = 1 to N do
3: for s = 0 to N do
4: t=s+jL
5: if t &gt; N then
6: break
7: end if
# Create incomplete spans
8: C[s][t][icp] = maxs&lt;r&lt;t;lEL2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))
9: C[t][s][icp] = maxs&lt;r&lt;t;lEL2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))
# Create complete spans
10: C[s][t][cp] = maxs&lt;r&lt;t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))
11: C[t][s][cp] = maxs&lt;r&lt;t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))
12: end for
13: end for
</listItem>
<bodyText confidence="0.535517666666667">
follow the way of (McDonald, 2006) and (Carreras,
2007) to represent spans. The other two operations
corresponding to left-arc are similar.
</bodyText>
<figureCaption confidence="0.794153333333333">
Figure 1: Combining two spans into an incomplete span
Figure 1 illustrates line 8 of the algorithm in Al-
gorithm 1, which combines two complete spans into
</figureCaption>
<bodyText confidence="0.999491272727273">
an incomplete span. A complete span means that
only the head word can link with other words fur-
ther, noted as “—*” or “+—”. An incomplete span
indicates that both the start and end words of the
span will link with other spans in the future, noted as
“--;” or “F--”. In this operation, we combine two
smaller spans, sps→r and spr+1←t, into sps99Kt with
adding arcs→t. As shown in the following formu-
las, the score of sps99Kt is composed of three parts:
the score of sps→r, the score of spr+1←t, and the
score of adding arcs→t. The score of arcs→t is
determined by four different feature sets: unigram
features, bigram features, sibling features and left
grandchildren features (or inside grandchildren fea-
tures, meaning that the grandchildren lie between s
and t). Note that the sibling features are only related
to the nearest sibling node of t, which is denoted as
sck here. And the inside grandchildren features are
related to all the children of t. This is different from
the models used by Carreras (2007) and Johansson
and Nugues (2008). They only used the left-most
child of t, which is tck, here.
</bodyText>
<equation confidence="0.9941154">
ficp(s, r, t, l) = funi(sf, t, l) U fbi(s, t, l)
U fsib(s, sck, t) U lUi=1 fgrand(s, t, tci, l)1
Sicp(s, r, t, l) = w · ficp(s, r, t, l)
S(sps99Kt) = S(sps→r) + S(spr+1←t)
+ Sicp(s, r, t, l)
</equation>
<bodyText confidence="0.999747285714286">
In Figure 2 we combine sps99Kr and spr→t into
sps→t, which explains line 10 in Algorithm 1. The
score of sps→t also includes three parts, as shown
in the following formulas. Although there is no new
arc added in this operation, the third part is neces-
sary because it reflects the right (or called outside)
grandchildren information of arcs→r.
</bodyText>
<figureCaption confidence="0.960754">
Figure 2: Combining two spans into a complete span
</figureCaption>
<equation confidence="0.8967175">
fcp(s, r, t, l) = Uki=1 fgrand(s, r, rci, l)
Scp(s, r, t, l) = w · fcp(s, r, t, l)
S(sps→t) = S(sps99Kr)
+ S(spr→t) + Scp(s, r, t, l)
</equation>
<page confidence="0.952797">
50
</page>
<subsectionHeader confidence="0.959543">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.9999915625">
As shown above, features used in our model can be
decomposed into four parts: unigram features, bi-
gram features, sibling features, and grandchildren
features. Each part can be seen as two different sets:
arc-related and label-related features, except sibling
features, because we do not consider labels when us-
ing sibling features. Arc-related features can be un-
derstood as back-off of label-related features. Actu-
ally, label-related features are gained by simply at-
taching the label to the arc-features.
The unigram and bigram features used in our
model are similar to those of (Che et al., 2008), ex-
cept that we use bigram label-related features. The
sibling features we use are similar to those of (Mc-
Donald, 2006), and the grandchildren features are
similar to those of (Carreras, 2007).
</bodyText>
<sectionHeader confidence="0.99399" genericHeader="method">
3 Predicate Classification
</sectionHeader>
<bodyText confidence="0.999944333333333">
The predicate classification is regarded as a super-
vised word sense disambiguation (WSD) task here.
The task is divided into four steps:
</bodyText>
<listItem confidence="0.978699352941176">
1. Target words selection: predicates with multi-
ple senses appearing in the training data are selected
as target words.
2. Feature extraction: features in the context
around these target words are extracted as shown in
Table 4. The detailed explanation about these fea-
tures can be found from (Che et al., 2008).
3. Classification: for each target word, a Support
Vector Machine (SVM) classifier is used to classify
its sense. As reported by Lee and Ng (2002) and
Guo et al. (2007), SVM shows good performance on
the WSD task. Here libsvm (Chang and Lin, 2001)
is used. The linear kernel function is used and the
trade off parameter C is 1.
4. Post processing: for each predicate in the test
data which does not appear in the training data, its
first sense in the frame files is used.
</listItem>
<sectionHeader confidence="0.988049" genericHeader="method">
4 Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.999433">
The semantic role labeling (SRL) can be divided
into two separate stages: semantic role classification
(SRC) and post inference (PI).
During the SRC stage, a Maximum en-
tropy (Berger et al., 1996) classifier is used to
predict the probabilities of a word in the sentence
</bodyText>
<tableCaption confidence="0.979826">
Table 1: No-duplicated-roles for different languages
</tableCaption>
<bodyText confidence="0.998922911764706">
to be each semantic role. We add a virtual role
“NULL” (presenting none of roles is assigned)
to the roles set, so we do not need semantic role
identification stage anymore. For a predicate
of each language, two classifiers (one for noun
predicates, and the other for verb predicates) predict
probabilities of each word in a sentence to be each
semantic role (including virtual role “NULL”). The
features used in this stage are listed in Table 4.
The probability of each word to be a semantic role
for a predicate is given by the SRC stage. The re-
sults generated by selecting the roles with the largest
probabilities, however, do not satisfy some con-
strains. As we did in the last year’s system (Che et
al., 2008), we use the ILP (Integer Linear Program-
ming) (Punyakanok et al., 2004) to get the global op-
timization, which is satisfied with three constrains:
C1: Each word should be labeled with one and
only one label (including the virtual label “NULL”).
C2: Roles with a small probability should never
be labeled (except for the virtual role “NULL”). The
threshold we use in our system is 0.3.
C3: Statistics show that some roles (except for
the virtual role “NULL”) usually appear once for
a predicate. We impose a no-duplicate-roles con-
straint with a no-duplicate-roles list, which is con-
structed according to the times of semantic roles’
duplication for each single predicate. Table 1 shows
the no-duplicate-roles for different languages.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit1. The
classifier parameters are tuned with the development
data for different languages respectively. lp solve
5.52 is chosen as our ILP problem solver.
</bodyText>
<footnote confidence="0.999876">
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
2http://sourceforge.net/projects/lpsolve
</footnote>
<table confidence="0.707700363636364">
Language
No-duplicated-roles
arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc
A0, A1, A2, A3, A4, A5,
ACT, ADDR, CRIT, LOC, PAT, DIR3, COND
A0, A1, A2, A3, A4, A5,
A0, A1, A2, A3, A4, A5,
DE, GA, TMP, WO
arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,
arg2-loc, arg2-null, arg4-des, argL-null, argM-
cau, argM-ext, argM-fin
</table>
<figure confidence="0.938994428571428">
Catalan
Chinese
Czech
English
German
Japanese
Spanish
</figure>
<page confidence="0.994336">
51
</page>
<sectionHeader confidence="0.999109" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995371">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999989076923077">
We participate in the CoNLL 2009 shared task
with all 7 languages: Catalan (Taul´e et al., 2008),
Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et
al., 2006), English (Surdeanu et al., 2008), Ger-
man (Burchardt et al., 2006), Japanese (Kawahara
et al., 2002), and Spanish (Taul´e et al., 2008). Be-
sides the closed challenge, we also submitted the
open challenge results. Our open challenge strategy
is very simple. We add the SRL development data
of each language into their training data. The pur-
pose is to examine the effect of the additional data,
especially for out-of-domain (ood) data.
Three machines (with 2.5GHz Xeon CPU and
16G memory) were used to train our models. Dur-
ing the peak time, Amazon’s EC2 (Elastic Com-
pute Cloud)3 was used, too. Our system requires
15G memory at most and the longest training time
is about 36 hours.
During training the predicate classification (PC)
and the semantic role labeling (SRL) models, golden
syntactic dependency parsing results are used. Pre-
vious experiments show that the PC and SRL test re-
sults based on golden parse trees are slightly worse
than that based on cross trained parse trees. It is,
however, a pity that we have no enough time and ma-
chines to do cross training for so many languages.
</bodyText>
<subsectionHeader confidence="0.911443">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9999836">
In order to examine the performance of the ILP
based post inference (PI) for different languages, we
adopt a simple PI strategy as baseline, which se-
lects the most likely label (including the virtual la-
bel “NULL”) except for those duplicate non-virtual
labels with lower probabilities (lower than 0.5). Ta-
ble 2 shows their performance on development data.
We can see that the ILP based post inference can
improve the precision but decrease the recall. Ex-
cept for Czech, almost all languages are improved.
Among them, English benefits most.
The final system results are shown in Table 3.
Comparing with our CoNLL 2008 (Che et al., 2008)
syntactic parsing results on English4, we can see that
our new high-order model improves about 1%.
</bodyText>
<footnote confidence="0.971452">
3http://aws.amazon.com/ec2/
4devel: 85.94%, test: 87.51% and ood: 80.73%
</footnote>
<table confidence="0.999990933333333">
Precision Recall F1
Catalan simple 78.68 77.14 77.90
Catalan ILP 79.42 76.49 77.93
Chinese simple 80.74 74.36 77.42
Chinese ILP 81.97 73.92 77.74
Czech simple 88.54 84.68 86.57
Czech ILP 89.23 84.05 86.56
English simple 83.03 83.55 83.29
English ILP 85.63 83.03 84.31
German simple 78.88 75.87 77.34
German ILP 82.04 74.10 77.87
Japanese simple 88.04 70.68 78.41
Japanese ILP 89.23 70.16 78.56
Spanish simple 76.73 75.92 76.33
Spanish ILP 77.71 75.34 76.51
</table>
<tableCaption confidence="0.999915">
Table 2: Comparison between different PI strategies
</tableCaption>
<bodyText confidence="0.999922777777778">
For the open challenge, because we did not mod-
ify the syntactic training data, its results are the same
as the closed ones. We can, therefore, examine the
effect of the additional training data on SRL. We can
see that along with the development data are added
into the training data, the performance on the in-
domain test data is increased. However, it is inter-
esting that the additional data is harmful to the ood
test.
</bodyText>
<sectionHeader confidence="0.991063" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999051">
Our CoNLL 2009 Shared Task system is com-
posed of three cascaded components. The pseudo-
projective high-order syntactic dependency model
outperforms our CoNLL 2008 model (in English).
The additional in-domain (devel) SRL data can help
the in-domain test. However, it is harmful to the ood
test. Our final system achieves promising results. In
the future, we will study how to solve the domain
adaptive problem and how to do joint learning be-
tween syntactic and semantic parsing.
</bodyText>
<sectionHeader confidence="0.99172" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9963614">
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60675034, and the “863” National High-
Tech Research and Development of China via grant
2008AA01Z144.
</bodyText>
<page confidence="0.994405">
52
</page>
<table confidence="0.999815666666667">
Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score
devel test ood devel test ood devel test ood
Catalan closed 86.65 86.56 —– 77.93 77.10 —– 82.30 81.84 —–
open —– —– 77.36 —– 81.97
Chinese closed 75.73 75.49 —– 77.74 77.15 —– 76.79 76.38 —–
open —– —– 77.23 —– 76.42
Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66
open —– —– 86.57 85.21 —– 83.31 80.63
English closed 87.09 88 .48 81.57 84.30 85.51 73.82 85.70 87.00 77.71
open —– —– 85.61 73.66 —– 87.05 77.63
German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19
open —– —– 78.61 70.09 —– 82.44 73.20
Japanese closed 92.55 92 &apos;57 _ 78.56 78.26 —– 85.86 85.65 —–
open —– —– 78.35 —– 85.70
Spanish closed 87.22 87 .33 - 76.51 76.47 —– 81.87 81.90 —–
open —– —– 76.66 —– 82.00
Average closed —– 85.23 77.90 —– 79.94 76.38 —– 82.64 77.19
open 80.06 76.32 82.70 77.15
</table>
<tableCaption confidence="0.99803">
Table 3: Final system results
</tableCaption>
<sectionHeader confidence="0.991418" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998581">
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In LREC-2006.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP/CoNLL-
2007.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded
syntactic and semantic dependency parsing system. In
CoNLL-2008.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies.
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. HIT-IR-WSD: A wsd system for
english lexical sample task. In SemEval-2007.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka, Marie
Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP-2008.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In LREC-2002.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP-
2002.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In ACL-2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1).
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Coling-2004.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Mariona Taul´e, Maria Ant`onia Martf, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In LREC-2008.
</reference>
<page confidence="0.99606">
53
</page>
<table confidence="0.946492764705882">
Catalan Chinese Czech English German Japanese Spanish
ChildrenPOS ♦ 0 ♦0
ChildrenPOSNoDup ♦ 0 ♦ 0
ConstituentPOSPattern ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
ConstituentPOSPattern+DepRelation ♦ 0 ♦ 0 ♦ 0
ConstituentPOSPattern+DepwordLemma ♦ 0 ♦ 0 ♦ 0
ConstituentPOSPattern+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
DepRelation ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0
DepRelation+DepwordLemma ♦ 0 ♦ 0
DepRelation+Headword ♦ A ♦ A ♦ ♦ A ♦ A ♦
DepRelation+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0
DepRelation+HeadwordLemma+DepwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
DepRelation+HeadwordPOS ♦ A ♦ A ♦ A ♦ A ♦ A ♦
Depword ♦ 0 ♦ 0
DepwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
DepwordLemma+HeadwordLemma ♦ 0 ♦ 0 ♦ 0
DepwordLemma+RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
</table>
<figure confidence="0.850025596491228">
DepwordPOS ♦ A ♦ A ♦ A ♦ 0 ♦ A ♦ A ♦ 0 ♦ A
DepwordPOS+HeadwordPOS ♦ 0 ♦ 0
DownPathLength ♦ 0 ♦ 0
FirstLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
FirstPOS ♦ 0 ♦ 0
FirstPOS+DepwordPOS ♦ 0 ♦ 0 ♦ 0
FirstWord ♦ 0 ♦ 0
Headword ♦ A ♦ A ♦ A ♦ A ♦ A ♦ 0 ♦
HeadwordLemma ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ 0 ♦
HeadwordLemma+RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
HeadwordPOS ♦ A ♦ A ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A
LastLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
LastPOS ♦ 0 ♦ 0
LastWord ♦ 0
Path ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
Path+RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
PathLength ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
PFEAT ♦ A ♦ A ♦ A
PFEATSplit ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0
PFEATSplitRemoveNULL ♦ A ♦ A ♦ A
PositionWithPredicate ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
Predicate ♦ A ♦ 0 ♦ A ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ 0
Predicate+PredicateFamilyship ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
PredicateBagOfPOSNumbered A ♦ A ♦ A ♦ A
PredicateBagOfPOSNumberedWindow5 ♦ A ♦ A ♦ A ♦ A ♦ A
PredicateBagOfPOSOrdered ♦ A ♦ A ♦ A ♦ A ♦
PredicateBagOfPOSOrderedWindow5 ♦ A ♦ A ♦ A ♦ A ♦ A ♦ A
PredicateBagOfPOSWindow5 ♦ ♦ A ♦ A ♦ A ♦ A ♦
PredicateBagOfWords A ♦ A ♦ ♦ A ♦ A
PredicateBagOfWordsAndIsDesOfPRED ♦ A ♦ A A ♦ A ♦ A
PredicateBagOfWordsOrdered A ♦ A ♦ A A ♦ A ♦ A
PredicateChildrenPOS ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ A ♦ 0
PredicateChildrenPOSNoDup ♦ A ♦ A ♦ A ♦ A ♦ A ♦ A
PredicateChildrenREL ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ 0 ♦ A
PredicateChildrenRELNoDup ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ 0 ♦ A
PredicateFamilyship ♦ 0
PredicateLemma ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ 0 ♦ A ♦ 0
PredicateLemma+PredicateFamilyship ♦ 0 ♦ 0 ♦ 0
PredicateSense ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
PredicateSense+DepRelation ♦ 0 ♦ 0
PredicateSense+DepwordLemma ♦ 0 ♦ 0
PredicateSense+DepwordPOS ♦ 0 ♦ 0
PredicateSiblingsPOS ♦ A ♦ A ♦ ♦ A ♦ A ♦ A
PredicateSiblingsPOSNoDup ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ A ♦ 0
PredicateSiblingsREL ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ A
PredicateSiblingsRELNoDup ♦ A ♦ A ♦ 0 A ♦ A ♦ A ♦ 0 ♦ A ♦ 0
PredicateVoiceEn ♦ A
PredicateWindow5Bigram ♦ A ♦ A ♦ A ♦ A
PredicateWindow5BigramPOS ♦ A ♦ A ♦ A ♦ A ♦ A ♦ A
RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0
SiblingsPOS ♦ 0 ♦ 0
SiblingsREL ♦
SiblingsRELNoDup ♦ 0 ♦ 0
UpPath ♦ 0 ♦ 0 ♦ 0 ♦
UpPathLength 54 ♦ 0
UpRelationPath ♦ 0 ♦ 0 ♦ 0
UpRelationPath+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0
</figure>
<tableCaption confidence="0.96836">
Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). ♦: noun predicate
</tableCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999787">Multilingual Dependency-based Syntactic and Semantic Parsing</title>
<author confidence="0.874507">Wanxiang Che</author>
<author confidence="0.874507">Zhenghua Li</author>
<author confidence="0.874507">Yongqiang Li</author>
<author confidence="0.874507">Yuhang Guo</author>
<author confidence="0.874507">Bing Qin</author>
<author confidence="0.874507">Ting</author>
<affiliation confidence="0.959755333333333">Information Retrieval School of Computer Science and Harbin Institute of Technology, China,</affiliation>
<email confidence="0.982888">lzh,yqli,yhguo,qinb,</email>
<abstract confidence="0.990013557077626">Our CoNLL 2009 Shared Task system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. A pseudo-projective high-order graph-based model is used in our syntactic dependency parser. A support vector machine (SVM) model is used to classify predicate senses. Semantic role labeling is achieved using maximum entropy (MaxEnt) model based semantic role classification and integer linear programming (ILP) based post inference. Finally, we win the first place in the joint task, including both the closed and open challenges. 1 System Architecture Our CoNLL 2009 Shared Task (Hajiˇc et al., 2009): multilingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (Mc- Donald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 2.1 Syntactic Label Determining The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS). On the other hand, keeping all possible labels for each arc made the decoding inefficient. Therefore, in the system of this year, we adopt approximate techniques to compromise, as shown in the following formulas. c, = d, = arg max c, c, = c, = arg For each arc, we firstly use unigram features to the labels. The second parameter of whether the node is the head of the arc, and the third parameter indicates the direction. the whole label set. Then we re-rank the labels by combining the bigram features, and choose labels. During decoding, we only use the chosen for each arc 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A span is defined as a substring of the input sentence whose sub-tree is already produced. Only the start or end words of a span can link with other spans. In this way, the algorithm parses the left and the right dependence of a word independently, and combines them in the later stage. We follow McDonald (2006)’s implementation of first-order Eisner parsing algorithm by modifying its scoring method to incorporate high-order features. Our extended algorithm is shown in Algorithm 1. There are four different span-combining operations. Here we explain two of them that correspond right-arc &lt; as shown in Figure 1 and 2. We 49 of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared pages 49–54, Colorado, June 2009. Association for Computational Linguistics 1 Eisner Parsing Algorithm 1: = icp complete; incomplete for 1 for 0 4: if &gt; N 6: break 7: end if 8: = + + r, t, 9: = + + r, s, 10: = + + r, t, 11: = + + r, s, 12: end for 13: end for follow the way of (McDonald, 2006) and (Carreras, 2007) to represent spans. The other two operations corresponding to left-arc are similar. Figure 1: Combining two spans into an incomplete span Figure 1 illustrates line 8 of the algorithm in Algorithm 1, which combines two complete spans into an incomplete span. A complete span means that only the head word can link with other words furnoted as or An incomplete span indicates that both the start and end words of the span will link with other spans in the future, noted as or In this operation, we combine two spans, As shown in the following formuthe score of composed of three parts: score of the score of and the of adding The score of determined by four different feature sets: unigram features, bigram features, sibling features and left grandchildren features (or inside grandchildren feameaning that the grandchildren lie between Note that the sibling features are only related the nearest sibling node of which is denoted as And the inside grandchildren features are to all the children of This is different from the models used by Carreras (2007) and Johansson and Nugues (2008). They only used the left-most of t, which is r, t, = t, t, t, r, t, = r, t, = + r, t, Figure 2 we combine which explains line 10 in Algorithm 1. The of includes three parts, as shown in the following formulas. Although there is no new arc added in this operation, the third part is necessary because it reflects the right (or called outside) information of Figure 2: Combining two spans into a complete span r, t, = r, r, t, = r, t, = + r, t, 50 2.3 Features As shown above, features used in our model can be decomposed into four parts: unigram features, bigram features, sibling features, and grandchildren features. Each part can be seen as two different sets: arc-related and label-related features, except sibling features, because we do not consider labels when using sibling features. Arc-related features can be understood as back-off of label-related features. Actually, label-related features are gained by simply attaching the label to the arc-features. The unigram and bigram features used in our model are similar to those of (Che et al., 2008), except that we use bigram label-related features. The sibling features we use are similar to those of (Mc- Donald, 2006), and the grandchildren features are similar to those of (Carreras, 2007). 3 Predicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the off parameter 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence Table 1: No-duplicated-roles for different languages to be each semantic role. We add a virtual role “NULL” (presenting none of roles is assigned) to the roles set, so we do not need semantic role identification stage anymore. For a of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles’ duplication for each single predicate. Table 1 shows the no-duplicate-roles for different languages. Our maximum entropy classifier is implemented Maximum Entropy Modeling The classifier parameters are tuned with the development data for different languages respectively. lp solve is chosen as our ILP problem solver.</abstract>
<email confidence="0.349141">toolkit.html</email>
<note confidence="0.637747041666667">Language No-duplicated-roles arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc A0, A1, A2, A3, A4, A5, ACT, ADDR, CRIT, LOC, PAT, DIR3, COND A0, A1, A2, A3, A4, A5, A0, A1, A2, A3, A4, A5, DE, GA, TMP, WO arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr, arg2-loc, arg2-null, arg4-des, argL-null, argMcau, argM-ext, argM-fin Catalan Chinese Czech English German Japanese Spanish 51 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et</note>
<abstract confidence="0.963773820512821">al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Comwas used, too. Our system requires 15G memory at most and the longest training time is about 36 hours. During training the predicate classification (PC) and the semantic role labeling (SRL) models, golden syntactic dependency parsing results are used. Previous experiments show that the PC and SRL test results based on golden parse trees are slightly worse than that based on cross trained parse trees. It is, however, a pity that we have no enough time and machines to do cross training for so many languages. 5.2 Results and Discussion In order to examine the performance of the ILP based post inference (PI) for different languages, we adopt a simple PI strategy as baseline, which selects the most likely label (including the virtual label “NULL”) except for those duplicate non-virtual labels with lower probabilities (lower than 0.5). Table 2 shows their performance on development data. We can see that the ILP based post inference can improve the precision but decrease the recall. Except for Czech, almost all languages are improved. Among them, English benefits most. The final system results are shown in Table 3. Comparing with our CoNLL 2008 (Che et al., 2008) parsing results on we can see that our new high-order model improves about 1%.</abstract>
<note confidence="0.947641705882353">85.94%, test: 87.51% and ood: 80.73% Precision Recall F1 Catalan simple 78.68 77.14 77.90 Catalan ILP 79.42 76.49 77.93 Chinese simple 80.74 74.36 77.42 Chinese ILP 81.97 73.92 77.74 Czech simple 88.54 84.68 86.57 Czech ILP 89.23 84.05 86.56 English simple 83.03 83.55 83.29 English ILP 85.63 83.03 84.31 German simple 78.88 75.87 77.34 German ILP 82.04 74.10 77.87 Japanese simple 88.04 70.68 78.41 Japanese ILP 89.23 70.16 78.56 Spanish simple 76.73 75.92 76.33 Spanish ILP 77.71 75.34 76.51 Table 2: Comparison between different PI strategies</note>
<abstract confidence="0.9949615">For the open challenge, because we did not modify the syntactic training data, its results are the same as the closed ones. We can, therefore, examine the effect of the additional training data on SRL. We can see that along with the development data are added into the training data, the performance on the indomain test data is increased. However, it is interesting that the additional data is harmful to the ood test. 6 Conclusion and Future Work Our CoNLL 2009 Shared Task system is composed of three cascaded components. The pseudoprojective high-order syntactic dependency model outperforms our CoNLL 2008 model (in English). The additional in-domain (devel) SRL data can help the in-domain test. However, it is harmful to the ood test. Our final system achieves promising results. In the future, we will study how to solve the domain adaptive problem and how to do joint learning between syntactic and semantic parsing.</abstract>
<note confidence="0.890852515151515">Acknowledgments This work was supported by National Natural Science Foundation of China (NSFC) via grant 60803093, 60675034, and the “863” National High- Tech Research and Development of China via grant 2008AA01Z144. 52 Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score devel test ood devel test ood devel test ood Catalan closed open 86.65 86.56 —– 77.93 77.10 —– 82.30 81.84 —– —– —– 77.36 —– 81.97 Chinese closed open 75.73 75.49 —– 77.74 77.15 —– 76.79 76.38 —– —– —– 77.23 —– 76.42 Czech closed open 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66 —– —– 86.57 85.21 —– 83.31 80.63 English closed open 87.09 88 .48 81.57 84.30 85.51 73.82 85.70 87.00 77.71 —– —– 85.61 73.66 —– 87.05 77.63 German closed open 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19 —– —– 78.61 70.09 —– 82.44 73.20 Japanese closed open 92.55 92 &apos;57 _ 78.56 78.26 —– 85.86 85.65 —– —– —– 78.35 —– 85.70 Spanish closed open 87.22 87 .33 - 76.51 76.47 —– 81.87 81.90 —– —– —– 76.66 —– 82.00 Average closed open —– 85.23 77.90 —– 79.94 76.38 —– 82.64 77.19 80.06 76.32 82.70 77.15 Table 3: Final system results References Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach natural language processing. Lin- 22. Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.</note>
<title confidence="0.60457">The SALSA corpus: a German corpus resource for semantics. In</title>
<author confidence="0.605318">Experiments with a higher-order</author>
<abstract confidence="0.7588965">dependency parser. In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded syntactic and semantic dependency parsing system. In Jason Eisner. 2000. Bilexical grammars and their cubicparsing algorithms. In in Probabilistic</abstract>
<title confidence="0.871266">Other Parsing</title>
<author confidence="0.904343">HIT-IR-WSD A wsd system for lexical sample task In Jan Hajiˇc</author>
<author confidence="0.904343">Jarmila Panevov´a</author>
<author confidence="0.904343">Eva Hajiˇcov´a</author>
<author confidence="0.904343">Petr Petr Pajas</author>
<author confidence="0.904343">Jan JiˇrfHavelka</author>
<author confidence="0.904343">Marie</author>
<note confidence="0.3074625">and Zdenˇek 2006. Prague Dependency Treebank 2.0.</note>
<author confidence="0.954992">Jan Hajiˇc</author>
<author confidence="0.954992">Massimiliano Ciaramita</author>
<author confidence="0.954992">Richard Johans-</author>
<affiliation confidence="0.291157">son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs</affiliation>
<address confidence="0.353368">M`arquez, Adam Meyers, Joakim Nivre, Sebastian</address>
<author confidence="0.943306">Jan Pavel Straˇn´ak</author>
<author confidence="0.943306">Mihai Surdeanu</author>
<note confidence="0.815531492307692">Nianwen Xue, and Yi Zhang. 2009. The CoNLL- 2009 shared task: Syntactic and semantic dependenin multiple languages. In Richard Johansson and Pierre Nugues. Dependency-based semantic role labeling of Prop- In Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged In Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning alfor word sense disambiguation. In EMNLP- McDonald. 2006. Learning and Tree Algorithms for Dependency Ph.D. thesis, University of Pennsylvania. Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective parsing. In Martha Palmer and Nianwen Xue. 2009. Adding semanroles to the Chinese Treebank. Language 15(1). Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear inference. In Mihai Surdeanu, Richard Johansson, Adam Meyers, Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL- 2008 shared task on joint parsing of syntactic and sedependencies. In Mariona Taul´e, Maria Ant`onia Martf, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora Catalan and Spanish. In 53 Catalan Chinese Czech English German Japanese Spanish ChildrenPOS ♦ 0 ♦0 ChildrenPOSNoDup ♦ 0 ♦ 0 ConstituentPOSPattern ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ConstituentPOSPattern+DepRelation ♦ 0 ♦ 0 ♦ 0 ConstituentPOSPattern+DepwordLemma ♦ 0 ♦ 0 ♦ 0 ConstituentPOSPattern+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 DepRelation ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 DepRelation+DepwordLemma ♦ 0 ♦ 0 DepRelation+Headword ♦ A ♦ A ♦ ♦ A ♦ A ♦ DepRelation+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 DepRelation+HeadwordLemma+DepwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 DepRelation+HeadwordPOS ♦ A ♦ A ♦ A ♦ A ♦ A ♦ Depword ♦ 0 ♦ 0 DepwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 DepwordLemma+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 DepwordLemma+RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 DepwordPOS ♦ A ♦ A ♦ A ♦ 0 ♦ A ♦ A ♦ 0 ♦ A DepwordPOS+HeadwordPOS ♦ 0 ♦ 0 DownPathLength ♦ 0 ♦ 0 FirstLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 FirstPOS ♦ 0 ♦ 0 FirstPOS+DepwordPOS ♦ 0 ♦ 0 ♦ 0 FirstWord ♦ 0 ♦ 0 Headword ♦ A ♦ A ♦ A ♦ A ♦ A ♦ 0 ♦ HeadwordLemma ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ 0 ♦ HeadwordLemma+RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 HeadwordPOS ♦ A ♦ A ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A LastLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 LastPOS ♦ 0 ♦ 0 LastWord ♦ 0 Path ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 Path+RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 PathLength ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0</note>
<title confidence="0.722253611111111">PFEAT ♦ A ♦ A ♦ A PFEATSplit ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 PFEATSplitRemoveNULL ♦ A ♦ A ♦ A PositionWithPredicate ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 Predicate ♦ A ♦ 0 ♦ A ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ 0 Predicate+PredicateFamilyship ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 PredicateBagOfPOSNumbered A ♦ A ♦ A ♦ A PredicateBagOfPOSNumberedWindow5 ♦ A ♦ A ♦ A ♦ A ♦ A PredicateBagOfPOSOrdered ♦ A ♦ A ♦ A ♦ A ♦ PredicateBagOfPOSOrderedWindow5 ♦ A ♦ A ♦ A ♦ A ♦ A ♦ A PredicateBagOfPOSWindow5 ♦ ♦ A ♦ A ♦ A ♦ A ♦ PredicateBagOfWords A ♦ A ♦ ♦ A ♦ A PredicateBagOfWordsAndIsDesOfPRED ♦ A ♦ A A ♦ A ♦ A PredicateBagOfWordsOrdered A ♦ A ♦ A A ♦ A ♦ A PredicateChildrenPOS ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ A ♦ 0 PredicateChildrenPOSNoDup ♦ A ♦ A ♦ A ♦ A ♦ A ♦ A PredicateChildrenREL ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ 0 ♦ A PredicateChildrenRELNoDup ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ 0 ♦ A</title>
<note confidence="0.714135739130435">PredicateFamilyship ♦ 0 PredicateLemma ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ A ♦ 0 ♦ 0 ♦ A ♦ 0 PredicateLemma+PredicateFamilyship ♦ 0 ♦ 0 ♦ 0 PredicateSense ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 PredicateSense+DepRelation ♦ 0 ♦ 0 PredicateSense+DepwordLemma ♦ 0 ♦ 0 PredicateSense+DepwordPOS ♦ 0 ♦ 0 PredicateSiblingsPOS ♦ A ♦ A ♦ ♦ A ♦ A ♦ A PredicateSiblingsPOSNoDup ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ A ♦ 0 PredicateSiblingsREL ♦ A ♦ 0 ♦ A ♦ A ♦ A ♦ A ♦ A PredicateSiblingsRELNoDup ♦ A ♦ A ♦ 0 A ♦ A ♦ A ♦ 0 ♦ A ♦ 0 PredicateVoiceEn ♦ A PredicateWindow5Bigram ♦ A ♦ A ♦ A ♦ A PredicateWindow5BigramPOS ♦ A ♦ A ♦ A ♦ A ♦ A ♦ A RelationPath ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 ♦ 0 SiblingsPOS ♦ 0 ♦ 0 SiblingsREL ♦ SiblingsRELNoDup ♦ 0 ♦ 0 UpPath ♦ 0 54 ♦ 0 ♦ 0 ♦ UpPathLength ♦ 0 UpRelationPath ♦ 0 ♦ 0 ♦ 0 UpRelationPath+HeadwordLemma ♦ 0 ♦ 0 ♦ 0 ♦ 0 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). noun predicate</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="8502" citStr="Berger et al., 1996" startWordPosition="1440" endWordPosition="1443">classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence Table 1: No-duplicated-roles for different languages to be each semantic role. We add a virtual role “NULL” (presenting none of roles is assigned) to the roles set, so we do not need semantic role identification stage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each w</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In LREC-2006.</booktitle>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In LREC-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In EMNLP/CoNLL2007.</booktitle>
<contexts>
<context position="1553" citStr="Carreras (2007)" startWordPosition="228" endWordPosition="229">et al., 2009): multilingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 2.1 Syntactic Label Determining The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS). On the other hand, keeping all possible labels for each arc made the decoding inefficient. Therefore, in the system of this year, we adopt approximate techn</context>
<context position="4464" citStr="Carreras, 2007" startWordPosition="738" endWordPosition="739">icp # cp: complete; icp: incomplete 2: for j = 1 to N do 3: for s = 0 to N do 4: t=s+jL 5: if t &gt; N then 6: break 7: end if # Create incomplete spans 8: C[s][t][icp] = maxs&lt;r&lt;t;lEL2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l)) 9: C[t][s][icp] = maxs&lt;r&lt;t;lEL2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l)) # Create complete spans 10: C[s][t][cp] = maxs&lt;r&lt;t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l)) 11: C[t][s][cp] = maxs&lt;r&lt;t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l)) 12: end for 13: end for follow the way of (McDonald, 2006) and (Carreras, 2007) to represent spans. The other two operations corresponding to left-arc are similar. Figure 1: Combining two spans into an incomplete span Figure 1 illustrates line 8 of the algorithm in Algorithm 1, which combines two complete spans into an incomplete span. A complete span means that only the head word can link with other words further, noted as “—*” or “+—”. An incomplete span indicates that both the start and end words of the span will link with other spans in the future, noted as “--;” or “F--”. In this operation, we combine two smaller spans, sps→r and spr+1←t, into sps99Kt with adding ar</context>
<context position="5712" citStr="Carreras (2007)" startWordPosition="953" endWordPosition="954">ormulas, the score of sps99Kt is composed of three parts: the score of sps→r, the score of spr+1←t, and the score of adding arcs→t. The score of arcs→t is determined by four different feature sets: unigram features, bigram features, sibling features and left grandchildren features (or inside grandchildren features, meaning that the grandchildren lie between s and t). Note that the sibling features are only related to the nearest sibling node of t, which is denoted as sck here. And the inside grandchildren features are related to all the children of t. This is different from the models used by Carreras (2007) and Johansson and Nugues (2008). They only used the left-most child of t, which is tck, here. ficp(s, r, t, l) = funi(sf, t, l) U fbi(s, t, l) U fsib(s, sck, t) U lUi=1 fgrand(s, t, tci, l)1 Sicp(s, r, t, l) = w · ficp(s, r, t, l) S(sps99Kt) = S(sps→r) + S(spr+1←t) + Sicp(s, r, t, l) In Figure 2 we combine sps99Kr and spr→t into sps→t, which explains line 10 in Algorithm 1. The score of sps→t also includes three parts, as shown in the following formulas. Although there is no new arc added in this operation, the third part is necessary because it reflects the right (or called outside) grandchi</context>
<context position="7332" citStr="Carreras, 2007" startWordPosition="1242" endWordPosition="1243"> seen as two different sets: arc-related and label-related features, except sibling features, because we do not consider labels when using sibling features. Arc-related features can be understood as back-off of label-related features. Actually, label-related features are gained by simply attaching the label to the arc-features. The unigram and bigram features used in our model are similar to those of (Che et al., 2008), except that we use bigram label-related features. The sibling features we use are similar to those of (McDonald, 2006), and the grandchildren features are similar to those of (Carreras, 2007). 3 Predicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As repor</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In EMNLP/CoNLL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2001</date>
<contexts>
<context position="8057" citStr="Chang and Lin, 2001" startWordPosition="1361" endWordPosition="1364">uation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence Table 1: No-duplicated-roles for different languages to be each semantic role. W</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yuxuan Hu</author>
<author>Yongqiang Li</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A cascaded syntactic and semantic dependency parsing system.</title>
<date>2008</date>
<booktitle>In CoNLL-2008.</booktitle>
<contexts>
<context position="1210" citStr="Che et al., 2008" startWordPosition="167" endWordPosition="170">ssify predicate senses. Semantic role labeling is achieved using maximum entropy (MaxEnt) model based semantic role classification and integer linear programming (ILP) based post inference. Finally, we win the first place in the joint task, including both the closed and open challenges. 1 System Architecture Our CoNLL 2009 Shared Task (Hajiˇc et al., 2009): multilingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 2.1 Sy</context>
<context position="7139" citStr="Che et al., 2008" startWordPosition="1208" endWordPosition="1211"> l) 50 2.3 Features As shown above, features used in our model can be decomposed into four parts: unigram features, bigram features, sibling features, and grandchildren features. Each part can be seen as two different sets: arc-related and label-related features, except sibling features, because we do not consider labels when using sibling features. Arc-related features can be understood as back-off of label-related features. Actually, label-related features are gained by simply attaching the label to the arc-features. The unigram and bigram features used in our model are similar to those of (Che et al., 2008), except that we use bigram label-related features. The sibling features we use are similar to those of (McDonald, 2006), and the grandchildren features are similar to those of (Carreras, 2007). 3 Predicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed </context>
<context position="9343" citStr="Che et al., 2008" startWordPosition="1586" endWordPosition="1589"> the roles set, so we do not need semantic role identification stage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according t</context>
<context position="12740" citStr="Che et al., 2008" startWordPosition="2127" endWordPosition="2130">e the performance of the ILP based post inference (PI) for different languages, we adopt a simple PI strategy as baseline, which selects the most likely label (including the virtual label “NULL”) except for those duplicate non-virtual labels with lower probabilities (lower than 0.5). Table 2 shows their performance on development data. We can see that the ILP based post inference can improve the precision but decrease the recall. Except for Czech, almost all languages are improved. Among them, English benefits most. The final system results are shown in Table 3. Comparing with our CoNLL 2008 (Che et al., 2008) syntactic parsing results on English4, we can see that our new high-order model improves about 1%. 3http://aws.amazon.com/ec2/ 4devel: 85.94%, test: 87.51% and ood: 80.73% Precision Recall F1 Catalan simple 78.68 77.14 77.90 Catalan ILP 79.42 76.49 77.93 Chinese simple 80.74 74.36 77.42 Chinese ILP 81.97 73.92 77.74 Czech simple 88.54 84.68 86.57 Czech ILP 89.23 84.05 86.56 English simple 83.03 83.55 83.29 English ILP 85.63 83.03 84.31 German simple 78.88 75.87 77.34 German ILP 82.04 74.10 77.87 Japanese simple 88.04 70.68 78.41 Japanese ILP 89.23 70.16 78.56 Spanish simple 76.73 75.92 76.33 </context>
</contexts>
<marker>Che, Li, Hu, Li, Qin, Liu, Li, 2008</marker>
<rawString>Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded syntactic and semantic dependency parsing system. In CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>In Advances in Probabilistic and Other Parsing Technologies.</booktitle>
<contexts>
<context position="2883" citStr="Eisner (2000)" startWordPosition="471" endWordPosition="472">L1 (h, c) = arg max &apos; (w · flbl uni(h, c, l)) flbl bi (h, c, l) = flbl 2 (h, c, l) L2(h, c) = arg maxK� l�L1(h,c)(w · {flbl uni ∪ flbl bi }) For each arc, we firstly use unigram features to choose the K1-best labels. The second parameter of flbl 1 (·) indicates whether the node is the head of the arc, and the third parameter indicates the direction. L denotes the whole label set. Then we re-rank the labels by combining the bigram features, and choose K2-best labels. During decoding, we only use the K2 labels chosen for each arc (K2 ¿ K1 &lt; |L|). 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A span is defined as a substring of the input sentence whose sub-tree is already produced. Only the start or end words of a span can link with other spans. In this way, the algorithm parses the left and the right dependence of a word independently, and combines them in the later stage. We follow McDonald (2006)’s implementation of first-order Eisner parsing algorithm by modifying its scoring method to incorporate high-order features. Our extended algorithm is shown in Algorithm 1. There are four different span-combining operations. Here we explain tw</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. In Advances in Probabilistic and Other Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhang Guo</author>
<author>Wanxiang Che</author>
<author>Yuxuan Hu</author>
<author>Wei Zhang</author>
<author>Ting Liu</author>
</authors>
<title>HIT-IR-WSD: A wsd system for english lexical sample task.</title>
<date>2007</date>
<booktitle>In SemEval-2007.</booktitle>
<contexts>
<context position="7978" citStr="Guo et al. (2007)" startWordPosition="1347" endWordPosition="1350">The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence T</context>
</contexts>
<marker>Guo, Che, Hu, Zhang, Liu, 2007</marker>
<rawString>Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang, and Ting Liu. 2007. HIT-IR-WSD: A wsd system for english lexical sample task. In SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Marie Mikulov´a JiˇrfHavelka</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, JiˇrfHavelka, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In CoNLL-2009.</booktitle>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In CoNLL-2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In EMNLP-2008.</booktitle>
<contexts>
<context position="1585" citStr="Johansson and Nugues (2008)" startWordPosition="231" endWordPosition="235">lingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 2.1 Syntactic Label Determining The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS). On the other hand, keeping all possible labels for each arc made the decoding inefficient. Therefore, in the system of this year, we adopt approximate techniques to compromise, as shown in</context>
<context position="5744" citStr="Johansson and Nugues (2008)" startWordPosition="956" endWordPosition="959">f sps99Kt is composed of three parts: the score of sps→r, the score of spr+1←t, and the score of adding arcs→t. The score of arcs→t is determined by four different feature sets: unigram features, bigram features, sibling features and left grandchildren features (or inside grandchildren features, meaning that the grandchildren lie between s and t). Note that the sibling features are only related to the nearest sibling node of t, which is denoted as sck here. And the inside grandchildren features are related to all the children of t. This is different from the models used by Carreras (2007) and Johansson and Nugues (2008). They only used the left-most child of t, which is tck, here. ficp(s, r, t, l) = funi(sf, t, l) U fbi(s, t, l) U fsib(s, sck, t) U lUi=1 fgrand(s, t, tci, l)1 Sicp(s, r, t, l) = w · ficp(s, r, t, l) S(sps99Kt) = S(sps→r) + S(spr+1←t) + Sicp(s, r, t, l) In Figure 2 we combine sps99Kr and spr→t into sps→t, which explains line 10 in Algorithm 1. The score of sps→t also includes three parts, as shown in the following formulas. Although there is no new arc added in this operation, the third part is necessary because it reflects the right (or called outside) grandchildren information of arcs→r. Fig</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of PropBank. In EMNLP-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In LREC-2002.</booktitle>
<contexts>
<context position="11088" citStr="Kawahara et al., 2002" startWordPosition="1846" endWordPosition="1849">-pat, arg2-atr, arg2-loc A0, A1, A2, A3, A4, A5, ACT, ADDR, CRIT, LOC, PAT, DIR3, COND A0, A1, A2, A3, A4, A5, A0, A1, A2, A3, A4, A5, DE, GA, TMP, WO arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr, arg2-loc, arg2-null, arg4-des, argL-null, argMcau, argM-ext, argM-fin Catalan Chinese Czech English German Japanese Spanish 51 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Compute Cloud)3 was used, too. Our system requires 15G memory at most and the longest training time is about 36 hours. During training the</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In LREC-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In EMNLP2002.</booktitle>
<contexts>
<context position="7956" citStr="Lee and Ng (2002)" startWordPosition="1342" endWordPosition="1345">dicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In EMNLP2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1416" citStr="McDonald, 2006" startWordPosition="205" endWordPosition="207">e first place in the joint task, including both the closed and open challenges. 1 System Architecture Our CoNLL 2009 Shared Task (Hajiˇc et al., 2009): multilingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 2.1 Syntactic Label Determining The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS). On the other hand, </context>
<context position="3239" citStr="McDonald (2006)" startWordPosition="538" endWordPosition="539">denotes the whole label set. Then we re-rank the labels by combining the bigram features, and choose K2-best labels. During decoding, we only use the K2 labels chosen for each arc (K2 ¿ K1 &lt; |L|). 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A span is defined as a substring of the input sentence whose sub-tree is already produced. Only the start or end words of a span can link with other spans. In this way, the algorithm parses the left and the right dependence of a word independently, and combines them in the later stage. We follow McDonald (2006)’s implementation of first-order Eisner parsing algorithm by modifying its scoring method to incorporate high-order features. Our extended algorithm is shown in Algorithm 1. There are four different span-combining operations. Here we explain two of them that correspond to right-arc (s &lt; t), as shown in Figure 1 and 2. We 49 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49–54, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Algorithm 1 High-order Eisner Parsing Algorithm 1: C[s][s][c] = 0, 0 &lt; s &lt; N</context>
<context position="7259" citStr="McDonald, 2006" startWordPosition="1230" endWordPosition="1232"> features, sibling features, and grandchildren features. Each part can be seen as two different sets: arc-related and label-related features, except sibling features, because we do not consider labels when using sibling features. Arc-related features can be understood as back-off of label-related features. Actually, label-related features are gained by simply attaching the label to the arc-features. The unigram and bigram features used in our model are similar to those of (Che et al., 2008), except that we use bigram label-related features. The sibling features we use are similar to those of (McDonald, 2006), and the grandchildren features are similar to those of (Carreras, 2007). 3 Predicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Suppor</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In ACL-2005.</booktitle>
<contexts>
<context position="1727" citStr="Nivre and Nilsson, 2005" startWordPosition="256" endWordPosition="259">role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 2.1 Syntactic Label Determining The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS). On the other hand, keeping all possible labels for each arc made the decoding inefficient. Therefore, in the system of this year, we adopt approximate techniques to compromise, as shown in the following formulas. flbl uni(h, c, l) = flbl 1 (h,1, (d, l) ∪ filbl(c, 0, d, l) L1 (h, c) = arg max &apos; (w · flbl uni(h, c, l)) flbl bi (h,</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="10959" citStr="Palmer and Xue, 2009" startWordPosition="1825" endWordPosition="1828">c.uk/s0450736/maxent toolkit.html 2http://sourceforge.net/projects/lpsolve Language No-duplicated-roles arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc A0, A1, A2, A3, A4, A5, ACT, ADDR, CRIT, LOC, PAT, DIR3, COND A0, A1, A2, A3, A4, A5, A0, A1, A2, A3, A4, A5, DE, GA, TMP, WO arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr, arg2-loc, arg2-null, arg4-des, argL-null, argMcau, argM-ext, argM-fin Catalan Chinese Czech English German Japanese Spanish 51 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Compute C</context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Coling-2004.</booktitle>
<contexts>
<context position="9414" citStr="Punyakanok et al., 2004" startWordPosition="1598" endWordPosition="1601">tage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles’ duplication for each single predicate. T</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. In Coling-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llufs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL-2008.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Martf</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In LREC-2008.</booktitle>
<marker>Taul´e, Martf, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Martf, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In LREC-2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>