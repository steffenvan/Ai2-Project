<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000154">
<title confidence="0.997263">
Improved Translation with Source Syntax Labels
</title>
<author confidence="0.997953">
Hieu Hoang
</author>
<affiliation confidence="0.9989755">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.9809">
h.hoang@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.99478" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961333333334">
We present a new translation model
that include undecorated hierarchical-style
phrase rules, decorated source-syntax
rules, and partially decorated rules.
Results show an increase in translation
performance of up to 0.8% BLEU for
German–English translation when trained
on the news-commentary corpus, using
syntactic annotation from a source lan-
guage parser. We also experimented with
annotation from shallow taggers and found
this increased performance by 0.5% BLEU.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992603125">
Hierarchical decoding is usually described as a
formally syntactic model without linguistic com-
mitments, in contrast with syntactic decoding
which constrains rules and production with lin-
guistically motivated labels. However, the decod-
ing mechanism for both hierarchical and syntactic
systems are identical and the rule extraction are
similar.
Hierarchical and syntax statistical machine
translation have made great progress in the last
few years and can claim to represent the state of
the art in the field. Both use synchronous con-
text free grammar (SCFG) formalism, consisting
of rewrite rules which simultaneously parse the in-
put sentence and generate the output sentence. The
most common algorithm for decoding with SCFG
is currently CKY+ with cube pruning works for
both hierarchical and syntactic systems, as imple-
mented in Hiero (Chiang, 2005), Joshua (Li et al.,
2009), and Moses (Hoang et al., 2009)
Rewrite rules in hierarchical systems have gen-
eral applicability as their non-terminals are undec-
orated, giving hierarchical system broad coverage.
However, rules may be used in inappropriate sit-
uations without the labeled constraints. The gen-
eral applicability of undecorated rules create spu-
rious ambiguity which decreases translation per-
formance by causing the decoder to spend more
time sifting through duplicate hypotheses. Syntac-
tic systems makes use of linguistically motivated
information to bias the search space at the expense
of limiting model coverage.
</bodyText>
<author confidence="0.689023">
Philipp Koehn
</author>
<affiliation confidence="0.905402">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.886741">
pkoehn@inf.ed.ac.uk
</email>
<bodyText confidence="0.999980666666667">
This paper presents work on combining hier-
archical and syntax translation, utilizing the high
coverage of hierarchical decoding and the in-
sights that syntactic information can bring. We
seek to balance the generality of using undeco-
rated non-terminals with the specificity of labeled
non-terminals. Specifically, we will use syntac-
tic labels from a source language parser to label
non-terminal in production rules. However, other
source span information, such as chunk tags, can
also be used.
We investigate two methods for combining the
hierarchical and syntactic approach. In the first
method, syntactic translation rules are used con-
currently with a hierarchical phrase rules. Each
ruleset is trained independently and used concur-
rently to decode sentences. However, results for
this method do not improve.
The second method uses one translation model
containing both hierarchical and syntactic rules.
Moreover, an individual rule can contain both
decorated syntactic non-terminals, and undeco-
rated hierarchical-style non-terminals (also, the
left-hand-side non-terminal may, or may not be
decorated). This results in a 0.8% improvement
over the hierarchical baseline and analysis suggest
that long-range ordering has been improved.
We then applied the same methods but using
linguistic annotation from a chunk tagger (Abney,
1991) instead of a parser and obtained an improve-
ment of 0.5% BLEU over the hierarchical base-
line, showing that gains with additional source-
side annotation can be obtained with simpler tools.
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="introduction">
2 Past Work
</sectionHeader>
<bodyText confidence="0.999604">
Hierarchical machine translation (Chiang, 2005)
extends the phrase-based model by allowing the
use of non-contiguous phrase pairs (’production
rules’). It promises better re-ordering of transla-
tion as the reordering rules are an implicit part of
the translation model. Also, hierarchical rules fol-
low the recursive structure of the sentence, reflect-
ing the linguistic notion of language.
However, the hierarchical model has several
limitations. The model makes no use of linguis-
tic information, thus creating a simple model with
broad coverage. However, (Chiang, 2005) also
describe heuristic constraints that are used during
</bodyText>
<page confidence="0.987742">
409
</page>
<note confidence="0.453614">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 409–417,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999893855555556">
rule extraction to reduce spurious ambiguity. The
resulting translation model does reduces spurious
ambiguity but also reduces the search space in an
arbitrary manner which adversely affects transla-
tion quality.
Syntactic labels from parse trees can be used
to annotate non-terminals in the translation model.
This reduces incorrect rule application by restrict-
ing rule extraction and application. However,
as noted in (Ambati and Lavie, 2008) and else-
where,the naive approach of constraining every
non-terminal to a syntactic constituent severely
limits the coverage of the resulting grammar,
therefore, several approaches have been used to
improve coverage when using syntactic informa-
tion.
Zollmann and Venugopal (2006) allow rules to
be extracted where non-terminals do not exactly
span a target constituent. The non-terminals are
then labeled with complex labels which amalga-
mates multiple labels in the span. This increase
coverage at the expense of increasing data sparsity
as the non-terminal symbol set increases dramati-
cally. Huang and Chiang (2008) use parse infor-
mation of the source language, production rules
consists of source tree fragments and target lan-
guages strings. During decoding, a packed for-
est of the source sentence is used as input, the
production rule tree fragments are applied to the
packed forest. Liu et al. (2009) uses joint decod-
ing with a hierarchical and tree-to-string model
and find that translation performance increase for a
Chinese-English task. Galley et al. (2004) creates
minimal translation rules which can explain a par-
allel sentence pair but the rules generated are not
optimized to produce good translations or cover-
age in any SMT system. This work was extended
and described in (Galley et al., 2006) which cre-
ates rules composed of smaller, minimal rules, as
well as dealing with unaligned words. These mea-
sures are essential for creating good SMT systems,
but again, the rules syntax are strictly constrained
by a parser.
Others have sought to add soft linguistic con-
straints to hierarchical models using addition fea-
ture functions. Marton and Resnik (2008) add fea-
ture functions to penalize or reward non-terminals
which cross constituent boundaries of the source
sentence. This follows on from earlier work in
(Chiang, 2005) but they see gains when finer grain
feature functions which different constituency
types. The weights for feature function is tuned
in batches due to the deficiency of MERT when
presented with many features. Chiang et al. (2008)
rectified this deficiency by using the MIRA to tune
all feature function weights in combination. How-
ever, the translation model continues to be hierar-
chical.
Chiang et al. (2009) added thousands of
linguistically-motivated features to hierarchical
and syntax systems, however, the source syntax
features are derived from the research above. The
translation model remain constant but the parame-
terization changes.
Shen et al. (2009) discusses soft syntax con-
straints and context features in a dependency tree
translation model. The POS tag of the target head
word is used as a soft constraint when applying
rules. Also, a source context language model and
a dependency language model are also used as fea-
tures.
Most SMT systems uses the Viterbi approxi-
mation whereby the derivations in the log-linear
model is not marginalized, but the maximum
derivation is returned. String-to-tree models build
on this so that the most probable derivation, in-
cluding syntactic labels, is assumed to the most
probable translation. This fragments the deriva-
tion probability and the further partition the search
space, leading to pruning errors. Venugopal et al.
(2009) attempts to address this by efficiently es-
timating the score over an equivalent unlabeled
derivation from a target syntax model.
Ambati and Lavie (2008); Ambati et al. (2009)
notes that tree-to-tree often underperform models
with parse tree only on one side due to the non-
isomorphic structure of languages. This motivates
the creation of an isomorphic backbone into the
target parse tree, while leaving the source parse
unchanged.
</bodyText>
<sectionHeader confidence="0.994965" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.998181277777778">
In extending the phrase-based model to the hier-
archical model, non-terminals are used in transla-
tion rules to denote subphrases. Hierarchical non-
terminals are undecorated so are unrestricted to the
span they cover. In contrast, SCFG-based syntac-
tic models restrict the extraction and application
of non-terminals, typically to constituency spans
of a parse tree or forest. Our soft syntax model
combine the hierarchical and source-syntactic ap-
proaches, allowing translation rules with undeco-
rated and decorated non-terminals with informa-
tion from a source language tool.
We give an example of the rules extracted from
an aligned sentence in Figure 1, with a parse tree
on the source side.
Lexicalized rules with decorated non-terminals
are extracted, we list five (non-exhaustive) exam-
ples below.
</bodyText>
<page confidence="0.996292">
410
</page>
<figureCaption confidence="0.999758">
Figure 1: Aligned parsed sentence
</figureCaption>
<figure confidence="0.949031833333333">
NP —* Musharrafs letzter Akt
# Musharraf &apos;s Last Act
NP —* NE1 letzter Akt # X1 Last Act
NP —* NE1 ADJA2 Akt # X1 X2 Act
NP —* NE1 letzter NN2 # X1 Last X2
TOP —* NE1 ADJA2 Akt ? # X1 X2 Act ?
</figure>
<bodyText confidence="0.994871">
Hierarchical style rules are also extracted where
the span doesn’t exactly match a parse constituent.
We list 2 below.
</bodyText>
<equation confidence="0.822316666666667">
X —* letzter Akt # Last Act
X —* letzter X1 # Last X1
Unlexicalized rules with decorated non-
terminals are also extracted:
TOP —* NP1 PUNC2 # X1 X2
NP —* NE1 ADJA2 NN3 # X1 X2 X3
</equation>
<bodyText confidence="0.998205">
Rules are also extracted which contains a mix-
ture of decorated and undecorated non-terminals.
These rules can also be lexicalized or unlexical-
ized. A non-exhaustive sample is given below:
</bodyText>
<equation confidence="0.912823">
X —* ADJA1 Akt # X1 Act
NP —* NE1 X2 # X1 X2
TOP —* NE1 letzter X2 # X1 Last X2
</equation>
<bodyText confidence="0.999982846153846">
At decoding time, the parse tree of the input
sentence is available to the decoder. Decorated
non-terminals in rules must match the constituent
span in the input sentence but the undecorated X
symbol can match any span.
Formally, we model translation as a string-
to-string translation using a synchronous CFG
that constrain the application of non-terminals to
matching source span labels. The source words
and span labels are represented as an unweighted
word lattice, &lt; V, E &gt;, where each edge in the
lattice correspond to a word or non-terminal label
over the corresponding source span. In the soft
syntax experiments, edges with the default source
label, X, are also created for all spans. Nodes
in the lattice represent word positions in the sen-
tence.
We encode the lattice in a chart, as described
in (Dyer et al., 2008). A chart is is a tuple of 2-
dimensional matrices &lt; F, R &gt;. Fi,j is the word
or non-terminal label of the jth transition starting
word position i. Ri,j is the end word position of
the node on the right of the jth transition leaving
word position i.
The input sentence is decoded with a set of
translation rules of the form
</bodyText>
<equation confidence="0.863969">
X —*&lt; αLs, -y, —&gt;
</equation>
<bodyText confidence="0.999922230769231">
where α and -y and strings of terminals and non-
terminals. Ls and the string α are drawn from the
same source alphabet, As. -y is the target string,
also consisting of terminals and non-terminals. —
is the one-to-one correspondence between non-
terminals in α and -y. Ls is the left-hand-side of
the source. As a string-to-string model, the left-
hand-side of the target is always the default target
non-terminal label, X.
Decoding follows the CKY+ algorithms which
process contiguous spans of the source sentence
bottom up. We describe the algorithm as inference
rules, below, omitting the target side for brevity.
</bodyText>
<equation confidence="0.907001071428571">
Initialization
[X —* •αLs, i, i] (X —* αLs) E G
Terminal Symbol
[X —* α • Fj,kQLs, i, j]
[X —* αFj,k • QLs, i, j + 1]
Non-Terminal Symbol
[X —* α • Fj,kQLs, i, j] [X, j, Rj,k]
[X —* αFj,k • QLs, i, Rj,k]
411
Left Hand Side
[X → α • Ls, i, Ri,j] [Fi,j = Ls]
[X → αLs•, i, Ri,j]
Goal
[X → αLs•, 0, |V  |− 1]
</equation>
<bodyText confidence="0.99995356">
This model allows translation rules to take ad-
vantage of both syntactic label and word context.
The presence of default label edges between every
node allows undecorated non-terminals to be ap-
plied to any span, allowing flexibility in the trans-
lation model.
This contrasts with the approach by (Zollmann
and Venugopal, 2006) in attempting to improve the
coverage of syntactic translation. Rather than cre-
ating ad-hoc schemes to categories non-terminals
with syntactic labels when they do not span syn-
tactic constituencies, we only use labels that are
presented by the parser or shallow tagger. Nor do
we try to expand the space where rules can ap-
ply by propagating uncertainty from the parser in
building input forests, as in (Mi et al., 2008), but
we build ambiguity into the translation rule.
The model also differs from (Marton and
Resnik, 2008; Chiang et al., 2008, 2009) by adding
informative labels to rule non-terminals and re-
quiring them to match the source span label. The
soft constraint in our model pertain not to a ad-
ditional feature functions based on syntactic infor-
mation, but to the availability of syntactic and non-
syntactic informed rules.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="method">
4 Parameterization
</sectionHeader>
<bodyText confidence="0.96297275">
In common with most current SMT systems, the
decoding goal of finding the most probable target
language sentence ˆt, given a source language sen-
tence s
</bodyText>
<equation confidence="0.998159">
tˆ = argmaxt p(t|s) (1)
</equation>
<bodyText confidence="0.999602666666667">
The argmax function defines the search objec-
tive of the decoder. We estimate p(t|s) by decom-
posing it into component models
</bodyText>
<equation confidence="0.978999666666667">
p(t|s) = Z
1 � h&apos;m(t,s)λ- (2)
m
</equation>
<bodyText confidence="0.999118333333333">
where h&apos;m(t, s) is the feature function for compo-
nent m and λm is the weight given to component
m. Z is a normalization factor which is ignored in
practice. Components are translation model scor-
ing functions, language model, and other features.
The problem is typically presented in log-space,
which simplifies computations, but otherwise does
not change the problem due to the monotonicity of
the log function (hm = log h&apos; m)
</bodyText>
<equation confidence="0.9835325">
log p(t|s) = � λm hm(t, s) (3)
m
</equation>
<bodyText confidence="0.999321">
An advantage of our model over (Marton and
Resnik, 2008; Chiang et al., 2008, 2009) is the
number of feature functions remains the same,
therefore, the tuning algorithm does not need to be
replaced; we continue to use MERT (Och, 2003).
</bodyText>
<sectionHeader confidence="0.989319" genericHeader="method">
5 Rule Extraction
</sectionHeader>
<bodyText confidence="0.97336525">
Rule extraction follows the algorithm described in
(Chiang, 2005). We note the heuristics used for hi-
erarchical phrases extraction include the following
constraints:
</bodyText>
<listItem confidence="0.996888333333333">
1. all rules must be at least partially lexicalized,
2. non-terminals cannot be consecutive,
3. a maximum of two non-terminals per rule,
4. maximum source and target span width of 10
word
5. maximum of 5 source symbols
</listItem>
<bodyText confidence="0.998925">
In the source syntax model, non-terminals are re-
stricted to source spans that are syntactic phrases
which severely limits the rules that can be ex-
tracted or applied during decoding. Therefore, we
can adapt the heuristics, dropping some of the con-
straints, without introducing too much complexity.
</bodyText>
<listItem confidence="0.9944755">
1. consecutive non-terminals are allowed
2. a maximum of three non-terminals,
3. all non-terminals and LHS must span a parse
constituent
</listItem>
<bodyText confidence="0.9998705">
In the soft syntax model, we relax the constraint
of requiring all non-terminals to span parse con-
stituents. Where there is no constituency spans,
the default symbol X is used to denote an undeco-
rated non-terminal. This gives rise to rules which
mixes decorated and undecorated non-terminals.
To maintain decoding speed and minimize spu-
rious ambiguity, item (1) in the syntactic extrac-
tion heuristics is adapted to prohibit consecutive
undecorated non-terminals. This combines the
strength of syntactic rules but also gives the trans-
lation model more flexibility and higher coverage
from having undecorated non-terminals. There-
fore, the heuristics become:
</bodyText>
<listItem confidence="0.998203166666667">
1. consecutive non-terminals are allowed, but
consecutive undecorated non-terminals are
prohibited
2. a maximum of three non-terminals,
3. all non-terminals and LHS must span a parse
constituent
</listItem>
<page confidence="0.991971">
412
</page>
<subsectionHeader confidence="0.980393">
5.1 Rule probabilities
</subsectionHeader>
<bodyText confidence="0.99997925">
Maximum likelihood phrase probabilities, p(ilg),
are calculated for phrase pairs, using fractional
counts as described in (Chiang, 2005). The max-
imum likelihood estimates are smoothed using
Good-Turing discounting (Foster et al., 2006). A
phrase count feature function is also create for
each translation model, however, the lexical and
backward probabilities are not used.
</bodyText>
<sectionHeader confidence="0.996394" genericHeader="method">
6 Decoding
</sectionHeader>
<bodyText confidence="0.999979166666667">
We use the Moses implementation of the SCFG-
based approach (Hoang et al., 2009) which sup-
port hierarchical and syntactic training and decod-
ing used in this paper. The decoder implements
the CKY+ algorithm with cube pruning, as well as
histogram and beam pruning, all pruning param-
eters were identical for all experiments for fairer
comparison.
All non-terminals can cover a maximum of 7
source words, similar to the maximum rule span
feature other hierarchical decoders to speed up de-
coding time.
</bodyText>
<sectionHeader confidence="0.99945" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99899">
We trained on the New Commentary 2009 cor-
pus1, tuning on a hold-out set. Table 1 gives more
details on the corpus. nc test2007 was used for
testing.
</bodyText>
<table confidence="0.9986126">
German English
Train Sentences 82,306
Words 2,034,373 1,965,325
Tune Sentences 2000
Test Sentences 1026
</table>
<tableCaption confidence="0.999966">
Table 1: Training, tuning, and test conditions
</tableCaption>
<bodyText confidence="0.999881142857143">
The training corpus was cleaned and filtered us-
ing standard methods found in the Moses toolkit
(Koehn et al., 2007) and aligned using GIZA++
(Och and Ney, 2003). Standard MERT weight tun-
ing was used throughout. The English half of the
training data was also used to create a trigram lan-
guage model which was used for each experiment.
All experiments use truecase data and results are
reported in case-sensitive BLEU scores (Papineni
et al., 2001).
The German side was parsed with the Bitpar
parser2. 2042 sentences in the training corpus
failed to parse and were discarded from the train-
ing for both hierarchical and syntactic models to
</bodyText>
<footnote confidence="0.9988915">
1http://www.statmt.org/wmt09/
2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html
</footnote>
<table confidence="0.998441777777778">
# Model % BLEU
Using parse tree
1 Hierarchical 15.9
2 Syntax rules 14.9
3 Joint hier. + syntax rules 16.1
4 Soft syntax rules 16.7
Using chunk tags
5 Hierarchical 16.3
6 Soft syntax 16.8
</table>
<tableCaption confidence="0.918816">
Table 2: German–English results for hierarchical
and syntactic models, in %BLEU
</tableCaption>
<bodyText confidence="0.999454888888889">
ensure that train on identical amounts of data.
Similarly, 991 out of 1026 sentences were parsable
in the test set. To compare like-for-like, the base-
line translates the same 991 sentences, but evalu-
ated over 1026 sentences. (In the experiments with
chunk tags below, all 1026 sentences are used).
We use as a baseline the vanilla hierarchical
model which obtained a BLEU score of 15.9%
(see Table 2, line 1).
</bodyText>
<subsectionHeader confidence="0.993837">
7.1 Syntactic translation
</subsectionHeader>
<bodyText confidence="0.999985714285714">
Using the naive translation model constrained
with syntactic non-terminals significantly de-
creases translation quality, Table 2, line 2. We
then ran hierarchical concurrently with the syntac-
tic models, line 3, but see little improvement over
the hierarchical baseline. However, we see a gain
of 0.8% BLEU when using the soft syntax model.
</bodyText>
<subsectionHeader confidence="0.97174">
7.2 Reachability
</subsectionHeader>
<bodyText confidence="0.999933095238095">
The increased performance using the soft syn-
tax model can be partially explained by studying
the effect of changes to the extraction and decod-
ing algorithms has to the capacity of the transla-
tion pipeline. We run some analysis in which we
trained the phrase models with a corpus of one
sentence and attempt to decode the same sentence.
Pruning and recombination were disabled during
decoding to negate the effect of language model
context and model scores.
The first thousand sentences of the training cor-
pus was analyzed, Table 3. The hierarchical model
successfully decode over half of the sentences
while a translation model constrained by a source
syntax parse tree manages only 113 sentences, il-
lustrating the severe degradation in coverage when
a naive syntax model is used.
Decoding with a hierarchical and syntax model
jointly (line 3) only decode one extra sentence
over the hierarchical model, suggesting that the
expressive power of the hierarchical model almost
</bodyText>
<page confidence="0.998502">
413
</page>
<table confidence="0.9941236">
# Model Reachable sentences
1 Hierarchical 57.8%
2 Syntax rules 11.3%
3 Joint hier. + syntax rules 57.9%
4 Soft syntax rules 58.5%
</table>
<tableCaption confidence="0.8354525">
Table 3: Reachability of 1000 training sentences:
can they be translated with the model?
</tableCaption>
<figureCaption confidence="0.997574">
Figure 2: Source span lengths
Figure 3: Length and count of glue rules used de-
coding test set
</figureCaption>
<bodyText confidence="0.997965333333333">
completely subsumes that of the syntactic model.
The MERT tuning adjust the weights so that the
syntactic model is very rarely applied during joint
decoding, suggesting that the tuning stage prefers
the broader coverage of the hierarchical model
over the precision of the syntactic model.
However, the soft syntax model slightly in-
creases the reachability of the target sentences,
lines 4.
</bodyText>
<subsectionHeader confidence="0.995718">
7.3 Rule Span Width
</subsectionHeader>
<bodyText confidence="0.99996965">
The soft syntactic model contains rules with three
non-terminals, as opposed to 2 in the hierarchical
model, and consecutive non-terminals in the hope
that the rules will have the context and linguistic
information to apply over longer spans. There-
fore, it is surprising that when decoding with a
soft syntactic grammar, significantly more words
are translated singularly and the use of long span-
ning rules is reduced, Figure 2.
However, looking at the usage of the glue rules
paints a different picture. There is significantly
less usage of the glue rules when decoding with
the soft syntax model, Figure 3. The use of
the glue rule indicates a failure of the translation
model to explain the translation so the decrease
in its usage is evidence of the better explanatory
power of the soft syntactic model.
An example of an input sentence, and the best
translation found by the hierarchical and soft syn-
tax model can be seen in Table 4. Figure 4 is the
</bodyText>
<figureCaption confidence="0.991486">
Figure 4: Example input parse tree
parse tree given to the soft syntax model.
</figureCaption>
<figure confidence="0.548662857142857">
Input
laut J´anos Veres w¨are dies im ersten Quartal 2008
m¨oglich .
Hierarchical output
according to J´anos Veres this in the first quarter of 2008
would be possible.
Soft Syntax
</figure>
<note confidence="0.482717">
according to J´anos Veres this would be possible in the
first quarter of 2008 .
</note>
<tableCaption confidence="0.995968">
Table 4: Example input and best output found
</tableCaption>
<bodyText confidence="0.998749625">
Both output are lexically identical but the output
of the hierarchical model needs to be reordered to
be grammatically correct. Contrast the derivations
produced by the hierarchical grammar, Figure 5,
with that produced with the soft syntax model,
Figure 6. The soft syntax derivation makes use
of several non-lexicalized to dictate word order,
shown below.
</bodyText>
<equation confidence="0.999833">
X NE1 NE2 # X1 X2
X V AFIN1 PD52 # X1 X2
X ADJA1 NN2 # X1 X2
X APPRART1 X2 CARD3 # X1 X2 X3
X PP1 X2PUNC3#X2 X1 X3
</equation>
<page confidence="0.998869">
414
</page>
<tableCaption confidence="0.796073">
Table 5: Effect on %BLEU of varying number of
non-terminals
</tableCaption>
<table confidence="0.993709571428571">
# non-terms % BLEU
2 16.5
3 16.8
5 16.8
# Model % BLEU
1 Hierarchical 10.2
2 Soft syntax 10.6
</table>
<tableCaption confidence="0.715298">
Table 6: English–German results in %BLEU
</tableCaption>
<figureCaption confidence="0.999639">
Figure 5: Derivation with Hierarchical model
Figure 6: Derivation with soft syntax model
</figureCaption>
<bodyText confidence="0.999947705882353">
The soft syntax derivation include several rules
which are partially decorated. Crucially, the last
rule in the list above reorders the PP phrase
and the non-syntactic phrase X to generate the
grammatically correct output. The other non-
lexicalized rules monotonically concatenate the
output. This can be performed by the glue rule, but
nevertheless, the use of empirically backed rules
allows the decoder to better compare hypotheses.
The derivation also rely less on the glue rules than
the hierarchical model (shown in solid rectangles).
Reducing the maximum number of non-
terminals per rule reduces translation quality but
increasing it has little effect on the soft syntax
model, Table 5. This seems to indicate that non-
terminals are useful as context when applying
rules up to a certain extent.
</bodyText>
<subsectionHeader confidence="0.98441">
7.4 English to German
</subsectionHeader>
<bodyText confidence="0.999467">
We experimented with the reverse language direc-
tion to see if the soft syntax model still increased
translation quality. The results were positive but
less pronounced, Table 6.
</bodyText>
<subsectionHeader confidence="0.997961">
7.5 Using Chunk Tags
</subsectionHeader>
<bodyText confidence="0.999939333333333">
Parse trees of the source language provide use-
ful information that we have exploited to create a
better translation model. However, parsers are an
expensive resource as they frequently need manu-
ally annotated training treebanks. Parse accuracy
is also problematic and particularly brittle when
given sentences not in the same domain as the
training corpus. This also causes some sentences
to be unparseable. For example, our original test
corpus of 1026 sentences contained 35 unparsable
sentences. Thus, high quality parsers are unavail-
able for many source languages of interest.
Parse forests can be used to mitigate the accu-
racy problem, allowing the decoder to choose from
many alternative parses, (Mi et al., 2008).
The soft syntax translation model is not depen-
dent on the linguistic information being in a tree
structure, only that the labels identify contiguous
spans. Chunk taggers (Abney, 1991) does just
that. They offer higher accuracy than syntactic
parser, are not so brittle to out-of-domain data and
identify chunk phrases similar to parser-based syn-
tactic phrases that may be useful in guiding re-
ordering.
We apply the soft syntax approach as in the pre-
vious sections but replacing the use of parse con-
stituents with chunk phrases.
</bodyText>
<figureCaption confidence="0.998789">
Figure 7: Chunked sentence
</figureCaption>
<page confidence="0.995174">
415
</page>
<subsectionHeader confidence="0.952343">
7.6 Experiments with Chunk Tags
</subsectionHeader>
<bodyText confidence="0.999808260869565">
We use the same data as described earlier in
this chapter to train, tune and test our approach.
The Treetagger chunker (Schmidt and Schulte im
Walde, 2000) was used to tag the source (German)
side of the corpus. The chunker successfully pro-
cessed all sentences in the training and test dataset
so no sentences were excluded. The increase train-
ing data, as well as the ability to translate all sen-
tences in the test set, explains the higher hierar-
chical baseline than the previous experiments with
parser data. We use the noun, verb and preposi-
tional chunks, as well as part-of-speech tags, emit-
ted by the chunker.
Results are shown in Table 2, line 5 &amp; 6. Using
chunk tags, we see a modest gain of 0.5% BLEU.
The same example sentence in Table 4 is shown
with chunk tags in Figure 7. The soft syntax
model with chunk tags produced the derivation
tree shown in Figure 8. The derivation make use
of an unlexicalized rule local reordering. In this
example, it uses the same number of glue rule as
the hierarchical derivation but the output is gram-
matically correct.
</bodyText>
<figureCaption confidence="0.998305">
Figure 8: Translated chunked sentence
</figureCaption>
<bodyText confidence="0.999982166666667">
However, overall, the number of glue rules used
shows the same reduction that we saw using soft
syntax in the earlier section, as can be seen in Fig-
ure 9. Again, the soft syntax model, this time us-
ing chunk tags, is able to reduce the use of the glue
rule with empirically informed rules.
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998007666666667">
We show in this paper that combining the gener-
ality of the hierarchical approach with the speci-
ficity of syntactic approach can improve transla-
</bodyText>
<figureCaption confidence="0.899122">
Figure 9: Chunk - Length and count of glue rules
used decoding test set
</figureCaption>
<bodyText confidence="0.997111857142857">
tion. A reason for the improvement is the bet-
ter long-range reordering made possible by the in-
crease capacity of the translation model.
Future work in this direction includes us-
ing tree-to-tree approaches, automatically created
constituency labels, and back-off methods be-
tween decorated and undecorated rules.
</bodyText>
<sectionHeader confidence="0.996358" genericHeader="acknowledgments">
9 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999638666666666">
This work was supported in part by the EuroMa-
trixPlus project funded by the European Commis-
sion (7th Framework Programme) and in part un-
der the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.930432566666667">
Abney, S. (1991). Parsing by chunks. In Robert Berwick,
Steven Abney, and Carol Tenny: Principle-Based Parsing.
Kluwer Academic Publishers.
Ambati, V. and Lavie, A. (2008). Improving syntax driven
translation models by re-structuring divergent and non-
isomorphic parse tree structures. In AMTA.
Ambati, V., Lavie, A., and Carbonell, J. (2009). Extraction of
syntactic translation models from parallel data using syn-
tax from source and target languages. In MT Summit.
Chiang, D. (2005). A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 263–270, Ann Arbor, Michigan.
Association for Computational Linguistics.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001 new
features for statistical machine translation. In Proceedings
of Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 218–226, Boulder,
Colorado. Association for Computational Linguistics.
Chiang, D., Marton, Y., and Resnik, P. (2008). Online large-
margin training of syntactic and structural translation fea-
tures. In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages 224–
233, Honolulu, Hawaii. Association for Computational
Linguistics.
Dyer, C., Muresan, S., and Resnik, P. (2008). Generalizing
word lattice translation. In Proceedings of ACL-08: HLT,
pages 1012–1020, Columbus, Ohio. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.988229">
416
</page>
<reference confidence="0.99986976744186">
Foster, G., Kuhn, R., and Johnson, H. (2006). Phrasetable
smoothing for statistical machine translation. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 53–61, Sydney, Aus-
tralia. Association for Computational Linguistics.
Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S.,
Wang, W., and Thayer, I. (2006). Scalable inference and
training of context-rich syntactic translation models. In
Proceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961–968,
Sydney, Australia. Association for Computational Lin-
guistics.
Galley, M., Hopkins, M., Knight, K., and Marcu, D. (2004).
What’s in a translation rule? In Proceedings of the Joint
Conference on Human Language Technologies and the
Annual Meeting of the North American Chapter of the As-
sociation of Computational Linguistics (HLT-NAACL).
Hoang, H., Koehn, P., and Lopez, A. (2009). A Unified
Framework for Phrase-Based, Hierarchical, and Syntax-
Based Statistical Machine Translation. In Proc. of the
International Workshop on Spoken Language Translation,
pages 152–159, Tokyo, Japan.
Huang, L. and Chiang, D. (2008). Forest-based translation
rule extraction. In EMNLP, Honolulu, Hawaii.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,
Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,
E. (2007). Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Ses-
sions, pages 177–180, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Li, Z., Callison-Burch, C., Dyer, C., Khudanpur, S.,
Schwartz, L., Thornton, W., Weese, J., and Zaidan, O.
(2009). Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 135–139,
Athens, Greece. Association for Computational Linguis-
tics.
Liu, Y., Mi, H., Feng, Y., and Liu, Q. (2009). Joint decod-
ing with multiple translation models. In In Proceedings of
ACL/IJCNLP 2009, pages 576–584, Singapore.
Marton, Y. and Resnik, P. (2008). Soft syntactic constraints
for hierarchical phrased-based translation. In Proceedings
ofACL-08: HLT, pages 1003–1011, Columbus, Ohio. As-
sociation for Computational Linguistics.
Mi, H., Huang, L., and Liu, Q. (2008). Forest-based trans-
lation. In Proceedings of ACL-08: HLT, pages 192–199,
Columbus, Ohio. Association for Computational Linguis-
tics.
Och, F. J. (2003). Minimum error rate training for statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association of Computational Linguistics
(ACL).
Och, F. J. and Ney, H. (2003). A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19–52.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001).
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176(W0109-022),
IBM Research Report.
Schmidt, H. and Schulte im Walde, S. (2000). Robust
German noun chunking with a probabilistic context-free
grammar. In Proceedings of the International Conference
on Computational Linguistics (COLING).
Shen, L., Xu, J., Zhang, B., Matsoukas, S., and Weischedel,
R. (2009). Effective use of linguistic and contextual infor-
mation for statistical machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 72–80, Singapore. Associa-
tion for Computational Linguistics.
Venugopal, A., Zollmann, A., Smith, N. A., and Vogel, S.
(2009). Preference grammars: Softening syntactic con-
straints to improve statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 236–244,
Boulder, Colorado. Association for Computational Lin-
guistics.
Zollmann, A. and Venugopal, A. (2006). Syntax augmented
machine translation via chart parsing. In Proceedings on
the Workshop on Statistical Machine Translation, pages
138–141, New York City. Association for Computational
Linguistics.
</reference>
<page confidence="0.997446">
417
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372182">
<title confidence="0.993111">Improved Translation with Source Syntax Labels</title>
<author confidence="0.879862">Hieu</author>
<affiliation confidence="0.998034">School of University of</affiliation>
<email confidence="0.977849">h.hoang@sms.ed.ac.uk</email>
<abstract confidence="0.99909225">We present a new translation model that include undecorated hierarchical-style phrase rules, decorated source-syntax rules, and partially decorated rules. Results show an increase in translation of up to 0.8% German–English translation when trained on the news-commentary corpus, using syntactic annotation from a source language parser. We also experimented with annotation from shallow taggers and found</abstract>
<note confidence="0.443863">increased performance by 0.5%</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny: Principle-Based Parsing.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3506" citStr="Abney, 1991" startWordPosition="507" endWordPosition="508">used concurrently to decode sentences. However, results for this method do not improve. The second method uses one translation model containing both hierarchical and syntactic rules. Moreover, an individual rule can contain both decorated syntactic non-terminals, and undecorated hierarchical-style non-terminals (also, the left-hand-side non-terminal may, or may not be decorated). This results in a 0.8% improvement over the hierarchical baseline and analysis suggest that long-range ordering has been improved. We then applied the same methods but using linguistic annotation from a chunk tagger (Abney, 1991) instead of a parser and obtained an improvement of 0.5% BLEU over the hierarchical baseline, showing that gains with additional sourceside annotation can be obtained with simpler tools. 2 Past Work Hierarchical machine translation (Chiang, 2005) extends the phrase-based model by allowing the use of non-contiguous phrase pairs (’production rules’). It promises better re-ordering of translation as the reordering rules are an implicit part of the translation model. Also, hierarchical rules follow the recursive structure of the sentence, reflecting the linguistic notion of language. However, the </context>
<context position="24918" citStr="Abney, 1991" startWordPosition="4028" endWordPosition="4029">n given sentences not in the same domain as the training corpus. This also causes some sentences to be unparseable. For example, our original test corpus of 1026 sentences contained 35 unparsable sentences. Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al., 2008). The soft syntax translation model is not dependent on the linguistic information being in a tree structure, only that the labels identify contiguous spans. Chunk taggers (Abney, 1991) does just that. They offer higher accuracy than syntactic parser, are not so brittle to out-of-domain data and identify chunk phrases similar to parser-based syntactic phrases that may be useful in guiding reordering. We apply the soft syntax approach as in the previous sections but replacing the use of parse constituents with chunk phrases. Figure 7: Chunked sentence 415 7.6 Experiments with Chunk Tags We use the same data as described earlier in this chapter to train, tune and test our approach. The Treetagger chunker (Schmidt and Schulte im Walde, 2000) was used to tag the source (German) </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, S. (1991). Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny: Principle-Based Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ambati</author>
<author>A Lavie</author>
</authors>
<title>Improving syntax driven translation models by re-structuring divergent and nonisomorphic parse tree structures.</title>
<date>2008</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="4966" citStr="Ambati and Lavie, 2008" startWordPosition="720" endWordPosition="723">ngs of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 409–417, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics rule extraction to reduce spurious ambiguity. The resulting translation model does reduces spurious ambiguity but also reduces the search space in an arbitrary manner which adversely affects translation quality. Syntactic labels from parse trees can be used to annotate non-terminals in the translation model. This reduces incorrect rule application by restricting rule extraction and application. However, as noted in (Ambati and Lavie, 2008) and elsewhere,the naive approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang a</context>
<context position="8333" citStr="Ambati and Lavie (2008)" startWordPosition="1249" endWordPosition="1252">el are also used as features. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments the derivation probability and the further partition the search space, leading to pruning errors. Venugopal et al. (2009) attempts to address this by efficiently estimating the score over an equivalent unlabeled derivation from a target syntax model. Ambati and Lavie (2008); Ambati et al. (2009) notes that tree-to-tree often underperform models with parse tree only on one side due to the nonisomorphic structure of languages. This motivates the creation of an isomorphic backbone into the target parse tree, while leaving the source parse unchanged. 3 Model In extending the phrase-based model to the hierarchical model, non-terminals are used in translation rules to denote subphrases. Hierarchical nonterminals are undecorated so are unrestricted to the span they cover. In contrast, SCFG-based syntactic models restrict the extraction and application of non-terminals,</context>
</contexts>
<marker>Ambati, Lavie, 2008</marker>
<rawString>Ambati, V. and Lavie, A. (2008). Improving syntax driven translation models by re-structuring divergent and nonisomorphic parse tree structures. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ambati</author>
<author>A Lavie</author>
<author>J Carbonell</author>
</authors>
<title>Extraction of syntactic translation models from parallel data using syntax from source and target languages. In</title>
<date>2009</date>
<publisher>MT Summit.</publisher>
<contexts>
<context position="8355" citStr="Ambati et al. (2009)" startWordPosition="1253" endWordPosition="1256">res. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments the derivation probability and the further partition the search space, leading to pruning errors. Venugopal et al. (2009) attempts to address this by efficiently estimating the score over an equivalent unlabeled derivation from a target syntax model. Ambati and Lavie (2008); Ambati et al. (2009) notes that tree-to-tree often underperform models with parse tree only on one side due to the nonisomorphic structure of languages. This motivates the creation of an isomorphic backbone into the target parse tree, while leaving the source parse unchanged. 3 Model In extending the phrase-based model to the hierarchical model, non-terminals are used in translation rules to denote subphrases. Hierarchical nonterminals are undecorated so are unrestricted to the span they cover. In contrast, SCFG-based syntactic models restrict the extraction and application of non-terminals, typically to constitu</context>
</contexts>
<marker>Ambati, Lavie, Carbonell, 2009</marker>
<rawString>Ambati, V., Lavie, A., and Carbonell, J. (2009). Extraction of syntactic translation models from parallel data using syntax from source and target languages. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1468" citStr="Chiang, 2005" startWordPosition="210" endWordPosition="211"> mechanism for both hierarchical and syntactic systems are identical and the rule extraction are similar. Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field. Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence. The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage. However, rules may be used in inappropriate situations without the labeled constraints. The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses. Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of l</context>
<context position="3752" citStr="Chiang, 2005" startWordPosition="546" endWordPosition="547">tic non-terminals, and undecorated hierarchical-style non-terminals (also, the left-hand-side non-terminal may, or may not be decorated). This results in a 0.8% improvement over the hierarchical baseline and analysis suggest that long-range ordering has been improved. We then applied the same methods but using linguistic annotation from a chunk tagger (Abney, 1991) instead of a parser and obtained an improvement of 0.5% BLEU over the hierarchical baseline, showing that gains with additional sourceside annotation can be obtained with simpler tools. 2 Past Work Hierarchical machine translation (Chiang, 2005) extends the phrase-based model by allowing the use of non-contiguous phrase pairs (’production rules’). It promises better re-ordering of translation as the reordering rules are an implicit part of the translation model. Also, hierarchical rules follow the recursive structure of the sentence, reflecting the linguistic notion of language. However, the hierarchical model has several limitations. The model makes no use of linguistic information, thus creating a simple model with broad coverage. However, (Chiang, 2005) also describe heuristic constraints that are used during 409 Proceedings of th</context>
<context position="6800" citStr="Chiang, 2005" startWordPosition="1011" endWordPosition="1012">rage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. The translation model remain </context>
<context position="14584" citStr="Chiang, 2005" startWordPosition="2353" endWordPosition="2354">s are translation model scoring functions, language model, and other features. The problem is typically presented in log-space, which simplifies computations, but otherwise does not change the problem due to the monotonicity of the log function (hm = log h&apos; m) log p(t|s) = � λm hm(t, s) (3) m An advantage of our model over (Marton and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature functions remains the same, therefore, the tuning algorithm does not need to be replaced; we continue to use MERT (Och, 2003). 5 Rule Extraction Rule extraction follows the algorithm described in (Chiang, 2005). We note the heuristics used for hierarchical phrases extraction include the following constraints: 1. all rules must be at least partially lexicalized, 2. non-terminals cannot be consecutive, 3. a maximum of two non-terminals per rule, 4. maximum source and target span width of 10 word 5. maximum of 5 source symbols In the source syntax model, non-terminals are restricted to source spans that are syntactic phrases which severely limits the rules that can be extracted or applied during decoding. Therefore, we can adapt the heuristics, dropping some of the constraints, without introducing too </context>
<context position="16352" citStr="Chiang, 2005" startWordPosition="2621" endWordPosition="2622">is adapted to prohibit consecutive undecorated non-terminals. This combines the strength of syntactic rules but also gives the translation model more flexibility and higher coverage from having undecorated non-terminals. Therefore, the heuristics become: 1. consecutive non-terminals are allowed, but consecutive undecorated non-terminals are prohibited 2. a maximum of three non-terminals, 3. all non-terminals and LHS must span a parse constituent 412 5.1 Rule probabilities Maximum likelihood phrase probabilities, p(ilg), are calculated for phrase pairs, using fractional counts as described in (Chiang, 2005). The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al., 2006). A phrase count feature function is also create for each translation model, however, the lexical and backward probabilities are not used. 6 Decoding We use the Moses implementation of the SCFGbased approach (Hoang et al., 2009) which support hierarchical and syntactic training and decoding used in this paper. The decoder implements the CKY+ algorithm with cube pruning, as well as histogram and beam pruning, all pruning parameters were identical for all experiments for fairer comparison. All non-</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, D. (2005). A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>218--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="7208" citStr="Chiang et al. (2009)" startWordPosition="1075" endWordPosition="1078">feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. The translation model remain constant but the parameterization changes. Shen et al. (2009) discusses soft syntax constraints and context features in a dependency tree translation model. The POS tag of the target head word is used as a soft constraint when applying rules. Also, a source context language model and a dependency language model are also used as features. Most SMT systems uses the Viterbi approximation whereby the derivati</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>Chiang, D., Knight, K., and Wang, W. (2009). 11,001 new features for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 218–226, Boulder, Colorado. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online largemargin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="7029" citStr="Chiang et al. (2008)" startWordPosition="1046" endWordPosition="1049">ng good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. The translation model remain constant but the parameterization changes. Shen et al. (2009) discusses soft syntax constraints and context features in a dependency tree translation model. The POS tag of the target head word is used as a soft constraint when ap</context>
<context position="13163" citStr="Chiang et al., 2008" startWordPosition="2107" endWordPosition="2110">n model. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule. The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label. The soft constraint in our model pertain not to a additional feature functions based on syntactic information, but to the availability of syntactic and nonsyntactic informed rules. 4 Parameterization In common with most current SMT systems, the decoding goal of finding the most probable target language sentence ˆt, given a source language sentence s tˆ = argmaxt p(t|s) (1) The argmax function defines the search objective of the decoder. We estimate p(t|s) by decomposing it into compone</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>Chiang, D., Marton, Y., and Resnik, P. (2008). Online largemargin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224– 233, Honolulu, Hawaii. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>S Muresan</author>
<author>P Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1012--1020</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="11037" citStr="Dyer et al., 2008" startWordPosition="1719" endWordPosition="1722">tch any span. Formally, we model translation as a stringto-string translation using a synchronous CFG that constrain the application of non-terminals to matching source span labels. The source words and span labels are represented as an unweighted word lattice, &lt; V, E &gt;, where each edge in the lattice correspond to a word or non-terminal label over the corresponding source span. In the soft syntax experiments, edges with the default source label, X, are also created for all spans. Nodes in the lattice represent word positions in the sentence. We encode the lattice in a chart, as described in (Dyer et al., 2008). A chart is is a tuple of 2- dimensional matrices &lt; F, R &gt;. Fi,j is the word or non-terminal label of the jth transition starting word position i. Ri,j is the end word position of the node on the right of the jth transition leaving word position i. The input sentence is decoded with a set of translation rules of the form X —*&lt; αLs, -y, —&gt; where α and -y and strings of terminals and nonterminals. Ls and the string α are drawn from the same source alphabet, As. -y is the target string, also consisting of terminals and non-terminals. — is the one-to-one correspondence between nonterminals in α a</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Dyer, C., Muresan, S., and Resnik, P. (2008). Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012–1020, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
<author>H Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>53--61</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="16451" citStr="Foster et al., 2006" startWordPosition="2633" endWordPosition="2636">syntactic rules but also gives the translation model more flexibility and higher coverage from having undecorated non-terminals. Therefore, the heuristics become: 1. consecutive non-terminals are allowed, but consecutive undecorated non-terminals are prohibited 2. a maximum of three non-terminals, 3. all non-terminals and LHS must span a parse constituent 412 5.1 Rule probabilities Maximum likelihood phrase probabilities, p(ilg), are calculated for phrase pairs, using fractional counts as described in (Chiang, 2005). The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al., 2006). A phrase count feature function is also create for each translation model, however, the lexical and backward probabilities are not used. 6 Decoding We use the Moses implementation of the SCFGbased approach (Hoang et al., 2009) which support hierarchical and syntactic training and decoding used in this paper. The decoder implements the CKY+ algorithm with cube pruning, as well as histogram and beam pruning, all pruning parameters were identical for all experiments for fairer comparison. All non-terminals can cover a maximum of 7 source words, similar to the maximum rule span feature other hie</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>Foster, G., Kuhn, R., and Johnson, H. (2006). Phrasetable smoothing for statistical machine translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 53–61, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="6272" citStr="Galley et al., 2006" startWordPosition="925" endWordPosition="928">ts of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different c</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S., Wang, W., and Thayer, I. (2006). Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="6031" citStr="Galley et al. (2004)" startWordPosition="884" endWordPosition="887">ltiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008)</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, M., Hopkins, M., Knight, K., and Marcu, D. (2004). What’s in a translation rule? In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang</author>
<author>P Koehn</author>
<author>A Lopez</author>
</authors>
<title>A Unified Framework for Phrase-Based, Hierarchical, and SyntaxBased Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<pages>152--159</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1526" citStr="Hoang et al., 2009" startWordPosition="219" endWordPosition="222">ms are identical and the rule extraction are similar. Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field. Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence. The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage. However, rules may be used in inappropriate situations without the labeled constraints. The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses. Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage. Philipp Koehn School of Informatic</context>
<context position="16679" citStr="Hoang et al., 2009" startWordPosition="2670" endWordPosition="2673">ted non-terminals are prohibited 2. a maximum of three non-terminals, 3. all non-terminals and LHS must span a parse constituent 412 5.1 Rule probabilities Maximum likelihood phrase probabilities, p(ilg), are calculated for phrase pairs, using fractional counts as described in (Chiang, 2005). The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al., 2006). A phrase count feature function is also create for each translation model, however, the lexical and backward probabilities are not used. 6 Decoding We use the Moses implementation of the SCFGbased approach (Hoang et al., 2009) which support hierarchical and syntactic training and decoding used in this paper. The decoder implements the CKY+ algorithm with cube pruning, as well as histogram and beam pruning, all pruning parameters were identical for all experiments for fairer comparison. All non-terminals can cover a maximum of 7 source words, similar to the maximum rule span feature other hierarchical decoders to speed up decoding time. 7 Experiments We trained on the New Commentary 2009 corpus1, tuning on a hold-out set. Table 1 gives more details on the corpus. nc test2007 was used for testing. German English Trai</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hoang, H., Koehn, P., and Lopez, A. (2009). A Unified Framework for Phrase-Based, Hierarchical, and SyntaxBased Statistical Machine Translation. In Proc. of the International Workshop on Spoken Language Translation, pages 152–159, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="5582" citStr="Huang and Chiang (2008)" startWordPosition="811" endWordPosition="814">, 2008) and elsewhere,the naive approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or</context>
</contexts>
<marker>Huang, Chiang, 2008</marker>
<rawString>Huang, L. and Chiang, D. (2008). Forest-based translation rule extraction. In EMNLP, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="17526" citStr="Koehn et al., 2007" startWordPosition="2809" endWordPosition="2812">l experiments for fairer comparison. All non-terminals can cover a maximum of 7 source words, similar to the maximum rule span feature other hierarchical decoders to speed up decoding time. 7 Experiments We trained on the New Commentary 2009 corpus1, tuning on a hold-out set. Table 1 gives more details on the corpus. nc test2007 was used for testing. German English Train Sentences 82,306 Words 2,034,373 1,965,325 Tune Sentences 2000 Test Sentences 1026 Table 1: Training, tuning, and test conditions The training corpus was cleaned and filtered using standard methods found in the Moses toolkit (Koehn et al., 2007) and aligned using GIZA++ (Och and Ney, 2003). Standard MERT weight tuning was used throughout. The English half of the training data was also used to create a trigram language model which was used for each experiment. All experiments use truecase data and results are reported in case-sensitive BLEU scores (Papineni et al., 2001). The German side was parsed with the Bitpar parser2. 2042 sentences in the training corpus failed to parse and were discarded from the training for both hierarchical and syntactic models to 1http://www.statmt.org/wmt09/ 2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/Bi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>C Callison-Burch</author>
<author>C Dyer</author>
<author>S Khudanpur</author>
<author>L Schwartz</author>
<author>W Thornton</author>
<author>J Weese</author>
<author>O Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="1494" citStr="Li et al., 2009" startWordPosition="213" endWordPosition="216">rarchical and syntactic systems are identical and the rule extraction are similar. Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field. Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence. The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage. However, rules may be used in inappropriate situations without the labeled constraints. The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses. Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage. Ph</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Li, Z., Callison-Burch, C., Dyer, C., Khudanpur, S., Schwartz, L., Thornton, W., Weese, J., and Zaidan, O. (2009). Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>H Mi</author>
<author>Y Feng</author>
<author>Q Liu</author>
</authors>
<title>Joint decoding with multiple translation models. In</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP 2009,</booktitle>
<pages>576--584</pages>
<contexts>
<context position="5870" citStr="Liu et al. (2009)" startWordPosition="860" endWordPosition="863">ules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Liu, Y., Mi, H., Feng, Y., and Liu, Q. (2009). Joint decoding with multiple translation models. In In Proceedings of ACL/IJCNLP 2009, pages 576–584, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6631" citStr="Marton and Resnik (2008)" startWordPosition="983" endWordPosition="986">sk. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of lin</context>
<context position="13142" citStr="Marton and Resnik, 2008" startWordPosition="2103" endWordPosition="2106">ibility in the translation model. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule. The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label. The soft constraint in our model pertain not to a additional feature functions based on syntactic information, but to the availability of syntactic and nonsyntactic informed rules. 4 Parameterization In common with most current SMT systems, the decoding goal of finding the most probable target language sentence ˆt, given a source language sentence s tˆ = argmaxt p(t|s) (1) The argmax function defines the search objective of the decoder. We estimate p(t|s) by decomp</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Marton, Y. and Resnik, P. (2008). Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings ofACL-08: HLT, pages 1003–1011, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Forest-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="13038" citStr="Mi et al., 2008" startWordPosition="2086" endWordPosition="2089">dges between every node allows undecorated non-terminals to be applied to any span, allowing flexibility in the translation model. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule. The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label. The soft constraint in our model pertain not to a additional feature functions based on syntactic information, but to the availability of syntactic and nonsyntactic informed rules. 4 Parameterization In common with most current SMT systems, the decoding goal of finding the most probable target language sentence ˆt, given a source language sentence s tˆ = argmaxt </context>
<context position="24733" citStr="Mi et al., 2008" startWordPosition="3997" endWordPosition="4000"> translation model. However, parsers are an expensive resource as they frequently need manually annotated training treebanks. Parse accuracy is also problematic and particularly brittle when given sentences not in the same domain as the training corpus. This also causes some sentences to be unparseable. For example, our original test corpus of 1026 sentences contained 35 unparsable sentences. Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al., 2008). The soft syntax translation model is not dependent on the linguistic information being in a tree structure, only that the labels identify contiguous spans. Chunk taggers (Abney, 1991) does just that. They offer higher accuracy than syntactic parser, are not so brittle to out-of-domain data and identify chunk phrases similar to parser-based syntactic phrases that may be useful in guiding reordering. We apply the soft syntax approach as in the previous sections but replacing the use of parse constituents with chunk phrases. Figure 7: Chunked sentence 415 7.6 Experiments with Chunk Tags We use </context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Mi, H., Huang, L., and Liu, Q. (2008). Forest-based translation. In Proceedings of ACL-08: HLT, pages 192–199, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14499" citStr="Och, 2003" startWordPosition="2341" endWordPosition="2342">o component m. Z is a normalization factor which is ignored in practice. Components are translation model scoring functions, language model, and other features. The problem is typically presented in log-space, which simplifies computations, but otherwise does not change the problem due to the monotonicity of the log function (hm = log h&apos; m) log p(t|s) = � λm hm(t, s) (3) m An advantage of our model over (Marton and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature functions remains the same, therefore, the tuning algorithm does not need to be replaced; we continue to use MERT (Och, 2003). 5 Rule Extraction Rule extraction follows the algorithm described in (Chiang, 2005). We note the heuristics used for hierarchical phrases extraction include the following constraints: 1. all rules must be at least partially lexicalized, 2. non-terminals cannot be consecutive, 3. a maximum of two non-terminals per rule, 4. maximum source and target span width of 10 word 5. maximum of 5 source symbols In the source syntax model, non-terminals are restricted to source spans that are syntactic phrases which severely limits the rules that can be extracted or applied during decoding. Therefore, we</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. J. (2003). Minimum error rate training for statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17571" citStr="Och and Ney, 2003" startWordPosition="2817" endWordPosition="2820">erminals can cover a maximum of 7 source words, similar to the maximum rule span feature other hierarchical decoders to speed up decoding time. 7 Experiments We trained on the New Commentary 2009 corpus1, tuning on a hold-out set. Table 1 gives more details on the corpus. nc test2007 was used for testing. German English Train Sentences 82,306 Words 2,034,373 1,965,325 Tune Sentences 2000 Test Sentences 1026 Table 1: Training, tuning, and test conditions The training corpus was cleaned and filtered using standard methods found in the Moses toolkit (Koehn et al., 2007) and aligned using GIZA++ (Och and Ney, 2003). Standard MERT weight tuning was used throughout. The English half of the training data was also used to create a trigram language model which was used for each experiment. All experiments use truecase data and results are reported in case-sensitive BLEU scores (Papineni et al., 2001). The German side was parsed with the Bitpar parser2. 2042 sentences in the training corpus failed to parse and were discarded from the training for both hierarchical and syntactic models to 1http://www.statmt.org/wmt09/ 2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html # Model % BLEU Using parse tree 1 H</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. and Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report RC22176(W0109-022), IBM Research Report.</tech>
<contexts>
<context position="17857" citStr="Papineni et al., 2001" startWordPosition="2865" endWordPosition="2868">07 was used for testing. German English Train Sentences 82,306 Words 2,034,373 1,965,325 Tune Sentences 2000 Test Sentences 1026 Table 1: Training, tuning, and test conditions The training corpus was cleaned and filtered using standard methods found in the Moses toolkit (Koehn et al., 2007) and aligned using GIZA++ (Och and Ney, 2003). Standard MERT weight tuning was used throughout. The English half of the training data was also used to create a trigram language model which was used for each experiment. All experiments use truecase data and results are reported in case-sensitive BLEU scores (Papineni et al., 2001). The German side was parsed with the Bitpar parser2. 2042 sentences in the training corpus failed to parse and were discarded from the training for both hierarchical and syntactic models to 1http://www.statmt.org/wmt09/ 2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html # Model % BLEU Using parse tree 1 Hierarchical 15.9 2 Syntax rules 14.9 3 Joint hier. + syntax rules 16.1 4 Soft syntax rules 16.7 Using chunk tags 5 Hierarchical 16.3 6 Soft syntax 16.8 Table 2: German–English results for hierarchical and syntactic models, in %BLEU ensure that train on identical amounts of data. Simila</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176(W0109-022), IBM Research Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmidt</author>
<author>Schulte im Walde</author>
<author>S</author>
</authors>
<title>Robust German noun chunking with a probabilistic context-free grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<marker>Schmidt, Walde, S, 2000</marker>
<rawString>Schmidt, H. and Schulte im Walde, S. (2000). Robust German noun chunking with a probabilistic context-free grammar. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>B Zhang</author>
<author>S Matsoukas</author>
<author>R Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>72--80</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="7461" citStr="Shen et al. (2009)" startWordPosition="1110" endWordPosition="1113">e functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. The translation model remain constant but the parameterization changes. Shen et al. (2009) discusses soft syntax constraints and context features in a dependency tree translation model. The POS tag of the target head word is used as a soft constraint when applying rules. Also, a source context language model and a dependency language model are also used as features. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments th</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Shen, L., Xu, J., Zhang, B., Matsoukas, S., and Weischedel, R. (2009). Effective use of linguistic and contextual information for statistical machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>N A Smith</author>
<author>S Vogel</author>
</authors>
<title>Preference grammars: Softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="8180" citStr="Venugopal et al. (2009)" startWordPosition="1225" endWordPosition="1228">The POS tag of the target head word is used as a soft constraint when applying rules. Also, a source context language model and a dependency language model are also used as features. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments the derivation probability and the further partition the search space, leading to pruning errors. Venugopal et al. (2009) attempts to address this by efficiently estimating the score over an equivalent unlabeled derivation from a target syntax model. Ambati and Lavie (2008); Ambati et al. (2009) notes that tree-to-tree often underperform models with parse tree only on one side due to the nonisomorphic structure of languages. This motivates the creation of an isomorphic backbone into the target parse tree, while leaving the source parse unchanged. 3 Model In extending the phrase-based model to the hierarchical model, non-terminals are used in translation rules to denote subphrases. Hierarchical nonterminals are u</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Venugopal, A., Zollmann, A., Smith, N. A., and Vogel, S. (2009). Preference grammars: Softening syntactic constraints to improve statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 236–244, Boulder, Colorado. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<institution>City. Association for Computational Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="5245" citStr="Zollmann and Venugopal (2006)" startWordPosition="759" endWordPosition="762">urious ambiguity but also reduces the search space in an arbitrary manner which adversely affects translation quality. Syntactic labels from parse trees can be used to annotate non-terminals in the translation model. This reduces incorrect rule application by restricting rule extraction and application. However, as noted in (Ambati and Lavie, 2008) and elsewhere,the naive approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed </context>
<context position="12619" citStr="Zollmann and Venugopal, 2006" startWordPosition="2015" endWordPosition="2018">Initialization [X —* •αLs, i, i] (X —* αLs) E G Terminal Symbol [X —* α • Fj,kQLs, i, j] [X —* αFj,k • QLs, i, j + 1] Non-Terminal Symbol [X —* α • Fj,kQLs, i, j] [X, j, Rj,k] [X —* αFj,k • QLs, i, Rj,k] 411 Left Hand Side [X → α • Ls, i, Ri,j] [Fi,j = Ls] [X → αLs•, i, Ri,j] Goal [X → αLs•, 0, |V |− 1] This model allows translation rules to take advantage of both syntactic label and word context. The presence of default label edges between every node allows undecorated non-terminals to be applied to any span, allowing flexibility in the translation model. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule. The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-termina</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Zollmann, A. and Venugopal, A. (2006). Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>