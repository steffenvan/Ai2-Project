<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000978">
<title confidence="0.999559">
Active Learning with Efficient Feature Weighting Methods
for Improving Data Quality and Classification Accuracy
</title>
<author confidence="0.986564">
Justin Martineau1, Lu Chen2∗, Doreen Cheng3, and Amit Sheth4
</author>
<affiliation confidence="0.657439">
1,3 Samsung Research America, Silicon Valley
</affiliation>
<address confidence="0.698924">
1,3 75 W Plumeria Dr. San Jose, CA 95134 USA
2,4 Kno.e.sis Center, Wright State University
2,4 3640 Colonel Glenn Hwy. Fairborn, OH 45435 USA
1,3 {justin.m, doreen.c}@samsung.com
</address>
<email confidence="0.762494">
2,4 {chen, amit}@knoesis.org
</email>
<sectionHeader confidence="0.987767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851105263158">
Many machine learning datasets are noisy
with a substantial number of mislabeled
instances. This noise yields sub-optimal
classification performance. In this paper
we study a large, low quality annotated
dataset, created quickly and cheaply us-
ing Amazon Mechanical Turk to crowd-
source annotations. We describe compu-
tationally cheap feature weighting tech-
niques and a novel non-linear distribution
spreading algorithm that can be used to it-
eratively and interactively correcting mis-
labeled instances to significantly improve
annotation quality at low cost. Eight dif-
ferent emotion extraction experiments on
Twitter data demonstrate that our approach
is just as effective as more computation-
ally expensive techniques. Our techniques
save a considerable amount of time.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990112616666667">
Supervised classification algorithms require anno-
tated data to teach the machine, by example, how
to perform a specific task. There are generally two
ways to collect annotations of a dataset: through
a few expert annotators, or through crowdsourc-
ing services (e.g., Amazon’s Mechanical Turk).
High-quality annotations can be produced by ex-
pert annotators, but the process is usually slow
and costly. The latter option is appealing since it
creates a large annotated dataset at low cost. In
recent years, there have been an increasing num-
ber of studies (Su et al., 2007; Kittur et al., 2008;
Sheng et al., 2008; Snow et al., 2008; Callison-
Burch, 2009) using crowdsourcing for data anno-
tation. However, because annotators that are re-
cruited this way may lack expertise and motiva-
tion, the annotations tend to be more noisy and
∗This author’s research was done during an internship
with Samsung Research America.
unreliable, which significantly reduces the perfor-
mance of the classification model. This is a chal-
lenge faced by many real world applications –
given a large, quickly and cheaply created, low
quality annotated dataset, how can one improve
its quality and learn an accurate classifier from
it?
Re-annotating the whole dataset is too expen-
sive. To reduce the annotation effort, it is desirable
to have an algorithm that selects the most likely
mislabeled examples first for re-labeling. The pro-
cess of selecting and re-labeling data points can be
conducted with multiple rounds to iteratively im-
prove the data quality. This is similar to the strat-
egy of active learning. The basic idea of active
learning is to learn an accurate classifier using less
training data. An active learner uses a small set of
labeled data to iteratively select the most informa-
tive instances from a large pool of unlabeled data
for human annotators to label (Settles, 2010). In
this work, we borrow the idea of active learning to
interactively and iteratively correct labeling errors.
The crucial step is to effectively and efficiently
select the most likely mislabeled instances. An in-
tuitive idea is to design algorithms that classify
the data points and rank them according to the
decreasing confidence scores of their labels. The
data points with the highest confidence scores but
conflicting preliminary labels are most likely mis-
labeled. The algorithm should be computationally
cheap as well as accurate, so it fits well with ac-
tive learning and other problems that require fre-
quent iterations on large datasets. Specifically,
we propose a novel non-linear distribution spread-
ing algorithm, which first uses Delta IDF tech-
nique (Martineau and Finin, 2009) to weight fea-
tures, and then leverages the distribution of Delta
IDF scores of a feature across different classes
to efficiently recognize discriminative features for
the classification task in the presence of misla-
beled data. The idea is that some effective fea-
</bodyText>
<page confidence="0.977525">
1104
</page>
<note confidence="0.8500935">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1104–1112,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.991891363636363">
tures may be subdued due to label noise, and the
proposed techniques are capable of counteracting
such effect, so that the performance of classifica-
tion algorithms could be less affected by the noise.
With the proposed algorithm, the active learner be-
comes more accurate and resistant to label noise,
thus the mislabeled data points can be more easily
and accurately identified.
We consider emotion analysis as an interest-
ing and challenging problem domain of this study,
and conduct comprehensive experiments on Twit-
ter data. We employ Amazon’s Mechanical Turk
(AMT) to label the emotions of Twitter data, and
apply the proposed methods to the AMT dataset
with the goals of improving the annotation quality
at low cost, as well as learning accurate emotion
classifiers. Extensive experiments show that, the
proposed techniques are as effective as more com-
putational expensive techniques (e.g, Support Vec-
tor Machines) but require significantly less time
for training/running, which makes it well-suited
for active learning.
</bodyText>
<sectionHeader confidence="0.999715" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999299628205129">
Research on handling noisy dataset of mislabeled
instances has focused on three major groups of
techniques: (1) noise tolerance, (2) noise elimi-
nation, and (3) noise correction.
Noise tolerance techniques aim to improve
the learning algorithm itself to avoid over-fitting
caused by mislabeled instances in the training
phase, so that the constructed classifier becomes
more noise-tolerant. Decision tree (Mingers,
1989; Vannoorenberghe and Denoeux, 2002) and
boosting (Jiang, 2001; Kalaia and Servediob,
2005; Karmaker and Kwek, 2006) are two learn-
ing algorithms that have been investigated in many
studies. Mingers (1989) explores pruning methods
for identifying and removing unreliable branches
from a decision tree to reduce the influence of
noise. Vannoorenberghe and Denoeux (2002) pro-
pose a method based on belief decision trees to
handle uncertain labels in the training set. Jiang
(2001) studies some theoretical aspects of regres-
sion and classification boosting algorithms in deal-
ing with noisy data. Kalaia and Servediob (2005)
present a boosting algorithm which can achieve
arbitrarily high accuracy in the presence of data
noise. Karmaker and Kwek (2006) propose a mod-
ified AdaBoost algorithm – ORBoost, which min-
imizes the impact of outliers and becomes more
tolerant to class label noise. One of the main dis-
advantages of noise tolerance techniques is that
they are learning algorithm-dependent. In con-
trast, noise elimination/correction approaches are
more generic and can be more easily applied to
various problems.
A large number of studies have explored noise
elimination techniques (Brodley and Friedl, 1999;
Verbaeten and Van Assche, 2003; Zhu et al., 2003;
Muhlenbach et al., 2004; Guan et al., 2011), which
identifies and removes mislabeled examples from
the dataset as a pre-processing step before build-
ing classifiers. One widely used approach (Brod-
ley and Friedl, 1999; Verbaeten and Van Assche,
2003) is to create an ensemble classifier that com-
bines the outputs of multiple classifiers by either
majority vote or consensus, and an instance is
tagged as mislabeled and removed from the train-
ing set if it is classified into a different class than
its training label by the ensemble classifier. The
similar approach is adopted by Guan et al. (2011)
and they further demonstrate that its performance
can be significantly improved by utilizing unla-
beled data. To deal with the noise in large or
distributed datasets, Zhu et al. (2003) propose a
partition-based approach, which constructs clas-
sification rules from each subset of the dataset,
and then evaluates each instance using these rules.
Two noise identification schemes, majority and
non-objection, are used to combine the decision
from each set of rules to decide whether an in-
stance is mislabeled. Muhlenbach et al. (2004)
propose a different approach, which represents
the proximity between instances in a geometrical
neighborhood graph, and an instance is consid-
ered suspect if in its neighborhood the proportion
of examples of the same class is not significantly
greater than in the dataset itself.
Removing mislabeled instances has been
demonstrated to be effective in increasing the
classification accuracy in prior studies, but there
are also some major drawbacks. For example,
useful information can be removed with noise
elimination, since annotation errors are likely to
occur on ambiguous instances that are potentially
valuable for learning algorithms. In addition,
when the noise ratio is high, there may not be
adequate amount of data remaining for building
an accurate classifier. The proposed approach
does not suffer these limitations.
Instead of eliminating the mislabeled examples
</bodyText>
<page confidence="0.98875">
1105
</page>
<bodyText confidence="0.999898086956522">
from training data, some researchers (Zeng and
Martinez, 2001; Rebbapragada et al., 2012; Lax-
man et al., 2013) propose to correct labeling er-
rors either with or without consulting human ex-
perts. Zeng and Martinez (2001) present an ap-
proach based on backpropagation neural networks
to automatically correct the mislabeled data. Lax-
man et al. (2012) propose an algorithm which first
trains individual SVM classifiers on several small,
class-balanced, random subsets of the dataset, and
then reclassifies each training instance using a ma-
jority vote of these individual classifiers. How-
ever, the automatic correction may introduce new
noise to the dataset by mistakenly changing a cor-
rect label to a wrong one.
In many scenarios, it is worth the effort and
cost to fix the labeling errors by human experts,
in order to obtain a high quality dataset that can
be reused by the community. Rebbapragada et al.
(2012) propose a solution called Active Label Cor-
rection (ALC) which iteratively presents the ex-
perts with small sets of suspected mislabeled in-
stances at each round. Our work employs a sim-
ilar framework that uses active learning for data
cleaning.
In Active Learning (Settles, 2010) a small set of
labeled data is used to find documents that should
be annotated from a large pool of unlabeled doc-
uments. Many different strategies have been used
to select the best points to annotate. These strate-
gies can be generally divided into two groups: (1)
selecting points in poorly sampled regions, and (2)
selecting points that will have the greatest impact
on models that were constructed using the dataset.
Active learning for data cleaning differs from
traditional active learning because the data already
has low quality labels. It uses the difference be-
tween the low quality label for each data point and
a prediction of the label using supervised machine
learning models built upon the low quality labels.
Unlike the work in (Rebbapragada et al., 2012),
this paper focuses on developing algorithms that
can enhance the ability of active learner on identi-
fying labeling errors, which we consider as a key
challenge of this approach but ALC has not ad-
dressed.
</bodyText>
<sectionHeader confidence="0.731073" genericHeader="method">
3 An Active Learning Framework for
</sectionHeader>
<subsectionHeader confidence="0.784819">
Label Correction
</subsectionHeader>
<bodyText confidence="0.9998540625">
Let Dˆ = {(x1, y1), ..., (xn, yn)} be a dataset of
binary labeled instances, where the instance xi be-
longs to domain X, and its label yi ∈ {−1, +1}.
Dˆ contains an unknown number of mislabeled
data points. The problem is to obtain a high-
quality dataset D by fixing labeling errors in ˆD,
and learn an accurate classifier C from it.
Algorithm 1 illustrates an active learning ap-
proach to the problem. This algorithm takes the
noisy dataset Dˆ as input. The training set T is
initialized with the data in Dˆ and then updated
each round with new labels generated during re-
annotation. Data sets Sr and S are used to main-
tain the instances that have been selected for re-
annotation in the whole process and in the current
iteration, respectively.
</bodyText>
<figure confidence="0.7835756">
Data: noisy data Dˆ
Result: cleaned data D, classifier C
Initialize training set T = Dˆ ;
Initialize re-annotated data sets Sr = ∅;
S = ∅ ;
repeat
Train classifier C using T ;
Use C to select a set S of m suspected
mislabeled instances from T ;
Experts re-annotate the instances in
S − (Sr ∩ S) ;
Update T with the new labels in S ;
Sr = Sr ∪ S; S = ∅ ;
until for I iterations;
D = T ;
</figure>
<figureCaption confidence="0.615214">
Algorithm 1: Active Learning Approach for La-
</figureCaption>
<bodyText confidence="0.97795975">
bel Correction
In each iteration, the algorithm trains classifiers
using the training data in T. In practice, we ap-
ply k-fold cross-validation. We partition T into k
subsets, and each time we keep a different subset
as testing data and train a classifier using the other
k − 1 subsets of data. This process is repeated k
times so that we get a classifier for each of the k
subsets. The goal is to use the classifiers to ef-
ficiently and accurately seek out the most likely
mislabeled instances from T for expert annotators
to examine and re-annotate. When applying a clas-
sifier to classify the instances in the correspond-
ing data subset, we get the probability about how
likely one instance belongs to a class. The top m
instances with the highest probabilities belonging
to some class but conflicting preliminary labels are
selected as the most likely errors for annotators to
fix. During the re-annotation process we keep the
old labels hidden to prevent that information from
</bodyText>
<page confidence="0.964074">
1106
</page>
<bodyText confidence="0.99992975">
biasing annotators’ decisions. Similarly, we keep
the probability scores hidden while annotating.
This process is done with multiple iterations of
training, sampling, and re-annotating. We main-
tain the re-annotated instances in Sr to avoid an-
notating the same instance multiple times. After
each round of annotation, we compare the old la-
bels to the new labels to measure the degree of im-
pact this process is having on the dataset. We stop
re-annotating on the Ith round after we decide that
the reward for an additional round of annotation is
too low to justify.
</bodyText>
<sectionHeader confidence="0.978049" genericHeader="method">
4 Feature Weighting Methods
</sectionHeader>
<bodyText confidence="0.999986027777778">
Building the classifier C that allows the most
likely mislabeled instances to be selected and an-
notated is the essence of the active learning ap-
proach. There are two main goals of developing
this classifier: (1) accurately predicting the labels
of data points and ranking them based on predic-
tion confidence, so that the most likely errors can
be effectively identified; (2) requiring less time on
training, so that the saved time can be spent on cor-
recting more labeling errors. Thus we aim to build
a classifier that is both accurate and time efficient.
Labeling noise affects the classification accu-
racy. One possible reason is that some effective
features that should be given high weights are in-
hibited in the training phase due to the labeling
errors. For example, emoticon “:D” is a good in-
dicator for emotion happy, however, if by mis-
take many instances containing this emoticon are
not correctly labeled as happy, this class-specific
feature would be underestimated during training.
Following this idea, we develop computationally
cheap feature weighting techniques to counteract
such effect by boosting the weight of discrimina-
tive features, so that they would not be subdued
and the instances with such features would have
higher chance to be correctly classified.
Specifically, we propose a non-linear distribu-
tion spreading algorithm for feature weighting.
This algorithm first utilizes Delta IDF to weigh the
features, and then non-linearly spreads out the dis-
tribution of features’ Delta IDF scores to exagger-
ate the weight of discriminative features. We first
introduce Delta-IDF technique, and then describe
our algorithm of distribution spreading. Since we
focus on n-gram features, we use the words feature
and term interchangeably in this paper.
</bodyText>
<subsectionHeader confidence="0.939487">
4.1 Delta IDF Weighting Scheme
</subsectionHeader>
<bodyText confidence="0.99875205">
Different from the commonly used TF (term fre-
quency) or TF.IDF (term frequency.inverse doc-
ument frequency) weighting schemes, Delta IDF
treats the positive and negative training instances
as two separate corpora, and weighs the terms by
how biased they are to one corpus. The more bi-
ased a term is to one class, the higher (absolute
value of) weight it will get. Delta IDF boosts the
importance of terms that tend to be class-specific
in the dataset, since they are usually effective fea-
tures in distinguishing one class from another.
Each training instance (e.g., a document)
is represented as a feature vector: xi =
(w1,i, ..., w|V |,i), where each dimension in the vec-
tor corresponds to a n-gram term in vocabulary
V = {ti, ..., t|V |}, |V  |is the number of unique
terms, and wj,i(1 G j G |V |) is the weight of
term tj in instance xi. Delta IDF (Martineau and
Finin, 2009) assigns score A idfj to term tj in V
as:
</bodyText>
<equation confidence="0.9959105">
A idfj = log (N + 1)(Pj + 1) (1)
(Nj + 1)(P + 1)
</equation>
<bodyText confidence="0.996342260869565">
where P (or N) is the number of positively (or
negatively) labeled training instances, Pj (or Nj)
is the number of positively (or negatively) labeled
training instances with term tj. Simple add-one
smoothing is used to smooth low frequency terms
and prevent dividing by zero when a term appears
in only one corpus. We calculate the Delta IDF
score of every term in V , and get the Delta IDF
weight vector A = (A idfi, ..., A idf|V |) for all
terms.
When the dataset is imblanced, to avoid build-
ing a biased model, we down sample the majority
class before calculating the Delta IDF score and
then use the a bias balancing procedure to balance
the Delta IDF weight vector. This procedure first
divides the Delta IDF weight vector to two vec-
tors, one of which contains all the features with
positive scores, and the other of which contains all
the features with negative scores. It then applies
L2 normalization to each of the two vectors, and
add them together to create the final vector.
For each instance, we can calculate the
TF.Delta-IDF score as its weight:
</bodyText>
<equation confidence="0.995554">
wj,i = tfj,i x A idfj (2)
</equation>
<bodyText confidence="0.999812666666667">
where tfj,i is the number of times term tj occurs
in document xi, and A idfj is the Delta IDF score
of tj.
</bodyText>
<page confidence="0.988141">
1107
</page>
<sectionHeader confidence="0.661011" genericHeader="method">
4.2 A Non-linear Distribution Spreading
Algorithm
</sectionHeader>
<bodyText confidence="0.999614384615385">
Delta IDF technique boosts the weight of features
with strong discriminative power. The model’s
ability to discriminate at the feature level can be
further enhanced by leveraging the distribution of
feature weights across multiple classes, e.g., mul-
tiple emotion categories funny, happy, sad, ex-
citing, boring, etc.. The distinction of multiple
classes can be used to further force feature bias
scores apart to improve the identification of class-
specific features in the presence of labeling errors.
Let L be a set of target classes, and |L |be the
number of classes in L. For each class l E L,
we create a binary labeled dataset Dl. Let V l
</bodyText>
<equation confidence="0.535328">
ˆ
</equation>
<bodyText confidence="0.977215818181818">
be the vocabulary of dataset ˆDl, V be the vo-
cabulary of all datasets, and |V  |is the number of
unique terms in V . Using Formula (1) and dataset
Dl, we get the Delta IDF weight vector for each
ˆ
class l: Al = (A idfl1,...,A idfl|V |). Note that
A idflj = 0 for any term tj E V − V l. For a
class u, we calculate the spreading score spreaduj
of each feature tj E V using a non-linear distri-
bution spreading formula as following (where s is
the configurable spread parameter):
</bodyText>
<equation confidence="0.850666333333333">
spreaduj = A idfju x (3)
ElEL−u |A idfju − A idflj|s
|L |− 1
</equation>
<bodyText confidence="0.991866384615385">
For any term tj E V , we can get its Delta IDF
score on a class l. The distribution of Delta IDF
scores of tj on all classes in L is represented as
δj = {A idf1j , ..., A idf|L|
j }.
The mechanism of Formula (3) is to non-
linearly spread out the distribution, so that the
importance of class-specific features can be fur-
ther boosted to counteract the effect of noisy la-
bels. Specifically, according to Formula (3), a
high (absolute value of) spread score indicates that
the Delta IDF score of that term on that class is
high and deviates greatly from the scores on other
classes. In other words, our algorithm assigns high
spread score (absolute value) to a term on a class
for which the term has strong discriminative power
and very specific to that class compared with to
other classes. When the dataset is imbalanced, we
apply the similar bias balancing procedure as de-
scribed in Section 4.1 to the spreading model.
While these feature weighting models can be
used to score and rank instances for data clean-
ing, better classification and regression models can
be built by using the feature weights generated by
these models as a pre-weight on the data points for
other machine learning algorithms.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999865733333333">
We conduct experiments on a Twitter dataset that
contains tweets about TV shows and movies. The
goal is to extract consumers’ emotional reactions
to multimedia content, which has broad commer-
cial applications including targeted advertising,
intelligent search, and recommendation. To create
the dataset, we collected 2 billion unique tweets
using Twitter API queries for a list of known TV
shows and movies on IMDB. Spam tweets were
filtered out using a set of heuristics and manually
crafted rules. From the set of 2 billion tweets we
randomly selected a small subset of 100K tweets
about the 60 most highly mentioned TV shows
and movies in the dataset. Tweets were randomly
sampled for each show using the round robin algo-
rithm. Duplicates were not allowed. This samples
an equal number of tweets for each show. We then
sent these tweets to Amazon Mechanical Turk for
annotation.
We defined our own set of emotions to anno-
tate. The widely accepted emotion taxonomies, in-
cluding Ekmans Basic Emotions (Ekman, 1999),
Russells Circumplex model (Russell and Barrett,
1999), and Plutchiks emotion wheel (Plutchik,
2001), did not fit well for TV shows and Movies.
For example, the emotion expressed by laughter
is a very important emotion for TV shows and
movies, but this emotion is not covered by the tax-
onomies listed above. After browsing through the
raw dataset, reviewing the literature on emotion
analysis, and considering the TV and movie prob-
lem domain, we decided to focus on eight emo-
tions: funny, happy, sad, exciting, boring, angry,
fear, and heartwarming.
Emotion annotation is a non-trivial task that
is typically time-consuming, expensive and error-
prone. This task is difficult because: (1) There are
multiple emotions to annotate. In this work, we
annotate eight different emotions. (2) Emotion ex-
pressions could be subtle and ambiguous and thus
are easy to miss when labeling quickly. (3) The
dataset is very imbalanced, which increases the
problem of confirmation bias. As minority classes,
emotional tweets can be easily missed because the
last X tweets are all not emotional, and the annota-
</bodyText>
<page confidence="0.984705">
1108
</page>
<table confidence="0.99886675">
Funny Happy Sad Exciting Boring Angry Fear Heartwarming
# Pos. 1,324 405 618 313 209 92 164 24
# Neg. 88,782 95,639 84,212 79,902 82,443 57,326 46,746 15,857
# Total 90,106 96,044 84,830 80,215 82,652 57,418 46,910 15,881
</table>
<tableCaption confidence="0.995701">
Table 1: Amazon Mechanical Turk annotation label counts.
</tableCaption>
<table confidence="0.999502">
Funny Happy Sad Exciting Boring Angry Fear Heartwarming
# Pos. 1,781 4,847 788 1,613 216 763 285 326
# Neg. 88,277 91,075 84,031 78,573 82,416 56,584 46,622 15,542
# Total1 90,058 95,922 84,819 80,186 82,632 57,347 46,907 15,868
</table>
<tableCaption confidence="0.999647">
Table 2: Ground truth annotation label counts for each emotion.2
</tableCaption>
<bodyText confidence="0.999761191780822">
tors do not expect the next one to be either. Due to
these reasons, there is a lack of sufficient and high
quality labeled data for emotion research. Some
researchers have studied harnessing Twitter hash-
tags to automatically create an emotion annotated
dataset (Wang et al., 2012).
In order to evaluate our approach in real world
scenarios, instead of creating a high quality anno-
tated dataset and then introducing artificial noise,
we followed the common practice of crowdsouc-
ing, and collected emotion annotations through
Amazon Mechanical Turk (AMT). This AMT an-
notated dataset was used as the low quality dataset
Dˆ in our evaluation. After that, the same dataset
was annotated independently by a group of expert
annotators to create the ground truth. We evaluate
the proposed approach on two factors, the effec-
tiveness of the models for emotion classification,
and the improvement of annotation quality pro-
vided by the active learning procedure. We first
describe the AMT annotation and ground truth an-
notation, and then discuss the baselines and exper-
imental results.
Amazon Mechanical Turk Annotation: we
posted the set of 100K tweets to the workers on
AMT for emotion annotation. We defined a set
of annotation guidelines, which specified rules and
examples to help annotators determine when to tag
a tweet with an emotion. We applied substantial
quality control to our AMT workers to improve the
initial quality of annotation following the common
practice of crowdsourcing. Each tweet was anno-
tated by at least two workers. We used a series of
tests to identify bad workers. These tests include
(1) identifying workers with poor pairwise agree-
ment, (2) identifying workers with poor perfor-
mance on English language annotation, (3) iden-
tifying workers that were annotating at unrealis-
tic speeds, (4) identifying workers with near ran-
dom annotation distributions, and (5) identifying
workers that annotate each tweet for a given TV
show the same (or nearly the same) way. We man-
ually inspected any worker with low performance
on any of these tests before we made a final deci-
sion about using any of their annotations.
For further quality control, we also gathered ad-
ditional annotations from additional workers for
tweets where only one out of two workers iden-
tified an emotion. After these quality control steps
we defined minimum emotion annotation thresh-
olds to determine and assign preliminary emo-
tion labels to tweets. Note that some tweets were
discarded as mixed examples for each emotion
based upon thresholds for how many times they
were tagged, and it resulted in different number of
tweets in each emotion dataset. See Table 1 for the
statistics of the annotations collected from AMT.
Ground Truth Annotation: After we obtained
the annotated dataset from AMT, we posted the
same dataset (without the labels) to a group of ex-
pert annotators. The experts followed the same an-
notation guidelines, and each tweet was labeled by
at least two experts. When there was a disagree-
ment between two experts, they discussed to reach
an agreement or gathered additional opinion from
another expert to decide the label of a tweet. We
used this annotated dataset as ground truth. See
Table 2 for the statistics of the ground truth an-
notations. Compared with the ground truth, many
emotion bearing tweets were missed by the AMT
annotators, despite the quality control we applied.
It demonstrates the challenge of annotation by
crowdsourcing. The imbalanced class distribution
</bodyText>
<footnote confidence="0.99820775">
1The total number of tweets is lower than the AMT dataset
because the experts removed some off-topic tweets.
2Expert annotators had a Kappa agreement score of 0.639
before meeting to resolve their differences.
</footnote>
<page confidence="0.99652">
1109
</page>
<figure confidence="0.99957331707317">
0.58
Macro−averaged F1 Score
0.53
0.48
0.43
0.38
0.33
0.28
0 4500 9000 13500 18000 22500 27000
Number of Instances Re−annotated
0 4500 9000 13500 18000 22500 27000
Number of Instances Re−annotated
Macro−averaged MAP
0.64
0.60
0.56
0.52
0.48
0.44
0.40
0.36
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
Method ● Spread SVM−TF SVM−Delta−IDF
Method ● Spread SVM−TF SVM−Delta−IDF
(a) Macro-Averaged MAP (b) Macro-Averaged F1 Score
</figure>
<figureCaption confidence="0.998068">
Figure 1: Performance comparison of mislabeled instance selection methods. Classifiers become more accurate as more in-
stances are re-annotated. Spread achieves comparable performance with SVMs in terms of both MAP and F1 Score.
</figureCaption>
<bodyText confidence="0.999678869565218">
aggravates the confirmation bias – the minority
class examples are especially easy to miss when
labeling quickly due to their rare presence in the
dataset.
Evaluation Metric: We evaluated the results
with both Mean Average Precision (MAP) and F1
Score. Average Precision (AP) is the average of
the algorithm’s precision at every position in the
confidence ranked list of results where a true emo-
tional document has been identified. Thus, AP
places extra emphasis on getting the front of the
list correct. MAP is the mean of the average pre-
cision scores for each ranked list. This is highly
desirable for many practical application such as
intelligent search, recommendation, and target ad-
vertising where users almost never see results that
are not at the top of the list. F1 is a widely-used
measure of classification accuracy.
Methods: We evaluated the overall perfor-
mance relative to the common SVM bag of words
approach that can be ubiquitously found in text
mining literature. We implemented the following
four classification methods:
</bodyText>
<listItem confidence="0.922326428571429">
• Delta-IDF: Takes the dot product of the
Delta IDF weight vector (Formula 1) with the
document’s term frequency vector.
• Spread: Takes the dot product of the distri-
bution spread weight vector (Formula 3) with
the document’s term frequency vector. For
all the experiments, we used spread parame-
ter s = 2.
• SVM-TF: Uses a bag of words SVM with
term frequency weights.
• SVM-Delta-IDF: Uses a bag of words SVM
classification with TF.Delta-IDF weights
(Formula 2) in the feature vectors before
training or testing an SVM.
</listItem>
<bodyText confidence="0.999439903225807">
We employed each method to build the active
learner C described in Algorithm 1. We used
standard bag of unigram and bigram words rep-
resentation and topic-based fold cross validation.
Since in real world applications people are primar-
ily concerned with how well the algorithm will
work for new TV shows or movies that may not
be included in the training data, we defined a test
fold for each TV show or movie in our labeled data
set. Each test fold corresponded to a training fold
containing all the labeled data from all the other
TV shows and movies. We call it topic-based fold
cross validation.
We built the SVM classifiers using LIB-
LINEAR (Fan et al., 2008) and applied its
L2-regularized support vector regression model.
Based on the dot product or SVM regression
scores, we ranked the tweets by how strongly they
express the emotion. We selected the top m tweets
with the highest dot product or regression scores
but conflicting preliminary AMT labels as the sus-
pected mislabeled instances for re-annotation, just
as described in Algorithm 1. For the experimental
purpose, the re-annotation was done by assigning
the ground truth labels to the selected instances.
Since the dataset is highly imbalanced, we ap-
plied the under-sampling strategy when training
the classifiers.
Figure 1 compares the performance of differ-
ent approaches in each iteration after a certain
number of potentially mislabeled instances are re-
</bodyText>
<page confidence="0.969244">
1110
</page>
<bodyText confidence="0.999970960784314">
annotated. The X axis shows the total number
of data points that have been examined for each
emotion so far till the current iteration (i.e., 300,
900, 1800, 3000, 4500, 6900, 10500, 16500, and
26100). We reported both the macro-averaged
MAP (Figure 1a) and the macro-averaged F1
Score (Figure 1b) on eight emotions as the over-
all performance of three competitive methods –
Spread, SVM-Delta-IDF and SVM-TF. We have
also conducted experiments using Delta-IDF, but
its performance is low and not comparable with
the other three methods.
Generally, Figure 1 shows consistent perfor-
mance gains as more labels are corrected during
active learning. In comparison, SVM-Delta-IDF
significantly outperforms SVM-TF with respect
to both MAP and F1 Score. SVM-TF achieves
higher MAP and F1 Score than Spread at the first
few iterations, but then it is beat by Spread after
16,500 tweets had been selected and re-annotated
till the eighth iteration. Overall, at the end of the
active learning process, Spread outperforms SVM-
TF by 3.03% the MAP score (and by 4.29% the F1
score), and SVM-Delta-IDF outperforms SVM-
TF by 8.59% the MAP score (and by 5.26% the
F1 score). Spread achieves a F1 Score of 58.84%,
which is quite competitive compared to 59.82%
achieved by SVM-Delta-IDF, though SVM-Delta-
IDF outperforms Spread with respect to MAP.
Spread and Delta-IDF are superior with respect
to the time efficiency. Figure 2 shows the average
training time of the four methods on eight emo-
tions. The time spent training SVM-TF classi-
fiers is twice that of SVM-Delta-IDF classifiers,
12 times that of Spread classifiers, and 31 times
that of Delta-IDF classifiers. In our experiments,
on average, it took 258.8 seconds to train a SVM-
TF classifier for one emotion. In comparison, the
average training time of a Spread classifier was
only 21.4 seconds, and it required almost no pa-
rameter tuning. In total, our method Spread saved
up to (258.8 − 21.4) * 9 * 8 = 17092.8 seconds
(4.75 hours) over nine iterations of active learning
for all the eight emotions. This is enough time to
re-annotate thousands of data points.
The other important quantity to measure is an-
notation quality. One measure of improvement for
annotation quality is the number of mislabeled in-
stances that can be fixed after a certain number of
active learning iterations. Better methods can fix
more labels with fewer iterations.
</bodyText>
<figure confidence="0.9751674">
Average Traming Time (s) 00
100
0
Delta−IDF Spread SVM−Delta−IDF SVM−TF
Method
</figure>
<figureCaption confidence="0.999131166666667">
Figure 2: Average training time on eight emotions. Spread re-
quires only one-twelfth of the time spent to training an SVM-
TF classifier. Note that the time spent tuning the SVM’s pa-
rameters has not been included, but is considerable. Com-
pared with such computationally expensive methods, Spread
is more appropriate for use with active learning.
</figureCaption>
<figure confidence="0.99681575">
0 4500 9000 13500 18000 22500 27000
Number of Instances Re−annotated
● Spread SVM−TF SVM−Delta−IDF
Delta−IDF Random
</figure>
<figureCaption confidence="0.9848824">
Figure 3: Accumulated average percentage of fixed labels on
eight emotions. Spreading the feature weights reduces the
number of data points that must be examined in order to cor-
rect the mislabeled instances. SVMs require slightly fewer
points but take far longer to build.
</figureCaption>
<bodyText confidence="0.999636941176471">
Besides the four methods, we also implemented
a random baseline (Random) which randomly se-
lected the specified number of instances for re-
annotation in each round. We compared the im-
proved dataset with the final ground truth at the
end of each round to monitor the progress. Figure
3 reports the accumulated average percentage of
corrected labels on all emotions in each iteration
of the active learning process.
According to the figure, SVM-Delta-IDF and
SVM-TF are the most advantageous methods, fol-
lowed by Spread and Delta-IDF. After the last
iteration, SVM-Delta-IDF, SVM-TF, Spread and
Delta-IDF has fixed 85.23%, 85.85%, 81.05%
and 58.66% of the labels, respectively, all of
which significantly outperform the Random base-
line (29.74%).
</bodyText>
<figure confidence="0.999129809523809">
Percentage of Fixed Labels
100%
40%
20%
90%
80%
70%
60%
50%
30%
10%
0%
●
●
●●
●
●
●
●
●
Method
</figure>
<page confidence="0.98798">
1111
</page>
<sectionHeader confidence="0.997512" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989523809524">
In this paper, we explored an active learning ap-
proach to improve data annotation quality for
classification tasks. Instead of training the ac-
tive learner using computationally expensive tech-
niques (e.g., SVM-TF), we used a novel non-linear
distribution spreading algorithm. This algorithm
first weighs the features using the Delta-IDF tech-
nique, and then non-linearly spreads out the distri-
bution of the feature scores to enhance the model’s
ability to discriminate at the feature level. The
evaluation shows that our algorithm has the fol-
lowing advantages: (1) It intelligently ordered the
data points for annotators to annotate the most
likely errors first. The accuracy was at least com-
parable with computationally expensive baselines
(e.g. SVM-TF). (2) The algorithm trained and ran
much faster than SVM-TF, allowing annotators to
finish more annotations than competitors. (3) The
annotation process improved the dataset quality
by positively impacting the accuracy of classifiers
that were built upon it.
</bodyText>
<sectionHeader confidence="0.998733" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849367816092">
Carla E Brodley and Mark A Friedl. 1999. Identifying
mislabeled training data. Journal of Artificial Intel-
ligence Research, 11:131–167.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon’s
mechanical turk. In Proceedings of EMNLP, pages
286–295. ACL.
Paul Ekman. 1999. Basic emotions. Handbook of cog-
nition and emotion, 4:5–60.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Donghai Guan, Weiwei Yuan, Young-Koo Lee, and
Sungyoung Lee. 2011. Identifying mislabeled train-
ing data with the aid of unlabeled data. Applied In-
telligence, 35(3):345–358.
Wenxin Jiang. 2001. Some theoretical aspects of
boosting in the presence of noisy data. In Proceed-
ings of ICML. Citeseer.
Adam Tauman Kalaia and Rocco A Servediob. 2005.
Boosting in the presence of noise. Journal of Com-
puter and System Sciences, 71:266–290.
Amitava Karmaker and Stephen Kwek. 2006. A
boosting approach to remove class label noise. In-
ternational Journal of Hybrid Intelligent Systems,
3(3):169–177.
Aniket Kittur, Ed H Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk.
In Proceedings of CHI, pages 453–456. ACM.
Srivatsan Laxman, Sushil Mittal, and Ramarathnam
Venkatesan. 2013. Error correction in learning us-
ing svms. arXiv preprint arXiv:1301.2012.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In
Proceedings of ICWSM.
John Mingers. 1989. An empirical comparison of
pruning methods for decision tree induction. Ma-
chine learning, 4(2):227–243.
Fabrice Muhlenbach, St´ephane Lallich, and Djamel A
Zighed. 2004. Identifying and handling mislabelled
instances. Journal of Intelligent Information Sys-
tems, 22(1):89–109.
Robert Plutchik. 2001. The nature of emotions. Amer-
ican Scientist, 89(4):344–350.
Umaa Rebbapragada, Carla E Brodley, Damien Sulla-
Menashe, and Mark A Friedl. 2012. Active label
correction. In Proceedings of ICDM, pages 1080–
1085. IEEE.
James A Russell and Lisa Feldman Barrett. 1999. Core
affect, prototypical emotional episodes, and other
things called emotion: dissecting the elephant. Jour-
nal ofpersonality and social psychology, 76(5):805.
Burr Settles. 2010. Active learning literature sur-
vey. Technical Report 1648, University of Wiscon-
sin, Madison.
Victor S Sheng, Foster Provost, and Panagiotis G
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of KDD, pages 614–622. ACM.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254–263.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C
Baker. 2007. Internet-scale collection of human-
reviewed data. In Proceedings of WWW, pages 231–
240. ACM.
P Vannoorenberghe and T Denoeux. 2002. Handling
uncertain labels in multiclass problems using belief
decision trees. In Proceedings of IPMU, volume 3,
pages 1919–1926.
Sofie Verbaeten and Anneleen Van Assche. 2003. En-
semble methods for noise elimination in classifica-
tion problems. In Multiple classifier systems, pages
317–325. Springer.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P Sheth. 2012. Harnessing twitter” big
data” for automatic emotion identification. In Pro-
ceedings of SocialCom, pages 587–592. IEEE.
Xinchuan Zeng and Tony R Martinez. 2001. An al-
gorithm for correcting mislabeled data. Intelligent
data analysis, 5(6):491–502.
Xingquan Zhu, Xindong Wu, and Qijun Chen. 2003.
Eliminating class noise in large datasets. In Pro-
ceedings of ICML, pages 920–927.
</reference>
<page confidence="0.996332">
1112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.104125">
<title confidence="0.995788">Active Learning with Efficient Feature Weighting for Improving Data Quality and Classification Accuracy</title>
<author confidence="0.999557">Lu Doreen</author>
<affiliation confidence="0.660121">1,3Samsung Research America, Silicon</affiliation>
<address confidence="0.990880666666667">1,375 W Plumeria Dr. San Jose, CA 95134 2,4Kno.e.sis Center, Wright State 2,43640 Colonel Glenn Hwy. Fairborn, OH 45435</address>
<note confidence="0.2306685">1,3 2,4</note>
<abstract confidence="0.99988665">Many machine learning datasets are noisy with a substantial number of mislabeled instances. This noise yields sub-optimal classification performance. In this paper we study a large, low quality annotated dataset, created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations. We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost. Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques. Our techniques save a considerable amount of time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carla E Brodley</author>
<author>Mark A Friedl</author>
</authors>
<title>Identifying mislabeled training data.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--131</pages>
<contexts>
<context position="6997" citStr="Brodley and Friedl, 1999" startWordPosition="1072" endWordPosition="1075">y data. Kalaia and Servediob (2005) present a boosting algorithm which can achieve arbitrarily high accuracy in the presence of data noise. Karmaker and Kwek (2006) propose a modified AdaBoost algorithm – ORBoost, which minimizes the impact of outliers and becomes more tolerant to class label noise. One of the main disadvantages of noise tolerance techniques is that they are learning algorithm-dependent. In contrast, noise elimination/correction approaches are more generic and can be more easily applied to various problems. A large number of studies have explored noise elimination techniques (Brodley and Friedl, 1999; Verbaeten and Van Assche, 2003; Zhu et al., 2003; Muhlenbach et al., 2004; Guan et al., 2011), which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers. One widely used approach (Brodley and Friedl, 1999; Verbaeten and Van Assche, 2003) is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier. The si</context>
</contexts>
<marker>Brodley, Friedl, 1999</marker>
<rawString>Carla E Brodley and Mark A Friedl. 1999. Identifying mislabeled training data. Journal of Artificial Intelligence Research, 11:131–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>286--295</pages>
<publisher>ACL.</publisher>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk. In Proceedings of EMNLP, pages 286–295. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ekman</author>
</authors>
<title>Basic emotions. Handbook of cognition and emotion,</title>
<date>1999</date>
<pages>4--5</pages>
<contexts>
<context position="21439" citStr="Ekman, 1999" startWordPosition="3557" endWordPosition="3558">ies on IMDB. Spam tweets were filtered out using a set of heuristics and manually crafted rules. From the set of 2 billion tweets we randomly selected a small subset of 100K tweets about the 60 most highly mentioned TV shows and movies in the dataset. Tweets were randomly sampled for each show using the round robin algorithm. Duplicates were not allowed. This samples an equal number of tweets for each show. We then sent these tweets to Amazon Mechanical Turk for annotation. We defined our own set of emotions to annotate. The widely accepted emotion taxonomies, including Ekmans Basic Emotions (Ekman, 1999), Russells Circumplex model (Russell and Barrett, 1999), and Plutchiks emotion wheel (Plutchik, 2001), did not fit well for TV shows and Movies. For example, the emotion expressed by laughter is a very important emotion for TV shows and movies, but this emotion is not covered by the taxonomies listed above. After browsing through the raw dataset, reviewing the literature on emotion analysis, and considering the TV and movie problem domain, we decided to focus on eight emotions: funny, happy, sad, exciting, boring, angry, fear, and heartwarming. Emotion annotation is a non-trivial task that is </context>
</contexts>
<marker>Ekman, 1999</marker>
<rawString>Paul Ekman. 1999. Basic emotions. Handbook of cognition and emotion, 4:5–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="29669" citStr="Fan et al., 2008" startWordPosition="4911" endWordPosition="4914">learner C described in Algorithm 1. We used standard bag of unigram and bigram words representation and topic-based fold cross validation. Since in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set. Each test fold corresponded to a training fold containing all the labeled data from all the other TV shows and movies. We call it topic-based fold cross validation. We built the SVM classifiers using LIBLINEAR (Fan et al., 2008) and applied its L2-regularized support vector regression model. Based on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion. We selected the top m tweets with the highest dot product or regression scores but conflicting preliminary AMT labels as the suspected mislabeled instances for re-annotation, just as described in Algorithm 1. For the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances. Since the dataset is highly imbalanced, we applied the under-sampling strategy when trainin</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghai Guan</author>
<author>Weiwei Yuan</author>
<author>Young-Koo Lee</author>
<author>Sungyoung Lee</author>
</authors>
<title>Identifying mislabeled training data with the aid of unlabeled data.</title>
<date>2011</date>
<journal>Applied Intelligence,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="7092" citStr="Guan et al., 2011" startWordPosition="1089" endWordPosition="1092">ccuracy in the presence of data noise. Karmaker and Kwek (2006) propose a modified AdaBoost algorithm – ORBoost, which minimizes the impact of outliers and becomes more tolerant to class label noise. One of the main disadvantages of noise tolerance techniques is that they are learning algorithm-dependent. In contrast, noise elimination/correction approaches are more generic and can be more easily applied to various problems. A large number of studies have explored noise elimination techniques (Brodley and Friedl, 1999; Verbaeten and Van Assche, 2003; Zhu et al., 2003; Muhlenbach et al., 2004; Guan et al., 2011), which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers. One widely used approach (Brodley and Friedl, 1999; Verbaeten and Van Assche, 2003) is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier. The similar approach is adopted by Guan et al. (2011) and they further demonstrate that its performan</context>
</contexts>
<marker>Guan, Yuan, Lee, Lee, 2011</marker>
<rawString>Donghai Guan, Weiwei Yuan, Young-Koo Lee, and Sungyoung Lee. 2011. Identifying mislabeled training data with the aid of unlabeled data. Applied Intelligence, 35(3):345–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenxin Jiang</author>
</authors>
<title>Some theoretical aspects of boosting in the presence of noisy data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML. Citeseer.</booktitle>
<contexts>
<context position="5851" citStr="Jiang, 2001" startWordPosition="898" endWordPosition="899">Machines) but require significantly less time for training/running, which makes it well-suited for active learning. 2 Related Work Research on handling noisy dataset of mislabeled instances has focused on three major groups of techniques: (1) noise tolerance, (2) noise elimination, and (3) noise correction. Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant. Decision tree (Mingers, 1989; Vannoorenberghe and Denoeux, 2002) and boosting (Jiang, 2001; Kalaia and Servediob, 2005; Karmaker and Kwek, 2006) are two learning algorithms that have been investigated in many studies. Mingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise. Vannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set. Jiang (2001) studies some theoretical aspects of regression and classification boosting algorithms in dealing with noisy data. Kalaia and Servediob (2005) present a boosting algorithm which can ach</context>
</contexts>
<marker>Jiang, 2001</marker>
<rawString>Wenxin Jiang. 2001. Some theoretical aspects of boosting in the presence of noisy data. In Proceedings of ICML. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Tauman</author>
</authors>
<title>Kalaia and Rocco A Servediob.</title>
<date>2005</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>71--266</pages>
<marker>Tauman, 2005</marker>
<rawString>Adam Tauman Kalaia and Rocco A Servediob. 2005. Boosting in the presence of noise. Journal of Computer and System Sciences, 71:266–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amitava Karmaker</author>
<author>Stephen Kwek</author>
</authors>
<title>A boosting approach to remove class label noise.</title>
<date>2006</date>
<journal>International Journal of Hybrid Intelligent Systems,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="5905" citStr="Karmaker and Kwek, 2006" startWordPosition="904" endWordPosition="907">time for training/running, which makes it well-suited for active learning. 2 Related Work Research on handling noisy dataset of mislabeled instances has focused on three major groups of techniques: (1) noise tolerance, (2) noise elimination, and (3) noise correction. Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant. Decision tree (Mingers, 1989; Vannoorenberghe and Denoeux, 2002) and boosting (Jiang, 2001; Kalaia and Servediob, 2005; Karmaker and Kwek, 2006) are two learning algorithms that have been investigated in many studies. Mingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise. Vannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set. Jiang (2001) studies some theoretical aspects of regression and classification boosting algorithms in dealing with noisy data. Kalaia and Servediob (2005) present a boosting algorithm which can achieve arbitrarily high accuracy in the presence of data</context>
</contexts>
<marker>Karmaker, Kwek, 2006</marker>
<rawString>Amitava Karmaker and Stephen Kwek. 2006. A boosting approach to remove class label noise. International Journal of Hybrid Intelligent Systems, 3(3):169–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Ed H Chi</author>
<author>Bongwon Suh</author>
</authors>
<title>Crowdsourcing user studies with mechanical turk.</title>
<date>2008</date>
<booktitle>In Proceedings of CHI,</booktitle>
<pages>453--456</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1806" citStr="Kittur et al., 2008" startWordPosition="266" endWordPosition="269">mount of time. 1 Introduction Supervised classification algorithms require annotated data to teach the machine, by example, how to perform a specific task. There are generally two ways to collect annotations of a dataset: through a few expert annotators, or through crowdsourcing services (e.g., Amazon’s Mechanical Turk). High-quality annotations can be produced by expert annotators, but the process is usually slow and costly. The latter option is appealing since it creates a large annotated dataset at low cost. In recent years, there have been an increasing number of studies (Su et al., 2007; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; CallisonBurch, 2009) using crowdsourcing for data annotation. However, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and ∗This author’s research was done during an internship with Samsung Research America. unreliable, which significantly reduces the performance of the classification model. This is a challenge faced by many real world applications – given a large, quickly and cheaply created, low quality annotated dataset, how can one improve its quality and learn an accurate class</context>
</contexts>
<marker>Kittur, Chi, Suh, 2008</marker>
<rawString>Aniket Kittur, Ed H Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with mechanical turk. In Proceedings of CHI, pages 453–456. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srivatsan Laxman</author>
<author>Sushil Mittal</author>
<author>Ramarathnam Venkatesan</author>
</authors>
<title>Error correction in learning using svms. arXiv preprint arXiv:1301.2012.</title>
<date>2013</date>
<contexts>
<context position="9167" citStr="Laxman et al., 2013" startWordPosition="1411" endWordPosition="1415">fication accuracy in prior studies, but there are also some major drawbacks. For example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms. In addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier. The proposed approach does not suffer these limitations. Instead of eliminating the mislabeled examples 1105 from training data, some researchers (Zeng and Martinez, 2001; Rebbapragada et al., 2012; Laxman et al., 2013) propose to correct labeling errors either with or without consulting human experts. Zeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data. Laxman et al. (2012) propose an algorithm which first trains individual SVM classifiers on several small, class-balanced, random subsets of the dataset, and then reclassifies each training instance using a majority vote of these individual classifiers. However, the automatic correction may introduce new noise to the dataset by mistakenly changing a correct label to a wrong one. In</context>
</contexts>
<marker>Laxman, Mittal, Venkatesan, 2013</marker>
<rawString>Srivatsan Laxman, Sushil Mittal, and Ramarathnam Venkatesan. 2013. Error correction in learning using svms. arXiv preprint arXiv:1301.2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Martineau</author>
<author>Tim Finin</author>
</authors>
<title>Delta tfidf: An improved feature space for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="3865" citStr="Martineau and Finin, 2009" startWordPosition="598" endWordPosition="601"> select the most likely mislabeled instances. An intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels. The data points with the highest confidence scores but conflicting preliminary labels are most likely mislabeled. The algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets. Specifically, we propose a novel non-linear distribution spreading algorithm, which first uses Delta IDF technique (Martineau and Finin, 2009) to weight features, and then leverages the distribution of Delta IDF scores of a feature across different classes to efficiently recognize discriminative features for the classification task in the presence of mislabeled data. The idea is that some effective fea1104 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1104–1112, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tures may be subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance</context>
<context position="16689" citStr="Martineau and Finin, 2009" startWordPosition="2698" endWordPosition="2701">ey are to one corpus. The more biased a term is to one class, the higher (absolute value of) weight it will get. Delta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another. Each training instance (e.g., a document) is represented as a feature vector: xi = (w1,i, ..., w|V |,i), where each dimension in the vector corresponds to a n-gram term in vocabulary V = {ti, ..., t|V |}, |V |is the number of unique terms, and wj,i(1 G j G |V |) is the weight of term tj in instance xi. Delta IDF (Martineau and Finin, 2009) assigns score A idfj to term tj in V as: A idfj = log (N + 1)(Pj + 1) (1) (Nj + 1)(P + 1) where P (or N) is the number of positively (or negatively) labeled training instances, Pj (or Nj) is the number of positively (or negatively) labeled training instances with term tj. Simple add-one smoothing is used to smooth low frequency terms and prevent dividing by zero when a term appears in only one corpus. We calculate the Delta IDF score of every term in V , and get the Delta IDF weight vector A = (A idfi, ..., A idf|V |) for all terms. When the dataset is imblanced, to avoid building a biased mo</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>Justin Martineau and Tim Finin. 2009. Delta tfidf: An improved feature space for sentiment analysis. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Mingers</author>
</authors>
<title>An empirical comparison of pruning methods for decision tree induction.</title>
<date>1989</date>
<booktitle>Machine learning,</booktitle>
<pages>4--2</pages>
<contexts>
<context position="5789" citStr="Mingers, 1989" startWordPosition="890" endWordPosition="891">as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning. 2 Related Work Research on handling noisy dataset of mislabeled instances has focused on three major groups of techniques: (1) noise tolerance, (2) noise elimination, and (3) noise correction. Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant. Decision tree (Mingers, 1989; Vannoorenberghe and Denoeux, 2002) and boosting (Jiang, 2001; Kalaia and Servediob, 2005; Karmaker and Kwek, 2006) are two learning algorithms that have been investigated in many studies. Mingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise. Vannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set. Jiang (2001) studies some theoretical aspects of regression and classification boosting algorithms in dealing with noisy data. Kalaia a</context>
</contexts>
<marker>Mingers, 1989</marker>
<rawString>John Mingers. 1989. An empirical comparison of pruning methods for decision tree induction. Machine learning, 4(2):227–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrice Muhlenbach</author>
</authors>
<title>St´ephane Lallich, and Djamel A Zighed.</title>
<date>2004</date>
<journal>Journal of Intelligent Information Systems,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Muhlenbach, 2004</marker>
<rawString>Fabrice Muhlenbach, St´ephane Lallich, and Djamel A Zighed. 2004. Identifying and handling mislabelled instances. Journal of Intelligent Information Systems, 22(1):89–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Plutchik</author>
</authors>
<title>The nature of emotions.</title>
<date>2001</date>
<journal>American Scientist,</journal>
<volume>89</volume>
<issue>4</issue>
<contexts>
<context position="21540" citStr="Plutchik, 2001" startWordPosition="3570" endWordPosition="3571">rom the set of 2 billion tweets we randomly selected a small subset of 100K tweets about the 60 most highly mentioned TV shows and movies in the dataset. Tweets were randomly sampled for each show using the round robin algorithm. Duplicates were not allowed. This samples an equal number of tweets for each show. We then sent these tweets to Amazon Mechanical Turk for annotation. We defined our own set of emotions to annotate. The widely accepted emotion taxonomies, including Ekmans Basic Emotions (Ekman, 1999), Russells Circumplex model (Russell and Barrett, 1999), and Plutchiks emotion wheel (Plutchik, 2001), did not fit well for TV shows and Movies. For example, the emotion expressed by laughter is a very important emotion for TV shows and movies, but this emotion is not covered by the taxonomies listed above. After browsing through the raw dataset, reviewing the literature on emotion analysis, and considering the TV and movie problem domain, we decided to focus on eight emotions: funny, happy, sad, exciting, boring, angry, fear, and heartwarming. Emotion annotation is a non-trivial task that is typically time-consuming, expensive and errorprone. This task is difficult because: (1) There are mul</context>
</contexts>
<marker>Plutchik, 2001</marker>
<rawString>Robert Plutchik. 2001. The nature of emotions. American Scientist, 89(4):344–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umaa Rebbapragada</author>
<author>Carla E Brodley</author>
<author>Damien SullaMenashe</author>
<author>Mark A Friedl</author>
</authors>
<title>Active label correction.</title>
<date>2012</date>
<booktitle>In Proceedings of ICDM,</booktitle>
<pages>1080--1085</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9145" citStr="Rebbapragada et al., 2012" startWordPosition="1407" endWordPosition="1410">ve in increasing the classification accuracy in prior studies, but there are also some major drawbacks. For example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms. In addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier. The proposed approach does not suffer these limitations. Instead of eliminating the mislabeled examples 1105 from training data, some researchers (Zeng and Martinez, 2001; Rebbapragada et al., 2012; Laxman et al., 2013) propose to correct labeling errors either with or without consulting human experts. Zeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data. Laxman et al. (2012) propose an algorithm which first trains individual SVM classifiers on several small, class-balanced, random subsets of the dataset, and then reclassifies each training instance using a majority vote of these individual classifiers. However, the automatic correction may introduce new noise to the dataset by mistakenly changing a correct la</context>
<context position="11012" citStr="Rebbapragada et al., 2012" startWordPosition="1716" endWordPosition="1719">gies have been used to select the best points to annotate. These strategies can be generally divided into two groups: (1) selecting points in poorly sampled regions, and (2) selecting points that will have the greatest impact on models that were constructed using the dataset. Active learning for data cleaning differs from traditional active learning because the data already has low quality labels. It uses the difference between the low quality label for each data point and a prediction of the label using supervised machine learning models built upon the low quality labels. Unlike the work in (Rebbapragada et al., 2012), this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors, which we consider as a key challenge of this approach but ALC has not addressed. 3 An Active Learning Framework for Label Correction Let Dˆ = {(x1, y1), ..., (xn, yn)} be a dataset of binary labeled instances, where the instance xi belongs to domain X, and its label yi ∈ {−1, +1}. Dˆ contains an unknown number of mislabeled data points. The problem is to obtain a highquality dataset D by fixing labeling errors in ˆD, and learn an accurate classifier C from it. Algorithm </context>
</contexts>
<marker>Rebbapragada, Brodley, SullaMenashe, Friedl, 2012</marker>
<rawString>Umaa Rebbapragada, Carla E Brodley, Damien SullaMenashe, and Mark A Friedl. 2012. Active label correction. In Proceedings of ICDM, pages 1080– 1085. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James A Russell</author>
<author>Lisa Feldman Barrett</author>
</authors>
<title>Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant. Journal ofpersonality and social psychology,</title>
<date>1999</date>
<pages>76--5</pages>
<contexts>
<context position="21494" citStr="Russell and Barrett, 1999" startWordPosition="3562" endWordPosition="3565">t using a set of heuristics and manually crafted rules. From the set of 2 billion tweets we randomly selected a small subset of 100K tweets about the 60 most highly mentioned TV shows and movies in the dataset. Tweets were randomly sampled for each show using the round robin algorithm. Duplicates were not allowed. This samples an equal number of tweets for each show. We then sent these tweets to Amazon Mechanical Turk for annotation. We defined our own set of emotions to annotate. The widely accepted emotion taxonomies, including Ekmans Basic Emotions (Ekman, 1999), Russells Circumplex model (Russell and Barrett, 1999), and Plutchiks emotion wheel (Plutchik, 2001), did not fit well for TV shows and Movies. For example, the emotion expressed by laughter is a very important emotion for TV shows and movies, but this emotion is not covered by the taxonomies listed above. After browsing through the raw dataset, reviewing the literature on emotion analysis, and considering the TV and movie problem domain, we decided to focus on eight emotions: funny, happy, sad, exciting, boring, angry, fear, and heartwarming. Emotion annotation is a non-trivial task that is typically time-consuming, expensive and errorprone. Thi</context>
</contexts>
<marker>Russell, Barrett, 1999</marker>
<rawString>James A Russell and Lisa Feldman Barrett. 1999. Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant. Journal ofpersonality and social psychology, 76(5):805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2010</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin, Madison.</institution>
<contexts>
<context position="3077" citStr="Settles, 2010" startWordPosition="477" endWordPosition="478">ensive. To reduce the annotation effort, it is desirable to have an algorithm that selects the most likely mislabeled examples first for re-labeling. The process of selecting and re-labeling data points can be conducted with multiple rounds to iteratively improve the data quality. This is similar to the strategy of active learning. The basic idea of active learning is to learn an accurate classifier using less training data. An active learner uses a small set of labeled data to iteratively select the most informative instances from a large pool of unlabeled data for human annotators to label (Settles, 2010). In this work, we borrow the idea of active learning to interactively and iteratively correct labeling errors. The crucial step is to effectively and efficiently select the most likely mislabeled instances. An intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels. The data points with the highest confidence scores but conflicting preliminary labels are most likely mislabeled. The algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that r</context>
<context position="10243" citStr="Settles, 2010" startWordPosition="1590" endWordPosition="1591">ifiers. However, the automatic correction may introduce new noise to the dataset by mistakenly changing a correct label to a wrong one. In many scenarios, it is worth the effort and cost to fix the labeling errors by human experts, in order to obtain a high quality dataset that can be reused by the community. Rebbapragada et al. (2012) propose a solution called Active Label Correction (ALC) which iteratively presents the experts with small sets of suspected mislabeled instances at each round. Our work employs a similar framework that uses active learning for data cleaning. In Active Learning (Settles, 2010) a small set of labeled data is used to find documents that should be annotated from a large pool of unlabeled documents. Many different strategies have been used to select the best points to annotate. These strategies can be generally divided into two groups: (1) selecting points in poorly sampled regions, and (2) selecting points that will have the greatest impact on models that were constructed using the dataset. Active learning for data cleaning differs from traditional active learning because the data already has low quality labels. It uses the difference between the low quality label for</context>
</contexts>
<marker>Settles, 2010</marker>
<rawString>Burr Settles. 2010. Active learning literature survey. Technical Report 1648, University of Wisconsin, Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Get another label? improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>614--622</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1826" citStr="Sheng et al., 2008" startWordPosition="270" endWordPosition="273">oduction Supervised classification algorithms require annotated data to teach the machine, by example, how to perform a specific task. There are generally two ways to collect annotations of a dataset: through a few expert annotators, or through crowdsourcing services (e.g., Amazon’s Mechanical Turk). High-quality annotations can be produced by expert annotators, but the process is usually slow and costly. The latter option is appealing since it creates a large annotated dataset at low cost. In recent years, there have been an increasing number of studies (Su et al., 2007; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; CallisonBurch, 2009) using crowdsourcing for data annotation. However, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and ∗This author’s research was done during an internship with Samsung Research America. unreliable, which significantly reduces the performance of the classification model. This is a challenge faced by many real world applications – given a large, quickly and cheaply created, low quality annotated dataset, how can one improve its quality and learn an accurate classifier from it? Re-an</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. 2008. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of KDD, pages 614–622. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Su</author>
<author>Dmitry Pavlov</author>
<author>Jyh-Herng Chow</author>
<author>Wendell C Baker</author>
</authors>
<title>Internet-scale collection of humanreviewed data.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>231--240</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1785" citStr="Su et al., 2007" startWordPosition="262" endWordPosition="265"> a considerable amount of time. 1 Introduction Supervised classification algorithms require annotated data to teach the machine, by example, how to perform a specific task. There are generally two ways to collect annotations of a dataset: through a few expert annotators, or through crowdsourcing services (e.g., Amazon’s Mechanical Turk). High-quality annotations can be produced by expert annotators, but the process is usually slow and costly. The latter option is appealing since it creates a large annotated dataset at low cost. In recent years, there have been an increasing number of studies (Su et al., 2007; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; CallisonBurch, 2009) using crowdsourcing for data annotation. However, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and ∗This author’s research was done during an internship with Samsung Research America. unreliable, which significantly reduces the performance of the classification model. This is a challenge faced by many real world applications – given a large, quickly and cheaply created, low quality annotated dataset, how can one improve its quality and le</context>
</contexts>
<marker>Su, Pavlov, Chow, Baker, 2007</marker>
<rawString>Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C Baker. 2007. Internet-scale collection of humanreviewed data. In Proceedings of WWW, pages 231– 240. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vannoorenberghe</author>
<author>T Denoeux</author>
</authors>
<title>Handling uncertain labels in multiclass problems using belief decision trees.</title>
<date>2002</date>
<booktitle>In Proceedings of IPMU,</booktitle>
<volume>3</volume>
<pages>1919--1926</pages>
<contexts>
<context position="5825" citStr="Vannoorenberghe and Denoeux, 2002" startWordPosition="892" endWordPosition="895">tional expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning. 2 Related Work Research on handling noisy dataset of mislabeled instances has focused on three major groups of techniques: (1) noise tolerance, (2) noise elimination, and (3) noise correction. Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant. Decision tree (Mingers, 1989; Vannoorenberghe and Denoeux, 2002) and boosting (Jiang, 2001; Kalaia and Servediob, 2005; Karmaker and Kwek, 2006) are two learning algorithms that have been investigated in many studies. Mingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise. Vannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set. Jiang (2001) studies some theoretical aspects of regression and classification boosting algorithms in dealing with noisy data. Kalaia and Servediob (2005) present a boosti</context>
</contexts>
<marker>Vannoorenberghe, Denoeux, 2002</marker>
<rawString>P Vannoorenberghe and T Denoeux. 2002. Handling uncertain labels in multiclass problems using belief decision trees. In Proceedings of IPMU, volume 3, pages 1919–1926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sofie Verbaeten</author>
<author>Anneleen Van Assche</author>
</authors>
<title>Ensemble methods for noise elimination in classification problems.</title>
<date>2003</date>
<booktitle>In Multiple classifier systems,</booktitle>
<pages>317--325</pages>
<publisher>Springer.</publisher>
<marker>Verbaeten, Van Assche, 2003</marker>
<rawString>Sofie Verbaeten and Anneleen Van Assche. 2003. Ensemble methods for noise elimination in classification problems. In Multiple classifier systems, pages 317–325. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbo Wang</author>
<author>Lu Chen</author>
<author>Krishnaprasad Thirunarayan</author>
<author>Amit P Sheth</author>
</authors>
<title>Harnessing twitter” big data” for automatic emotion identification.</title>
<date>2012</date>
<booktitle>In Proceedings of SocialCom,</booktitle>
<pages>587--592</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="23389" citStr="Wang et al., 2012" startWordPosition="3872" endWordPosition="3875">on Mechanical Turk annotation label counts. Funny Happy Sad Exciting Boring Angry Fear Heartwarming # Pos. 1,781 4,847 788 1,613 216 763 285 326 # Neg. 88,277 91,075 84,031 78,573 82,416 56,584 46,622 15,542 # Total1 90,058 95,922 84,819 80,186 82,632 57,347 46,907 15,868 Table 2: Ground truth annotation label counts for each emotion.2 tors do not expect the next one to be either. Due to these reasons, there is a lack of sufficient and high quality labeled data for emotion research. Some researchers have studied harnessing Twitter hashtags to automatically create an emotion annotated dataset (Wang et al., 2012). In order to evaluate our approach in real world scenarios, instead of creating a high quality annotated dataset and then introducing artificial noise, we followed the common practice of crowdsoucing, and collected emotion annotations through Amazon Mechanical Turk (AMT). This AMT annotated dataset was used as the low quality dataset Dˆ in our evaluation. After that, the same dataset was annotated independently by a group of expert annotators to create the ground truth. We evaluate the proposed approach on two factors, the effectiveness of the models for emotion classification, and the improv</context>
</contexts>
<marker>Wang, Chen, Thirunarayan, Sheth, 2012</marker>
<rawString>Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan, and Amit P Sheth. 2012. Harnessing twitter” big data” for automatic emotion identification. In Proceedings of SocialCom, pages 587–592. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinchuan Zeng</author>
<author>Tony R Martinez</author>
</authors>
<title>An algorithm for correcting mislabeled data. Intelligent data analysis,</title>
<date>2001</date>
<pages>5--6</pages>
<contexts>
<context position="9118" citStr="Zeng and Martinez, 2001" startWordPosition="1403" endWordPosition="1406">emonstrated to be effective in increasing the classification accuracy in prior studies, but there are also some major drawbacks. For example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms. In addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier. The proposed approach does not suffer these limitations. Instead of eliminating the mislabeled examples 1105 from training data, some researchers (Zeng and Martinez, 2001; Rebbapragada et al., 2012; Laxman et al., 2013) propose to correct labeling errors either with or without consulting human experts. Zeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data. Laxman et al. (2012) propose an algorithm which first trains individual SVM classifiers on several small, class-balanced, random subsets of the dataset, and then reclassifies each training instance using a majority vote of these individual classifiers. However, the automatic correction may introduce new noise to the dataset by mista</context>
</contexts>
<marker>Zeng, Martinez, 2001</marker>
<rawString>Xinchuan Zeng and Tony R Martinez. 2001. An algorithm for correcting mislabeled data. Intelligent data analysis, 5(6):491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingquan Zhu</author>
<author>Xindong Wu</author>
<author>Qijun Chen</author>
</authors>
<title>Eliminating class noise in large datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>920--927</pages>
<contexts>
<context position="7047" citStr="Zhu et al., 2003" startWordPosition="1081" endWordPosition="1084">orithm which can achieve arbitrarily high accuracy in the presence of data noise. Karmaker and Kwek (2006) propose a modified AdaBoost algorithm – ORBoost, which minimizes the impact of outliers and becomes more tolerant to class label noise. One of the main disadvantages of noise tolerance techniques is that they are learning algorithm-dependent. In contrast, noise elimination/correction approaches are more generic and can be more easily applied to various problems. A large number of studies have explored noise elimination techniques (Brodley and Friedl, 1999; Verbaeten and Van Assche, 2003; Zhu et al., 2003; Muhlenbach et al., 2004; Guan et al., 2011), which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers. One widely used approach (Brodley and Friedl, 1999; Verbaeten and Van Assche, 2003) is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier. The similar approach is adopted by Guan et al. (2011) an</context>
</contexts>
<marker>Zhu, Wu, Chen, 2003</marker>
<rawString>Xingquan Zhu, Xindong Wu, and Qijun Chen. 2003. Eliminating class noise in large datasets. In Proceedings of ICML, pages 920–927.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>