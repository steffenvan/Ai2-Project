<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995263">
Using Masks, Suffix Array-based Data Structures and Multidimensional
Arrays to Compute Positional Ngram Statistics from Corpora
</title>
<author confidence="0.997229">
Alexandre Gil*
</author>
<affiliation confidence="0.9975585">
Computer Science Department
New University of Lisbon
</affiliation>
<address confidence="0.652095">
Caparica, Portugal
</address>
<email confidence="0.997465">
agil@pt.ibm.com
</email>
<author confidence="0.961761">
Gaël Dias
</author>
<affiliation confidence="0.996634">
Centre of Mathematics
Beira Interior University
</affiliation>
<address confidence="0.58221">
Covilhã, Portugal
</address>
<email confidence="0.997259">
ddg@di.ubi.pt
</email>
<sectionHeader confidence="0.996633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979863636364">
This paper describes an implementation to
compute positional ngram statistics (i.e. Fre-
quency and Mutual Expectation) based on
masks, suffix array-based data structures and
multidimensional arrays. Positional ngrams
are ordered sequences of words that represent
continuous or discontinuous substrings of a
corpus. In particular, the positional ngram
model has shown successful results for the ex-
traction of discontinuous collocations from
large corpora. However, its computation is
heavy. For instance, 4.299.742 positional
ngrams (n=1..7) can be generated from a
100.000-word size corpus in a seven-word
size window context. In comparison, only
700.000 ngrams would be computed for the
classical ngram model. It is clear that huge ef-
forts need to be made to process positional
ngram statistics in reasonable time and space.
Our solution shows O(h(F) N log N) time
complexity where N is the corpus size and
h(F) a function of the window context.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987409375">
Many models have been proposed to evaluate word de-
pendencies. One of the most successful statistical mod-
els is certainly the ngram model (Jelinek, 1990).
However, in order to overcome its conceptual rigidity,
T. Kuhn et al. (1994) have defined the polygram model
that estimates the probability of an ngram by interpolat-
ing the relative frequencies of all its kgrams (k ≤ n).
Another way to account for variable length dependen-
cies is the n-multigram model designed by Deligne and
Bimbot (1995).
All these models have in common the fact that they
need to compute continuous string frequencies. This
task can be colossal when gigabytes of data need to be
processed. Indeed, Yamamoto and Church (2000) show
that there exist N(N+1)/2 substrings in a N-size corpus.
That is the reason why low order ngrams have been
commonly used in Natural Language Processing appli-
cations.
In the specific field of multiword unit extraction, Dias
(2002) has introduced the positional ngram model that
has evidenced successful results for the extraction of
discontinuous collocations from large corpora. Unlikely
previous models, positional ngrams are ordered se-
quences of tokens that represent continuous or discon-
tinuous substrings of a corpus computed in a (2.F+1)-
word size window context (F represents the context in
terms of words on the right and on the left of any word
in the corpus). As a consequence, the number of gener-
ated substrings rapidly explodes and reaches astronomic
figures. Dias (2002) shows that ∆ (Equation 1) posi-
tional ngrams can be computed for an N-size corpus in a
(2.F+1)-size window context.
</bodyText>
<equation confidence="0.9415715">
2. F+1 F F 
∆=(N−2.F)×1+F+∑ ∑∑C−1Ck−i−1 1 j
 k=3 i=1 j=1 
Equation 1: Number of positional ngrams
</equation>
<bodyText confidence="0.987077594594595">
In order to illustrate this equation, 4.299.742 positional
ngrams (n=1..7) would be generated from a 100.000-
word size corpus in a seven-word size window context.
In comparison, only 700.000 ngrams would be com-
* The authors want to thank Professor José Gabriel Pereira Lopes from the New University of Lisbon for his advices.
puted for the classical ngram model. It is clear that huge
efforts need to be made to process positional ngram
statistics in reasonable time and space.
In this paper, we describe an implementation that com-
putes the Frequency and the Mutual Expectation (Dias
et al. 1999) of any positional ngram with time complex-
ity O(h(F) N log N). The global architecture is based on
the definition of masks that allow virtually representing
any positional ngram in the corpus. Thus, we follow the
Virtual Corpus approach introduced by Kit and Wilks
(1998) and apply a suffix-array-like method, coupled to
the Multikey Quicksort algorithm (Bentley and Sedge-
wick, 1997), to compute positional ngram frequencies.
Finally, a multidimensional array is built to easily
process the Mutual Expectation, an association measure
for collocation extraction.
The evaluation of our C++ implementation has been
realized over the CETEMPúblico2 corpus and shows
satisfactory results. For example, it takes 8.59 minutes
to compute both frequency and Mutual Expectation for
a 1.092.7233-word corpus on an Intel Pentium III 900
MHz Personal Computer for a seven-word size window
context.
This article is divided into four sections: (1) we explain
the basic principles of positional ngrams and the mask
representation to build the Virtual Corpus; (2) we pre-
sent the suffix-array-based data structure that allows
counting occurrences of positional ngrams; (3) we show
how a multidimensional array eases the efficient com-
putation of the Mutual Expectation; (4) we present re-
sults over different size sub-corpora of the
CETEMPúblico corpus.
</bodyText>
<sectionHeader confidence="0.987587" genericHeader="method">
2 Positional Ngrams
</sectionHeader>
<bodyText confidence="0.999184666666667">
In the specific field of multiword unit extraction, Dias
(2002) has introduced the positional ngram model that
has evidenced successful results for the extraction of
discontinuous collocations from large corpora.
words to the right of the same pivot word and the pivot
word itself). In general terms, a collocation can be de-
fined as a specific4 continuous or discontinuous se-
quence of words in a (2.F+1)-word size window context
(i.e. F words to the left of a pivot word, F words to the
right of the same pivot word and the pivot word itself).
This situation is illustrated in Figure 1 for the colloca-
tion Ngram Statistics that fits in the window context.
</bodyText>
<figure confidence="0.763388">
F=3
Virtual Approach to Deriving Ngram Statistics from Large Scale
pivot
</figure>
<figureCaption confidence="0.999942">
Figure 1: 2.F-word size window context
</figureCaption>
<bodyText confidence="0.999932555555555">
Thus, as computation is involved, we need to process all
possible substrings (continuous or discontinuous) that fit
inside the window context and contain the pivot word.
Any of these substrings is called a positional ngram. For
instance, [Ngram Statistics] is a positional ngram as is the
discontinuous sequence [Ngram ___ from] where the gap
represented by the underline stands for any word occur-
ring between Ngram and from (in this case, Statistics).
More examples are given in Table 1.
</bodyText>
<table confidence="0.4452224">
Positional 2grams Positional 3grams
[Ngram Statistics from]
[Ngram Statistics ___ Large]
[Ngram ___ from Large]
[to ___ Ngram ___ from]
</table>
<tableCaption confidence="0.99825">
Table 1: Possible positional ngrams
</tableCaption>
<bodyText confidence="0.974544428571429">
In order to compute all the positional ngrams of a cor-
pus, we need to take into account all the words as possi-
ble pivot words.
A B C D E F G H I J K L M N .... X Y Z ....
A B C D E F G H I J K L M N .... X Y Z ....
A B C D E F G H I J K L M N .... X Y Z ....
A B C D E F G H I J K L M N .... X Y Z ....
</bodyText>
<figure confidence="0.914931875">
F=3
[Ngram Statistics]
[Ngram ___ from]
[Ngram ___ ___ Large]
[to ___ Ngram]
2.1 Principles ....
A B C D E F G H I J K L M N .... X Y Z ....
....
</figure>
<bodyText confidence="0.999888">
The original idea of the positional ngram model comes
from the lexicographic evidence that most lexical rela-
tions associate words separated by at most five other
words (Sinclair, 1974). As a consequence, lexical rela-
tions such as collocations can be continuous or discon-
tinuous sequences of words in a context of at most
eleven words (i.e. 5 words to the left of a pivot word, 5
</bodyText>
<footnote confidence="0.949944333333333">
2 The CETEMPúblico is a 180 million-word corpus of Portuguese. It
can be obtained at http://www.ldc.upenn.edu/.
3 This represents 46.986.831 positional ngrams.
</footnote>
<figureCaption confidence="0.996373">
Figure 2: One-window context for F=3
</figureCaption>
<bodyText confidence="0.99905">
A simple way would be to shift the two-window context
to the right so that each word would sequentially be
processed. However, this would inevitably lead to du-
plications of positional ngrams. Instead, we propose a
</bodyText>
<footnote confidence="0.85257025">
4 As specific, we intend a sequence that fits the definition of colloca-
tion given by Dias (2002): “A collocation is a recurrent sequence of
words that co-occur together more than expected by chance in a given
domain”.
</footnote>
<bodyText confidence="0.999745545454546">
The representation of positional ngrams is an essential
step towards efficient computation. For that, purpose,
we propose a reference representation rather than an
explicit structure of each positional ngram. The idea is
to adapt the suffix representation (Manber and Myers,
1990) to the positional ngram case.
Following the suffix representation, any continuous
corpus substring is virtually represented by a single po-
sition of the corpus as illustrated in Figure 3. In fact, the
substring is the sequence of words that goes from the
word referred by the position till the end of the corpus.
</bodyText>
<figureCaption confidence="0.99709">
Figure 3: Suffix Representation5
</figureCaption>
<bodyText confidence="0.999935894736842">
Unfortunately, the suffix representation can not directly
be extended to the specific case of positional ngrams.
One main reason aims at this situation: a positional
ngram may represent a discontinuous sequence of
words. In order to overcome this situation, we propose a
representation of positional ngrams based on masks.
As we saw in the previous section, the computation of
all the positional ngrams is a repetitive process. For
each word in the corpus, there exists an algorithmic
pattern that identifies all the possible positional ngrams
in a 2.F+1-word size window context. So, what we need
is a way to represent this pattern in an elegant and effi-
cient way.
One way is to use a set of masks that identify all the
valid sequences of words in a given window context.
Thus, each mask is nothing more than a sequence of 1
and 0 (where 1 stands for a word and 0 for a gap) that
represents a specific positional ngram in the window
context. An example is illustrated in Figure 4.
</bodyText>
<figureCaption confidence="0.987594">
Figure 4: Masks
</figureCaption>
<sectionHeader confidence="0.415617" genericHeader="method">
5 The $ symbol stands for the end of the corpus.
</sectionHeader>
<bodyText confidence="0.927267352941176">
one-window context that shifts to the right along the
corpus as illustrated in Figure 2. It is clear that the size
of the new window should be 2.F+1.
This new representation implies new restrictions. While
all combinations of words were valid positional ngrams
in the two-window context, this is not true for a one-
window context. Indeed, two restrictions must be ob-
served.
Restriction 1: Any substring, in order to be valid, must
contain the first word of the window context.
Restriction 2: For any continuous or discontinuous sub-
string in the window context, by shifting the substring
from left to right, excluding gaps and words on the right
and inserting gaps on the left, so that there always exists
a word in the central position cpos (Equation 2) of the
window, there should be at least one shift that contains
all the words of the substring in the context window.
</bodyText>
<figure confidence="0.980709272727273">
1
1


2.F+
=
+
2
cpos


</figure>
<figureCaption confidence="0.447439">
Equation 2: Central position of the window
</figureCaption>
<bodyText confidence="0.6376115">
For example, from the first case of Figure 2, the discon-
tinuous sequence [A B _ _ E _ G] is not a positional
ngram although it is a possible substring as it does not
follow the second restriction. Indeed, whenever we try
to align the sequence to the central position, at least one
word is lost as shown in Table 2:
</bodyText>
<figure confidence="0.975609916666667">
Central
word
Disappearing
words
Possible
shift
[_ _ A B _ _ E]
[_ _ _ A B _ _]
B
A
G
E, G
</figure>
<tableCaption confidence="0.805685">
Table 2: Shifting Substrings
</tableCaption>
<bodyText confidence="0.997961857142857">
In contrast, the sequence [A _ C _ E F _] is a positional
ngram as the shift [_ A _ C _ E F], with C in the central
position, includes all the words of the substring.
Basically, the first restriction aims at avoiding duplica-
tions and the second restriction simply guarantees that
no substring that would not be computed in a two-
window context is processed.
</bodyText>
<figure confidence="0.99176024137931">
2.2 Virtual Representation
corpus
2
3
4
9
1
5
6
7
8
A
1
2
B
3
C
4
A
B
5
B
6
7
C
A
8
9
$
substrings
A B C A B B C A $
B C A B B C A $
A B B C A $
B B C A
B C A $
A $
$
C A B B C A
C A
$
$
$
1 2 3 4 5 6 7 8 9 10
X X X
A B C D E F
1
0
0
1
1
1 0
G H I J
...
F = 3
corpus
mask
ngram
A _ _D E F _
</figure>
<figureCaption confidence="0.999966">
Figure 6: Virtual Representation
</figureCaption>
<bodyText confidence="0.99983925">
As we will see in the following section, this reference
representation will allow us to follow the Virtual Cor-
pus approach introduced by Kit and Wilks (1998) to
compute ngram frequencies.
</bodyText>
<sectionHeader confidence="0.770624" genericHeader="method">
3 Computing Frequency
</sectionHeader>
<bodyText confidence="0.999214285714286">
With the Virtual Corpus approach, counting continuous
substrings can easily and efficiently be achieved. After
sorting the suffix-array data structure presented in Fig-
ure 3, the count of an ngram consisting of any n words
in the corpus is simply the count of the number of adja-
cent indices that take the n words as prefix. We illus-
trate the Virtual Corpus approach in Figure 6.
</bodyText>
<figure confidence="0.873235666666667">
2gram Freq 3gram Freq
[A B] 2 [A B B] 1
[B B] 1 [B C A] 2
</figure>
<figureCaption confidence="0.999475">
Figure 6: Virtual Corpus Approach
</figureCaption>
<bodyText confidence="0.999859428571429">
Counting positional ngrams can be computed exactly in
the same way. The suffix-array structure is sorted using
lexicographic ordering for each mask in the array of
masks. After sorting, the count of a positional ngram in
the corpus is simply the count of adjacent indices that
stand for the same sequence. We illustrate the Virtual
Corpus approach for positional ngrams in Figure 7.
</bodyText>
<figureCaption confidence="0.999184">
Figure 7: Virtual Corpus for positional ngrams
</figureCaption>
<figure confidence="0.95634575">
0 1 1
1 0 0
1 0 1
..
4 1 0 0
5 1 0 0
6 1 0 0
masks
{ {0,2} , 7 } = [ C _ _ F G H _ ]
7 1 0 0 1 1 1 0
8
9
10
1
1
1
</figure>
<table confidence="0.69184125">
1 0 0 1 1 1 1
1 0 1 0 0 0 0
1 0 1 0 0 1 0
..
</table>
<bodyText confidence="0.8852916">
Computing all the masks is an easy and quick process.
In our implementation, the generation of masks is done
recursively and is negligible in terms of space and time.
In table 3, we give the number of masks h(F) for differ-
ent values of F.
</bodyText>
<figure confidence="0.93326935">
F h(F)
1 4
2 11
3 43
4 171
5 683
Table 3: Number of masks
In order to identify each mask and to prepare the refer-
ence representation of positional ngrams, an array of
masks is finally built as in Figure 5.
mask F=3
..
4 1 0 0 1 0 1 1
5 1 0 0 1 1 0 0
6 1 0 0 1 1 0 1
7 1 0 0 1 1 1 0
8 1 0 0 1 1 1 1
9 1 0 1 0 0 0 0
10 1 0 1 0 0 1 0
..
</figure>
<figureCaption confidence="0.999942">
Figure 5: Masks Array
</figureCaption>
<bodyText confidence="0.993702111111111">
From these structures, the virtual representation of any
positional ngram is straightforward. Indeed, any posi-
tional ngram can be identified by a position in the cor-
pus and a given mask. Taking into account that a corpus
is a set of documents, any positional ngram can be rep-
resented by the tuple {{iddoc, posdoc}, idmask} where iddoc
stands for the document id of the corpus, posdoc for a
given position in the document and idmask for a specific
mask. An example is illustrated in Figure 6.
</bodyText>
<figure confidence="0.997199222222222">
pos
doc
corpus 0
...
A B C D
0 1 2 3
E F G H I J K L M
4 5 6 7 8 9 10 11 12
B B C A $
A
C A $
A B C A B B C A $
A $
B B
B C A B B C A $
B C A $
C A B B C A $
C A $
$
corpus
4
8
2
6
3
7
9
1
5
A
1 2
B
3
C
4
A
B
5
6
B
7
C
8
A
9
$
...
4 1 0 0 1 0 1 1
5 1 0 0 1 1 0 0
6 10 0 1 1 0 1
7 10 0 1 1 1 0
8 1 0 0 1 1 1 1
9 1 0 1 0 0 0 0
10 1 0 1 0 0 1 0
...
...
8
4
9
2
6
5
7
3
1
corpus
1
A
masks
2
B
3
C
4
A
5
B
6
B
7
C
8
A A A B C A B B C A $
9
10 11
12
... _ ... _ _ _ _
A _ A _ _ _ _
A _ B _ _ _ _
A _ B _ _ _ _
A _ C _ _ _ _
C _ A _ _ _ _
C _ B _ _ _ _
B _ A _ _ _ _
B _ A _ _ _ _
B _ C _ _ _ _
13 14
15 16
17 18
</figure>
<bodyText confidence="0.940222428571429">
Different reasons have lead to use the Multikey Quick-
sort algorithm. First, it performs independently from the
vocabulary size. Second, it shows O(N log N) time
complexity in our specific case. Third, Anderson and
Nilsson (1998) show that it performs better than the
MSD radixsort and proves comparable results to the
newly introduced Forward radixsort.
</bodyText>
<figureCaption confidence="0.530111333333333">
Counting frequencies is just a preliminary step towards
collocation extraction. The following step attaches an
association measure to each positional ngram that
evaluates the interdependency between words inside a
given sequence. In the positional ngram model, Dias et
al. (1999) propose the Mutual Expectation measure.
</figureCaption>
<figure confidence="0.652347571428571">
4 Computing Mutual Expectation
4.1 Principles
The Mutual Expectation evaluates the degree of rigidity
that links together all the words contained in a posi-
tional ngram (Vn, n &gt;_ 2) based on the concept of Nor-
malized Expectation and relative frequency.
Normalized Expectation
</figure>
<bodyText confidence="0.997872333333333">
The basic idea of the Normalized Expectation is to
evaluate the cost, in terms of cohesiveness, of the loss of
one word in a positional ngram. Thus, the Normalized
Expectation measure is defined in Equation 3 where the
function k(.) returns the frequency of any positional
ngram7.
</bodyText>
<equation confidence="0.975550571428571">
NE � � p u ...p u ... p u �
11 1 1i i 1n n � �
k � � p u ...p u ... p u
11 1 1i i 1n n� �
n ^ ^
p22u2 ... p2iw ... p2nunD+E(1p11W1 ... pii u+ ... p1.unij i � 2 / /
Equation 3: Normalized Expectation
</equation>
<bodyText confidence="0.999822857142857">
For that purpose, any positional ngram is defined alge-
braically as a vector of words [p11 u1 p12 u2 ... p1n un]
where ui stands for any word in the positional ngram
and p1i represents the distance that separates words u1
and ui8. Thus, the positional ngram [A _ C D E _ _] would
be rewritten as [0 A +2 C +3 D +4 E] and its Normalized
Expectation would be given by Equation 4.
</bodyText>
<equation confidence="0.980459333333333">
n
1 � � k � �
�
</equation>
<bodyText confidence="0.997716">
The efficiency of the counting mainly resides in the use
of an adapted sort algorithm. Kit and Wilks (1998) pro-
pose to use a bucket-radixsort although they acknowl-
edge that the classical quicksort performs faster for
large-vocabulary corpora. Around the same perspective,
Yamamoto and Church (2000) use the Manber and
Myers’s algorithm (1990), an elegant radixsort-based
algorithm that takes at most O(N log N) time and shows
improved results when long repeated substrings are
common in the corpus.
For the specific case of positional ngrams, we have cho-
sen to implement the Multikey Quicksort algorithm
(Bentley and Sedgewick, 1997) that can be seen as a
mixture of the Ternary-Split Quicksort (Bentley and
McIlroy, 1993) and the MSD6 radixsort (Anderson and
Nilsson, 1998).
The algorithm processes as follows: (1) the array of
string is partitioned into three parts based on the first
symbol of each string. In order to process the split a
pivot element is chosen just as in the classical quicksort
giving rise to: one part with elements smaller than the
pivot, one part with elements equal to the pivot and one
part with elements larger than the pivot; (2) the smaller
and the larger parts are recursively processed in exactly
the same manner as the whole array; (3) the equal part is
also sorted recursively but with partitioning starting
from the second symbol of each string; (4) the process
goes on recursively: each time an equal part is being
processed, the considered position in each string is
moved forward by one symbol.
In Figure 8, we propose an illustration of the Multikey
Quicksort taken from the paper (Bentley and Sedge-
wick, 1997). The pivot is chosen using the median
method.
</bodyText>
<figureCaption confidence="0.99829">
Figure 8: Sorting 12 two-letter words.
</figureCaption>
<figure confidence="0.9636030625">
n t
o
e
f r
s
t
y
i
b o
s
n
t
a h
e
Sorted array
Unsorted array
</figure>
<bodyText confidence="0.8013656">
as at be by he in is it of on or to
as is be by on in at it of he or to
7 The &amp;quot;^&amp;quot; corresponds to a convention used in Algebra that consists in
writing a &amp;quot;^&amp;quot; on the top of the omitted term of a given succession
indexed from 1 to n.
</bodyText>
<page confidence="0.718421">
8 By statement, any pii is equal to zero.
6 MSD stands for Most Significant Digit.
</page>
<equation confidence="0.959527666666667">
] ) ( [
k 0A 2 C 3 D 4 E
++ + ] )
NE( [ 0A 2 C 3 D 4 E
++ + = k ( [ 0 A 2 C 3 D
++ ] ) + 
 
1  k ( [ 0 A 2 C 4 E
++ ] ) + 
 ( [ ] ) 
4 k 0A 3 D 4 E
++ +
 ] ) 
( [
 k 0 C 1 D 2 E
++ 
which is equivalent to
 k([ A _ C D _ _ _])+ 
 
1  k([ A _ C _ E _ _])+ 
4 ( [ ] ) 
A _ _ D E _ _
k +
 ] ) 
( [
 k C D E _ _ _ _
Equation 4: Normalized Expectation example
] ) ( [
k A _ C D E _ _] )
NE( [ A _ C D E _ _ =
</equation>
<bodyText confidence="0.755728333333333">
However, to understand the Matrix itself, we first need
to show how the sub-ngrams of any positional ngram
can be represented.
</bodyText>
<table confidence="0.7463251">
Representing sub-ngrams
A sub-ngram is obtained by extracting one word at a
time from its related positional ngram as shown in Fig-
ure 9.
13 1 0 0 1 1 0 0
14 1 0 1 1 1 0 0
15 1 0 1 1 0 0 0 masks
16 1 0 1 0 1 0 0
17 1 0 1 0 0 0 0
18 1 1 1 0 0 0 0
</table>
<figure confidence="0.67871375">
corpus
0 1 2 3 4 5 6 7 8 9 ...
pos
doc
</figure>
<subsectionHeader confidence="0.929176">
Mutual Expectation
</subsectionHeader>
<bodyText confidence="0.9995215">
One effective criterion for multiword lexical unit identi-
fication is frequency. From this assumption, Dias et al.
(1999) pose that between two positional ngrams with
the same Normalized Expectation, the most frequent
positional ngram is more likely to be a collocation. So,
the Mutual Expectation of any positional ngram is de-
fined in Equation 5 based on its Normalized Expectation
and its relative frequency.
</bodyText>
<figure confidence="0.995466117647059">
A B C D E F G H I J ...
ngram {{0,0),14) A _ C D E _ _
sub-ngram 1 A _ C D _ _ _
sub-ngram 2 A _ C _ E _ _
sub-ngram 3 A _ _ D E _ _
sub-ngram 4 _ _ C D E _ _
delta=2
0
...
ME ( [ p u ...p u ... p u
11 1 1i i 1n n] )
p ( [ 11 1
p u ...p u ... p up u ...p u ... p u
1i i 1n n] ) ( [ 11 1
× NE 1n n] )
1i i
=
</figure>
<figureCaption confidence="0.9198">
Figure 9: Sub-ngrams
Equation 5: Mutual Expectation
</figureCaption>
<bodyText confidence="0.9998492">
In order to compute the Mutual Expectation of any posi-
tional ngram, it is necessary to build a data structure that
allows rapid and efficient search over the space of all
positional ngrams. For that purpose, we propose a mul-
tidimensional array structure called Matrix9.
</bodyText>
<subsectionHeader confidence="0.99134">
4.2 Matrix
</subsectionHeader>
<bodyText confidence="0.999915">
The attentive reader will have noticed that the denomi-
nator of the Normalized Expectation formula is the av-
erage frequency of all the positional (n-1)grams
included in a given positional ngram. These specific
positional ngrams are called positional sub-ngrams of
order n-110. So, in order to compute the Normalized
Expectation and a fortiori the Mutual Expectation, it is
necessary to access efficiently to the sub-ngrams fre-
quencies. This operation is done through the Matrix.
</bodyText>
<footnote confidence="0.8859744">
9 The Matrix also speeds up the extraction process that applies the
GenLocalMaxs algorithm (Gaël Dias, 2002). We do not present this
algorithm due to lack of space.
10 In order to ease the reading, we will use the term sub-ngrams to
denote positional sub-ngrams of order n-1.
</footnote>
<bodyText confidence="0.998362">
By representing a sub-ngram, we mean calculating its
virtual representation that identifies its related substring.
The previous figure shows that representing the first
three sub-ngrams of the positional ngram {{0,0},14} is
straightforward as they all contain the first word of the
window context. The only difficulty is to know the
mask they are associated to. Knowing this, the first three
sub-ngrams would respectively be represented as:
{{0,0},15}, {{0,0},16}, {{0,0},13}.
For the last sub-ngram, the situation is different. The
first word of the window context is omitted. As a con-
sequence, in order to calculate its virtual representation,
we need to know the position of the first word of the
substring as well as its corresponding mask. In this case,
the position in the document of the positional sub-ngram
is simply the position of its related positional ngram
plus the distance that separates the first word of the
window context from the first word of the substring. We
call delta this distance. The obvious representation of
the fourth sub-ngram is then {{0,2},18} where the position
is calculated as 0+(delta=2)=2.
In order to represent the sub-ngrams of any positional
ngram, all we need is to keep track of the masks related
to the mask of the positional ngram and the respective
deltas. Thus, it is clear that for each mask, there exists a
set of pairs {idmask, delta} that allows identifying all the
sub-ngrams of any given positional ngram. Each pair is
called a submask and is associated to its upper mask11 as
illustrated in Figure 10.
</bodyText>
<figureCaption confidence="0.784371">
Figure 10: Submasks
</figureCaption>
<bodyText confidence="0.9999064">
Now that all necessary virtual representations are well-
established, in order to calculate the Mutual Expecta-
tion, we need to build a structure that allows efficiently
accessing any positional ngram frequency. This is the
objective of the Matrix, a 2-dimension array structure.
</bodyText>
<subsectionHeader confidence="0.682989">
2-dimension Array Structure
</subsectionHeader>
<bodyText confidence="0.998449761904762">
Searching for specific positional ngrams in a huge sam-
ple space can be overwhelming. To overcome this com-
putation problem, two solutions are possible: (1) keep
the suffix array-based data structure and design opti-
mized search algorithms or (2) design a new data struc-
ture to ease the searching process. We chose the second
solution as our complete system heavily depends on
searching through the entire space of positional
ngrams12 and, as a consequence, we hardly believe that
improved results may be reached following the second
solution.
This new structure is a 2-dimension array where lines
stand for the masks ids and the columns for the posi-
tions in the corpus. Thus, each cell of the 2-dimension
array represents a given positional ngram as shown in
Figure 11. This structure is called the Matrix.
The frequency of each positional ngram can easily be
represented by all its positions in the corpus. Indeed, a
given positional ngram is a substring that can appear in
different positions of the corpus being the count of these
positions its frequency. From the previous suffix array-
</bodyText>
<footnote confidence="0.9791502">
11 The upper mask is the mask from which the submasks are calcu-
lated. While upper masks represent positional ngrams, submasks
represent sub-ngrams.
12 In fact, this choice mainly has to do with the extraction process and
the application of the GenLocalMaxs algorithm.
</footnote>
<bodyText confidence="0.998951166666667">
based data structure, calculating all these positions is
straightforward.
Calculating the Mutual Expectation is also straightfor-
ward and fast as accessing to any positional ngram can
be done in O(1) time complexity. We will illustrate this
reality in the next section.
</bodyText>
<figure confidence="0.511386">
mask
</figure>
<figureCaption confidence="0.998679">
Figure 11: The Matrix
</figureCaption>
<bodyText confidence="0.9997">
The illustration of our architecture is now complete. We
now need to test our assumptions. For that purpose, we
present results of our implementation over the
CETEMPúblico corpus.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.993503857142857">
We have conducted a number of experiments of our
C++ implementation on the CETEMPúblico Portuguese
corpus to derive positional ngram statistics (Frequency
and Mutual Expectation). The experiments have been
realized on an Intel Pentium 900 MHz PC with 390MB
of RAM. From the original corpus, we have randomly
defined 5 different size sub-corpora that we present in
</bodyText>
<tableCaption confidence="0.85708">
Table 4.
</tableCaption>
<table confidence="0.999656">
corpus 01 02 03 04 05
Size in 0.7 3.1 5.3 6.7 8.8
Mb
# of 114.373 506.259 864.790 1.092.723 1.435.930
words
# of 4.917.781 21.768.879 37.185.712 46.986.831 61.744.732
ngrams13
</table>
<tableCaption confidence="0.999383">
Table 4: Sub-corpora
</tableCaption>
<bodyText confidence="0.999778125">
For each sub-corpus we have calculated the execution
time of different stages of the process: (1) the tokeniza-
tion that transforms the corpus into a set of integers; (2)
the preparation of the mask structure and the construc-
tion of the suffix-array data structure; (3) the sorting of
the suffix-array data structure and the creation of the
Matrix; (4) the calculation of the ME. The results are
given in Table 5.
</bodyText>
<page confidence="0.904877">
13 The window context of the experiment is F=3.
</page>
<figure confidence="0.980917411764706">
mask
mask
mask
2.F+1
idmask delta
submasks
M’
N
...
...
...
...
...
...
pos
ME
Pos
</figure>
<table confidence="0.965853666666667">
corpus 01 02 03 04 05
Tokeniz. 0:00:01 0:00:04 0:00:08 0:00:09 0:00:17
Masks/Suffix 0:00:04 0:00:14 0:00:25 0:00:31 0:00:40
Matrix 0:00:35 0:03:23 0:06:16 0:08:11 0:11:12
ME 0:00:00 0:00:03 0:00:06 0:00:08 0:00:10
total 0:00:40 0:03:44 0:06:55 0:08:59 0:12:19
</table>
<tableCaption confidence="0.999207">
Table 5: Execution Time in (hh:mm:ss)
</tableCaption>
<bodyText confidence="0.999857125">
The results clearly show that the construction of the
Matrix and the sort operation over the suffix-array data
structure are the most time consuming procedures. On
the contrary, the computation of the Mutual Expectation
is quick due to the direct access to sub-ngrams frequen-
cies enabled by the Matrix. In order to understand the
evolution of the results, we present, in Figure 12, a
graphical representation of the results.
</bodyText>
<figureCaption confidence="0.992717">
Figure 12: Evolution of execution time
</figureCaption>
<bodyText confidence="0.999866">
The graphical representation illustrates a linear time
complexity. In fact, Alexandre Gil (2002) has proved
that, mainly due to the implementation of the Multikey
Quicksort algorithm, our implementation evidences a
time complexity of O(h(F) N log N) where N is the size
of the corpus and h(F) a function of the window con-
text.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.969862">
In this paper, we have described an implementation to
compute positional ngram statistics based on masks,
suffix array-based data structure and multidimensional
arrays. Our C++ solution shows that it takes 8.59 min-
utes to compute both frequency and Mutual Expectation
for a 1.092.723-word corpus on an Intel Pentium III 900
MHz for a seven-word size window context. In fact, our
architecture evidences O(h(F) N log N) time complex-
ity. To some extent, this work proposes a response to
the conclusion of (Kit and Wilks, 1998) that claims that
“[...] a utility for extracting discontinuous co-
occurrences of corpus tokens, of any distance from each
other, can be implemented based on this program [The
Virtual Corpus Approach]”.
</bodyText>
<sectionHeader confidence="0.974908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99519354">
Alexandre Gil. 2002. Extracção eficiente de padrões textuais
utilizando algoritmos e estruturas de dados avançadas.
Master Thesis, New University of Lisbon, Portugal.
Arne Anderson and Stefan Nilsson. 1998. Implementing
Radixsort. ACM Journal of Experimental Algorithmics,
Vol. 3. citeseer.nj.nec.com/79696.html
Chunyu Kit and Yorick Wilks. 1998. The Virtual Approach to
Deriving Ngram Statistics from Large Scale Corpora. In-
ternational Conference on Chinese Information Processing
Conference, Beijing, China, 223-229. cite-
seer.nj.nec.com/kit98virtual.html.
Gaël Dias, Sylvie Guilloré, and José Lopes. 1999. Language
Independent Automatic Acquisition of Rigid Multiword
Units from Unrestricted Text corpora. Traitement Automa-
tique des Langues Naturelles, Institut d’Etudes Scientifi-
ques, Cargèse, France, 333-339.
www.di.ubi.pt/~ddg/publications/taln1999.ps.gz
Gaël Dias 2002. Extraction Automatique d’Associations Lexi-
cales à partir de Corpora. PhD Thesis. New University of
Lisbon (Portugal) and University of Orléans (France).
www.di.ubi.pt/~ddg/publications/thesis.pdf.gz
John Sinclair. 1974. English Lexical Collocations: A study in
computational linguistics. Singapore, reprinted as chapter 2
of Foley, J. A. (ed). 1996, John Sinclair on Lexis and Lexi-
cography, Uni Press.
Jon Bentley and Robert Sedgewick. 1997. Fast Algorithms for
Sorting and Searching Strings. 8th Annual ACM-SIAM
Symposium on Discrete Algorithms, New Orléans. cite-
seer.nj.nec.com/bentley97fast.html.
Jon Bentley and Douglas McIlroy. 1993. Engineering a sort
function. Software - Practice and Experience, 23(11):1249-
1265.
Mikio Yamamoto and Kenneth Church. 2000. Using Suffix
Arrays to Compute Term Frequency and Document Fre-
quency for All Substrings in a corpus. Association for
Computational Linguistics, 27(1):1-30.
www.research.att.com/~kwc/CL_suffix_array.pdf
Sabine Deligne and Frédéric Bimbot. 1995. Language Model-
ling by Variable Length Sequences: Theoretical Formula-
tion and Evaluation of Multigrams. ICASSP-95. Detroit,
Michigan, 1:169-172. cite-
seer.nj.nec.com/deligne95language.html
T. Kuhn, H. Nieman, E.G. Schukat-Talamazzini. 1994. Er-
godic Hidden Markov Models and Polygrams for Language
Modelling. ICASSP-94, 1:357-360. cite-
seer.nj.nec.com/kuhn94ergodic.html
Udi Manber and Gene Myers. 1990. Suffix-arrays: A new
method for on-line string searches. First Annual ACM-
SIAM Symposium on Discrete Algorithms. 319-327.
www.cs.arizona.edu/people/udi/suffix.ps
</reference>
<figure confidence="0.996262785714286">
Execution Time (hh:mm:ss)
0:12:
0:11:31
0:10:
0:08:
0:07:12
0:05:
0:04:19
0:02:
0:01:
0:00:
Pentium III, 900 MHz, 390 MB
114373 506259 864790 1092723 1435930
# of words in the corpus
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.218828">
<title confidence="0.719964">Using Masks, Suffix Array-based Data Structures and Arrays to Compute Positional Ngram Statistics from Corpora</title>
<affiliation confidence="0.9864905">Computer Science New University of</affiliation>
<address confidence="0.947899">Caparica, Portugal</address>
<email confidence="0.999702">agil@pt.ibm.com</email>
<author confidence="0.532396">Gaël</author>
<affiliation confidence="0.9925605">Centre of Beira Interior</affiliation>
<address confidence="0.998819">Covilhã, Portugal</address>
<email confidence="0.99487">ddg@di.ubi.pt</email>
<abstract confidence="0.998473391304348">This paper describes an implementation to compute positional ngram statistics (i.e. Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays. Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus. In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora. However, its computation is heavy. For instance, 4.299.742 positional ngrams (n=1..7) can be generated from a 100.000-word size corpus in a seven-word size window context. In comparison, only 700.000 ngrams would be computed for the classical ngram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. solution shows N log time where the corpus size and function of the window context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandre Gil</author>
</authors>
<title>Extracção eficiente de padrões textuais utilizando algoritmos e estruturas de dados avançadas. Master Thesis,</title>
<date>2002</date>
<institution>New University of Lisbon, Portugal.</institution>
<contexts>
<context position="26823" citStr="Gil (2002)" startWordPosition="5103" endWordPosition="5104">:59 0:12:19 Table 5: Execution Time in (hh:mm:ss) The results clearly show that the construction of the Matrix and the sort operation over the suffix-array data structure are the most time consuming procedures. On the contrary, the computation of the Mutual Expectation is quick due to the direct access to sub-ngrams frequencies enabled by the Matrix. In order to understand the evolution of the results, we present, in Figure 12, a graphical representation of the results. Figure 12: Evolution of execution time The graphical representation illustrates a linear time complexity. In fact, Alexandre Gil (2002) has proved that, mainly due to the implementation of the Multikey Quicksort algorithm, our implementation evidences a time complexity of O(h(F) N log N) where N is the size of the corpus and h(F) a function of the window context. 6 Conclusion In this paper, we have described an implementation to compute positional ngram statistics based on masks, suffix array-based data structure and multidimensional arrays. Our C++ solution shows that it takes 8.59 minutes to compute both frequency and Mutual Expectation for a 1.092.723-word corpus on an Intel Pentium III 900 MHz for a seven-word size window</context>
</contexts>
<marker>Gil, 2002</marker>
<rawString>Alexandre Gil. 2002. Extracção eficiente de padrões textuais utilizando algoritmos e estruturas de dados avançadas. Master Thesis, New University of Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Anderson</author>
<author>Stefan Nilsson</author>
</authors>
<title>Implementing Radixsort.</title>
<date>1998</date>
<journal>ACM Journal of Experimental Algorithmics,</journal>
<volume>3</volume>
<pages>79696</pages>
<contexts>
<context position="14755" citStr="Anderson and Nilsson (1998)" startWordPosition="2863" endWordPosition="2866">B 5 6 B 7 C 8 A 9 $ ... 4 1 0 0 1 0 1 1 5 1 0 0 1 1 0 0 6 10 0 1 1 0 1 7 10 0 1 1 1 0 8 1 0 0 1 1 1 1 9 1 0 1 0 0 0 0 10 1 0 1 0 0 1 0 ... ... 8 4 9 2 6 5 7 3 1 corpus 1 A masks 2 B 3 C 4 A 5 B 6 B 7 C 8 A A A B C A B B C A $ 9 10 11 12 ... _ ... _ _ _ _ A _ A _ _ _ _ A _ B _ _ _ _ A _ B _ _ _ _ A _ C _ _ _ _ C _ A _ _ _ _ C _ B _ _ _ _ B _ A _ _ _ _ B _ A _ _ _ _ B _ C _ _ _ _ 13 14 15 16 17 18 Different reasons have lead to use the Multikey Quicksort algorithm. First, it performs independently from the vocabulary size. Second, it shows O(N log N) time complexity in our specific case. Third, Anderson and Nilsson (1998) show that it performs better than the MSD radixsort and proves comparable results to the newly introduced Forward radixsort. Counting frequencies is just a preliminary step towards collocation extraction. The following step attaches an association measure to each positional ngram that evaluates the interdependency between words inside a given sequence. In the positional ngram model, Dias et al. (1999) propose the Mutual Expectation measure. 4 Computing Mutual Expectation 4.1 Principles The Mutual Expectation evaluates the degree of rigidity that links together all the words contained in a pos</context>
<context position="17123" citStr="Anderson and Nilsson, 1998" startWordPosition="3281" endWordPosition="3284">ugh they acknowledge that the classical quicksort performs faster for large-vocabulary corpora. Around the same perspective, Yamamoto and Church (2000) use the Manber and Myers’s algorithm (1990), an elegant radixsort-based algorithm that takes at most O(N log N) time and shows improved results when long repeated substrings are common in the corpus. For the specific case of positional ngrams, we have chosen to implement the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997) that can be seen as a mixture of the Ternary-Split Quicksort (Bentley and McIlroy, 1993) and the MSD6 radixsort (Anderson and Nilsson, 1998). The algorithm processes as follows: (1) the array of string is partitioned into three parts based on the first symbol of each string. In order to process the split a pivot element is chosen just as in the classical quicksort giving rise to: one part with elements smaller than the pivot, one part with elements equal to the pivot and one part with elements larger than the pivot; (2) the smaller and the larger parts are recursively processed in exactly the same manner as the whole array; (3) the equal part is also sorted recursively but with partitioning starting from the second symbol of each </context>
</contexts>
<marker>Anderson, Nilsson, 1998</marker>
<rawString>Arne Anderson and Stefan Nilsson. 1998. Implementing Radixsort. ACM Journal of Experimental Algorithmics, Vol. 3. citeseer.nj.nec.com/79696.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Yorick Wilks</author>
</authors>
<title>The Virtual Approach to Deriving Ngram Statistics from Large</title>
<date>1998</date>
<booktitle>Scale Corpora. International Conference on Chinese Information Processing Conference,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="3865" citStr="Kit and Wilks (1998)" startWordPosition="607" endWordPosition="610"> José Gabriel Pereira Lopes from the New University of Lisbon for his advices. puted for the classical ngram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. In this paper, we describe an implementation that computes the Frequency and the Mutual Expectation (Dias et al. 1999) of any positional ngram with time complexity O(h(F) N log N). The global architecture is based on the definition of masks that allow virtually representing any positional ngram in the corpus. Thus, we follow the Virtual Corpus approach introduced by Kit and Wilks (1998) and apply a suffix-array-like method, coupled to the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997), to compute positional ngram frequencies. Finally, a multidimensional array is built to easily process the Mutual Expectation, an association measure for collocation extraction. The evaluation of our C++ implementation has been realized over the CETEMPúblico2 corpus and shows satisfactory results. For example, it takes 8.59 minutes to compute both frequency and Mutual Expectation for a 1.092.7233-word corpus on an Intel Pentium III 900 MHz Personal Computer for a seven-word size win</context>
<context position="11722" citStr="Kit and Wilks (1998)" startWordPosition="2070" endWordPosition="2073">uplications and the second restriction simply guarantees that no substring that would not be computed in a twowindow context is processed. 2.2 Virtual Representation corpus 2 3 4 9 1 5 6 7 8 A 1 2 B 3 C 4 A B 5 B 6 7 C A 8 9 $ substrings A B C A B B C A $ B C A B B C A $ A B B C A $ B B C A B C A $ A $ $ C A B B C A C A $ $ $ 1 2 3 4 5 6 7 8 9 10 X X X A B C D E F 1 0 0 1 1 1 0 G H I J ... F = 3 corpus mask ngram A _ _D E F _ Figure 6: Virtual Representation As we will see in the following section, this reference representation will allow us to follow the Virtual Corpus approach introduced by Kit and Wilks (1998) to compute ngram frequencies. 3 Computing Frequency With the Virtual Corpus approach, counting continuous substrings can easily and efficiently be achieved. After sorting the suffix-array data structure presented in Figure 3, the count of an ngram consisting of any n words in the corpus is simply the count of the number of adjacent indices that take the n words as prefix. We illustrate the Virtual Corpus approach in Figure 6. 2gram Freq 3gram Freq [A B] 2 [A B B] 1 [B B] 1 [B C A] 2 Figure 6: Virtual Corpus Approach Counting positional ngrams can be computed exactly in the same way. The suffi</context>
<context position="16456" citStr="Kit and Wilks (1998)" startWordPosition="3178" endWordPosition="3181"> i 1n n� � n ^ ^ p22u2 ... p2iw ... p2nunD+E(1p11W1 ... pii u+ ... p1.unij i � 2 / / Equation 3: Normalized Expectation For that purpose, any positional ngram is defined algebraically as a vector of words [p11 u1 p12 u2 ... p1n un] where ui stands for any word in the positional ngram and p1i represents the distance that separates words u1 and ui8. Thus, the positional ngram [A _ C D E _ _] would be rewritten as [0 A +2 C +3 D +4 E] and its Normalized Expectation would be given by Equation 4. n 1 � � k � � � The efficiency of the counting mainly resides in the use of an adapted sort algorithm. Kit and Wilks (1998) propose to use a bucket-radixsort although they acknowledge that the classical quicksort performs faster for large-vocabulary corpora. Around the same perspective, Yamamoto and Church (2000) use the Manber and Myers’s algorithm (1990), an elegant radixsort-based algorithm that takes at most O(N log N) time and shows improved results when long repeated substrings are common in the corpus. For the specific case of positional ngrams, we have chosen to implement the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997) that can be seen as a mixture of the Ternary-Split Quicksort (Bentley and</context>
</contexts>
<marker>Kit, Wilks, 1998</marker>
<rawString>Chunyu Kit and Yorick Wilks. 1998. The Virtual Approach to Deriving Ngram Statistics from Large Scale Corpora. International Conference on Chinese Information Processing Conference, Beijing, China, 223-229. citeseer.nj.nec.com/kit98virtual.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaël Dias</author>
<author>Sylvie Guilloré</author>
<author>José Lopes</author>
</authors>
<title>Language Independent Automatic Acquisition of Rigid Multiword Units from Unrestricted Text corpora.</title>
<date>1999</date>
<booktitle>Traitement Automatique des Langues Naturelles, Institut d’Etudes Scientifiques,</booktitle>
<pages>333--339</pages>
<location>Cargèse,</location>
<contexts>
<context position="3594" citStr="Dias et al. 1999" startWordPosition="562" endWordPosition="565">f positional ngrams In order to illustrate this equation, 4.299.742 positional ngrams (n=1..7) would be generated from a 100.000- word size corpus in a seven-word size window context. In comparison, only 700.000 ngrams would be com* The authors want to thank Professor José Gabriel Pereira Lopes from the New University of Lisbon for his advices. puted for the classical ngram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. In this paper, we describe an implementation that computes the Frequency and the Mutual Expectation (Dias et al. 1999) of any positional ngram with time complexity O(h(F) N log N). The global architecture is based on the definition of masks that allow virtually representing any positional ngram in the corpus. Thus, we follow the Virtual Corpus approach introduced by Kit and Wilks (1998) and apply a suffix-array-like method, coupled to the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997), to compute positional ngram frequencies. Finally, a multidimensional array is built to easily process the Mutual Expectation, an association measure for collocation extraction. The evaluation of our C++ implementati</context>
<context position="15160" citStr="Dias et al. (1999)" startWordPosition="2922" endWordPosition="2925">ns have lead to use the Multikey Quicksort algorithm. First, it performs independently from the vocabulary size. Second, it shows O(N log N) time complexity in our specific case. Third, Anderson and Nilsson (1998) show that it performs better than the MSD radixsort and proves comparable results to the newly introduced Forward radixsort. Counting frequencies is just a preliminary step towards collocation extraction. The following step attaches an association measure to each positional ngram that evaluates the interdependency between words inside a given sequence. In the positional ngram model, Dias et al. (1999) propose the Mutual Expectation measure. 4 Computing Mutual Expectation 4.1 Principles The Mutual Expectation evaluates the degree of rigidity that links together all the words contained in a positional ngram (Vn, n &gt;_ 2) based on the concept of Normalized Expectation and relative frequency. Normalized Expectation The basic idea of the Normalized Expectation is to evaluate the cost, in terms of cohesiveness, of the loss of one word in a positional ngram. Thus, the Normalized Expectation measure is defined in Equation 3 where the function k(.) returns the frequency of any positional ngram7. NE </context>
<context position="19440" citStr="Dias et al. (1999)" startWordPosition="3845" endWordPosition="3848">xpectation example ] ) ( [ k A _ C D E _ _] ) NE( [ A _ C D E _ _ = However, to understand the Matrix itself, we first need to show how the sub-ngrams of any positional ngram can be represented. Representing sub-ngrams A sub-ngram is obtained by extracting one word at a time from its related positional ngram as shown in Figure 9. 13 1 0 0 1 1 0 0 14 1 0 1 1 1 0 0 15 1 0 1 1 0 0 0 masks 16 1 0 1 0 1 0 0 17 1 0 1 0 0 0 0 18 1 1 1 0 0 0 0 corpus 0 1 2 3 4 5 6 7 8 9 ... pos doc Mutual Expectation One effective criterion for multiword lexical unit identification is frequency. From this assumption, Dias et al. (1999) pose that between two positional ngrams with the same Normalized Expectation, the most frequent positional ngram is more likely to be a collocation. So, the Mutual Expectation of any positional ngram is defined in Equation 5 based on its Normalized Expectation and its relative frequency. A B C D E F G H I J ... ngram {{0,0),14) A _ C D E _ _ sub-ngram 1 A _ C D _ _ _ sub-ngram 2 A _ C _ E _ _ sub-ngram 3 A _ _ D E _ _ sub-ngram 4 _ _ C D E _ _ delta=2 0 ... ME ( [ p u ...p u ... p u 11 1 1i i 1n n] ) p ( [ 11 1 p u ...p u ... p up u ...p u ... p u 1i i 1n n] ) ( [ 11 1 × NE 1n n] ) 1i i = Fig</context>
</contexts>
<marker>Dias, Guilloré, Lopes, 1999</marker>
<rawString>Gaël Dias, Sylvie Guilloré, and José Lopes. 1999. Language Independent Automatic Acquisition of Rigid Multiword Units from Unrestricted Text corpora. Traitement Automatique des Langues Naturelles, Institut d’Etudes Scientifiques, Cargèse, France, 333-339. www.di.ubi.pt/~ddg/publications/taln1999.ps.gz</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaël Dias</author>
</authors>
<title>Extraction Automatique d’Associations Lexicales à partir de Corpora. PhD Thesis.</title>
<date>2002</date>
<institution>New University of Lisbon (Portugal) and University of Orléans</institution>
<note>www.di.ubi.pt/~ddg/publications/thesis.pdf.gz</note>
<contexts>
<context position="2224" citStr="Dias (2002)" startWordPosition="338" endWordPosition="339"> the relative frequencies of all its kgrams (k ≤ n). Another way to account for variable length dependencies is the n-multigram model designed by Deligne and Bimbot (1995). All these models have in common the fact that they need to compute continuous string frequencies. This task can be colossal when gigabytes of data need to be processed. Indeed, Yamamoto and Church (2000) show that there exist N(N+1)/2 substrings in a N-size corpus. That is the reason why low order ngrams have been commonly used in Natural Language Processing applications. In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora. Unlikely previous models, positional ngrams are ordered sequences of tokens that represent continuous or discontinuous substrings of a corpus computed in a (2.F+1)- word size window context (F represents the context in terms of words on the right and on the left of any word in the corpus). As a consequence, the number of generated substrings rapidly explodes and reaches astronomic figures. Dias (2002) shows that ∆ (Equation 1) positional ngrams c</context>
<context position="5010" citStr="Dias (2002)" startWordPosition="779" endWordPosition="780">el Pentium III 900 MHz Personal Computer for a seven-word size window context. This article is divided into four sections: (1) we explain the basic principles of positional ngrams and the mask representation to build the Virtual Corpus; (2) we present the suffix-array-based data structure that allows counting occurrences of positional ngrams; (3) we show how a multidimensional array eases the efficient computation of the Mutual Expectation; (4) we present results over different size sub-corpora of the CETEMPúblico corpus. 2 Positional Ngrams In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora. words to the right of the same pivot word and the pivot word itself). In general terms, a collocation can be defined as a specific4 continuous or discontinuous sequence of words in a (2.F+1)-word size window context (i.e. F words to the left of a pivot word, F words to the right of the same pivot word and the pivot word itself). This situation is illustrated in Figure 1 for the collocation Ngram Statistics that fits in the window context. F=3 Vir</context>
<context position="7711" citStr="Dias (2002)" startWordPosition="1289" endWordPosition="1290">us sequences of words in a context of at most eleven words (i.e. 5 words to the left of a pivot word, 5 2 The CETEMPúblico is a 180 million-word corpus of Portuguese. It can be obtained at http://www.ldc.upenn.edu/. 3 This represents 46.986.831 positional ngrams. Figure 2: One-window context for F=3 A simple way would be to shift the two-window context to the right so that each word would sequentially be processed. However, this would inevitably lead to duplications of positional ngrams. Instead, we propose a 4 As specific, we intend a sequence that fits the definition of collocation given by Dias (2002): “A collocation is a recurrent sequence of words that co-occur together more than expected by chance in a given domain”. The representation of positional ngrams is an essential step towards efficient computation. For that, purpose, we propose a reference representation rather than an explicit structure of each positional ngram. The idea is to adapt the suffix representation (Manber and Myers, 1990) to the positional ngram case. Following the suffix representation, any continuous corpus substring is virtually represented by a single position of the corpus as illustrated in Figure 3. In fact, t</context>
<context position="20956" citStr="Dias, 2002" startWordPosition="4149" endWordPosition="4150">lled Matrix9. 4.2 Matrix The attentive reader will have noticed that the denominator of the Normalized Expectation formula is the average frequency of all the positional (n-1)grams included in a given positional ngram. These specific positional ngrams are called positional sub-ngrams of order n-110. So, in order to compute the Normalized Expectation and a fortiori the Mutual Expectation, it is necessary to access efficiently to the sub-ngrams frequencies. This operation is done through the Matrix. 9 The Matrix also speeds up the extraction process that applies the GenLocalMaxs algorithm (Gaël Dias, 2002). We do not present this algorithm due to lack of space. 10 In order to ease the reading, we will use the term sub-ngrams to denote positional sub-ngrams of order n-1. By representing a sub-ngram, we mean calculating its virtual representation that identifies its related substring. The previous figure shows that representing the first three sub-ngrams of the positional ngram {{0,0},14} is straightforward as they all contain the first word of the window context. The only difficulty is to know the mask they are associated to. Knowing this, the first three sub-ngrams would respectively be represe</context>
</contexts>
<marker>Dias, 2002</marker>
<rawString>Gaël Dias 2002. Extraction Automatique d’Associations Lexicales à partir de Corpora. PhD Thesis. New University of Lisbon (Portugal) and University of Orléans (France). www.di.ubi.pt/~ddg/publications/thesis.pdf.gz</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>English Lexical Collocations: A study in computational linguistics. Singapore, reprinted as chapter</title>
<date>1974</date>
<volume>2</volume>
<publisher>Press.</publisher>
<location>Uni</location>
<contexts>
<context position="7009" citStr="Sinclair, 1974" startWordPosition="1170" endWordPosition="1171">e all the positional ngrams of a corpus, we need to take into account all the words as possible pivot words. A B C D E F G H I J K L M N .... X Y Z .... A B C D E F G H I J K L M N .... X Y Z .... A B C D E F G H I J K L M N .... X Y Z .... A B C D E F G H I J K L M N .... X Y Z .... F=3 [Ngram Statistics] [Ngram ___ from] [Ngram ___ ___ Large] [to ___ Ngram] 2.1 Principles .... A B C D E F G H I J K L M N .... X Y Z .... .... The original idea of the positional ngram model comes from the lexicographic evidence that most lexical relations associate words separated by at most five other words (Sinclair, 1974). As a consequence, lexical relations such as collocations can be continuous or discontinuous sequences of words in a context of at most eleven words (i.e. 5 words to the left of a pivot word, 5 2 The CETEMPúblico is a 180 million-word corpus of Portuguese. It can be obtained at http://www.ldc.upenn.edu/. 3 This represents 46.986.831 positional ngrams. Figure 2: One-window context for F=3 A simple way would be to shift the two-window context to the right so that each word would sequentially be processed. However, this would inevitably lead to duplications of positional ngrams. Instead, we prop</context>
</contexts>
<marker>Sinclair, 1974</marker>
<rawString>John Sinclair. 1974. English Lexical Collocations: A study in computational linguistics. Singapore, reprinted as chapter 2 of Foley, J. A. (ed). 1996, John Sinclair on Lexis and Lexicography, Uni Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Bentley</author>
<author>Robert Sedgewick</author>
</authors>
<title>Fast Algorithms for Sorting and Searching Strings.</title>
<date>1997</date>
<booktitle>8th Annual ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<location>New Orléans.</location>
<contexts>
<context position="3977" citStr="Bentley and Sedgewick, 1997" startWordPosition="622" endWordPosition="626">gram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. In this paper, we describe an implementation that computes the Frequency and the Mutual Expectation (Dias et al. 1999) of any positional ngram with time complexity O(h(F) N log N). The global architecture is based on the definition of masks that allow virtually representing any positional ngram in the corpus. Thus, we follow the Virtual Corpus approach introduced by Kit and Wilks (1998) and apply a suffix-array-like method, coupled to the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997), to compute positional ngram frequencies. Finally, a multidimensional array is built to easily process the Mutual Expectation, an association measure for collocation extraction. The evaluation of our C++ implementation has been realized over the CETEMPúblico2 corpus and shows satisfactory results. For example, it takes 8.59 minutes to compute both frequency and Mutual Expectation for a 1.092.7233-word corpus on an Intel Pentium III 900 MHz Personal Computer for a seven-word size window context. This article is divided into four sections: (1) we explain the basic principles of positional ngram</context>
<context position="16982" citStr="Bentley and Sedgewick, 1997" startWordPosition="3258" endWordPosition="3261">fficiency of the counting mainly resides in the use of an adapted sort algorithm. Kit and Wilks (1998) propose to use a bucket-radixsort although they acknowledge that the classical quicksort performs faster for large-vocabulary corpora. Around the same perspective, Yamamoto and Church (2000) use the Manber and Myers’s algorithm (1990), an elegant radixsort-based algorithm that takes at most O(N log N) time and shows improved results when long repeated substrings are common in the corpus. For the specific case of positional ngrams, we have chosen to implement the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997) that can be seen as a mixture of the Ternary-Split Quicksort (Bentley and McIlroy, 1993) and the MSD6 radixsort (Anderson and Nilsson, 1998). The algorithm processes as follows: (1) the array of string is partitioned into three parts based on the first symbol of each string. In order to process the split a pivot element is chosen just as in the classical quicksort giving rise to: one part with elements smaller than the pivot, one part with elements equal to the pivot and one part with elements larger than the pivot; (2) the smaller and the larger parts are recursively processed in exactly the</context>
</contexts>
<marker>Bentley, Sedgewick, 1997</marker>
<rawString>Jon Bentley and Robert Sedgewick. 1997. Fast Algorithms for Sorting and Searching Strings. 8th Annual ACM-SIAM Symposium on Discrete Algorithms, New Orléans. citeseer.nj.nec.com/bentley97fast.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Bentley</author>
<author>Douglas McIlroy</author>
</authors>
<title>Engineering a sort function. Software - Practice and Experience,</title>
<date>1993</date>
<pages>23--11</pages>
<contexts>
<context position="17071" citStr="Bentley and McIlroy, 1993" startWordPosition="3273" endWordPosition="3276">ilks (1998) propose to use a bucket-radixsort although they acknowledge that the classical quicksort performs faster for large-vocabulary corpora. Around the same perspective, Yamamoto and Church (2000) use the Manber and Myers’s algorithm (1990), an elegant radixsort-based algorithm that takes at most O(N log N) time and shows improved results when long repeated substrings are common in the corpus. For the specific case of positional ngrams, we have chosen to implement the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997) that can be seen as a mixture of the Ternary-Split Quicksort (Bentley and McIlroy, 1993) and the MSD6 radixsort (Anderson and Nilsson, 1998). The algorithm processes as follows: (1) the array of string is partitioned into three parts based on the first symbol of each string. In order to process the split a pivot element is chosen just as in the classical quicksort giving rise to: one part with elements smaller than the pivot, one part with elements equal to the pivot and one part with elements larger than the pivot; (2) the smaller and the larger parts are recursively processed in exactly the same manner as the whole array; (3) the equal part is also sorted recursively but with p</context>
</contexts>
<marker>Bentley, McIlroy, 1993</marker>
<rawString>Jon Bentley and Douglas McIlroy. 1993. Engineering a sort function. Software - Practice and Experience, 23(11):1249-1265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Yamamoto</author>
<author>Kenneth Church</author>
</authors>
<title>Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a corpus. Association for Computational Linguistics,</title>
<date>2000</date>
<pages>27--1</pages>
<contexts>
<context position="1989" citStr="Yamamoto and Church (2000)" startWordPosition="298" endWordPosition="301"> most successful statistical models is certainly the ngram model (Jelinek, 1990). However, in order to overcome its conceptual rigidity, T. Kuhn et al. (1994) have defined the polygram model that estimates the probability of an ngram by interpolating the relative frequencies of all its kgrams (k ≤ n). Another way to account for variable length dependencies is the n-multigram model designed by Deligne and Bimbot (1995). All these models have in common the fact that they need to compute continuous string frequencies. This task can be colossal when gigabytes of data need to be processed. Indeed, Yamamoto and Church (2000) show that there exist N(N+1)/2 substrings in a N-size corpus. That is the reason why low order ngrams have been commonly used in Natural Language Processing applications. In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora. Unlikely previous models, positional ngrams are ordered sequences of tokens that represent continuous or discontinuous substrings of a corpus computed in a (2.F+1)- word size window context (F represents the context</context>
<context position="16647" citStr="Yamamoto and Church (2000)" startWordPosition="3205" endWordPosition="3208">s a vector of words [p11 u1 p12 u2 ... p1n un] where ui stands for any word in the positional ngram and p1i represents the distance that separates words u1 and ui8. Thus, the positional ngram [A _ C D E _ _] would be rewritten as [0 A +2 C +3 D +4 E] and its Normalized Expectation would be given by Equation 4. n 1 � � k � � � The efficiency of the counting mainly resides in the use of an adapted sort algorithm. Kit and Wilks (1998) propose to use a bucket-radixsort although they acknowledge that the classical quicksort performs faster for large-vocabulary corpora. Around the same perspective, Yamamoto and Church (2000) use the Manber and Myers’s algorithm (1990), an elegant radixsort-based algorithm that takes at most O(N log N) time and shows improved results when long repeated substrings are common in the corpus. For the specific case of positional ngrams, we have chosen to implement the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997) that can be seen as a mixture of the Ternary-Split Quicksort (Bentley and McIlroy, 1993) and the MSD6 radixsort (Anderson and Nilsson, 1998). The algorithm processes as follows: (1) the array of string is partitioned into three parts based on the first symbol of e</context>
</contexts>
<marker>Yamamoto, Church, 2000</marker>
<rawString>Mikio Yamamoto and Kenneth Church. 2000. Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a corpus. Association for Computational Linguistics, 27(1):1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Frédéric Bimbot</author>
</authors>
<title>Language Modelling by Variable Length Sequences: Theoretical Formulation and Evaluation of Multigrams.</title>
<date>1995</date>
<booktitle>ICASSP-95.</booktitle>
<pages>1--169</pages>
<location>Detroit, Michigan,</location>
<contexts>
<context position="1784" citStr="Deligne and Bimbot (1995)" startWordPosition="264" endWordPosition="267">solution shows O(h(F) N log N) time complexity where N is the corpus size and h(F) a function of the window context. 1 Introduction Many models have been proposed to evaluate word dependencies. One of the most successful statistical models is certainly the ngram model (Jelinek, 1990). However, in order to overcome its conceptual rigidity, T. Kuhn et al. (1994) have defined the polygram model that estimates the probability of an ngram by interpolating the relative frequencies of all its kgrams (k ≤ n). Another way to account for variable length dependencies is the n-multigram model designed by Deligne and Bimbot (1995). All these models have in common the fact that they need to compute continuous string frequencies. This task can be colossal when gigabytes of data need to be processed. Indeed, Yamamoto and Church (2000) show that there exist N(N+1)/2 substrings in a N-size corpus. That is the reason why low order ngrams have been commonly used in Natural Language Processing applications. In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora. Unlikely p</context>
</contexts>
<marker>Deligne, Bimbot, 1995</marker>
<rawString>Sabine Deligne and Frédéric Bimbot. 1995. Language Modelling by Variable Length Sequences: Theoretical Formulation and Evaluation of Multigrams. ICASSP-95. Detroit, Michigan, 1:169-172. citeseer.nj.nec.com/deligne95language.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kuhn</author>
<author>H Nieman</author>
<author>E G Schukat-Talamazzini</author>
</authors>
<title>Ergodic Hidden Markov Models and Polygrams for Language Modelling.</title>
<date>1994</date>
<volume>94</volume>
<pages>1--357</pages>
<contexts>
<context position="1521" citStr="Kuhn et al. (1994)" startWordPosition="220" endWordPosition="223">ord size corpus in a seven-word size window context. In comparison, only 700.000 ngrams would be computed for the classical ngram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. Our solution shows O(h(F) N log N) time complexity where N is the corpus size and h(F) a function of the window context. 1 Introduction Many models have been proposed to evaluate word dependencies. One of the most successful statistical models is certainly the ngram model (Jelinek, 1990). However, in order to overcome its conceptual rigidity, T. Kuhn et al. (1994) have defined the polygram model that estimates the probability of an ngram by interpolating the relative frequencies of all its kgrams (k ≤ n). Another way to account for variable length dependencies is the n-multigram model designed by Deligne and Bimbot (1995). All these models have in common the fact that they need to compute continuous string frequencies. This task can be colossal when gigabytes of data need to be processed. Indeed, Yamamoto and Church (2000) show that there exist N(N+1)/2 substrings in a N-size corpus. That is the reason why low order ngrams have been commonly used in Na</context>
</contexts>
<marker>Kuhn, Nieman, Schukat-Talamazzini, 1994</marker>
<rawString>T. Kuhn, H. Nieman, E.G. Schukat-Talamazzini. 1994. Ergodic Hidden Markov Models and Polygrams for Language Modelling. ICASSP-94, 1:357-360. citeseer.nj.nec.com/kuhn94ergodic.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix-arrays: A new method for on-line string searches.</title>
<date>1990</date>
<booktitle>First Annual ACMSIAM Symposium on Discrete Algorithms.</booktitle>
<pages>319--327</pages>
<contexts>
<context position="8113" citStr="Manber and Myers, 1990" startWordPosition="1348" endWordPosition="1351">d sequentially be processed. However, this would inevitably lead to duplications of positional ngrams. Instead, we propose a 4 As specific, we intend a sequence that fits the definition of collocation given by Dias (2002): “A collocation is a recurrent sequence of words that co-occur together more than expected by chance in a given domain”. The representation of positional ngrams is an essential step towards efficient computation. For that, purpose, we propose a reference representation rather than an explicit structure of each positional ngram. The idea is to adapt the suffix representation (Manber and Myers, 1990) to the positional ngram case. Following the suffix representation, any continuous corpus substring is virtually represented by a single position of the corpus as illustrated in Figure 3. In fact, the substring is the sequence of words that goes from the word referred by the position till the end of the corpus. Figure 3: Suffix Representation5 Unfortunately, the suffix representation can not directly be extended to the specific case of positional ngrams. One main reason aims at this situation: a positional ngram may represent a discontinuous sequence of words. In order to overcome this situati</context>
</contexts>
<marker>Manber, Myers, 1990</marker>
<rawString>Udi Manber and Gene Myers. 1990. Suffix-arrays: A new method for on-line string searches. First Annual ACMSIAM Symposium on Discrete Algorithms. 319-327.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>