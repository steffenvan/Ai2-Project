<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039147">
<title confidence="0.955839">
Non-Expert Correction of Automatically Generated Relation Annotations
</title>
<author confidence="0.967438">
Matthew R. Gormley*† and Adam Gerber*† and Mary Harper*‡ and Mark Dredze *†
</author>
<affiliation confidence="0.9922">
*Human Language Technology Center of Excellence
†Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21211, USA
‡Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742 USA
</affiliation>
<email confidence="0.998156">
mrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999090083333333">
We explore a new way to collect human an-
notated relations in text using Amazon Me-
chanical Turk. Given a knowledge base of
relations and a corpus, we identify sentences
which mention both an entity and an attribute
that have some relation in the knowledge base.
Each noisy sentence/relation pair is presented
to multiple turkers, who are asked whether the
sentence expresses the relation. We describe
a design which encourages user efficiency and
aids discovery of cheating. We also present
results on inter-annotator agreement.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931730769231">
Relation extraction (RE) is the task of determining
the existence and type of relation between two tex-
tual entity mentions. Slot filling, a general form of
relation extraction, includes relations between non-
entities, such as a person and an occupation, age, or
cause of death (McNamee and Dang, 2009).
RE annotated data, such as ACE (2008), is expen-
sive to produce so systems take different approaches
to minimizing data needs. For example, tree kernels
can reduce feature sparsity and generalize across
many examples (GuoDong et al., 2007; Zhou et
al., 2009). Distant supervision automatically gen-
erates noisy training examples from a knowledge
base (KB) without needing annotations (Bunescu
and Mooney, 2007; Mintz et al., 2009). While
this method can quickly generate training data, it
also generates many false examples. We reduce the
noise in such examples by using Amazon Mechani-
cal Turk (MTurk), which has been shown to produce
high quality annotations for a variety of natural lan-
guage processing tasks (Snow et al., 2008).
We use MTurk for annotation of textual relations
to establish an inexpensive and rapid method of cre-
ating data for slot filling. We present a two step an-
notation process: (1) automatic creation of noisy ex-
amples, and (2) human validation of examples.
</bodyText>
<sectionHeader confidence="0.990946" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.99877">
2.1 Automatic generation of noisy examples
</subsectionHeader>
<bodyText confidence="0.999888461538462">
To create noisy examples we use a similar approach
to Mintz et al. (2009). We extract relations from a
KB in the form of tuples, (e, r, v), where e is an
entity, v is a value, and r is a relation that holds
between them; for example (J.R.R. Tolkien, occu-
pation, author). Our KB is Freebase1, an online
database of structured information, and our corpus
is from the TAC KBP task (McNamee and Dang,
2009)2. For each tuple, we find sentences in a cor-
pus that contain both an exact mention of the entity
e and of the value v. Of course, such sentences may
not attest to the relation r, so the process produces
many incorrect examples.
</bodyText>
<subsectionHeader confidence="0.997831">
2.2 Human Intelligence Tasks
</subsectionHeader>
<bodyText confidence="0.999445833333333">
A Human Intelligence Task (HIT) is a short paid task
on MTurk. In our HITs, we present the turker with
ten relation examples as sentence/relation pairs. For
each example, the user is asked to select from three
annotation options: the sentence (1) expresses the
relation, (2) does not express the relation, or (3) the
</bodyText>
<footnote confidence="0.9998365">
1http://www.freebase.com
2http://projects.ldc.upenn.edu/kbp/
</footnote>
<page confidence="0.914767">
204
</page>
<note confidence="0.7071605">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 204–207,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.801226090909091">
1. The sentence expresses the relation.
Sentence: For the past eleven years, James has
lived in Tucson.
Relation: “Tucson” is the residence of “James”
2. The sentence does not express the relation.
Sentence: Samuel first met Divya in 1990, while
she was still a student.
Relation: “Divya” is a spouse of “Samuel”
3. The relation does not make sense.
Sentence: Soojin was born in January.
Relation: “January” is the birth place of “Soojin”
</listItem>
<figureCaption confidence="0.999541">
Figure 1: The three annotation options with examples.
</figureCaption>
<bodyText confidence="0.999872">
relation does not make sense (figure 1.)
Of the ten examples that comprise each HIT,
seven are automatically generated by the method
above. The correct answer is known for the three re-
maining examples; these are included for quality as-
surance (control examples.) The three control exam-
ples are a positive example (expresses the relation,) a
negative example (contradicts the true relation,) and
a nonsense example (relation is nonsensical.)
All control examples derive from a subset of the
automatically generated person examples. Positive
examples were randomly sampled and hand anno-
tated. Negative examples are familial relations in
which we change the relation type so that it would
not be expressed in the sentence. For example,
the relation “Barack Obama is the parent of Malia
Obama” would be changed to “Barack Obama is a
sibling of Malia Obama.” To generate nonsense ex-
amples we employ the same method for a different
mapping of relations, which produces relations like
“New Zealand is the gender of John Key.”
</bodyText>
<subsectionHeader confidence="0.994845">
2.3 HIT Design
</subsectionHeader>
<bodyText confidence="0.999875083333333">
MTurk is a marketplace so users have total freedom
in choosing which HITs to complete. As such, HIT
design should maximize its appeal. We assume that
users find appealing those HITs through which they
may maximize their own monetary gain, while mini-
mizing moment-to-moment frustrations. We empha-
sized clarity and ease of use.
The layout consists of three sections (figure 2).
The leftmost section is a progress list, which shows
the user’s answers and current position; the middle
section contains the current relation example and an-
notation options; the rightmost section (not pictured)
</bodyText>
<table confidence="0.9997485">
# HITs Cost Time (hours)
Trial 50 $2.75 27
Batch 1 500 $27.50 34
Batch 2 765 $42.08 25
Batch 3 500 $27.50 22
Total 1815 $99.83 108
</table>
<tableCaption confidence="0.999966">
Table 1: Size, cost and time to complete each HITs batch.
</tableCaption>
<bodyText confidence="0.999300875">
contains instructions. All sections and all UI ele-
ments remain visible and in the same position for the
duration of the HIT, with only the text of the sen-
tence and relation changing according to question
number. Because only a single question is displayed
at a time, we are able to minimize user actions such
as scrolling, clicking small targets, or making large
mouse movements. Additionally, we can monitor
how much time a user spends on each question.
At all times the user is able to consult the instruc-
tions for the task, which include examples of each
annotation option. The user is also reminded of the
technical requirements for the HIT and expectations
for honesty and accuracy. A comment box provides
users with the opportunity to ask questions, make
suggestions, or clarify their responses.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999996818181818">
We submitted a trial run and three full batches of
HITs. Table 1 summarizes the costs and completion
times for all HITs. The HITs were labeled rapidly
and for a low cost ($0.05 per HIT, i.e., .5¢ per anno-
tation). Each HIT was assigned to five unique work-
ers. We found that 50% of the 352 different workers
completed 2 or more HITs (figure 3.) Our results
exclude a trial run of 50 hits. Across the 17,650 ex-
amples the mean time spent was 20.77 seconds, with
a standard deviation of 99.96 seconds. The median
time per example was 10.0 seconds.
</bodyText>
<subsectionHeader confidence="0.996646">
3.1 Analysis
</subsectionHeader>
<bodyText confidence="0.999997125">
To evaluate the annotations, two of the authors an-
notated a random sample of 247 (10%) of the 2471
noisy examples. In addition, we analyzed the work-
ers agreement with the control examples.
We used two metrics to assess agreement. The
first metric is pairwise percent agreement (Pair-
wise): the average of the example agreement scores,
where the example agreement score is the percent of
</bodyText>
<page confidence="0.998788">
205
</page>
<figureCaption confidence="0.985961">
Figure 2: An example HIT with instructions excluded.
</figureCaption>
<figure confidence="0.835866777777778">
# Ex. R Exact-rc Pairwise
E1/E2 247 2 0.64 0.81
E1/M 247 2 0.29 0.60
E2/M 247 2 0.39 0.70
C/M 1059 2 0.90 0.93
T(sample) 247 5 0.31 0.69
T(control) 1059 5 0.52 0.68
T(all) 3530 5 0.45 0.68
# HITS
140
120
100
60
40
20
80
0
Workers
</figure>
<figureCaption confidence="0.9984765">
Figure 3: The number of HITs per worker, with columns
sorted left to right.
</figureCaption>
<bodyText confidence="0.9905283">
pairs of annotators that agreed for a particular exam-
ple. The second metric is the exact kappa coefficient
(Exact-K) (Conger, 1980), which takes into account
that agreement can occur by chance. The number of
annotators (R) varies with the test scenario.
Table 2 presents the inter-annotator agreement
scores for various subsets of the examples and com-
binations of annotators. On a sample of examples,
we evaluated agreement between the first and sec-
ond expert annotators (E1/E2) and also the agree-
ment between each expert and the majority vote of
the workers (E1/M and E2/M). The agreement be-
tween the two experts is substantially higher than
their individual agreements with the majority. Yet,
we achieve our goal of reducing noise.
We also analyzed the agreement between the
known control answer and the majority vote of the
workers (C/M). This high level of agreement sup-
ports our belief that the automatically generated neg-
ative and nonsense examples were easier to identify
</bodyText>
<tableCaption confidence="0.98202">
Table 2: Inter-annotator agreement
</tableCaption>
<bodyText confidence="0.999917928571429">
than noisy negative and nonsense examples. Finally,
we evaluated the agreement between the five work-
ers for different subsets of the data: the sample of
noisy examples (T(sample)), the control examples
only (T(control)), and all examples (T(all)). Table 3
lists the number of examples collected and the agree-
ment scores for all workers for each relation type.
Table 4 shows the divergence of the workers’ an-
notations from those of an expert. The high level
of confusability for those examples which the expert
annotated as Not Expressed suggests their inherent
difficulty. The workers labeled more examples as
Expressed than the expert, but both labeled few ex-
amples as Nonsense.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="method">
4 Quality Control
</sectionHeader>
<bodyText confidence="0.999787125">
We identify spurious responses and unreliable users
in two ways. First, worker responses are compared
to control examples; greater agreement with controls
should indicate greater confidence in the user. We
filtered any worker whose agreement with the con-
trols was less than 0.85 (Control Filtered). The sec-
ond approach uses behavioral data. Because only a
single example is visible at any time, we can mea-
</bodyText>
<figure confidence="0.621407">
0 50 100 150 200 250 300 350
</figure>
<page confidence="0.928396">
206
</page>
<table confidence="0.999977842105263">
Relation # Ex. Exact-r. Pairwise
siblings 13 0.67 0.82
children 12 0.57 0.83
gender 80 0.46 0.70
place of death 40 0.43 0.68
parent 12 0.40 0.64
spouse 54 0.37 0.65
title 71 0.30 0.78
residences 228 0.29 0.60
ethnicity 38 0.28 0.54
occupation 551 0.26 0.77
activism 4 0.26 0.55
religion 22 0.23 0.55
place of birth 160 0.20 0.64
nationality 1044 0.19 0.67
schools attended 8 0.16 0.55
employee of 132 0.16 0.70
charges 2 0.14 0.70
Total 2471 0.35 0.69
</table>
<tableCaption confidence="0.929830333333333">
Table 3: Inter-annotator agreement across relation type.
# Ex. is the number of noisy examples. Exact-r. and Pair-
wise agreement are among the five workers.
</tableCaption>
<table confidence="0.999884833333333">
Worker Total
E NE Nn
Expert-1 561 89 20 670
284 248 28 560
1 1 3 5
Total 846 338 51 1235
</table>
<tableCaption confidence="0.986576666666667">
Table 4: Confusion matrix of expert-1 and user’s anno-
tations on the sample of noisy examples, for the choices
Expressed (E), Not Expressed (NE), and Nonsense (Nn)
</tableCaption>
<bodyText confidence="0.999971666666667">
sure how much time a user spends on each exam-
ple. The UI is designed to allow for the extremely
rapid completion of examples and of the HIT in gen-
eral. Thus, a user could complete the HIT in only a
few seconds without even reading any of the exam-
ples. Still other users spend only a moment on all-
but-one question, and then several minutes on the
remaining question. Here, we filter a user answering
three or more questions each in under three seconds
(Time Filtered). We combine these two approaches
(Control and Time), which yields the highest expert-
agreement levels (table 5.)
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9831805">
Using non-expert annotators from Amazon Mechan-
ical Turk for the correction of noisy, automatically
</bodyText>
<table confidence="0.9969878">
E1/M E2/M
Unfiltered 0.28 0.38
Time Filtered 0.32 0.43
Control Filtered 0.34 0.47
Control and Time 0.37 0.48
</table>
<tableCaption confidence="0.975063833333333">
Table 5: Exact-r. scores for three levels of quality control
and a baseline, between each expert and the majority vote
on 231 sampled examples. For a fair comparison, we re-
duced the sample size to include only examples for which
each level of quality control had at least one worker an-
notation remaining.
</tableCaption>
<bodyText confidence="0.9997782">
generated examples is inexpensive and fast. We
achieve good inter-annotator agreement using qual-
ity assurance measures to detect cheating. The result
is thousands of new annotated slot filling example
sentences for 17 person relations.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999879">
We would like to thank the 352 turkers who made
this work possible.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990821714285714">
NnNEE
ACE. 2008. Automatic content extraction.
http://projects.ldc.upenn.edu/ace/.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Association for Computational Linguistics (ACL).
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. Psychological Bulletin,
88(2):322–328.
Z. GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoM-
ing. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Empirical Methods in Natural Language Processing
(EMNLP).
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 knowledge base population track. In Text
Analysis Conference (TAC).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Association for Computational Linguistics
(ACL).
R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast—but is it good?: evaluating non-expert
annotations for natural language tasks. In Empirical
Methods in Natural Language Processing (EMNLP).
G. Zhou, L. Qian, and J. Fan. 2009. Tree kernel-based
semantic relation extraction with rich syntactic and se-
mantic information. Information Sciences.
</reference>
<page confidence="0.998012">
207
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347325">
<title confidence="0.99993">Non-Expert Correction of Automatically Generated Relation Annotations</title>
<author confidence="0.863682">R Dredze</author>
<affiliation confidence="0.43694">Human Language Technology Center of</affiliation>
<title confidence="0.951276">for Language and Speech</title>
<author confidence="0.860934">Johns Hopkins University</author>
<author confidence="0.860934">Baltimore</author>
<affiliation confidence="0.941133">for Computational Linguistics and Information University of Maryland, College Park, MD 20742 USA</affiliation>
<email confidence="0.999684">mrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu</email>
<abstract confidence="0.999172846153846">We explore a new way to collect human annotated relations in text using Amazon Mechanical Turk. Given a knowledge base of relations and a corpus, we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base. Each noisy sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation. We describe a design which encourages user efficiency and aids discovery of cheating. We also present results on inter-annotator agreement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>Automatic content extraction.</title>
<date>2008</date>
<note>http://projects.ldc.upenn.edu/ace/.</note>
<contexts>
<context position="1369" citStr="ACE (2008)" startWordPosition="196" endWordPosition="197">sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation. We describe a design which encourages user efficiency and aids discovery of cheating. We also present results on inter-annotator agreement. 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce high</context>
</contexts>
<marker>ACE, 2008</marker>
<rawString>ACE. 2008. Automatic content extraction. http://projects.ldc.upenn.edu/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1739" citStr="Bunescu and Mooney, 2007" startWordPosition="249" endWordPosition="252">n between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce high quality annotations for a variety of natural language processing tasks (Snow et al., 2008). We use MTurk for annotation of textual relations to establish an inexpensive and rapid method of creating data for slot filling. We present a two step annotation process: (1) automatic creation of noisy examples, and (2) human validation of examples. 2 Method 2.1 Automatic gen</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>R. Bunescu and R. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Conger</author>
</authors>
<title>Integration and generalization of kappas for multiple raters.</title>
<date>1980</date>
<journal>Psychological Bulletin,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="8170" citStr="Conger, 1980" startWordPosition="1346" endWordPosition="1347">agreement (Pairwise): the average of the example agreement scores, where the example agreement score is the percent of 205 Figure 2: An example HIT with instructions excluded. # Ex. R Exact-rc Pairwise E1/E2 247 2 0.64 0.81 E1/M 247 2 0.29 0.60 E2/M 247 2 0.39 0.70 C/M 1059 2 0.90 0.93 T(sample) 247 5 0.31 0.69 T(control) 1059 5 0.52 0.68 T(all) 3530 5 0.45 0.68 # HITS 140 120 100 60 40 20 80 0 Workers Figure 3: The number of HITs per worker, with columns sorted left to right. pairs of annotators that agreed for a particular example. The second metric is the exact kappa coefficient (Exact-K) (Conger, 1980), which takes into account that agreement can occur by chance. The number of annotators (R) varies with the test scenario. Table 2 presents the inter-annotator agreement scores for various subsets of the examples and combinations of annotators. On a sample of examples, we evaluated agreement between the first and second expert annotators (E1/E2) and also the agreement between each expert and the majority vote of the workers (E1/M and E2/M). The agreement between the two experts is substantially higher than their individual agreements with the majority. Yet, we achieve our goal of reducing nois</context>
</contexts>
<marker>Conger, 1980</marker>
<rawString>A.J. Conger. 1980. Integration and generalization of kappas for multiple raters. Psychological Bulletin, 88(2):322–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z GuoDong</author>
<author>M Zhang</author>
<author>D H Ji</author>
<author>Z H U QiaoMing</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1569" citStr="GuoDong et al., 2007" startWordPosition="225" endWordPosition="228">f cheating. We also present results on inter-annotator agreement. 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce high quality annotations for a variety of natural language processing tasks (Snow et al., 2008). We use MTurk for annotation of textual relations to establish an inexpensive and rapid method of creating d</context>
</contexts>
<marker>GuoDong, Zhang, Ji, QiaoMing, 2007</marker>
<rawString>Z. GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoMing. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Dang</author>
</authors>
<title>knowledge base population track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>In Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="1330" citStr="McNamee and Dang, 2009" startWordPosition="187" endWordPosition="190">ave some relation in the knowledge base. Each noisy sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation. We describe a design which encourages user efficiency and aids discovery of cheating. We also present results on inter-annotator agreement. 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk</context>
<context position="2767" citStr="McNamee and Dang, 2009" startWordPosition="430" endWordPosition="433">apid method of creating data for slot filling. We present a two step annotation process: (1) automatic creation of noisy examples, and (2) human validation of examples. 2 Method 2.1 Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v is a value, and r is a relation that holds between them; for example (J.R.R. Tolkien, occupation, author). Our KB is Freebase1, an online database of structured information, and our corpus is from the TAC KBP task (McNamee and Dang, 2009)2. For each tuple, we find sentences in a corpus that contain both an exact mention of the entity e and of the value v. Of course, such sentences may not attest to the relation r, so the process produces many incorrect examples. 2.2 Human Intelligence Tasks A Human Intelligence Task (HIT) is a short paid task on MTurk. In our HITs, we present the turker with ten relation examples as sentence/relation pairs. For each example, the user is asked to select from three annotation options: the sentence (1) expresses the relation, (2) does not express the relation, or (3) the 1http://www.freebase.com </context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Dang. 2009. Overview of the TAC 2009 knowledge base population track. In Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1760" citStr="Mintz et al., 2009" startWordPosition="253" endWordPosition="256">ty mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce high quality annotations for a variety of natural language processing tasks (Snow et al., 2008). We use MTurk for annotation of textual relations to establish an inexpensive and rapid method of creating data for slot filling. We present a two step annotation process: (1) automatic creation of noisy examples, and (2) human validation of examples. 2 Method 2.1 Automatic generation of noisy exam</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>L Qian</author>
<author>J Fan</author>
</authors>
<title>Tree kernel-based semantic relation extraction with rich syntactic and semantic information. Information Sciences.</title>
<date>2009</date>
<contexts>
<context position="1589" citStr="Zhou et al., 2009" startWordPosition="229" endWordPosition="232">esent results on inter-annotator agreement. 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce high quality annotations for a variety of natural language processing tasks (Snow et al., 2008). We use MTurk for annotation of textual relations to establish an inexpensive and rapid method of creating data for slot filling</context>
</contexts>
<marker>Zhou, Qian, Fan, 2009</marker>
<rawString>G. Zhou, L. Qian, and J. Fan. 2009. Tree kernel-based semantic relation extraction with rich syntactic and semantic information. Information Sciences.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>