<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003264">
<title confidence="0.990034">
Applying Coreference to Improve Name Recognition
</title>
<author confidence="0.98555">
Heng JI and Ralph GRISHMAN
</author>
<affiliation confidence="0.998337">
Department of Computer Science
</affiliation>
<address confidence="0.935820666666667">
New York University
715 Broadway, 7th Floor
New York, NY 10003, U.S.A.
</address>
<email confidence="0.999654">
hengji@cs.nyu.edu, grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.99253" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879375">
We present a novel method of applying the
results of coreference resolution to improve
Name Recognition for Chinese. We consider
first some methods for gauging the confidence
of individual tags assigned by a statistical
name tagger. For names with low confidence,
we show how these names can be filtered
using coreference features to improve
accuracy. In addition, we present rules which
use coreference information to correct some
name tagging errors. Finally, we show how
these gains can be magnified by clustering
documents and using cross-document
coreference in these clusters. These combined
methods yield an absolute improvement of
about 3.1% in tagger F score.
</bodyText>
<sectionHeader confidence="0.998692" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997246644067797">
The problem of name recognition and
classification has been intensively studied since
1995, when it was introduced as part of the MUC-
6 Evaluation (Grishman and Sundheim, 1996). A
wide variety of machine learning methods have
been applied to this problem, including Hidden
Markov Models (Bikel et al. 1997), Maximum
Entropy methods (Borthwick et al. 1998, Chieu
and Ng 2002), Decision Trees (Sekine et al. 1998),
Conditional Random Fields (McCallum and Li
2003), Class-based Language Model (Sun et al.
2002), Agent-based Approach (Ye et al. 2002) and
Support Vector Machines. However, the
performance of even the best of these models1 has
been limited by the amount of labeled training data
available to them and the range of features which
they employ. In particular, most of these methods
classify an instance of a name based on the
information about that instance alone, and very
local context of that instance – typically, one or
1 The best results reported for Chinese named entity
recognition, on the MET-2 test corpus, are 0.92 to 0.95
F-measure for the different name types (Ye et al. 2002).
two words preceding and following the name. If a
name has not been seen before, and appears in a
relatively uninformative context, it becomes very
hard to classify.
We propose to use more global information to
improve the performance of name recognition.
Some name taggers have incorporated a name
cache or similar mechanism which makes use of
names previously recognized in the document. In
our approach, we perform coreference analysis and
then use detailed evidence from other phrases in
the document which are co-referential with this
name in order to disambiguate the name. This
allows us to perform a richer set of corrections
than with a name cache. We then go one step
further and process similar documents containing
instances of the same name, and combine the
evidence from these additional instances. At each
step we are able to demonstrate a small but
consistent improvement in named entity
recognition.
The rest of the paper is organized as follows.
Section 2 briefly describes the baseline name
tagger and coreference resolver used in this paper.
Section 3 considers methods for assessing the
confidence of name tagging decisions. Section 4
examines the distribution of name errors, as a
motivation for using coreference information.
Section 5 shows the coreference features we use
and how they are incorporated into a statistical
name filter. Section 6 describes additional rules
using coreference to improve name recognition.
Section 7 provides the flow graph of the improved
system. Section 8 reports and discusses the
experimental results while Section 9 summarizes
the conclusions.
</bodyText>
<sectionHeader confidence="0.976868" genericHeader="introduction">
2 Baseline Systems
</sectionHeader>
<bodyText confidence="0.999970476190476">
The task we consider in this paper is to identify
three classes of names in Chinese text: persons
(PER), organizations (ORG), and geo-political
entities (GPE). Geo-political entities are locations
which have an associated government, such as
cities, states, and countries.2 Name recognition in
Chinese poses extra challenges because neither
capitalization nor word segmentation clues are
explicitly provided, although most of the
techniques we describe are more generally
applicable.
Our study builds on an extraction system
developed for the ACE evaluation, a multi-site
evaluation of information extraction organized by
the U.S. Government. Following ACE
terminology, we will use the term mention to refer
to a name or noun phrase of one of the types of
interest, and the term entity for a set of coreferring
mentions. We briefly describe in this section the
baseline Chinese named entity tagger, as well as
the coreference system, used in our experiments.
</bodyText>
<subsectionHeader confidence="0.990372">
2.1 Chinese Name Tagger
</subsectionHeader>
<bodyText confidence="0.99972475">
Our baseline name tagger consists of an HMM
tagger augmented with a set of post-processing
rules. The HMM tagger generally follows the
NYMBLE model (Bikel et al, 1997), but with a
larger number of states (12) to handle name
prefixes and suffixes, and transliterated foreign
names separately. It operates on the output of a
word segmenter from Tsinghua University. It uses
a trigram model with dynamic backoff. The post-
processing rules correct some omissions and
systematic errors using name lists (for example, a
list of all Chinese last names; lists of organization
and location suffixes) and particular contextual
patterns (for example, verbs occurring with
people’s names). They also deal with
abbreviations and nested organization names.
</bodyText>
<subsectionHeader confidence="0.998655">
2.2 Chinese Coreference Resolver
</subsectionHeader>
<bodyText confidence="0.949233727272727">
For this study we have used a rule-based
coreference resolver. Table 1 lists the main rules
and patterns used. We have extensive rules for
name-name coreference, including rules specific to
the particular name types. For these experiments,
we do not attempt to resolve pronouns, and we
only resolve names with nominals when the name
and nominal appear in close proximity in a specific
structure, as listed in Table 1.
We have used the MUC coreference scoring
metric (Vilain et al, 1995) to evaluate this resolver,
excluding all pronouns and limiting ourselves to
noun phrases of semantic type PER, ORG, and
GPE. Using a perfect (hand-generated) set of
mentions, we obtain a recall of 82.7% and
precision of 95.1%, for an F score of 88.47%.
2 This class is used in the U.S. Government’s ACE
evaluations; it excludes locations without governments,
such as bodies of water and mountains.
Using the mentions generated by our extraction
system, we obtain a recall of 74.3%, a precision of
84.5%, and an F score of 79.07%.3
</bodyText>
<sectionHeader confidence="0.991801" genericHeader="method">
3 Confidence Measures
</sectionHeader>
<bodyText confidence="0.996644">
In order to decide when we need to rely on
global (coreference) information for name tagging,
we want to have some assessment of the
confidence that the name tagger has in individual
tagging decisions. In this paper, we use two tools
to reach this goal. The first method is to use three
manually built proper name lists which include
common names of each type (selected from the
high frequency names in the user query blog of
COMPASS, a Chinese search engine, and name
lists provided by Linguistic Data Consortium; the
PER list includes 147 names, the GPE list 226
names, and the ORG list 130 names). Names on
these lists are accepted without further review.
The second method is to have the HMM tagger
compute a probability margin for the identification
of a particular name as being of a particular type.
Scheffer et al. (2001) used a similar method to
identify good candidates for tagging in an active
learner. During decoding, the HMM tagger seeks
the path of maximal probability through the Viterbi
lattice. Suppose we wish to evaluate the
confidence with which words wi, ..., wj are
identified as a name of type T. We compute
Margin (wi,..., wj; T) = log P1 – log P2
Here P1 is the maximum path probability and P2 is
the maximum probability among all paths for
which some word in wi, ..., wj is assigned a tag
other than T.
A large margin indicates greater confidence in
the tag assignment. If we exclude names tagged
with a margin below a threshold, we can increase
the precision of name tagging at some cost in recall.
Figure 1 shows the trade-off between margin
threshold and name recognition performance.
Names with a margin over 3.0 are accepted on this
basis.
</bodyText>
<footnote confidence="0.934871428571429">
3 In our scoring, we use the ACE keys and only score
mentions which appear in both the key and system
response. This therefore includes only mentions
identified as being in the ACE semantic categories by
both the key and the system response. Thus these
scores cannot be directly compared against coreference
scores involving all noun phrases.
</footnote>
<figure confidence="0.9593094">
99
97
95
93
91
89
87
85
0 1 2 3 4 5 6 7 8 9 10 11 12
Threshold
</figure>
<figureCaption confidence="0.980945">
Figure 1: Tradeoff between Margin Threshold and
name recognition performance
</figureCaption>
<sectionHeader confidence="0.897841" genericHeader="method">
4 Distribution of Name Errors
</sectionHeader>
<bodyText confidence="0.9999764375">
We consider now names which did not pass the
confidence measure tests: names not on the
common name list, which were tagged with a
margin below the threshold. We counted the
accuracy of these “obscure” names as a function of
the number of mentions in an entity; the results are
shown in Table 2.
The table shows that the accuracy of name
recognition increases as the entity includes more
mentions. In other words, if a name has more
coref-ed mentions, it is more likely to be correct.
This also provides us a linguistic intuition: if
people mention an obscure name in a text, they
tend to emphasize it later by repeating the same
name or describe it with nominal mentions.
The table also indicates that the accuracy of
single name entities (singletons) is much lower
than the overall accuracy. So, although they
constitute only about 10% of all names, increasing
their accuracy can significantly improve overall
performance. Coreference information can play a
great role here. Take the 157 PER singletons as an
example; 56% are incorrect names. Among these
incorrect names, 73% actually belong to the other
two name types. Many of these can be easily fixed
by searching for coreference to other mentions
without type restriction. Among the correct names,
71% can be confirmed by the presence of a title
word or a Chinese last name. From these
observations we can conclude that without strong
confirmation features, singletons are much less
likely to be correct names.
</bodyText>
<sectionHeader confidence="0.98064" genericHeader="method">
5 Incorporating Coreference Information
into Name Recognition
</sectionHeader>
<bodyText confidence="0.999942941176471">
We make use of several features of the
coreference relations a name is involved in; the
features are listed in Table 3. Using these features,
we built an independent classifier to predict if a
name identified by the baseline name tagger is
correct or not. (Note that this classifier is trained
on all name mentions, but during test only
‘obscure’ names which failed the tests in section 3
are processed by this classifier.) Each name
corresponds to a feature vector which consists of
the factors described in Table 3. The PER context
words are generated from the context patterns
described in (Ji and Luo, 2001). We used a
Support Vector Machine to implement the
classifier, because of its state-of-the-art
performance and good generalization ability. We
used a polynomial kernel of degree 3.
</bodyText>
<sectionHeader confidence="0.988254" genericHeader="method">
6 Name Rules based on Coreference
</sectionHeader>
<bodyText confidence="0.999786166666667">
Besides the factors in the above statistical model,
additional coreference information can be used to
filter and in some cases correct the tagging
produced by the HMM. We developed the
following rules to correct names generated by the
baseline tagger.
</bodyText>
<subsectionHeader confidence="0.998089">
6.1 Name Structure Errors
</subsectionHeader>
<bodyText confidence="0.99781">
Sometimes the Name tagger outputs names
which are too short (incomplete) or too long. We
can make use of the relation among mentions in
the same entity to fix them. For example, nested
ORGs are traditionally difficult to recognize
correctly. Errors in ORG names can take the
following forms:
</bodyText>
<listItem confidence="0.975216473684211">
(1) Head Missed. Examples: “rPP9-E*(M)/
Chinese Art (Group)”, “rPP9?;kt(9)/ Chinese
Student (Union)”, “��W�fjfj(Pf)/ Russian
Nuclear Power (Instituition)”
Rule 1: If an ORG name x is coref-ed with other
mentions with head y (an ORG suffix), and in the
original text x is immediately followed by y, then
tag xy instead of x; otherwise discard x.
(2) Modifier Missed. Rule 1 can also be used to
restore missed modifiers. For example, “(0-丁
f))Q?;k / (Edinburgh) University”; “(ff fy)�h
Pk�aA / (Peng Cheng) Limited Corporation”, and
some incomplete translated PER names such as
“(E)CWH / (Pa)lestine”.
(3) Name Too Long
Rule 2: If a name x has no coref-ed mentions
but part of it, x&apos;, is identical to a name in another
entity y, and y includes at least two mentions; then
tag x&apos; instead of x.
</listItem>
<table confidence="0.981509820512821">
.
Precision(%)
Rule Type Rule Description
Name &amp; All Ident(i, j) Mentioni and Mentionj are identical
Name
Abbrev(i, j) Mentioni is an abbreviation of Mentionj
Modifier(i, j) Mentionj = Modifier + “de” + Mentioni
Formal(i, j) Formal and informal ways of referring to the same entity
(Ex. “XP9P9 / American Defense Dept. &amp;
AA/ Pentagon”)
PER Substring(i, j) Mentioni is a substring of Mentionj
Title(i, j) Mentionj = Mentioni + title word; or
Mentionj = LastName + title word
ORG Head(i, j)
GPE j) Mentioni and Mentionj have the same headHead(i,
Mentioni and Mentionj have the same head
Capital(i, j) Mentioni: country name;
Mentionj: name of the capital of this country
Applied in restricted context.
Country(i, j) Mentioni and Mentionj are different names referring to the same
country.
(Ex. “rPP9 / China &amp; *9 / Huaxia &amp; of fP9 / Republic”)
Name &amp; All RSub(i, j) Namei is a right substring of Nominalj
Nominal
Apposition(i, j) Nominalj is the apposite of Namei
Modifier2(i, j) Nominalj = Determiner/Modifier + Namei/ head
GPE Ref(i, j) Nominalj = Namei + GPE Ref Word
(examples of GPE Ref Word: “��fflj / Side”, “A ff/Government”,
“of fP9 / Republic”, “nMHA�ff/ Municipality”)
Nominal&amp; All IdentN(i, j) Nominali and Nominalj are identical
Nominal
Modifier3(i, j) Nominalj = Determiner/Modifier + Nominali
Table1: Main rules used in the Coreference Resolver
Number of mentions 1 2 3 4 5 6 7 8 &gt;8
per entity
Name Type
PER 43.94 87.07 91.23 87.95 91.57 91.92 94.74 92.31 97.36
GPE 55.81 88.8 96.07 100 100 100 100 95.83 97.46
ORG 64.71 80.59 89.47 94.29 100 100 -- -- 100
</table>
<tableCaption confidence="0.986103">
Table 2 Accuracy(%) of ‘obscure’ name recognition
</tableCaption>
<table confidence="0.999124538461538">
Factor Description
Coreference Type Average of weights of coreference relations for which this mention
Weight is antecedent: 0.8 for name-name coreference; 0.5 for apposition;
0.3 for other name-nominal coreference
Mention First Is first name mention in the entity
Weight Mention
Head Includes head word of name
Idiom Name is part of an idiom
PER context For PER Name, has context word in text
PER title For PER Name, includes title word
ORG suffix For ORG Name, includes suffix word
Entity Weight Number of mentions in entity / total number of mentions in all
entities in document which include a name mention
</table>
<tableCaption confidence="0.998086">
Table 3 Coreference factors for name recognition
</tableCaption>
<subsectionHeader confidence="0.997929">
6.2 Name Type Errors
</subsectionHeader>
<bodyText confidence="0.998811243243243">
Some names are mistakenly recognized as other
name types. For example, the name tagger has
difficulty in distinguishing transliterated PER
name and transliterated GPE names.
To solve this problem we designed the
following rules based on the relation among
entities.
Rule 3: If namei is recognized as type1, the
entity it belongs to has only one mention; and
namej is recognized as type2, the entity it belongs
to has at least two mentions; and namei is identical
with namej or namei is a substring of namej, then
correct type1 to type2.
For example, if “ 克 里 姆 林 / Kremlin” is
mistakenly identified as PER, while “克里姆林宫
/ Kremlin Palace” is correctly identified as ORG,
and in coreference results, “克里姆林 / Kremlin”
belongs to a singleton entity, while “克里姆林宫 /
Kremlin Palace” has coref-ed mentions, then we
correct the type of “克里姆林 / Kremlin” to ORG.
Another common mistake gives rise to the
sequence “PER+title+PER”, because our name
tagger uses the title word as an important context
feature for a person name (either preceding or
following the title). But this is an impossible
structure in Chinese. We can also use coreference
information to fix it.
Rule 4: If “PER+title+PER” appears in the
name tagger’s output, then we discard the PER
name with lower coref certainty; and check
whether it is coref-ed to other mentions in a GPE
entity or ORG entity; if it is, correct the type.
Using this rule we can correctly identify “[斯里
兰卡 / Sri Lanka GPE] 总理 / Premier [班达拉耐
克 / Bandaranaike PER]”, instead of “[斯里兰卡 /
Sri Lanka PER] 总理 / Premier [班达拉耐克 /
Bandaranaike PER]”.
</bodyText>
<subsectionHeader confidence="0.991088">
6.3 Name Abbreviation Errors
</subsectionHeader>
<bodyText confidence="0.999052636363636">
Name abbreviations are difficult to recognize
correctly due to a lack of training data. Usually
people adopt a separate list of abbreviations or
design separate rules (Sun et al. 2002) to identify
them. But many wrong abbreviation names might
be produced. We find that coreference
information helps to select abbreviations.
Rule 5: If an abbreviation name has no coref-ed
mentions and it is not adjacent to another
abbreviation (ex. “中/China 美/America”), then
we discard it.
</bodyText>
<sectionHeader confidence="0.956199" genericHeader="method">
7 System Flow
</sectionHeader>
<bodyText confidence="0.9820185">
Combining all the methods presented above, the
flow of our final system is shown in Figure 2:
</bodyText>
<figure confidence="0.990889">
Coreference
Resolver
Coreference
Rules to fix name
SVM classifier to select
correct names using
coreference features
</figure>
<figureCaption confidence="0.998949">
Figure 2 System Flow
</figureCaption>
<sectionHeader confidence="0.994881" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994371">
8.1 Training and Test Data
</subsectionHeader>
<bodyText confidence="0.9352731">
For our experiments, we used the Beijing
University Insititute of Computational Linguistics
corpus – 2978 documents from the People’s Daily
in 1998, one million words with name tags – and
the training corpus for the 2003 ACE evaluation,
223 documents. 153 of our ACE documents were
used as our test set. 4 The 153 documents
contained 1614 names. Of the system-tagged
names, 959 were considered ‘obscure’: were not
on a name list and had a margin below the
threshold. These were the names to which the
rules and classifier were applied. We ran all the
following experiments using the MUC scorer.
4 The test set was divided into two parts, of 95
documents and 58 documents. We trained two name
tagger and classifier models, each time using one part
of the test set along with all the other documents, and
evaluated on the other part of the test set. The results
reported here are the combined results for the entire
test set.
</bodyText>
<figure confidence="0.9876">
Name Nominal
tagger mention
Input
Output
</figure>
<subsectionHeader confidence="0.970723">
8.2 Overall Performance Comparison
</subsectionHeader>
<bodyText confidence="0.9987495">
Table 4 shows the performance of the baseline
system; Table 5 the system with rule-based
corrections; and Table 6 the system with both
rules and the SVM classifier.
</bodyText>
<table confidence="0.9998182">
Name Precision Recall F
PER 90.9 88.2 89.5
GPE 82.3 90.8 86.3
ORG 92.1 91.8 91.9
ALL 87.8 90.5 89.1
</table>
<tableCaption confidence="0.928665">
Table 4 Baseline Name Tagger
</tableCaption>
<table confidence="0.9999562">
Name Precision Recall F
PER 93.3 87.5 90.3
GPE 83.5 90.4 86.8
ORG 90.9 92.1 91.5
ALL 88.5 90.3 89.4
</table>
<tableCaption confidence="0.997874">
Table 5 Results with Coref Rules Alone
</tableCaption>
<table confidence="0.9999532">
Name Precision Recall F
PER 95.7 84.4 89.7
GPE 88.0 91.7 89.8
ORG 94.5 91.2 92.8
ALL 92.2 89.6 90.9
</table>
<tableCaption confidence="0.982157">
Table 6 Results for Single Document System
</tableCaption>
<bodyText confidence="0.985946576923077">
The gains we observed from coreference within
single documents suggested that further
improvement might be possible by gathering
evidence from several related documents.5 We
did this in two stages. First, we clustered the 153
documents in the test set into 38 topical clusters.
Most (29) of the clusters had only two documents;
the largest had 28 documents. We then applied
the same procedures, treating the entire cluster as
a single document. This yielded another 1.0%
improvement in overall F score (Table 7).
The improvement in F score was consistent for
the larger clusters (3 or more documents): the F
score improved for 8 of those clusters and
remained the same for the 9th. To heighten the
multi-document benefit, we took 11 of the small
5 Borthwick (1999) did use some cross-document
information across the entire test corpus, maintaining
in effect a name cache for the corpus, in addition to one
for the document. No attempt was made to select or
cluster documents.
(2 document clusters) and enlarged them by
retrieving related documents from
sina.com.cn. In total, we added 52 texts to
these 11 clusters. The net result was a further
improvement of 0.3% in F score (Table 8).6
</bodyText>
<table confidence="0.9998238">
Name Precision Recall F
PER 93.3 86.8 90.5
GPE 95.2 90.0 92.5
ORG 92.9 91.7 92.3
ALL 93.8 90.1 91.9
</table>
<tableCaption confidence="0.813624">
Table 7 Results for Mutiple Document System
</tableCaption>
<table confidence="0.999932">
Name Precision Recall F
PER 94.7 87.1 90.7
GPE 95.6 89.6 92.5
ORG 95.8 90.3 93.0
ALL 95.4 89.2 92.2
</table>
<tableCaption confidence="0.9349245">
Table 8 Results for Mutiple Document System
with additional retrieved texts
</tableCaption>
<subsectionHeader confidence="0.998275">
8.3 Contribution of Coreference Features
</subsectionHeader>
<bodyText confidence="0.99941875">
Since feature selection is crucial to SVMs, we
did experiments to determine how precision
increased as each feature was added. The results
are shown in Figure 3. We can see that each
feature in the SVM helps to select correct names
from the output of the baseline name tagger,
although some (like FirstMention) are more
crucial than others.
</bodyText>
<figureCaption confidence="0.861877">
Figure 3 Contributions of features
</figureCaption>
<footnote confidence="0.822698333333333">
6 Scores are still computed on the 153 test
documents ; the retrieved documents are excluded
from the scoring.
</footnote>
<page confidence="0.708615">
93
</page>
<figure confidence="0.88295775">
87
Precision(%)
92
91
90
89
88
Feature
</figure>
<subsectionHeader confidence="0.86088">
8.4 Comparison to Cache Model
</subsectionHeader>
<bodyText confidence="0.999879">
Some named entity systems use a name cache,
in which tokens or complete names which have
been previously assigned a tag are available as
features in tagging the remainder of a document.
Other systems have made a second tagging pass
which uses information on token sequences
tagged in the first pass (Borthwick 1999), or have
used as features information about features
assigned to other instances of the same token
(Chieu and Ng 2002). Our system, while more
complex, makes use of a richer set of global
features, involving the detailed structure of
individual mentions, and in particular makes use
of both name – name and name – nominal
relations.
We have compared the performance of our
method (applied to single documents) with a
voted cache model, which takes into account the
number of times a particular name has been
previously assigned each type of tag:
</bodyText>
<table confidence="0.99876675">
System Precision Recall F
baseline 88.8 90.5 89.1
voted cache 87.6 92.8 90.1
current 92.2 89.6 90.9
</table>
<tableCaption confidence="0.999927">
Table 9. Comparison with voted cache
</tableCaption>
<bodyText confidence="0.999827166666667">
Compared to a simple voted cache model, our
model provides a greater improvement in name
recognition F score; in particular, it can
substantially increase the precision of name
recognition. The voted cache model can recover
some missed names, but at some loss in precision.
</bodyText>
<sectionHeader confidence="0.990117" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99967962962963">
In this paper, we presented a novel idea of
applying coreference information to improve
name recognition. We used both a statistical filter
based on a set of coreference features and rules
for correcting specific errors in name recognition.
Overall, we obtained an absolute improvement of
3.1% in F score. Put another way, we were able
to eliminate about 60% of erroneous name tags
with only a small loss in recall.
The methods were tested on a Chinese name
tagger, but most of the techniques should be
applicable to other languages. More generally, it
offers an example of using global and cross-
document information to improve local decisions
for information extraction. Such methods will be
important for breaking the ‘performance ceiling’
in many areas of information extraction.
In the future, we plan to experiment with
improvements in coreference resolution (in
particular, adding pronoun resolution) to see if we
can obtain further gains in name recognition. We
also intend to explore the production of multiple
tagging hypotheses by our statistical name tagger,
with the alternative hypotheses then reranked
using global information. This may allow us to
replace some of our hand-coded error-correction
rules with corpus-trained methods.
</bodyText>
<sectionHeader confidence="0.995955" genericHeader="acknowledgments">
10 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9977981">
This research was supported by the Defense
Advanced Research Projects Agency as part of the
Translingual Information Detection, Extraction
and Summarization (TIDES) program, under
Grant N66001-001-1-8917 from the Space and
Naval Warfare Systems Center San Diego, and by
the National Science Foundation under Grants
IIS-0081962 and 0325657. This paper does not
necessarily reflect the position or the policy of the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884338983051">
Daniel M. Bikel, Scott Miller, Richard Schuartz,
and Ralph Weischedel. 1999. Nymble: a high-
performance Learning Name-finder. Proc. Fifth
Conf. On Applied Natural Language Processing,
Washington, D.C.
Andrew Borthwick. 1999. A Maximum Entropy
Approach to Named Entity Recognition. Ph.D.
Dissertation, Dept. of Computer Science, New
York University.
Andrew Borthwick, John Sterling, Eugene
Agichtein, and Ralph Grishman. 1998.
Exploiting Diverse Knowledge Sources via
Maximum Entropy in Named Entity
Recognition. Proc. Sixth Workshop on Very
Large Corpora, Montreal, Canada.
Hai Leong Chieu and Hwee Tou Ng. 2002.
Named Entity Recognition: A Maximum
Entropy Approach Using Global Information.
Proc.: 17th Int’l Conf. on Computational
Linguistics (COLING 2002), Taipei, Taiwan.
Ralph Grishman and Beth Sundheim. 1996.
Message understanding conference - 6: A brief
history. Proc. 16th Int’l Conference on
Computational Linguistics (COLING 96),
Copenhagen.
Heng Ji, Zhensheng Luo, 2001. A Chinese Name
Identifying System Based on Inverse Name
Frequency Model and Rules. Natural Language
Processing and Knowledge Engineering
(NLPKE) Mini Symposium of 2001 IEEE
International Conference on Systems, Man, and
Cybernetics (SMC2001)
Andrew McCallum and Wei Li. 2003. Early
results for Named Entity Recognition With
Conditional Random Fields, Feature Induction,
and Web-Enhanced Lexicons. Proc. Seventh
Conf. on Computational Natural Language
Learning (CONLL-2003), Edmonton, Canada.
Tobias Scheffer, Christian Decomain, and Stefan
Wrobel. 2001. Active Hidden Markov Models
for Information Extraction. Proc. Int’l
Symposium on Intelligent Data Analysis (IDA-
2001).
Satoshi Sekine, Ralph Grishman and Hiroyuki
Shinnou. 1998. A Decision Tree Method for
Finding and Classifying Names in Japanese
Texts. Proc. Sixth Workshop on Very Large
Corpora; Montreal, Canada.
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou
and Changning Huang. 2002. Chinese Named
Entity Identification Using Class-based
Language Model. Coling 2002.
Marc Vilain, John Burger, John Aberdeen, Dennis
Connelly, Lynette Hirschman. 1995. A model --
Theoretic Coreference Scoring Scheme. MUC-6
Proceedings, Nov. 1995.
Shiren Ye, Tat-Seng Chua, Liu Jimin. 2002. An
Agent-based Approach to Chinese Named
Entity Recognition. Coling 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976810">
<title confidence="0.999895">Applying Coreference to Improve Name Recognition</title>
<author confidence="0.9995">Heng JI</author>
<author confidence="0.9995">Ralph GRISHMAN</author>
<affiliation confidence="0.9989915">Department of Computer Science New York University</affiliation>
<address confidence="0.9970395">Broadway, Floor New York, NY 10003, U.S.A.</address>
<email confidence="0.999958">hengji@cs.nyu.edu,grishman@cs.nyu.edu</email>
<abstract confidence="0.999105764705882">We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese. We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger. For names with low confidence, we show how these names can be filtered using coreference features to improve accuracy. In addition, we present rules which use coreference information to correct some name tagging errors. Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schuartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance Learning Name-finder.</title>
<date>1999</date>
<booktitle>Proc. Fifth Conf. On Applied Natural Language Processing,</booktitle>
<location>Washington, D.C.</location>
<marker>Bikel, Miller, Schuartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schuartz, and Ralph Weischedel. 1999. Nymble: a highperformance Learning Name-finder. Proc. Fifth Conf. On Applied Natural Language Processing, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. Dissertation,</tech>
<institution>Dept. of Computer Science, New York University.</institution>
<contexts>
<context position="19336" citStr="Borthwick (1999)" startWordPosition="3214" endWordPosition="3215">al related documents.5 We did this in two stages. First, we clustered the 153 documents in the test set into 38 topical clusters. Most (29) of the clusters had only two documents; the largest had 28 documents. We then applied the same procedures, treating the entire cluster as a single document. This yielded another 1.0% improvement in overall F score (Table 7). The improvement in F score was consistent for the larger clusters (3 or more documents): the F score improved for 8 of those clusters and remained the same for the 9th. To heighten the multi-document benefit, we took 11 of the small 5 Borthwick (1999) did use some cross-document information across the entire test corpus, maintaining in effect a name cache for the corpus, in addition to one for the document. No attempt was made to select or cluster documents. (2 document clusters) and enlarged them by retrieving related documents from sina.com.cn. In total, we added 52 texts to these 11 clusters. The net result was a further improvement of 0.3% in F score (Table 8).6 Name Precision Recall F PER 93.3 86.8 90.5 GPE 95.2 90.0 92.5 ORG 92.9 91.7 92.3 ALL 93.8 90.1 91.9 Table 7 Results for Mutiple Document System Name Precision Recall F PER 94.7</context>
<context position="20995" citStr="Borthwick 1999" startWordPosition="3497" endWordPosition="3498">line name tagger, although some (like FirstMention) are more crucial than others. Figure 3 Contributions of features 6 Scores are still computed on the 153 test documents ; the retrieved documents are excluded from the scoring. 93 87 Precision(%) 92 91 90 89 88 Feature 8.4 Comparison to Cache Model Some named entity systems use a name cache, in which tokens or complete names which have been previously assigned a tag are available as features in tagging the remainder of a document. Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002). Our system, while more complex, makes use of a richer set of global features, involving the detailed structure of individual mentions, and in particular makes use of both name – name and name – nominal relations. We have compared the performance of our method (applied to single documents) with a voted cache model, which takes into account the number of times a particular name has been previously assigned each type of tag: System Precision Recall F baseline 88.8 90.5 89.1 vot</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. Dissertation, Dept. of Computer Science, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
<author>John Sterling</author>
<author>Eugene Agichtein</author>
<author>Ralph Grishman</author>
</authors>
<title>Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition.</title>
<date>1998</date>
<booktitle>Proc. Sixth Workshop on Very Large Corpora,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1263" citStr="Borthwick et al. 1998" startWordPosition="187" endWordPosition="190">to correct some name tagging errors. Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results r</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Andrew Borthwick, John Sterling, Eugene Agichtein, and Ralph Grishman. 1998. Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. Proc. Sixth Workshop on Very Large Corpora, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named Entity Recognition: A Maximum Entropy Approach Using Global Information.</title>
<date>2002</date>
<booktitle>Proc.: 17th Int’l Conf. on Computational Linguistics (COLING</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1283" citStr="Chieu and Ng 2002" startWordPosition="191" endWordPosition="194">gging errors. Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese </context>
<context position="21114" citStr="Chieu and Ng 2002" startWordPosition="3515" endWordPosition="3518"> 6 Scores are still computed on the 153 test documents ; the retrieved documents are excluded from the scoring. 93 87 Precision(%) 92 91 90 89 88 Feature 8.4 Comparison to Cache Model Some named entity systems use a name cache, in which tokens or complete names which have been previously assigned a tag are available as features in tagging the remainder of a document. Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002). Our system, while more complex, makes use of a richer set of global features, involving the detailed structure of individual mentions, and in particular makes use of both name – name and name – nominal relations. We have compared the performance of our method (applied to single documents) with a voted cache model, which takes into account the number of times a particular name has been previously assigned each type of tag: System Precision Recall F baseline 88.8 90.5 89.1 voted cache 87.6 92.8 90.1 current 92.2 89.6 90.9 Table 9. Comparison with voted cache Compared to a simple voted cache mo</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2002. Named Entity Recognition: A Maximum Entropy Approach Using Global Information. Proc.: 17th Int’l Conf. on Computational Linguistics (COLING 2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference - 6: A brief history.</title>
<date>1996</date>
<booktitle>Proc. 16th Int’l Conference on Computational Linguistics (COLING 96),</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="1085" citStr="Grishman and Sundheim, 1996" startWordPosition="159" endWordPosition="162">. For names with low confidence, we show how these names can be filtered using coreference features to improve accuracy. In addition, we present rules which use coreference information to correct some name tagging errors. Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference - 6: A brief history. Proc. 16th Int’l Conference on Computational Linguistics (COLING 96), Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Zhensheng Luo</author>
</authors>
<title>A Chinese Name Identifying System Based on Inverse Name Frequency Model and Rules.</title>
<date>2001</date>
<booktitle>Natural Language Processing and Knowledge Engineering (NLPKE) Mini Symposium of 2001 IEEE International Conference on Systems, Man, and Cybernetics (SMC2001)</booktitle>
<contexts>
<context position="10753" citStr="Ji and Luo, 2001" startWordPosition="1760" endWordPosition="1763">ition We make use of several features of the coreference relations a name is involved in; the features are listed in Table 3. Using these features, we built an independent classifier to predict if a name identified by the baseline name tagger is correct or not. (Note that this classifier is trained on all name mentions, but during test only ‘obscure’ names which failed the tests in section 3 are processed by this classifier.) Each name corresponds to a feature vector which consists of the factors described in Table 3. The PER context words are generated from the context patterns described in (Ji and Luo, 2001). We used a Support Vector Machine to implement the classifier, because of its state-of-the-art performance and good generalization ability. We used a polynomial kernel of degree 3. 6 Name Rules based on Coreference Besides the factors in the above statistical model, additional coreference information can be used to filter and in some cases correct the tagging produced by the HMM. We developed the following rules to correct names generated by the baseline tagger. 6.1 Name Structure Errors Sometimes the Name tagger outputs names which are too short (incomplete) or too long. We can make use of t</context>
</contexts>
<marker>Ji, Luo, 2001</marker>
<rawString>Heng Ji, Zhensheng Luo, 2001. A Chinese Name Identifying System Based on Inverse Name Frequency Model and Rules. Natural Language Processing and Knowledge Engineering (NLPKE) Mini Symposium of 2001 IEEE International Conference on Systems, Man, and Cybernetics (SMC2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for Named Entity Recognition With Conditional Random Fields, Feature Induction, and Web-Enhanced Lexicons.</title>
<date>2003</date>
<booktitle>Proc. Seventh Conf. on Computational Natural Language Learning (CONLL-2003),</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1370" citStr="McCallum and Li 2003" startWordPosition="204" endWordPosition="207">nts and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese named entity recognition, on the MET-2 test corpus, are 0.92 to 0.95 F-measure for the </context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for Named Entity Recognition With Conditional Random Fields, Feature Induction, and Web-Enhanced Lexicons. Proc. Seventh Conf. on Computational Natural Language Learning (CONLL-2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Scheffer</author>
<author>Christian Decomain</author>
<author>Stefan Wrobel</author>
</authors>
<title>Active Hidden Markov Models for Information Extraction.</title>
<date>2001</date>
<booktitle>Proc. Int’l Symposium on Intelligent Data Analysis (IDA2001).</booktitle>
<contexts>
<context position="7256" citStr="Scheffer et al. (2001)" startWordPosition="1157" endWordPosition="1160">is paper, we use two tools to reach this goal. The first method is to use three manually built proper name lists which include common names of each type (selected from the high frequency names in the user query blog of COMPASS, a Chinese search engine, and name lists provided by Linguistic Data Consortium; the PER list includes 147 names, the GPE list 226 names, and the ORG list 130 names). Names on these lists are accepted without further review. The second method is to have the HMM tagger compute a probability margin for the identification of a particular name as being of a particular type. Scheffer et al. (2001) used a similar method to identify good candidates for tagging in an active learner. During decoding, the HMM tagger seeks the path of maximal probability through the Viterbi lattice. Suppose we wish to evaluate the confidence with which words wi, ..., wj are identified as a name of type T. We compute Margin (wi,..., wj; T) = log P1 – log P2 Here P1 is the maximum path probability and P2 is the maximum probability among all paths for which some word in wi, ..., wj is assigned a tag other than T. A large margin indicates greater confidence in the tag assignment. If we exclude names tagged with </context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001. Active Hidden Markov Models for Information Extraction. Proc. Int’l Symposium on Intelligent Data Analysis (IDA2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
<author>Hiroyuki Shinnou</author>
</authors>
<title>A Decision Tree Method for Finding and Classifying Names in Japanese Texts.</title>
<date>1998</date>
<booktitle>Proc. Sixth Workshop on Very Large Corpora;</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1320" citStr="Sekine et al. 1998" startWordPosition="197" endWordPosition="200">hese gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese named entity recognition, on the MET-</context>
</contexts>
<marker>Sekine, Grishman, Shinnou, 1998</marker>
<rawString>Satoshi Sekine, Ralph Grishman and Hiroyuki Shinnou. 1998. A Decision Tree Method for Finding and Classifying Names in Japanese Texts. Proc. Sixth Workshop on Very Large Corpora; Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Sun</author>
<author>Jianfeng Gao</author>
<author>Lei Zhang</author>
</authors>
<title>Ming Zhou and Changning Huang.</title>
<date>2002</date>
<contexts>
<context position="1416" citStr="Sun et al. 2002" startWordPosition="211" endWordPosition="214">lusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese named entity recognition, on the MET-2 test corpus, are 0.92 to 0.95 F-measure for the different name types (Ye et al. 2002). two wor</context>
<context position="16418" citStr="Sun et al. 2002" startWordPosition="2714" endWordPosition="2717">+title+PER” appears in the name tagger’s output, then we discard the PER name with lower coref certainty; and check whether it is coref-ed to other mentions in a GPE entity or ORG entity; if it is, correct the type. Using this rule we can correctly identify “[斯里 兰卡 / Sri Lanka GPE] 总理 / Premier [班达拉耐 克 / Bandaranaike PER]”, instead of “[斯里兰卡 / Sri Lanka PER] 总理 / Premier [班达拉耐克 / Bandaranaike PER]”. 6.3 Name Abbreviation Errors Name abbreviations are difficult to recognize correctly due to a lack of training data. Usually people adopt a separate list of abbreviations or design separate rules (Sun et al. 2002) to identify them. But many wrong abbreviation names might be produced. We find that coreference information helps to select abbreviations. Rule 5: If an abbreviation name has no coref-ed mentions and it is not adjacent to another abbreviation (ex. “中/China 美/America”), then we discard it. 7 System Flow Combining all the methods presented above, the flow of our final system is shown in Figure 2: Coreference Resolver Coreference Rules to fix name SVM classifier to select correct names using coreference features Figure 2 System Flow 8 Experiments 8.1 Training and Test Data For our experiments, w</context>
</contexts>
<marker>Sun, Gao, Zhang, 2002</marker>
<rawString>Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou and Changning Huang. 2002. Chinese Named Entity Identification Using Class-based Language Model. Coling 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connelly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model --Theoretic Coreference Scoring Scheme.</title>
<date>1995</date>
<booktitle>MUC-6 Proceedings,</booktitle>
<contexts>
<context position="5873" citStr="Vilain et al, 1995" startWordPosition="919" endWordPosition="922">bs occurring with people’s names). They also deal with abbreviations and nested organization names. 2.2 Chinese Coreference Resolver For this study we have used a rule-based coreference resolver. Table 1 lists the main rules and patterns used. We have extensive rules for name-name coreference, including rules specific to the particular name types. For these experiments, we do not attempt to resolve pronouns, and we only resolve names with nominals when the name and nominal appear in close proximity in a specific structure, as listed in Table 1. We have used the MUC coreference scoring metric (Vilain et al, 1995) to evaluate this resolver, excluding all pronouns and limiting ourselves to noun phrases of semantic type PER, ORG, and GPE. Using a perfect (hand-generated) set of mentions, we obtain a recall of 82.7% and precision of 95.1%, for an F score of 88.47%. 2 This class is used in the U.S. Government’s ACE evaluations; it excludes locations without governments, such as bodies of water and mountains. Using the mentions generated by our extraction system, we obtain a recall of 74.3%, a precision of 84.5%, and an F score of 79.07%.3 3 Confidence Measures In order to decide when we need to rely on glo</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connelly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connelly, Lynette Hirschman. 1995. A model --Theoretic Coreference Scoring Scheme. MUC-6 Proceedings, Nov. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiren Ye</author>
<author>Tat-Seng Chua</author>
<author>Liu Jimin</author>
</authors>
<title>An Agent-based Approach to Chinese Named Entity Recognition. Coling</title>
<date>2002</date>
<contexts>
<context position="1455" citStr="Ye et al. 2002" startWordPosition="217" endWordPosition="220"> absolute improvement of about 3.1% in tagger F score. 1 Introduction The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese named entity recognition, on the MET-2 test corpus, are 0.92 to 0.95 F-measure for the different name types (Ye et al. 2002). two words preceding and following the name. If</context>
</contexts>
<marker>Ye, Chua, Jimin, 2002</marker>
<rawString>Shiren Ye, Tat-Seng Chua, Liu Jimin. 2002. An Agent-based Approach to Chinese Named Entity Recognition. Coling 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>