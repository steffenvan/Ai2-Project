<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.937962">
Intonational Boundaries, Speech Repairs and
Discourse Markers: Modeling Spoken Dialog
</title>
<author confidence="0.677019">
Peter A. Heeman and James F. Allen
</author>
<affiliation confidence="0.869853">
Department of Computer Science
University of Rochester
Rochester NY 14627, USA
</affiliation>
<email confidence="0.952741">
{heeman,james}Ocs.rochester.edu
</email>
<sectionHeader confidence="0.994502" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984875">
To understand a speaker&apos;s turn of a con-
versation, one needs to segment it into in-
tonational phrases, clean up any speech re-
pairs that might have occurred, and iden-
tify discourse markers. In this paper, we
argue that these problems must be resolved
together, and that they must be resolved
early in the processing stream. We put for-
ward a statistical language model that re-
solves these problems, does POS tagging,
and can be used as the language model of
a speech recognizer. We find that by ac-
counting for the interactions between these
tasks that the performance on each task
improves, as does POS tagging and per-
plexity.
</bodyText>
<sectionHeader confidence="0.997908" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995437567164179">
Interactive spoken dialog provides many new chal-
lenges for natural language understanding systems.
One of the most critical challenges is simply de-
termining the speaker&apos;s intended utterances: both
segmenting the speaker&apos;s turn into utterances and
determining the intended words in each utterance.
Since there is no well-agreed to definition of what
an utterance is, we instead focus on intonational
phrases (Silverman et al., 1992), which end with an
acoustically signaled boundary tone. Even assuming
perfect word recognition, the problem of determin-
ing the intended words is complicated due to the
occurrence of speech repairs, which occur where the
speaker goes back and changes (or repeats) some-
thing she just said. The words that are replaced
or repeated are no longer part of the intended ut-
terance, and so need to be identified. The follow-
ing example, from the Trains corpus (Heeman and
Allen, 1995), gives an example of a speech repair
with the words that the speaker intends to be re-
placed marked by reparandum, the words that are
the intended replacement marked as alteration, and
the cue phrases and filled pauses that tend to occur
in between marked as the editing term.
Example 1 (d92a-5.2 utt34)
we&apos;ll pick up a tank of uh the tanker of oranges
reparandurni editing term alteration
interruption point
Much work has been done on both detect-
ing boundary tones (e.g. (Wang and Hirschberg,
1992; Wightman and Ostendorf, 1994; Stolcke and
Shriberg, 1996a; Kompe et al., 1994; Mast et al.,
1996)) and on speech repair detection and correction
(e.g. (Hindle, 1983; Bear, Dowding, and Shriberg,
1992; Nakatani and Hirschberg, 1994; Heeman and
Allen, 1994; Stolcke and Shriberg, 1996b)). This
work has focused on one of the issues in isolation of
the other. However, these two issues are intertwined.
Cues such as the presence of silence, final syllable
lengthening, and presence of filled pauses tend to
mark both events. Even the presence of word cor-
respondences, a tradition cue for detecting and cor-
recting speech repairs, sometimes marks boundary
tones as well, as illustrated by the following example
where the intonational phrase boundary is marked
with the ToBI symbol %.
Example 2 (d93-83.3 utt73)
that&apos;s all you need % you only need one boxcar
Intonational phrases and speech repairs also in-
teract with the identification of discourse markers.
Discourse markers (Schiffrin, 1987; Hirschberg and
Litman, 1993; Byron and Heeman, 1997) are used
to relate new speech to the current discourse state.
Lexical items that can function as discourse mark-
ers, such as &amp;quot;well&amp;quot; and &amp;quot;okay,&amp;quot; are ambiguous as to
whether they are being used as discourse markers
or not. The complication is that discourse markers
tend to be used to introduce a new utterance, or
can be an utterance all to themselves (such as the
acknowledgment &amp;quot;okay&amp;quot; or &amp;quot;alright&amp;quot;), or can be used
as part of the editing term of a speech repair, or to
begin the alteration. Hence, the problem of identi-
fying discourse markers also needs to be addressed
with the segmentation and speech repair problems.
These three phenomena of spoken dialog, however,
cannot be resolved without recourse to syntactic in-
formation. Speech repairs, for example, are often
</bodyText>
<page confidence="0.99632">
254
</page>
<bodyText confidence="0.999978607142857">
signaled by syntactic anomalies. Furthermore, in
order to determine the extent of the reparandum,
one needs to take into account the parallel structure
that typically exists between the reparandum and al-
teration, which relies on at identifying the syntactic
roles, or part-of-speech (POS) tags, of the words in-
volved (Bear, Dowding, and Shriberg, 1992; Heeman
and Allen, 1994). However, speech repairs disrupt
the context that is needed to determine the POS
tags (Hindle, 1983). Hence, speech repairs, as well
as boundary tones and discourse markers, must be
resolved during syntactic disambiguation.
Of course when dealing with spoken dialogue, one
cannot forget the initial problem of determining the
actual words that the speaker is saying. Speech rec-
ognizers rely on being able to predict the probabil-
ity of what word will be said next. Just as intona-
tional phrases and speech repairs disrupt the local
context that is needed for syntactic disambiguation,
the same holds for predicting what word will come
next. If a speech repair or intonational phrase oc-
curs, this will alter the probability estimate. But
more importantly, speech repairs and intonational
phrases have acoustic correlates such as the pres-
ence of silence. Current speech recognition language
models cannot account for the presence of silence,
and tend to simply ignore it. By modeling speech re-
pairs and intonational boundaries, we can take into
account the acoustic correlates and hence use more
of the available information.
From the above discussion, it is clear that we need
to model these dialogue phenomena together and
very early on in the speech processing stream, in
fact, during speech recognition. Currently, the ap-
proaches that work best in speech recognition are
statistical approaches that are able to assign proba-
bility estimates for what word will occur next given
the previous words. Hence, in this paper, we in-
troduce a statistical language model that can de-
tect speech repairs, boundary tones, and discourse
markers, and can assign POS tags, and can use this
information to better predict what word will occur
next.
In the rest of the paper, we first introduce the
Trains corpus. We then introduce a statistical lan-
guage model that incorporates POS tagging and the
identification of discourse markers. We then aug-
ment this model with speech repair detection and
correction and intonational boundary tone detec-
tion. We then present the results of this model on
the Trains corpus and show that it can better ac-
count for these discourse events than can be achieved
by modeling them individually. We also show that
by modeling these two phenomena that we can in-
crease our POS tagging performance by 8.6%, and
improve our ability to predict the next word.
</bodyText>
<table confidence="0.998429545454545">
Dialogs 98
Speakers 34
Words 58298
Turns 6163
Discourse Markers 8278
Boundary Tones 10947
Turn-Internal Boundary Tones 5535
Abridged Repairs 423
Modification Repairs 1302
Fresh Starts 671
Editing Terms 1128
</table>
<tableCaption confidence="0.9813215">
Table 1: Frequency of Tones, Repairs and Editing
Terms in the Trains Corpus
</tableCaption>
<sectionHeader confidence="0.971296" genericHeader="introduction">
2 Trains Corpus
</sectionHeader>
<bodyText confidence="0.9311284">
As part of the TRAINS project (Allen et al., 1995),
which is a long term research project to build a con-
versationally proficient planning assistant, we have
collected a corpus of problem solving dialogs (Hee-
man and Allen, 1995). The dialogs involve two hu-
man participants, one who is playing the role of a
user and has a certain task to accomplish, and an-
other who is playing the role of the system by acting
as a planning assistant. The collection methodology
was designed to make the setting as close to human-
computer interaction as possible, but was not a wiz-
ard scenario, where one person pretends to be a com-
puter. Rather, the user knows that he is talking to
another person.
The TRAINS corpus consists of about six and half
hours of speech. Table 1 gives some general statistics
about the corpus, including the number of dialogs,
speakers, words, speaker turns, and occurrences of
discourse markers, boundary tones and speech re-
pairs.
The speech repairs in the Trains corpus have been
hand-annotated. We have divided the repairs into
three types: fresh starts, modification repairs, and
abridged repairs.&apos; A fresh start is where the speaker
abandons the current utterance and starts again,
where the abandonment seems acoustically signaled.
Example 3 (d93-12.1 utt30)
so it&apos;ll take urn so you want to do what
reparandum I editing term alteration
interruption point
The second type of repairs are the modification re-
pairs. These include all other repairs in which the
reparandum is not empty.
Example 4 (d92a-1.3 utt65)
so that will total will take seven hours to do that
</bodyText>
<footnote confidence="0.8287765">
reparandural alteration
interruption point
1This classification is similar to that of Hindle (1983)
and Levelt (1983).
</footnote>
<page confidence="0.998274">
255
</page>
<bodyText confidence="0.9737282">
The third type of repairs are the abridged repairs,
which consist solely of an editing term. Note that
utterance initial filled pauses are not treated as
abridged repairs.
Example 5 (d93-14.3 utt42)
we need to um manage to get the bananas to Dansville
I editing term
interruption point
There is typically a correspondence between
the reparandum and the alteration, and following
Bear et al. (1992), we annotate this using the la-
bels m for word matching and r for word replace-
ments (words of the same syntactic category). Each
pair is given a unique index. Other words in the
reparandum and alteration are annotated with an
x. Also, editing terms (filled pauses and clue words)
are labeled with et, and the interruption point with
ip, which will occur before any editing terms asso-
ciated with the repair, and after a word fragment,
if present. The interruption point is also marked as
to whether the repair is a fresh start, modification
repair, or abridged repair, in which cases, we use
ip:can, ip:mod and ip:abr, respectively. The ex-
ample below illustrates how a repair is annotated in
this scheme.
</bodyText>
<equation confidence="0.6875465">
Example 6 (d93-15.2 utt42)
engine two from Elmi(ra)- or engine three from Elmira
ml r2 m3 m4 Tet ml r2 m3 m4
ip:mod
</equation>
<sectionHeader confidence="0.62606" genericHeader="method">
3 A POS-Based Language Model
</sectionHeader>
<bodyText confidence="0.999829052631579">
The goal of a speech recognizer is to find the se-
quence of words W that is maximal given the acous-
tic signal A. However, for detecting and correcting
speech repairs, and identifying boundary tones and
discourse markers, we need to augment the model
so that it incorporates shallow statistical analysis, in
the form of POS tagging. The POS tagset, based on
the Penn Treebank tagset (Marcus, Santorini, and
Marcinkiewicz, 1993), includes special tags for de-
noting when a word is being used as a discourse
marker. In this section, we give an overview of our
basic language model that incorporates POS tag-
ging. Full details can be found in (Beeman and
Allen, 1997; Heeman, 1997).
To add in POS tagging, we change the goal of the
speech recognition process to find the best word and
POS tags given the acoustic signal. The derivation
of the acoustic model and language model is now as
follows.
</bodyText>
<equation confidence="0.8736835">
WP = arg max Pr( WP IA)
W,P
arg max Pr(AIWP)Pr(WP)
WP Pr(A)
arg max Pr(A I W P)Pr(W P)
WP
</equation>
<bodyText confidence="0.999826428571429">
The first term Pr(AIWP) is the factor due to
the acoustic model, which we can approximate by
Pr(AIW). The second term Pr(WP) is the factor
due to the language model. We rewrite Pr(WP) as
Pr(Wi,NPLN), where N is the number of words in
the sequence. We now rewrite the language model
probability as follows.
</bodyText>
<equation confidence="0.99894">
Pr(Wi.dvPi,N)
HPr(wi Pl,i-1)
i=1,N
= JJ Pr(WilWi,i-1P1,i) W1,i-1P1,i-1)
i=1,N
</equation>
<bodyText confidence="0.99982982051282">
We now have two probability distributions that we
need to estimate, which we do using decision trees
(Breiman et al., 1984; Bahl et al., 1989). The de-
cision tree algorithm has the advantage that it uses
information theoretic measures to construct equiva-
lence classes of the context in order to cope with
sparseness of data. The decision tree algorithm
starts with all of the training data in a single leaf
node. For each leaf node, it looks for the question
to ask of the context such that splitting the node
into two leaf nodes results in the biggest decrease
in impurity, where the impurity measures how well
each leaf predicts the events in the node. After the
tree is grown, a heldout dataset is used to smooth
the probabilities of each node with its parent (Bahl
et al., 1989).
To allow the decision tree to ask about the words
and POS tags in the context, we cluster the words
and POS tags using the algorithm of Brown et
al. (1992) into a binary classification tree. This gives
an implicit binary encoding for each word and POS
tag, thus allowing the decision tree to ask about the
words and POS tags using simple binary questions,
such as &apos;is the third bit of the POS tag encoding
equal to one?&apos; Figure 1 shows a POS classification
tree. The binary encoding for a POS tag is deter-
mined by the sequence of top and bottom edges that
leads from the root node to the node for the POS
tag.
Unlike other work (e.g. (Black et al., 1992; Mager-
man, 1995)), we treat the word identities as a further
refinement of the POS tags; thus we build a word
classification tree for each POS tag. This has the
advantage of avoiding unnecessary data fragmenta-
tion, since the POS tags and word identities are no
longer separate sources of information. As well, it
constrains the task of building the word classifica-
tion trees since the major distinctions are captured
by the PUS classification tree.
</bodyText>
<sectionHeader confidence="0.986433" genericHeader="method">
4 Augmenting the Model
</sectionHeader>
<bodyText confidence="0.995539666666667">
Just as we redefined the speech recognition prob-
lem so as to account for POS tagging and identify-
ing discourse markers, we do the same for modeling
</bodyText>
<page confidence="0.997158">
256
</page>
<figureCaption confidence="0.999802">
Figure 1: POS Classification Tree
</figureCaption>
<bodyText confidence="0.9990412">
boundary tones and speech repairs. We introduce
null tokens between each pair of consecutive words
wi..1 and wi (Heeman and Allen, 1994), which will
be tagged as to the occurrence of these events. The
boundary tone tag Ti indicates if word wi_1 ends an
intonational boundary (Ti=T), or not (Ti=null).
For detecting speech repairs, we have the prob-
lem that repairs are often accompanied by an edit-
ing term, such as &amp;quot;urn&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;okay&amp;quot;, or &amp;quot;well&amp;quot;,
and these must be identified as such. Furthermore,
an editing term might be composed of a number of
words, such as &amp;quot;let&apos;s see&amp;quot; or &amp;quot;uh well&amp;quot;. Hence we use
two tags: an editing term tag Ei and a repair tag Ri.
The editing term tag indicates if wi starts an edit-
ing term (Ei=Push), if wi continues an editing term
(Ei=ET), if wi_1 ends an editing term (Ei=Pop),
or otherwise (Ei=null). The repair tag Ri indicates
whether word wi is the onset of the alteration of a
fresh start (Ri=C), a modification repair (Ri=M),
or an abridged repair (Ri=A), or there is not a re-
pair (Ri=null). Note that for repairs with an edit-
ing term, the repair is tagged after the extent of the
editing term has been determined. Below we give an
example showing all non-null tone, editing term and
repair tags.
</bodyText>
<subsectionHeader confidence="0.298681">
Example 7 (d93-18.1 utt47)
</subsectionHeader>
<bodyText confidence="0.983848166666667">
it takes one Push you ET know Pop M two hours T
If a modification repair or fresh start occurs,
we need to determine the extent (or the onset)
of the reparandum, which we refer to as correct-
ing the speech repair. Often, speech repairs have
strong word correspondences between the reparan-
</bodyText>
<equation confidence="0.767109">
we&apos;ll pick up a tank of uh the tanker of oranges
t
</equation>
<figureCaption confidence="0.981958">
Figure 2: Cross Serial Correspondences
</figureCaption>
<bodyText confidence="0.99697052631579">
dum and alteration, involving word matches and
word replacements. Hence, knowing the extent of
the reparandum means that we can use the reparan-
dum to predict the words (and their POS tags) that
make up the alteration. For Ri E {Mod, Can}, we
define Oi to indicate the onset of the reparandum.2
If we are in the midst of processing a repair, we
need to determine if there is a word correspondence
from the reparandum to the current word wi. The
tag Li is used to indicate which word in the reparan-
dum is licensing the correspondence. Word cor-
respondences tend to exhibit a cross serial depen-
dency; in other words if we have a correspondence
between wi in the reparandum and wk in the alter-
ation, any correspondence with a word in the alter-
ation after wk will be to a word that is after wj, as il-
lustrated in Figure 2. This means that if wi involves
a word correspondence, it will most likely be with a
word that follows the last word in the reparandum
that has a word correspondence. Hence, we restrict
Li to only those words that are after the last word in
the reparandum that has a correspondence (or from
the reparandum onset if there is not yet a correspon-
dence). If there is no word correspondence for wi, we
set Li to the first word after the last correspondence.
The second tag involved in the correspondences is
Ci, which indicates the type of correspondence be-
tween the word indicated by Li and the current word
wi. We focus on word correspondences that involve
either a word match (C=m), a word replacement
(Ci=r), where both words are of the same POS tag,
or no correspondence (Ci=x).
Now that we have defined these six additional tags
for modeling boundary tones and speech repairs, we
redefine the speech recognition problem so that its
goal is to find the maximal assignment for the words
as well as the POS, boundary tone, and speech repair
tags.
</bodyText>
<equation confidence="0.975834375">
W PCLORET = arg max Pr(WCLORETIA)
WPCLORET
The result is that we now have eight probability dis-
tributions that we need to estimate.
Pr(Ti I L1,1-1 0i,i-i )
Pr(Ei E1,1-1 )
Pr (Ri 11/1(1,i-i L1,1-101,1-1 )
Pr(Li I 1/171,i-i L1,1-1 )
</equation>
<bodyText confidence="0.938169333333333">
2Rather than estimate 01 directly, we instead query
each potential onset to see how likely it is to be the actual
onset of the reparandum.
</bodyText>
<figure confidence="0.9500254375">
MUMBLE
UH_D
UH-FP
FRAGMENT
CCD DOD
DOP
DOZ
Sc
EX
WP
RB_ WRB
DO
HAVE
BE
VB HAVED
HAVEZ
BED
VBZ
BEZ
VBD
VBP BEG
HAVEP HAVEG
BEN
PPREP
PD RBR
RB
VBG
RP VBN
MD
TO
DP
PRP
CC
PREP
JJ
JJS
JJR
CD
DT
PAPS
WDT
NN
NNS
NNP
257
Pr(Ci I P1,i-1C1,i-1L1,iO1,R1, ELi )
Pr(Pi Wi,i-i L 01,i111,i )
Pr( W1 I
</figure>
<bodyText confidence="0.99937275">
The context for each of the probability distribu-
tions includes all of the previous context. In princi-
pal, we could give all of this context to the decision
tree algorithm and let it decide what information
is relevant in constructing equivalence classes of the
contexts. However, the amount of training data is
limited (as are the learning techniques) and so we
need to encode the context in order to simplify the
task of constructing meaningful equivalence classes.
We start with the words and their POS tags that
are in the context and for each non-null tone, editing
term (we also skip over E=ET), and repair tag, we
insert it into the appropriate place, just as Kompe et
al. (1994) do for boundary tones in their language
model. Below we give the encoded context for the
word &amp;quot;know&amp;quot; from Example 7
</bodyText>
<equation confidence="0.977653">
Example 8 (d93-18.1 utt47)
it/PRP takes/VBP one/CD Push you/PRP
</equation>
<bodyText confidence="0.958962461538462">
The result of this is that the non-null tag values are
treated just as if they were lexical items.&apos; Further-
more, if an editing term is completed, or the extent
of a repair is known, we can also clean up the edit-
ing term or reparandum, respectively, in the same
way that Stolcke and Shriberg (1996b) clean up filled
pauses, and simple repair patterns. This means that
we can then generalize between fluent speech and
instances that have a repair. For instance, in the
two examples below, the context for the word &amp;quot;get&amp;quot;
and its POS tag will be the same for both, namely
&amp;quot;so/CC_D we/PRP need/VBP to/TO&amp;quot;.
Example 9 (d93-11.1 utt46)
so we need to get the three tankers
Example 10 (d92a-2.2 utt6)
so we need to Push urn Pop A get a tanker of OJ
We also include other features of the context. For
instance, we include a variable to indicate if we are
currently processing an editing term, and whether
a non-filled pause editing term was seen. For es-
timating R, we include the editing terms as well.
For estimating 0i, we include whether the proposed
reparandum includes discourse markers, filled pauses
that are not part of an editing term, boundary terms,
and whether the proposed reparandum overlaps with
any previous repair.
</bodyText>
<sectionHeader confidence="0.997953" genericHeader="method">
5 Silences
</sectionHeader>
<bodyText confidence="0.999919666666667">
Silence, as well as other acoustic information, can
also give evidence as to whether an intonational
phrase, speech repair, or editing term occurred. We
</bodyText>
<footnote confidence="0.991654">
3Since we treat the non-null tags as lexical items, we
associate a unique POS tag with each value.
</footnote>
<figureCaption confidence="0.9126425">
Figure 3: Preference for tone, editing term, and re-
pair tags given the length of silence
</figureCaption>
<bodyText confidence="0.9990305">
include Si, the silence duration between word wi_1
and wi, as part of the context for conditioning the
probability distributions for the tone T, editing
term Ei, and repair Ri tags. Due to sparseness of
data, we make several the independence assumptions
so that we can separate the silence information from
the rest of the context. For example, for the tone
tag, let Res ii represent the rest of the context that
is used to condition T. By assuming that Res ti and
Si are independent, and are independent given
</bodyText>
<equation confidence="0.906409333333333">
we can rewrite Pr(T,ISiResii) as follows.
Pr(T, ISiRest,) = Pr(T, IResii)Pr(Ti ISi-i)
r(T,IS;)
</equation>
<bodyText confidence="0.987316176470588">
We can now use PPq as a factor to modify the
T,)
tone probability in order to take into account the
silence duration. In Figure 3, we give the factors
by which we adjust the tag probabilities given the
amount of silence. Again, due to sparse of data,
we collapse the values of the tone, editing term and
repair tag into six classes: boundary tones, editing
term pushes, editing term pops, modification repairs
and fresh starts (without an editing term). From
the figure, we see that if there is no silence between
wi_1 and wi, the null interpretation for the tone,
repair and editing term tags is preferred. Since the
independence assumptions that we have to make are
too strong, we normalize the adjusted tone, editing
term and repair tag probabilities to ensure that they
sum to one over all of the values of the tags.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="method">
6 Example
</sectionHeader>
<bodyText confidence="0.9999455">
To demonstrate how the model works, consider the
following example.
</bodyText>
<equation confidence="0.860308">
Example 11 (d92a-2.1 utt9)
</equation>
<bodyText confidence="0.937200714285714">
will take a total of um let&apos;s see total of s- of 7 hours
reparandurn I et reparandum I
zp ip
The language model considers all possible interpre-
tations (at least those that do not get pruned) and
assigns a probability to each. Below, we give the
probabilities for the correct interpretation of the
</bodyText>
<equation confidence="0.830265">
Pr(Ti)
</equation>
<page confidence="0.989978">
258
</page>
<bodyText confidence="0.965988448275862">
word &amp;quot;urn&amp;quot;, given â€¢the correct interpretation of the
words &amp;quot;will take a total of&amp;quot;. For reference, we give
a simplified view of the context that is used for each
probability.
Pr(T6=nullla total of)=0.98
Pr(E6=Pushia total 00=0.28
Pr(R6=nullia total of Push)=1.00
Pr(P6=UH_FPla total of Push)=0.75
Pr(Ws=umla total of Push UH_FP)=0.33
Given the correct interpretation of the previous
words, the probability of the filled pause &amp;quot;urn&amp;quot; along
with the correct POS tag, boundary tone tag, and
repair tags is 0.0665.
Now lets consider predicting the second instance
of &amp;quot;total&amp;quot;, which is the first word of the alteration of
the first repair, whose editing term &amp;quot;urn let&apos;s see&amp;quot;,
which ends with a boundary tone, has just finished.
Pr(Tio=TIPush let&apos;s see)=0.93
Pr(Ei0=PoplPush let&apos;s see Tone)=0.79
Pr(Rio=Mla total of Push let&apos;s see Pop) = 0.26
Pr(010=totallwill take a total of R10=Mod)=0.07
Pr(Lio=totalltotal of Ri0=Mod)=0.94
Pr(Cio=mjwill take a Lio=total/NN) = 0.87 4
Pr(Pio=NNIwill take a Lio=total/NN C1o=m)=1
Pr(Wio=totallwill take a NN L10 =total Cio=m)=1
Given the correct interpretation of the previous
words, the probability of the word &amp;quot;total&amp;quot; along with
the correct POS tag, boundary tone tag, and repair
tags is 0.011.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
7 Results
</sectionHeader>
<bodyText confidence="0.99893036">
To demonstrate our model, we use a 6-fold cross
validation procedure, in which we use each sixth of
the corpus for testing data, and the rest for train-
ing data. We start with the word transcriptions of
the Trains corpus, thus allowing us to get a clearer
indication of the performance of our model without
having to take into account the poor performance
of speech recognizers on spontaneous speech. All si-
lence durations are automatically obtained from a
word aligner (Ent, 1994).
Table 2 shows how POS tagging, discourse marker
identification and perplexity benefit by modeling the
speaker&apos;s utterance. The POS tagging results are re-
ported as the percentage of words that were assigned
the wrong tag. The detection of discourse markers is
reported using recall and precision. The recall rate
of X is the number of X events that were correctly
determined by the algorithm over the number of oc-
currences of X. The precision rate is the number
of X events that were correctly determined over the
number of times that the algorithm guessed X. The
error rate is the number of X events that the algo-
rithm missed plus the number of X events that it
incorrectly guessed as occurring over the number of
X events. The last measure is perplexity, which is
</bodyText>
<table confidence="0.999413181818182">
Base Tones Tones
Model Repairs Repairs
Corrections Corrections
Silences
POS Tagging
Error Rate 2.95 2.86 2.69
Discourse Markers
Recall 96.60 96.60 97.14
Precision 95.76 95.86 96.31
Error Rate 7.67 7.56 6.57
Perplexity 24.35 23.05 22.45
</table>
<tableCaption confidence="0.90714">
Table 2: POS Tagging and Perplexity Results
</tableCaption>
<table confidence="0.999808">
Tones Tones Tones
Silences Repairs
Corrections
Silences
Within Turn
Recall 64.9 70.2 70.5
Precision 67.4 68.7 69.4
Error Rate 66.5 61.9 60.5
All Tones
Recall 80.9 83.5 83.9
Precision 81.0 81.3 81.8
Error Rate 38.0 35.7 34.8
Perplexity 24.12 23.78 22.45
</table>
<tableCaption confidence="0.998677">
Table 3: Detecting Intonational Phrases
</tableCaption>
<bodyText confidence="0.917374333333333">
a way of measuring how well the language model is
able to predict the next word. The perplexity of a
test set of N words wi,N is calculated as follows.
</bodyText>
<equation confidence="0.524728">
2-47 log2 Pr(tv,Itv,,,_,)
</equation>
<bodyText confidence="0.999953541666667">
The second column of Table 2 gives the results
of the POS-based model, the third column gives
the results of incorporating the detection and cor-
rection of speech repairs and detection of intona-
tional phrase boundary tones, and the fourth col-
umn gives the results of adding in silence informa-
tion. As can be seen, modeling the user&apos;s utterances
improves POS tagging, identification of discourse
markers, and word perplexity; with the POS er-
ror rate decreasing by 3.1% and perplexity by 5.3%.
Furthermore, adding in silence information to help
detect the boundary tones and speech repairs results
in a further improvement, with the overall POS tag-
ging error rate decreasing by 8.6% and reducing per-
plexity by 7.8%. In contrast, a word-based trigram
backoff model (Katz, 1987) built with the CMU sta-
tistical language modeling toolkit (Rosenfeld, 1995)
achieved a perplexity of 26.13. Thus our full lan-
guage model results in 14.1% reduction in perplex-
ity.
Table 3 gives the results of detecting intonational
boundaries. The second column gives the results
of adding the boundary tone detection to the POS
model, the third column adds silence information,
</bodyText>
<page confidence="0.995062">
259
</page>
<table confidence="0.999169846153846">
Tones
Repairs Repairs
Repairs Corrections Corrections
Repairs Silences Silences Silences
Detection
Recall 67.9 72.7 75.7 77.0
Precision 80.6 77.9 80.8 84.8
Error Rate 48.5 47.9 42.4 36.8
Correction
Recall 62.4 65.0
Precision 66.6 71.5
Error Rate 68.9 60.9
Perplexity 24.11 23.72 23.04 22.45
</table>
<tableCaption confidence="0.99959">
Table 4: Detecting and Correcting Speech Repairs
</tableCaption>
<bodyText confidence="0.999953434782609">
and the fourth column adds speech repair detection
and correction. We see that adding in silence infor-
mation gives a noticeable improvement in detecting
boundary tones. Furthermore, adding in the speech
repair detection and correction further improves the
results of identifying boundary tones. Hence to de-
tect intonational phrase boundaries in spontaneous
speech, one should also model speech repairs.
Table 4 gives the results of detecting and correct-
ing speech repairs. The detection results report the
number of repairs that were detected, regardless of
whether the type of repair (e.g. modification repair
versus abridged repair) was properly determined.
The second column gives the results of adding speech
repair detection to the POS model. The third col-
umn adds in silence information. Unlike the case for
boundary tones, adding silence does not have much
of an effect.4 The fourth column adds in speech re-
pair correction, and shows that taking into account
the correction, gives better detection rates (Heeman,
Loken-Kim, and Allen, 1996). The fifth column adds
in boundary tone detection, which improves both the
detection and correction of speech repairs.
</bodyText>
<sectionHeader confidence="0.880544" genericHeader="method">
8 Comparison to Other Work
</sectionHeader>
<bodyText confidence="0.9999358">
Comparing the performance of this model to oth-
ers that have been proposed in the literature is very
difficult, due to differences in corpora, and different
input assumptions. However, it is useful to compare
the different techniques that are used.
Bear et at. (1992) used a simple pattern matching
approach on ATIS word transcriptions. They ex-
clude all turns that have a repair that just consists
of a filled pause or word fragment On this subset
they obtained a correction recall rate of 43% and a
precision of 50%.
Nakatani and Hirschberg (1994) examined how
speech repairs can be detected using a variety of
information, including acoustic, presence of word
&apos;Silence has a bigger effect on detection and correc-
tion if boundary tones are modeled.
matchings, and POS tags. Using these clues they
were able to train a decision tree which achieved a
recall rate of 86.1% and a precision of 92.1% on a set
of turns in which each turn contained at least one
speech repair.
Stolcke and Shriberg (1996b) examined whether
perplexity can be improved by modeling simple
types of speech repairs in a language model. They
find that doing so actually makes perplexity worse,
and they attribute this to not having a linguistic seg-
mentation available, which would help in modeling
filled pauses. We feel that speech repair modeling
must be combined with detecting utterance bound-
aries and discourse markers, and should take advan-
tage of acoustic information.
For detecting boundary tones, the model of
Wightman and Ostendorf (1994) achieves a recall
rate of 78.1% and a precision of 76.8%. Their better
performance is partly attributed to richer (speaker
dependent) acoustic modeling, including phoneme
duration, energy, and pitch. However, their model
was trained and tested on professionally read speech,
rather than spontaneous speech.
Wang and Hirschberg (1992) did employ sponta-
neous speech, namely, the ATIS corpus. For turn-
internal boundary tones, they achieved a recall rate
of 38.5% and a precision of 72.9% using a decision
tree approach that combined both textual features,
such as POS tags, and syntactic constituents with
intonational features. One explanation for the differ-
ence in performance was that our model was trained
on approximately ten times as much data. Secondly,
their decision trees are used to classify each data
point independently of the next, whereas we find
the best interpretation over the entire turn, and in-
corporate speech repairs.
The models of Kompe et at. (1994) and Mast et
at. (1996) are the most similar to our model in
terms of incorporating a language model. Mast et
at. achieve a recall rate of 85.0% and a precision of
53.1% on identifying dialog acts in a German cor-
pus. Their model employs richer acoustic modeling,
however, it does not account for other aspects of ut-
terance modeling, such as speech repairs.
</bodyText>
<sectionHeader confidence="0.993792" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999961083333333">
In this paper, we have shown that the problems
of identifying intonational boundaries and discourse
markers, and resolving speech repairs can be tack-
led by a statistical language model, which uses lo-
cal context. We have also shown that these tasks,
along with POS tagging, should be resolved to-
gether. Since our model can give a probability esti-
mate for the next word, it can be used as the lan-
guage model for a speech recognizer. In terms of
perplexity, our model gives a 14% improvement over
word-based language models. Part of this improve-
ment is due to being able to exploit silence durations,
</bodyText>
<page confidence="0.977118">
260
</page>
<bodyText confidence="0.9996948">
which traditional word-based language models tend
to ignore. Our next step is to incorporate this model
into a speech recognizer in order to validate that the
improved perplexity does in fact lead to a better
word recognition rate.
</bodyText>
<sectionHeader confidence="0.987322" genericHeader="acknowledgments">
10 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9903154">
This material is based upon work supported by the
NSF under grant IRI-9623665 and by ONR under
grant N00014-95-1-1088. Final preparation of this
paper was done while the first author was visiting
CNET, France Telecom.
</bodyText>
<sectionHeader confidence="0.997529" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996404341880342">
Allen, J. F., L. Schubert, G. Ferguson, P. Heeman,
C. Hwang, T. Kato, M. Light, N. Martin, B. Miller,
M. Poesio, and D. Traum. 1995. The Trains project:
A case study in building a conversational planning
agent. Journal of Experimental and Theoretical Al,
7:7-48.
Bahl, L. R., P. F. Brown, P. V. deSouza, and R. L. Mer-
cer. 1989. A tree-based statistical language model
for natural lnaguage speech recognition. IEEE Trans-
actions on Acoustics, Speech, and Signal Processing,
36(7):1001-1008.
Bear, J., J. Dowding, and E. Shriberg. 1992. Integrating
multiple knowledge sources for detection and correc-
tion of repairs in human-computer dialog. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 56-63.
Black, E., F. Jelinek, J. Lafferty, R. Mercer, and
S. Roukos. 1992. Decision tree models applied to the
labeling of text with parts-of-speech. In Proceedings of
the DARPA Speech and Natural Language Workshop,
pages 117-121. Morgan Kaufmann.
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J.
Stone. 1984. Classification and Regression Trees.
Monterrey, CA: Wadsworth Sz Brooks.
Brown, P. F., V. J. Della Pietra, P. V. deSouza, J. C.
Lai, and R. L. Mercer. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4):467-479.
Byron, D. K. and P. A. Heeman. 1997. Discourse marker
use in task-oriented spoken dialog. In Proceedings of
the 5&apos; European Conference on Speech Communica-
tion and Technology (Eurospeech ), Rhodes, Greece.
Entropic Research Laboratory, Inc., 1994. Aligner Ref-
erence Manual. Version 1.3.
Heeman, P. and J. Allen. 1994. Detecting and correct-
ing speech repairs. In Proceedings of the 32th Annual
Meeting of the Association for Computational Linguis-
tics, pages 295-302, Las Cruces, New Mexico, June.
Heeman, P. A. 1997. Speech repairs, intonational
boundaries and discourse markers: Modeling speakers&apos;
utterances in spoken dialog. Doctoral dissertation.
Heeman, P. A. and J. F. Allen. 1995. The Trains spo-
ken dialog corpus. CD-ROM, Linguistics Data Con-
sortium.
Heeman, P. A. and J. F. Allen. 1997. Incorporating POS
tagging into language modeling. In Proceedings of the
5th European Conference on Speech Communication
and Technology (Eurospeech ), Rhodes, Greece.
Heeman, P. A., K. Loken-Kim, and J. F. Allen. 1996.
Combining the detection and correction of speech re-
pairs. In Proceedings of the 4rd International Con-
ference on Spoken Language Processing (ICSLP-96),
pages 358-361, Philadephia, October.
Hindle, D. 1983. Deterministic parsing of syntactic non-
fluencies. In Proceedings of the 21st Annual Meeting of
the Association for Computational Linguistics, pages
123-128.
Hirschberg, J. and D. Litman. 1993. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3):501-530.
Katz, S. M. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, pages 400-401, March.
Kompe, R., A. Batliner, A. Kiefling, U. Kilian, H. Nie-
mann, E. Noth, and P. Regel-Brietzmann. 1994. Au-
tomatic classification of prosodically marked phrase
boundaries in german. In Proceedings of the Interna-
tional Conference on Audio, Speech and Signal Pro-
cessing (ICASSP), pages 173-176, Adelaide.
Levelt, W. J. M. 1983. Monitoring and self-repair in
speech. Cognition, 14:41-104.
Magerman, D. M. 1995. Statistical decision trees foi
parsing. In Proceedings of the 33th Annual Meeting of
the Association for Computational Linguistics, pages
7-14, Cambridge, MA, June.
Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
Mast, M., R. Kompe, S. Harbeck, A. Kiellling, H. Nie-
mann, E. Noth, E. G. Schukat-Talamazzini, and
V. Warnke. 1996. Dialog act classification with the
help of prosody. In Proceedings of the 4rd Inter-
national Conference on Spoken Language Processing
(ICSLP-96), Philadephia, October.
Nakatani, C. H. and J. Hirschberg. 1994. A
corpus-based study of repair cues in spontaneous
speech. Journal of the Acoustical Society of America,
95(3):1603-1616.
Rosenfeld, R. 1995. The CMU statistical language mod-
eling toolkit and its use in the 1994 ARPA CSR eval-
uation. In Proceedings of the ARPA Spoken Language
Systems Technology Workshop, San Mateo, California,
1995. Morgan Kaufmann.
Schiffrin, D. 1987. Discourse Markers. New York: Cam-
bridge University Press.
Silverman, K., M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert, and
J. Hirschberg. 1992. ToBI: A standard for labelling
English prosody. In Proceedings of the 2nd Inter-
national Conference on Spoken Language Processing
(ICSLP-92), pages 867-870.
Stolcke, A. and E. Shriberg. 1996a. Automatic linguistic
segmentation of conversational speech. In Proceedings
of the 4rd International Conference on Spoken Lan-
guage Processing (ICSLP-96), October.
Stolcke, A. and E. Shriberg. 1996b. Statistical language
modeling for speech disfluencies. In Proceedings of the
International Conference on Audio, Speech and Signal
Processing (ICASSP), May.
Wang, M. Q. and J. Hirschberg. 1992. Automatic classi-
fication of intonational phrase boundaries. Computer
Speech and Language, 6:175-196.
Wightman, C. W. and M. Ostendorf. 1994. Automatic
labeling of prosodic patterns. IEEE Transactions on
speech and audio processing, October.
</reference>
<page confidence="0.997606">
261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848825">
<title confidence="0.993062">Intonational Boundaries, Speech Repairs and Discourse Markers: Modeling Spoken Dialog</title>
<author confidence="0.999697">Peter A Heeman</author>
<author confidence="0.999697">James F Allen</author>
<affiliation confidence="0.9998915">Department of Computer Science University of Rochester</affiliation>
<address confidence="0.99953">Rochester NY 14627, USA</address>
<email confidence="0.99979">heemanOcs.rochester.edu</email>
<email confidence="0.99979">jamesOcs.rochester.edu</email>
<abstract confidence="0.991454470588235">To understand a speaker&apos;s turn of a conversation, one needs to segment it into intonational phrases, clean up any speech repairs that might have occurred, and identify discourse markers. In this paper, we argue that these problems must be resolved together, and that they must be resolved early in the processing stream. We put forward a statistical language model that resolves these problems, does POS tagging, and can be used as the language model of a speech recognizer. We find that by accounting for the interactions between these tasks that the performance on each task improves, as does POS tagging and perplexity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>L Schubert</author>
<author>G Ferguson</author>
<author>P Heeman</author>
<author>C Hwang</author>
<author>T Kato</author>
<author>M Light</author>
<author>N Martin</author>
<author>B Miller</author>
<author>M Poesio</author>
<author>D Traum</author>
</authors>
<title>The Trains project: A case study in building a conversational planning agent.</title>
<date>1995</date>
<journal>Journal of Experimental and Theoretical Al,</journal>
<pages>7--7</pages>
<contexts>
<context position="7192" citStr="Allen et al., 1995" startWordPosition="1165" endWordPosition="1168"> show that it can better account for these discourse events than can be achieved by modeling them individually. We also show that by modeling these two phenomena that we can increase our POS tagging performance by 8.6%, and improve our ability to predict the next word. Dialogs 98 Speakers 34 Words 58298 Turns 6163 Discourse Markers 8278 Boundary Tones 10947 Turn-Internal Boundary Tones 5535 Abridged Repairs 423 Modification Repairs 1302 Fresh Starts 671 Editing Terms 1128 Table 1: Frequency of Tones, Repairs and Editing Terms in the Trains Corpus 2 Trains Corpus As part of the TRAINS project (Allen et al., 1995), which is a long term research project to build a conversationally proficient planning assistant, we have collected a corpus of problem solving dialogs (Heeman and Allen, 1995). The dialogs involve two human participants, one who is playing the role of a user and has a certain task to accomplish, and another who is playing the role of the system by acting as a planning assistant. The collection methodology was designed to make the setting as close to humancomputer interaction as possible, but was not a wizard scenario, where one person pretends to be a computer. Rather, the user knows that he</context>
</contexts>
<marker>Allen, Schubert, Ferguson, Heeman, Hwang, Kato, Light, Martin, Miller, Poesio, Traum, 1995</marker>
<rawString>Allen, J. F., L. Schubert, G. Ferguson, P. Heeman, C. Hwang, T. Kato, M. Light, N. Martin, B. Miller, M. Poesio, and D. Traum. 1995. The Trains project: A case study in building a conversational planning agent. Journal of Experimental and Theoretical Al, 7:7-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
</authors>
<title>A tree-based statistical language model for natural lnaguage speech recognition.</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<pages>36--7</pages>
<contexts>
<context position="11593" citStr="Bahl et al., 1989" startWordPosition="1918" endWordPosition="1921">. WP = arg max Pr( WP IA) W,P arg max Pr(AIWP)Pr(WP) WP Pr(A) arg max Pr(A I W P)Pr(W P) WP The first term Pr(AIWP) is the factor due to the acoustic model, which we can approximate by Pr(AIW). The second term Pr(WP) is the factor due to the language model. We rewrite Pr(WP) as Pr(Wi,NPLN), where N is the number of words in the sequence. We now rewrite the language model probability as follows. Pr(Wi.dvPi,N) HPr(wi Pl,i-1) i=1,N = JJ Pr(WilWi,i-1P1,i) W1,i-1P1,i-1) i=1,N We now have two probability distributions that we need to estimate, which we do using decision trees (Breiman et al., 1984; Bahl et al., 1989). The decision tree algorithm has the advantage that it uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node. For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where the impurity measures how well each leaf predicts the events in the node. After the tree is grown, a heldout dataset is used to smooth the probabilities of each n</context>
</contexts>
<marker>Bahl, Brown, deSouza, Mercer, 1989</marker>
<rawString>Bahl, L. R., P. F. Brown, P. V. deSouza, and R. L. Mercer. 1989. A tree-based statistical language model for natural lnaguage speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 36(7):1001-1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bear</author>
<author>J Dowding</author>
<author>E Shriberg</author>
</authors>
<title>Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>56--63</pages>
<contexts>
<context position="2484" citStr="Bear, Dowding, and Shriberg, 1992" startWordPosition="398" endWordPosition="402">ker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 </context>
<context position="4452" citStr="Bear, Dowding, and Shriberg, 1992" startWordPosition="716" endWordPosition="720">ence, the problem of identifying discourse markers also needs to be addressed with the segmentation and speech repair problems. These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often 254 signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparandum, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the syntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994). However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983). Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation. Of course when dealing with spoken dialogue, one cannot forget the initial problem of determining the actual words that the speaker is saying. Speech recognizers rely on being able to predict the probability of what word will be said next. Just as intonational phrases and speech repairs disrupt the local context that is needed for syntactic disambi</context>
<context position="9239" citStr="Bear et al. (1992)" startWordPosition="1504" endWordPosition="1507">ndum is not empty. Example 4 (d92a-1.3 utt65) so that will total will take seven hours to do that reparandural alteration interruption point 1This classification is similar to that of Hindle (1983) and Levelt (1983). 255 The third type of repairs are the abridged repairs, which consist solely of an editing term. Note that utterance initial filled pauses are not treated as abridged repairs. Example 5 (d93-14.3 utt42) we need to um manage to get the bananas to Dansville I editing term interruption point There is typically a correspondence between the reparandum and the alteration, and following Bear et al. (1992), we annotate this using the labels m for word matching and r for word replacements (words of the same syntactic category). Each pair is given a unique index. Other words in the reparandum and alteration are annotated with an x. Also, editing terms (filled pauses and clue words) are labeled with et, and the interruption point with ip, which will occur before any editing terms associated with the repair, and after a word fragment, if present. The interruption point is also marked as to whether the repair is a fresh start, modification repair, or abridged repair, in which cases, we use ip:can, i</context>
</contexts>
<marker>Bear, Dowding, Shriberg, 1992</marker>
<rawString>Bear, J., J. Dowding, and E. Shriberg. 1992. Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R Mercer</author>
<author>S Roukos</author>
</authors>
<title>Decision tree models applied to the labeling of text with parts-of-speech.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>117--121</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="12887" citStr="Black et al., 1992" startWordPosition="2157" endWordPosition="2160">bout the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as &apos;is the third bit of the POS tag encoding equal to one?&apos; Figure 1 shows a POS classification tree. The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag. Unlike other work (e.g. (Black et al., 1992; Magerman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag. This has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information. As well, it constrains the task of building the word classification trees since the major distinctions are captured by the PUS classification tree. 4 Augmenting the Model Just as we redefined the speech recognition problem so as to account for POS tagging and identifying discourse markers, we do t</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Mercer, Roukos, 1992</marker>
<rawString>Black, E., F. Jelinek, J. Lafferty, R. Mercer, and S. Roukos. 1992. Decision tree models applied to the labeling of text with parts-of-speech. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 117-121. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J H Friedman</author>
<author>R A Olshen</author>
<author>C J Stone</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth Sz Brooks.</publisher>
<location>Monterrey, CA:</location>
<contexts>
<context position="11573" citStr="Breiman et al., 1984" startWordPosition="1914" endWordPosition="1917">odel is now as follows. WP = arg max Pr( WP IA) W,P arg max Pr(AIWP)Pr(WP) WP Pr(A) arg max Pr(A I W P)Pr(W P) WP The first term Pr(AIWP) is the factor due to the acoustic model, which we can approximate by Pr(AIW). The second term Pr(WP) is the factor due to the language model. We rewrite Pr(WP) as Pr(Wi,NPLN), where N is the number of words in the sequence. We now rewrite the language model probability as follows. Pr(Wi.dvPi,N) HPr(wi Pl,i-1) i=1,N = JJ Pr(WilWi,i-1P1,i) W1,i-1P1,i-1) i=1,N We now have two probability distributions that we need to estimate, which we do using decision trees (Breiman et al., 1984; Bahl et al., 1989). The decision tree algorithm has the advantage that it uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node. For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where the impurity measures how well each leaf predicts the events in the node. After the tree is grown, a heldout dataset is used to smooth the pro</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. Monterrey, CA: Wadsworth Sz Brooks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="12389" citStr="Brown et al. (1992)" startWordPosition="2063" endWordPosition="2066">ata. The decision tree algorithm starts with all of the training data in a single leaf node. For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where the impurity measures how well each leaf predicts the events in the node. After the tree is grown, a heldout dataset is used to smooth the probabilities of each node with its parent (Bahl et al., 1989). To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as &apos;is the third bit of the POS tag encoding equal to one?&apos; Figure 1 shows a POS classification tree. The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag. Unlike other work (e.g. (Black et al., 1992; Magerman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we buil</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, P. F., V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Byron</author>
<author>P A Heeman</author>
</authors>
<title>Discourse marker use in task-oriented spoken dialog.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5&apos; European Conference on Speech Communication and Technology (Eurospeech ),</booktitle>
<location>Rhodes, Greece.</location>
<contexts>
<context position="3326" citStr="Byron and Heeman, 1997" startWordPosition="532" endWordPosition="535">resence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 utt73) that&apos;s all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers. Discourse markers (Schiffrin, 1987; Hirschberg and Litman, 1993; Byron and Heeman, 1997) are used to relate new speech to the current discourse state. Lexical items that can function as discourse markers, such as &amp;quot;well&amp;quot; and &amp;quot;okay,&amp;quot; are ambiguous as to whether they are being used as discourse markers or not. The complication is that discourse markers tend to be used to introduce a new utterance, or can be an utterance all to themselves (such as the acknowledgment &amp;quot;okay&amp;quot; or &amp;quot;alright&amp;quot;), or can be used as part of the editing term of a speech repair, or to begin the alteration. Hence, the problem of identifying discourse markers also needs to be addressed with the segmentation and spe</context>
</contexts>
<marker>Byron, Heeman, 1997</marker>
<rawString>Byron, D. K. and P. A. Heeman. 1997. Discourse marker use in task-oriented spoken dialog. In Proceedings of the 5&apos; European Conference on Speech Communication and Technology (Eurospeech ), Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<date>1994</date>
<journal>Aligner Reference Manual. Version</journal>
<volume>1</volume>
<institution>Entropic Research Laboratory, Inc.,</institution>
<contexts>
<context position="18384" citStr="(1994)" startWordPosition="3153" endWordPosition="3153">ous context. In principal, we could give all of this context to the decision tree algorithm and let it decide what information is relevant in constructing equivalence classes of the contexts. However, the amount of training data is limited (as are the learning techniques) and so we need to encode the context in order to simplify the task of constructing meaningful equivalence classes. We start with the words and their POS tags that are in the context and for each non-null tone, editing term (we also skip over E=ET), and repair tag, we insert it into the appropriate place, just as Kompe et al. (1994) do for boundary tones in their language model. Below we give the encoded context for the word &amp;quot;know&amp;quot; from Example 7 Example 8 (d93-18.1 utt47) it/PRP takes/VBP one/CD Push you/PRP The result of this is that the non-null tag values are treated just as if they were lexical items.&apos; Furthermore, if an editing term is completed, or the extent of a repair is known, we can also clean up the editing term or reparandum, respectively, in the same way that Stolcke and Shriberg (1996b) clean up filled pauses, and simple repair patterns. This means that we can then generalize between fluent speech and ins</context>
<context position="28446" citStr="(1994)" startWordPosition="4820" endWordPosition="4820">d correction of speech repairs. 8 Comparison to Other Work Comparing the performance of this model to others that have been proposed in the literature is very difficult, due to differences in corpora, and different input assumptions. However, it is useful to compare the different techniques that are used. Bear et at. (1992) used a simple pattern matching approach on ATIS word transcriptions. They exclude all turns that have a repair that just consists of a filled pause or word fragment On this subset they obtained a correction recall rate of 43% and a precision of 50%. Nakatani and Hirschberg (1994) examined how speech repairs can be detected using a variety of information, including acoustic, presence of word &apos;Silence has a bigger effect on detection and correction if boundary tones are modeled. matchings, and POS tags. Using these clues they were able to train a decision tree which achieved a recall rate of 86.1% and a precision of 92.1% on a set of turns in which each turn contained at least one speech repair. Stolcke and Shriberg (1996b) examined whether perplexity can be improved by modeling simple types of speech repairs in a language model. They find that doing so actually makes p</context>
<context position="30388" citStr="(1994)" startWordPosition="5131" endWordPosition="5131"> namely, the ATIS corpus. For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents with intonational features. One explanation for the difference in performance was that our model was trained on approximately ten times as much data. Secondly, their decision trees are used to classify each data point independently of the next, whereas we find the best interpretation over the entire turn, and incorporate speech repairs. The models of Kompe et at. (1994) and Mast et at. (1996) are the most similar to our model in terms of incorporating a language model. Mast et at. achieve a recall rate of 85.0% and a precision of 53.1% on identifying dialog acts in a German corpus. Their model employs richer acoustic modeling, however, it does not account for other aspects of utterance modeling, such as speech repairs. 9 Conclusion In this paper, we have shown that the problems of identifying intonational boundaries and discourse markers, and resolving speech repairs can be tackled by a statistical language model, which uses local context. We have also shown</context>
</contexts>
<marker>1994</marker>
<rawString>Entropic Research Laboratory, Inc., 1994. Aligner Reference Manual. Version 1.3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heeman</author>
<author>J Allen</author>
</authors>
<title>Detecting and correcting speech repairs.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Las Cruces, New Mexico,</location>
<contexts>
<context position="2539" citStr="Heeman and Allen, 1994" startWordPosition="407" endWordPosition="410">e the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 utt73) that&apos;s all you need % you only need one boxcar I</context>
<context position="4477" citStr="Heeman and Allen, 1994" startWordPosition="721" endWordPosition="724">scourse markers also needs to be addressed with the segmentation and speech repair problems. These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often 254 signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparandum, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the syntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994). However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983). Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation. Of course when dealing with spoken dialogue, one cannot forget the initial problem of determining the actual words that the speaker is saying. Speech recognizers rely on being able to predict the probability of what word will be said next. Just as intonational phrases and speech repairs disrupt the local context that is needed for syntactic disambiguation, the same holds f</context>
<context position="13682" citStr="Heeman and Allen, 1994" startWordPosition="2291" endWordPosition="2294">of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information. As well, it constrains the task of building the word classification trees since the major distinctions are captured by the PUS classification tree. 4 Augmenting the Model Just as we redefined the speech recognition problem so as to account for POS tagging and identifying discourse markers, we do the same for modeling 256 Figure 1: POS Classification Tree boundary tones and speech repairs. We introduce null tokens between each pair of consecutive words wi..1 and wi (Heeman and Allen, 1994), which will be tagged as to the occurrence of these events. The boundary tone tag Ti indicates if word wi_1 ends an intonational boundary (Ti=T), or not (Ti=null). For detecting speech repairs, we have the problem that repairs are often accompanied by an editing term, such as &amp;quot;urn&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;okay&amp;quot;, or &amp;quot;well&amp;quot;, and these must be identified as such. Furthermore, an editing term might be composed of a number of words, such as &amp;quot;let&apos;s see&amp;quot; or &amp;quot;uh well&amp;quot;. Hence we use two tags: an editing term tag Ei and a repair tag Ri. The editing term tag indicates if wi starts an editing term (Ei=Push), if wi conti</context>
</contexts>
<marker>Heeman, Allen, 1994</marker>
<rawString>Heeman, P. and J. Allen. 1994. Detecting and correcting speech repairs. In Proceedings of the 32th Annual Meeting of the Association for Computational Linguistics, pages 295-302, Las Cruces, New Mexico, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
</authors>
<title>Speech repairs, intonational boundaries and discourse markers: Modeling speakers&apos; utterances in spoken dialog. Doctoral dissertation.</title>
<date>1997</date>
<contexts>
<context position="3326" citStr="Heeman, 1997" startWordPosition="534" endWordPosition="535"> silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 utt73) that&apos;s all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers. Discourse markers (Schiffrin, 1987; Hirschberg and Litman, 1993; Byron and Heeman, 1997) are used to relate new speech to the current discourse state. Lexical items that can function as discourse markers, such as &amp;quot;well&amp;quot; and &amp;quot;okay,&amp;quot; are ambiguous as to whether they are being used as discourse markers or not. The complication is that discourse markers tend to be used to introduce a new utterance, or can be an utterance all to themselves (such as the acknowledgment &amp;quot;okay&amp;quot; or &amp;quot;alright&amp;quot;), or can be used as part of the editing term of a speech repair, or to begin the alteration. Hence, the problem of identifying discourse markers also needs to be addressed with the segmentation and spe</context>
<context position="10762" citStr="Heeman, 1997" startWordPosition="1772" endWordPosition="1773">rds W that is maximal given the acoustic signal A. However, for detecting and correcting speech repairs, and identifying boundary tones and discourse markers, we need to augment the model so that it incorporates shallow statistical analysis, in the form of POS tagging. The POS tagset, based on the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz, 1993), includes special tags for denoting when a word is being used as a discourse marker. In this section, we give an overview of our basic language model that incorporates POS tagging. Full details can be found in (Beeman and Allen, 1997; Heeman, 1997). To add in POS tagging, we change the goal of the speech recognition process to find the best word and POS tags given the acoustic signal. The derivation of the acoustic model and language model is now as follows. WP = arg max Pr( WP IA) W,P arg max Pr(AIWP)Pr(WP) WP Pr(A) arg max Pr(A I W P)Pr(W P) WP The first term Pr(AIWP) is the factor due to the acoustic model, which we can approximate by Pr(AIW). The second term Pr(WP) is the factor due to the language model. We rewrite Pr(WP) as Pr(Wi,NPLN), where N is the number of words in the sequence. We now rewrite the language model probability a</context>
</contexts>
<marker>Heeman, 1997</marker>
<rawString>Heeman, P. A. 1997. Speech repairs, intonational boundaries and discourse markers: Modeling speakers&apos; utterances in spoken dialog. Doctoral dissertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>J F Allen</author>
</authors>
<title>The Trains spoken dialog corpus.</title>
<date>1995</date>
<journal>CD-ROM, Linguistics Data Consortium.</journal>
<contexts>
<context position="1785" citStr="Heeman and Allen, 1995" startWordPosition="282" endWordPosition="285">ch utterance. Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases (Silverman et al., 1992), which end with an acoustically signaled boundary tone. Even assuming perfect word recognition, the problem of determining the intended words is complicated due to the occurrence of speech repairs, which occur where the speaker goes back and changes (or repeats) something she just said. The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. The following example, from the Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) a</context>
<context position="7369" citStr="Heeman and Allen, 1995" startWordPosition="1193" endWordPosition="1197"> increase our POS tagging performance by 8.6%, and improve our ability to predict the next word. Dialogs 98 Speakers 34 Words 58298 Turns 6163 Discourse Markers 8278 Boundary Tones 10947 Turn-Internal Boundary Tones 5535 Abridged Repairs 423 Modification Repairs 1302 Fresh Starts 671 Editing Terms 1128 Table 1: Frequency of Tones, Repairs and Editing Terms in the Trains Corpus 2 Trains Corpus As part of the TRAINS project (Allen et al., 1995), which is a long term research project to build a conversationally proficient planning assistant, we have collected a corpus of problem solving dialogs (Heeman and Allen, 1995). The dialogs involve two human participants, one who is playing the role of a user and has a certain task to accomplish, and another who is playing the role of the system by acting as a planning assistant. The collection methodology was designed to make the setting as close to humancomputer interaction as possible, but was not a wizard scenario, where one person pretends to be a computer. Rather, the user knows that he is talking to another person. The TRAINS corpus consists of about six and half hours of speech. Table 1 gives some general statistics about the corpus, including the number of </context>
</contexts>
<marker>Heeman, Allen, 1995</marker>
<rawString>Heeman, P. A. and J. F. Allen. 1995. The Trains spoken dialog corpus. CD-ROM, Linguistics Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>J F Allen</author>
</authors>
<title>Incorporating POS tagging into language modeling.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology (Eurospeech ),</booktitle>
<location>Rhodes, Greece.</location>
<marker>Heeman, Allen, 1997</marker>
<rawString>Heeman, P. A. and J. F. Allen. 1997. Incorporating POS tagging into language modeling. In Proceedings of the 5th European Conference on Speech Communication and Technology (Eurospeech ), Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>K Loken-Kim</author>
<author>J F Allen</author>
</authors>
<title>Combining the detection and correction of speech repairs.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4rd International Conference on Spoken Language Processing (ICSLP-96),</booktitle>
<pages>358--361</pages>
<location>Philadephia,</location>
<contexts>
<context position="27751" citStr="Heeman, Loken-Kim, and Allen, 1996" startWordPosition="4700" endWordPosition="4704">ives the results of detecting and correcting speech repairs. The detection results report the number of repairs that were detected, regardless of whether the type of repair (e.g. modification repair versus abridged repair) was properly determined. The second column gives the results of adding speech repair detection to the POS model. The third column adds in silence information. Unlike the case for boundary tones, adding silence does not have much of an effect.4 The fourth column adds in speech repair correction, and shows that taking into account the correction, gives better detection rates (Heeman, Loken-Kim, and Allen, 1996). The fifth column adds in boundary tone detection, which improves both the detection and correction of speech repairs. 8 Comparison to Other Work Comparing the performance of this model to others that have been proposed in the literature is very difficult, due to differences in corpora, and different input assumptions. However, it is useful to compare the different techniques that are used. Bear et at. (1992) used a simple pattern matching approach on ATIS word transcriptions. They exclude all turns that have a repair that just consists of a filled pause or word fragment On this subset they </context>
</contexts>
<marker>Heeman, Loken-Kim, Allen, 1996</marker>
<rawString>Heeman, P. A., K. Loken-Kim, and J. F. Allen. 1996. Combining the detection and correction of speech repairs. In Proceedings of the 4rd International Conference on Spoken Language Processing (ICSLP-96), pages 358-361, Philadephia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Deterministic parsing of syntactic nonfluencies.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>123--128</pages>
<contexts>
<context position="2449" citStr="Hindle, 1983" startWordPosition="396" endWordPosition="397"> that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the </context>
<context position="4578" citStr="Hindle, 1983" startWordPosition="739" endWordPosition="740">ena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often 254 signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparandum, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the syntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994). However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983). Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation. Of course when dealing with spoken dialogue, one cannot forget the initial problem of determining the actual words that the speaker is saying. Speech recognizers rely on being able to predict the probability of what word will be said next. Just as intonational phrases and speech repairs disrupt the local context that is needed for syntactic disambiguation, the same holds for predicting what word will come next. If a speech repair or intonational phrase occurs, this will a</context>
<context position="8818" citStr="Hindle (1983)" startWordPosition="1437" endWordPosition="1438">ts, modification repairs, and abridged repairs.&apos; A fresh start is where the speaker abandons the current utterance and starts again, where the abandonment seems acoustically signaled. Example 3 (d93-12.1 utt30) so it&apos;ll take urn so you want to do what reparandum I editing term alteration interruption point The second type of repairs are the modification repairs. These include all other repairs in which the reparandum is not empty. Example 4 (d92a-1.3 utt65) so that will total will take seven hours to do that reparandural alteration interruption point 1This classification is similar to that of Hindle (1983) and Levelt (1983). 255 The third type of repairs are the abridged repairs, which consist solely of an editing term. Note that utterance initial filled pauses are not treated as abridged repairs. Example 5 (d93-14.3 utt42) we need to um manage to get the bananas to Dansville I editing term interruption point There is typically a correspondence between the reparandum and the alteration, and following Bear et al. (1992), we annotate this using the labels m for word matching and r for word replacements (words of the same syntactic category). Each pair is given a unique index. Other words in the r</context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Hindle, D. 1983. Deterministic parsing of syntactic nonfluencies. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, pages 123-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="3301" citStr="Hirschberg and Litman, 1993" startWordPosition="528" endWordPosition="531">tertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 utt73) that&apos;s all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers. Discourse markers (Schiffrin, 1987; Hirschberg and Litman, 1993; Byron and Heeman, 1997) are used to relate new speech to the current discourse state. Lexical items that can function as discourse markers, such as &amp;quot;well&amp;quot; and &amp;quot;okay,&amp;quot; are ambiguous as to whether they are being used as discourse markers or not. The complication is that discourse markers tend to be used to introduce a new utterance, or can be an utterance all to themselves (such as the acknowledgment &amp;quot;okay&amp;quot; or &amp;quot;alright&amp;quot;), or can be used as part of the editing term of a speech repair, or to begin the alteration. Hence, the problem of identifying discourse markers also needs to be addressed with</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Hirschberg, J. and D. Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<pages>400--401</pages>
<contexts>
<context position="25983" citStr="Katz, 1987" startWordPosition="4430" endWordPosition="4431">repairs and detection of intonational phrase boundary tones, and the fourth column gives the results of adding in silence information. As can be seen, modeling the user&apos;s utterances improves POS tagging, identification of discourse markers, and word perplexity; with the POS error rate decreasing by 3.1% and perplexity by 5.3%. Furthermore, adding in silence information to help detect the boundary tones and speech repairs results in a further improvement, with the overall POS tagging error rate decreasing by 8.6% and reducing perplexity by 7.8%. In contrast, a word-based trigram backoff model (Katz, 1987) built with the CMU statistical language modeling toolkit (Rosenfeld, 1995) achieved a perplexity of 26.13. Thus our full language model results in 14.1% reduction in perplexity. Table 3 gives the results of detecting intonational boundaries. The second column gives the results of adding the boundary tone detection to the POS model, the third column adds silence information, 259 Tones Repairs Repairs Repairs Corrections Corrections Repairs Silences Silences Silences Detection Recall 67.9 72.7 75.7 77.0 Precision 80.6 77.9 80.8 84.8 Error Rate 48.5 47.9 42.4 36.8 Correction Recall 62.4 65.0 Pre</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, pages 400-401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kompe</author>
<author>A Batliner</author>
<author>A Kiefling</author>
<author>U Kilian</author>
<author>H Niemann</author>
<author>E Noth</author>
<author>P Regel-Brietzmann</author>
</authors>
<title>Automatic classification of prosodically marked phrase boundaries in german.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP),</booktitle>
<pages>173--176</pages>
<location>Adelaide.</location>
<contexts>
<context position="2362" citStr="Kompe et al., 1994" startWordPosition="380" endWordPosition="383">he Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustra</context>
<context position="18384" citStr="Kompe et al. (1994)" startWordPosition="3150" endWordPosition="3153"> of the previous context. In principal, we could give all of this context to the decision tree algorithm and let it decide what information is relevant in constructing equivalence classes of the contexts. However, the amount of training data is limited (as are the learning techniques) and so we need to encode the context in order to simplify the task of constructing meaningful equivalence classes. We start with the words and their POS tags that are in the context and for each non-null tone, editing term (we also skip over E=ET), and repair tag, we insert it into the appropriate place, just as Kompe et al. (1994) do for boundary tones in their language model. Below we give the encoded context for the word &amp;quot;know&amp;quot; from Example 7 Example 8 (d93-18.1 utt47) it/PRP takes/VBP one/CD Push you/PRP The result of this is that the non-null tag values are treated just as if they were lexical items.&apos; Furthermore, if an editing term is completed, or the extent of a repair is known, we can also clean up the editing term or reparandum, respectively, in the same way that Stolcke and Shriberg (1996b) clean up filled pauses, and simple repair patterns. This means that we can then generalize between fluent speech and ins</context>
</contexts>
<marker>Kompe, Batliner, Kiefling, Kilian, Niemann, Noth, Regel-Brietzmann, 1994</marker>
<rawString>Kompe, R., A. Batliner, A. Kiefling, U. Kilian, H. Niemann, E. Noth, and P. Regel-Brietzmann. 1994. Automatic classification of prosodically marked phrase boundaries in german. In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP), pages 173-176, Adelaide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Monitoring and self-repair in speech.</title>
<date>1983</date>
<journal>Cognition,</journal>
<pages>14--41</pages>
<contexts>
<context position="8836" citStr="Levelt (1983)" startWordPosition="1440" endWordPosition="1441">epairs, and abridged repairs.&apos; A fresh start is where the speaker abandons the current utterance and starts again, where the abandonment seems acoustically signaled. Example 3 (d93-12.1 utt30) so it&apos;ll take urn so you want to do what reparandum I editing term alteration interruption point The second type of repairs are the modification repairs. These include all other repairs in which the reparandum is not empty. Example 4 (d92a-1.3 utt65) so that will total will take seven hours to do that reparandural alteration interruption point 1This classification is similar to that of Hindle (1983) and Levelt (1983). 255 The third type of repairs are the abridged repairs, which consist solely of an editing term. Note that utterance initial filled pauses are not treated as abridged repairs. Example 5 (d93-14.3 utt42) we need to um manage to get the bananas to Dansville I editing term interruption point There is typically a correspondence between the reparandum and the alteration, and following Bear et al. (1992), we annotate this using the labels m for word matching and r for word replacements (words of the same syntactic category). Each pair is given a unique index. Other words in the reparandum and alte</context>
</contexts>
<marker>Levelt, 1983</marker>
<rawString>Levelt, W. J. M. 1983. Monitoring and self-repair in speech. Cognition, 14:41-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical decision trees foi parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>7--14</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="12904" citStr="Magerman, 1995" startWordPosition="2161" endWordPosition="2163">OS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as &apos;is the third bit of the POS tag encoding equal to one?&apos; Figure 1 shows a POS classification tree. The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag. Unlike other work (e.g. (Black et al., 1992; Magerman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag. This has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information. As well, it constrains the task of building the word classification trees since the major distinctions are captured by the PUS classification tree. 4 Augmenting the Model Just as we redefined the speech recognition problem so as to account for POS tagging and identifying discourse markers, we do the same for model</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, D. M. 1995. Statistical decision trees foi parsing. In Proceedings of the 33th Annual Meeting of the Association for Computational Linguistics, pages 7-14, Cambridge, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="10512" citStr="Marcus, Santorini, and Marcinkiewicz, 1993" startWordPosition="1724" endWordPosition="1728"> The example below illustrates how a repair is annotated in this scheme. Example 6 (d93-15.2 utt42) engine two from Elmi(ra)- or engine three from Elmira ml r2 m3 m4 Tet ml r2 m3 m4 ip:mod 3 A POS-Based Language Model The goal of a speech recognizer is to find the sequence of words W that is maximal given the acoustic signal A. However, for detecting and correcting speech repairs, and identifying boundary tones and discourse markers, we need to augment the model so that it incorporates shallow statistical analysis, in the form of POS tagging. The POS tagset, based on the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz, 1993), includes special tags for denoting when a word is being used as a discourse marker. In this section, we give an overview of our basic language model that incorporates POS tagging. Full details can be found in (Beeman and Allen, 1997; Heeman, 1997). To add in POS tagging, we change the goal of the speech recognition process to find the best word and POS tags given the acoustic signal. The derivation of the acoustic model and language model is now as follows. WP = arg max Pr( WP IA) W,P arg max Pr(AIWP)Pr(WP) WP Pr(A) arg max Pr(A I W P)Pr(W P) WP The first term Pr(AIWP) is the factor due to </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mast</author>
<author>R Kompe</author>
<author>S Harbeck</author>
<author>A Kiellling</author>
<author>H Niemann</author>
<author>E Noth</author>
<author>E G Schukat-Talamazzini</author>
<author>V Warnke</author>
</authors>
<title>Dialog act classification with the help of prosody.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4rd International Conference on Spoken Language Processing (ICSLP-96),</booktitle>
<location>Philadephia,</location>
<contexts>
<context position="2382" citStr="Mast et al., 1996" startWordPosition="384" endWordPosition="387">eman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following</context>
</contexts>
<marker>Mast, Kompe, Harbeck, Kiellling, Niemann, Noth, Schukat-Talamazzini, Warnke, 1996</marker>
<rawString>Mast, M., R. Kompe, S. Harbeck, A. Kiellling, H. Niemann, E. Noth, E. G. Schukat-Talamazzini, and V. Warnke. 1996. Dialog act classification with the help of prosody. In Proceedings of the 4rd International Conference on Spoken Language Processing (ICSLP-96), Philadephia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Nakatani</author>
<author>J Hirschberg</author>
</authors>
<title>A corpus-based study of repair cues in spontaneous speech.</title>
<date>1994</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>95--3</pages>
<contexts>
<context position="2515" citStr="Nakatani and Hirschberg, 1994" startWordPosition="403" endWordPosition="406">y reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 utt73) that&apos;s all you need % yo</context>
<context position="28446" citStr="Nakatani and Hirschberg (1994)" startWordPosition="4817" endWordPosition="4820">es both the detection and correction of speech repairs. 8 Comparison to Other Work Comparing the performance of this model to others that have been proposed in the literature is very difficult, due to differences in corpora, and different input assumptions. However, it is useful to compare the different techniques that are used. Bear et at. (1992) used a simple pattern matching approach on ATIS word transcriptions. They exclude all turns that have a repair that just consists of a filled pause or word fragment On this subset they obtained a correction recall rate of 43% and a precision of 50%. Nakatani and Hirschberg (1994) examined how speech repairs can be detected using a variety of information, including acoustic, presence of word &apos;Silence has a bigger effect on detection and correction if boundary tones are modeled. matchings, and POS tags. Using these clues they were able to train a decision tree which achieved a recall rate of 86.1% and a precision of 92.1% on a set of turns in which each turn contained at least one speech repair. Stolcke and Shriberg (1996b) examined whether perplexity can be improved by modeling simple types of speech repairs in a language model. They find that doing so actually makes p</context>
</contexts>
<marker>Nakatani, Hirschberg, 1994</marker>
<rawString>Nakatani, C. H. and J. Hirschberg. 1994. A corpus-based study of repair cues in spontaneous speech. Journal of the Acoustical Society of America, 95(3):1603-1616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ARPA Spoken Language Systems Technology Workshop,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="26058" citStr="Rosenfeld, 1995" startWordPosition="4441" endWordPosition="4442">ourth column gives the results of adding in silence information. As can be seen, modeling the user&apos;s utterances improves POS tagging, identification of discourse markers, and word perplexity; with the POS error rate decreasing by 3.1% and perplexity by 5.3%. Furthermore, adding in silence information to help detect the boundary tones and speech repairs results in a further improvement, with the overall POS tagging error rate decreasing by 8.6% and reducing perplexity by 7.8%. In contrast, a word-based trigram backoff model (Katz, 1987) built with the CMU statistical language modeling toolkit (Rosenfeld, 1995) achieved a perplexity of 26.13. Thus our full language model results in 14.1% reduction in perplexity. Table 3 gives the results of detecting intonational boundaries. The second column gives the results of adding the boundary tone detection to the POS model, the third column adds silence information, 259 Tones Repairs Repairs Repairs Corrections Corrections Repairs Silences Silences Silences Detection Recall 67.9 72.7 75.7 77.0 Precision 80.6 77.9 80.8 84.8 Error Rate 48.5 47.9 42.4 36.8 Correction Recall 62.4 65.0 Precision 66.6 71.5 Error Rate 68.9 60.9 Perplexity 24.11 23.72 23.04 22.45 Ta</context>
</contexts>
<marker>Rosenfeld, 1995</marker>
<rawString>Rosenfeld, R. 1995. The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation. In Proceedings of the ARPA Spoken Language Systems Technology Workshop, San Mateo, California, 1995. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schiffrin</author>
</authors>
<title>Discourse Markers. New York:</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3272" citStr="Schiffrin, 1987" startWordPosition="526" endWordPosition="527">two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %. Example 2 (d93-83.3 utt73) that&apos;s all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers. Discourse markers (Schiffrin, 1987; Hirschberg and Litman, 1993; Byron and Heeman, 1997) are used to relate new speech to the current discourse state. Lexical items that can function as discourse markers, such as &amp;quot;well&amp;quot; and &amp;quot;okay,&amp;quot; are ambiguous as to whether they are being used as discourse markers or not. The complication is that discourse markers tend to be used to introduce a new utterance, or can be an utterance all to themselves (such as the acknowledgment &amp;quot;okay&amp;quot; or &amp;quot;alright&amp;quot;), or can be used as part of the editing term of a speech repair, or to begin the alteration. Hence, the problem of identifying discourse markers al</context>
</contexts>
<marker>Schiffrin, 1987</marker>
<rawString>Schiffrin, D. 1987. Discourse Markers. New York: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Silverman</author>
<author>M Beckman</author>
<author>J Pitrelli</author>
<author>M Ostendorf</author>
<author>C Wightman</author>
<author>P Price</author>
<author>J Pierrehumbert</author>
<author>J Hirschberg</author>
</authors>
<title>ToBI: A standard for labelling English prosody.</title>
<date>1992</date>
<booktitle>In Proceedings of the 2nd International Conference on Spoken Language Processing (ICSLP-92),</booktitle>
<pages>867--870</pages>
<contexts>
<context position="1310" citStr="Silverman et al., 1992" startWordPosition="202" endWordPosition="205">odel of a speech recognizer. We find that by accounting for the interactions between these tasks that the performance on each task improves, as does POS tagging and perplexity. 1 Introduction Interactive spoken dialog provides many new challenges for natural language understanding systems. One of the most critical challenges is simply determining the speaker&apos;s intended utterances: both segmenting the speaker&apos;s turn into utterances and determining the intended words in each utterance. Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases (Silverman et al., 1992), which end with an acoustically signaled boundary tone. Even assuming perfect word recognition, the problem of determining the intended words is complicated due to the occurrence of speech repairs, which occur where the speaker goes back and changes (or repeats) something she just said. The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. The following example, from the Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words </context>
</contexts>
<marker>Silverman, Beckman, Pitrelli, Ostendorf, Wightman, Price, Pierrehumbert, Hirschberg, 1992</marker>
<rawString>Silverman, K., M. Beckman, J. Pitrelli, M. Ostendorf, C. Wightman, P. Price, J. Pierrehumbert, and J. Hirschberg. 1992. ToBI: A standard for labelling English prosody. In Proceedings of the 2nd International Conference on Spoken Language Processing (ICSLP-92), pages 867-870.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Automatic linguistic segmentation of conversational speech.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4rd International Conference on Spoken Language Processing (ICSLP-96),</booktitle>
<contexts>
<context position="2341" citStr="Stolcke and Shriberg, 1996" startWordPosition="376" endWordPosition="379">The following example, from the Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones</context>
<context position="18861" citStr="Stolcke and Shriberg (1996" startWordPosition="3236" endWordPosition="3239">d for each non-null tone, editing term (we also skip over E=ET), and repair tag, we insert it into the appropriate place, just as Kompe et al. (1994) do for boundary tones in their language model. Below we give the encoded context for the word &amp;quot;know&amp;quot; from Example 7 Example 8 (d93-18.1 utt47) it/PRP takes/VBP one/CD Push you/PRP The result of this is that the non-null tag values are treated just as if they were lexical items.&apos; Furthermore, if an editing term is completed, or the extent of a repair is known, we can also clean up the editing term or reparandum, respectively, in the same way that Stolcke and Shriberg (1996b) clean up filled pauses, and simple repair patterns. This means that we can then generalize between fluent speech and instances that have a repair. For instance, in the two examples below, the context for the word &amp;quot;get&amp;quot; and its POS tag will be the same for both, namely &amp;quot;so/CC_D we/PRP need/VBP to/TO&amp;quot;. Example 9 (d93-11.1 utt46) so we need to get the three tankers Example 10 (d92a-2.2 utt6) so we need to Push urn Pop A get a tanker of OJ We also include other features of the context. For instance, we include a variable to indicate if we are currently processing an editing term, and whether a </context>
<context position="28895" citStr="Stolcke and Shriberg (1996" startWordPosition="4895" endWordPosition="4898"> repair that just consists of a filled pause or word fragment On this subset they obtained a correction recall rate of 43% and a precision of 50%. Nakatani and Hirschberg (1994) examined how speech repairs can be detected using a variety of information, including acoustic, presence of word &apos;Silence has a bigger effect on detection and correction if boundary tones are modeled. matchings, and POS tags. Using these clues they were able to train a decision tree which achieved a recall rate of 86.1% and a precision of 92.1% on a set of turns in which each turn contained at least one speech repair. Stolcke and Shriberg (1996b) examined whether perplexity can be improved by modeling simple types of speech repairs in a language model. They find that doing so actually makes perplexity worse, and they attribute this to not having a linguistic segmentation available, which would help in modeling filled pauses. We feel that speech repair modeling must be combined with detecting utterance boundaries and discourse markers, and should take advantage of acoustic information. For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. Their better perfor</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Stolcke, A. and E. Shriberg. 1996a. Automatic linguistic segmentation of conversational speech. In Proceedings of the 4rd International Conference on Spoken Language Processing (ICSLP-96), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Statistical language modeling for speech disfluencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP),</booktitle>
<contexts>
<context position="2341" citStr="Stolcke and Shriberg, 1996" startWordPosition="376" endWordPosition="379">The following example, from the Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones</context>
<context position="18861" citStr="Stolcke and Shriberg (1996" startWordPosition="3236" endWordPosition="3239">d for each non-null tone, editing term (we also skip over E=ET), and repair tag, we insert it into the appropriate place, just as Kompe et al. (1994) do for boundary tones in their language model. Below we give the encoded context for the word &amp;quot;know&amp;quot; from Example 7 Example 8 (d93-18.1 utt47) it/PRP takes/VBP one/CD Push you/PRP The result of this is that the non-null tag values are treated just as if they were lexical items.&apos; Furthermore, if an editing term is completed, or the extent of a repair is known, we can also clean up the editing term or reparandum, respectively, in the same way that Stolcke and Shriberg (1996b) clean up filled pauses, and simple repair patterns. This means that we can then generalize between fluent speech and instances that have a repair. For instance, in the two examples below, the context for the word &amp;quot;get&amp;quot; and its POS tag will be the same for both, namely &amp;quot;so/CC_D we/PRP need/VBP to/TO&amp;quot;. Example 9 (d93-11.1 utt46) so we need to get the three tankers Example 10 (d92a-2.2 utt6) so we need to Push urn Pop A get a tanker of OJ We also include other features of the context. For instance, we include a variable to indicate if we are currently processing an editing term, and whether a </context>
<context position="28895" citStr="Stolcke and Shriberg (1996" startWordPosition="4895" endWordPosition="4898"> repair that just consists of a filled pause or word fragment On this subset they obtained a correction recall rate of 43% and a precision of 50%. Nakatani and Hirschberg (1994) examined how speech repairs can be detected using a variety of information, including acoustic, presence of word &apos;Silence has a bigger effect on detection and correction if boundary tones are modeled. matchings, and POS tags. Using these clues they were able to train a decision tree which achieved a recall rate of 86.1% and a precision of 92.1% on a set of turns in which each turn contained at least one speech repair. Stolcke and Shriberg (1996b) examined whether perplexity can be improved by modeling simple types of speech repairs in a language model. They find that doing so actually makes perplexity worse, and they attribute this to not having a linguistic segmentation available, which would help in modeling filled pauses. We feel that speech repair modeling must be combined with detecting utterance boundaries and discourse markers, and should take advantage of acoustic information. For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. Their better perfor</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Stolcke, A. and E. Shriberg. 1996b. Statistical language modeling for speech disfluencies. In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Q Wang</author>
<author>J Hirschberg</author>
</authors>
<title>Automatic classification of intonational phrase boundaries.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--175</pages>
<contexts>
<context position="2283" citStr="Wang and Hirschberg, 1992" startWordPosition="368" endWordPosition="371">of the intended utterance, and so need to be identified. The following example, from the Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and</context>
<context position="29751" citStr="Wang and Hirschberg (1992)" startWordPosition="5024" endWordPosition="5027">n available, which would help in modeling filled pauses. We feel that speech repair modeling must be combined with detecting utterance boundaries and discourse markers, and should take advantage of acoustic information. For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. Their better performance is partly attributed to richer (speaker dependent) acoustic modeling, including phoneme duration, energy, and pitch. However, their model was trained and tested on professionally read speech, rather than spontaneous speech. Wang and Hirschberg (1992) did employ spontaneous speech, namely, the ATIS corpus. For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents with intonational features. One explanation for the difference in performance was that our model was trained on approximately ten times as much data. Secondly, their decision trees are used to classify each data point independently of the next, whereas we find the best interpretation over the entire turn, and incorporate speech repai</context>
</contexts>
<marker>Wang, Hirschberg, 1992</marker>
<rawString>Wang, M. Q. and J. Hirschberg. 1992. Automatic classification of intonational phrase boundaries. Computer Speech and Language, 6:175-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Wightman</author>
<author>M Ostendorf</author>
</authors>
<title>Automatic labeling of prosodic patterns.</title>
<date>1994</date>
<journal>IEEE Transactions on</journal>
<contexts>
<context position="2313" citStr="Wightman and Ostendorf, 1994" startWordPosition="372" endWordPosition="375">and so need to be identified. The following example, from the Trains corpus (Heeman and Allen, 1995), gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term. Example 1 (d92a-5.2 utt34) we&apos;ll pick up a tank of uh the tanker of oranges reparandurni editing term alteration interruption point Much work has been done on both detecting boundary tones (e.g. (Wang and Hirschberg, 1992; Wightman and Ostendorf, 1994; Stolcke and Shriberg, 1996a; Kompe et al., 1994; Mast et al., 1996)) and on speech repair detection and correction (e.g. (Hindle, 1983; Bear, Dowding, and Shriberg, 1992; Nakatani and Hirschberg, 1994; Heeman and Allen, 1994; Stolcke and Shriberg, 1996b)). This work has focused on one of the issues in isolation of the other. However, these two issues are intertwined. Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events. Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, so</context>
<context position="29417" citStr="Wightman and Ostendorf (1994)" startWordPosition="4976" endWordPosition="4979">92.1% on a set of turns in which each turn contained at least one speech repair. Stolcke and Shriberg (1996b) examined whether perplexity can be improved by modeling simple types of speech repairs in a language model. They find that doing so actually makes perplexity worse, and they attribute this to not having a linguistic segmentation available, which would help in modeling filled pauses. We feel that speech repair modeling must be combined with detecting utterance boundaries and discourse markers, and should take advantage of acoustic information. For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. Their better performance is partly attributed to richer (speaker dependent) acoustic modeling, including phoneme duration, energy, and pitch. However, their model was trained and tested on professionally read speech, rather than spontaneous speech. Wang and Hirschberg (1992) did employ spontaneous speech, namely, the ATIS corpus. For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents w</context>
</contexts>
<marker>Wightman, Ostendorf, 1994</marker>
<rawString>Wightman, C. W. and M. Ostendorf. 1994. Automatic labeling of prosodic patterns. IEEE Transactions on speech and audio processing, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>