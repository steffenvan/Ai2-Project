<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028826">
<title confidence="0.98324">
IWNLP: Inverse Wiktionary for Natural Language Processing
</title>
<author confidence="0.99028">
Matthias Liebeck and Stefan Conrad
</author>
<affiliation confidence="0.773024">
Institute of Computer Science
Heinrich-Heine-University D¨usseldorf
</affiliation>
<address confidence="0.752876">
D-40225 D¨usseldorf, Germany
</address>
<email confidence="0.998076">
{liebeck,conrad}@cs.uni-duesseldorf.de
</email>
<sectionHeader confidence="0.994774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931954545455">
Nowadays, there are a lot of natural lan-
guage processing pipelines that are based
on training data created by a few experts.
This paper examines how the prolifera-
tion of the internet and its collaborative
application possibilities can be practically
used for NLP. For that purpose, we ex-
amine how the German version of Wik-
tionary can be used for a lemmatization
task. We introduce IWNLP, an open-
source parser for Wiktionary, that reim-
plements several MediaWiki markup lan-
guage templates for conjugated verbs and
declined adjectives. The lemmatization
task is evaluated on three German corpora
on which we compare our results with ex-
isting software for lemmatization. With
Wiktionary as a resource, we obtain a high
accuracy for the lemmatization of nouns
and can even improve on the results of
existing software for the lemmatization of
nouns.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997625066666667">
Wiktionary is an internet-based dictionary and the-
saurus that lists words, inflected forms and rela-
tions (e.g. synonyms) between words. Just as
Wikipedia, Wiktionary uses MediaWiki as a plat-
form but focuses on word definitions and their
meaning, rather than explaining each word in de-
tail, as Wikipedia does. The dictionary contains
articles, which can each list multiple entries for
different languages and multiple parts of speech.
For instance, the English word home has entries as
a noun, verb, adjective and as an adverb.
Each article is rendered by the MediaWiki en-
gine from a text-based input, which uses the Me-
diaWiki syntax and relies heavily on the use of
templates. The articles are editable by everyone,
</bodyText>
<tableCaption confidence="0.9065835">
Table 1: Declension of the German noun Turm
(tower)
</tableCaption>
<table confidence="0.967136571428571">
Case Singular Plural
Nominative der Turm die T¨urme
Genitive des Turmes der T¨urme
des Turms
Dative dem Turm den T¨urmen
dem Turme
Accusative den Turm die T¨urme
</table>
<bodyText confidence="0.998680777777778">
even by unregistered users. Although vandalism is
possible, most of the vandalized entries are iden-
tified by other users who watch a list of the lat-
est changes and subsequently revert these entries
to previously correct versions. All text content
is licensed under the Creative Commons License,
which makes it attractive for academic use.
There are currently 111 localized versions of
Wiktionary, which contain more than 1000 arti-
cles1. A localized version can establish own rules
via majority votes and public opinion. For exam-
ple, the German version of Wiktionary2 currently
enforces a 5-source-rule, which requires that each
entry that is not listed in a common dictionary is
documented by at least 5 different sources. The
German version of Wiktionary has grown over the
last years and currently contains almost 400000 ar-
ticles3. Each word is listed with its part-of-speech
tag, among other information. If a word is in-
flectable (nouns, verbs, adjectives, pronouns and
articles are inflectable in the German language),
all inflected forms are also enumerated. Table 1
shows the declension of the noun Turm (tower).
Wiktionary provides information that can be used
as a resource for Natural Language Processing
(NLP), for instance for part-of-speech tagging, for
lemmatization and as a thesaurus.
</bodyText>
<footnote confidence="0.999908666666667">
1https://meta.wikimedia.org/wiki/Wiktionary
2https://de.wiktionary.org
3https://de.wiktionary.org/wiki/Wiktionary:Meilensteine
</footnote>
<page confidence="0.575255">
414
</page>
<bodyText confidence="0.822692909090909">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 414–418,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
The rest of the paper is structured as follows:
Section 2 gives on overview of previous applica-
tions of Wiktionary for natural language process-
ing purposes. Section 3 outlines the basic steps of
parsing Wiktionary. The use of Wiktionary as a
lemmatizer is evaluated in section 4 and compared
with existing software for lemmatization. Finally,
we conclude in chapter 5 and outline future work.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999856266666667">
The closest work to ours is JWKTL (Zesch et al.,
2008). JWKTL is a Wiktionary parser that was
originally developed for the English and the Ger-
man version of Wiktionary, but it now also sup-
ports Russian. Our work differs from JWKTL, be-
cause we currently focus more on inflections in the
German version than JWKTL. Therefore, we have
a larger coverage of inflections, because we addi-
tionally reimplemented several templates from the
namespace Flexion. Also, we have an improved
handling of special syntactic cases, as compared
to JWKTL.
Wiktionary has previously been used for sev-
eral NLP tasks. The use of the German edition
as a thesaurus has been investigated by Meyer
and Gurevych (2010). The authors compared the
semantic relations in Wiktionary with GermaNet
(Hamp and Feldweg, 1997) and OpenThesaurus
(Naber, 2005).
Smedt et al. (2014) developed a part-of-speech
tagger based on entries in the Italian version of
Wiktionary. They achieved an accuracy of 85,5 %
with Wiktionary alone. By using morphological
and contextual rules, they improve their tagging
to an accuracy of 92,9 %. Li et al. (2012) also
used Wiktionary to create a part-of-speech tagger,
which is based on a hidden Markov model. Their
evaluation of 9 different languages shows an aver-
age accuracy of 84,5 %, with English having the
best result with an accuracy of 87,1 %.
</bodyText>
<sectionHeader confidence="0.862107" genericHeader="method">
3 Parsing Wiktionary
</sectionHeader>
<bodyText confidence="0.999699142857143">
There are multiple ways to parse Wiktionary. It is
possible to crawl all existing articles from the on-
line servers. To reduce stress from the servers and
to easily reproduce our parsing results, we parse
the latest of the monthly XML dumps4 from Wik-
tionary. For this paper, we use the currently latest
dump 20150407.
</bodyText>
<footnote confidence="0.778116">
4http://dumps.wikimedia.org/dewiktionary/
</footnote>
<bodyText confidence="0.993121793103448">
We iterate over every article in the XML dump
and parse articles which contain German word en-
tries. These articles can be separated into two
groups: the ones in the main namespace (with-
out any preceding namespace, like ‘namespace:’)
and the ones in the namespace Flexion. First, we
describe how we parse the articles in the main
namespace. An article can contain entries for mul-
tiple languages. Therefore, we divide its text con-
tent into language blocks (== heading ==) and skip
non-German language blocks. Afterward, we ex-
tract one or more entries (=== heading ===) from
each German language block. If an article lists
more than one entry with the same name, its word
forms will be different from each other. For in-
stance, the German word Mutter5, contains an en-
try for mother and for nut, which have different
plural forms. We parse the part-of-speech tag for
each entry. If a word is inflectable, we will also
parse its inflections, which are listed in a key-
value-pair template. Depending on the part-of-
speech tag, different templates are used in Wik-
tionary for which we use different parsers. We
provide parsers for nouns, verbs, adjective and
pronouns. The key-value-template for the adjec-
tive gelb (yellow) is displayed in Figure 1.
== gelb ({{Language|German}}) ==
=== {{POS|Adjective|German}} ===
{{German Adjective Overview
</bodyText>
<figure confidence="0.9502515">
|Positive=gelb
|Comparative=gelber
|Superlative=am gelbsten
}}
</figure>
<figureCaption confidence="0.987289">
Figure 1: Adjective template for the word gelb
(yellow), with keywords translated into English
</figureCaption>
<bodyText confidence="0.999948083333333">
At this point, we should point out that the inflec-
tions for verbs and adjectives in the main names-
pace are only a small portion of all possible inflec-
tions. For example, a verb in the main namespace
only lists one inflection for the past tense (first
person singular), while other possible past tense
forms are not listed.
Fortunately, it is possible that a verb or an ad-
jective has an additional article in the namespace
Flexion, where all inflections are listed. However,
the parsing of these inflections is more challeng-
ing, because the articles use complex templates.
</bodyText>
<footnote confidence="0.986187">
5https://de.wiktionary.org/wiki/Mutter
</footnote>
<page confidence="0.9962">
415
</page>
<bodyText confidence="0.9999715">
Although the parsing of the parameters for the
templates remains the same, it is more difficult
to retrieve the rendered output by the MediaWiki
engine (and thus the inflections) from these tem-
plates, because it is very rare that inflections are
listed as a key-value-pair. Instead, these templates
require principal parts, which are combined with
suffixes. The users of Wiktionary have created
templates, that take care of special cases, for in-
stance for a verb conjugation, where the suffix
’est’ is added to a verb stem instead of ’st’, if the
last character of a verb stem is a ’t’. Wiktionary
uses a MediaWiki extension called ParserFunc-
tions, which allows the use of control flows, like
if-statements and switch statements. Special cases
for the conjugation of verbs and the declension of
adjectives are covered by a nested control flow.
We have analyzed these templates and reimple-
mented the template of the adjectives and the most
frequently used templates for verbs into IWNLP
as C# code. In total, Wiktionary currently contains
3705 verb conjugations in the Flexion namespace,
which use several templates. We have limited our
implementation to the three most used verb con-
jugation templates (weak inseparable (51,4 %), ir-
regular (27,2 %), regular (12,4 %)).
Altogether, we have extracted 74254 different
words and 281457 different word forms. To re-
duce errors while parsing, we have written more
than 150 unit tests to ensure that our parser oper-
ates as accurate as possible on various notations
and special cases. During the development of
IWNLP, we have manually corrected more than
200 erroneous Wiktionary articles, which con-
tained wrong syntax or false content. To guarantee
that we didn’t worsen the quality of these articles,
we’ve consulted experienced Wiktionary users be-
fore performing these changes.
Our parser and its output will be made available
under an open-source license.6
</bodyText>
<sectionHeader confidence="0.996778" genericHeader="method">
4 Lemmatization
</sectionHeader>
<bodyText confidence="0.99978975">
Wiktionary can be used as a resource for multi-
ple NLP tasks. Currently, we are interested in us-
ing Wiktionary as a resource for a lemmatization
task, where we want to determine a lemma for a
given inflected form. For each lemma, Wiktionary
lists multiple inflected forms. As outlined in sec-
tion 3, we have parsed the inflected forms for each
lemma. For our lemmatization task, we inverse
</bodyText>
<footnote confidence="0.938151">
6http://www.iwnlp.com
</footnote>
<bodyText confidence="0.99985124">
this mapping to retrieve a list of possible lem-
mas for a given inflection, hence our project name
IWNLP. For example, we use the information pre-
sented in Table 1 to retrieve T¨urme H Turm. For
each lemma l in Wiktionary, we have also added
a mapping l H l. Our mapping will also be avail-
able via download.
It is possible, that an inflected form maps to
more than one lemma. For instance, the word
Kohle maps to Kohle (coal) and Kohl (cabbage).
In total, our mapping contains 2035 words, which
map to more than one lemma.
With this paper, we want to evaluate how good
Wiktionary performs in a lemmatization task. Ad-
ditionally, we want to validate our assumption,
that by first looking up word forms and their lem-
mas in Wiktionary, we should be able to improve
the performance of existing software for lemmati-
zation.
Therefore, we evaluate IWNLP and existing
software on three German corpora, which list
words and their lemmas: TIGER Corpus (Brants
et al., 2004), Hamburg Dependency Treebank
(HDT) (Foth et al., 2014) and T¨uBa-D/Z (Telljo-
hann et al., 2012) release 9.1. The TIGER Cor-
pus consists of 50472 sentences from the German
newspaper Frankfurter Rundschau. The Hamburg
Dependency Treebank (part A) contains 101981
sentences from the German IT news site Heise on-
line. The T¨uBa-D/Z corpus comprises of 85358
sentences from the newspaper die tageszeitung
(taz). Each word in these corpora is listed with its
part-of-speech tag from the STTS tagset (Schiller
et al., 1999). We evaluate the lemmatization for
nouns (POS tag NN), verbs (POS tags V*) and ad-
jectives (POS tags ADJA and ADJD). Due to the
low amount of different articles and pronouns in
the German language, we ignore them in our eval-
uation.
In our experiments, we look up the nouns, verbs
and adjectives from each corpus in IWNLP. If we
map a word form to more than one lemma in
IWNLP, we treat this case as if there would be no
entry for this particular word form in IWNLP. The
same policy is applied in all of our experiments.
We preserve case sensitivity, which worsens our
results slightly. In a modification, that we name
keep, we assume that a word w will be its own
lemma, if w does not have an entry in the map-
ping. IWNLP is compared with a mapping7 ex-
</bodyText>
<footnote confidence="0.995275">
7http://www.danielnaber.de/morphologie/index en.html
</footnote>
<page confidence="0.992706">
416
</page>
<table confidence="0.999329416666667">
Method TIGER Corpus T¨uBa-D/Z HDT
Noun Verb Adj Noun Verb Adj Noun Verb Adj
IWNLP 0,734 0,837 0,633 0,720 0,809 0,567 0,607 0,864 0,613
IWNLP + keep 0,894 0,854 0,692 0,897 0,827 0,650 0,647 0,882 0,699
Morphy 0,196 0,713 0,531 0,181 0,671 0,490 0,163 0,675 0,475
Morphy + keep 0,857 0,962 0,763 0,860 0,916 0,744 0,619 0,963 0,735
Mate Tools — — — 0,926 0,927 0,852 0,639 0,971 0,712
TreeTagger 0,860 0,974 0,867 0,848 0,930 0,832 0,611 0,977 0,687
IWNLP + Mate Tools — — — 0,943 0,929 0,841 0,653 0,976 0,751
Morphy + Mate Tools — — — 0,918 0,932 0,837 0,627 0,974 0,744
IWNLP + TreeTagger 0,888 0,969 0,869 0,879 0,927 0,795 0,641 0,973 0,724
Morphy + TreeTagger 0,859 0,970 0,810 0,843 0,926 0,787 0,602 0,968 0,713
</table>
<tableCaption confidence="0.999498">
Table 2: Lemmatization accuracy for nouns, verbs and adjectives in all three corpora
</tableCaption>
<bodyText confidence="0.995513731707317">
tracted from Morphy (Lezius et al., 1998), a tool
for morphological analysis.
For our comparison with existing software, that
can be used for lemmatization, we have chosen
Mate Tools (Bj¨orkelund et al., 2010) and Tree-
Tagger (Schmid, 1994), which both accept token-
based input.
The results of our experiments are shown in Ta-
ble 2. In a direct comparison between IWNLP and
Morphy, IWNLP outperforms Morphy in the ba-
sic variant in all POS tags across all corpora. With
the modification keep, the results of IWNLP and
Morphy improve. IWNLP + keep is still superior
for nouns, but Morphy + keep achieves better re-
sults for verbs and adjectives. The results from
Mate Tools on the TIGER Corpus are excluded
from Table 2 because Mate Tools was trained on
the TIGER Corpus and, therefore, cannot be eval-
uated on it. The direct comparison of Mate Tools
and TreeTagger shows that Mate Tools achieves
an accuracy that is at least 2 % better in four of the
six cases. In the other two cases, TreeTagger only
performs slightly better.
For the lemmatization of nouns, IWNLP is able
to improve on the results of Mate Tools and Tree-
Tagger across all three corpora. In total, IWNLP
enhances the results of Mate Tools in five of the
six test cases. Surprisingly, the additional lookup
of word forms in IWNLP and Morphy can impair
the accuracy for verbs and adjectives. In our future
work, we will systematically analyze which words
are responsible for worsening the results, correct
their Wiktionary articles and improve our lookup
in IWNLP.
The overall bad performance for the lemmatiza-
tion of nouns in the HDT corpus can be explained
by the gold lemmas for compound nouns, which
are often defined as the last word in the compound
noun. For instance, HDT defines that Freiheit
(freedom) is the gold lemma for Meinungsfreiheit
(freedom of speech).
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986416666667">
We have presented IWNLP, a parser for the Ger-
man version of Wiktionary. The current focus of
the parser lies in the extraction of inflected forms.
They have been used to construct a mapping from
inflected forms to lemmas, which can be utilized
in a lemmatization task. We evaluated our IWNLP
lemmatizer on three German corpora. The results
for the lemmatization of nouns show that IWNLP
outperforms existing software on the TIGER Cor-
pus and can improve their results on the T¨uBa-D/Z
and the HDT corpora. However, we have also dis-
covered that we still need to improve IWNLP to
get better results for the lemmatization of verbs
and adjectives. We will try to resolve the correct
lemma for an inflected form if multiple lemmas
are possible.
Additionally, IWNLP will be extended to parse
hyponyms and hypernyms for nouns. We plan to
compare the use of Wiktionary as thesaurus with
GermaNet (Hamp and Feldweg, 1997).
We expect that the presented results for the
lemmatization task will improve with every new
monthly dump if Wiktionary continues to grow
and improve through a community effort.
</bodyText>
<sectionHeader confidence="0.998026" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.977971">
This work is part of the graduate school NRW
Fortschrittskolleg Online-Partizipation. We thank
</bodyText>
<page confidence="0.993423">
417
</page>
<bodyText confidence="0.716361">
the Wiktionary user Yoursmile for his help.
</bodyText>
<sectionHeader confidence="0.948837" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854833333334">
Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A High-Performance Syn-
tactic and Semantic Dependency Parser. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, COL-
ING ’10, pages 33–36. Association for Computa-
tional Linguistics.
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen-Schirra, Esther K¨onig, Wolfgang Lezius,
Christian Rohrer, George Smith, and Hans Uszko-
reit. 2004. TIGER: Linguistic Interpretation of a
German Corpus. Research on Language and Com-
putation, 2(4):597–620.
Kilian A. Foth, Arne K¨ohn, Niels Beuck, and Wolfgang
Menzel. 2014. Because Size Does Matter: The
Hamburg Dependency Treebank. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC-2014), pages 2326–
2333.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9–15.
Wolfgang Lezius, Reinhard Rapp, and Manfred Wet-
tler. 1998. A Freely Available Morphological Ana-
lyzer, Disambiguator and Context Sensitive Lemma-
tizer for German. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics -
Volume 2, COLING ’98, pages 743–748. Associa-
tion for Computational Linguistics.
Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly
Supervised Part-of-speech Tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1389–1398. Association for Computational
Linguistics.
Christian M. Meyer and Iryna Gurevych. 2010. Worth
Its Weight in Gold or Yet Another Resource - A
Comparative Study of Wiktionary, OpenThesaurus
and GermaNet. In Computational Linguistics and
Intelligent Text Processing, 11th International Con-
ference, CICLing 2010, volume 6008 of Lecture
Notes in Computer Science, pages 38–49. Springer.
Daniel Naber. 2005. OpenThesaurus: ein offenes
deutsches Wortnetz. In Sprachtechnologie, mo-
bile Kommunikation und linguistische Ressourcen:
Beitr¨age zur GLDV-Tagung 2005 in Bonn, pages
422–433. Peter-Lang-Verlag.
Anne Schiller, Simone Teufel, Christine St¨ockert, and
Christine Thielen. 1999. Guidelines f¨ur das Tagging
deutscher Textcorpora mit STTS (kleines und großes
Tagset). Technical report, Universit¨at Stuttgart, Uni-
versit¨at T¨ubingen.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing.
Tom De Smedt, Fabio Marfia, Matteo Matteucci, and
Walter Daelemans. 2014. Using Wiktionary to
Build an Italian Part-of-Speech Tagger. In Natu-
ral Language Processing and Information Systems
- 19th International Conference on Applications of
Natural Language to Information Systems, NLDB
2014, volume 8455 of Lecture Notes in Computer
Science, pages 1–8. Springer.
Heike Telljohann, Erhard W. Hinrichs, Sandra K¨ubler,
Heike Zinsmeister, and Kathrin Beck. 2012. Style-
book for the T¨ubingen Treebank of Written Ger-
man (T¨uBa-D/Z). Technical report, University of
T¨ubingen.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of the
Sixth International Conference on Language Re-
sources and Evaluation (LREC’08). European Lan-
guage Resources Association.
</reference>
<page confidence="0.993062">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915215">
<title confidence="0.998993">IWNLP: Inverse Wiktionary for Natural Language Processing</title>
<author confidence="0.975791">Liebeck</author>
<affiliation confidence="0.9947765">Institute of Computer Heinrich-Heine-University</affiliation>
<address confidence="0.972429">D-40225 D¨usseldorf,</address>
<abstract confidence="0.998798347826087">Nowadays, there are a lot of natural language processing pipelines that are based on training data created by a few experts. This paper examines how the proliferation of the internet and its collaborative application possibilities can be practically used for NLP. For that purpose, we examine how the German version of Wiktionary can be used for a lemmatization task. We introduce IWNLP, an opensource parser for Wiktionary, that reimplements several MediaWiki markup language templates for conjugated verbs and declined adjectives. The lemmatization task is evaluated on three German corpora on which we compare our results with existing software for lemmatization. With Wiktionary as a resource, we obtain a high accuracy for the lemmatization of nouns and can even improve on the results of existing software for the lemmatization of nouns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Bernd Bohnet</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>A High-Performance Syntactic and Semantic Dependency Parser.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, COLING ’10,</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bj¨orkelund, Bohnet, Hafdell, Nugues, 2010</marker>
<rawString>Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A High-Performance Syntactic and Semantic Dependency Parser. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, COLING ’10, pages 33–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Peter Eisenberg</author>
<author>Silvia Hansen-Schirra</author>
<author>Esther K¨onig</author>
<author>Wolfgang Lezius</author>
<author>Christian Rohrer</author>
<author>George Smith</author>
<author>Hans Uszkoreit</author>
</authors>
<title>TIGER: Linguistic Interpretation of a German Corpus.</title>
<date>2004</date>
<journal>Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<marker>Brants, Dipper, Eisenberg, Hansen-Schirra, K¨onig, Lezius, Rohrer, Smith, Uszkoreit, 2004</marker>
<rawString>Sabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen-Schirra, Esther K¨onig, Wolfgang Lezius, Christian Rohrer, George Smith, and Hans Uszkoreit. 2004. TIGER: Linguistic Interpretation of a German Corpus. Research on Language and Computation, 2(4):597–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian A Foth</author>
<author>Arne K¨ohn</author>
<author>Niels Beuck</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Because Size Does Matter: The Hamburg Dependency Treebank.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014),</booktitle>
<pages>2326--2333</pages>
<marker>Foth, K¨ohn, Beuck, Menzel, 2014</marker>
<rawString>Kilian A. Foth, Arne K¨ohn, Niels Beuck, and Wolfgang Menzel. 2014. Because Size Does Matter: The Hamburg Dependency Treebank. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), pages 2326– 2333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet -a Lexical-Semantic Net for German.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>9--15</pages>
<contexts>
<context position="4929" citStr="Hamp and Feldweg, 1997" startWordPosition="753" endWordPosition="756">ary, but it now also supports Russian. Our work differs from JWKTL, because we currently focus more on inflections in the German version than JWKTL. Therefore, we have a larger coverage of inflections, because we additionally reimplemented several templates from the namespace Flexion. Also, we have an improved handling of special syntactic cases, as compared to JWKTL. Wiktionary has previously been used for several NLP tasks. The use of the German edition as a thesaurus has been investigated by Meyer and Gurevych (2010). The authors compared the semantic relations in Wiktionary with GermaNet (Hamp and Feldweg, 1997) and OpenThesaurus (Naber, 2005). Smedt et al. (2014) developed a part-of-speech tagger based on entries in the Italian version of Wiktionary. They achieved an accuracy of 85,5 % with Wiktionary alone. By using morphological and contextual rules, they improve their tagging to an accuracy of 92,9 %. Li et al. (2012) also used Wiktionary to create a part-of-speech tagger, which is based on a hidden Markov model. Their evaluation of 9 different languages shows an average accuracy of 84,5 %, with English having the best result with an accuracy of 87,1 %. 3 Parsing Wiktionary There are multiple way</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -a Lexical-Semantic Net for German. In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Lezius</author>
<author>Reinhard Rapp</author>
<author>Manfred Wettler</author>
</authors>
<title>A Freely Available Morphological Analyzer, Disambiguator and Context Sensitive Lemmatizer for German.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics -Volume 2, COLING ’98,</booktitle>
<pages>743--748</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13428" citStr="Lezius et al., 1998" startWordPosition="2170" endWordPosition="2173">1 0,181 0,671 0,490 0,163 0,675 0,475 Morphy + keep 0,857 0,962 0,763 0,860 0,916 0,744 0,619 0,963 0,735 Mate Tools — — — 0,926 0,927 0,852 0,639 0,971 0,712 TreeTagger 0,860 0,974 0,867 0,848 0,930 0,832 0,611 0,977 0,687 IWNLP + Mate Tools — — — 0,943 0,929 0,841 0,653 0,976 0,751 Morphy + Mate Tools — — — 0,918 0,932 0,837 0,627 0,974 0,744 IWNLP + TreeTagger 0,888 0,969 0,869 0,879 0,927 0,795 0,641 0,973 0,724 Morphy + TreeTagger 0,859 0,970 0,810 0,843 0,926 0,787 0,602 0,968 0,713 Table 2: Lemmatization accuracy for nouns, verbs and adjectives in all three corpora tracted from Morphy (Lezius et al., 1998), a tool for morphological analysis. For our comparison with existing software, that can be used for lemmatization, we have chosen Mate Tools (Bj¨orkelund et al., 2010) and TreeTagger (Schmid, 1994), which both accept tokenbased input. The results of our experiments are shown in Table 2. In a direct comparison between IWNLP and Morphy, IWNLP outperforms Morphy in the basic variant in all POS tags across all corpora. With the modification keep, the results of IWNLP and Morphy improve. IWNLP + keep is still superior for nouns, but Morphy + keep achieves better results for verbs and adjectives. T</context>
</contexts>
<marker>Lezius, Rapp, Wettler, 1998</marker>
<rawString>Wolfgang Lezius, Reinhard Rapp, and Manfred Wettler. 1998. A Freely Available Morphological Analyzer, Disambiguator and Context Sensitive Lemmatizer for German. In Proceedings of the 17th International Conference on Computational Linguistics -Volume 2, COLING ’98, pages 743–748. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly Supervised Part-of-speech Tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1389--1398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly Supervised Part-of-speech Tagging. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1389–1398. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>Worth Its Weight in Gold or Yet Another Resource - A Comparative Study of Wiktionary, OpenThesaurus and GermaNet.</title>
<date>2010</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing, 11th International Conference, CICLing 2010,</booktitle>
<volume>6008</volume>
<pages>38--49</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4831" citStr="Meyer and Gurevych (2010)" startWordPosition="739" endWordPosition="742"> a Wiktionary parser that was originally developed for the English and the German version of Wiktionary, but it now also supports Russian. Our work differs from JWKTL, because we currently focus more on inflections in the German version than JWKTL. Therefore, we have a larger coverage of inflections, because we additionally reimplemented several templates from the namespace Flexion. Also, we have an improved handling of special syntactic cases, as compared to JWKTL. Wiktionary has previously been used for several NLP tasks. The use of the German edition as a thesaurus has been investigated by Meyer and Gurevych (2010). The authors compared the semantic relations in Wiktionary with GermaNet (Hamp and Feldweg, 1997) and OpenThesaurus (Naber, 2005). Smedt et al. (2014) developed a part-of-speech tagger based on entries in the Italian version of Wiktionary. They achieved an accuracy of 85,5 % with Wiktionary alone. By using morphological and contextual rules, they improve their tagging to an accuracy of 92,9 %. Li et al. (2012) also used Wiktionary to create a part-of-speech tagger, which is based on a hidden Markov model. Their evaluation of 9 different languages shows an average accuracy of 84,5 %, with Engl</context>
</contexts>
<marker>Meyer, Gurevych, 2010</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2010. Worth Its Weight in Gold or Yet Another Resource - A Comparative Study of Wiktionary, OpenThesaurus and GermaNet. In Computational Linguistics and Intelligent Text Processing, 11th International Conference, CICLing 2010, volume 6008 of Lecture Notes in Computer Science, pages 38–49. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Naber</author>
</authors>
<title>OpenThesaurus: ein offenes deutsches Wortnetz. In Sprachtechnologie, mobile Kommunikation und linguistische Ressourcen: Beitr¨age zur GLDV-Tagung 2005 in Bonn,</title>
<date>2005</date>
<pages>422--433</pages>
<contexts>
<context position="4961" citStr="Naber, 2005" startWordPosition="759" endWordPosition="760">work differs from JWKTL, because we currently focus more on inflections in the German version than JWKTL. Therefore, we have a larger coverage of inflections, because we additionally reimplemented several templates from the namespace Flexion. Also, we have an improved handling of special syntactic cases, as compared to JWKTL. Wiktionary has previously been used for several NLP tasks. The use of the German edition as a thesaurus has been investigated by Meyer and Gurevych (2010). The authors compared the semantic relations in Wiktionary with GermaNet (Hamp and Feldweg, 1997) and OpenThesaurus (Naber, 2005). Smedt et al. (2014) developed a part-of-speech tagger based on entries in the Italian version of Wiktionary. They achieved an accuracy of 85,5 % with Wiktionary alone. By using morphological and contextual rules, they improve their tagging to an accuracy of 92,9 %. Li et al. (2012) also used Wiktionary to create a part-of-speech tagger, which is based on a hidden Markov model. Their evaluation of 9 different languages shows an average accuracy of 84,5 %, with English having the best result with an accuracy of 87,1 %. 3 Parsing Wiktionary There are multiple ways to parse Wiktionary. It is pos</context>
</contexts>
<marker>Naber, 2005</marker>
<rawString>Daniel Naber. 2005. OpenThesaurus: ein offenes deutsches Wortnetz. In Sprachtechnologie, mobile Kommunikation und linguistische Ressourcen: Beitr¨age zur GLDV-Tagung 2005 in Bonn, pages 422–433. Peter-Lang-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
<author>Simone Teufel</author>
<author>Christine St¨ockert</author>
<author>Christine Thielen</author>
</authors>
<title>Guidelines f¨ur das Tagging deutscher Textcorpora mit STTS (kleines und großes Tagset).</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Universit¨at Stuttgart, Universit¨at T¨ubingen.</institution>
<marker>Schiller, Teufel, St¨ockert, Thielen, 1999</marker>
<rawString>Anne Schiller, Simone Teufel, Christine St¨ockert, and Christine Thielen. 1999. Guidelines f¨ur das Tagging deutscher Textcorpora mit STTS (kleines und großes Tagset). Technical report, Universit¨at Stuttgart, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="13626" citStr="Schmid, 1994" startWordPosition="2203" endWordPosition="2204"> 0,611 0,977 0,687 IWNLP + Mate Tools — — — 0,943 0,929 0,841 0,653 0,976 0,751 Morphy + Mate Tools — — — 0,918 0,932 0,837 0,627 0,974 0,744 IWNLP + TreeTagger 0,888 0,969 0,869 0,879 0,927 0,795 0,641 0,973 0,724 Morphy + TreeTagger 0,859 0,970 0,810 0,843 0,926 0,787 0,602 0,968 0,713 Table 2: Lemmatization accuracy for nouns, verbs and adjectives in all three corpora tracted from Morphy (Lezius et al., 1998), a tool for morphological analysis. For our comparison with existing software, that can be used for lemmatization, we have chosen Mate Tools (Bj¨orkelund et al., 2010) and TreeTagger (Schmid, 1994), which both accept tokenbased input. The results of our experiments are shown in Table 2. In a direct comparison between IWNLP and Morphy, IWNLP outperforms Morphy in the basic variant in all POS tags across all corpora. With the modification keep, the results of IWNLP and Morphy improve. IWNLP + keep is still superior for nouns, but Morphy + keep achieves better results for verbs and adjectives. The results from Mate Tools on the TIGER Corpus are excluded from Table 2 because Mate Tools was trained on the TIGER Corpus and, therefore, cannot be evaluated on it. The direct comparison of Mate T</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of the International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom De Smedt</author>
<author>Fabio Marfia</author>
<author>Matteo Matteucci</author>
<author>Walter Daelemans</author>
</authors>
<title>Using Wiktionary to Build an Italian Part-of-Speech Tagger.</title>
<date>2014</date>
<booktitle>In Natural Language Processing and Information Systems - 19th International Conference on Applications of Natural Language to Information Systems, NLDB 2014,</booktitle>
<volume>8455</volume>
<pages>1--8</pages>
<publisher>Springer.</publisher>
<marker>De Smedt, Marfia, Matteucci, Daelemans, 2014</marker>
<rawString>Tom De Smedt, Fabio Marfia, Matteo Matteucci, and Walter Daelemans. 2014. Using Wiktionary to Build an Italian Part-of-Speech Tagger. In Natural Language Processing and Information Systems - 19th International Conference on Applications of Natural Language to Information Systems, NLDB 2014, volume 8455 of Lecture Notes in Computer Science, pages 1–8. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard W Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Heike Zinsmeister</author>
<author>Kathrin Beck</author>
</authors>
<title>Stylebook for the T¨ubingen Treebank of Written German (T¨uBa-D/Z).</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>University of T¨ubingen.</institution>
<marker>Telljohann, Hinrichs, K¨ubler, Zinsmeister, Beck, 2012</marker>
<rawString>Heike Telljohann, Erhard W. Hinrichs, Sandra K¨ubler, Heike Zinsmeister, and Kathrin Beck. 2012. Stylebook for the T¨ubingen Treebank of Written German (T¨uBa-D/Z). Technical report, University of T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). European Language Resources Association.</booktitle>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). European Language Resources Association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>