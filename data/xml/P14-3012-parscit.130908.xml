<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000112">
<title confidence="0.998036">
Improving Text Normalization via Unsupervised Model and
Discriminative Reranking
</title>
<author confidence="0.999754">
Chen Li and Yang Liu
</author>
<affiliation confidence="0.9953645">
The University of Texas at Dallas
Computer Science Department
</affiliation>
<email confidence="0.998174">
chenli,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855227272727">
Various models have been developed for
normalizing informal text. In this paper,
we propose two methods to improve nor-
malization performance. First is an unsu-
pervised approach that automatically iden-
tifies pairs of a non-standard token and
proper word from a large unlabeled cor-
pus. We use semantic similarity based on
continuous word vector representation, to-
gether with other surface similarity mea-
surement. Second we propose a reranking
strategy to combine the results from differ-
ent systems. This allows us to incorporate
information that is hard to model in indi-
vidual systems as well as consider multi-
ple systems to generate a final rank for a
test case. Both word- and sentence-level
optimization schemes are explored in this
study. We evaluate our approach on data
sets used in prior studies, and demonstrate
that our proposed methods perform better
than the state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998419446428571">
There has been a lot of research efforts recently
on analysis of social media text (e.g., from Twit-
ter and Facebook) (Ritter et al., 2011; Owoputi et
al., 2013; Liu et al., 2012b). One challenge in
processing social media text is how to deal with
the frequently occurring non-standard words, such
as bday (meaning birthday), snd (meaning sound)
and gl (meaning girl) . Normalizing informal text
(changing non-standard words to standard ones)
will ease subsequent language processing mod-
ules.
Text normalization has been an important topic
for the text-to-speech field. See (Sproat et al.,
2001) for a good report of this problem. Recently,
much research on normalization has been done
for social text domain, which has many abbrevi-
ations or non-standard tokens. A simple approach
for normalization would be applying traditional
spell checking model, which is usually based on
edit distance (Damerau, 1964; Levenshtein, 1966).
However, this model can not well handle the non-
standard words in social media text due to the large
variation in generating them.
Another line of work in normalization adopts
a noisy channel model. For a non-standard to-
ken A, this method finds the most possible stan-
dard word Sˆ based on the Bayes rule: Sˆ =
argmaxP(S|A) = argmaxP(A|S) ∗ P(S).
Different methods have been used to compute
P(A|S). Pennell and Liu (2010) used a CRF se-
quence modeling approach for deletion-based ab-
breviations. Liu et al. (2011) further extended this
work by considering more types of non-standard
words without explicit pre-categorization for non-
standard tokens.
In addition, the noisy channel model has also
been utilized on the sentence level. Choudhury et
al. (2007) used a hidden Markov model to sim-
ulate SMS message generation, considering the
non-standard tokens in the input sentence as emis-
sion states in HMM and labeling results as pos-
sible candidates. Cook and Stevenson (2009) ex-
tended work by adding several more subsystems
in this error model according to the most common
non-standard token’s formation process.
Machine translation (MT) is another commonly
chosen method for text normalization. It is also
used on both the token and the sentence level. Aw
et al. (2006) treated SMS as another language, and
used MT methods to translate this ‘foreign lan-
guage’ to regular English. Contractor et al. (2010)
used an MT model as well but the focus of their
work is to utilize an unsupervised method to clean
noisy text. Pennell and Liu (2011) firstly intro-
duced an MT method at the token level which
translates an unnormalized token to a possible cor-
</bodyText>
<page confidence="0.972437">
86
</page>
<note confidence="0.837615">
Proceedings of the ACL 2014 Student Research Workshop, pages 86–93,
</note>
<bodyText confidence="0.959639888888889">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
rect word.
Recently, a new line of work surges relying on
the analysis of huge amount of twitter data, of-
ten in an unsupervised fashion. By using con-
text information from a large corpus, Han et al.
(2012) generated possible variant and normaliza-
tion pairs, and constructed a dictionary of lexical
variants of known words, which are further ranked
by string similarity. This dictionary can facilitate
lexical normalization via simple string substitu-
tion. Hassan and Menezes (2013) proposed an ap-
proach based on the random walk algorithm on a
contextual similarity bipartite graph, constructed
from n-gram sequences on a large unlabeled text
corpus. Yang and Eisenstein (2013) presented a
unified unsupervised statistical model for text nor-
malization.
</bodyText>
<sectionHeader confidence="0.839244" genericHeader="introduction">
2 Previous Normalization Methods Used
in Reranking
</sectionHeader>
<bodyText confidence="0.999919833333333">
In this work we adopt several normalization meth-
ods developed in previous studies. The following
briefly describes these previous approaches. Next
section will introduce our proposed methods using
unsupervised learning and discriminative rerank-
ing for system combination.
</bodyText>
<subsectionHeader confidence="0.997356">
2.1 Character-block level MT
</subsectionHeader>
<bodyText confidence="0.999096375">
Pennell and Liu (2011) proposed to use a
character-level MT model for text normalization.
The idea is similar to traditional translation,
except that the translation unit is characters,
not words. Formally, for a non-standard word
A = a1a2...an , the MT method finds the
most likely standard word S = s1s2...sm (az
and sz are the characters in the words): S =
</bodyText>
<equation confidence="0.9986295">
argmaxP(SIA) = argmaxP(A|S)P(S) =
argmaxP(a1a2...an|s1s2...sm)P(s1s2...sm)
</equation>
<bodyText confidence="0.999242">
where P(a1a2...an|s1s2...sm) is from a character-
level translation model, and P(s1s2...sm) is from
a character-level language model. (Li and Liu,
2012a) modified this approach to perform the
translation at the character-block level in order
to generate better alignment between characters
(analogous to the word vs. phrase based alignment
in traditional MT). This system generates one
ranked list of word candidates.
</bodyText>
<subsectionHeader confidence="0.99624">
2.2 Character-level Two-step MT
</subsectionHeader>
<bodyText confidence="0.999969230769231">
Li and Liu (2012b) extended the character-level
MT model by incorporating the pronunciation in-
formation. They first translate non-standard words
to possible pronunciations, which are then trans-
lated to standard words in the second step. This
method has been shown to yield high coverage
(high accuracy in its n-best hypotheses). There are
two candidate lists generated by this two-step MT
method. The first one is based on the pronuncia-
tion list produced in the first step (some phonetic
sequences directly correspond to standard words).
The second list is generated from the second trans-
lation step.
</bodyText>
<subsectionHeader confidence="0.999415">
2.3 Character-Block level Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.999975368421053">
Pennell and Liu (2010) used sequence labeling
model (CRF) for normalizing deletion-based ab-
breviation at the character-level. The model labels
every character in a standard word as ‘Y’ or ‘N’
to represent whether it appears or not in a possible
abbreviation token. The features used for the clas-
sification task represent the character’s position,
pronunciation and context information. Using the
sequence labeling model, a standard word can
generate many possible non-standard words. A re-
verse look-up table is used to store the correspond-
ing possible standard words for the non-standard
words for reverse lookup during testing. Liu et al.
(2011) extended the above model to handle other
types of non-standard words. (Li and Liu, 2012a)
used character-blocks (same ones as that in the
character-block MT method above) as the units in
this sequence labeling framework. There is one
list of word candidates from this method.
</bodyText>
<subsectionHeader confidence="0.999432">
2.4 Spell Checker
</subsectionHeader>
<bodyText confidence="0.99982">
The forth normalization subsystem is the Jazzy
Spell Checker1, which is based on edit distance
and integrates a phonetic matching algorithm as
well. This provides one list of hypotheses.
</bodyText>
<sectionHeader confidence="0.991321" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.999617777777778">
All the above models except the Spell Checker are
supervised methods that need labeled data con-
sisting of pairs of non-standard words and proper
words. In this paper we propose an unsupervised
method to create the lookup table of the non-
standard words and their corresponding proper
words offline. We further propose to use differ-
ent discriminative reranking approaches to com-
bine multiple individual systems.
</bodyText>
<footnote confidence="0.982011">
1http://jazzy.sourceforge.net
</footnote>
<page confidence="0.998486">
87
</page>
<subsectionHeader confidence="0.970369">
3.1 Unsupervised Corpus-based Similarity
for Normalization
</subsectionHeader>
<bodyText confidence="0.999993205128205">
Previous work has shown that unlabeled text can
be used to induce unsupervised word clusters
that can improve performance of many supervised
NLP tasks (Koo et al., 2008; Turian et al., 2010;
T¨ackstr¨om et al., 2012). We investigate using a
large unlabeled Twitter corpus to automatically
identify pairs of non-standard words and their cor-
responding standard words.
We use the Edinburgh Twitter corpus (Petro-
vic et al., 2010), and a dictionary obtained
from http://ciba.iciba.com/ to identify all the in-
vocabulary and out-of-vocabulary (OOV) words in
the corpus. The task is then to automatically find
the corresponding OOV words (if any) for each
dictionary word, and the likelihood of each pair.
The key question is how to compute this likelihood
or similarity.
We propose to use an unsupervised method
based on the large corpus to induce dense real-
valued low-dimension word embedding and then
use the inner product as a measure of semantic
similarity. We use the continuous bag-of-words
model that is similar to the feedforward neural
network language model to compute vector rep-
resentations of words. This model was first in-
troduced by (Mikolov et al., 2013). We use the
tool word2vec2 to implement this model. Two
constraints are used in order to eliminate unlikely
word pairs: (I) OOV words need to begin with the
same letter as the dictionary standard word; (II)
OOV words can only consist of English letter and
digits.
In addition to considering the above semantic
similarity, for the normalization task, we use other
information including the surface character level
similarity based on longest common sequence be-
tween the two tokens, and the frequency of the to-
ken. The final score between a dictionary word w
and an OOV word t is:
</bodyText>
<equation confidence="0.948641">
longest common string(w, t)
sim(w, t) =
(1)
length(t)
</equation>
<bodyText confidence="0.9972155">
The first and second term share the same property
of visual prime value used in (Liu et al., 2012a).
</bodyText>
<footnote confidence="0.737838">
2https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.999992111111111">
The third term is the vector-based semantic simi-
larity of the two words, calculated by our proposed
model. The last term is the length of longest com-
mon sequence between the two words divided by
the length of the OOV word.
Using this method, we can identify all the pos-
sible OOV words for each dictionary word based
on an unlabeled large corpus. Each pair has a
similarity score. Then a reverse lookup table is
created to store the corresponding possible stan-
dard words for each non-standard word, which is
used during testing. This framework is similar to
the sequence labeling method described in Sec-
tion 2.3 in the sense of creating the mapping ta-
ble between the OOV and dictionary words. How-
ever, the difference is that this is an unsupervised
method whereas the sequence labeling uses super-
vised learning to generate possible candidates.
</bodyText>
<subsectionHeader confidence="0.9961805">
3.2 Reranking for System Combination
3.2.1 Word Level Reranking
</subsectionHeader>
<bodyText confidence="0.9998485625">
Each of the above systems has its own strength and
weakness. The MT model and the sequence la-
beling models have better precision, the two-step
MT model has a broader coverage of candidates,
and the spell checker has a high confidence for
simple non-standard words. Therefore combining
these systems is expected to yield better overall
results. We propose to use a supervised maximum
entropy reranking model to combine our proposed
unsupervised method with those described in Sec-
tion 2 (4 systems that have 5 candidate lists). The
features we used in the normalization reranking
model are shown in Table 1. This maxent rerank-
ing method has shown success in many previous
work such as (Charniak and Johnson, 2005; Ji et
al., 2006).
</bodyText>
<subsectionHeader confidence="0.406671">
Features:
</subsectionHeader>
<bodyText confidence="0.894011666666667">
1.Boolean value to indicate whether a candidate is on the
list of each system. There are 6 lists and thus 6 such fea-
tures.
</bodyText>
<footnote confidence="0.8402872">
2.A concatenation of the 6 boolean features above.
3.The position of this candidate in each candidate list. If
this candidate is not on a list, the value of this feature is -1
for that list.
4.The unigram language model probability of the candi-
date.
5.Boolean value to indicate whether the first character of
the candidate and non-standard word is the same.
6.Boolean value to indicate whether the last character of
the candidate and non-standard word is the same.
</footnote>
<tableCaption confidence="0.735771">
Table 1: Features for Reranking.
</tableCaption>
<table confidence="0.841677">
The first three features are related to the indi-
length(t)
* log(TermFreq(t))
* inner product(vec(w), vec(t))
longest common seq(w, t)
*
</table>
<page confidence="0.997256">
88
</page>
<bodyText confidence="0.9999918125">
vidual systems, and the last three features com-
pare the candidate with the non-standard word. It
is computationally expensive to include informa-
tion represented in the last three features in the in-
dividual systems since they need to consider more
candidates in the normalization step; whereas in
reranking, only a small set of word candidates
are evaluated, thus it is more feasible to use such
global features in the reranking model. We also
tried some other lexical features such as the length
difference of the non-standard word and the can-
didate, whether non-standard word contains num-
bers, etc. But they did not obtain performance
gain. Another advantage of the reranker is that we
can use information about multiple systems, such
as the first three features.
</bodyText>
<sectionHeader confidence="0.642864" genericHeader="method">
3.2.2 Sentence Level Reranking and
Decoding
</sectionHeader>
<bodyText confidence="0.999991341463415">
In the above reranking method, we only use infor-
mation about the individual words. When contex-
tual words are available (in sentences or Tweets),
we can use that information. If a sentence con-
taining OOV words is given during testing, we
can perform standard sentence level Viterbi decod-
ing to combine information from the normaliza-
tion candidates and language model scores.
Furthermore, if sentences are available during
training (not just isolated word pairs as used in all
the previous supervised individual systems and the
Maxent reranking above), we can also use contex-
tual information for training the reranker. This can
be achieved in two different ways. First, we add
the Language Model score from context words as
features in the reranker. In this work, in addition to
the features in Table 1, we add a trigram probabil-
ity to represent the context information. For every
candidate of a non-standard word, we use trigram
probability from the language model. The trigram
consists of this candidate, and the previous and the
following token of the non-standard word. If the
previous/following word is also a non-standard to-
ken, then we calculate the trigram using all of their
candidates and then take the average. After adding
the additional LM probability feature, the same
Maxent reranking method as above is used, which
optimizes the word level accuracy.
The second method is to change the training ob-
jective and perform the optimization at the sen-
tence level. The feature set can be the same as the
word level reranker, or with the additional contex-
tual LM score features. To train the model (feature
weights), we perform sentence level Viterbi de-
coding on the training set to find the best hypoth-
esis for each non-standard word. If the hypothe-
sis is incorrect, we update the feature weight us-
ing structured perceptron strategy (Collins, 2002).
We will explore these different feature and train-
ing configurations for reranking in the following
experiments.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99704">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998874">
The following data sets are used in our experi-
ments. We use Data 1 and Data 2 as test data, and
Data 3 as training data for all the supervised mod-
els.
</bodyText>
<listItem confidence="0.9959585">
• Data 1: 558 pairs of non-standard tokens and
standard words collected from 549 tweets in
2010 by (Han and Baldwin, 2011).
• Data 2: 3,962 pairs of non-standard tokens
and standard words collected from 6,160
tweets between 2009 and 2010 by (Liu et al.,
2011).
• Data 3: 2,333 unique pairs of non-standard
tokens and standard words, collected from
2,577 Twitter messages (selected from the
Edinburgh Twitter corpus) used in (Pennell
and Liu, 2011). We made some changes on
this data, removing the pairs that have more
than one proper words, and sentences that
only contain such pairs.3
• Data 4: About 10 million twitter messages
</listItem>
<bodyText confidence="0.8728130625">
selected from the the Edinburgh Twitter cor-
pus mentioned above, consisting of 3 million
unique tokens. This data is used by the un-
supervised method to create the mapping ta-
ble, and also for building the word-based lan-
guage model needed in sentence level nor-
malization.
The dictionary we used is obtained from
http://ciba.iciba.com/, which includes 75,262 En-
glish word entries and their corresponding pho-
netic symbols (IPA symbols). This is used in var-
ious modules in the normalization systems. The
number of the final standard words used to create
the look-up table is 10,105 because we only use
the words that have the same number of character-
block segments and phones. These 10,105 words
</bodyText>
<footnote confidence="0.925964">
3http://www.hlt.utdallas.edu/∼chenli/normalization
</footnote>
<page confidence="0.999754">
89
</page>
<bodyText confidence="0.999975928571429">
cover 90.77% and 93.74% standard words in Data
set 1 and Data set 2 respectively. For the non-
standard words created in the CRF model, they
cover 80.47% and 86.47% non-standard words in
Data set1 and Data set 2. This coverage using the
non-standard words identified by the new unsuper-
vised model is 91.99% and 92.32% for the two
data sets, higher than that by the CRF model.
During experiments, we use CRF++ toolkit 4
for our sequence labeling model, SRILM toolkit
(Stolcke, 2002) to build all the language models,
Giza++ (Och and Ney, 2003) for automatic word
alignment, and Moses (Koehn et al., 2007) for
translation decoding in three MT systems.
</bodyText>
<sectionHeader confidence="0.7435685" genericHeader="method">
4.2 Isolated Word Iormalization
Experiments .
</sectionHeader>
<bodyText confidence="0.999961192307692">
Table 2 shows the isolated word normalization re-
sults on the two test data sets for various systems.
The performance metrics include the accuracy for
the top-1 candidate and other top-N candidates.
Coverage means how many test cases correct an-
swers can be obtained in the final list regardless
of its positions. The top part presents the results
on Data Set 1 and the bottom shows the results on
Data Set 2. We can see that our proposed unsu-
pervised corpus similarity model achieves better
top-1 accuracy than the other individual systems
described in Section 2. Its top-n coverage is not
always the best – the 2-step MT method has advan-
tages in its coverage. The results in the table also
show that reranking improves system performance
over any of the used individual systems, which is
expected. After reranking, on Data set 1, our sys-
tem yields better performance than previously re-
ported ones. On Data set 2, it has better top-1 ac-
curacy than (Liu et al., 2012a), but slightly worse
top-N coverage. However, the method in (Liu et
al., 2012a) has higher computational cost because
of the calculation of the prime visual values for
each non-standard word on the fly during testing.
In addition, they also used more training data than
ours.
</bodyText>
<subsectionHeader confidence="0.999121">
4.3 Sentence Level Normalization Results
</subsectionHeader>
<bodyText confidence="0.999592">
We have already seen that after reranking we ob-
tain better word-level normalization performance,
for both top-1 and other top-N candidates. One
follow-up question is whether this improved per-
formance carries over to sentence level normaliza-
</bodyText>
<footnote confidence="0.966796">
4http://crfpp.googlecode.com/
</footnote>
<table confidence="0.999858826086956">
System Accuracy %
Top1 Top3 Top10 Top20 Cover
Data 1
MT 61.81 73.53 78.50 79.57 80.00
MT21 39.61 52.93 63.59 65.36 65.72
MT22 53.64 68.56 77.44 80.46 88.10
SL 53.29 61.99 69.09 71.92 75.85
SC 50.27 56.31 56.84 57.02 57.02
UCS 61.81 69.98 74.60 76.55 82.17
Rerank 77.14 86.96 93.04 94.82 95.90
Sys1 75.69 n/a n/a n/a n/a
Sys2 73 81.9 86.7 89.2 94.2
Data 2
MT 55.02 63.3 66.99 67.77 68.00
MT21 35.64 47.65 54.67 56.01 56.4
MT22 49.02 62.49 70.99 74.86 80.07
SL 46.52 55.05 61.21 62.97 66.21
SC 51.16 55.48 55.88 55.88 55.88
UCS 57.29 65.75 70.55 72.64 80.84
Rerank 74.44 84.57 90.25 92.37 93.5
Sys1 69.81 82.51 92.24 93.79 95.71
Sys2 62.6 75.1 84 87.5 90.7
Sys3 73.04 n/a n/a n/a n/a
</table>
<tableCaption confidence="0.851019">
Table 2: MT: Character-block Level MT;
MT21&amp;MT22: First&amp;Second step in Character-
</tableCaption>
<bodyText confidence="0.98307952173913">
level Two-step MT; SL: Sequence Labeling sys-
tem; SC: Spell Checker; UCS: Unsupervised Cor-
pus Similarity Model; Sys1 is from (Liu et al.,
2012a); Sys2 is from (Li and Liu, 2012a); Sys3
is from (Yang and Eisenstein, 2013).
tion when context information is used via the in-
corporation of a language model. Since detecting
which tokens need normalization in the first place
is a hard task itself in social media text and is an
open question currently, similar to some previous
work, we assume that we already know the non-
standard words that need to be normalized for a
given sentence. Then the sentence-level normal-
ization task is just to find which candidate from
the n-best lists for each of those already ‘detected’
non-standard words is the best one. We use the
tweets in the Data set 1 described above because
Data set 2 only has token pairs but not sentences.
Table 3 shows the sentence level normaliza-
tion results using different reranking configura-
tions with respect to the features used in the
reranker and the training process. Regarding fea-
tures, reranker 1 and 3 use the features described
</bodyText>
<page confidence="0.995153">
90
</page>
<bodyText confidence="0.99994592">
in Section 3.2.1, i.e., features based on the words
only, without the additional trigram LM probabil-
ity feature; reranker 2 and 4 use the additional LM
probability feature. About training, reranker 1 and
2 use the Maxent reranking that is trained and op-
timized for the word level; reranker 3 and 4 use
structure perceptron training at the sentence level.
Note that all of the systems perform Viterbi decod-
ing during testing to determine the final top one
candidate for each non-standard word in the sen-
tence. The scores from the reranked normalization
output and the LM probabilities are combined in
decoding. From the results, we can see that adding
contextual information (LM probabilities) as fea-
tures in the reranker is useful. When this feature
is not used, using sentence-level training objec-
tive benefits (reranker 3 outperforms 1); however,
when this feature is used, performing sentence-
level training via structure perceptron is not useful
(reranker 2 outperforms 4), partly because the con-
textual information is incorporated in the features
already and using it in sentence-level decoding for
training is redundant and does not bring additional
gain. Finally compared to the previously report
results, our system performs the best.
</bodyText>
<table confidence="0.99906575">
System Acc % System Acc %
Reranker1 84.30 Reranker2 86.91
Reranker3 85.03 Reranker4 85.37
Sys1 84.13 Sys2 82.23
</table>
<tableCaption confidence="0.8310945">
Table 3: Sentence level normalization results on
Data Set 1 using different reranking setups. Sys1
is from (Liu et al., 2012a); Sys2 is from (Yang and
Eisenstein, 2013). Acc % is the top one accuracy.
</tableCaption>
<subsectionHeader confidence="0.9997765">
4.4 Impact of Unsupervised Corpus
Similarity Model
</subsectionHeader>
<bodyText confidence="0.999905869565217">
Our last question is regarding unsupervised model
importance in the reranking system and contribu-
tions of its different similarity measure compo-
nents. We conduct the following two experiments:
First, we removed the new model and just use the
other remaining models in reranking (five candi-
date lists). Second, we kept this new model but
changed the corpus similarity measure (removed
the third item in Eq(1) that represents the seman-
tic similarity). This way we can evaluate the im-
pact of the semantic similarity measure based on
the continuous word vector representation.
Table 4 shows the word level and sentence re-
sults on Data set 1 and 2 using these different
setups. Because of space limit, we only present
the top one accuracy. The other top-n results
have similar patterns. Sentence level normaliza-
tion uses the Reranker 2 described above. We can
see that there is a degradation in both of the new
setups, suggesting that the unsupervised method
itself is beneficial, and in particular the word vec-
tor based semantic similarity component is crucial
to the system performance.
</bodyText>
<table confidence="0.998843">
System Word Level Sent Level
Data1 Data2 Data1
system-A 73.75 70.33 84.51
system-B 74.77 70.83 86.22
system-C 77.14 74.44 86.91
</table>
<tableCaption confidence="0.989614">
Table 4: Word level and Sentence level normaliza-
</tableCaption>
<bodyText confidence="0.869824">
tion results (top-1 accuracy in %) after reranking
on Data Set 1 and 2. System-A is without using
the unsupervised model, system-B is without its
semantic similarity measure, and system-C is our
proposed system.
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999979428571429">
In this paper, we proposed a novel normalization
system by using unsupervised methods in a large
corpus to identify non-standard words and their
corresponding proper words. We further combine
it with several previously developed normalization
systems by a reranking strategy. In addition, we
explored different sentence level reranking meth-
ods to evaluate the impact of context information.
Our experiments show that the reranking system
not only significantly improves the word level nor-
malization accuracy, but also helps the sentence
level decoding. In the future work, we plan to ex-
plore more useful features and also leverage pair-
wise and link reranking strategy.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999895">
We thank the NSF for travel and conference sup-
port for this paper. The work is also partially sup-
ported by DARPA Contract No. FA8750-13-2-
0041. Any opinions, findings, and conclusions or
recommendations expressed are those of the au-
thor and do not necessarily reflect the views of the
funding agencies.
</bodyText>
<page confidence="0.993218">
91
</page>
<figure confidence="0.519466125">
Chen Li and Yang Liu. 2012a. Improving text nor-
malization using character-blocks based models and
system combination. In Proceedings of COLING
2012.
References
Aiti Aw, Min Zhang, Juan Xiao, Jian Su, and Jian Su.
2006. A phrase-based statistical model for sms text
normalization. In Processing of COLING/ACL.
</figure>
<reference confidence="0.998999336842105">
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd ACL.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
ofEMNLP.
Danish Contractor, Tanveer A. Faruquie, L. Venkata
Subramaniam, and L. Venkata Subramaniam. 2010.
Unsupervised cleansing of noisy text. In Proceed-
ings of COLING.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings ofNAACL.
Fred J Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM, 7(3):171–176.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceeding of 49th ACL.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 EMNLP.
Hany Hassan and Arul Menezes. 2013. Social text
normalization using contextual graph random walks.
In Proceedings ofACL.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006.
Re-ranking algorithms for name tagging. In Pro-
ceedings of the Workshop on Computationally Hard
Problems and Joint Inference in Speech and Lan-
guage Processing.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, Evan Herbst, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings ofACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings ofACL.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Chen Li and Yang Liu. 2012b. Normalization of text
messages using character- and phone-based machine
translation approaches. In Proceedings of 13th In-
terspeech.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normal-
izing text messages without pre-categorization nor
supervision. In Proceedings of the 49th ACL: short
papers.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A
broad-coverage normalization system for social me-
dia language. In Proceedings of the 50th ACL.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012b. Joint
inference of named entity recognition and normal-
ization for tweets. In Proceedings ofACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
ICLR.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings ofNAACL.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
sms abbreviations. In Proceedings of 5th IJCNLP.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
ofNAACL.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings ofEMNLP.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech &amp; Language, 15(3):287–
333.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
</reference>
<page confidence="0.93296">
92
</page>
<reference confidence="0.998048454545454">
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings ofACL.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings ofEMNLP.
</reference>
<page confidence="0.999165">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981847">
<title confidence="0.996868">Improving Text Normalization via Unsupervised Model Discriminative Reranking</title>
<author confidence="0.990937">Li</author>
<affiliation confidence="0.9990405">The University of Texas at Computer Science</affiliation>
<email confidence="0.998738">chenli,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.999941260869565">Various models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation, together with other surface similarity measurement. Second we propose a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both wordand sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="11669" citStr="Charniak and Johnson, 2005" startWordPosition="1844" endWordPosition="1847">quence labeling models have better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Section 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in Table 1. This maxent reranking method has shown success in many previous work such as (Charniak and Johnson, 2005; Ji et al., 2006). Features: 1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such features. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candidate. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<publisher>IJDAR.</publisher>
<contexts>
<context position="2788" citStr="Choudhury et al. (2007)" startWordPosition="435" endWordPosition="438">. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP(S|A) = argmaxP(A|S) ∗ P(S). Different methods have been used to compute P(A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign langua</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. IJDAR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="15121" citStr="Collins, 2002" startWordPosition="2418" endWordPosition="2419">ditional LM probability feature, the same Maxent reranking method as above is used, which optimizes the word level accuracy. The second method is to change the training objective and perform the optimization at the sentence level. The feature set can be the same as the word level reranker, or with the additional contextual LM score features. To train the model (feature weights), we perform sentence level Viterbi decoding on the training set to find the best hypothesis for each non-standard word. If the hypothesis is incorrect, we update the feature weight using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). • Data 2: 3,962 pairs of non-standard tokens and standard words collected from 6,160 tweets between 2009 and 2010 by (Liu et al., 2011). • Data 3: 2,333 unique pairs of non-s</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danish Contractor</author>
<author>Tanveer A Faruquie</author>
<author>L Venkata Subramaniam</author>
<author>L Venkata Subramaniam</author>
</authors>
<title>Unsupervised cleansing of noisy text.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="3436" citStr="Contractor et al. (2010)" startWordPosition="541" endWordPosition="544">del to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible cor86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al</context>
</contexts>
<marker>Contractor, Faruquie, Subramaniam, Subramaniam, 2010</marker>
<rawString>Danish Contractor, Tanveer A. Faruquie, L. Venkata Subramaniam, and L. Venkata Subramaniam. 2010. Unsupervised cleansing of noisy text. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="3006" citStr="Cook and Stevenson (2009)" startWordPosition="471" endWordPosition="474"> Different methods have been used to compute P(A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at </context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="2015" citStr="Damerau, 1964" startWordPosition="310" endWordPosition="311"> such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbreviations or non-standard tokens. A simple approach for normalization would be applying traditional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP(S|A) = argmaxP(A|S) ∗ P(S). Different methods have been used to compute P(A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred J Damerau. 1964. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7(3):171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceeding of 49th ACL.</booktitle>
<contexts>
<context position="15545" citStr="Han and Baldwin, 2011" startWordPosition="2491" endWordPosition="2494">decoding on the training set to find the best hypothesis for each non-standard word. If the hypothesis is incorrect, we update the feature weight using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). • Data 2: 3,962 pairs of non-standard tokens and standard words collected from 6,160 tweets between 2009 and 2010 by (Liu et al., 2011). • Data 3: 2,333 unique pairs of non-standard tokens and standard words, collected from 2,577 Twitter messages (selected from the Edinburgh Twitter corpus) used in (Pennell and Liu, 2011). We made some changes on this data, removing the pairs that have more than one proper words, and sentences that only contain such pairs.3 • Data 4: About 10 million twitter messages selected from the the Edinburgh Twitter corpus mentioned above, consisting of 3 million uniq</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceeding of 49th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 EMNLP.</booktitle>
<contexts>
<context position="4044" citStr="Han et al. (2012)" startWordPosition="646" endWordPosition="649">l. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible cor86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2 Previous Normalization Methods Used in Reranking In </context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Arul Menezes</author>
</authors>
<title>Social text normalization using contextual graph random walks.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4319" citStr="Hassan and Menezes (2013)" startWordPosition="685" endWordPosition="688">f the ACL 2014 Student Research Workshop, pages 86–93, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2 Previous Normalization Methods Used in Reranking In this work we adopt several normalization methods developed in previous studies. The following briefly describes these previous approaches. Next section will introduce our proposed methods using unsupervised learning and discriminative reranking for system combination. 2.1 Ch</context>
</contexts>
<marker>Hassan, Menezes, 2013</marker>
<rawString>Hany Hassan and Arul Menezes. 2013. Social text normalization using contextual graph random walks. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Cynthia Rudin</author>
<author>Ralph Grishman</author>
</authors>
<title>Re-ranking algorithms for name tagging.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing.</booktitle>
<contexts>
<context position="11687" citStr="Ji et al., 2006" startWordPosition="1848" endWordPosition="1851">better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Section 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in Table 1. This maxent reranking method has shown success in many previous work such as (Charniak and Johnson, 2005; Ji et al., 2006). Features: 1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such features. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candidate. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard word is the same. </context>
</contexts>
<marker>Ji, Rudin, Grishman, 2006</marker>
<rawString>Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006. Re-ranking algorithms for name tagging. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="17398" citStr="Koehn et al., 2007" startWordPosition="2798" endWordPosition="2801">/normalization 89 cover 90.77% and 93.74% standard words in Data set 1 and Data set 2 respectively. For the nonstandard words created in the CRF model, they cover 80.47% and 86.47% non-standard words in Data set1 and Data set 2. This coverage using the non-standard words identified by the new unsupervised model is 91.99% and 92.32% for the two data sets, higher than that by the CRF model. During experiments, we use CRF++ toolkit 4 for our sequence labeling model, SRILM toolkit (Stolcke, 2002) to build all the language models, Giza++ (Och and Ney, 2003) for automatic word alignment, and Moses (Koehn et al., 2007) for translation decoding in three MT systems. 4.2 Isolated Word Iormalization Experiments . Table 2 shows the isolated word normalization results on the two test data sets for various systems. The performance metrics include the accuracy for the top-1 candidate and other top-N candidates. Coverage means how many test cases correct answers can be obtained in the final list regardless of its positions. The top part presents the results on Data Set 1 and the bottom shows the results on Data Set 2. We can see that our proposed unsupervised corpus similarity model achieves better top-1 accuracy th</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8286" citStr="Koo et al., 2008" startWordPosition="1285" endWordPosition="1288">rvised methods that need labeled data consisting of pairs of non-standard words and proper words. In this paper we propose an unsupervised method to create the lookup table of the nonstandard words and their corresponding proper words offline. We further propose to use different discriminative reranking approaches to combine multiple individual systems. 1http://jazzy.sourceforge.net 87 3.1 Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We p</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<booktitle>In Soviet physics doklady,</booktitle>
<volume>10</volume>
<pages>707</pages>
<contexts>
<context position="2035" citStr="Levenshtein, 1966" startWordPosition="312" endWordPosition="313">meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbreviations or non-standard tokens. A simple approach for normalization would be applying traditional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP(S|A) = argmaxP(A|S) ∗ P(S). Different methods have been used to compute P(A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without expli</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet physics doklady, volume 10, page 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of text messages using character- and phone-based machine translation approaches.</title>
<date>2012</date>
<booktitle>In Proceedings of 13th Interspeech.</booktitle>
<contexts>
<context position="5526" citStr="Li and Liu, 2012" startWordPosition="861" endWordPosition="864">n. 2.1 Character-block level MT Pennell and Liu (2011) proposed to use a character-level MT model for text normalization. The idea is similar to traditional translation, except that the translation unit is characters, not words. Formally, for a non-standard word A = a1a2...an , the MT method finds the most likely standard word S = s1s2...sm (az and sz are the characters in the words): S = argmaxP(SIA) = argmaxP(A|S)P(S) = argmaxP(a1a2...an|s1s2...sm)P(s1s2...sm) where P(a1a2...an|s1s2...sm) is from a characterlevel translation model, and P(s1s2...sm) is from a character-level language model. (Li and Liu, 2012a) modified this approach to perform the translation at the character-block level in order to generate better alignment between characters (analogous to the word vs. phrase based alignment in traditional MT). This system generates one ranked list of word candidates. 2.2 Character-level Two-step MT Li and Liu (2012b) extended the character-level MT model by incorporating the pronunciation information. They first translate non-standard words to possible pronunciations, which are then translated to standard words in the second step. This method has been shown to yield high coverage (high accuracy</context>
<context position="7204" citStr="Li and Liu, 2012" startWordPosition="1119" endWordPosition="1122">-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard words. A reverse look-up table is used to store the corresponding possible standard words for the non-standard words for reverse lookup during testing. Liu et al. (2011) extended the above model to handle other types of non-standard words. (Li and Liu, 2012a) used character-blocks (same ones as that in the character-block MT method above) as the units in this sequence labeling framework. There is one list of word candidates from this method. 2.4 Spell Checker The forth normalization subsystem is the Jazzy Spell Checker1, which is based on edit distance and integrates a phonetic matching algorithm as well. This provides one list of hypotheses. 3 Proposed Method All the above models except the Spell Checker are supervised methods that need labeled data consisting of pairs of non-standard words and proper words. In this paper we propose an unsuperv</context>
<context position="19981" citStr="Li and Liu, 2012" startWordPosition="3235" endWordPosition="3238">ys2 73 81.9 86.7 89.2 94.2 Data 2 MT 55.02 63.3 66.99 67.77 68.00 MT21 35.64 47.65 54.67 56.01 56.4 MT22 49.02 62.49 70.99 74.86 80.07 SL 46.52 55.05 61.21 62.97 66.21 SC 51.16 55.48 55.88 55.88 55.88 UCS 57.29 65.75 70.55 72.64 80.84 Rerank 74.44 84.57 90.25 92.37 93.5 Sys1 69.81 82.51 92.24 93.79 95.71 Sys2 62.6 75.1 84 87.5 90.7 Sys3 73.04 n/a n/a n/a n/a Table 2: MT: Character-block Level MT; MT21&amp;MT22: First&amp;Second step in Characterlevel Two-step MT; SL: Sequence Labeling system; SC: Spell Checker; UCS: Unsupervised Corpus Similarity Model; Sys1 is from (Liu et al., 2012a); Sys2 is from (Li and Liu, 2012a); Sys3 is from (Yang and Eisenstein, 2013). tion when context information is used via the incorporation of a language model. Since detecting which tokens need normalization in the first place is a hard task itself in social media text and is an open question currently, similar to some previous work, we assume that we already know the nonstandard words that need to be normalized for a given sentence. Then the sentence-level normalization task is just to find which candidate from the n-best lists for each of those already ‘detected’ non-standard words is the best one. We use the tweets in the </context>
</contexts>
<marker>Li, Liu, 2012</marker>
<rawString>Chen Li and Yang Liu. 2012b. Normalization of text messages using character- and phone-based machine translation approaches. In Proceedings of 13th Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL: short</booktitle>
<pages>papers.</pages>
<contexts>
<context position="2546" citStr="Liu et al. (2011)" startWordPosition="399" endWordPosition="402">aditional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP(S|A) = argmaxP(A|S) ∗ P(S). Different methods have been used to compute P(A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Ma</context>
<context position="7116" citStr="Liu et al. (2011)" startWordPosition="1104" endWordPosition="1107">equence labeling model (CRF) for normalizing deletion-based abbreviation at the character-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard words. A reverse look-up table is used to store the corresponding possible standard words for the non-standard words for reverse lookup during testing. Liu et al. (2011) extended the above model to handle other types of non-standard words. (Li and Liu, 2012a) used character-blocks (same ones as that in the character-block MT method above) as the units in this sequence labeling framework. There is one list of word candidates from this method. 2.4 Spell Checker The forth normalization subsystem is the Jazzy Spell Checker1, which is based on edit distance and integrates a phonetic matching algorithm as well. This provides one list of hypotheses. 3 Proposed Method All the above models except the Spell Checker are supervised methods that need labeled data consisti</context>
<context position="15682" citStr="Liu et al., 2011" startWordPosition="2515" endWordPosition="2518">ght using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). • Data 2: 3,962 pairs of non-standard tokens and standard words collected from 6,160 tweets between 2009 and 2010 by (Liu et al., 2011). • Data 3: 2,333 unique pairs of non-standard tokens and standard words, collected from 2,577 Twitter messages (selected from the Edinburgh Twitter corpus) used in (Pennell and Liu, 2011). We made some changes on this data, removing the pairs that have more than one proper words, and sentences that only contain such pairs.3 • Data 4: About 10 million twitter messages selected from the the Edinburgh Twitter corpus mentioned above, consisting of 3 million unique tokens. This data is used by the unsupervised method to create the mapping table, and also for building the word-based language model </context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision. In Proceedings of the 49th ACL: short papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broad-coverage normalization system for social media language.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th ACL.</booktitle>
<contexts>
<context position="1288" citStr="Liu et al., 2012" startWordPosition="198" endWordPosition="201">ts from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbreviations or non-standard tokens. A simple approa</context>
<context position="10011" citStr="Liu et al., 2012" startWordPosition="1571" endWordPosition="1574">airs: (I) OOV words need to begin with the same letter as the dictionary standard word; (II) OOV words can only consist of English letter and digits. In addition to considering the above semantic similarity, for the normalization task, we use other information including the surface character level similarity based on longest common sequence between the two tokens, and the frequency of the token. The final score between a dictionary word w and an OOV word t is: longest common string(w, t) sim(w, t) = (1) length(t) The first and second term share the same property of visual prime value used in (Liu et al., 2012a). 2https://code.google.com/p/word2vec/ The third term is the vector-based semantic similarity of the two words, calculated by our proposed model. The last term is the length of longest common sequence between the two words divided by the length of the OOV word. Using this method, we can identify all the possible OOV words for each dictionary word based on an unlabeled large corpus. Each pair has a similarity score. Then a reverse lookup table is created to store the corresponding possible standard words for each non-standard word, which is used during testing. This framework is similar to th</context>
<context position="18455" citStr="Liu et al., 2012" startWordPosition="2980" endWordPosition="2983"> Data Set 1 and the bottom shows the results on Data Set 2. We can see that our proposed unsupervised corpus similarity model achieves better top-1 accuracy than the other individual systems described in Section 2. Its top-n coverage is not always the best – the 2-step MT method has advantages in its coverage. The results in the table also show that reranking improves system performance over any of the used individual systems, which is expected. After reranking, on Data set 1, our system yields better performance than previously reported ones. On Data set 2, it has better top-1 accuracy than (Liu et al., 2012a), but slightly worse top-N coverage. However, the method in (Liu et al., 2012a) has higher computational cost because of the calculation of the prime visual values for each non-standard word on the fly during testing. In addition, they also used more training data than ours. 4.3 Sentence Level Normalization Results We have already seen that after reranking we obtain better word-level normalization performance, for both top-1 and other top-N candidates. One follow-up question is whether this improved performance carries over to sentence level normaliza4http://crfpp.googlecode.com/ System Accu</context>
<context position="19947" citStr="Liu et al., 2012" startWordPosition="3228" endWordPosition="3231">95.90 Sys1 75.69 n/a n/a n/a n/a Sys2 73 81.9 86.7 89.2 94.2 Data 2 MT 55.02 63.3 66.99 67.77 68.00 MT21 35.64 47.65 54.67 56.01 56.4 MT22 49.02 62.49 70.99 74.86 80.07 SL 46.52 55.05 61.21 62.97 66.21 SC 51.16 55.48 55.88 55.88 55.88 UCS 57.29 65.75 70.55 72.64 80.84 Rerank 74.44 84.57 90.25 92.37 93.5 Sys1 69.81 82.51 92.24 93.79 95.71 Sys2 62.6 75.1 84 87.5 90.7 Sys3 73.04 n/a n/a n/a n/a Table 2: MT: Character-block Level MT; MT21&amp;MT22: First&amp;Second step in Characterlevel Two-step MT; SL: Sequence Labeling system; SC: Spell Checker; UCS: Unsupervised Corpus Similarity Model; Sys1 is from (Liu et al., 2012a); Sys2 is from (Li and Liu, 2012a); Sys3 is from (Yang and Eisenstein, 2013). tion when context information is used via the incorporation of a language model. Since detecting which tokens need normalization in the first place is a hard task itself in social media text and is an open question currently, similar to some previous work, we assume that we already know the nonstandard words that need to be normalized for a given sentence. Then the sentence-level normalization task is just to find which candidate from the n-best lists for each of those already ‘detected’ non-standard words is the b</context>
<context position="22384" citStr="Liu et al., 2012" startWordPosition="3631" endWordPosition="3634">ture is used, performing sentencelevel training via structure perceptron is not useful (reranker 2 outperforms 4), partly because the contextual information is incorporated in the features already and using it in sentence-level decoding for training is redundant and does not bring additional gain. Finally compared to the previously report results, our system performs the best. System Acc % System Acc % Reranker1 84.30 Reranker2 86.91 Reranker3 85.03 Reranker4 85.37 Sys1 84.13 Sys2 82.23 Table 3: Sentence level normalization results on Data Set 1 using different reranking setups. Sys1 is from (Liu et al., 2012a); Sys2 is from (Yang and Eisenstein, 2013). Acc % is the top one accuracy. 4.4 Impact of Unsupervised Corpus Similarity Model Our last question is regarding unsupervised model importance in the reranking system and contributions of its different similarity measure components. We conduct the following two experiments: First, we removed the new model and just use the other remaining models in reranking (five candidate lists). Second, we kept this new model but changed the corpus similarity measure (removed the third item in Eq(1) that represents the semantic similarity). This way we can evalua</context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A broad-coverage normalization system for social media language. In Proceedings of the 50th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Xiangyang Zhou</author>
<author>Zhongyang Fu</author>
<author>Furu Wei</author>
</authors>
<title>Joint inference of named entity recognition and normalization for tweets.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1288" citStr="Liu et al., 2012" startWordPosition="198" endWordPosition="201">ts from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbreviations or non-standard tokens. A simple approa</context>
<context position="10011" citStr="Liu et al., 2012" startWordPosition="1571" endWordPosition="1574">airs: (I) OOV words need to begin with the same letter as the dictionary standard word; (II) OOV words can only consist of English letter and digits. In addition to considering the above semantic similarity, for the normalization task, we use other information including the surface character level similarity based on longest common sequence between the two tokens, and the frequency of the token. The final score between a dictionary word w and an OOV word t is: longest common string(w, t) sim(w, t) = (1) length(t) The first and second term share the same property of visual prime value used in (Liu et al., 2012a). 2https://code.google.com/p/word2vec/ The third term is the vector-based semantic similarity of the two words, calculated by our proposed model. The last term is the length of longest common sequence between the two words divided by the length of the OOV word. Using this method, we can identify all the possible OOV words for each dictionary word based on an unlabeled large corpus. Each pair has a similarity score. Then a reverse lookup table is created to store the corresponding possible standard words for each non-standard word, which is used during testing. This framework is similar to th</context>
<context position="18455" citStr="Liu et al., 2012" startWordPosition="2980" endWordPosition="2983"> Data Set 1 and the bottom shows the results on Data Set 2. We can see that our proposed unsupervised corpus similarity model achieves better top-1 accuracy than the other individual systems described in Section 2. Its top-n coverage is not always the best – the 2-step MT method has advantages in its coverage. The results in the table also show that reranking improves system performance over any of the used individual systems, which is expected. After reranking, on Data set 1, our system yields better performance than previously reported ones. On Data set 2, it has better top-1 accuracy than (Liu et al., 2012a), but slightly worse top-N coverage. However, the method in (Liu et al., 2012a) has higher computational cost because of the calculation of the prime visual values for each non-standard word on the fly during testing. In addition, they also used more training data than ours. 4.3 Sentence Level Normalization Results We have already seen that after reranking we obtain better word-level normalization performance, for both top-1 and other top-N candidates. One follow-up question is whether this improved performance carries over to sentence level normaliza4http://crfpp.googlecode.com/ System Accu</context>
<context position="19947" citStr="Liu et al., 2012" startWordPosition="3228" endWordPosition="3231">95.90 Sys1 75.69 n/a n/a n/a n/a Sys2 73 81.9 86.7 89.2 94.2 Data 2 MT 55.02 63.3 66.99 67.77 68.00 MT21 35.64 47.65 54.67 56.01 56.4 MT22 49.02 62.49 70.99 74.86 80.07 SL 46.52 55.05 61.21 62.97 66.21 SC 51.16 55.48 55.88 55.88 55.88 UCS 57.29 65.75 70.55 72.64 80.84 Rerank 74.44 84.57 90.25 92.37 93.5 Sys1 69.81 82.51 92.24 93.79 95.71 Sys2 62.6 75.1 84 87.5 90.7 Sys3 73.04 n/a n/a n/a n/a Table 2: MT: Character-block Level MT; MT21&amp;MT22: First&amp;Second step in Characterlevel Two-step MT; SL: Sequence Labeling system; SC: Spell Checker; UCS: Unsupervised Corpus Similarity Model; Sys1 is from (Liu et al., 2012a); Sys2 is from (Li and Liu, 2012a); Sys3 is from (Yang and Eisenstein, 2013). tion when context information is used via the incorporation of a language model. Since detecting which tokens need normalization in the first place is a hard task itself in social media text and is an open question currently, similar to some previous work, we assume that we already know the nonstandard words that need to be normalized for a given sentence. Then the sentence-level normalization task is just to find which candidate from the n-best lists for each of those already ‘detected’ non-standard words is the b</context>
<context position="22384" citStr="Liu et al., 2012" startWordPosition="3631" endWordPosition="3634">ture is used, performing sentencelevel training via structure perceptron is not useful (reranker 2 outperforms 4), partly because the contextual information is incorporated in the features already and using it in sentence-level decoding for training is redundant and does not bring additional gain. Finally compared to the previously report results, our system performs the best. System Acc % System Acc % Reranker1 84.30 Reranker2 86.91 Reranker3 85.03 Reranker4 85.37 Sys1 84.13 Sys2 82.23 Table 3: Sentence level normalization results on Data Set 1 using different reranking setups. Sys1 is from (Liu et al., 2012a); Sys2 is from (Yang and Eisenstein, 2013). Acc % is the top one accuracy. 4.4 Impact of Unsupervised Corpus Similarity Model Our last question is regarding unsupervised model importance in the reranking system and contributions of its different similarity measure components. We conduct the following two experiments: First, we removed the new model and just use the other remaining models in reranking (five candidate lists). Second, we kept this new model but changed the corpus similarity measure (removed the third item in Eq(1) that represents the semantic similarity). This way we can evalua</context>
</contexts>
<marker>Liu, Zhou, Zhou, Fu, Wei, 2012</marker>
<rawString>Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang Fu, and Furu Wei. 2012b. Joint inference of named entity recognition and normalization for tweets. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="9280" citStr="Mikolov et al., 2013" startWordPosition="1444" endWordPosition="1447"> words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsupervised method based on the large corpus to induce dense realvalued low-dimension word embedding and then use the inner product as a measure of semantic similarity. We use the continuous bag-of-words model that is similar to the feedforward neural network language model to compute vector representations of words. This model was first introduced by (Mikolov et al., 2013). We use the tool word2vec2 to implement this model. Two constraints are used in order to eliminate unlikely word pairs: (I) OOV words need to begin with the same letter as the dictionary standard word; (II) OOV words can only consist of English letter and digits. In addition to considering the above semantic similarity, for the normalization task, we use other information including the surface character level similarity based on longest common sequence between the two tokens, and the frequency of the token. The final score between a dictionary word w and an OOV word t is: longest common strin</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17337" citStr="Och and Ney, 2003" startWordPosition="2788" endWordPosition="2791">nes. These 10,105 words 3http://www.hlt.utdallas.edu/∼chenli/normalization 89 cover 90.77% and 93.74% standard words in Data set 1 and Data set 2 respectively. For the nonstandard words created in the CRF model, they cover 80.47% and 86.47% non-standard words in Data set1 and Data set 2. This coverage using the non-standard words identified by the new unsupervised model is 91.99% and 92.32% for the two data sets, higher than that by the CRF model. During experiments, we use CRF++ toolkit 4 for our sequence labeling model, SRILM toolkit (Stolcke, 2002) to build all the language models, Giza++ (Och and Ney, 2003) for automatic word alignment, and Moses (Koehn et al., 2007) for translation decoding in three MT systems. 4.2 Isolated Word Iormalization Experiments . Table 2 shows the isolated word normalization results on the two test data sets for various systems. The performance metrics include the accuracy for the top-1 candidate and other top-N candidates. Coverage means how many test cases correct answers can be obtained in the final list regardless of its positions. The top part presents the results on Data Set 1 and the bottom shows the results on Data Set 2. We can see that our proposed unsupervi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of text messages for text-to-speech.</title>
<date>2010</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="2456" citStr="Pennell and Liu (2010)" startWordPosition="384" endWordPosition="387"> abbreviations or non-standard tokens. A simple approach for normalization would be applying traditional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP(S|A) = argmaxP(A|S) ∗ P(S). Different methods have been used to compute P(A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems i</context>
<context position="6492" citStr="Pennell and Liu (2010)" startWordPosition="1008" endWordPosition="1011">el MT model by incorporating the pronunciation information. They first translate non-standard words to possible pronunciations, which are then translated to standard words in the second step. This method has been shown to yield high coverage (high accuracy in its n-best hypotheses). There are two candidate lists generated by this two-step MT method. The first one is based on the pronunciation list produced in the first step (some phonetic sequences directly correspond to standard words). The second list is generated from the second translation step. 2.3 Character-Block level Sequence Labeling Pennell and Liu (2010) used sequence labeling model (CRF) for normalizing deletion-based abbreviation at the character-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard words. A reverse look-up table is used to store the corresponding possible standard words for the non-standard words for reverse lookup during te</context>
</contexts>
<marker>Pennell, Liu, 2010</marker>
<rawString>Deana Pennell and Yang Liu. 2010. Normalization of text messages for text-to-speech. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A character-level machine translation approach for normalization of sms abbreviations.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th IJCNLP.</booktitle>
<contexts>
<context position="3570" citStr="Pennell and Liu (2011)" startWordPosition="567" endWordPosition="570"> results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible cor86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which ar</context>
<context position="4964" citStr="Pennell and Liu (2011)" startWordPosition="777" endWordPosition="780">ased on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2 Previous Normalization Methods Used in Reranking In this work we adopt several normalization methods developed in previous studies. The following briefly describes these previous approaches. Next section will introduce our proposed methods using unsupervised learning and discriminative reranking for system combination. 2.1 Character-block level MT Pennell and Liu (2011) proposed to use a character-level MT model for text normalization. The idea is similar to traditional translation, except that the translation unit is characters, not words. Formally, for a non-standard word A = a1a2...an , the MT method finds the most likely standard word S = s1s2...sm (az and sz are the characters in the words): S = argmaxP(SIA) = argmaxP(A|S)P(S) = argmaxP(a1a2...an|s1s2...sm)P(s1s2...sm) where P(a1a2...an|s1s2...sm) is from a characterlevel translation model, and P(s1s2...sm) is from a character-level language model. (Li and Liu, 2012a) modified this approach to perform t</context>
<context position="15870" citStr="Pennell and Liu, 2011" startWordPosition="2544" endWordPosition="2547"> 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). • Data 2: 3,962 pairs of non-standard tokens and standard words collected from 6,160 tweets between 2009 and 2010 by (Liu et al., 2011). • Data 3: 2,333 unique pairs of non-standard tokens and standard words, collected from 2,577 Twitter messages (selected from the Edinburgh Twitter corpus) used in (Pennell and Liu, 2011). We made some changes on this data, removing the pairs that have more than one proper words, and sentences that only contain such pairs.3 • Data 4: About 10 million twitter messages selected from the the Edinburgh Twitter corpus mentioned above, consisting of 3 million unique tokens. This data is used by the unsupervised method to create the mapping table, and also for building the word-based language model needed in sentence level normalization. The dictionary we used is obtained from http://ciba.iciba.com/, which includes 75,262 English word entries and their corresponding phonetic symbols </context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of sms abbreviations. In Proceedings of 5th IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>The edinburgh twitter corpus.</title>
<date>2010</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="8543" citStr="Petrovic et al., 2010" startWordPosition="1324" endWordPosition="1328">her propose to use different discriminative reranking approaches to combine multiple individual systems. 1http://jazzy.sourceforge.net 87 3.1 Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsupervised method based on the large corpus to induce dense realvalued low-dimension word embedding and then use the inner product as a measure of semantic similarity. We use the continuous bag-of-words model that is similar to the feedfo</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. The edinburgh twitter corpus. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="1248" citStr="Ritter et al., 2011" startWordPosition="190" endWordPosition="193">e a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbreviations</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley F Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>333</pages>
<contexts>
<context position="1705" citStr="Sproat et al., 2001" startWordPosition="260" endWordPosition="263">t systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbreviations or non-standard tokens. A simple approach for normalization would be applying traditional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standa</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W. Black, Stanley F. Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech &amp; Language, 15(3):287– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="17276" citStr="Stolcke, 2002" startWordPosition="2779" endWordPosition="2780">t have the same number of characterblock segments and phones. These 10,105 words 3http://www.hlt.utdallas.edu/∼chenli/normalization 89 cover 90.77% and 93.74% standard words in Data set 1 and Data set 2 respectively. For the nonstandard words created in the CRF model, they cover 80.47% and 86.47% non-standard words in Data set1 and Data set 2. This coverage using the non-standard words identified by the new unsupervised model is 91.99% and 92.32% for the two data sets, higher than that by the CRF model. During experiments, we use CRF++ toolkit 4 for our sequence labeling model, SRILM toolkit (Stolcke, 2002) to build all the language models, Giza++ (Och and Ney, 2003) for automatic word alignment, and Moses (Koehn et al., 2007) for translation decoding in three MT systems. 4.2 Isolated Word Iormalization Experiments . Table 2 shows the isolated word normalization results on the two test data sets for various systems. The performance metrics include the accuracy for the top-1 candidate and other top-N candidates. Coverage means how many test cases correct answers can be obtained in the final list regardless of its positions. The top part presents the results on Data Set 1 and the bottom shows the </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8307" citStr="Turian et al., 2010" startWordPosition="1289" endWordPosition="1292">t need labeled data consisting of pairs of non-standard words and proper words. In this paper we propose an unsupervised method to create the lookup table of the nonstandard words and their corresponding proper words offline. We further propose to use different discriminative reranking approaches to combine multiple individual systems. 1http://jazzy.sourceforge.net 87 3.1 Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsu</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="4514" citStr="Yang and Eisenstein (2013)" startWordPosition="715" endWordPosition="718">elying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2 Previous Normalization Methods Used in Reranking In this work we adopt several normalization methods developed in previous studies. The following briefly describes these previous approaches. Next section will introduce our proposed methods using unsupervised learning and discriminative reranking for system combination. 2.1 Character-block level MT Pennell and Liu (2011) proposed to use a character-level MT model for text normalization. The idea is similar to traditional translation, except that the translation unit i</context>
<context position="20025" citStr="Yang and Eisenstein, 2013" startWordPosition="3242" endWordPosition="3245">MT 55.02 63.3 66.99 67.77 68.00 MT21 35.64 47.65 54.67 56.01 56.4 MT22 49.02 62.49 70.99 74.86 80.07 SL 46.52 55.05 61.21 62.97 66.21 SC 51.16 55.48 55.88 55.88 55.88 UCS 57.29 65.75 70.55 72.64 80.84 Rerank 74.44 84.57 90.25 92.37 93.5 Sys1 69.81 82.51 92.24 93.79 95.71 Sys2 62.6 75.1 84 87.5 90.7 Sys3 73.04 n/a n/a n/a n/a Table 2: MT: Character-block Level MT; MT21&amp;MT22: First&amp;Second step in Characterlevel Two-step MT; SL: Sequence Labeling system; SC: Spell Checker; UCS: Unsupervised Corpus Similarity Model; Sys1 is from (Liu et al., 2012a); Sys2 is from (Li and Liu, 2012a); Sys3 is from (Yang and Eisenstein, 2013). tion when context information is used via the incorporation of a language model. Since detecting which tokens need normalization in the first place is a hard task itself in social media text and is an open question currently, similar to some previous work, we assume that we already know the nonstandard words that need to be normalized for a given sentence. Then the sentence-level normalization task is just to find which candidate from the n-best lists for each of those already ‘detected’ non-standard words is the best one. We use the tweets in the Data set 1 described above because Data set </context>
<context position="22428" citStr="Yang and Eisenstein, 2013" startWordPosition="3638" endWordPosition="3641">evel training via structure perceptron is not useful (reranker 2 outperforms 4), partly because the contextual information is incorporated in the features already and using it in sentence-level decoding for training is redundant and does not bring additional gain. Finally compared to the previously report results, our system performs the best. System Acc % System Acc % Reranker1 84.30 Reranker2 86.91 Reranker3 85.03 Reranker4 85.37 Sys1 84.13 Sys2 82.23 Table 3: Sentence level normalization results on Data Set 1 using different reranking setups. Sys1 is from (Liu et al., 2012a); Sys2 is from (Yang and Eisenstein, 2013). Acc % is the top one accuracy. 4.4 Impact of Unsupervised Corpus Similarity Model Our last question is regarding unsupervised model importance in the reranking system and contributions of its different similarity measure components. We conduct the following two experiments: First, we removed the new model and just use the other remaining models in reranking (five candidate lists). Second, we kept this new model but changed the corpus similarity measure (removed the third item in Eq(1) that represents the semantic similarity). This way we can evaluate the impact of the semantic similarity mea</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In Proceedings ofEMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>