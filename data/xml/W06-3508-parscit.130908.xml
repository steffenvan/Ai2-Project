<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022065">
<title confidence="0.951672">
Searching for Grammar Right
</title>
<author confidence="0.936488">
Vanessa Micelli
</author>
<affiliation confidence="0.853438">
European Media Laboratory
</affiliation>
<address confidence="0.771157">
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.910046">
{firstname.lastname@eml-d.villa-bosch.de}
</email>
<sectionHeader confidence="0.993791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979230769231">
This paper describes our ongoing work in
and thoughts on developing a grammar
learning system based on a construction
grammar formalism. Necessary modules
are presented and first results and chal-
lenges in formalizing the grammar are
shown up. Furthermore, we point out the
major reasons why we chose construction
grammar as the most fitting formalism for
our purposes. Then our approach and
ideas of learning new linguistic phenom-
ena, ranging from holophrastic construc-
tions to compositional ones, is presented.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961692307693">
Since any particular language1 changes constantly
(Cf. Hopper and Traugott, 2003; Bybee, 1998) –
and even varies across domains, users, registers
etc. – scalable natural language understanding sys-
tems must be able to cope with language variation
and change. Moreover, due to the fact that any
natural language understanding system, which is
based on some formal representation of that lan-
guage’s grammar, will always only be able to rep-
resent a portion of what is going on in any
particular language at the present time, we need to
find systematic ways of endowing natural language
understanding systems with means of learning new
</bodyText>
<footnote confidence="0.954061333333333">
1 This claim also holds within any solidified system of con-
ventionalized form-meaning pairings, e.g. dialects, chro-
nolects, sociolects, idiolects, jargons, etc.
</footnote>
<page confidence="0.994596">
57
</page>
<bodyText confidence="0.999865694444444">
forms, new meanings and, ultimately, new form-
meaning pairings, i.e. constructions.
Constructions are the basic building blocks,
posited by a particular grammar framework called
Construction Grammar, and are defined as follows:
“C is a construction iffdef C is a form-meaning pair
&lt;Fi, Si&gt; such that some aspect of Fi or some aspect
of Si is not strictly predictable from C’s component
parts or from other previously established con-
structions.” (Goldberg, 1995:4).
Construction Grammar originated from earlier
insights in functional and usage-based models of
language mainly supposed by cognitive linguists
(e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay,
2002; Talmy, 1988; etc.). It has been devised to
handle actually occurring natural language, which
notoriously contains non-literal, elliptic, context-
dependent, metaphorical or underspecified linguis-
tic expressions. These phenomena still present a
challenge for today’s natural language understand-
ing systems. In addition to these advantages, we
adhere to principles proposed by other constructiv-
ists as e.g. Tomasello (2003) that language acquisi-
tion is a usage-based phenomenon, contrasting
approaches by generative grammarians who as-
sume an innate grammar (Chomsky, 1981). Fur-
thermore, we agree to the idea that grammatical
phenomena also contribute to the semantics of a
sentence which is the reason why syntax cannot be
defined independently of semantics of a grammar.
A more detailed outline of construction grammar
and the principles we adhered to in formalizing it
will be given in sections 2 and 3.
The input to the system is natural language data
as found on the web, as e.g. in news tickers or
blogs, initially restricted to the soccer domain. As
</bodyText>
<note confidence="0.845122">
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 57–64,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999585833333333">
the learning process develops the input will gradu-
ally be extended to other domains. A description of
the corpus and its selection process will be given in
section 4. Section 5 provides an outlook on the
learning paradigm, while the last section presents
some future issues and conclusions.
</bodyText>
<sectionHeader confidence="0.986013" genericHeader="method">
2 Grammar Formalism
</sectionHeader>
<bodyText confidence="0.9999608">
The most crucial foundation that is needed to build
a grammar learning system is a grammar formal-
ism. Therefore, we are designing a new formaliza-
tion of construction grammar called ECtoloG
(Porzel et al., 2006; Micelli et al., in press).
One existing formal computational model of
construction grammar is the Embodied Construc-
tion Grammar (ECG) (Chang et al., 2002; Bergen
and Chang, 2002), with its main focus being on
language understanding and later simulation2. A
congruent and parallel development has led to
FCG which simulates the emergence of language
(Steels, 2005). FCG is mainly based on the same
primitives and operators as ECG is. We decided to
employ ECG in our model mainly for historical
reasons (see details about its development in the
following section), adhering to its main primitives
and operators, but employing the state of the art in
knowledge representation. We adopt insights and
mechanisms of FCG where applicable.
</bodyText>
<subsectionHeader confidence="0.999864">
2.1 Construction Grammar and ECG
</subsectionHeader>
<bodyText confidence="0.999628875">
One main difference between West Coast Gram-
mar (Langacker, 1987; Lakoff, 1987) and East
Coast Grammar (Chomsky, 1965; Katz, 1972) is
the fact that construction grammar offers a vertical
– not a horizontal – organisation of any knowledge
concerning a language’s grammar. That is, that
generative grammars split form from function.
Syntax, morphology, a lexicon or other formal
components of the grammar constitute form, while
the conventional function is defined by semantics.
All constructions of a language, however, form
in Langacker’s terms “a structured inventory of
conventional linguistic units” (Langacker,
1987:54). This inventory is network-structured, i.e.
there are at least taxonomic links among the con-
structions (Diessel, 2004). This structure presents
</bodyText>
<footnote confidence="0.822542">
2 For a detailed ECG analysis of a declarative utterance, i.e.
the sentence Harry walked into the cafe, see Bergen and
Chang (2002).
</footnote>
<bodyText confidence="0.9998555">
one of the main differences between generative
and construction grammars (Croft, to appear). One
of the most cited examples that evidences the ne-
cessity, that there can be no explicit separation be-
tween syntax and semantics, is Goldberg’s
example sentence (Goldberg, 1995:29):
</bodyText>
<listItem confidence="0.381667">
(1) he sneezed the napkin off the table.
</listItem>
<bodyText confidence="0.99982244117647">
The whole meaning of this sentence cannot be
gathered from the meanings of the discrete words.
The direct object the napkin is not postulated by
the verb to sneeze. This intransitive verb would
have three arguments in a lexico-semantic theory:
‘X causes Y to move Z by sneezing’. Goldberg
states that the additional meaning of caused motion
which is added to the conventional meaning of the
verb sneeze is offered by the respective caused-
motion construction. Based on this background
ECG – a formal computational model of construc-
tion grammar – was developed within the Neural
Theory of Language project (NTL) and the EDU
project (EDU).
While other approaches consider language as
completely independent from the organism which
uses it, ECG claims that several characteristics of
the user’s sensorimotor system can influence his or
her language (Gallese and Lakoff, 2005). The
needed dynamic and inferential semantics in ECG
is represented by embodied schemas. These sche-
mas are known under the term of image schemas in
traditional cognitive semantics and constitute
schematic recurring patterns of sensorimotor ex-
perience (Johnson, 1987; Lakoff, 1987).
The current ASCII format of ECG is insufficient
for building scalable NLU systems in the long run.
Therefore, our attempt at formalizing construction
grammar results in an ontological model that com-
bines two ontological modeling frameworks en-
dowed with a construction grammar layer, based
on the main ideas behind ECG. The following sec-
tion describes the resulting ontology, pointing out
main challenges and advantages of that approach.
</bodyText>
<sectionHeader confidence="0.996125" genericHeader="method">
3 Formalizing Construction Grammar
</sectionHeader>
<bodyText confidence="0.9988752">
The ontological frameworks mentioned above are
Descriptions &amp; Situations (D&amp;S) (Gangemi and
Mika, 2003) and Ontology of Information Objects
(OIO) (Guarino, 2006), which both are extensions
of the Descriptive Ontology for Linguistic and
</bodyText>
<page confidence="0.996877">
58
</page>
<bodyText confidence="0.999762666666667">
Cognitive Engineering (DOLCE) (Masolo et al.,
2003).
D&amp;S is an ontology for representing a variety of
reified contexts and states of affairs. In contrast to
physical objects or events, the extensions of on-
tologies to the domain of non-physical objects pose
a challenge to the ontology engineer. The reason
for this lies in the fact that non-physical objects are
taken to have meaning only in combination with
some other ground entity. Accordingly, their logi-
cal representation is generally set at the level of
theories or models and not at the level of concepts
or relations (see Gangemi and Mika, 2003). It is,
therefore, important to keep in mind that the mean-
ing of a given linguistic expression emerges only
through the combination of both linguistic and
conceptual knowledge with “basic” ontological
knowledge, as modeled in such ground ontologies.
Next to the support via dedicated editors and in-
ference engines, one of the central advantages of
our ensuing ontological model over the currently
used ASCII-format of ECG lies in its compatibility
with other ground ontologies developed within the
Semantic Web framework.3
</bodyText>
<subsectionHeader confidence="0.999972">
3.1 Modeling of Constructions
</subsectionHeader>
<bodyText confidence="0.999963705882353">
Constructions are modeled in the ECtoloG as in-
formation-objects. According to the specification
of the OIO, information objects have – amongst
others – the following properties: They are social
objects realizable by some entity and they can ex-
press a description, which represents in this ontol-
ogy the ontological equivalent of a meaning or a
conceptualization. Since a construction constitutes
a pairing of form and meaning according to the
original theory of construction grammar, both
properties are of advantage for our ontological
model. To keep the construction’s original struc-
ture, the form pole can be modeled with the help of
the realized-by property4 while the meaning pole is
built via the edns:expresses property. Both proc-
esses are described more detailed in the following
section.
</bodyText>
<subsectionHeader confidence="0.974345">
Holophrastic Constructions
</subsectionHeader>
<bodyText confidence="0.998528">
The class of lexical constructions is modeled as a
subclass of referringConstruction. Since it is a
</bodyText>
<footnote confidence="0.665995666666667">
3 For more details see Porzel et al. (2006).
4 We adhere to the convention to present both ontological
properties, classes, and instances in italics.
</footnote>
<bodyText confidence="0.999978066666666">
subclass of the class information-object it inherits
the edns:expresses property. The referringCon-
struction class has a restriction on this property
that denotes, that at least one of the values of the
edns:expresses property is of type schema. Model-
ing this restriction is done by means of the built-in
owl:someValuesFrom constraint. The restriction
counts for all constructions that express a schema.
It has no effect on the whole class of constructions,
i.e. it is possible that there exist constructions that
do not express a single schema, as e.g. composi-
tional ones, whose meaning is a composite of all
constructions and schemas that constitute that
compositional construction.
The form pole of each construction is modeled
with the help of the realized-by property. This
property designates that a (physical) representation
– as e.g. the orthographic form of the construction
– realizes a non-physical object – in this case our
construction. This property is also inherited from
the class information-object, the superclass of con-
structions. What fills the range of that property is
the class of edns:physical-realization. Therefore,
we define an instance of inf:writing, which then
fills the form pole of the respective construction.
This instance has once more a relation which con-
nects it to instances of the class inf:word which
designate the realization of the instance of the
inf:writing class.
This way of modeling the form pole of each lexi-
cal construction enables us to automatically popu-
late our model with new instances of constructions,
as will be described more detailed in section 5.1.
Analogous to the modeling of meaning in the
original ECG, the meaning pole is ‘filled’ with an
instance of the class of image schema. This can be
done with the help of the edns:expresses relation.
This relation is defined, according to the specifica-
tion of the D&amp;S ontology, as a relation between
information objects that are used as representations
(signs) and their content, i.e. their meaning or con-
ceptualization. In this ontology, content is reified
as a description, which offered us the possibility to
model image schemas as such. How image sche-
mas are modeled will be described in section 3.2.
</bodyText>
<subsectionHeader confidence="0.967408">
Compositional Constructions
</subsectionHeader>
<bodyText confidence="0.999903">
Compositional constructions are constructions
which are on a higher level of abstraction than
holophrastic ones. This means, that there exist con-
structions which combine different constructions
</bodyText>
<page confidence="0.936196">
59
</page>
<bodyText confidence="0.982741095744681">
into one unit. ECG designed a so-called construc- 3.2 Modeling of Image Schemas
tional block, wherein several constructions are Following Johnson and Lakoff (Johnson, 1987;
subsumed under and accessible in one more com- Lakoff and Johnson, 1980; Lakoff, 1987) image
plex construction. schemas are schematic representations that capture
An example is the DetNoun construction, which recurrent patterns of sensorimotor experience. Ac-
combines a determiner and a noun to form one cording to ECG, a schema is a description whose
unit. There is the possibility to model different purpose is filling the meaning pole of a construc-
constraints both in the form pole and in the mean- tion. It consists of a list of schematic roles that can
ing pole of a construction. A form constraint ap- serve as simulation parameters.
plying to this exact construction is determining that In ECG, schemas can be evoked by or can evoke
the determiner comes before the noun. This under- other schemas, i.e. particular schematic-roles of
standing of before corresponds to Allen’s defini- another schema can be imported. A schema can,
tion of his interval relations (Allen, 1983), which therefore, be defined against the background of
states that they don’t necessarily have to follow another schema7. The property evokes and its in-
each other but that there could be some modifiers verse property evoked-by have been defined as
in between the two components of this construc- subproperties of the dol:generically-dependent-on
tion. property and its inverse property dol:generic-
A meaning constraint of this construction deter- dependent respectively. Generic dependence is
mines, that the meaning of the noun, used in this defined in the DOLCE ontology as the dependence
respective construction, is assigned to the meaning on an individual of a given type at some time.
of the resulting complex construction.5 To be able The class of image schemas is modeled as a sub-
to represent these phenomena, we firstly defined a class of edns:description (see definition of descrip-
class construction-parameter, that denotes a sub- tion in 3.1), in order to enable being employed in
class of edns:parameter, a subclass of the meaning pole of constructions.
edns:concept. There is a property restriction on the
class that states that all values of the requisite-for
property have to be of type construction. This de-
termines instances of the class construction-
parameter to be used only in constructions on a
higher level of abstraction. All constructions used
on level 0 of a grammar6, i.e. lexical constructions,
are at the same time instances of the class con-
struction-parameter so that they can be used in
more abstract constructions. The form and mean-
ing constraints still need to be modeled in our
framework. To determine which constructions are
used in which more abstract construction, new
properties are defined. These properties are sub-
properties of the requisite-for property. An exam-
ple is the requisite-detnoun-akk-sg property. This
property defines that the accusative singular de-
terminer construction and the corresponding noun
construction can be requisite-for the compositional
construction that combines these two lexical con-
structions into one noun phrase.
Schematic Roles
The class of schematic-roles is a subclass of the
edns:concept class. In the specification of D&amp;S a
concept is classified as a non-physical object
which again is defined by a description. Its func-
tion is classifying entities from a ground ontology
in order to build situations that can satisfy the de-
scription. Schematic roles are parameters that al-
low other schemas or constructions to refer to the
schema&apos;s key variable features, e.g. the role of a
trajector in a Trajector Landmark-Schema can be
played by the same entity that denotes the mover
in e.g. a caused-motion schema.
At the moment, they are modeled with the help
of the edns:defines property. A schema defines its
schematic roles with this property, denoting a sub-
property of the edns:component property. Accord-
ing to the D&amp;S specification, a component is a
proper part with a role or function in a system or a
context. It is also stated, that roles can be different
for the same entity, and the evaluation of them
changes according to the kind of entity. This
means, that instances of the class schema and its
5 For further information about which operators are used to
model these features in ECG we refer to Bergen and Chang
(2002), Chang et al. (2002) and Bryant (2004).
6 Following Bryant’s (2004) division of constructions into 5
levels of different degrees of schematicity.
7 To clarify this claim see Langacker’s hypotenuse example
(Langacker, 1987:183ff.).
60
subclasses can have instances of the class sche-
matic-role as their components. The schematic-
roles class has to fulfil the necessary condition,
that at least one of the values of the edns:defined-
by property is of type schema.
The domain of the defines property is a descrip-
tion (which can be our schemas) and its range is set
to either concepts or figures (which are our sche-
matic roles). The problem occurring hereby is that
the roles cannot be filled by complete classes
which is necessary in a lot of cases, since the pa-
rameters are not always filled with atomic values
but possibly with whole classes of entities. There-
fore, one could think about modeling schematic
roles as properties, setting the domain on the corre-
sponding schema class and the range on the corre-
sponding class whose subclasses and instances can
possibly fill its range.
</bodyText>
<subsectionHeader confidence="0.982429">
3.3 Linguistic Information
</subsectionHeader>
<bodyText confidence="0.999986673469388">
Since linguistic information as e.g. grammatical
gender, its case, or the part-of-speech of a word is
needed for analyzing natural language texts, this
information has to be modeled, as well, in the EC-
toloG. Therefore, we integrated the LingInfo
model (Buitelaar et al., 2006) into the ECtoloG.
LingInfo constitutes an ontological model that
provides other ontologies with linguistic informa-
tion for different languages, momentarily for Eng-
lish, French, and German. Main objective of this
ontology is to provide a mapping between onto-
logical concepts and lexical items. That is, that the
possibility is offered to assign linguistic informa-
tion as e.g. the orthographic term, its grammatical
gender, its part-of-speech, stem etc. to classes and
properties. For our purposes, the LingInfo ontology
had to be converted from RDFS into OWL-DL
format and then integrated into the ECtoloG. For
that reason, a new subclass of owl:class was de-
fined: ClassWithLingInfo. Instances of this meta-
class are linked through the linginfo property to
LingInfo classes. The LingInfo class is used to as-
sociate a term, a language, and morphosyntactic
information to classes from the ground ontology;
e.g. a class CafeConstruction, which is an instance
of ClassWithLingInfo, from an ontology proper,
can be associated through the property linginfo
with Café, an instance of the class LingInfo. Thus,
the information that the term is German, its part-
of-speech is noun and its grammatical gender neu-
ter is obtained.
Following this approach, our classes of lexical
constructions were defined as subclasses of
ClassWithLingInfo, being thereby provided with all
the necessary linguistic information as defined
above. The central challenge resulting from this
approach is, that through the definition of a meta-
class the ontological format is no longer OWL-DL
but goes to OWL-Full which thwarts the employ-
ment of Description Logic reasoners. Reasoning
will not stay computable and decidable. Future
work will address this challenge by means of inter-
twining the LingInfo model with the ECtoloG
grammar model in such a way, that the computa-
tional and inferential properties of OWL-DL re-
main unchallenged.
Another possibility could be obtaining linguistic
information for lexical items through an external
lexicon.
</bodyText>
<sectionHeader confidence="0.872186" genericHeader="method">
4 The Web as a Corpus
</sectionHeader>
<bodyText confidence="0.999830166666666">
The Seed Corpus C: The primary corpus C in this
work is the portion of the World Wide Web con-
fined to web pages containing natural language
texts on soccer. To extract natural language texts
out of web documents automatically we are using
wrapper agents that fulfil this job (see Porzel et al.,
2006). Our first goal is to build a grammar that can
deal with all occurring language phenomena – i.e.
both holophrastic and compositional ones – con-
tained in that corpus C.
Corpus C’: Next step is the development of a cor-
pus C’, where C’ = C + ε and ε is constituted by a
set of new documents. This new corpus is not de-
signed in an arbitrary manner. We search similar
pages, adding add them to our original corpus C, as
we expect the likelihood of still pretty good cover-
age together with some new constructions to be
maximal, thereby enabling our incremental learn-
ing approach. The question emerging hereby is:
what constitutes a similar web page? What, there-
fore, has to be explored are various similarity met-
rics, defining similarity in a concrete way and
evaluate the results against human annotations (see
Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.985179">
4.1 Similarity Metric
</subsectionHeader>
<bodyText confidence="0.996084333333333">
To be able to answer the question which texts are
actually similar, similarity needs to be defined pre-
cisely. Different approaches could be employed,
</bodyText>
<page confidence="0.998435">
61
</page>
<bodyText confidence="0.996675912280702">
i.e. regarding similarity in terms of syntactic or
semantic phenomena or a combination of both.
Since construction grammar makes no separation
between syntax and semantics, phenomena that
should be counted are both constructions and im-
age schemas. As for holophrastic constructions this
presents less of a challenge, we rather expect
counting compositional ones being a ‘tough
cookie’.
To detect image schemas in natural text auto-
matically, we seek to employ different methodolo-
gies, e.g. LSA (Kintsch and van Dijk, 1978), using
synonym sets (Fellbaum, 1998) or other ontolo-
gies, which could assist in discovering the seman-
tics of an unknown word with its corresponding
schematic roles and the appropriate fillers. This or
a similar methodology will be applied in the auto-
matic acquisition process as well.
Another important point is that some terms, or
some constructions, need to get a higher relevance
factor than others, which will highly depend on
context. Such a relevance factor can rank terms or
constructions according to their importance in the
respective text. Ranking functions that can be ex-
amined are, e.g., the TF/IDF function (e.g. Salton,
1989) or other so called bag of words approaches.
Term statistics in general is often used to deter-
mine a scalable measure of similarity between
documents so it is said to be a good measure for
topical closeness. Also part-of-speech statistics
could be partly helpful in defining similarity of
documents based on the ensuing type/token ratio.
The following five steps need to be executed in
determining the similarity of two documents:
Step 1: Processing of the document D; analyzing
the text and creating a list of all occurring words,
constructions and/or image schemas. We assume
that the best choice is counting constructions and
corresponding image schemas, since they represent
the semantics of the given text.
Step 2: Weighing of schemas and constructions
Step 3: Processing of the document D+1; execut-
ing of step 1 and 2 for this document.
Step 4: Comparing the documents; possibly adding
synonyms of sources as e.g. WordNet (Fellbaum,
1998).
Step 5: Calculating the documents’ similarity; de-
fining a threshold up to which documents are con-
sidered as being similar. If a document is said to be
similar, it is added to the corpus, which becomes
the new corpus C’.
Analysis of the New Corpus C’: The new corpus
C’ is analyzed, whereby the coverage results in
coverage A of C’ where:
A = 100% - (Sh + Sc)
Sh denotes all the holophrastic phenomena and Sc
all compositional phenomena not observed in C.
</bodyText>
<sectionHeader confidence="0.980594" genericHeader="method">
5 Grammar Learning
</sectionHeader>
<bodyText confidence="0.999873875">
To generate a grammar that covers this new corpus
C’ different strategies have to be applied for holo-
phrastic items Sh which are lexical constructions in
our approach and for compositional ones Sc –
meaning constructions on a higher level of abstrac-
tion as e.g. constructions that capture grammatical
phenomena such as noun phrases or even whole
sentences.
</bodyText>
<subsectionHeader confidence="0.998972">
5.1 Learning Lexical Constructions
</subsectionHeader>
<bodyText confidence="0.99998852">
Analogous to the fast mapping process (Carey,
1978) of learning new words based on exposure
without additional training or feedback on the cor-
rectness of its meaning, we are employing a
method of filling our ontology with whole para-
digms of new terms8, enabled through the model-
ing of constructions described in 3.1. First step
herein is employing a tool – Morphy (Lezius,
2002) – that enables morphological analysis and
synthesis. The analysis of a term yields informa-
tion about its stem, its part-of-speech, its case, its
number, and its grammatical gender. This informa-
tion can then easily be integrated automatically
into the ECtoloG.
As already mentioned in section 4.3, we are not
only trying to automatically acquire the form pole
of the constructions, but also its image schematic
meaning, that means the network of the schemas
that hierarchically form the meaning pole of such a
term, applying ontology learning mechanisms (e.g.
Loos, 2006) and methods similar to those de-
scribed in section 4.3. Additionally, investigations
are underway to connect the grammar learning
framework proposed herein to a computer vision
system that provides supplementary feedback con-
</bodyText>
<footnote confidence="0.7560194">
8 We are aware of the fact that fast mapping in humans is lim-
ited to color terms, shapes or texture terms, but are employing
the method on other kinds of terms, nevertheless, since the
grammar learning paradigm in our approach is still in its baby
shoes.
</footnote>
<page confidence="0.99883">
62
</page>
<bodyText confidence="0.9996285">
cerning the hypothesized semantics of individual
forms in the case of multi-media information.
</bodyText>
<subsectionHeader confidence="0.999771">
5.2 Learning Compositional Constructions
</subsectionHeader>
<bodyText confidence="0.999978153846154">
Learning of compositional constructions still pre-
sents an issue which has not been accounted for,
yet. What has already been proposed (Narayanan,
inter alia) is that we have to assume a strong induc-
tive bias and different learning algorithms, as e.g.
some form of Bayesian learning or model merging
(Stolcke, 1994) or reinforcement learning (Sutton
and Barto, 1998).
Another important step that has to be employed is
the (re)organization of the so-called constructicon,
i.e. our inventory of constructions and schemas.
These need to be merged, split or maybe thrown
out again, depending on their utility, similarity etc.
</bodyText>
<subsectionHeader confidence="0.963079">
5.3 Ambiguity
</subsectionHeader>
<bodyText confidence="0.999984">
Currently the problem of ambiguity is solved by
endowing the analyzer with a chart and employing
the semantic density algorithm described in (Bry-
ant, 2004). In the future probabilistic reasoning
frameworks as proposed by (Narayanan and Juraf-
sky, 2005) in combination with ontology-based
coherence measures as proposed by (Loos and
Porzel, 2004) constitute promising approaches for
handling problems of construal, whether it be on a
pragmatic, semantic, syntactic or phonological
level.
</bodyText>
<sectionHeader confidence="0.990112" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999765153846154">
In this paper we described our ongoing work in
and thoughts on developing a grammar learning
system based on a construction grammar formal-
ism used in a question-answering system. We de-
scribed necessary modules and presented first
results and challenges in formalizing construction
grammar. Furthermore, we pointed out our motiva-
tion for choosing construction grammar and the,
therefore, resulting advantages. Then our approach
and ideas of learning new linguistic phenomena,
ranging from holophrastic constructions to compo-
sitional ones, were presented. What should be kept
in mind is that our grammar model has to be
strongly adaptable to language phenomena, as e.g.
language variation and change, maps, metaphors,
or mental spaces.
Evaluations in the light of the precision/coverage
trade-off still present an enormous challenge (as
with all adaptive and learning systems). In the fu-
ture we will examine the feasibility of adapting
ontology evaluating frameworks, as e.g. proposed
by Porzel and Malaka (2005) for the task of gram-
mar learning. We hope that future evaluations will
show that our resulting system and, therefore, its
grammar will be robust and adaptable enough to be
worth being called ‘Grammar Right’.
</bodyText>
<sectionHeader confidence="0.998412" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992730605263158">
Allen, J.F. 1983. Maintaining Knowledge about Tempo-
ral Intervals. Communications of the ACM, 26(11).
Bergen, B. and Chang, N. 2002. Simulation-Based Lan-
guage Understanding in Embodied Construction
Grammar. ICSI TR-02-004, Berkeley, CA, USA.
Bryant, J. 2004. Recovering coherent interpretations
using semantic integration of partial parses. Proceed-
ings of the 3rd ROMAND workshop, Geneva, Switzer-
land.
Buitelaar, P., Declerck, T., Frank, A., Racioppa, S., Kie-
sel, M., Sintek, M., Engel, M., Romanelli, M.,
Sonntag, D., Loos, B., Micelli, V., Porzel, R. and
Cimiano, P. 2006. LingInfo: Design and Applications
of a Model for the Integration of Linguistic Informa-
tion in Ontologies. Proceedings of OntoLex 2006. In
Press. Genoa, Italy.
Bybee, J. 1998. A functionalist approach to grammar
and its evolution. Evolution of Communication 2(2).
Carey, S. 1978. The child as word-learner. Linguistic
theory and psychological reality. MIT Press, Cam-
bridge, MA.
Chang, N., Feldman, J., Porzel, R. and Sanders, K.
2002. Scaling Cognitive Linguistics: Formalisms for
Language Understanding. Proceedings of the 1st
ScaNaLU Workshop, Heidelberg, Germany.
Chomsky, N. 1965. Aspects of the theory of syntax. MIT
Press. Cambridge, Mass.
Croft, W. To appear. Logical and typological arguments
for Radical Construction Grammar. Construction
Grammar(s): Cognitive and cross-language dimen-
sions (Constructional approaches to Language, 1).
John Benjamins, Amsterdam.
Diessel, H. 2004. The Acquisition of Complex Sen-
tences. Cambridge Studies in Linguistics (105).
Cambridge University Press, Cambridge.
EDU: Even Deeper Understanding http://www.eml-
development.de/english/research/edu/index.php (last
access: 31/03/06).
</reference>
<page confidence="0.995124">
63
</page>
<reference confidence="0.999206440860215">
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, Mass.
Fillmore, C. and Kay, P. 1987. The goals of Construc-
tion Grammar. Berkeley Cognitive Science Program
TR 50. University of California, Berkeley.
Gallese, V. and Lakoff, G. 2005. The brain´s concepts:
the role of the sensory-motor system in conceptual
knowledge. Cognitive Neuropsychology 21/2005.
Gangemi A., Mika P. 2003. Understanding the Semantic
Web through Descriptions and Situations. Proceed-
ings of ODBASE03 Conference, Springer.
Goldberg, A. 1995. Constructions: A Construction
Grammar Approach to Argument Structure. Univer-
sity of Chicago Press. Chicago.
Guarino, N. 2006. Ontology Library. WonderWeb De-
liverable D202, I STC-CNR, Padova, Italy.
www.loa-cnr.it/Papers/Deliverable%202.pdf (last ac-
cess: 31/03/2006).
Hopper, P. and Traugott, E. 2003. Grammaticalization.
Cambridge University Press. Cambridge, UK.
Johnson, M. 1987. The Body in the Mind: The Bodily
Basis of Meaning, Imagination, and Reason. Univer-
sity of Chicago Press. Chicago.
Katz, J. J. 1972. Semantic theory. Harper &amp; Row. New
York.
Kay, P. 2002. An Informal Sketch of a Formal Architec-
ture for Construction Grammar. Grammars 1/5.
Kintsch, W. and van Dijk, T. 1978. Toward a model of
text comprehension and production. Psychological
Review, 85 (5).
Lakoff, G. and Johnson, M. 1980. Metaphors We Live
By. Chicago University Press. London.
Lakoff, G. 1987. Women, Fire, and Dangerous Things.
University of Chicago Press. Chicago and London.
Langacker, R. 1987. Foundations of Cognitive Gram-
mar, Vol. 1. University Press. Stanford.
Lezius, W. 2000. Morphy - German Morphology, Part-
of-Speech Tagging and Applications. Proceedings of
the 9th EURALEX International Congress, Stuttgart,
Germany.
Loos, B. 2006. Scaling natural language understanding
via user-driven ontology learning. This volume.
Loos, B. and Porzel, R. 2004. Resolution of Lexical
Ambiguities in Spoken Dialogue System. Proceed-
ings of the 5th SIGdial Workshop on Discourse and
Dialogue, Cambridge, Massachusetts, USA.
Masolo, C., Borgo, S., Gangemi, A., Guarino, N., and
Oltramari, A. 2003. Ontology Library. WonderWeb
Deliverable D18, I STC-CNR, Padova, Italy.
http://wonderweb.semanticweb.org/deliverables/docu
ments/D18.pdf.
Micelli, V., Porzel, R., and Gangemi, A. In Press. EC-
toloG: Construction Grammar meets the Semantic
Web. Proceedings of ICCG4, Tokyo, Japan.
Narayanan, S. and Jurafsky, D. 2005. A Bayesian Model
of Human Sentence Processing (in preparation).
Narayanan, S. 2006. Lecture Series given at Interdisci-
plinary College, Guenne, Lake Moehne, March 10th
– 17th.
NTL: Neural Theory of Language
http://www.icsi.berkeley.edu/NTL/
Papineni, K., Roukos, S., Ward, T. and Zhu, W. 2002.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics
(ACL), Philadelphia.
Porzel, R. and Malaka, R. 2005. A Task-based Frame-
work for Ontology Learning, Population and Evalua-
tion. Ontology Learning from Text: Methods,
Evaluation and Applications Frontiers. Artificial In-
telligence and Applications Series, Vol. 123, IOS
Press.
Porzel, R., Micelli, V., Aras, H., and Zorn, H.-P. 2006.
Tying the Knot: Ground Entities, Descriptions and
Information Objects for Construction-based Informa-
tion Extraction. Proceedings of OntoLex 2006. In
Press. Genoa, Italy.
Salton, G. 1989. Automatic Text Processing: the Trans-
formation, Analysis, and Retrieval of Information by
Computer. Addison-Wesley. Reading, MA.
Steels, L. 2005. The Role of Construction Grammar in
Fluid Language Grounding. Submitted. Elsevier Sci-
ence.
Stolcke, A. 1994. Bayesian Learning of Probabilistic
Language Models. Ph.D. Thesis, Computer Science
Division, University of California at Berkeley.
Sutton, R. and Barto, A. 1998. Reinforcement Learning:
An Introduction. MIT Press, Cambridge.
Talmy, L. 1988. Force dynamics in language and cogni-
tion. Cognitive Science, 12.
Tomasello, M. 2003. Constructing a Language: A Us-
age-Based Theory of Language Acquisition. Harvard
University Press.
</reference>
<page confidence="0.999415">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.561578">
<title confidence="0.997918">Searching for Grammar Right</title>
<author confidence="0.993679">Vanessa Micelli</author>
<affiliation confidence="0.817071">European Media Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.999884">69118 Heidelberg, Germany</address>
<email confidence="0.991312">{firstname.lastname@eml-d.villa-bosch.de}</email>
<abstract confidence="0.991876857142857">This paper describes our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism. Necessary modules are presented and first results and challenges in formalizing the grammar are shown up. Furthermore, we point out the major reasons why we chose construction grammar as the most fitting formalism for our purposes. Then our approach and ideas of learning new linguistic phenomena, ranging from holophrastic constructions to compositional ones, is presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>Maintaining Knowledge about Temporal Intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="13559" citStr="Allen, 1983" startWordPosition="2097" endWordPosition="2098">he possibility to model different purpose is filling the meaning pole of a construcconstraints both in the form pole and in the mean- tion. It consists of a list of schematic roles that can ing pole of a construction. A form constraint ap- serve as simulation parameters. plying to this exact construction is determining that In ECG, schemas can be evoked by or can evoke the determiner comes before the noun. This under- other schemas, i.e. particular schematic-roles of standing of before corresponds to Allen’s defini- another schema can be imported. A schema can, tion of his interval relations (Allen, 1983), which therefore, be defined against the background of states that they don’t necessarily have to follow another schema7. The property evokes and its ineach other but that there could be some modifiers verse property evoked-by have been defined as in between the two components of this construc- subproperties of the dol:generically-dependent-on tion. property and its inverse property dol:genericA meaning constraint of this construction deter- dependent respectively. Generic dependence is mines, that the meaning of the noun, used in this defined in the DOLCE ontology as the dependence respectiv</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>Allen, J.F. 1983. Maintaining Knowledge about Temporal Intervals. Communications of the ACM, 26(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bergen</author>
<author>N Chang</author>
</authors>
<title>Simulation-Based Language Understanding in Embodied Construction Grammar. ICSI TR-02-004,</title>
<date>2002</date>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="4073" citStr="Bergen and Chang, 2002" startWordPosition="615" endWordPosition="618">omains. A description of the corpus and its selection process will be given in section 4. Section 5 provides an outlook on the learning paradigm, while the last section presents some future issues and conclusions. 2 Grammar Formalism The most crucial foundation that is needed to build a grammar learning system is a grammar formalism. Therefore, we are designing a new formalization of construction grammar called ECtoloG (Porzel et al., 2006; Micelli et al., in press). One existing formal computational model of construction grammar is the Embodied Construction Grammar (ECG) (Chang et al., 2002; Bergen and Chang, 2002), with its main focus being on language understanding and later simulation2. A congruent and parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Grammar and ECG One main diff</context>
<context position="5558" citStr="Bergen and Chang (2002)" startWordPosition="842" endWordPosition="845">is, that generative grammars split form from function. Syntax, morphology, a lexicon or other formal components of the grammar constitute form, while the conventional function is defined by semantics. All constructions of a language, however, form in Langacker’s terms “a structured inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i.e. there are at least taxonomic links among the constructions (Diessel, 2004). This structure presents 2 For a detailed ECG analysis of a declarative utterance, i.e. the sentence Harry walked into the cafe, see Bergen and Chang (2002). one of the main differences between generative and construction grammars (Croft, to appear). One of the most cited examples that evidences the necessity, that there can be no explicit separation between syntax and semantics, is Goldberg’s example sentence (Goldberg, 1995:29): (1) he sneezed the napkin off the table. The whole meaning of this sentence cannot be gathered from the meanings of the discrete words. The direct object the napkin is not postulated by the verb to sneeze. This intransitive verb would have three arguments in a lexico-semantic theory: ‘X causes Y to move Z by sneezing’. </context>
<context position="16853" citStr="Bergen and Chang (2002)" startWordPosition="2625" endWordPosition="2628">hema. At the moment, they are modeled with the help of the edns:defines property. A schema defines its schematic roles with this property, denoting a subproperty of the edns:component property. According to the D&amp;S specification, a component is a proper part with a role or function in a system or a context. It is also stated, that roles can be different for the same entity, and the evaluation of them changes according to the kind of entity. This means, that instances of the class schema and its 5 For further information about which operators are used to model these features in ECG we refer to Bergen and Chang (2002), Chang et al. (2002) and Bryant (2004). 6 Following Bryant’s (2004) division of constructions into 5 levels of different degrees of schematicity. 7 To clarify this claim see Langacker’s hypotenuse example (Langacker, 1987:183ff.). 60 subclasses can have instances of the class schematic-role as their components. The schematicroles class has to fulfil the necessary condition, that at least one of the values of the edns:definedby property is of type schema. The domain of the defines property is a description (which can be our schemas) and its range is set to either concepts or figures (which are</context>
</contexts>
<marker>Bergen, Chang, 2002</marker>
<rawString>Bergen, B. and Chang, N. 2002. Simulation-Based Language Understanding in Embodied Construction Grammar. ICSI TR-02-004, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bryant</author>
</authors>
<title>Recovering coherent interpretations using semantic integration of partial parses.</title>
<date>2004</date>
<booktitle>Proceedings of the 3rd ROMAND workshop,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="16892" citStr="Bryant (2004)" startWordPosition="2634" endWordPosition="2635">lp of the edns:defines property. A schema defines its schematic roles with this property, denoting a subproperty of the edns:component property. According to the D&amp;S specification, a component is a proper part with a role or function in a system or a context. It is also stated, that roles can be different for the same entity, and the evaluation of them changes according to the kind of entity. This means, that instances of the class schema and its 5 For further information about which operators are used to model these features in ECG we refer to Bergen and Chang (2002), Chang et al. (2002) and Bryant (2004). 6 Following Bryant’s (2004) division of constructions into 5 levels of different degrees of schematicity. 7 To clarify this claim see Langacker’s hypotenuse example (Langacker, 1987:183ff.). 60 subclasses can have instances of the class schematic-role as their components. The schematicroles class has to fulfil the necessary condition, that at least one of the values of the edns:definedby property is of type schema. The domain of the defines property is a description (which can be our schemas) and its range is set to either concepts or figures (which are our schematic roles). The problem occu</context>
<context position="26900" citStr="Bryant, 2004" startWordPosition="4256" endWordPosition="4258">assume a strong inductive bias and different learning algorithms, as e.g. some form of Bayesian learning or model merging (Stolcke, 1994) or reinforcement learning (Sutton and Barto, 1998). Another important step that has to be employed is the (re)organization of the so-called constructicon, i.e. our inventory of constructions and schemas. These need to be merged, split or maybe thrown out again, depending on their utility, similarity etc. 5.3 Ambiguity Currently the problem of ambiguity is solved by endowing the analyzer with a chart and employing the semantic density algorithm described in (Bryant, 2004). In the future probabilistic reasoning frameworks as proposed by (Narayanan and Jurafsky, 2005) in combination with ontology-based coherence measures as proposed by (Loos and Porzel, 2004) constitute promising approaches for handling problems of construal, whether it be on a pragmatic, semantic, syntactic or phonological level. 6 Concluding Remarks In this paper we described our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism used in a question-answering system. We described necessary modules and presented first results and challe</context>
</contexts>
<marker>Bryant, 2004</marker>
<rawString>Bryant, J. 2004. Recovering coherent interpretations using semantic integration of partial parses. Proceedings of the 3rd ROMAND workshop, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>T Declerck</author>
<author>A Frank</author>
<author>S Racioppa</author>
<author>M Kiesel</author>
<author>M Sintek</author>
<author>M Engel</author>
<author>M Romanelli</author>
<author>D Sonntag</author>
<author>B Loos</author>
<author>V Micelli</author>
<author>R Porzel</author>
<author>P Cimiano</author>
</authors>
<title>LingInfo: Design and Applications of a Model for the Integration of Linguistic Information in Ontologies.</title>
<date>2006</date>
<booktitle>Proceedings of OntoLex</booktitle>
<publisher>In Press.</publisher>
<location>Genoa, Italy.</location>
<contexts>
<context position="18235" citStr="Buitelaar et al., 2006" startWordPosition="2852" endWordPosition="2855">meters are not always filled with atomic values but possibly with whole classes of entities. Therefore, one could think about modeling schematic roles as properties, setting the domain on the corresponding schema class and the range on the corresponding class whose subclasses and instances can possibly fill its range. 3.3 Linguistic Information Since linguistic information as e.g. grammatical gender, its case, or the part-of-speech of a word is needed for analyzing natural language texts, this information has to be modeled, as well, in the ECtoloG. Therefore, we integrated the LingInfo model (Buitelaar et al., 2006) into the ECtoloG. LingInfo constitutes an ontological model that provides other ontologies with linguistic information for different languages, momentarily for English, French, and German. Main objective of this ontology is to provide a mapping between ontological concepts and lexical items. That is, that the possibility is offered to assign linguistic information as e.g. the orthographic term, its grammatical gender, its part-of-speech, stem etc. to classes and properties. For our purposes, the LingInfo ontology had to be converted from RDFS into OWL-DL format and then integrated into the EC</context>
</contexts>
<marker>Buitelaar, Declerck, Frank, Racioppa, Kiesel, Sintek, Engel, Romanelli, Sonntag, Loos, Micelli, Porzel, Cimiano, 2006</marker>
<rawString>Buitelaar, P., Declerck, T., Frank, A., Racioppa, S., Kiesel, M., Sintek, M., Engel, M., Romanelli, M., Sonntag, D., Loos, B., Micelli, V., Porzel, R. and Cimiano, P. 2006. LingInfo: Design and Applications of a Model for the Integration of Linguistic Information in Ontologies. Proceedings of OntoLex 2006. In Press. Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bybee</author>
</authors>
<title>A functionalist approach to grammar and its evolution.</title>
<date>1998</date>
<journal>Evolution of Communication</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="795" citStr="Bybee, 1998" startWordPosition="108" endWordPosition="109">r describes our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism. Necessary modules are presented and first results and challenges in formalizing the grammar are shown up. Furthermore, we point out the major reasons why we chose construction grammar as the most fitting formalism for our purposes. Then our approach and ideas of learning new linguistic phenomena, ranging from holophrastic constructions to compositional ones, is presented. 1 Introduction Since any particular language1 changes constantly (Cf. Hopper and Traugott, 2003; Bybee, 1998) – and even varies across domains, users, registers etc. – scalable natural language understanding systems must be able to cope with language variation and change. Moreover, due to the fact that any natural language understanding system, which is based on some formal representation of that language’s grammar, will always only be able to represent a portion of what is going on in any particular language at the present time, we need to find systematic ways of endowing natural language understanding systems with means of learning new 1 This claim also holds within any solidified system of convent</context>
</contexts>
<marker>Bybee, 1998</marker>
<rawString>Bybee, J. 1998. A functionalist approach to grammar and its evolution. Evolution of Communication 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carey</author>
</authors>
<title>The child as word-learner. Linguistic theory and psychological reality.</title>
<date>1978</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="24594" citStr="Carey, 1978" startWordPosition="3890" endWordPosition="3891">sults in coverage A of C’ where: A = 100% - (Sh + Sc) Sh denotes all the holophrastic phenomena and Sc all compositional phenomena not observed in C. 5 Grammar Learning To generate a grammar that covers this new corpus C’ different strategies have to be applied for holophrastic items Sh which are lexical constructions in our approach and for compositional ones Sc – meaning constructions on a higher level of abstraction as e.g. constructions that capture grammatical phenomena such as noun phrases or even whole sentences. 5.1 Learning Lexical Constructions Analogous to the fast mapping process (Carey, 1978) of learning new words based on exposure without additional training or feedback on the correctness of its meaning, we are employing a method of filling our ontology with whole paradigms of new terms8, enabled through the modeling of constructions described in 3.1. First step herein is employing a tool – Morphy (Lezius, 2002) – that enables morphological analysis and synthesis. The analysis of a term yields information about its stem, its part-of-speech, its case, its number, and its grammatical gender. This information can then easily be integrated automatically into the ECtoloG. As already m</context>
</contexts>
<marker>Carey, 1978</marker>
<rawString>Carey, S. 1978. The child as word-learner. Linguistic theory and psychological reality. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chang</author>
<author>J Feldman</author>
<author>R Porzel</author>
<author>K Sanders</author>
</authors>
<title>Scaling Cognitive Linguistics: Formalisms for Language Understanding.</title>
<date>2002</date>
<booktitle>Proceedings of the 1st ScaNaLU Workshop,</booktitle>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="4048" citStr="Chang et al., 2002" startWordPosition="611" endWordPosition="614"> extended to other domains. A description of the corpus and its selection process will be given in section 4. Section 5 provides an outlook on the learning paradigm, while the last section presents some future issues and conclusions. 2 Grammar Formalism The most crucial foundation that is needed to build a grammar learning system is a grammar formalism. Therefore, we are designing a new formalization of construction grammar called ECtoloG (Porzel et al., 2006; Micelli et al., in press). One existing formal computational model of construction grammar is the Embodied Construction Grammar (ECG) (Chang et al., 2002; Bergen and Chang, 2002), with its main focus being on language understanding and later simulation2. A congruent and parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Gram</context>
<context position="16874" citStr="Chang et al. (2002)" startWordPosition="2629" endWordPosition="2632"> are modeled with the help of the edns:defines property. A schema defines its schematic roles with this property, denoting a subproperty of the edns:component property. According to the D&amp;S specification, a component is a proper part with a role or function in a system or a context. It is also stated, that roles can be different for the same entity, and the evaluation of them changes according to the kind of entity. This means, that instances of the class schema and its 5 For further information about which operators are used to model these features in ECG we refer to Bergen and Chang (2002), Chang et al. (2002) and Bryant (2004). 6 Following Bryant’s (2004) division of constructions into 5 levels of different degrees of schematicity. 7 To clarify this claim see Langacker’s hypotenuse example (Langacker, 1987:183ff.). 60 subclasses can have instances of the class schematic-role as their components. The schematicroles class has to fulfil the necessary condition, that at least one of the values of the edns:definedby property is of type schema. The domain of the defines property is a description (which can be our schemas) and its range is set to either concepts or figures (which are our schematic roles)</context>
</contexts>
<marker>Chang, Feldman, Porzel, Sanders, 2002</marker>
<rawString>Chang, N., Feldman, J., Porzel, R. and Sanders, K. 2002. Scaling Cognitive Linguistics: Formalisms for Language Understanding. Proceedings of the 1st ScaNaLU Workshop, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the theory of syntax.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4776" citStr="Chomsky, 1965" startWordPosition="728" endWordPosition="729">nd parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Grammar and ECG One main difference between West Coast Grammar (Langacker, 1987; Lakoff, 1987) and East Coast Grammar (Chomsky, 1965; Katz, 1972) is the fact that construction grammar offers a vertical – not a horizontal – organisation of any knowledge concerning a language’s grammar. That is, that generative grammars split form from function. Syntax, morphology, a lexicon or other formal components of the grammar constitute form, while the conventional function is defined by semantics. All constructions of a language, however, form in Langacker’s terms “a structured inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i.e. there are at least taxonomic links among the cons</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, N. 1965. Aspects of the theory of syntax. MIT Press. Cambridge, Mass.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Croft</author>
</authors>
<title>To appear. Logical and typological arguments for Radical Construction Grammar. Construction Grammar(s): Cognitive and cross-language dimensions (Constructional approaches to Language, 1). John Benjamins,</title>
<location>Amsterdam.</location>
<marker>Croft, </marker>
<rawString>Croft, W. To appear. Logical and typological arguments for Radical Construction Grammar. Construction Grammar(s): Cognitive and cross-language dimensions (Constructional approaches to Language, 1). John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Diessel</author>
</authors>
<title>The Acquisition of Complex Sentences. Cambridge Studies in Linguistics (105).</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5401" citStr="Diessel, 2004" startWordPosition="818" endWordPosition="819">2) is the fact that construction grammar offers a vertical – not a horizontal – organisation of any knowledge concerning a language’s grammar. That is, that generative grammars split form from function. Syntax, morphology, a lexicon or other formal components of the grammar constitute form, while the conventional function is defined by semantics. All constructions of a language, however, form in Langacker’s terms “a structured inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i.e. there are at least taxonomic links among the constructions (Diessel, 2004). This structure presents 2 For a detailed ECG analysis of a declarative utterance, i.e. the sentence Harry walked into the cafe, see Bergen and Chang (2002). one of the main differences between generative and construction grammars (Croft, to appear). One of the most cited examples that evidences the necessity, that there can be no explicit separation between syntax and semantics, is Goldberg’s example sentence (Goldberg, 1995:29): (1) he sneezed the napkin off the table. The whole meaning of this sentence cannot be gathered from the meanings of the discrete words. The direct object the napkin</context>
</contexts>
<marker>Diessel, 2004</marker>
<rawString>Diessel, H. 2004. The Acquisition of Complex Sentences. Cambridge Studies in Linguistics (105). Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="false">
<authors>
<author>EDU Even</author>
</authors>
<title>Deeper Understanding http://www.emldevelopment.de/english/research/edu/index.php (last access:</title>
<pages>31--03</pages>
<marker>Even, </marker>
<rawString>EDU: Even Deeper Understanding http://www.emldevelopment.de/english/research/edu/index.php (last access: 31/03/06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="22128" citStr="Fellbaum, 1998" startWordPosition="3482" endWordPosition="3483">ed precisely. Different approaches could be employed, 61 i.e. regarding similarity in terms of syntactic or semantic phenomena or a combination of both. Since construction grammar makes no separation between syntax and semantics, phenomena that should be counted are both constructions and image schemas. As for holophrastic constructions this presents less of a challenge, we rather expect counting compositional ones being a ‘tough cookie’. To detect image schemas in natural text automatically, we seek to employ different methodologies, e.g. LSA (Kintsch and van Dijk, 1978), using synonym sets (Fellbaum, 1998) or other ontologies, which could assist in discovering the semantics of an unknown word with its corresponding schematic roles and the appropriate fillers. This or a similar methodology will be applied in the automatic acquisition process as well. Another important point is that some terms, or some constructions, need to get a higher relevance factor than others, which will highly depend on context. Such a relevance factor can rank terms or constructions according to their importance in the respective text. Ranking functions that can be examined are, e.g., the TF/IDF function (e.g. Salton, 19</context>
<context position="23675" citStr="Fellbaum, 1998" startWordPosition="3733" endWordPosition="3734">n ratio. The following five steps need to be executed in determining the similarity of two documents: Step 1: Processing of the document D; analyzing the text and creating a list of all occurring words, constructions and/or image schemas. We assume that the best choice is counting constructions and corresponding image schemas, since they represent the semantics of the given text. Step 2: Weighing of schemas and constructions Step 3: Processing of the document D+1; executing of step 1 and 2 for this document. Step 4: Comparing the documents; possibly adding synonyms of sources as e.g. WordNet (Fellbaum, 1998). Step 5: Calculating the documents’ similarity; defining a threshold up to which documents are considered as being similar. If a document is said to be similar, it is added to the corpus, which becomes the new corpus C’. Analysis of the New Corpus C’: The new corpus C’ is analyzed, whereby the coverage results in coverage A of C’ where: A = 100% - (Sh + Sc) Sh denotes all the holophrastic phenomena and Sc all compositional phenomena not observed in C. 5 Grammar Learning To generate a grammar that covers this new corpus C’ different strategies have to be applied for holophrastic items Sh which</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fillmore</author>
<author>P Kay</author>
</authors>
<title>The goals of Construction Grammar.</title>
<date>1987</date>
<journal>Berkeley Cognitive Science Program TR</journal>
<volume>50</volume>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="2144" citStr="Fillmore and Kay, 1987" startWordPosition="314" endWordPosition="317">ltimately, new formmeaning pairings, i.e. constructions. Constructions are the basic building blocks, posited by a particular grammar framework called Construction Grammar, and are defined as follows: “C is a construction iffdef C is a form-meaning pair &lt;Fi, Si&gt; such that some aspect of Fi or some aspect of Si is not strictly predictable from C’s component parts or from other previously established constructions.” (Goldberg, 1995:4). Construction Grammar originated from earlier insights in functional and usage-based models of language mainly supposed by cognitive linguists (e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay, 2002; Talmy, 1988; etc.). It has been devised to handle actually occurring natural language, which notoriously contains non-literal, elliptic, contextdependent, metaphorical or underspecified linguistic expressions. These phenomena still present a challenge for today’s natural language understanding systems. In addition to these advantages, we adhere to principles proposed by other constructivists as e.g. Tomasello (2003) that language acquisition is a usage-based phenomenon, contrasting approaches by generative grammarians who assume an innate grammar (Chomsky, 1981). Furthermore, we a</context>
</contexts>
<marker>Fillmore, Kay, 1987</marker>
<rawString>Fillmore, C. and Kay, P. 1987. The goals of Construction Grammar. Berkeley Cognitive Science Program TR 50. University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Gallese</author>
<author>G Lakoff</author>
</authors>
<title>The brain´s concepts: the role of the sensory-motor system in conceptual knowledge.</title>
<date>2005</date>
<journal>Cognitive Neuropsychology</journal>
<pages>21--2005</pages>
<contexts>
<context position="6749" citStr="Gallese and Lakoff, 2005" startWordPosition="1032" endWordPosition="1035"> causes Y to move Z by sneezing’. Goldberg states that the additional meaning of caused motion which is added to the conventional meaning of the verb sneeze is offered by the respective causedmotion construction. Based on this background ECG – a formal computational model of construction grammar – was developed within the Neural Theory of Language project (NTL) and the EDU project (EDU). While other approaches consider language as completely independent from the organism which uses it, ECG claims that several characteristics of the user’s sensorimotor system can influence his or her language (Gallese and Lakoff, 2005). The needed dynamic and inferential semantics in ECG is represented by embodied schemas. These schemas are known under the term of image schemas in traditional cognitive semantics and constitute schematic recurring patterns of sensorimotor experience (Johnson, 1987; Lakoff, 1987). The current ASCII format of ECG is insufficient for building scalable NLU systems in the long run. Therefore, our attempt at formalizing construction grammar results in an ontological model that combines two ontological modeling frameworks endowed with a construction grammar layer, based on the main ideas behind ECG</context>
</contexts>
<marker>Gallese, Lakoff, 2005</marker>
<rawString>Gallese, V. and Lakoff, G. 2005. The brain´s concepts: the role of the sensory-motor system in conceptual knowledge. Cognitive Neuropsychology 21/2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gangemi</author>
<author>P Mika</author>
</authors>
<title>Understanding the Semantic Web through Descriptions and Situations.</title>
<date>2003</date>
<booktitle>Proceedings of ODBASE03 Conference,</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="7607" citStr="Gangemi and Mika, 2003" startWordPosition="1158" endWordPosition="1161">motor experience (Johnson, 1987; Lakoff, 1987). The current ASCII format of ECG is insufficient for building scalable NLU systems in the long run. Therefore, our attempt at formalizing construction grammar results in an ontological model that combines two ontological modeling frameworks endowed with a construction grammar layer, based on the main ideas behind ECG. The following section describes the resulting ontology, pointing out main challenges and advantages of that approach. 3 Formalizing Construction Grammar The ontological frameworks mentioned above are Descriptions &amp; Situations (D&amp;S) (Gangemi and Mika, 2003) and Ontology of Information Objects (OIO) (Guarino, 2006), which both are extensions of the Descriptive Ontology for Linguistic and 58 Cognitive Engineering (DOLCE) (Masolo et al., 2003). D&amp;S is an ontology for representing a variety of reified contexts and states of affairs. In contrast to physical objects or events, the extensions of ontologies to the domain of non-physical objects pose a challenge to the ontology engineer. The reason for this lies in the fact that non-physical objects are taken to have meaning only in combination with some other ground entity. Accordingly, their logical re</context>
</contexts>
<marker>Gangemi, Mika, 2003</marker>
<rawString>Gangemi A., Mika P. 2003. Understanding the Semantic Web through Descriptions and Situations. Proceedings of ODBASE03 Conference, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldberg</author>
</authors>
<title>Constructions: A Construction Grammar Approach to Argument Structure.</title>
<date>1995</date>
<publisher>University of Chicago Press. Chicago.</publisher>
<contexts>
<context position="1955" citStr="Goldberg, 1995" startWordPosition="291" endWordPosition="292">aim also holds within any solidified system of conventionalized form-meaning pairings, e.g. dialects, chronolects, sociolects, idiolects, jargons, etc. 57 forms, new meanings and, ultimately, new formmeaning pairings, i.e. constructions. Constructions are the basic building blocks, posited by a particular grammar framework called Construction Grammar, and are defined as follows: “C is a construction iffdef C is a form-meaning pair &lt;Fi, Si&gt; such that some aspect of Fi or some aspect of Si is not strictly predictable from C’s component parts or from other previously established constructions.” (Goldberg, 1995:4). Construction Grammar originated from earlier insights in functional and usage-based models of language mainly supposed by cognitive linguists (e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay, 2002; Talmy, 1988; etc.). It has been devised to handle actually occurring natural language, which notoriously contains non-literal, elliptic, contextdependent, metaphorical or underspecified linguistic expressions. These phenomena still present a challenge for today’s natural language understanding systems. In addition to these advantages, we adhere to principles proposed by other constructivists as </context>
<context position="5831" citStr="Goldberg, 1995" startWordPosition="886" endWordPosition="887"> inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i.e. there are at least taxonomic links among the constructions (Diessel, 2004). This structure presents 2 For a detailed ECG analysis of a declarative utterance, i.e. the sentence Harry walked into the cafe, see Bergen and Chang (2002). one of the main differences between generative and construction grammars (Croft, to appear). One of the most cited examples that evidences the necessity, that there can be no explicit separation between syntax and semantics, is Goldberg’s example sentence (Goldberg, 1995:29): (1) he sneezed the napkin off the table. The whole meaning of this sentence cannot be gathered from the meanings of the discrete words. The direct object the napkin is not postulated by the verb to sneeze. This intransitive verb would have three arguments in a lexico-semantic theory: ‘X causes Y to move Z by sneezing’. Goldberg states that the additional meaning of caused motion which is added to the conventional meaning of the verb sneeze is offered by the respective causedmotion construction. Based on this background ECG – a formal computational model of construction grammar – was deve</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Goldberg, A. 1995. Constructions: A Construction Grammar Approach to Argument Structure. University of Chicago Press. Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Guarino</author>
</authors>
<title>Ontology Library. WonderWeb Deliverable D202, I STC-CNR,</title>
<date>2006</date>
<pages>31--03</pages>
<location>Padova,</location>
<note>www.loa-cnr.it/Papers/Deliverable%202.pdf (last access:</note>
<contexts>
<context position="7665" citStr="Guarino, 2006" startWordPosition="1168" endWordPosition="1169">ormat of ECG is insufficient for building scalable NLU systems in the long run. Therefore, our attempt at formalizing construction grammar results in an ontological model that combines two ontological modeling frameworks endowed with a construction grammar layer, based on the main ideas behind ECG. The following section describes the resulting ontology, pointing out main challenges and advantages of that approach. 3 Formalizing Construction Grammar The ontological frameworks mentioned above are Descriptions &amp; Situations (D&amp;S) (Gangemi and Mika, 2003) and Ontology of Information Objects (OIO) (Guarino, 2006), which both are extensions of the Descriptive Ontology for Linguistic and 58 Cognitive Engineering (DOLCE) (Masolo et al., 2003). D&amp;S is an ontology for representing a variety of reified contexts and states of affairs. In contrast to physical objects or events, the extensions of ontologies to the domain of non-physical objects pose a challenge to the ontology engineer. The reason for this lies in the fact that non-physical objects are taken to have meaning only in combination with some other ground entity. Accordingly, their logical representation is generally set at the level of theories or </context>
</contexts>
<marker>Guarino, 2006</marker>
<rawString>Guarino, N. 2006. Ontology Library. WonderWeb Deliverable D202, I STC-CNR, Padova, Italy. www.loa-cnr.it/Papers/Deliverable%202.pdf (last access: 31/03/2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hopper</author>
<author>E Traugott</author>
</authors>
<title>Grammaticalization.</title>
<date>2003</date>
<publisher>Cambridge University Press. Cambridge, UK.</publisher>
<contexts>
<context position="781" citStr="Hopper and Traugott, 2003" startWordPosition="104" endWordPosition="107">osch.de} Abstract This paper describes our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism. Necessary modules are presented and first results and challenges in formalizing the grammar are shown up. Furthermore, we point out the major reasons why we chose construction grammar as the most fitting formalism for our purposes. Then our approach and ideas of learning new linguistic phenomena, ranging from holophrastic constructions to compositional ones, is presented. 1 Introduction Since any particular language1 changes constantly (Cf. Hopper and Traugott, 2003; Bybee, 1998) – and even varies across domains, users, registers etc. – scalable natural language understanding systems must be able to cope with language variation and change. Moreover, due to the fact that any natural language understanding system, which is based on some formal representation of that language’s grammar, will always only be able to represent a portion of what is going on in any particular language at the present time, we need to find systematic ways of endowing natural language understanding systems with means of learning new 1 This claim also holds within any solidified sys</context>
</contexts>
<marker>Hopper, Traugott, 2003</marker>
<rawString>Hopper, P. and Traugott, E. 2003. Grammaticalization. Cambridge University Press. Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>The Body in the Mind: The Bodily Basis of Meaning, Imagination, and Reason.</title>
<date>1987</date>
<publisher>University of Chicago Press. Chicago.</publisher>
<contexts>
<context position="7015" citStr="Johnson, 1987" startWordPosition="1073" endWordPosition="1074">nstruction grammar – was developed within the Neural Theory of Language project (NTL) and the EDU project (EDU). While other approaches consider language as completely independent from the organism which uses it, ECG claims that several characteristics of the user’s sensorimotor system can influence his or her language (Gallese and Lakoff, 2005). The needed dynamic and inferential semantics in ECG is represented by embodied schemas. These schemas are known under the term of image schemas in traditional cognitive semantics and constitute schematic recurring patterns of sensorimotor experience (Johnson, 1987; Lakoff, 1987). The current ASCII format of ECG is insufficient for building scalable NLU systems in the long run. Therefore, our attempt at formalizing construction grammar results in an ontological model that combines two ontological modeling frameworks endowed with a construction grammar layer, based on the main ideas behind ECG. The following section describes the resulting ontology, pointing out main challenges and advantages of that approach. 3 Formalizing Construction Grammar The ontological frameworks mentioned above are Descriptions &amp; Situations (D&amp;S) (Gangemi and Mika, 2003) and Ont</context>
<context position="12578" citStr="Johnson, 1987" startWordPosition="1938" endWordPosition="1939">ir meaning or conceptualization. In this ontology, content is reified as a description, which offered us the possibility to model image schemas as such. How image schemas are modeled will be described in section 3.2. Compositional Constructions Compositional constructions are constructions which are on a higher level of abstraction than holophrastic ones. This means, that there exist constructions which combine different constructions 59 into one unit. ECG designed a so-called construc- 3.2 Modeling of Image Schemas tional block, wherein several constructions are Following Johnson and Lakoff (Johnson, 1987; subsumed under and accessible in one more com- Lakoff and Johnson, 1980; Lakoff, 1987) image plex construction. schemas are schematic representations that capture An example is the DetNoun construction, which recurrent patterns of sensorimotor experience. Accombines a determiner and a noun to form one cording to ECG, a schema is a description whose unit. There is the possibility to model different purpose is filling the meaning pole of a construcconstraints both in the form pole and in the mean- tion. It consists of a list of schematic roles that can ing pole of a construction. A form constr</context>
</contexts>
<marker>Johnson, 1987</marker>
<rawString>Johnson, M. 1987. The Body in the Mind: The Bodily Basis of Meaning, Imagination, and Reason. University of Chicago Press. Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Katz</author>
</authors>
<title>Semantic theory.</title>
<date>1972</date>
<publisher>Harper &amp; Row.</publisher>
<location>New York.</location>
<contexts>
<context position="4789" citStr="Katz, 1972" startWordPosition="730" endWordPosition="731">elopment has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Grammar and ECG One main difference between West Coast Grammar (Langacker, 1987; Lakoff, 1987) and East Coast Grammar (Chomsky, 1965; Katz, 1972) is the fact that construction grammar offers a vertical – not a horizontal – organisation of any knowledge concerning a language’s grammar. That is, that generative grammars split form from function. Syntax, morphology, a lexicon or other formal components of the grammar constitute form, while the conventional function is defined by semantics. All constructions of a language, however, form in Langacker’s terms “a structured inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i.e. there are at least taxonomic links among the constructions (Di</context>
</contexts>
<marker>Katz, 1972</marker>
<rawString>Katz, J. J. 1972. Semantic theory. Harper &amp; Row. New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kay</author>
</authors>
<title>An Informal Sketch of a Formal Architecture for Construction Grammar.</title>
<date>2002</date>
<journal>Grammars</journal>
<volume>1</volume>
<contexts>
<context position="2155" citStr="Kay, 2002" startWordPosition="318" endWordPosition="319">ng pairings, i.e. constructions. Constructions are the basic building blocks, posited by a particular grammar framework called Construction Grammar, and are defined as follows: “C is a construction iffdef C is a form-meaning pair &lt;Fi, Si&gt; such that some aspect of Fi or some aspect of Si is not strictly predictable from C’s component parts or from other previously established constructions.” (Goldberg, 1995:4). Construction Grammar originated from earlier insights in functional and usage-based models of language mainly supposed by cognitive linguists (e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay, 2002; Talmy, 1988; etc.). It has been devised to handle actually occurring natural language, which notoriously contains non-literal, elliptic, contextdependent, metaphorical or underspecified linguistic expressions. These phenomena still present a challenge for today’s natural language understanding systems. In addition to these advantages, we adhere to principles proposed by other constructivists as e.g. Tomasello (2003) that language acquisition is a usage-based phenomenon, contrasting approaches by generative grammarians who assume an innate grammar (Chomsky, 1981). Furthermore, we agree to the</context>
</contexts>
<marker>Kay, 2002</marker>
<rawString>Kay, P. 2002. An Informal Sketch of a Formal Architecture for Construction Grammar. Grammars 1/5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
<author>T van Dijk</author>
</authors>
<title>Toward a model of text comprehension and production.</title>
<date>1978</date>
<journal>Psychological Review,</journal>
<volume>85</volume>
<issue>5</issue>
<marker>Kintsch, van Dijk, 1978</marker>
<rawString>Kintsch, W. and van Dijk, T. 1978. Toward a model of text comprehension and production. Psychological Review, 85 (5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
<author>M Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>Chicago University Press. London.</publisher>
<contexts>
<context position="12651" citStr="Lakoff and Johnson, 1980" startWordPosition="1948" endWordPosition="1951">reified as a description, which offered us the possibility to model image schemas as such. How image schemas are modeled will be described in section 3.2. Compositional Constructions Compositional constructions are constructions which are on a higher level of abstraction than holophrastic ones. This means, that there exist constructions which combine different constructions 59 into one unit. ECG designed a so-called construc- 3.2 Modeling of Image Schemas tional block, wherein several constructions are Following Johnson and Lakoff (Johnson, 1987; subsumed under and accessible in one more com- Lakoff and Johnson, 1980; Lakoff, 1987) image plex construction. schemas are schematic representations that capture An example is the DetNoun construction, which recurrent patterns of sensorimotor experience. Accombines a determiner and a noun to form one cording to ECG, a schema is a description whose unit. There is the possibility to model different purpose is filling the meaning pole of a construcconstraints both in the form pole and in the mean- tion. It consists of a list of schematic roles that can ing pole of a construction. A form constraint ap- serve as simulation parameters. plying to this exact constructio</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>Lakoff, G. and Johnson, M. 1980. Metaphors We Live By. Chicago University Press. London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Women, Fire, and Dangerous Things.</title>
<date>1987</date>
<institution>University of Chicago Press. Chicago and London.</institution>
<contexts>
<context position="2120" citStr="Lakoff, 1987" startWordPosition="312" endWordPosition="313">eanings and, ultimately, new formmeaning pairings, i.e. constructions. Constructions are the basic building blocks, posited by a particular grammar framework called Construction Grammar, and are defined as follows: “C is a construction iffdef C is a form-meaning pair &lt;Fi, Si&gt; such that some aspect of Fi or some aspect of Si is not strictly predictable from C’s component parts or from other previously established constructions.” (Goldberg, 1995:4). Construction Grammar originated from earlier insights in functional and usage-based models of language mainly supposed by cognitive linguists (e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay, 2002; Talmy, 1988; etc.). It has been devised to handle actually occurring natural language, which notoriously contains non-literal, elliptic, contextdependent, metaphorical or underspecified linguistic expressions. These phenomena still present a challenge for today’s natural language understanding systems. In addition to these advantages, we adhere to principles proposed by other constructivists as e.g. Tomasello (2003) that language acquisition is a usage-based phenomenon, contrasting approaches by generative grammarians who assume an innate grammar (Chomsky, </context>
<context position="4738" citStr="Lakoff, 1987" startWordPosition="722" endWordPosition="723">g and later simulation2. A congruent and parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Grammar and ECG One main difference between West Coast Grammar (Langacker, 1987; Lakoff, 1987) and East Coast Grammar (Chomsky, 1965; Katz, 1972) is the fact that construction grammar offers a vertical – not a horizontal – organisation of any knowledge concerning a language’s grammar. That is, that generative grammars split form from function. Syntax, morphology, a lexicon or other formal components of the grammar constitute form, while the conventional function is defined by semantics. All constructions of a language, however, form in Langacker’s terms “a structured inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i.e. there are a</context>
<context position="7030" citStr="Lakoff, 1987" startWordPosition="1075" endWordPosition="1076">mar – was developed within the Neural Theory of Language project (NTL) and the EDU project (EDU). While other approaches consider language as completely independent from the organism which uses it, ECG claims that several characteristics of the user’s sensorimotor system can influence his or her language (Gallese and Lakoff, 2005). The needed dynamic and inferential semantics in ECG is represented by embodied schemas. These schemas are known under the term of image schemas in traditional cognitive semantics and constitute schematic recurring patterns of sensorimotor experience (Johnson, 1987; Lakoff, 1987). The current ASCII format of ECG is insufficient for building scalable NLU systems in the long run. Therefore, our attempt at formalizing construction grammar results in an ontological model that combines two ontological modeling frameworks endowed with a construction grammar layer, based on the main ideas behind ECG. The following section describes the resulting ontology, pointing out main challenges and advantages of that approach. 3 Formalizing Construction Grammar The ontological frameworks mentioned above are Descriptions &amp; Situations (D&amp;S) (Gangemi and Mika, 2003) and Ontology of Inform</context>
<context position="12666" citStr="Lakoff, 1987" startWordPosition="1952" endWordPosition="1953">which offered us the possibility to model image schemas as such. How image schemas are modeled will be described in section 3.2. Compositional Constructions Compositional constructions are constructions which are on a higher level of abstraction than holophrastic ones. This means, that there exist constructions which combine different constructions 59 into one unit. ECG designed a so-called construc- 3.2 Modeling of Image Schemas tional block, wherein several constructions are Following Johnson and Lakoff (Johnson, 1987; subsumed under and accessible in one more com- Lakoff and Johnson, 1980; Lakoff, 1987) image plex construction. schemas are schematic representations that capture An example is the DetNoun construction, which recurrent patterns of sensorimotor experience. Accombines a determiner and a noun to form one cording to ECG, a schema is a description whose unit. There is the possibility to model different purpose is filling the meaning pole of a construcconstraints both in the form pole and in the mean- tion. It consists of a list of schematic roles that can ing pole of a construction. A form constraint ap- serve as simulation parameters. plying to this exact construction is determinin</context>
</contexts>
<marker>Lakoff, 1987</marker>
<rawString>Lakoff, G. 1987. Women, Fire, and Dangerous Things. University of Chicago Press. Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Langacker</author>
</authors>
<date>1987</date>
<journal>Foundations of Cognitive Grammar,</journal>
<volume>1</volume>
<publisher>University Press. Stanford.</publisher>
<contexts>
<context position="4723" citStr="Langacker, 1987" startWordPosition="720" endWordPosition="721">uage understanding and later simulation2. A congruent and parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Grammar and ECG One main difference between West Coast Grammar (Langacker, 1987; Lakoff, 1987) and East Coast Grammar (Chomsky, 1965; Katz, 1972) is the fact that construction grammar offers a vertical – not a horizontal – organisation of any knowledge concerning a language’s grammar. That is, that generative grammars split form from function. Syntax, morphology, a lexicon or other formal components of the grammar constitute form, while the conventional function is defined by semantics. All constructions of a language, however, form in Langacker’s terms “a structured inventory of conventional linguistic units” (Langacker, 1987:54). This inventory is network-structured, i</context>
<context position="17075" citStr="Langacker, 1987" startWordPosition="2660" endWordPosition="2661">a component is a proper part with a role or function in a system or a context. It is also stated, that roles can be different for the same entity, and the evaluation of them changes according to the kind of entity. This means, that instances of the class schema and its 5 For further information about which operators are used to model these features in ECG we refer to Bergen and Chang (2002), Chang et al. (2002) and Bryant (2004). 6 Following Bryant’s (2004) division of constructions into 5 levels of different degrees of schematicity. 7 To clarify this claim see Langacker’s hypotenuse example (Langacker, 1987:183ff.). 60 subclasses can have instances of the class schematic-role as their components. The schematicroles class has to fulfil the necessary condition, that at least one of the values of the edns:definedby property is of type schema. The domain of the defines property is a description (which can be our schemas) and its range is set to either concepts or figures (which are our schematic roles). The problem occurring hereby is that the roles cannot be filled by complete classes which is necessary in a lot of cases, since the parameters are not always filled with atomic values but possibly wi</context>
</contexts>
<marker>Langacker, 1987</marker>
<rawString>Langacker, R. 1987. Foundations of Cognitive Grammar, Vol. 1. University Press. Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lezius</author>
</authors>
<title>Morphy - German Morphology, Partof-Speech Tagging and Applications.</title>
<date>2000</date>
<booktitle>Proceedings of the 9th EURALEX International Congress,</booktitle>
<location>Stuttgart, Germany.</location>
<marker>Lezius, 2000</marker>
<rawString>Lezius, W. 2000. Morphy - German Morphology, Partof-Speech Tagging and Applications. Proceedings of the 9th EURALEX International Congress, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Loos</author>
</authors>
<title>Scaling natural language understanding via user-driven ontology learning. This volume.</title>
<date>2006</date>
<contexts>
<context position="25492" citStr="Loos, 2006" startWordPosition="4036" endWordPosition="4037">employing a tool – Morphy (Lezius, 2002) – that enables morphological analysis and synthesis. The analysis of a term yields information about its stem, its part-of-speech, its case, its number, and its grammatical gender. This information can then easily be integrated automatically into the ECtoloG. As already mentioned in section 4.3, we are not only trying to automatically acquire the form pole of the constructions, but also its image schematic meaning, that means the network of the schemas that hierarchically form the meaning pole of such a term, applying ontology learning mechanisms (e.g. Loos, 2006) and methods similar to those described in section 4.3. Additionally, investigations are underway to connect the grammar learning framework proposed herein to a computer vision system that provides supplementary feedback con8 We are aware of the fact that fast mapping in humans is limited to color terms, shapes or texture terms, but are employing the method on other kinds of terms, nevertheless, since the grammar learning paradigm in our approach is still in its baby shoes. 62 cerning the hypothesized semantics of individual forms in the case of multi-media information. 5.2 Learning Compositio</context>
</contexts>
<marker>Loos, 2006</marker>
<rawString>Loos, B. 2006. Scaling natural language understanding via user-driven ontology learning. This volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Loos</author>
<author>R Porzel</author>
</authors>
<title>Resolution of Lexical Ambiguities in Spoken Dialogue System.</title>
<date>2004</date>
<booktitle>Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="27089" citStr="Loos and Porzel, 2004" startWordPosition="4282" endWordPosition="4285">o, 1998). Another important step that has to be employed is the (re)organization of the so-called constructicon, i.e. our inventory of constructions and schemas. These need to be merged, split or maybe thrown out again, depending on their utility, similarity etc. 5.3 Ambiguity Currently the problem of ambiguity is solved by endowing the analyzer with a chart and employing the semantic density algorithm described in (Bryant, 2004). In the future probabilistic reasoning frameworks as proposed by (Narayanan and Jurafsky, 2005) in combination with ontology-based coherence measures as proposed by (Loos and Porzel, 2004) constitute promising approaches for handling problems of construal, whether it be on a pragmatic, semantic, syntactic or phonological level. 6 Concluding Remarks In this paper we described our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism used in a question-answering system. We described necessary modules and presented first results and challenges in formalizing construction grammar. Furthermore, we pointed out our motivation for choosing construction grammar and the, therefore, resulting advantages. Then our approach and ideas </context>
</contexts>
<marker>Loos, Porzel, 2004</marker>
<rawString>Loos, B. and Porzel, R. 2004. Resolution of Lexical Ambiguities in Spoken Dialogue System. Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Masolo</author>
<author>S Borgo</author>
<author>A Gangemi</author>
<author>N Guarino</author>
<author>A Oltramari</author>
</authors>
<date>2003</date>
<booktitle>Ontology Library. WonderWeb Deliverable D18, I STC-CNR,</booktitle>
<location>Padova,</location>
<note>http://wonderweb.semanticweb.org/deliverables/docu ments/D18.pdf.</note>
<contexts>
<context position="7794" citStr="Masolo et al., 2003" startWordPosition="1185" endWordPosition="1188">ruction grammar results in an ontological model that combines two ontological modeling frameworks endowed with a construction grammar layer, based on the main ideas behind ECG. The following section describes the resulting ontology, pointing out main challenges and advantages of that approach. 3 Formalizing Construction Grammar The ontological frameworks mentioned above are Descriptions &amp; Situations (D&amp;S) (Gangemi and Mika, 2003) and Ontology of Information Objects (OIO) (Guarino, 2006), which both are extensions of the Descriptive Ontology for Linguistic and 58 Cognitive Engineering (DOLCE) (Masolo et al., 2003). D&amp;S is an ontology for representing a variety of reified contexts and states of affairs. In contrast to physical objects or events, the extensions of ontologies to the domain of non-physical objects pose a challenge to the ontology engineer. The reason for this lies in the fact that non-physical objects are taken to have meaning only in combination with some other ground entity. Accordingly, their logical representation is generally set at the level of theories or models and not at the level of concepts or relations (see Gangemi and Mika, 2003). It is, therefore, important to keep in mind th</context>
</contexts>
<marker>Masolo, Borgo, Gangemi, Guarino, Oltramari, 2003</marker>
<rawString>Masolo, C., Borgo, S., Gangemi, A., Guarino, N., and Oltramari, A. 2003. Ontology Library. WonderWeb Deliverable D18, I STC-CNR, Padova, Italy. http://wonderweb.semanticweb.org/deliverables/docu ments/D18.pdf.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V Micelli</author>
<author>R Porzel</author>
<author>A Gangemi</author>
</authors>
<booktitle>In Press. ECtoloG: Construction Grammar meets the Semantic Web. Proceedings of ICCG4,</booktitle>
<location>Tokyo, Japan.</location>
<marker>Micelli, Porzel, Gangemi, </marker>
<rawString>Micelli, V., Porzel, R., and Gangemi, A. In Press. ECtoloG: Construction Grammar meets the Semantic Web. Proceedings of ICCG4, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>A Bayesian Model of Human Sentence Processing (in preparation).</title>
<date>2005</date>
<contexts>
<context position="26996" citStr="Narayanan and Jurafsky, 2005" startWordPosition="4268" endWordPosition="4272">orm of Bayesian learning or model merging (Stolcke, 1994) or reinforcement learning (Sutton and Barto, 1998). Another important step that has to be employed is the (re)organization of the so-called constructicon, i.e. our inventory of constructions and schemas. These need to be merged, split or maybe thrown out again, depending on their utility, similarity etc. 5.3 Ambiguity Currently the problem of ambiguity is solved by endowing the analyzer with a chart and employing the semantic density algorithm described in (Bryant, 2004). In the future probabilistic reasoning frameworks as proposed by (Narayanan and Jurafsky, 2005) in combination with ontology-based coherence measures as proposed by (Loos and Porzel, 2004) constitute promising approaches for handling problems of construal, whether it be on a pragmatic, semantic, syntactic or phonological level. 6 Concluding Remarks In this paper we described our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism used in a question-answering system. We described necessary modules and presented first results and challenges in formalizing construction grammar. Furthermore, we pointed out our motivation for choosin</context>
</contexts>
<marker>Narayanan, Jurafsky, 2005</marker>
<rawString>Narayanan, S. and Jurafsky, D. 2005. A Bayesian Model of Human Sentence Processing (in preparation).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
</authors>
<title>Lecture Series given at Interdisciplinary College,</title>
<date>2006</date>
<location>Guenne, Lake Moehne,</location>
<marker>Narayanan, 2006</marker>
<rawString>Narayanan, S. 2006. Lecture Series given at Interdisciplinary College, Guenne, Lake Moehne, March 10th – 17th.</rawString>
</citation>
<citation valid="false">
<title>NTL: Neural Theory of Language http://www.icsi.berkeley.edu/NTL/</title>
<marker></marker>
<rawString>NTL: Neural Theory of Language http://www.icsi.berkeley.edu/NTL/</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="21393" citStr="Papineni et al., 2002" startWordPosition="3367" endWordPosition="3370">t of a corpus C’, where C’ = C + ε and ε is constituted by a set of new documents. This new corpus is not designed in an arbitrary manner. We search similar pages, adding add them to our original corpus C, as we expect the likelihood of still pretty good coverage together with some new constructions to be maximal, thereby enabling our incremental learning approach. The question emerging hereby is: what constitutes a similar web page? What, therefore, has to be explored are various similarity metrics, defining similarity in a concrete way and evaluate the results against human annotations (see Papineni et al., 2002). 4.1 Similarity Metric To be able to answer the question which texts are actually similar, similarity needs to be defined precisely. Different approaches could be employed, 61 i.e. regarding similarity in terms of syntactic or semantic phenomena or a combination of both. Since construction grammar makes no separation between syntax and semantics, phenomena that should be counted are both constructions and image schemas. As for holophrastic constructions this presents less of a challenge, we rather expect counting compositional ones being a ‘tough cookie’. To detect image schemas in natural te</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T. and Zhu, W. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Porzel</author>
<author>R Malaka</author>
</authors>
<title>A Task-based Framework for Ontology Learning, Population and Evaluation.</title>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Evaluation and Applications Frontiers. Artificial Intelligence and Applications Series,</booktitle>
<volume>123</volume>
<publisher>IOS Press.</publisher>
<marker>Porzel, Malaka, 2005</marker>
<rawString>Porzel, R. and Malaka, R. 2005. A Task-based Framework for Ontology Learning, Population and Evaluation. Ontology Learning from Text: Methods, Evaluation and Applications Frontiers. Artificial Intelligence and Applications Series, Vol. 123, IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Porzel</author>
<author>V Micelli</author>
<author>H Aras</author>
<author>H-P Zorn</author>
</authors>
<title>Tying the Knot: Ground Entities, Descriptions and Information Objects for Construction-based Information Extraction.</title>
<date>2006</date>
<booktitle>Proceedings of OntoLex</booktitle>
<publisher>In Press.</publisher>
<location>Genoa, Italy.</location>
<contexts>
<context position="3893" citStr="Porzel et al., 2006" startWordPosition="587" endWordPosition="590">rstanding, pages 57–64, New York City, June 2006. c�2006 Association for Computational Linguistics the learning process develops the input will gradually be extended to other domains. A description of the corpus and its selection process will be given in section 4. Section 5 provides an outlook on the learning paradigm, while the last section presents some future issues and conclusions. 2 Grammar Formalism The most crucial foundation that is needed to build a grammar learning system is a grammar formalism. Therefore, we are designing a new formalization of construction grammar called ECtoloG (Porzel et al., 2006; Micelli et al., in press). One existing formal computational model of construction grammar is the Embodied Construction Grammar (ECG) (Chang et al., 2002; Bergen and Chang, 2002), with its main focus being on language understanding and later simulation2. A congruent and parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and</context>
<context position="9866" citStr="Porzel et al. (2006)" startWordPosition="1513" endWordPosition="1516">ceptualization. Since a construction constitutes a pairing of form and meaning according to the original theory of construction grammar, both properties are of advantage for our ontological model. To keep the construction’s original structure, the form pole can be modeled with the help of the realized-by property4 while the meaning pole is built via the edns:expresses property. Both processes are described more detailed in the following section. Holophrastic Constructions The class of lexical constructions is modeled as a subclass of referringConstruction. Since it is a 3 For more details see Porzel et al. (2006). 4 We adhere to the convention to present both ontological properties, classes, and instances in italics. subclass of the class information-object it inherits the edns:expresses property. The referringConstruction class has a restriction on this property that denotes, that at least one of the values of the edns:expresses property is of type schema. Modeling this restriction is done by means of the built-in owl:someValuesFrom constraint. The restriction counts for all constructions that express a schema. It has no effect on the whole class of constructions, i.e. it is possible that there exist</context>
<context position="20564" citStr="Porzel et al., 2006" startWordPosition="3219" endWordPosition="3222">ss this challenge by means of intertwining the LingInfo model with the ECtoloG grammar model in such a way, that the computational and inferential properties of OWL-DL remain unchallenged. Another possibility could be obtaining linguistic information for lexical items through an external lexicon. 4 The Web as a Corpus The Seed Corpus C: The primary corpus C in this work is the portion of the World Wide Web confined to web pages containing natural language texts on soccer. To extract natural language texts out of web documents automatically we are using wrapper agents that fulfil this job (see Porzel et al., 2006). Our first goal is to build a grammar that can deal with all occurring language phenomena – i.e. both holophrastic and compositional ones – contained in that corpus C. Corpus C’: Next step is the development of a corpus C’, where C’ = C + ε and ε is constituted by a set of new documents. This new corpus is not designed in an arbitrary manner. We search similar pages, adding add them to our original corpus C, as we expect the likelihood of still pretty good coverage together with some new constructions to be maximal, thereby enabling our incremental learning approach. The question emerging her</context>
</contexts>
<marker>Porzel, Micelli, Aras, Zorn, 2006</marker>
<rawString>Porzel, R., Micelli, V., Aras, H., and Zorn, H.-P. 2006. Tying the Knot: Ground Entities, Descriptions and Information Objects for Construction-based Information Extraction. Proceedings of OntoLex 2006. In Press. Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="22731" citStr="Salton, 1989" startWordPosition="3580" endWordPosition="3581">aum, 1998) or other ontologies, which could assist in discovering the semantics of an unknown word with its corresponding schematic roles and the appropriate fillers. This or a similar methodology will be applied in the automatic acquisition process as well. Another important point is that some terms, or some constructions, need to get a higher relevance factor than others, which will highly depend on context. Such a relevance factor can rank terms or constructions according to their importance in the respective text. Ranking functions that can be examined are, e.g., the TF/IDF function (e.g. Salton, 1989) or other so called bag of words approaches. Term statistics in general is often used to determine a scalable measure of similarity between documents so it is said to be a good measure for topical closeness. Also part-of-speech statistics could be partly helpful in defining similarity of documents based on the ensuing type/token ratio. The following five steps need to be executed in determining the similarity of two documents: Step 1: Processing of the document D; analyzing the text and creating a list of all occurring words, constructions and/or image schemas. We assume that the best choice i</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G. 1989. Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley. Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Steels</author>
</authors>
<title>The Role of Construction Grammar in Fluid Language Grounding.</title>
<date>2005</date>
<publisher>Submitted. Elsevier Science.</publisher>
<contexts>
<context position="4258" citStr="Steels, 2005" startWordPosition="645" endWordPosition="646">s and conclusions. 2 Grammar Formalism The most crucial foundation that is needed to build a grammar learning system is a grammar formalism. Therefore, we are designing a new formalization of construction grammar called ECtoloG (Porzel et al., 2006; Micelli et al., in press). One existing formal computational model of construction grammar is the Embodied Construction Grammar (ECG) (Chang et al., 2002; Bergen and Chang, 2002), with its main focus being on language understanding and later simulation2. A congruent and parallel development has led to FCG which simulates the emergence of language (Steels, 2005). FCG is mainly based on the same primitives and operators as ECG is. We decided to employ ECG in our model mainly for historical reasons (see details about its development in the following section), adhering to its main primitives and operators, but employing the state of the art in knowledge representation. We adopt insights and mechanisms of FCG where applicable. 2.1 Construction Grammar and ECG One main difference between West Coast Grammar (Langacker, 1987; Lakoff, 1987) and East Coast Grammar (Chomsky, 1965; Katz, 1972) is the fact that construction grammar offers a vertical – not a hori</context>
</contexts>
<marker>Steels, 2005</marker>
<rawString>Steels, L. 2005. The Role of Construction Grammar in Fluid Language Grounding. Submitted. Elsevier Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Bayesian Learning of Probabilistic Language Models.</title>
<date>1994</date>
<tech>Ph.D. Thesis,</tech>
<institution>Computer Science Division, University of California at Berkeley.</institution>
<contexts>
<context position="26424" citStr="Stolcke, 1994" startWordPosition="4184" endWordPosition="4185">xture terms, but are employing the method on other kinds of terms, nevertheless, since the grammar learning paradigm in our approach is still in its baby shoes. 62 cerning the hypothesized semantics of individual forms in the case of multi-media information. 5.2 Learning Compositional Constructions Learning of compositional constructions still presents an issue which has not been accounted for, yet. What has already been proposed (Narayanan, inter alia) is that we have to assume a strong inductive bias and different learning algorithms, as e.g. some form of Bayesian learning or model merging (Stolcke, 1994) or reinforcement learning (Sutton and Barto, 1998). Another important step that has to be employed is the (re)organization of the so-called constructicon, i.e. our inventory of constructions and schemas. These need to be merged, split or maybe thrown out again, depending on their utility, similarity etc. 5.3 Ambiguity Currently the problem of ambiguity is solved by endowing the analyzer with a chart and employing the semantic density algorithm described in (Bryant, 2004). In the future probabilistic reasoning frameworks as proposed by (Narayanan and Jurafsky, 2005) in combination with ontolog</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Stolcke, A. 1994. Bayesian Learning of Probabilistic Language Models. Ph.D. Thesis, Computer Science Division, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="26475" citStr="Sutton and Barto, 1998" startWordPosition="4189" endWordPosition="4192">on other kinds of terms, nevertheless, since the grammar learning paradigm in our approach is still in its baby shoes. 62 cerning the hypothesized semantics of individual forms in the case of multi-media information. 5.2 Learning Compositional Constructions Learning of compositional constructions still presents an issue which has not been accounted for, yet. What has already been proposed (Narayanan, inter alia) is that we have to assume a strong inductive bias and different learning algorithms, as e.g. some form of Bayesian learning or model merging (Stolcke, 1994) or reinforcement learning (Sutton and Barto, 1998). Another important step that has to be employed is the (re)organization of the so-called constructicon, i.e. our inventory of constructions and schemas. These need to be merged, split or maybe thrown out again, depending on their utility, similarity etc. 5.3 Ambiguity Currently the problem of ambiguity is solved by endowing the analyzer with a chart and employing the semantic density algorithm described in (Bryant, 2004). In the future probabilistic reasoning frameworks as proposed by (Narayanan and Jurafsky, 2005) in combination with ontology-based coherence measures as proposed by (Loos and</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Sutton, R. and Barto, A. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talmy</author>
</authors>
<title>Force dynamics in language and cognition.</title>
<date>1988</date>
<journal>Cognitive Science,</journal>
<volume>12</volume>
<contexts>
<context position="2168" citStr="Talmy, 1988" startWordPosition="320" endWordPosition="321">, i.e. constructions. Constructions are the basic building blocks, posited by a particular grammar framework called Construction Grammar, and are defined as follows: “C is a construction iffdef C is a form-meaning pair &lt;Fi, Si&gt; such that some aspect of Fi or some aspect of Si is not strictly predictable from C’s component parts or from other previously established constructions.” (Goldberg, 1995:4). Construction Grammar originated from earlier insights in functional and usage-based models of language mainly supposed by cognitive linguists (e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay, 2002; Talmy, 1988; etc.). It has been devised to handle actually occurring natural language, which notoriously contains non-literal, elliptic, contextdependent, metaphorical or underspecified linguistic expressions. These phenomena still present a challenge for today’s natural language understanding systems. In addition to these advantages, we adhere to principles proposed by other constructivists as e.g. Tomasello (2003) that language acquisition is a usage-based phenomenon, contrasting approaches by generative grammarians who assume an innate grammar (Chomsky, 1981). Furthermore, we agree to the idea that gr</context>
</contexts>
<marker>Talmy, 1988</marker>
<rawString>Talmy, L. 1988. Force dynamics in language and cognition. Cognitive Science, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomasello</author>
</authors>
<title>Constructing a Language: A Usage-Based Theory of Language Acquisition.</title>
<date>2003</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="2576" citStr="Tomasello (2003)" startWordPosition="375" endWordPosition="376">Construction Grammar originated from earlier insights in functional and usage-based models of language mainly supposed by cognitive linguists (e.g. Lakoff, 1987; Fillmore and Kay, 1987; Kay, 2002; Talmy, 1988; etc.). It has been devised to handle actually occurring natural language, which notoriously contains non-literal, elliptic, contextdependent, metaphorical or underspecified linguistic expressions. These phenomena still present a challenge for today’s natural language understanding systems. In addition to these advantages, we adhere to principles proposed by other constructivists as e.g. Tomasello (2003) that language acquisition is a usage-based phenomenon, contrasting approaches by generative grammarians who assume an innate grammar (Chomsky, 1981). Furthermore, we agree to the idea that grammatical phenomena also contribute to the semantics of a sentence which is the reason why syntax cannot be defined independently of semantics of a grammar. A more detailed outline of construction grammar and the principles we adhered to in formalizing it will be given in sections 2 and 3. The input to the system is natural language data as found on the web, as e.g. in news tickers or blogs, initially res</context>
</contexts>
<marker>Tomasello, 2003</marker>
<rawString>Tomasello, M. 2003. Constructing a Language: A Usage-Based Theory of Language Acquisition. Harvard University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>