<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.171517">
<title confidence="0.952183">
FORUM ON CONNECTIONISM
Connectionist Models for Natural Language Processing
</title>
<note confidence="0.573084666666667">
David L. Waltz
Thinking Machines Corporation
245 First Street
</note>
<bodyText confidence="0.917024">
Cambridge, MA 02142
and
Program in Linguistics and Cognitive Science
Brandeis University
Brown 125
Waltham, MA 02254
</bodyText>
<sectionHeader confidence="0.78269" genericHeader="abstract">
PANELIST STATEMENT
</sectionHeader>
<bodyText confidence="0.998112285714286">
After an almost twenty year lull, there has been a
dramatic upsurge of interest in massively parallel models for
computation, descendants of perceptron and pandemonium
models, now dubbed &apos;connectionist models.&apos; Much of the
connectionist research has focused on models for natural lan-
guage processing. There have been three main reasons for
this increase in interest:
</bodyText>
<listItem confidence="0.9777796">
1. Scientific adequacy of the models
2. The availability of fine-grained parallel hardware
to run the models
3. The demonstration of powerful connectionist
learning models.
</listItem>
<bodyText confidence="0.999874678571429">
The scientific adequacy of models based on a small num-
ber of coarse-grained primitives (e.g. conceptual dependency),
popular in Al during the 70&apos;s, has been called into question
and substantially replaced by a current emphasis in much of
computational linguistics on lexicalist models (i.e., ones which
use words for representing concepts or meanings). However,
few people can doubt that words are too coarse, that they
have structure and properties and features. Connectionist
models offer very fine granularity; they can capture such
detail in a manner that still allows for tractable computation.
Such models also promise to make the integration of syntac-
tic, semantic, pragmatic, and memory models simpler and
more transparent.
Fine-grained hardware, such as the Connection Machine,
can allow models with millions of active elements, full
vocabularies, and rapid throughput, as well as powerful near-
term connectionist applications based on the use of associa-
tive memory and hardware support for interprocessor com-
munication. Meanwhile, connectionist learning models, such
as the Boltzmann Machine and its descendant, the backward
error propagation model, have demonstrated surprising
power in learning concepts from example; as for instance in
Sejnowski&apos;s NETtalk, which learned the pronunciation rules
for English from examples. The future promises yet more
surprising results as the concepts in even more radical
models, such as Minsky&apos;s Society of Minds model, are
digested and as new, even more powerful hardware becomes
available.
</bodyText>
<page confidence="0.9983">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.255754">
<title confidence="0.999382">FORUM ON CONNECTIONISM Connectionist Models for Natural Language Processing</title>
<author confidence="0.999979">David L Waltz</author>
<affiliation confidence="0.998952">Thinking Machines Corporation</affiliation>
<address confidence="0.9999355">245 First Street Cambridge, MA 02142</address>
<affiliation confidence="0.93484">and Program in Linguistics and Cognitive Science Brandeis University</affiliation>
<address confidence="0.993251">Brown 125 Waltham, MA 02254</address>
<abstract confidence="0.983854951219512">PANELIST STATEMENT After an almost twenty year lull, there has been a dramatic upsurge of interest in massively parallel models for computation, descendants of perceptron and pandemonium models, now dubbed &apos;connectionist models.&apos; Much of the connectionist research has focused on models for natural language processing. There have been three main reasons for this increase in interest: 1. Scientific adequacy of the models 2. The availability of fine-grained parallel hardware to run the models 3. The demonstration of powerful connectionist learning models. The scientific adequacy of models based on a small number of coarse-grained primitives (e.g. conceptual dependency), popular in Al during the 70&apos;s, has been called into question and substantially replaced by a current emphasis in much of computational linguistics on lexicalist models (i.e., ones which use words for representing concepts or meanings). However, few people can doubt that words are too coarse, that they have structure and properties and features. Connectionist models offer very fine granularity; they can capture such detail in a manner that still allows for tractable computation. Such models also promise to make the integration of syntactic, semantic, pragmatic, and memory models simpler and more transparent. Fine-grained hardware, such as the Connection Machine, can allow models with millions of active elements, full vocabularies, and rapid throughput, as well as powerful nearterm connectionist applications based on the use of associative memory and hardware support for interprocessor communication. Meanwhile, connectionist learning models, such as the Boltzmann Machine and its descendant, the backward error propagation model, have demonstrated surprising power in learning concepts from example; as for instance in Sejnowski&apos;s NETtalk, which learned the pronunciation rules for English from examples. The future promises yet more surprising results as the concepts in even more radical models, such as Minsky&apos;s Society of Minds model, are digested and as new, even more powerful hardware becomes available.</abstract>
<intro confidence="0.840614">185</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>