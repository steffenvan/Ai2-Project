<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000690">
<title confidence="0.990597">
Corpus-trained text generation for summarization
</title>
<author confidence="0.998085">
Min-Yen Kan and Kathleen R. McKeown
</author>
<affiliation confidence="0.9964955">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.981303">
New York, NY 10027, USA
</address>
<email confidence="0.999659">
{min,kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.995653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831333333333">
We explore how machine learning can be
employed to learn rulesets for the tradi-
tional modules of content planning and
surface realization. Our approach takes
advantage of semantically annotated cor-
pora to induce preferences for content
planning and constraints on realizations of
these plans. We applied this methodology
to an annotated corpus of indicative sum-
maries to derive constraint rules that can
assist in generating summaries for new,
unseen material.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984464788732">
Traditional natural language generation (NLG) ap-
proaches rely heavily on human experts to code dis-
course, semantic, and lexical resources. These re-
sources are used by systems to determine the dis-
course and sentential structure of the text, and its
word choice. This process can be very time con-
suming, involving experts that examine target docu-
ments and distill proper discourse plans and lexicons
that can produce the desired text.
In this paper, we investigate a novel approach
which automatically acquires such knowledge using
an annotated training corpus. Our method constructs
summarization system components by first learning
high-level content planning patterns and then learn-
ing low-level constraints on how to realize these
content plans in natural language. By applying this
approach to a training corpus consisting of docu-
ments belonging to the same domain and genre, the
system can generate a model for production of sim-
ilar texts. We show how this framework can be ap-
plied to automatic text summarization by using a
corpus of annotated bibliography entries as the train-
ing corpus to produce a model of indicative sum-
maries (Cremmins, 1982). These entries discuss dif-
ferent books but express the same reoccurring types
of information using different surface forms.
While the corpus from which plans and realiza-
tion patterns are acquired is restricted to input doc-
uments of the same genre that exhibit structural reg-
ularity, the learned plans can be applied to other do-
mains and genres. In this paper, we draw on input
from the genre of annotated bibliography entries, but
will apply the learned plans to generate summaries
of web-available consumer health texts.
A content plan consists of predicates specifying
what kind of information should occur in what order
in a generated summary. Each predicate will ulti-
mately be realized by one of the lexicalized phrases
that are associated with it. The research we present
focuses on learning rules that can predict the order
of predicates in a text and acquiring the lexicalized
phrases associated with each predicate, and is illus-
trated in Figure 1.
The acquisition of the content planning ruleset
works by finding occurrence patterns of predicates
in manually annotated training corpora. This mod-
ule determines what predicates are required or op-
tional in the plan, and uncovers ordering constraints
between them. Our approach in acquiring content
planning rules differs from related work in its inte-
gration of contextual constraints.
A second acquisition component for partial sur-
face realization considers frequent lexical depen-
dency patterns that are unique to specific predicates
(e.g., the Audience predicate in bibliography entries)
as predicate realizations and uncovers constraints
governing their usage. These patterns distinguish
between constituents that determine the semantics of
a predicate (which we call a predicate’s attributes)
as well as other associated text constituents that are
used to convey the information (e.g., surrounding
common phrases) in different surface forms.
In this paper, we first describe the role of indica-
tive summaries and show how their generation can
be viewed as an instance of our task. We then ex-
plain how our two acquisition algorithms function,
drawing examples from indicative summary genera-
tion. We examine the acquisition process for content
planning first, and partial surface realization second.
We show how these learned constraints can be ap-
plied to generate new summaries in the conclusion.
</bodyText>
<sectionHeader confidence="0.945809" genericHeader="method">
2 Application to summarization
</sectionHeader>
<bodyText confidence="0.99938582051282">
Automatic Text Summarization (ATS) is the process
of using a computerized algorithm to condense doc-
uments into a shorter form (for a current overview,
see (Mani and Maybury, 1999)). A particular type of
document summary that is the focus of our studies is
the indicative summary, a type of summary that hints
at a document’s content and does not substitute for
the full text. Card catalog entries from a library cata-
log and annotated bibliography entries are examples
of this type, and typically summarize a book in the
span of a few sentences. Such texts fall within a
single genre and thus fulfill our input prerequisite.
We have applied our corpus-trained technique to a
corpus of annotated bibliography entries and learned
what kinds of content (i.e., predicates) are included
and their ordering (the content planning module), as
well as learned how these predicates are expressed
(the partial surface realization module).
In a generation phase not detailed here, these
trained modules will produce multidocument sum-
maries for sets of consumer health texts that vary
greatly in discourse structure, length, topic and
wording (Kan et al., 2001b). The learned plans are
used to determine how to present these indicative
differences using text generation, in contrast to other
systems that use sentence extraction. Unlike other
generation systems that generate text from seman-
tic input, our summarization system uses the plans
to select content from full text and to generate vari-
ability in syntax and phrasing by choosing wordings
from variants of full phrases.
To investigate the viability of producing indica-
tive summaries using this approach, we collected a
corpus consisting of 2000 bibliography entries that
have been collected from various websites over var-
ious domains of knowledge. We processed the cor-
pus with Collins’ lexical dependency based parser
(Collins, 1996), and also added word stem informa-
tion using the Porter algorithm.
</bodyText>
<sectionHeader confidence="0.9852185" genericHeader="method">
3 Semantic annotation of summary
corpora
</sectionHeader>
<bodyText confidence="0.999969676470588">
Automatic semantic tagging of the corpus allows us
to infer what predicates are typically included in in-
dicative summaries. In our corpus of 2000 sum-
maries, we annotated a random 5% (= 100) of the
entries. We used the decision tree learner, rip-
per (Cohen, 1995), to induce a decision tree that
was used to automatically label a new corpus with
predicates, and used 5-fold cross validation to ensure
results were stable. We expanded on our previous in-
dicative summary tagset from (Kan et al., 2001a) to
a total of 24 predicates, detailed in Tables 1 and 2.
Nodes in the parse trees (corresponding to sen-
tences, phrases or individual words) in the train-
ing portion of the corpus were tagged by one of
the authors. Automatic tagging thus assigns one of
these 25 predicates (the 24 plus a default “none”)
to each node in the parse tree. By default, tagging
all nodes with “none” gives a high baseline accu-
racy of 99.47% (all 15,208 parse nodes in the 100
entries), but 0% accuracy on the 24 semantic predi-
cates. This was improved to 66% accuracy, as shown
in Table 3 by using features that represent the pred-
icate’s set of words, and relative and absolute po-
sition in the summary. We further introduced the
features that model local context of the preceeding
and succeeding predicates, and features that model
language genericity which marginally improved per-
formance. The genericity feature captures how uni-
form the language is for particular predicates across
instances. The idea was that the topicality predicates
(in Table 1) that express domain-specific knowledge
would vary in vocabulary across instances, but that
metadata predicates (in Table 2, such as Audience)
would have a more stable vocabulary.
</bodyText>
<figure confidence="0.991185193548387">
Content Planner Acquisition
Surface Realizer Acquisition
Content Planner KB Surface Realizer KB
Detail (26.8 of 29.7 instances)
(4.6 of 4.6 instances)
Topic Content Types (11.9 of 13.4 instances)
(4.8 of 6.9 instances)
Navigation
Overview
Overview
Contributor (2.8 of 2.8 instances)
Audience Content Types (2.02 of 3.37 inst...)
...
Content Types predicate KB:
Audience predicate KB:
... written for expressly for gifted children.
For adult readers.
For adult and children readers.
This book is intended for adult readers.
...
Overview
Author Contributor
&lt;Topic&gt; Presents and demonstrates the
connections between Alcott’s feminism and
her newly−discovered anonymous ... &lt;/Topic&gt;
&lt;Content Types&gt;Includes bibliography of all
known Alcott Thrillers, and a complete index
of terminology&lt;/ContentTypes&gt;&lt;Audience&gt;
For adult readers&lt;/Audience&gt;
Sample Annotated
Bibliography Entries
</figure>
<figureCaption confidence="0.9952415">
Figure 1: The content plan is a collection of probabilistic ordering constraints, while the surface realizer
consists of attribute values (underlined), and associated text that convey the predicate’s semantics.
</figureCaption>
<figure confidence="0.963779454545454">
Predicate # of occur- % entries hav-
rences ing predicate
Detail 139 47%
Quotations, extracted sentences, parts of a chronology, conclu-
sions
Overview 72 64%
(Generalized description of the entire resource, “This book is
about Louisa Alcott’s life.”)
Topic 34 28%
(High-level list of topics, e.g., “Topics include symptoms, treat-
ment ...”)
</figure>
<figureCaption confidence="0.7125725">
Table 1: Distribution of content-based topicality
predicates in the 100-entry annotated corpus.
</figureCaption>
<bodyText confidence="0.9999111">
Further analysis reveals that certain predicates are
recovered more often than others. For example, top-
icality predicates occur with less regularity and dis-
play more variability in their expression and thus
are more difficult to recover. Tags that occur sel-
domly are also not recovered by the current set of
features because of data sparseness. We feel that
an expansion of the fully annotated corpus or addi-
tional annotation with respect to these more sparse
tags would improve performance here.
</bodyText>
<sectionHeader confidence="0.961892" genericHeader="method">
4 Learning for the content planner
</sectionHeader>
<bodyText confidence="0.999844083333333">
The semantically annotated corpus is the basis for
learning the rule base for content planning. These
rules determine what the text and discourse structure
should look like, both in terms of a) content (“what
to say”) and b) its ordering (“where to say it”). We
examine each of these two tasks in turn.
Content determination. Documents in our in-
dicative summary corpus discuss different books
and thus have different predicate attribute values. In
addition, some predicates are present in some sum-
maries and not in others (e.g., author or editor). Ta-
bles 1 and 2 list the predicates and their frequency
in the training corpus. The presence of the predi-
cate may also be dependent on its value (e.g., Edi-
tion only occurs after the first edition).
Content ordering. The presence or absence of
particular predicates depends greatly on the pres-
ence or absence of its peers. Thus it is important to
encode content structuring information, represented
as local preferences rather than predefined schemas.
Duboue and McKeown (2001) detail an approach
for this problem which we initially tried that uses
techniques from computational biology, but which is
best suited for summaries with multiple instances of
the same predicate. Instead, we calculated bigram
statistics on pairs of adjacent predicates, recording
which occurred before another. These statistics are
used to find an ordering of the predicates that max-
imizes agreement with training observations. This
approach was also utilized in work done on premod-
ifier ordering (Shaw and Hatzivassiloglou,1999), in
which pairs of premodifiers were observed and used
to find ordering constraints. The technique is also
referred to as Majority Ordering in (Barzilay et al.,
2001), in which bigram orderings were elicited from
human subjects.
</bodyText>
<figure confidence="0.61319675">
(Background j Language) Overview Topic Size Media Types Authority
Collection Size (Comparison j Detail j Content Types j Navigation j
Query Relevance) Subjective Difficulty Author Purpose Style (Publisher j
Award j Readability j Audience j Contributor j Copyright)
</figure>
<figureCaption confidence="0.958807333333333">
Figure 2: Highest agreement full orderings of the
predicates using harmonic penalties. Predicates are
swappable where “I” occurs.
</figureCaption>
<bodyText confidence="0.990937">
We augmented the basic approach by expand-
</bodyText>
<table confidence="0.9854044">
Predicate # of oc- % entries
currences having
predicate
Media Type 55 48%
(e.g. “This book ...”, “A weblet ...”, “Spans 2 CDROMs”)
Author / Editor 43 27%
Content Types 41 29%
(e.g. “figures and tables”)
Subjective Assessment 36 24%
(e.g. “highly recommended”)
Authority / Authoritativeness 26 20%
Background/ Source 21 16%
(e.g. “based on a report”)
Navigation/ Internal Structure 16 11%
(e.g. “is organized into three parts”)
Collection Size 13 10%
Purpose 13 10%
Audience 12 12%
(e.g. “for adult readers”)
Contributor 12 12%
Name of the author of the annotated entry
Cross-resource Comparison 10 9%
(e.g., “similar to the other articles”
Size/Length 9 7%
Style 8 6%
(e.g., “in verse rhythm”, “showcased in soft watercolors”)
Query Relevance 4 3%
(text relevant to the theme of the collection)
Readability 4 4%
Difficulty 4 4%
(e.g., “requires no matrix algebra”)
Edition / Publication 3 3%
Language 2 2%
Copyright 2 1%
Award 2 1%
</table>
<tableCaption confidence="0.960419">
Table 2: Distribution of metadata and document-
derivable predicates in the 100-entry corpus.
</tableCaption>
<bodyText confidence="0.999668411764706">
ing the statistic to account for longer distance co-
occurences. Our statistic better models the fading
strength of context farther from the decision point
by utilizing information provided in all previous n
predicates. We constructed two backoff schemes:
one based on the harmonic series, the other based on
the quadratic. In both, a precedence relationship of
distance one (e.g. adjacent) is given a full strength
score, but a distance n relationship is given n unit
score in the harmonic and 1 in the quadratic. Each
particular pair of different predicates acculmulate
these weights as instances are found in the training
corpus, and a randomized hill-climbing algorithm is
used to find a maximally compliant ordering.
We use both the content determination and con-
tent ordering algorithms to generate a new summary
discourse plan. To do this, we examine which pred-
</bodyText>
<table confidence="0.9993056">
Accuracy Feature Type
majority +lexical +parse node +contextual
baseline &amp; positional &amp; genericity
24 predicates 0% 9% 66% 67%
24 + “none” 99.47% 99.51% 99.82% 99.83%
</table>
<tableCaption confidence="0.7958325">
Table 3: Summary semantic annotation accuracy,
using 5-fold C.V. Features are cumulative l to r.
</tableCaption>
<bodyText confidence="0.995716181818182">
icates are found in the library cataloguing database.
We use the content determination probabilities to
pick m number of predicates to be realized, where m
is the user-defined desired summary length. A ran-
domized algorithm selects n predicates from both
topicality (multiple selection allowed) and metadata
(selectable once only) catagories, biased for the per-
centages shown in Tables 1 and 2. The predicates
are ordered using either the harmonic or quadratic
penalized version of the algorithm and result in a
discourse plan for the summary.
</bodyText>
<sectionHeader confidence="0.985524" genericHeader="method">
5 Learning for partial surface realization
</sectionHeader>
<bodyText confidence="0.95829851948052">
While content planning concerns itself with the pres-
ence or absence of predicates and their ordering, the
task of surface realization is to convey the predi-
cates as natural language. In traditional NLG, sur-
face realization is often broken down into three sep-
arate tasks: (1) sentence planning, which takes in-
dividual messages or propositions and assigns them
to specific sentences and determines the sentences’
basic syntactic structure; (2) lexical choice, which
determines the words used, and (3) syntactic realiza-
tion, which uses a grammar to produce the sentence.
We are concerned with tasks 1 and 2. While there
has been work concentrating on inducing syntactic
generators (Langkilde, 2000; Bangalore and Ram-
bow, 2000; Ratnaparkhi, 2000; Varges and Mellish,
2001), for specific domains and general language,
there has been less work on other generation com-
ponents (Oh and Rudnicky, 2000).
Certain predicates, such as those that are content-
or topic-based (e.g., Overview and Detail features),
which are highly specific to the resource being sum-
marized are best handled by existing techniques of
sentence extraction or domain- and genre-specific
text grammars (Liddy, 1991; Rama and Srinivasan,
1993). However, many other predicates are more
domain-independent (e.g., Content Types and Audi-
ence). We focus on these metadata predicates, as
they comprise a large portion (57%) of the entries.
In our framework, a predicate has two compo-
nents (also shown in right hand side of Figure 1):
the attribute value itself (“adult readers”) and the
associated text that is used to cast this information
in the semantic role dictated by the predicate (“This
book is meant for &lt;attribute value&gt;” for the Audi-
ence predicate)1. In a stemmed dependency frame-
work, the attribute value is the child and the associ-
ated text the head of a dependency relationship (e.g.,
stemmed: “this book be mean for”head —� “adult
reader”,hied). In this framework, surface realization
begins with the process of choosing the most appro-
priate associated text among alternatives found in
the training text, given input attribute values. The
associated text and attributes are then realized as
sentences, phrases or words, which are combined to
form a new text by a sentence planner.
The first task is to differentiate attribute val-
ues from the associated text in the training corpus.
Our starting point is the collection of sentences or
phrases in the annotated corpus that are instances
of the same predicate (e.g., a collection of Audi-
ence sentences). Our analysis of these texts indi-
cates that attribute values are highly flexible in lo-
cation within the texts and in grammatical struc-
ture. In order to encode this flexibility, we capital-
ize on the stemmed, lexical dependency framework
used in parsing the entries. The framework conflates
phrases such as “index included”, “includes an in-
dex” and “inclusion of indices” (found in instances
of the Content Types predicate) together into a single
stemmed lexical dependency pair of “include”head
—� “index”,hied. For each collection of predicate in-
stances, our strategy first identifies highly frequent
(threshold = x¯ +2c,,) stemmed lexical dependency
pairs. Frequent child lexical items in the depen-
dency pair are potential attribute values in the sen-
tence (“index”,hied as an attribute value for Content
Types, as seen in Figure 3, #3). From this set, we
remove frequent dependency pairs that occur with
other predicates; this prevents frequent, corpus-wide
dependencies such as “book”head —� “this”,hied from
appearing as potential attribute values, as they are
1Our notion of “predicate” is identical to (Varges and Mel-
lish, 2001) notion of “slots” or “tags”; similarly, our “attribute
values” are equivalent to their term “fillers”.
not exclusively frequent within a single predicate (so
#1 and #2 are not attribute values for any of the 24
semantic predicates).
</bodyText>
<figure confidence="0.963938090909091">
#1, Topicality, bookhead -+ thischild :
(e.g., “This book discusses Alcott’s works ...”, “this book covers the theories”)
#2, Content Types,bookhead thischild :
(e.g., “This book also comes with a biography”, “is discussed in this book”)
#3, Content Types, includehead -+ indexchild :
(e.g.,“Indices are included”, “includes an index”, “The book includes an index”)
...
[below threshold]
#30, Content Types, includehead -+ figurechild:
(e.g., “Includes figures”, “figures and tables are included”)
...
</figure>
<figureCaption confidence="0.95686525">
Figure 3: A portion of the list of stemmed lexical
dependencies for various predicates, sorted by fre-
quency. Source snippets are given after the depen-
dency pair.
</figureCaption>
<bodyText confidence="0.994867230769231">
This method gave good (94.8%) precision, but
poor recall, due to the high threshold. To increase
recall, we noted that heads of these frequent de-
pendency pairs also served as heads in dependen-
cies with other less frequent child words (depen-
dency head “include” supports the frequent attribute
value “index” in #3, but also less frequent one such
as “figure” in #30). Including such pairs recovered
these less frequent attribute values (95% additional
attributes were recovered) with a minimal increase
in error (92.2% precision). Thus, in the simplified
example in Figure 3, “index” and “figure” are at-
tribute values for the Content Types predicate.
Text not identified as attribute values are labeled
associated text. From our perspective, these lexical
dependencies embody lexical choice and resulting
syntactic choices internal to the predicate: the alter-
native forms of the associated texts help to convey
the same semantic information (the attribute value)
but with different words and syntactic structures.
Which alternative is selected is subject to a number
of parameters, including stylistic ones. For the sur-
face realizer to make these decisions thus requires
that we learn stylistic constraints and apply them in
the generation process. We break this process down
into three subtasks:
</bodyText>
<listItem confidence="0.5478725">
• Identify and encode linguistic features. We
examined several linguistic features for their
potential to predict which alternative associated
text are used in particular contexts. Related
</listItem>
<bodyText confidence="0.9998302">
work on descriptive appositive language reuse
(Radev, 1998), and genre identification (Biber,
1989; Karlgren and Cutting, 1994; Kessler et
al., 1997) defines a large set of basic features to
use such as character and word-level features,
and positional and contextual features. We im-
plemented a total of 27 of these features in our
work to test their efficacy in identifying appro-
priate constraints.
Our problem is related to these previous stud-
ies, but differs in some key respects. Choosing
descriptions often involves choosing between
descriptions that convey different semantic in-
formation (“Clinton” as “senator” versus “pres-
ident”), whereas our associated texts generally
convey the same information but realize it dif-
ferently, similar to paraphrasing (Barzilay and
McKeown, 2001). Genre identification differs
from our problem mostly in scale; whereas
whole texts are input to the genre categoriza-
tion process, in our problem we have access
mostly to single sentences or clauses.
Genre identification work primarily focuses on
surface level features rather than assuming a
full parse of the text. Since we have access
to a full parse, we also model features that we
believe have a stylistic impact. We introduce
an additional 8 features that look for different
types of adjunct constructions, relative clause
construction, and passive constructions.
</bodyText>
<listItem confidence="0.787308714285714">
• Use machine learning to predict linguistic
features for the target predicate. We em-
ployed the decision tree learner, ripper, to
determine which features play a role in pre-
dicting the characteristics of the target associ-
ated text. We compute values of all 35 (27 +
8 new) features for four different contexts sur-
</listItem>
<bodyText confidence="0.994970421052632">
rounding the target predicate: the (1) previous
and (2) next predicates, as well as (3) global
and (4) composite features that combine other
basic features (e.g., difference between previ-
ous and next sentence length). This total of 4 x
35 = 140 features, are used to predict the val-
ues of the same features in the target predicate.
Since there are a total of 35 features, this results
in 35 different machine learners that separately
predict one of the 35 features of the target pred-
icate, shown graphically in Figure 4.
Table 4 counts the occurrences of these 35 fea-
tures in the induced ripper ruleset that is
used to structure the target predicate. This mea-
sure can be used to assess their relative impor-
tance. These results show that our additional
features are useful in modeling stylistics but
that the literature contributes significant fea-
tures as well.
</bodyText>
<table confidence="0.998612">
2 fea- 20 fea- 5 char-level 8 new
tures from tures from cues from features
(Radev98) (Karlgren&amp; (KesslerE-
Cutting94) tAl97)
20 25 7 24
</table>
<tableCaption confidence="0.859703">
Table 4: The number of features used by ripper
to determine the output feature of target predicates.
</tableCaption>
<bodyText confidence="0.980872285714286">
• Find best possible match between predicted
features and candidate predicate. The final
step is to use the predicted features of the predi-
cate to match a most appropriate associated text
to convey the predicate.
If the predicted features exactly match a train-
ing example’s associated text then the selec-
tion is trivial. However in practice this rarely
occurs and we must select from the avail-
able associated texts. As the choice is limited
to whole associated texts and not constituents
as in other stochastic approaches (Langkilde,
2000; Varges and Mellish, 2001), this search
process is constrained and does not present
an efficiency problem. For numerical features
(e.g., number of words), we use a normalized
difference between the desired value and avail-
able values from the training associated text to
calculate its goodness of fit. For set valued fea-
tures (e.g. parse node type: NP versus PP), the
feature either matches or does not (1 or 0).
Our algorithm weighs all features equally, and
an associated text is chosen such that the
matching score is maximized. In future work,
we will use human judgments of the realized
predicates in context to induce more appropri-
ate feature weights, and will evaluate this mod-
ule’s efficacy for generating appropriate text.
</bodyText>
<figure confidence="0.824497714285714">
Summary
140 input features: 35 from each of four contexts
prevSentLen, prevMaxDpth, prevNumWds, ... metaDiffNumAdvs, SentLen.
. . .
prevSentLen, prevMaxDpth, prevNumWds, ... metaDiffNumAdvs, TumAdvs.
classes: a feature of the target semantic slot
Machine Learning Vectors
</figure>
<figureCaption confidence="0.9997815">
Figure 4: Machine learning architecture for the features of the target predicate, with sample tuples shown.
Figure 5: Post content planner architecture with summary
</figureCaption>
<figure confidence="0.993278580645161">
Extracted topical sentences
Realized Predicates
: discusses the
Topics
symptoms and treatm ...
Author
: Dr. Lowenstein
Edition : third edition
Audience : for adult readers
Sentence Extractor
Lexical Dependency
Surface KB Acquirer
Topics : Ch. 1 − Introduction, 2
: Dr. Lowenstein
: 3 Cataloguing DB
Content Plan
Author
Edition
Surface
Realizer
KB
predicates
Generic
predicates
Target Generated Summary
Dr. Lowenstein discusses the symptoms and
treatment of leukemia in this popular third
edition of the The Leukemia Handbook...
Topicality
Sentence
Planner
</figure>
<sectionHeader confidence="0.772331" genericHeader="method">
6 Using the rulesets for generation
</sectionHeader>
<bodyText confidence="0.999971428571429">
This approach in this paper constructs rulesets that
capture patterns at both the content planning and sur-
face realization levels. While the work here does not
constitute an end-to-end generation system, the al-
gorithms for learning these knowledge bases are im-
plemented, and can be applied along with a source
of predicate attributes to generate new texts. In
our application of indicative summarization genera-
tion, cataloging records (such as the U.S. Machine
Readable Cataloging (MARC) guidelines (Library
of Congress, 2000)) can provide these predicate at-
tributes. Attribute values from the resource’s cata-
loguing record would interact with the surface real-
ization constraints to produce a set of sentences or
phrases that correspond to each predicate. A sen-
tence planning module could take the realized frag-
ments and organize them into sentences (accord-
ing to the content plan) to form a final generated
text. Figure 5 shows how this portion of the process
would work in conjunction with sentence extraction
to find sentences for any topicality predicates.
</bodyText>
<sectionHeader confidence="0.998337" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999983657894737">
In previous work, we performed a task-based eval-
uation of a rule-based indicative summary genera-
tion system. The study suggested that users were
more satisfied with this approach to presenting in-
formation retrieval results over other visualization
approaches (Kan and Klavans, 2002). The current
system described in this paper improves the system
with additional flexibility and variability in genera-
tion that is a key characteristic of human-produced
natural language. Task-based evaluation of this cur-
rent generation system is currently being done to as-
sess its effectiveness.
In this paper, we have described a new architec-
ture for NLG that takes advantage of annotated cor-
pora. Our method takes a new approach to NLG
by using machine learning to capture semantic and
stylistic constraints that are traditionally hand coded
by human experts. The induced constraints can be
used by traditional content planning and surface re-
alization in making their decisions.
To the best of our knowledge, our approach is the
first to use lexical dependencies in combination with
language constructs at different levels of granular-
ity (word, phrase, sentence) to allow for flexibility
in lexical and syntactic choice. We have explored
how this new approach can be utilized for the appli-
cation of indicative summary generation, which can
summarize a book with a few sentences. We have
detailed what types of semantic information (predi-
cates) are present in indicative summaries and how a
NLG architecture can utilize these resources to gen-
erate new summaries of unseen material.
We feel that this approach can be used to generate
texts in other domains where the target texts exhibit
strong regularity in content and its ordering, and that
this approach can be paired with other techniques
(such as heuristic-based sentence extraction) to ap-
ply to a wide range of texts.
</bodyText>
<sectionHeader confidence="0.999146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999645554216867">
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proc. of the 18th Intl. Conf. on Computational Lin-
guistics (COLING 2000), Saarbrucken, Germany.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proc.
ofACL/EACL 01.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2001. Sentence ordering in multidocument sum-
marization. In Proc. of Human Language Technology
’01, San Diego, CA, USA.
Douglas Biber. 1989. A typology of English texts. Lin-
guistics, 27:3–43.
William W. Cohen. 1995. Fast effective rule induction.
In Proc. 12th Intl. Conf. on Machine Learning, pages
115–123. Morgan Kaufmann.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc. of the
34th ACL, Santa Cruz.
Edward T. Cremmins. 1982. Art of Abstracting. ISI
Press.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In Proc. of the ACL-EACL 2001,
Toulouse, France.
Min-Yen Kan and Judith L. Klavans. 2002. Using librar-
ian techniques in automatic text summarization for in-
formation retrieval. In Proc. of Joint Conf. on Digital
Libraries, Portland, Oregon, USA, July.
Min-Yen Kan, Kathleen R. McKeown, and Judith L.
Klavans. 2001a. Applying natural language genera-
tion to indicative summarization. In Proc. of 8th Eu-
ropean Workshop on Natural Language Generation,
Toulouse, France.
Min-Yen Kan, Kathy McKeown, and Judith Klavans.
2001b. Domain-specific informative and indicative
summarization for information retrieval. In Proc.
of the Document Understanding Conference (DUC),
pages 19–26, New Orleans, USA.
Jussi Karlgren and Douglass Cutting. 1994. Recogniz-
ing text genres with simple metrics using discriminant
analysis. In Proc. of COLING ’94, Kyoto, Japan.
Brett Kessler, Geoffrey Nunberg, and Hinrich Sch¨utze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Association of Computational Linguis-
tics (ACL ’97), pages 32–38, Madrid, Spain.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In 6th Applied Natural Language Pro-
cessing Conf. (ANLP’2000), pages 170–177, Seattle,
Washington, USA.
Library of Congress. 2000. Marc 21 format for classifi-
cation data : including guidelines for content designa-
tion. Washington, D.C., USA. ISN 0660179903.
Elizabeth Liddy. 1991. The discourse-level structure of
empirical abstracts: An exploratory study. Informa-
tion Processing and Management, 27(1):55–81.
Inderjeet Mani and Mark Maybury, editors. 1999. Ad-
vances in Automatic Text Summarization. MIT Press.
Alice Oh and A Rudnicky. 2000. Stochastic lan-
guage generation for spoken dialogue systems. In
Proc. of the ANLP/NAACL 2000 Wrkshp. on Conver-
sational Systems, pages 27–32, Seattle, Washington,
USA, May.
Dragomir R. Radev. 1998. Learning correlations be-
tween linguistic indicators and semantic constraints:
Reuse of context-dependent descriptions of entities. In
Proc. of COLING/ACL 98, Montreal, Canada.
D. V. Rama and Padmini Srinivasan. 1993. An investi-
gaton of content representation using text grammars.
ACM Transactions on Information Systems, 11(1):51–
75, January.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proc. of the 6th
Applied Natural Language Processing Conf. (ANLP-
NAACL 2000), pages 194–201, Seattle, Washington,
USA.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proc. of the 37th Asso-
ciationfor Computational Linguistics, pages 135–143,
College Park, Maryland, USA, June.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In Proc. ofNAACL
01.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770917">
<title confidence="0.99961">Corpus-trained text generation for summarization</title>
<author confidence="0.999758">R Kan</author>
<affiliation confidence="0.999954">Department of Computer</affiliation>
<address confidence="0.900274">Columbia New York, NY 10027,</address>
<abstract confidence="0.997052230769231">We explore how machine learning can be employed to learn rulesets for the traditional modules of content planning and surface realization. Our approach takes advantage of semantically annotated corpora to induce preferences for content planning and constraints on realizations of these plans. We applied this methodology to an annotated corpus of indicative summaries to derive constraint rules that can assist in generating summaries for new, unseen material.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th Intl. Conf. on Computational Linguistics (COLING</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="15632" citStr="Bangalore and Rambow, 2000" startWordPosition="2439" endWordPosition="2443">ring, the task of surface realization is to convey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these meta</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proc. of the 18th Intl. Conf. on Computational Linguistics (COLING 2000), Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proc. ofACL/EACL 01.</booktitle>
<contexts>
<context position="21732" citStr="Barzilay and McKeown, 2001" startWordPosition="3384" endWordPosition="3387"> large set of basic features to use such as character and word-level features, and positional and contextual features. We implemented a total of 27 of these features in our work to test their efficacy in identifying appropriate constraints. Our problem is related to these previous studies, but differs in some key respects. Choosing descriptions often involves choosing between descriptions that convey different semantic information (“Clinton” as “senator” versus “president”), whereas our associated texts generally convey the same information but realize it differently, similar to paraphrasing (Barzilay and McKeown, 2001). Genre identification differs from our problem mostly in scale; whereas whole texts are input to the genre categorization process, in our problem we have access mostly to single sentences or clauses. Genre identification work primarily focuses on surface level features rather than assuming a full parse of the text. Since we have access to a full parse, we also model features that we believe have a stylistic impact. We introduce an additional 8 features that look for different types of adjunct constructions, relative clause construction, and passive constructions. • Use machine learning to pre</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proc. ofACL/EACL 01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence ordering in multidocument summarization.</title>
<date>2001</date>
<booktitle>In Proc. of Human Language Technology ’01,</booktitle>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="11672" citStr="Barzilay et al., 2001" startWordPosition="1816" endWordPosition="1819">hniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou,1999), in which pairs of premodifiers were observed and used to find ordering constraints. The technique is also referred to as Majority Ordering in (Barzilay et al., 2001), in which bigram orderings were elicited from human subjects. (Background j Language) Overview Topic Size Media Types Authority Collection Size (Comparison j Detail j Content Types j Navigation j Query Relevance) Subjective Difficulty Author Purpose Style (Publisher j Award j Readability j Audience j Contributor j Copyright) Figure 2: Highest agreement full orderings of the predicates using harmonic penalties. Predicates are swappable where “I” occurs. We augmented the basic approach by expandPredicate # of oc- % entries currences having predicate Media Type 55 48% (e.g. “This book ...”, “A w</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2001</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2001. Sentence ordering in multidocument summarization. In Proc. of Human Language Technology ’01, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>A typology of English texts.</title>
<date>1989</date>
<journal>Linguistics,</journal>
<pages>27--3</pages>
<contexts>
<context position="21044" citStr="Biber, 1989" startWordPosition="3281" endWordPosition="3282">ifferent words and syntactic structures. Which alternative is selected is subject to a number of parameters, including stylistic ones. For the surface realizer to make these decisions thus requires that we learn stylistic constraints and apply them in the generation process. We break this process down into three subtasks: • Identify and encode linguistic features. We examined several linguistic features for their potential to predict which alternative associated text are used in particular contexts. Related work on descriptive appositive language reuse (Radev, 1998), and genre identification (Biber, 1989; Karlgren and Cutting, 1994; Kessler et al., 1997) defines a large set of basic features to use such as character and word-level features, and positional and contextual features. We implemented a total of 27 of these features in our work to test their efficacy in identifying appropriate constraints. Our problem is related to these previous studies, but differs in some key respects. Choosing descriptions often involves choosing between descriptions that convey different semantic information (“Clinton” as “senator” versus “president”), whereas our associated texts generally convey the same info</context>
</contexts>
<marker>Biber, 1989</marker>
<rawString>Douglas Biber. 1989. A typology of English texts. Linguistics, 27:3–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. 12th Intl. Conf. on Machine Learning,</booktitle>
<pages>115--123</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6457" citStr="Cohen, 1995" startWordPosition="1007" endWordPosition="1008">his approach, we collected a corpus consisting of 2000 bibliography entries that have been collected from various websites over various domains of knowledge. We processed the corpus with Collins’ lexical dependency based parser (Collins, 1996), and also added word stem information using the Porter algorithm. 3 Semantic annotation of summary corpora Automatic semantic tagging of the corpus allows us to infer what predicates are typically included in indicative summaries. In our corpus of 2000 summaries, we annotated a random 5% (= 100) of the entries. We used the decision tree learner, ripper (Cohen, 1995), to induce a decision tree that was used to automatically label a new corpus with predicates, and used 5-fold cross validation to ensure results were stable. We expanded on our previous indicative summary tagset from (Kan et al., 2001a) to a total of 24 predicates, detailed in Tables 1 and 2. Nodes in the parse trees (corresponding to sentences, phrases or individual words) in the training portion of the corpus were tagged by one of the authors. Automatic tagging thus assigns one of these 25 predicates (the 24 plus a default “none”) to each node in the parse tree. By default, tagging all node</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast effective rule induction. In Proc. 12th Intl. Conf. on Machine Learning, pages 115–123. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th ACL,</booktitle>
<location>Santa Cruz.</location>
<contexts>
<context position="6088" citStr="Collins, 1996" startWordPosition="944" endWordPosition="945">trast to other systems that use sentence extraction. Unlike other generation systems that generate text from semantic input, our summarization system uses the plans to select content from full text and to generate variability in syntax and phrasing by choosing wordings from variants of full phrases. To investigate the viability of producing indicative summaries using this approach, we collected a corpus consisting of 2000 bibliography entries that have been collected from various websites over various domains of knowledge. We processed the corpus with Collins’ lexical dependency based parser (Collins, 1996), and also added word stem information using the Porter algorithm. 3 Semantic annotation of summary corpora Automatic semantic tagging of the corpus allows us to infer what predicates are typically included in indicative summaries. In our corpus of 2000 summaries, we annotated a random 5% (= 100) of the entries. We used the decision tree learner, ripper (Cohen, 1995), to induce a decision tree that was used to automatically label a new corpus with predicates, and used 5-fold cross validation to ensure results were stable. We expanded on our previous indicative summary tagset from (Kan et al., </context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proc. of the 34th ACL, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward T Cremmins</author>
</authors>
<title>Art of Abstracting.</title>
<date>1982</date>
<publisher>ISI Press.</publisher>
<contexts>
<context position="1825" citStr="Cremmins, 1982" startWordPosition="278" endWordPosition="279">tated training corpus. Our method constructs summarization system components by first learning high-level content planning patterns and then learning low-level constraints on how to realize these content plans in natural language. By applying this approach to a training corpus consisting of documents belonging to the same domain and genre, the system can generate a model for production of similar texts. We show how this framework can be applied to automatic text summarization by using a corpus of annotated bibliography entries as the training corpus to produce a model of indicative summaries (Cremmins, 1982). These entries discuss different books but express the same reoccurring types of information using different surface forms. While the corpus from which plans and realization patterns are acquired is restricted to input documents of the same genre that exhibit structural regularity, the learned plans can be applied to other domains and genres. In this paper, we draw on input from the genre of annotated bibliography entries, but will apply the learned plans to generate summaries of web-available consumer health texts. A content plan consists of predicates specifying what kind of information sho</context>
</contexts>
<marker>Cremmins, 1982</marker>
<rawString>Edward T. Cremmins. 1982. Art of Abstracting. ISI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Empirically estimating order constraints for content planning in generation.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL-EACL 2001,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="10975" citStr="Duboue and McKeown (2001)" startWordPosition="1711" endWordPosition="1714">hus have different predicate attribute values. In addition, some predicates are present in some summaries and not in others (e.g., author or editor). Tables 1 and 2 list the predicates and their frequency in the training corpus. The presence of the predicate may also be dependent on its value (e.g., Edition only occurs after the first edition). Content ordering. The presence or absence of particular predicates depends greatly on the presence or absence of its peers. Thus it is important to encode content structuring information, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou,1999), in which pairs of premodifiers were observed and used to find orderi</context>
</contexts>
<marker>Duboue, McKeown, 2001</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2001. Empirically estimating order constraints for content planning in generation. In Proc. of the ACL-EACL 2001, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Judith L Klavans</author>
</authors>
<title>Using librarian techniques in automatic text summarization for information retrieval.</title>
<date>2002</date>
<booktitle>In Proc. of Joint Conf. on Digital Libraries,</booktitle>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="27367" citStr="Kan and Klavans, 2002" startWordPosition="4282" endWordPosition="4285">nd to each predicate. A sentence planning module could take the realized fragments and organize them into sentences (according to the content plan) to form a final generated text. Figure 5 shows how this portion of the process would work in conjunction with sentence extraction to find sentences for any topicality predicates. 7 Conclusion In previous work, we performed a task-based evaluation of a rule-based indicative summary generation system. The study suggested that users were more satisfied with this approach to presenting information retrieval results over other visualization approaches (Kan and Klavans, 2002). The current system described in this paper improves the system with additional flexibility and variability in generation that is a key characteristic of human-produced natural language. Task-based evaluation of this current generation system is currently being done to assess its effectiveness. In this paper, we have described a new architecture for NLG that takes advantage of annotated corpora. Our method takes a new approach to NLG by using machine learning to capture semantic and stylistic constraints that are traditionally hand coded by human experts. The induced constraints can be used b</context>
</contexts>
<marker>Kan, Klavans, 2002</marker>
<rawString>Min-Yen Kan and Judith L. Klavans. 2002. Using librarian techniques in automatic text summarization for information retrieval. In Proc. of Joint Conf. on Digital Libraries, Portland, Oregon, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Kathleen R McKeown</author>
<author>Judith L Klavans</author>
</authors>
<title>Applying natural language generation to indicative summarization.</title>
<date>2001</date>
<booktitle>In Proc. of 8th European Workshop on Natural Language Generation,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="5357" citStr="Kan et al., 2001" startWordPosition="830" endWordPosition="833">few sentences. Such texts fall within a single genre and thus fulfill our input prerequisite. We have applied our corpus-trained technique to a corpus of annotated bibliography entries and learned what kinds of content (i.e., predicates) are included and their ordering (the content planning module), as well as learned how these predicates are expressed (the partial surface realization module). In a generation phase not detailed here, these trained modules will produce multidocument summaries for sets of consumer health texts that vary greatly in discourse structure, length, topic and wording (Kan et al., 2001b). The learned plans are used to determine how to present these indicative differences using text generation, in contrast to other systems that use sentence extraction. Unlike other generation systems that generate text from semantic input, our summarization system uses the plans to select content from full text and to generate variability in syntax and phrasing by choosing wordings from variants of full phrases. To investigate the viability of producing indicative summaries using this approach, we collected a corpus consisting of 2000 bibliography entries that have been collected from variou</context>
<context position="6692" citStr="Kan et al., 2001" startWordPosition="1045" endWordPosition="1048">llins, 1996), and also added word stem information using the Porter algorithm. 3 Semantic annotation of summary corpora Automatic semantic tagging of the corpus allows us to infer what predicates are typically included in indicative summaries. In our corpus of 2000 summaries, we annotated a random 5% (= 100) of the entries. We used the decision tree learner, ripper (Cohen, 1995), to induce a decision tree that was used to automatically label a new corpus with predicates, and used 5-fold cross validation to ensure results were stable. We expanded on our previous indicative summary tagset from (Kan et al., 2001a) to a total of 24 predicates, detailed in Tables 1 and 2. Nodes in the parse trees (corresponding to sentences, phrases or individual words) in the training portion of the corpus were tagged by one of the authors. Automatic tagging thus assigns one of these 25 predicates (the 24 plus a default “none”) to each node in the parse tree. By default, tagging all nodes with “none” gives a high baseline accuracy of 99.47% (all 15,208 parse nodes in the 100 entries), but 0% accuracy on the 24 semantic predicates. This was improved to 66% accuracy, as shown in Table 3 by using features that represent </context>
</contexts>
<marker>Kan, McKeown, Klavans, 2001</marker>
<rawString>Min-Yen Kan, Kathleen R. McKeown, and Judith L. Klavans. 2001a. Applying natural language generation to indicative summarization. In Proc. of 8th European Workshop on Natural Language Generation, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Kathy McKeown</author>
<author>Judith Klavans</author>
</authors>
<title>Domain-specific informative and indicative summarization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. of the Document Understanding Conference (DUC),</booktitle>
<pages>19--26</pages>
<location>New Orleans, USA.</location>
<contexts>
<context position="5357" citStr="Kan et al., 2001" startWordPosition="830" endWordPosition="833">few sentences. Such texts fall within a single genre and thus fulfill our input prerequisite. We have applied our corpus-trained technique to a corpus of annotated bibliography entries and learned what kinds of content (i.e., predicates) are included and their ordering (the content planning module), as well as learned how these predicates are expressed (the partial surface realization module). In a generation phase not detailed here, these trained modules will produce multidocument summaries for sets of consumer health texts that vary greatly in discourse structure, length, topic and wording (Kan et al., 2001b). The learned plans are used to determine how to present these indicative differences using text generation, in contrast to other systems that use sentence extraction. Unlike other generation systems that generate text from semantic input, our summarization system uses the plans to select content from full text and to generate variability in syntax and phrasing by choosing wordings from variants of full phrases. To investigate the viability of producing indicative summaries using this approach, we collected a corpus consisting of 2000 bibliography entries that have been collected from variou</context>
<context position="6692" citStr="Kan et al., 2001" startWordPosition="1045" endWordPosition="1048">llins, 1996), and also added word stem information using the Porter algorithm. 3 Semantic annotation of summary corpora Automatic semantic tagging of the corpus allows us to infer what predicates are typically included in indicative summaries. In our corpus of 2000 summaries, we annotated a random 5% (= 100) of the entries. We used the decision tree learner, ripper (Cohen, 1995), to induce a decision tree that was used to automatically label a new corpus with predicates, and used 5-fold cross validation to ensure results were stable. We expanded on our previous indicative summary tagset from (Kan et al., 2001a) to a total of 24 predicates, detailed in Tables 1 and 2. Nodes in the parse trees (corresponding to sentences, phrases or individual words) in the training portion of the corpus were tagged by one of the authors. Automatic tagging thus assigns one of these 25 predicates (the 24 plus a default “none”) to each node in the parse tree. By default, tagging all nodes with “none” gives a high baseline accuracy of 99.47% (all 15,208 parse nodes in the 100 entries), but 0% accuracy on the 24 semantic predicates. This was improved to 66% accuracy, as shown in Table 3 by using features that represent </context>
</contexts>
<marker>Kan, McKeown, Klavans, 2001</marker>
<rawString>Min-Yen Kan, Kathy McKeown, and Judith Klavans. 2001b. Domain-specific informative and indicative summarization for information retrieval. In Proc. of the Document Understanding Conference (DUC), pages 19–26, New Orleans, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Douglass Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In Proc. of COLING ’94,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="21072" citStr="Karlgren and Cutting, 1994" startWordPosition="3283" endWordPosition="3286">s and syntactic structures. Which alternative is selected is subject to a number of parameters, including stylistic ones. For the surface realizer to make these decisions thus requires that we learn stylistic constraints and apply them in the generation process. We break this process down into three subtasks: • Identify and encode linguistic features. We examined several linguistic features for their potential to predict which alternative associated text are used in particular contexts. Related work on descriptive appositive language reuse (Radev, 1998), and genre identification (Biber, 1989; Karlgren and Cutting, 1994; Kessler et al., 1997) defines a large set of basic features to use such as character and word-level features, and positional and contextual features. We implemented a total of 27 of these features in our work to test their efficacy in identifying appropriate constraints. Our problem is related to these previous studies, but differs in some key respects. Choosing descriptions often involves choosing between descriptions that convey different semantic information (“Clinton” as “senator” versus “president”), whereas our associated texts generally convey the same information but realize it diffe</context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>Jussi Karlgren and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proc. of COLING ’94, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
<author>Geoffrey Nunberg</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Association of Computational Linguistics (ACL ’97),</booktitle>
<pages>32--38</pages>
<location>Madrid,</location>
<marker>Kessler, Nunberg, Sch¨utze, 1997</marker>
<rawString>Brett Kessler, Geoffrey Nunberg, and Hinrich Sch¨utze. 1997. Automatic detection of text genre. In Proceedings of the 35th Association of Computational Linguistics (ACL ’97), pages 32–38, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In 6th Applied Natural Language Processing Conf. (ANLP’2000),</booktitle>
<pages>170--177</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="15604" citStr="Langkilde, 2000" startWordPosition="2437" endWordPosition="2438">es and their ordering, the task of surface realization is to convey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audie</context>
<context position="24250" citStr="Langkilde, 2000" startWordPosition="3806" endWordPosition="3807">eatures used by ripper to determine the output feature of target predicates. • Find best possible match between predicted features and candidate predicate. The final step is to use the predicted features of the predicate to match a most appropriate associated text to convey the predicate. If the predicted features exactly match a training example’s associated text then the selection is trivial. However in practice this rarely occurs and we must select from the available associated texts. As the choice is limited to whole associated texts and not constituents as in other stochastic approaches (Langkilde, 2000; Varges and Mellish, 2001), this search process is constrained and does not present an efficiency problem. For numerical features (e.g., number of words), we use a normalized difference between the desired value and available values from the training associated text to calculate its goodness of fit. For set valued features (e.g. parse node type: NP versus PP), the feature either matches or does not (1 or 0). Our algorithm weighs all features equally, and an associated text is chosen such that the matching score is maximized. In future work, we will use human judgments of the realized predicat</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In 6th Applied Natural Language Processing Conf. (ANLP’2000), pages 170–177, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Library of Congress</author>
</authors>
<title>Marc 21 format for classification data : including guidelines for content designation.</title>
<date>2000</date>
<pages>0660179903</pages>
<location>Washington, D.C., USA.</location>
<contexts>
<context position="26537" citStr="Congress, 2000" startWordPosition="4154" endWordPosition="4155">The Leukemia Handbook... Topicality Sentence Planner 6 Using the rulesets for generation This approach in this paper constructs rulesets that capture patterns at both the content planning and surface realization levels. While the work here does not constitute an end-to-end generation system, the algorithms for learning these knowledge bases are implemented, and can be applied along with a source of predicate attributes to generate new texts. In our application of indicative summarization generation, cataloging records (such as the U.S. Machine Readable Cataloging (MARC) guidelines (Library of Congress, 2000)) can provide these predicate attributes. Attribute values from the resource’s cataloguing record would interact with the surface realization constraints to produce a set of sentences or phrases that correspond to each predicate. A sentence planning module could take the realized fragments and organize them into sentences (according to the content plan) to form a final generated text. Figure 5 shows how this portion of the process would work in conjunction with sentence extraction to find sentences for any topicality predicates. 7 Conclusion In previous work, we performed a task-based evaluati</context>
</contexts>
<marker>Congress, 2000</marker>
<rawString>Library of Congress. 2000. Marc 21 format for classification data : including guidelines for content designation. Washington, D.C., USA. ISN 0660179903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Liddy</author>
</authors>
<title>The discourse-level structure of empirical abstracts: An exploratory study.</title>
<date>1991</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>27--1</pages>
<contexts>
<context position="16085" citStr="Liddy, 1991" startWordPosition="2510" endWordPosition="2511">ce. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as they comprise a large portion (57%) of the entries. In our framework, a predicate has two components (also shown in right hand side of Figure 1): the attribute value itself (“adult readers”) and the associated text that is used to cast this information in the semantic role dictated by the predicate (“This book is meant for &lt;attribute value&gt;” for the Audience predicate)1. In a stemmed dependency framework, the attribute value is t</context>
</contexts>
<marker>Liddy, 1991</marker>
<rawString>Elizabeth Liddy. 1991. The discourse-level structure of empirical abstracts: An exploratory study. Information Processing and Management, 27(1):55–81.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<editor>Inderjeet Mani and Mark Maybury, editors.</editor>
<publisher>MIT Press.</publisher>
<marker>1999</marker>
<rawString>Inderjeet Mani and Mark Maybury, editors. 1999. Advances in Automatic Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Oh</author>
<author>A Rudnicky</author>
</authors>
<title>Stochastic language generation for spoken dialogue systems.</title>
<date>2000</date>
<booktitle>In Proc. of the ANLP/NAACL 2000 Wrkshp. on Conversational Systems,</booktitle>
<pages>27--32</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="15802" citStr="Oh and Rudnicky, 2000" startWordPosition="2466" endWordPosition="2469"> (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as they comprise a large portion (57%) of the entries. In our framework, a predicate has two components (also shown in right hand side of Figure 1): the </context>
</contexts>
<marker>Oh, Rudnicky, 2000</marker>
<rawString>Alice Oh and A Rudnicky. 2000. Stochastic language generation for spoken dialogue systems. In Proc. of the ANLP/NAACL 2000 Wrkshp. on Conversational Systems, pages 27–32, Seattle, Washington, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Learning correlations between linguistic indicators and semantic constraints: Reuse of context-dependent descriptions of entities.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL 98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="21005" citStr="Radev, 1998" startWordPosition="3276" endWordPosition="3277">rmation (the attribute value) but with different words and syntactic structures. Which alternative is selected is subject to a number of parameters, including stylistic ones. For the surface realizer to make these decisions thus requires that we learn stylistic constraints and apply them in the generation process. We break this process down into three subtasks: • Identify and encode linguistic features. We examined several linguistic features for their potential to predict which alternative associated text are used in particular contexts. Related work on descriptive appositive language reuse (Radev, 1998), and genre identification (Biber, 1989; Karlgren and Cutting, 1994; Kessler et al., 1997) defines a large set of basic features to use such as character and word-level features, and positional and contextual features. We implemented a total of 27 of these features in our work to test their efficacy in identifying appropriate constraints. Our problem is related to these previous studies, but differs in some key respects. Choosing descriptions often involves choosing between descriptions that convey different semantic information (“Clinton” as “senator” versus “president”), whereas our associat</context>
</contexts>
<marker>Radev, 1998</marker>
<rawString>Dragomir R. Radev. 1998. Learning correlations between linguistic indicators and semantic constraints: Reuse of context-dependent descriptions of entities. In Proc. of COLING/ACL 98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Rama</author>
<author>Padmini Srinivasan</author>
</authors>
<title>An investigaton of content representation using text grammars.</title>
<date>1993</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>75</pages>
<contexts>
<context position="16113" citStr="Rama and Srinivasan, 1993" startWordPosition="2512" endWordPosition="2515">ncerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as they comprise a large portion (57%) of the entries. In our framework, a predicate has two components (also shown in right hand side of Figure 1): the attribute value itself (“adult readers”) and the associated text that is used to cast this information in the semantic role dictated by the predicate (“This book is meant for &lt;attribute value&gt;” for the Audience predicate)1. In a stemmed dependency framework, the attribute value is the child and the associated </context>
</contexts>
<marker>Rama, Srinivasan, 1993</marker>
<rawString>D. V. Rama and Padmini Srinivasan. 1993. An investigaton of content representation using text grammars. ACM Transactions on Information Systems, 11(1):51– 75, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proc. of the 6th Applied Natural Language Processing Conf. (ANLPNAACL</booktitle>
<pages>194--201</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="15651" citStr="Ratnaparkhi, 2000" startWordPosition="2444" endWordPosition="2445">alization is to convey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proc. of the 6th Applied Natural Language Processing Conf. (ANLPNAACL 2000), pages 194–201, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Ordering among premodifiers.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Associationfor Computational Linguistics,</booktitle>
<pages>135--143</pages>
<location>College Park, Maryland, USA,</location>
<marker>Shaw, Hatzivassiloglou, 1999</marker>
<rawString>James Shaw and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In Proc. of the 37th Associationfor Computational Linguistics, pages 135–143, College Park, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Chris Mellish</author>
</authors>
<title>Instancebased natural language generation.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL 01.</booktitle>
<contexts>
<context position="15678" citStr="Varges and Mellish, 2001" startWordPosition="2446" endWordPosition="2449">vey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as they comprise a large port</context>
<context position="18563" citStr="Varges and Mellish, 2001" startWordPosition="2902" endWordPosition="2906">d. For each collection of predicate instances, our strategy first identifies highly frequent (threshold = x¯ +2c,,) stemmed lexical dependency pairs. Frequent child lexical items in the dependency pair are potential attribute values in the sentence (“index”,hied as an attribute value for Content Types, as seen in Figure 3, #3). From this set, we remove frequent dependency pairs that occur with other predicates; this prevents frequent, corpus-wide dependencies such as “book”head —� “this”,hied from appearing as potential attribute values, as they are 1Our notion of “predicate” is identical to (Varges and Mellish, 2001) notion of “slots” or “tags”; similarly, our “attribute values” are equivalent to their term “fillers”. not exclusively frequent within a single predicate (so #1 and #2 are not attribute values for any of the 24 semantic predicates). #1, Topicality, bookhead -+ thischild : (e.g., “This book discusses Alcott’s works ...”, “this book covers the theories”) #2, Content Types,bookhead thischild : (e.g., “This book also comes with a biography”, “is discussed in this book”) #3, Content Types, includehead -+ indexchild : (e.g.,“Indices are included”, “includes an index”, “The book includes an index”) </context>
<context position="24277" citStr="Varges and Mellish, 2001" startWordPosition="3808" endWordPosition="3811">ipper to determine the output feature of target predicates. • Find best possible match between predicted features and candidate predicate. The final step is to use the predicted features of the predicate to match a most appropriate associated text to convey the predicate. If the predicted features exactly match a training example’s associated text then the selection is trivial. However in practice this rarely occurs and we must select from the available associated texts. As the choice is limited to whole associated texts and not constituents as in other stochastic approaches (Langkilde, 2000; Varges and Mellish, 2001), this search process is constrained and does not present an efficiency problem. For numerical features (e.g., number of words), we use a normalized difference between the desired value and available values from the training associated text to calculate its goodness of fit. For set valued features (e.g. parse node type: NP versus PP), the feature either matches or does not (1 or 0). Our algorithm weighs all features equally, and an associated text is chosen such that the matching score is maximized. In future work, we will use human judgments of the realized predicates in context to induce mor</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>Sebastian Varges and Chris Mellish. 2001. Instancebased natural language generation. In Proc. ofNAACL 01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>