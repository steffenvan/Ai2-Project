<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.102905">
<title confidence="0.956016">
Injecting Linguistics into NLP through Annotation
</title>
<author confidence="0.504059">
Eduard Hovy
</author>
<note confidence="0.9876845">
USC/ISI
4676 Admiralty Way
Marina del Rey, CA 90292
USA
</note>
<email confidence="0.993742">
hovy@isi.edu
</email>
<bodyText confidence="0.999422777777778">
Over the past 20 years, the size of the L in Com-
putational Linguistics has been shrinking relative
to the size of the C. The result is that we are in-
creasingly becoming a community of uninformed
but sophisticated engineers, applying to problems
very complex machine learning techniques that
use very simple (simplistic?) analyses/theories.
(Try finding a theoretical account of subjectiv-
ity, opinion, entailment, or inference in publica-
tions surrounding the associated competitions of
the past few years.)
When we grow tired of embarrassing ourselves,
what should we do? Fortunately, injecting some
linguistic (and other) sophistication into our work
is not that complicated. The key is annotation: by
using a theoretically informed set of choices rather
than a bottom-up naive one, we can have annota-
tors tag corpora with labels that reflect some un-
derlying theories. While the large-C contingent
of our community will not care, researchers in-
terested in investigating language rather than pro-
cessing will be able to find new ways to connect
with Corpus Linguists, Psycholinguists, and even
Ontologists.
It turns out that many of our surrounding aca-
demic communities – Linguists, Political Scien-
tists, Biocurators, etc. – have been performing an-
notation for years in order to build and prove their
theories. They have however been largely unaware
of the power of NLP technology and the benefits
we can bring to them. There is a natural marriage
– several, actually – waiting to happen.
What is the benefit to us? What’s wrong with
simply continuing to use half-baked annotation
schemes to train our machine learning systems on?
Several things:
</bodyText>
<listItem confidence="0.7246965">
• half-baked schemes generally fail in the long
run-that’s why more-sophisticated ones are
developed
• there are dozens to hundreds of graduate
students and young researchers in surround-
ing communities eager to help build cor-
pora by running annotation efforts and using
the problems uncovered while annotating to
drive further theory formation
• because they’re generally more ‘correct’,
</listItem>
<bodyText confidence="0.96306475">
more-sophisticated annotations allow stack-
ing of multiple phenomena upon the same
material with fewer internal inconsistencies
and problems.
Such stacking eventually enables multi-
phenomenon analysis and mutual disambiguation
in ways that an incommensurately annotated
corpus does not.
</bodyText>
<page confidence="0.974541">
79
</page>
<note confidence="0.6363775">
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, page 79,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.139070">
<title confidence="0.992054">Injecting Linguistics into NLP through Annotation</title>
<author confidence="0.552719">Eduard</author>
<address confidence="0.641692">4676 Admiralty</address>
<author confidence="0.953056">Marina del Rey</author>
<author confidence="0.953056">CA</author>
<email confidence="0.952951">hovy@isi.edu</email>
<abstract confidence="0.99718937037037">the past 20 years, the size of the Computational Linguistics has been shrinking relative the size of the The result is that we are increasingly becoming a community of uninformed but sophisticated engineers, applying to problems very complex machine learning techniques that use very simple (simplistic?) analyses/theories. (Try finding a theoretical account of subjectivity, opinion, entailment, or inference in publications surrounding the associated competitions of the past few years.) When we grow tired of embarrassing ourselves, what should we do? Fortunately, injecting some linguistic (and other) sophistication into our work is not that complicated. The key is annotation: by using a theoretically informed set of choices rather than a bottom-up naive one, we can have annotators tag corpora with labels that reflect some underlying theories. While the large-C contingent of our community will not care, researchers interested in investigating language rather than processing will be able to find new ways to connect with Corpus Linguists, Psycholinguists, and even Ontologists. It turns out that many of our surrounding academic communities – Linguists, Political Scientists, Biocurators, etc. – have been performing annotation for years in order to build and prove their theories. They have however been largely unaware of the power of NLP technology and the benefits we can bring to them. There is a natural marriage – several, actually – waiting to happen. What is the benefit to us? What’s wrong with simply continuing to use half-baked annotation schemes to train our machine learning systems on? Several things: • half-baked schemes generally fail in the long run-that’s why more-sophisticated ones are developed • there are dozens to hundreds of graduate students and young researchers in surrounding communities eager to help build corpora by running annotation efforts and using the problems uncovered while annotating to drive further theory formation • because they’re generally more ‘correct’, more-sophisticated annotations allow stacking of multiple phenomena upon the same material with fewer internal inconsistencies and problems. Such stacking eventually enables multiphenomenon analysis and mutual disambiguation in ways that an incommensurately annotated corpus does not.</abstract>
<note confidence="0.560652333333333">79 of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL page Sweden, 16 July 2010. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>