<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010173">
<title confidence="0.978363">
Phramer - An Open Source Statistical Phrase-Based Translator
</title>
<author confidence="0.998369">
Marian Olteanu, Chris Davis, Ionut Volosen and Dan Moldovan
</author>
<affiliation confidence="0.9916905">
Human Language Technology Research Institute
The University of Texas at Dallas
</affiliation>
<address confidence="0.927272">
Richardson, TX 75080
</address>
<email confidence="0.999559">
{marian,phoo,volosen,moldovan}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999135">
This paper describes the open-source
Phrase-Based Statistical Machine Transla-
tion Decoder - Phramer. The paper also
presents the UTD (HLTRI) system build
for the WMT06 shared task. Our goal was
to improve the translation quality by en-
hancing the translation table and by pre-
processing the source language text
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915333333333">
Despite the fact that the research in Statistical
Machine Translation (SMT) is very active, there
isn’t an abundance of open-source tools available
to the community. In this paper, we present
Phramer, an open-source system that embeds a
phrase-based decoder, a minimum error rate train-
ing (Och, 2003) module and various tools related
to Machine Translation (MT). The software is re-
leased under BSD license and it is available at
http://www.phramer.org/.
We also describe our Phramer-based system
that we build for the WMT06 shared task.
</bodyText>
<sectionHeader confidence="0.993932" genericHeader="introduction">
2 Phramer
</sectionHeader>
<bodyText confidence="0.958464">
Phramer is a phrase-based SMT system written in
Java. It includes:
</bodyText>
<listItem confidence="0.978329142857143">
• A decoder that is compatible with Pharaoh
(Koehn, 2004),
• A minimum error rate training (MERT) mod-
ule, compatible with Phramer’s decoder, with
Pharaoh and easily adaptable to other SMT
or non-SMT tasks and
• various tools.
</listItem>
<bodyText confidence="0.987729517241379">
The decoder is fully compatible with Pharaoh
1.2 in the algorithms that are implemented, input
files (configuration file, translation table, language
models) and command line. Some of the advantages
of Phramer over Pharaoh are: (1) source code
availability and its permissive license; (2) it is very
fast (1.5–3 times faster for most of the configura-
tions); (3) it can work with various storage layers for
the translation table (TT) and the language models
(LMs): memory, remote (access through TCP/IP),
disk (using SQLite databases1). Extensions for other
storage layers can be very easily implemented; (4) it
is more configurable; (5) it accepts compressed data
files (TTs and LMs); (6) it is very easy to extend; an
example is provided in the package – part-of-speech
decoding on either source language, target language
or both; support for POS-based language models;
(7) it can internally generate n-best lists. Thus no
external tools are required.
The MERT module is a highly modular, efficient
and customizable implementation of the algorithm
described in (Och, 2003). The release has imple-
mentations for BLEU (Papineni et al., 2002), WER
and PER error criteria and it has decoding interfaces
for Phramer and Pharaoh. It can be used to
search parameters over more than one million vari-
ables. It offers features as resume search, reuse hy-
potheses from previous runs and various strategies
to search for optimal A weight vectors.
</bodyText>
<footnote confidence="0.99322">
1http://www.sqlite.org/
</footnote>
<page confidence="0.963571">
146
</page>
<subsectionHeader confidence="0.773897">
Proceedings of the Workshop on Statistical Machine Translation, pages 146–149,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.996415">
The package contains a set of tools that include:
</bodyText>
<listItem confidence="0.9906526">
• Distributed decoding (compatible with both
Phramer and Pharaoh) – it automatically
splits decoding jobs and distributes them to
workers and assembles the results. It is compat-
ible with lattice generation, therefore it can also
be used during weights search (using MERT).
• Tools to process translation tables – filter the
TT based on the input file, flip TT to reuse it
for English-to-Foreign translation, filter the TT
by phrase length, convert the TT to a database.
</listItem>
<sectionHeader confidence="0.898497" genericHeader="method">
3 WMT06 Shared Task
</sectionHeader>
<bodyText confidence="0.999031">
We have assembled a system for participation in the
WMT 2006 shared task based on Phramer and
other tools. We participated in 5 subtasks: DE—*EN,
FR—*EN, ES—*EN, EN—*FR and EN—*ES.
</bodyText>
<subsectionHeader confidence="0.937581">
3.1 Baseline system
3.1.1 Translation table generation
</subsectionHeader>
<bodyText confidence="0.9999659">
To generate a translation table for each pair of lan-
guages starting from a sentence-aligned parallel cor-
pus, we used a modified version of the Pharaoh
training software 2. The software also required
GIZA++ word alignment tool(Och and Ney, 2003).
We generated for each phrase pair in the trans-
lation table 5 features: phrase translation probabil-
ity (both directions), lexical weighting (Koehn et al.,
2003) (both directions) and phrase penalty (constant
value).
</bodyText>
<subsectionHeader confidence="0.646031">
3.1.2 Decoder
</subsectionHeader>
<bodyText confidence="0.99979275">
The Phramer decoder was used to translate the
devtest2006 and test2006 files. We accelerated the
decoding process by using the distributed decoding
tool.
</bodyText>
<subsectionHeader confidence="0.957694">
3.1.3 Minimum Error Rate Training
</subsectionHeader>
<bodyText confidence="0.99984675">
We determined the weights to combine the mod-
els using the MERT component in Phramer. Be-
cause of the time constrains for the shared task sub-
mission3, we used Pharaoh+ Carmel4 as the de-
</bodyText>
<footnote confidence="0.966620166666667">
2http://www.iccs.inf.ed.ac.uk/—pkoehn/training.tgz
3After the shared task submission, we optimized a lot our
decoder. Before the optimizations (LM optimizations, fixing
bugs that affected performance), Phramer was 5 to 15 times
slower than Pharaoh.
4http://www.isi.edu/licensed-sw/carmel/
</footnote>
<bodyText confidence="0.810885">
coder for the MERT algorithm.
</bodyText>
<subsectionHeader confidence="0.773292">
3.1.4 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999994333333333">
We removed from the source text the words that
don’t appear either in the source side of the train-
ing corpus (thus we know that the translation table
will not be able to translate them) or in the lan-
guage model for the target language (and we esti-
mate that there is a low chance that the untranslated
word might actually be part of the reference transla-
tion). The purpose of this procedure is to minimize
the risk of inserting words into the automatic trans-
lation that are not in the reference translation.
We applied this preprocessing step only when the
target language was English.
</bodyText>
<subsectionHeader confidence="0.986225">
3.2 Enhancements to the baseline systems
</subsectionHeader>
<bodyText confidence="0.998422333333333">
Our goal was to improve the translation quality by
enhancing the the translation table.
The following enhancements were implemented:
</bodyText>
<listItem confidence="0.990117833333333">
• reduce the vocabulary size perceived by the
GIZA++ and preset alignment for certain
words
• “normalize” distortion between pairs of lan-
guages by reordering noun-adjective construc-
tions
</listItem>
<bodyText confidence="0.9999791">
The first enhancement identifies pairs of tokens in
the parallel sentences that, with a very high proba-
bility, align together and they don’t align with other
tokens in the sentence. These tokens are replaced
with a special identifier, chosen so that GIZA++ will
learn the alignment between them easier than before
replacement. The targeted token types are proper
nouns (detected when the same upper-cased token
were present in both the foreign sentence and the
English sentence) and numbers, also taking into ac-
count the differences between number representa-
tion in different languages (i.e.: 399.99 vs. 399,99).
Each distinct proper noun to be replaced in the sen-
tence was replaced with a specific identifier, distinct
from other replacement identifiers already used in
the sentence. The same procedure was applied also
for numbers. The specific identifiers were reused in
other sentences. This has the effect of reducing the
vocabulary, thus it provides a large number of in-
stances for the special token forms. The change in
</bodyText>
<page confidence="0.943955">
147
</page>
<figure confidence="0.9985828">
.
europe
of
council
the
of
assembly
parliamentary
the
for
romania
on
rapporteur
the
was
I
.
europe
of
council
the
of
assembly
parliamentary
the
for
romania
on
rapporteur
the
was
I
yo Pui ponente de la asamblea parlamentaria del consejo de europa para ruman’a . yo Pui ponente de la asamblea del consejo de europa para ruman’a .
parlamentaria
before reordering after reordering
</figure>
<figureCaption confidence="0.999607">
Figure 1: NN-ADJ reordering
</figureCaption>
<table confidence="0.9966355">
Corpus Before After
DE 195,290 184,754
FR 80,348 70,623
ES 102,885 92,827
</table>
<tableCaption confidence="0.8088415">
Table 1: Vocabulary size change due to forced align-
ment
</tableCaption>
<bodyText confidence="0.999858333333333">
the vocabulary size is shown in Table 1. To simplify
the process, we limited the replacement of tokens
to one-to-one (one real token to one special token),
so that the word alignment file can be directly used
together with the original parallel corpus to extract
phrases required for the generation of the translation
table. Table 2 shows an example of the output.
The second enhancement tries to improve the
quality of the translation by rearranging the words in
the source sentence to better match the correct word
order in the target language (Collins et al., 2005).
We focused on a very specific pattern – based on the
part-of-speech tags, changing the order of NN-ADJ
phrases in the non-English sentences. This process
was also applied to the input dev/test files, when the
target language was English. Figure 1 shows the re-
ordering process and its effect on the alignment.
The expected benefits are:
</bodyText>
<listItem confidence="0.917902666666667">
• Better word alignment due to an alignment
closer to the expected alignment (monotone).
• More phrases extracted from the word aligned
corpus. Monotone alignment tends to generate
more phrases than a random alignment.
• Higher mixture weight for the monotone dis-
</listItem>
<bodyText confidence="0.86744125">
tortion model because of fewer reordering con-
straints during MERT, thus the value of the
monotone distortion model increases, “tighten-
ing” the translation.
</bodyText>
<subsectionHeader confidence="0.99751">
3.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999971153846154">
We implemented the first enhancement on ES—*EN
subtask by part-of-speech tagging the Spanish text
using TreeTagger5 followed by a NN-ADJ inver-
sion heuristic.
The language models provided for the task was
used.
We used the 1,000 out of the 2,000 sentences
in each of the dev2006 datasets to determine
weights for the 8 models used during decoding (one
monotone distortion mode, one language model,
five translation models, one sentence length model)
through MERT. The weights were determined in-
dividually for each pair of source-target languages.
</bodyText>
<footnote confidence="0.9746815">
5http://www.ims.uni-stuttgart.de/projekte/corplex/
TreeTagger/DecisionTreeTagger.html
</footnote>
<page confidence="0.980695">
148
</page>
<table confidence="0.92115575">
There are 145 settlements in the West Bank , 16 in Gaza , 9 in East Jerusalem ; 400,000 people live in them .
Existen 145 asentamientos en Cisjordania , 16 en Gaza y 9 en Jerusaln Este; en ellos viven 400.000 personas.
There are [x1] settlements in the West Bank, [x2] in [y1] , [x3] in East Jerusalem; [x4] people live in them.
Existen [x1] asentamientos en Cisjordania , [x2] en [y1] y [x3] en Jerusaln Este; en ellos viven [x4] personas .
</table>
<tableCaption confidence="0.90206">
Table 2: Forced alignment example
</tableCaption>
<table confidence="0.9998834">
Subtask OOV forced NN-ADJ BLEU
filtering alignment inversion score
DE→EN √ — — 25.45
√ √ — 25.53
FR→EN √ — — 30.70
√ √ — 30.70
ES→EN √ — — 30.77
√ √ — 30.84
√ √ √ 30.92
EN→FR — — — 31.67
— — 31.79
√
EN→ES — — — 30.17
— — 30.11
√
</table>
<tableCaption confidence="0.99894">
Table 3: Results on the devtest2006 files
</tableCaption>
<table confidence="0.999950666666667">
Subtask BLEU 1/2/3/4-gram precision (bp)
DE→EN 22.96 58.8/28.8/16.5/9.9 (1.000)
FR→EN 27.78 61.8/33.6/21.0/13.7 (1.000)
ES→EN 29.93 63.5/36.0/23.0/15.2 (1.000)
EN→FR 28.87 60.0/34.7/22.7/15.2 (0.991)
EN→ES 29.00 62.9/35.8/23.0/15.1 (0.975)
</table>
<tableCaption confidence="0.999866">
Table 4: Results on the test2006 files
</tableCaption>
<bodyText confidence="0.999865">
Using these weights, we measured the BLEU score
on the devtest2006 datasets. Based on the model
chosen, we decoded the test2006 datasets using the
same weights as for devtest2006.
</bodyText>
<sectionHeader confidence="0.913128" genericHeader="evaluation">
3.4 Results
</sectionHeader>
<bodyText confidence="0.9994568">
Table 3 presents the results on the devtest2006 files
using different settings. Bold values represent the
result for the settings that were also chosen for the
final test. Table 4 shows the results on the submitted
files (test2006).
</bodyText>
<sectionHeader confidence="0.518002" genericHeader="conclusions">
3.5 Conclusions
</sectionHeader>
<bodyText confidence="0.9979320625">
The enhancements that we proposed provide small
improvements on the devtest2006 files. As expected,
when we used the NN-ADJ inversion the ratio ��
���
increased from 0.545 to 0.675. The LM is the only
model that opposes the tendency of the distortion
model towards monotone phrase order.
Phramer delivers a very good baseline system.
Using only the baseline system, we obtain +0.68 on
DE-*EN, +0.43 on FR-*EN and -0.18 on ES-*EN
difference in BLEU score compared to WPT05’s
best system (Koehn and Monz, 2005). This fact is
caused by the MERT module. This module is capa-
ble of estimating parameters over a large develop-
ment corpus in a reasonable time, thus it is able to
generate highly relevant parameters.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999411193548387">
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 531–540, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 119–124,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT/NAACL 2003, Edmonton, Canada.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings ofAMTA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311–318.
</reference>
<page confidence="0.998961">
149
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.328459">
<title confidence="0.999687">An Open Source Statistical Phrase-Based Translator</title>
<author confidence="0.992473">Marian Olteanu</author>
<author confidence="0.992473">Chris Davis</author>
<author confidence="0.992473">Ionut Volosen</author>
<author confidence="0.992473">Dan</author>
<affiliation confidence="0.9814635">Human Language Technology Research The University of Texas at</affiliation>
<address confidence="0.546435">Richardson, TX</address>
<abstract confidence="0.956758444444444">This paper describes the open-source Phrase-Based Statistical Machine Transla- Decoder - The paper also presents the UTD (HLTRI) system build for the WMT06 shared task. Our goal was to improve the translation quality by enhancing the translation table and by preprocessing the source language text</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="8100" citStr="Collins et al., 2005" startWordPosition="1277" endWordPosition="1280">le 1: Vocabulary size change due to forced alignment the vocabulary size is shown in Table 1. To simplify the process, we limited the replacement of tokens to one-to-one (one real token to one special token), so that the word alignment file can be directly used together with the original parallel corpus to extract phrases required for the generation of the translation table. Table 2 shows an example of the output. The second enhancement tries to improve the quality of the translation by rearranging the words in the source sentence to better match the correct word order in the target language (Collins et al., 2005). We focused on a very specific pattern – based on the part-of-speech tags, changing the order of NN-ADJ phrases in the non-English sentences. This process was also applied to the input dev/test files, when the target language was English. Figure 1 shows the reordering process and its effect on the alignment. The expected benefits are: • Better word alignment due to an alignment closer to the expected alignment (monotone). • More phrases extracted from the word aligned corpus. Monotone alignment tends to generate more phrases than a random alignment. • Higher mixture weight for the monotone di</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 531–540, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Shared task: Statistical machine translation between European languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>119--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<marker>Koehn, Monz, 2005</marker>
<rawString>Philipp Koehn and Christof Monz. 2005. Shared task: Statistical machine translation between European languages. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 119–124, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL 2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4240" citStr="Koehn et al., 2003" startWordPosition="658" endWordPosition="661">stem for participation in the WMT 2006 shared task based on Phramer and other tools. We participated in 5 subtasks: DE—*EN, FR—*EN, ES—*EN, EN—*FR and EN—*ES. 3.1 Baseline system 3.1.1 Translation table generation To generate a translation table for each pair of languages starting from a sentence-aligned parallel corpus, we used a modified version of the Pharaoh training software 2. The software also required GIZA++ word alignment tool(Och and Ney, 2003). We generated for each phrase pair in the translation table 5 features: phrase translation probability (both directions), lexical weighting (Koehn et al., 2003) (both directions) and phrase penalty (constant value). 3.1.2 Decoder The Phramer decoder was used to translate the devtest2006 and test2006 files. We accelerated the decoding process by using the distributed decoding tool. 3.1.3 Minimum Error Rate Training We determined the weights to combine the models using the MERT component in Phramer. Because of the time constrains for the shared task submission3, we used Pharaoh+ Carmel4 as the de2http://www.iccs.inf.ed.ac.uk/—pkoehn/training.tgz 3After the shared task submission, we optimized a lot our decoder. Before the optimizations (LM optimization</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT/NAACL 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="1274" citStr="Koehn, 2004" startWordPosition="189" endWordPosition="190">ranslation (SMT) is very active, there isn’t an abundance of open-source tools available to the community. In this paper, we present Phramer, an open-source system that embeds a phrase-based decoder, a minimum error rate training (Och, 2003) module and various tools related to Machine Translation (MT). The software is released under BSD license and it is available at http://www.phramer.org/. We also describe our Phramer-based system that we build for the WMT06 shared task. 2 Phramer Phramer is a phrase-based SMT system written in Java. It includes: • A decoder that is compatible with Pharaoh (Koehn, 2004), • A minimum error rate training (MERT) module, compatible with Phramer’s decoder, with Pharaoh and easily adaptable to other SMT or non-SMT tasks and • various tools. The decoder is fully compatible with Pharaoh 1.2 in the algorithms that are implemented, input files (configuration file, translation table, language models) and command line. Some of the advantages of Phramer over Pharaoh are: (1) source code availability and its permissive license; (2) it is very fast (1.5–3 times faster for most of the configurations); (3) it can work with various storage layers for the translation table (TT</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4079" citStr="Och and Ney, 2003" startWordPosition="633" endWordPosition="636">flip TT to reuse it for English-to-Foreign translation, filter the TT by phrase length, convert the TT to a database. 3 WMT06 Shared Task We have assembled a system for participation in the WMT 2006 shared task based on Phramer and other tools. We participated in 5 subtasks: DE—*EN, FR—*EN, ES—*EN, EN—*FR and EN—*ES. 3.1 Baseline system 3.1.1 Translation table generation To generate a translation table for each pair of languages starting from a sentence-aligned parallel corpus, we used a modified version of the Pharaoh training software 2. The software also required GIZA++ word alignment tool(Och and Ney, 2003). We generated for each phrase pair in the translation table 5 features: phrase translation probability (both directions), lexical weighting (Koehn et al., 2003) (both directions) and phrase penalty (constant value). 3.1.2 Decoder The Phramer decoder was used to translate the devtest2006 and test2006 files. We accelerated the decoding process by using the distributed decoding tool. 3.1.3 Minimum Error Rate Training We determined the weights to combine the models using the MERT component in Phramer. Because of the time constrains for the shared task submission3, we used Pharaoh+ Carmel4 as the </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="903" citStr="Och, 2003" startWordPosition="129" endWordPosition="130">ribes the open-source Phrase-Based Statistical Machine Translation Decoder - Phramer. The paper also presents the UTD (HLTRI) system build for the WMT06 shared task. Our goal was to improve the translation quality by enhancing the translation table and by preprocessing the source language text 1 Introduction Despite the fact that the research in Statistical Machine Translation (SMT) is very active, there isn’t an abundance of open-source tools available to the community. In this paper, we present Phramer, an open-source system that embeds a phrase-based decoder, a minimum error rate training (Och, 2003) module and various tools related to Machine Translation (MT). The software is released under BSD license and it is available at http://www.phramer.org/. We also describe our Phramer-based system that we build for the WMT06 shared task. 2 Phramer Phramer is a phrase-based SMT system written in Java. It includes: • A decoder that is compatible with Pharaoh (Koehn, 2004), • A minimum error rate training (MERT) module, compatible with Phramer’s decoder, with Pharaoh and easily adaptable to other SMT or non-SMT tasks and • various tools. The decoder is fully compatible with Pharaoh 1.2 in the algo</context>
<context position="2516" citStr="Och, 2003" startWordPosition="385" endWordPosition="386"> memory, remote (access through TCP/IP), disk (using SQLite databases1). Extensions for other storage layers can be very easily implemented; (4) it is more configurable; (5) it accepts compressed data files (TTs and LMs); (6) it is very easy to extend; an example is provided in the package – part-of-speech decoding on either source language, target language or both; support for POS-based language models; (7) it can internally generate n-best lists. Thus no external tools are required. The MERT module is a highly modular, efficient and customizable implementation of the algorithm described in (Och, 2003). The release has implementations for BLEU (Papineni et al., 2002), WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh. It can be used to search parameters over more than one million variables. It offers features as resume search, reuse hypotheses from previous runs and various strategies to search for optimal A weight vectors. 1http://www.sqlite.org/ 146 Proceedings of the Workshop on Statistical Machine Translation, pages 146–149, New York City, June 2006. c�2006 Association for Computational Linguistics The package contains a set of tools that include: • Distr</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="2582" citStr="Papineni et al., 2002" startWordPosition="394" endWordPosition="397">QLite databases1). Extensions for other storage layers can be very easily implemented; (4) it is more configurable; (5) it accepts compressed data files (TTs and LMs); (6) it is very easy to extend; an example is provided in the package – part-of-speech decoding on either source language, target language or both; support for POS-based language models; (7) it can internally generate n-best lists. Thus no external tools are required. The MERT module is a highly modular, efficient and customizable implementation of the algorithm described in (Och, 2003). The release has implementations for BLEU (Papineni et al., 2002), WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh. It can be used to search parameters over more than one million variables. It offers features as resume search, reuse hypotheses from previous runs and various strategies to search for optimal A weight vectors. 1http://www.sqlite.org/ 146 Proceedings of the Workshop on Statistical Machine Translation, pages 146–149, New York City, June 2006. c�2006 Association for Computational Linguistics The package contains a set of tools that include: • Distributed decoding (compatible with both Phramer and Pharaoh) – it au</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>