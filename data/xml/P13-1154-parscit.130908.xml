<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9869">
Beam Search for Solving Substitution Ciphers
</title>
<author confidence="0.946538">
Malte Nuhn and Julian Schamper and Hermann Ney
</author>
<affiliation confidence="0.939521">
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
</affiliation>
<email confidence="0.994703">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99844662962963">
In this paper we address the problem of
solving substitution ciphers using a beam
search approach. We present a concep-
tually consistent and easy to implement
method that improves the current state of
the art for decipherment of substitution ci-
phers and is able to use high order n-gram
language models. We show experiments
with 1:1 substitution ciphers in which the
guaranteed optimal solution for 3-gram
language models has 38.6% decipherment
error, while our approach achieves 4.13%
decipherment error in a fraction of time
by using a 6-gram language model. We
also apply our approach to the famous
Zodiac-408 cipher and obtain slightly bet-
ter (and near to optimal) results than pre-
viously published. Unlike the previous
state-of-the-art approach that uses addi-
tional word lists to evaluate possible deci-
pherments, our approach only uses a letter-
based 6-gram language model. Further-
more we use our algorithm to solve large
vocabulary substitution ciphers and im-
prove the best published decipherment er-
ror rate based on the Gigaword corpus of
7.8% to 6.0% error rate.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998">
State-of-the-art statistical machine translation
(SMT) systems use large amounts of parallel data
to estimate translation models. However, parallel
corpora are expensive and not available for every
domain.
Recently different works have been published
that train translation models using only non-
parallel data. Although first practical applications
of these approaches have been shown, the overall
decipherment accuracy of the proposed algorithms
is still low. Improving the core decipherment algo-
rithms is an important step for making decipher-
ment techniques useful for practical applications.
In this paper we present an effective beam
search algorithm which provides high decipher-
ment accuracies while having low computational
requirements. The proposed approach allows us-
ing high order n-gram language models, is scal-
able to large vocabulary sizes and can be adjusted
to account for a given amount of computational
resources. We show significant improvements in
decipherment accuracy in a variety of experiments
while being computationally more effective than
previous published works.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955217391304">
The experiments proposed in this paper touch
many of previously published works in the deci-
pherment field.
Regarding the decipherment of 1:1 substitution
ciphers various works have been published: Most
older papers do not use a statistical approach and
instead define some heuristic measures for scoring
candidate decipherments. Approaches like (Hart,
1994) and (Olson, 2007) use a dictionary to check
if a decipherment is useful. (Clark, 1998) defines
other suitability measures based on n-gram counts
and presents a variety of optimization techniques
like simulated annealing, genetic algorithms and
tabu search.
On the other hand, statistical approaches for
1:1 substitution ciphers were published in the nat-
ural language processing community: (Ravi and
Knight, 2008) solve 1:1 substitution ciphers opti-
mally by formulating the decipherment problem as
an integer linear program (ILP) while (Corlett and
Penn, 2010) solve the problem using A∗ search.
We use our own implementation of these methods
to report optimal solutions to 1:1 substitution ci-
</bodyText>
<page confidence="0.941421">
1568
</page>
<note confidence="0.913437">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.985986393939394">
phers for language model orders n = 2 and n = 3.
(Ravi and Knight, 2011a) report the first au-
tomatic decipherment of the Zodiac-408 cipher.
They use a combination of a 3-gram language
model and a word dictionary. We run our beam
search approach on the same cipher and report
better results without using an additional word
dictionary—just by using a high order n-gram lan-
guage model.
(Ravi and Knight, 2011b) report experiments on
large vocabulary substitution ciphers based on the
Transtac corpus. (Dou and Knight, 2012) improve
upon these results and provide state-of-the-art re-
sults on a large vocabulary word substitution ci-
pher based on the Gigaword corpus. We run our
method on the same corpus and report improve-
ments over the state of the art.
(Ravi and Knight, 2011b) and (Nuhn et al.,
2012) have shown that—even for larger vocabu-
lary sizes—it is possible to learn a full translation
model from non-parallel data. Even though this
work is currently only able to deal with substi-
tution ciphers, phenomena like reordering, inser-
tions and deletions can in principle be included in
our approach.
The 1:1 substitution cipher encrypts a given
plaintext into a ciphertext by replacing each plain-
text token with a unique substitute: This means
that the table s(e|f) contains all zeroes, except for
one “1.0” per f ∈ Vf and one “1.0” per e ∈ Ve.
For example the text
abadcab
would be enciphered to
</bodyText>
<sectionHeader confidence="0.334332" genericHeader="method">
BCBADBC
</sectionHeader>
<bodyText confidence="0.520346">
when using the substitution
</bodyText>
<equation confidence="0.8275994">
a b c d
A 0 0 0 1
B 1 0 0 0
C 0 1 0 0
D 0 0 1 0
</equation>
<bodyText confidence="0.999806">
In contrast to the 1:1 substitution cipher, the ho-
mophonic substitution cipher allows multiple ci-
pher tokens per plaintext token, which means that
the table s(e|f) is all zero, except for one “1.0” per
f ∈ Vf. For example the above plaintext could be
enciphered to
</bodyText>
<sectionHeader confidence="0.997869" genericHeader="method">
3 Definitions
</sectionHeader>
<bodyText confidence="0.998045">
In the following we will use the machine trans-
lation notation and denote the ciphertext with
</bodyText>
<equation confidence="0.9941128">
fN1 = f1 ... fj ... fN which consists of cipher
tokens fj ∈ Vf. We denote the plaintext with
eN1 = e1 ... ez ... eN (and its vocabulary Ve re-
spectively). We define
e0 = f0 = eN+1 = fN+1 = $ (1)
</equation>
<bodyText confidence="0.9290508">
with “$” being a special sentence boundary token.
We use the abbreviations V e = Ve ∪ {$} and V f
respectively.
A general substitution cipher uses a table
s(e|f) which contains for each cipher token f a
probability that the token f is substituted with the
plaintext token e. Such a table for substituting
cipher tokens {A, B, C, D} with plaintext tokens
{a, b, c, d} could for example look like
a b c d
</bodyText>
<table confidence="0.9375385">
A 0.1 0.2 0.3 0.4
B 0.4 0.2 0.1 0.3
C 0.4 0.1 0.2 0.3
D 0.3 0.4 0.2 0.1
</table>
<sectionHeader confidence="0.792529" genericHeader="method">
ABCDECF
</sectionHeader>
<bodyText confidence="0.931657">
when using the homophonic substitution
</bodyText>
<equation confidence="0.712884444444444">
a b c d
A 1 0 0 0
B 0 1 0 0
C 1 0 0 0
D 0 0 0 1
E 0 0 1 0
F 0 1 0 0
We will use the definition
s(e|f) (2)
</equation>
<bodyText confidence="0.9998688">
to characterize the maximum number of different
cipher symbols allowed per plaintext symbol.
We formalize the 1:1 substitutions with a bijec-
tive function 0 : Vf → Ve and homophonic sub-
stitutions with a general function 0 : Vf → Ve.
Following (Corlett and Penn, 2010), we call
cipher functions 0, for which not all 0(f)’s are
fixed, partial cipher functions . Further, 0&apos; is
said to extend 0, if for all f that are fixed in 0, it
holds that f is also fixed in 0&apos; with 0&apos;(f) = 0(f).
</bodyText>
<equation confidence="0.868387">
�
nmax = Max
e
f
</equation>
<page confidence="0.971055">
1569
</page>
<bodyText confidence="0.9998252">
The cardinality of 0 counts the number of fixed
f’s in 0.
When talking about partial cipher functions we
use the notation for relations, in which 0 C Vf x
Ve. For example with
</bodyText>
<equation confidence="0.9995894">
0 = {(A, a)} 00 = {(A, a), (B, b)}
it follows that 0 C100 and
|0 |= 1 |00 |= 2
0(A) = a 00(A) = a
0(B) = undefined 00(B) = b
</equation>
<bodyText confidence="0.993800666666667">
The general decipherment goal is to obtain a
mapping 0 such that the probability of the deci-
phered text is maximal:
</bodyText>
<equation confidence="0.9972825">
0ˆ = arg max p(0(f1)0(f2)0(f3)...0(fN)) (3)
φ
</equation>
<bodyText confidence="0.998329333333333">
Here p(... ) denotes the language model. De-
pending on the structure of the language model
Equation 3 can be further simplified.
</bodyText>
<sectionHeader confidence="0.990987" genericHeader="method">
4 Beam Search
</sectionHeader>
<bodyText confidence="0.9998016">
In this Section we present our beam search ap-
proach to solving Equation 3. We first present the
general algorithm, containing many higher level
functions. We then discuss possible instances of
these higher level functions.
</bodyText>
<subsectionHeader confidence="0.986583">
4.1 General Algorithm
</subsectionHeader>
<bodyText confidence="0.994926176470588">
Figure 1 shows the general structure of the beam
search algorithm for the decipherment of substi-
tution ciphers. The general idea is to keep track
of all partial hypotheses in two arrays H3 and Ht.
During search all possible extensions of the partial
hypotheses in H3 are generated and scored. Here,
the function EXT ORDER chooses which cipher
symbol is used next for extension, EXT LIMITS
decides which extensions are allowed, and SCORE
scores the new partial hypotheses. PRUNE then se-
lects a subset of these hypotheses which are stored
to Ht. Afterwards the array H3 is copied to Ht
and the search process continues with the updated
array H3.
Due to the structure of the algorithm the car-
dinality of all hypotheses in H3 increases in each
step. Thus only hypotheses of the same cardinality
</bodyText>
<footnote confidence="0.633502">
1shorthand notation for 0&apos; extends 0
</footnote>
<listItem confidence="0.943904954545455">
1: function BEAM SEARCH(EXT ORDER,
EXT LIMITS, PRUNE)
2: init sets H3, Ht
3: CARDINALITY = 0
4: H3.ADD((O, 0))
5: while CARDINALITY &lt; |Vf |do
6: f = EXT ORDER[CARDINALITY]
7: for all 0 E H3 do
8: for all e E Ve do
9: 00 := 0 U {(e, f)}
10: if EXT LIMITS(00) then
11: Ht.ADD(00,SCORE (00))
12: end if
13: end for
14: end for
15: PRUNE(Ht)
16: CARDINALITY = CARDINALITY + 1
17: H3 = Ht
18: Ht.CLEAR()
19: end while
20: return best scoring cipher function in H3
21: end function
</listItem>
<figureCaption confidence="0.7536208">
Figure 1: The general structure of the beam
search algorithm for decipherment of substitu-
tion ciphers. The high level functions SCORE,
EXT ORDER, EXT LIMITS and PRUNE are de-
scribed in Section 4.
</figureCaption>
<bodyText confidence="0.996812142857143">
are compared in the pruning step. When H3 con-
tains full cipher relations, the cipher relation with
the maximal score is returned.2
Figure 2 illustrates how the algorithm explores
the search space for a homophonic substitution ci-
pher. In the following we show several instances
of EXT ORDER, EXT LIMITS, SCORE, and PRUNE.
</bodyText>
<subsectionHeader confidence="0.989829">
4.2 Extension Limits (EXT LIMITS)
</subsectionHeader>
<bodyText confidence="0.977580416666667">
In addition to the implicit constraint of 0 being
a function Vf -+ Ve, one might be interested in
functions of a specific form:
For 1:1 substitution ciphers
(EXT LIMITS SIMPLE) 0 must fulfill that the
number of cipher letters f E Vf that map to any
e E Ve is at most one. Since partial hypotheses
violating this condition can never “recover” when
being extended, it becomes clear that these partial
hypotheses can be left out from search.
2n-best output can be implemented by returning the n best
scoring hypotheses in the final array H3.
</bodyText>
<page confidence="0.983961">
1570
</page>
<figureCaption confidence="0.7360384">
Figure 2: Illustration of the search space explored by the beam search algorithm with cipher vocabulary
Vf = {A, B, C, D}, plaintext vocabulary Ve = {a, b, c, d}, EXT ORDER = (B, C, A, D), homophonic
extension limits (EXT LIMITS HOMOPHONIC) with nmax = 4, and histogram pruning with nkeep = 4.
Hypotheses are visualized as nodes in the tree. The x-axis represents the extension order. At each level
only those 4 hypotheses that survived the histogram pruning process are extended.
</figureCaption>
<figure confidence="0.999340619047619">
a
b
c
d
. . .
a
b
c
d
. . .
. . .
a
b
c
d
a
b
c
d
. . .
a
b
c
d
. . .
a
b
c
d
a
b
0
c
a
b
c
d
. . .
a
b
c
d
. . .
a
b
c
d
a
b
c
d
B C A D
d
. . .
a
b
c
d
. . .
a
b
c
d
</figure>
<bodyText confidence="0.997359888888889">
Homophonic substitution ciphers can be han-
dled by the beam search algorithm, too. Here
the condition that 0 must fulfill is that the num-
ber of cipher letters f E Vf that map to any
e E Ve is at most nmax (which we will call
EXT LIMITS HOMOPHONIC). As soon as this con-
dition is violated, all further extensions will also
violate the condition. Thus, these partial hypothe-
ses can be left out.
</bodyText>
<subsectionHeader confidence="0.991687">
4.3 Score Estimation (SCORE)
</subsectionHeader>
<bodyText confidence="0.999886">
The score estimation function needs to predict
how good or bad a partial hypothesis (cipher func-
tion) might become. We propose simple heuristics
that use the n-gram counts rather than the original
ciphertext. The following formulas consider the
2-gram case. Equations for higher n-gram orders
can be obtained analogously.
With Equation 3 in mind, we want to estimate
the best possible score
</bodyText>
<equation confidence="0.922917818181818">
N+1H p(0&apos;(fj)j0&apos;(fj−1)) (4)
j=1
which can be obtained by extensions 0&apos; _Q 0. By
defining counts3
N+1
Nff0 = s(f, fi−1)s(f&apos;, fi) (5)
i=1
3δ denotes the Kronecker delta.
we can equivalently use the scores
E Nff0 log p(0&apos;(f&apos;)j0&apos;(f)) (6)
f,f0EV f
</equation>
<bodyText confidence="0.999946142857143">
Using this formulation it is easy to propose
a whole class of heuristics: We only present
the simplest heuristic, which we call TRIV-
IAL HEURISTIC. Its name stems from the fact that
it only evaluates those parts of a given 0&apos; that are
already fixed, and thus does not estimate any fu-
ture costs. Its score is calculated as
</bodyText>
<equation confidence="0.9869815">
E Nff0 log p(0&apos;(f&apos;)j0&apos;(f)). (7)
f,f0EO0
</equation>
<bodyText confidence="0.999684111111111">
Here f, f&apos; E 0&apos; denotes that f and f&apos; need to
be covered in 0&apos;. This heuristic is optimistic since
we implicitly use “0” as estimate for the non fixed
parts of the sum, for which Nff0 log p(·j·) � 0
holds.
It should be noted that this heuristic can be im-
plemented very efficiently. Given a partial hypoth-
esis 0 with given SCORE(0) the score of an exten-
sion 0&apos; can be calculated as
</bodyText>
<equation confidence="0.992558">
SCORE(0&apos;) = SCORE(0) + NEWLY FIXED(0, 0&apos;)
(8)
</equation>
<bodyText confidence="0.999482333333333">
where NEWLY FIXED only includes scores for
n-grams that have been newly fixed in 0&apos; during
the extension step from 0 to 0&apos;.
</bodyText>
<page confidence="0.957283">
1571
</page>
<subsectionHeader confidence="0.983109">
4.4 Extension Order (EXT ORDER)
</subsectionHeader>
<bodyText confidence="0.999605321428571">
For the choice which ciphertext symbol should be
fixed next during search, several possibilities ex-
ist: The overall goal is to choose an extension or-
der that leads to an overall low error rate. Intu-
itively it seems a good idea to first try to decipher
higher frequent words rather than the lowest fre-
quent ones. It is also clear that the choice of a good
extension order is dependent on the score estima-
tion function SCORE: The extension order should
lead to informative scores early on so that mislead-
ing hypotheses can be pruned out early.
In most of our experiments we will
make use of a very simple extension order:
HIGHEST UNIGRAM FREQUENCY simply fixes
the most frequent symbols first.
In case of the Zodiac-408, we use another strat-
egy that we call HIGHEST NGRAM COUNT ex-
tension order. In each step it greedily chooses
the symbol that will maximize the number of
fixed ciphertext n-grams. This strategy is use-
ful because the SCORE function we use is TRIV-
IAL HEURISTIC, which is not able to provide in-
formative scores if only few full n-grams are fixed.
larger vocabulary and so on. In our large vocabu-
lary word substitution cipher experiments we it-
eratively increase the vocabulary from the 1000
most frequent words, until we finally reach the
50000 most frequent words.
</bodyText>
<sectionHeader confidence="0.997528" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.849104869565217">
We conduct experiments on letter based 1:1 sub-
stitution ciphers, the homophonic substitution ci-
pher Zodiac-408, and word based 1:1 substitution
ciphers.
For a given reference mapping φref, we eval-
uate candidate mappings φ using two error mea-
sures: Mapping Error Rate MER(φ, φref) and
Symbol Error Rate SER(φ, φref). Roughly
speaking, SER reports the fraction of symbols
in the deciphered text that are not correct, while
MER reports the fraction of incorrect mappings
in φ.
Given a set of symbols Veval with unigram
counts N(v) for v ∈ Veval, and the total amount of
running symbols Neval = E N(v) we define
v∈Veval
4.5 Pruning (PRUNE) � 1 · δ(φ(v), φref (v))
We propose two pruning methods: MER= 1 − |Veval
HISTOGRAM PRUNING sorts all hypotheses v∈Veval N(v) · δ(φ(v), φref(v))
according to their score and then keeps only the � Neval
best nkeep hypotheses. SER = 1 −
THRESHOLD PRUNING keeps only those hy- v∈Veval
potheses φkeep for which
</bodyText>
<equation confidence="0.918215">
SCORE(φkeep) ≥ SCORE(φbest) − β (9)
</equation>
<bodyText confidence="0.9891065">
holds for a given parameter β ≥ 0. Even though
THRESHOLD PRUNING has the advantage of not
needing to sort all hypotheses, it has proven dif-
ficult to choose proper values for β. Due to this,
all experiments presented in this paper only use
HISTOGRAM PRUNING.
</bodyText>
<sectionHeader confidence="0.996161" genericHeader="method">
5 Iterative Beam Search
</sectionHeader>
<bodyText confidence="0.999943714285714">
(Ravi and Knight, 2011b) propose a so called “it-
erative EM algorithm”. The basic idea is to run a
decipherment algorithm—in their case an EM al-
gorithm based approach—on a subset of the vo-
cabulary. After having obtained the results from
the restricted vocabulary run, these results are used
to initialize a decipherment run with a larger vo-
cabulary. The results from this run will then be
used for a further decipherment run with an even
Thus the SER can be seen as a weighted form of
the MER, emphasizing errors for frequent words.
In decipherment experiments, SER will often be
lower than MER, since it is often easier to deci-
pher frequent words.
</bodyText>
<subsectionHeader confidence="0.995281">
6.1 Letter Substitution Ciphers
</subsectionHeader>
<bodyText confidence="0.999948181818182">
As ciphertext we use the text of the English
Wikipedia article about History4, remove all pic-
tures, tables, and captions, convert all letters to
lowercase, and then remove all non-letter and non-
space symbols. This corpus forms the basis for
shorter cryptograms of size 2, 4, 8, 16, 32, 64,128,
and 256—of which we generate 50 each. We make
sure that these shorter cryptograms do not end or
start in the middle of a word. We create the ci-
phertext using a 1:1 substitution cipher in which
we fix the mapping of the space symbol ’ ’. This
</bodyText>
<footnote confidence="0.954478">
4http://en.wikipedia.org/wiki/History
</footnote>
<page confidence="0.978572">
1572
</page>
<table confidence="0.9929595">
Order Beam MER [%] SER [%] RT [s]
3 10 33.15 25.27 0.01
100 12.00 6.95 0.06
1k 7.37 3.06 0.53
10k 5.10 1.42 5.33
100k 4.93 1.31 47.70
∞∗ 4.93 1.31 19 700.00
4 10 55.97 48.19 0.02
100 18.15 14.41 0.10
1k 5.13 3.42 0.89
10k 1.55 1.00 8.57
100k 0.39 0.06 81.34
5 10 69.19 60.13 0.02
100 35.57 29.02 0.14
1k 10.89 8.47 1.29
10k 0.38 0.06 11.91
100k 0.38 0.06 120.38
6 10 74.65 64.77 0.03
100 40.26 33.38 0.17
1k 13.53 10.08 1.58
10k 2.45 1.28 15.77
100k 0.09 0.05 151.85
</table>
<tableCaption confidence="0.997999">
Table 1: Symbol error rates (SER), Mapping er-
</tableCaption>
<bodyText confidence="0.990952">
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for decipherment of letter
substitution ciphers of length 128. Runtimes are
reported on a single core machine. Results for
beam size “∞” were obtained using A* search.
</bodyText>
<subsectionHeader confidence="0.738143">
Cipher Length
</subsectionHeader>
<bodyText confidence="0.9036504">
Figure 3: Symbol error rates for decipherment of
letter substitution ciphers of different lengths. Er-
ror bars show the 95% confidence interval based
on decipherment on 50 different ciphers. Beam
search was performed with a beam size of “100k”.
</bodyText>
<figure confidence="0.970370368421053">
2 4 8 16 32 64 128 256
100
90
80
60
50
40
30
20
70
10
0
Exact 2gram
Exact 3gram
Beam 3gram
Beam 4gram
Beam 5gram
Beam 6gram
Symbol Error Rate (%)
</figure>
<bodyText confidence="0.999260020833333">
makes our experiments comparable to those con-
ducted in (Ravi and Knight, 2008). Note that fix-
ing the ’ ’ symbol makes the problem much eas-
ier: The exact methods show much higher com-
putational demands for lengths beyond 256 letters
when not fixing the space symbol.
The plaintext language model we use a letter
based (Ve = {a, ... , z, }) language model trained
on a subset of the Gigaword corpus (Graff et al.,
2007).
We use extension limits fitting the 1:1 substi-
tution cipher nmax = 1 and histogram pruning
with different beam sizes.
For comparison we reimplemented the ILP ap-
proach from (Ravi and Knight, 2008) as well as
the A* approach from (Corlett and Penn, 2010).
Figure 3 shows the results of our algorithm for
different cipher length. We use a beam size of
100k for the 4, 5 and 6-gram case. Most remark-
ably our 6-gram beam search results are signifi-
cantly better than all methods presented in the lit-
erature. For the cipher length of 32 we obtain a
symbol error rate of just 4.1% where the optimal
solution (i.e. without search errors) for a 3-gram
language model has a symbol error rate as high as
38.3%.
Table 1 shows error rates and runtimes of our
algorithm for different beam sizes and language
model orders given a fixed ciphertext length of 128
letters. It can be seen that achieving close to op-
timal results is possible in a fraction of the CPU
time needed for the optimal solution: In the 3-
gram case the optimal solution is found in 1
400th
of the time needed using A* search. It can also
be seen that increasing the language model order
does not increase the runtime much while provid-
ing better results if the beam size is large enough:
If the beam size is not large enough, the decipher-
ment accuracy decreases when increasing the lan-
guage model order: This is because the higher or-
der heuristics do not give reliable scores if only
few n-grams are fixed.
To summarize: The beam search method is sig-
nificantly faster and obtains significantly better re-
sults than previously published methods. Further-
more it offers a good trade-off between CPU time
and decipherment accuracy.
</bodyText>
<page confidence="0.835884">
1573
</page>
<equation confidence="0.72453175">
i l i k e k i l l i n g p e o p l
e b e c a u s e i t i s s o m u c
h f u n i t i n m o r e f u n t h
a n k i l l i n g w i l d g a m e
i n t h e f o r r e s t b e c a u
s e m a n i s t h e m o a t r a n
g e r o u e a n a m a l o f a l l
t o k i l l s o m e t h i n g g i
</equation>
<figureCaption confidence="0.8884935">
Figure 4: First 136 letters of the Zodiac-408 cipher
and its decipherment.
</figureCaption>
<subsectionHeader confidence="0.999116">
6.2 Zodiac-408 Cipher
</subsectionHeader>
<bodyText confidence="0.999984777777778">
As ciphertext we use a transcription of the
Zodiac-408 cipher. It consists of 54 different sym-
bols and has a length of 408 symbols.5 The ci-
pher has been deciphered by hand before. It con-
tains some mistakes and ambiguities: For exam-
ple, it contains misspelled words like forrest (vs.
forest), experence (vs. experience), or paradice
(vs. paradise). Furthermore, the last 17 letters
of the cipher do not form understandable English
when applying the same homophonic substitution
that deciphers the rest of the cipher. This makes
the Zodiac-408 a good candidate for testing the ro-
bustness of a decipherment algorithm.
We assume a homophonic substitution cipher,
even though the cipher is not strictly homophonic:
It contains three cipher symbols that correspond
to two or more plaintext symbols. We ignore this
fact for our experiments, and count—in case of the
MER only—the decipherment for these symbols
as correct when the obtained mapping is contained
in the set of reference symbols. We use extension
limits with nmax = 8 and histogram pruning
with beam sizes of 10k up to 10M.
The plaintext language model is based on the
same subset of Gigaword (Graff et al., 2007) data
as the experiments for the letter substitution ci-
phers. However, we first removed all space sym-
</bodyText>
<footnote confidence="0.919042">
5hence its name
</footnote>
<table confidence="0.999446461538462">
Order Beam MER [%] SER [%] RT [s]
4 10k 71.43 67.16 222
100k 66.07 61.52 1 460
1M 39.29 34.80 12 701
10M 19.64 16.18 125 056
5 10k 94.64 96.57 257
100k 10.71 5.39 1 706
1M 8.93 3.19 14 724
10M 8.93 3.19 152 764
6 10k 87.50 84.80 262
100k 94.64 94.61 1 992
1M 8.93 2.70 17 701
10M 7.14 1.96 167 181
</table>
<tableCaption confidence="0.984322">
Table 2: Symbol error rates (SER), Mapping er-
</tableCaption>
<bodyText confidence="0.997335083333333">
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for the decipherment of the
Zodiac-408 cipher. Runtimes are reported on a
128-core machine.
bols from the training corpus before training the
actual letter based 4-gram, 5-gram, and 6-gram
language model on it. Other than (Ravi and
Knight, 2011a) we do not use any word lists and
by that avoid any degrees of freedom in how to in-
tegrate it into the search process: Only an n-gram
language model is used.
Figure 4 shows the first parts of the cipher and
our best decipherment. Table 2 shows the results
of our algorithm on the Zodiac-408 cipher for dif-
ferent language model orders and pruning settings.
To summarize: Our final decipherment—for
which we only use a 6-gram language model—has
a symbol error rate of only 2.0%, which is slightly
better than the best decipherment reported in (Ravi
and Knight, 2011a). They used an n-gram lan-
guage model together with a word dictionary and
obtained a symbol error rate of 2.2%. We thus ob-
tain better results with less modeling.
</bodyText>
<subsectionHeader confidence="0.997052">
6.3 Word Substitution Ciphers
</subsectionHeader>
<bodyText confidence="0.999924181818182">
As ciphertext, we use parts of the JRC corpus
(Steinberger et al., 2006) and the Gigaword cor-
pus (Graff et al., 2007). While the full JRC corpus
contains roughly 180k word types and consists of
approximately 70M running words, the full Giga-
word corpus contains around 2M word types and
roughly 1.5G running words.
We run experiments for three different setups:
The “JRC” and “Gigaword” setups use the first
half of the respective corpus as ciphertext, while
the plaintext language model of order n = 3 was
</bodyText>
<page confidence="0.969959">
1574
</page>
<table confidence="0.999243">
Setup Top MER [%] SER [%] RT [hh:mm]
Gigaword 1k 81.91 27.38 03h 10m
10k 30.29 8.55 09h 21m
20k 21.78 6.51 16h 25m
50k 19.40 5.96 49h 02m
JRC 1k 73.28 15.42 00h 32m
10k 15.82 2.61 13h 03m
JRC-Shuf 1k 76.83 19.04 00h 31m
10k 15.08 2.58 13h 03m
</table>
<tableCaption confidence="0.9964">
Table 3: Word error rates (WER), Mapping error
</tableCaption>
<bodyText confidence="0.9897011875">
rates (MER) and runtimes (RT) for iterative deci-
pherment run on the (TOP) most frequent words.
Error rates are evaluated on the full vocabulary.
Runtimes are reported on a 128-core machine.
trained on the second half. The “JRC-Shuf” setup
is created by randomly selecting half of the sen-
tences of the JRC corpus as ciphertext, while the
language model was trained on the complemen-
tary half of the corpus.
We encrypt the ciphertext using a 1:1 substi-
tution cipher on word level, imposing a much
larger vocabulary size. We use histogram prun-
ing with a beam size of 128 and use extension
limits of nmax = 1. Different to the previous
experiments, we use iterative beam search with
iterations as shown in Table 3.
The results for the Gigaword task are directly
comparable to the word substitution experiments
presented in (Dou and Knight, 2012). Their fi-
nal decipherment has a symbol error rate of 7.8%.
Our algorithm obtains 6.0% symbol error rate. It
should be noted that the improvements of 1.8%
symbol error rate correspond to a larger improve-
ment in terms of mapping error rate. This can also
be seen when looking at Table 3: An improvement
of the symbol error rate from 6.51% to 5.96% cor-
responds to an improvement of mapping error rate
from 21.78% to 19.40%.
To summarize: Using our beam search algo-
rithm in an iterative fashion, we are able to im-
prove the state-of-the-art decipherment accuracy
for word substitution ciphers.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977666666667">
We have presented a simple and effective beam
search approach to the decipherment problem. We
have shown in a variety of experiments—letter
substitution ciphers, the Zodiac-408, and word
substitution ciphers—that our approach outper-
forms the current state of the art while being con-
ceptually simpler and keeping computational de-
mands low.
We want to note that the presented algorithm is
not restricted to 1:1 and homophonic substitution
ciphers: It is possible to extend the algorithm to
solve n:m mappings. Along with more sophis-
ticated pruning strategies, score estimation func-
tions, and extension orders, this will be left for fu-
ture research.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999925333333333">
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. Experiments were
performed with computing resources granted by
JARA-HPC from RWTH Aachen University un-
der project “jara0040”.
</bodyText>
<sectionHeader confidence="0.999677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99976646875">
Andrew J. Clark. 1998. Optimisation heuristics for
cryptology. Ph.D. thesis, Faculty of Information
Technology, Queensland University of Technology.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040–1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 266–275,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium, Philadelphia.
George W. Hart. 1994. To decode short cryptograms.
Communications of the Association for Computing
Machinery (CACM), 37(9):102–108, September.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156–164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332–342, October.
</reference>
<page confidence="0.784774">
1575
</page>
<reference confidence="0.99946284">
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812–819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239–247, Portland, Oregon, June. Association for
Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011b. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12–21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaˇz Erjavec, and Dan Tufis¸. 2006.
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC), pages 2142–2147.
European Language Resources Association.
</reference>
<page confidence="0.992531">
1576
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399710">
<title confidence="0.793984">Beam Search for Solving Substitution Ciphers Nuhn Schamper</title>
<author confidence="0.600238">Human Language Technology</author>
<author confidence="0.600238">Pattern</author>
<affiliation confidence="0.990566">Computer Science Department, RWTH Aachen University, Aachen,</affiliation>
<email confidence="0.99568"><surname>@cs.rwth-aachen.de</email>
<abstract confidence="0.998707285714286">In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciand is able to use high order language models. We show experiments with 1:1 substitution ciphers in which the optimal solution for models has while our approach achieves decipherment error in a fraction of time using a language model. We also apply our approach to the famous and obtain slightly better (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterlanguage model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew J Clark</author>
</authors>
<title>Optimisation heuristics for cryptology.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Faculty of Information Technology, Queensland University of Technology.</institution>
<contexts>
<context position="2880" citStr="Clark, 1998" startWordPosition="428" endWordPosition="429"> significant improvements in decipherment accuracy in a variety of experiments while being computationally more effective than previous published works. 2 Related Work The experiments proposed in this paper touch many of previously published works in the decipherment field. Regarding the decipherment of 1:1 substitution ciphers various works have been published: Most older papers do not use a statistical approach and instead define some heuristic measures for scoring candidate decipherments. Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. (Clark, 1998) defines other suitability measures based on n-gram counts and presents a variety of optimization techniques like simulated annealing, genetic algorithms and tabu search. On the other hand, statistical approaches for 1:1 substitution ciphers were published in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We use our own implementation of these methods to report optimal solutions to 1:1 substitut</context>
</contexts>
<marker>Clark, 1998</marker>
<rawString>Andrew J. Clark. 1998. Optimisation heuristics for cryptology. Ph.D. thesis, Faculty of Information Technology, Queensland University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Corlett</author>
<author>Gerald Penn</author>
</authors>
<title>An exact A* method for deciphering letter-substitution ciphers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1040--1047</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3353" citStr="Corlett and Penn, 2010" startWordPosition="494" endWordPosition="497">r scoring candidate decipherments. Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. (Clark, 1998) defines other suitability measures based on n-gram counts and presents a variety of optimization techniques like simulated annealing, genetic algorithms and tabu search. On the other hand, statistical approaches for 1:1 substitution ciphers were published in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We use our own implementation of these methods to report optimal solutions to 1:1 substitution ci1568 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics phers for language model orders n = 2 and n = 3. (Ravi and Knight, 2011a) report the first automatic decipherment of the Zodiac-408 cipher. They use a combination of a 3-gram language model and a word dictionary. We run our beam search approach on the same cipher and repor</context>
<context position="6621" citStr="Corlett and Penn, 2010" startWordPosition="1108" endWordPosition="1111"> e. Such a table for substituting cipher tokens {A, B, C, D} with plaintext tokens {a, b, c, d} could for example look like a b c d A 0.1 0.2 0.3 0.4 B 0.4 0.2 0.1 0.3 C 0.4 0.1 0.2 0.3 D 0.3 0.4 0.2 0.1 ABCDECF when using the homophonic substitution a b c d A 1 0 0 0 B 0 1 0 0 C 1 0 0 0 D 0 0 0 1 E 0 0 1 0 F 0 1 0 0 We will use the definition s(e|f) (2) to characterize the maximum number of different cipher symbols allowed per plaintext symbol. We formalize the 1:1 substitutions with a bijective function 0 : Vf → Ve and homophonic substitutions with a general function 0 : Vf → Ve. Following (Corlett and Penn, 2010), we call cipher functions 0, for which not all 0(f)’s are fixed, partial cipher functions . Further, 0&apos; is said to extend 0, if for all f that are fixed in 0, it holds that f is also fixed in 0&apos; with 0&apos;(f) = 0(f). � nmax = Max e f 1569 The cardinality of 0 counts the number of fixed f’s in 0. When talking about partial cipher functions we use the notation for relations, in which 0 C Vf x Ve. For example with 0 = {(A, a)} 00 = {(A, a), (B, b)} it follows that 0 C100 and |0 |= 1 |00 |= 2 0(A) = a 00(A) = a 0(B) = undefined 00(B) = b The general decipherment goal is to obtain a mapping 0 such th</context>
<context position="18448" citStr="Corlett and Penn, 2010" startWordPosition="3282" endWordPosition="3285">cted in (Ravi and Knight, 2008). Note that fixing the ’ ’ symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol. The plaintext language model we use a letter based (Ve = {a, ... , z, }) language model trained on a subset of the Gigaword corpus (Graff et al., 2007). We use extension limits fitting the 1:1 substitution cipher nmax = 1 and histogram pruning with different beam sizes. For comparison we reimplemented the ILP approach from (Ravi and Knight, 2008) as well as the A* approach from (Corlett and Penn, 2010). Figure 3 shows the results of our algorithm for different cipher length. We use a beam size of 100k for the 4, 5 and 6-gram case. Most remarkably our 6-gram beam search results are significantly better than all methods presented in the literature. For the cipher length of 32 we obtain a symbol error rate of just 4.1% where the optimal solution (i.e. without search errors) for a 3-gram language model has a symbol error rate as high as 38.3%. Table 1 shows error rates and runtimes of our algorithm for different beam sizes and language model orders given a fixed ciphertext length of 128 letters</context>
</contexts>
<marker>Corlett, Penn, 2010</marker>
<rawString>Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering letter-substitution ciphers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040–1047, Uppsala, Sweden, July. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Dou</author>
<author>Kevin Knight</author>
</authors>
<title>Large scale decipherment for out-of-domain machine translation.</title>
<date>2012</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>266--275</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="4201" citStr="Dou and Knight, 2012" startWordPosition="629" endWordPosition="632">pages 1568–1576, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics phers for language model orders n = 2 and n = 3. (Ravi and Knight, 2011a) report the first automatic decipherment of the Zodiac-408 cipher. They use a combination of a 3-gram language model and a word dictionary. We run our beam search approach on the same cipher and report better results without using an additional word dictionary—just by using a high order n-gram language model. (Ravi and Knight, 2011b) report experiments on large vocabulary substitution ciphers based on the Transtac corpus. (Dou and Knight, 2012) improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus. We run our method on the same corpus and report improvements over the state of the art. (Ravi and Knight, 2011b) and (Nuhn et al., 2012) have shown that—even for larger vocabulary sizes—it is possible to learn a full translation model from non-parallel data. Even though this work is currently only able to deal with substitution ciphers, phenomena like reordering, insertions and deletions can in principle be included in our approach. The 1:1 substitution </context>
<context position="24637" citStr="Dou and Knight, 2012" startWordPosition="4460" endWordPosition="4463">he “JRC-Shuf” setup is created by randomly selecting half of the sentences of the JRC corpus as ciphertext, while the language model was trained on the complementary half of the corpus. We encrypt the ciphertext using a 1:1 substitution cipher on word level, imposing a much larger vocabulary size. We use histogram pruning with a beam size of 128 and use extension limits of nmax = 1. Different to the previous experiments, we use iterative beam search with iterations as shown in Table 3. The results for the Gigaword task are directly comparable to the word substitution experiments presented in (Dou and Knight, 2012). Their final decipherment has a symbol error rate of 7.8%. Our algorithm obtains 6.0% symbol error rate. It should be noted that the improvements of 1.8% symbol error rate correspond to a larger improvement in terms of mapping error rate. This can also be seen when looking at Table 3: An improvement of the symbol error rate from 6.51% to 5.96% corresponds to an improvement of mapping error rate from 21.78% to 19.40%. To summarize: Using our beam search algorithm in an iterative fashion, we are able to improve the state-of-the-art decipherment accuracy for word substitution ciphers. 7 Conclusi</context>
</contexts>
<marker>Dou, Knight, 2012</marker>
<rawString>Qing Dou and Kevin Knight. 2012. Large scale decipherment for out-of-domain machine translation. In Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 266–275, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Third Edition. Linguistic Data Consortium,</title>
<date>2007</date>
<location>Philadelphia.</location>
<contexts>
<context position="18194" citStr="Graff et al., 2007" startWordPosition="3238" endWordPosition="3241">s. Beam search was performed with a beam size of “100k”. 2 4 8 16 32 64 128 256 100 90 80 60 50 40 30 20 70 10 0 Exact 2gram Exact 3gram Beam 3gram Beam 4gram Beam 5gram Beam 6gram Symbol Error Rate (%) makes our experiments comparable to those conducted in (Ravi and Knight, 2008). Note that fixing the ’ ’ symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol. The plaintext language model we use a letter based (Ve = {a, ... , z, }) language model trained on a subset of the Gigaword corpus (Graff et al., 2007). We use extension limits fitting the 1:1 substitution cipher nmax = 1 and histogram pruning with different beam sizes. For comparison we reimplemented the ILP approach from (Ravi and Knight, 2008) as well as the A* approach from (Corlett and Penn, 2010). Figure 3 shows the results of our algorithm for different cipher length. We use a beam size of 100k for the 4, 5 and 6-gram case. Most remarkably our 6-gram beam search results are significantly better than all methods presented in the literature. For the cipher length of 32 we obtain a symbol error rate of just 4.1% where the optimal solutio</context>
<context position="21420" citStr="Graff et al., 2007" startWordPosition="3883" endWordPosition="3886">andidate for testing the robustness of a decipherment algorithm. We assume a homophonic substitution cipher, even though the cipher is not strictly homophonic: It contains three cipher symbols that correspond to two or more plaintext symbols. We ignore this fact for our experiments, and count—in case of the MER only—the decipherment for these symbols as correct when the obtained mapping is contained in the set of reference symbols. We use extension limits with nmax = 8 and histogram pruning with beam sizes of 10k up to 10M. The plaintext language model is based on the same subset of Gigaword (Graff et al., 2007) data as the experiments for the letter substitution ciphers. However, we first removed all space sym5hence its name Order Beam MER [%] SER [%] RT [s] 4 10k 71.43 67.16 222 100k 66.07 61.52 1 460 1M 39.29 34.80 12 701 10M 19.64 16.18 125 056 5 10k 94.64 96.57 257 100k 10.71 5.39 1 706 1M 8.93 3.19 14 724 10M 8.93 3.19 152 764 6 10k 87.50 84.80 262 100k 94.64 94.61 1 992 1M 8.93 2.70 17 701 10M 7.14 1.96 167 181 Table 2: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for the decipherment of th</context>
<context position="23113" citStr="Graff et al., 2007" startWordPosition="4191" endWordPosition="4194">s of our algorithm on the Zodiac-408 cipher for different language model orders and pruning settings. To summarize: Our final decipherment—for which we only use a 6-gram language model—has a symbol error rate of only 2.0%, which is slightly better than the best decipherment reported in (Ravi and Knight, 2011a). They used an n-gram language model together with a word dictionary and obtained a symbol error rate of 2.2%. We thus obtain better results with less modeling. 6.3 Word Substitution Ciphers As ciphertext, we use parts of the JRC corpus (Steinberger et al., 2006) and the Gigaword corpus (Graff et al., 2007). While the full JRC corpus contains roughly 180k word types and consists of approximately 70M running words, the full Gigaword corpus contains around 2M word types and roughly 1.5G running words. We run experiments for three different setups: The “JRC” and “Gigaword” setups use the first half of the respective corpus as ciphertext, while the plaintext language model of order n = 3 was 1574 Setup Top MER [%] SER [%] RT [hh:mm] Gigaword 1k 81.91 27.38 03h 10m 10k 30.29 8.55 09h 21m 20k 21.78 6.51 16h 25m 50k 19.40 5.96 49h 02m JRC 1k 73.28 15.42 00h 32m 10k 15.82 2.61 13h 03m JRC-Shuf 1k 76.83 </context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George W Hart</author>
</authors>
<title>To decode short cryptograms.</title>
<date>1994</date>
<journal>Communications of the Association for Computing Machinery (CACM),</journal>
<volume>37</volume>
<issue>9</issue>
<contexts>
<context position="2793" citStr="Hart, 1994" startWordPosition="413" endWordPosition="414"> and can be adjusted to account for a given amount of computational resources. We show significant improvements in decipherment accuracy in a variety of experiments while being computationally more effective than previous published works. 2 Related Work The experiments proposed in this paper touch many of previously published works in the decipherment field. Regarding the decipherment of 1:1 substitution ciphers various works have been published: Most older papers do not use a statistical approach and instead define some heuristic measures for scoring candidate decipherments. Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. (Clark, 1998) defines other suitability measures based on n-gram counts and presents a variety of optimization techniques like simulated annealing, genetic algorithms and tabu search. On the other hand, statistical approaches for 1:1 substitution ciphers were published in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We u</context>
</contexts>
<marker>Hart, 1994</marker>
<rawString>George W. Hart. 1994. To decode short cryptograms. Communications of the Association for Computing Machinery (CACM), 37(9):102–108, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Deciphering foreign language by combining language models and context vectors.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>156--164</pages>
<institution>Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic of</location>
<contexts>
<context position="4479" citStr="Nuhn et al., 2012" startWordPosition="678" endWordPosition="681">ge model and a word dictionary. We run our beam search approach on the same cipher and report better results without using an additional word dictionary—just by using a high order n-gram language model. (Ravi and Knight, 2011b) report experiments on large vocabulary substitution ciphers based on the Transtac corpus. (Dou and Knight, 2012) improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus. We run our method on the same corpus and report improvements over the state of the art. (Ravi and Knight, 2011b) and (Nuhn et al., 2012) have shown that—even for larger vocabulary sizes—it is possible to learn a full translation model from non-parallel data. Even though this work is currently only able to deal with substitution ciphers, phenomena like reordering, insertions and deletions can in principle be included in our approach. The 1:1 substitution cipher encrypts a given plaintext into a ciphertext by replacing each plaintext token with a unique substitute: This means that the table s(e|f) contains all zeroes, except for one “1.0” per f ∈ Vf and one “1.0” per e ∈ Ve. For example the text abadcab would be enciphered to BC</context>
</contexts>
<marker>Nuhn, Mauser, Ney, 2012</marker>
<rawString>Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 156–164, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin Olson</author>
</authors>
<title>Robust dictionary attack of short simple substitution ciphers.</title>
<date>2007</date>
<journal>Cryptologia,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="2811" citStr="Olson, 2007" startWordPosition="416" endWordPosition="417">ted to account for a given amount of computational resources. We show significant improvements in decipherment accuracy in a variety of experiments while being computationally more effective than previous published works. 2 Related Work The experiments proposed in this paper touch many of previously published works in the decipherment field. Regarding the decipherment of 1:1 substitution ciphers various works have been published: Most older papers do not use a statistical approach and instead define some heuristic measures for scoring candidate decipherments. Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. (Clark, 1998) defines other suitability measures based on n-gram counts and presents a variety of optimization techniques like simulated annealing, genetic algorithms and tabu search. On the other hand, statistical approaches for 1:1 substitution ciphers were published in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We use our own impleme</context>
</contexts>
<marker>Olson, 2007</marker>
<rawString>Edwin Olson. 2007. Robust dictionary attack of short simple substitution ciphers. Cryptologia, 31(4):332–342, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Attacking decipherment problems optimally with low-order ngram models.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>812--819</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3206" citStr="Ravi and Knight, 2008" startWordPosition="472" endWordPosition="475">ution ciphers various works have been published: Most older papers do not use a statistical approach and instead define some heuristic measures for scoring candidate decipherments. Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. (Clark, 1998) defines other suitability measures based on n-gram counts and presents a variety of optimization techniques like simulated annealing, genetic algorithms and tabu search. On the other hand, statistical approaches for 1:1 substitution ciphers were published in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We use our own implementation of these methods to report optimal solutions to 1:1 substitution ci1568 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics phers for language model orders n = 2 and n = 3. (Ravi and Knight, 2011a) report the first automatic decipherment of the Zodia</context>
<context position="17856" citStr="Ravi and Knight, 2008" startWordPosition="3174" endWordPosition="3177">ubstitution ciphers of length 128. Runtimes are reported on a single core machine. Results for beam size “∞” were obtained using A* search. Cipher Length Figure 3: Symbol error rates for decipherment of letter substitution ciphers of different lengths. Error bars show the 95% confidence interval based on decipherment on 50 different ciphers. Beam search was performed with a beam size of “100k”. 2 4 8 16 32 64 128 256 100 90 80 60 50 40 30 20 70 10 0 Exact 2gram Exact 3gram Beam 3gram Beam 4gram Beam 5gram Beam 6gram Symbol Error Rate (%) makes our experiments comparable to those conducted in (Ravi and Knight, 2008). Note that fixing the ’ ’ symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol. The plaintext language model we use a letter based (Ve = {a, ... , z, }) language model trained on a subset of the Gigaword corpus (Graff et al., 2007). We use extension limits fitting the 1:1 substitution cipher nmax = 1 and histogram pruning with different beam sizes. For comparison we reimplemented the ILP approach from (Ravi and Knight, 2008) as well as the A* approach from (Corlett and Penn, 2010). Figure</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order ngram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 812–819, Honolulu, Hawaii. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Bayesian inference for Zodiac and other homophonic ciphers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>239--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="3751" citStr="Ravi and Knight, 2011" startWordPosition="557" endWordPosition="560">lished in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We use our own implementation of these methods to report optimal solutions to 1:1 substitution ci1568 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics phers for language model orders n = 2 and n = 3. (Ravi and Knight, 2011a) report the first automatic decipherment of the Zodiac-408 cipher. They use a combination of a 3-gram language model and a word dictionary. We run our beam search approach on the same cipher and report better results without using an additional word dictionary—just by using a high order n-gram language model. (Ravi and Knight, 2011b) report experiments on large vocabulary substitution ciphers based on the Transtac corpus. (Dou and Knight, 2012) improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus. We run o</context>
<context position="15343" citStr="Ravi and Knight, 2011" startWordPosition="2721" endWordPosition="2724">We propose two pruning methods: MER= 1 − |Veval HISTOGRAM PRUNING sorts all hypotheses v∈Veval N(v) · δ(φ(v), φref(v)) according to their score and then keeps only the � Neval best nkeep hypotheses. SER = 1 − THRESHOLD PRUNING keeps only those hy- v∈Veval potheses φkeep for which SCORE(φkeep) ≥ SCORE(φbest) − β (9) holds for a given parameter β ≥ 0. Even though THRESHOLD PRUNING has the advantage of not needing to sort all hypotheses, it has proven difficult to choose proper values for β. Due to this, all experiments presented in this paper only use HISTOGRAM PRUNING. 5 Iterative Beam Search (Ravi and Knight, 2011b) propose a so called “iterative EM algorithm”. The basic idea is to run a decipherment algorithm—in their case an EM algorithm based approach—on a subset of the vocabulary. After having obtained the results from the restricted vocabulary run, these results are used to initialize a decipherment run with a larger vocabulary. The results from this run will then be used for a further decipherment run with an even Thus the SER can be seen as a weighted form of the MER, emphasizing errors for frequent words. In decipherment experiments, SER will often be lower than MER, since it is often easier to</context>
<context position="22238" citStr="Ravi and Knight, 2011" startWordPosition="4036" endWordPosition="4039">M 39.29 34.80 12 701 10M 19.64 16.18 125 056 5 10k 94.64 96.57 257 100k 10.71 5.39 1 706 1M 8.93 3.19 14 724 10M 8.93 3.19 152 764 6 10k 87.50 84.80 262 100k 94.64 94.61 1 992 1M 8.93 2.70 17 701 10M 7.14 1.96 167 181 Table 2: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for the decipherment of the Zodiac-408 cipher. Runtimes are reported on a 128-core machine. bols from the training corpus before training the actual letter based 4-gram, 5-gram, and 6-gram language model on it. Other than (Ravi and Knight, 2011a) we do not use any word lists and by that avoid any degrees of freedom in how to integrate it into the search process: Only an n-gram language model is used. Figure 4 shows the first parts of the cipher and our best decipherment. Table 2 shows the results of our algorithm on the Zodiac-408 cipher for different language model orders and pruning settings. To summarize: Our final decipherment—for which we only use a 6-gram language model—has a symbol error rate of only 2.0%, which is slightly better than the best decipherment reported in (Ravi and Knight, 2011a). They used an n-gram language mo</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011a. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 239–247, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT),</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3751" citStr="Ravi and Knight, 2011" startWordPosition="557" endWordPosition="560">lished in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗ search. We use our own implementation of these methods to report optimal solutions to 1:1 substitution ci1568 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics phers for language model orders n = 2 and n = 3. (Ravi and Knight, 2011a) report the first automatic decipherment of the Zodiac-408 cipher. They use a combination of a 3-gram language model and a word dictionary. We run our beam search approach on the same cipher and report better results without using an additional word dictionary—just by using a high order n-gram language model. (Ravi and Knight, 2011b) report experiments on large vocabulary substitution ciphers based on the Transtac corpus. (Dou and Knight, 2012) improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus. We run o</context>
<context position="15343" citStr="Ravi and Knight, 2011" startWordPosition="2721" endWordPosition="2724">We propose two pruning methods: MER= 1 − |Veval HISTOGRAM PRUNING sorts all hypotheses v∈Veval N(v) · δ(φ(v), φref(v)) according to their score and then keeps only the � Neval best nkeep hypotheses. SER = 1 − THRESHOLD PRUNING keeps only those hy- v∈Veval potheses φkeep for which SCORE(φkeep) ≥ SCORE(φbest) − β (9) holds for a given parameter β ≥ 0. Even though THRESHOLD PRUNING has the advantage of not needing to sort all hypotheses, it has proven difficult to choose proper values for β. Due to this, all experiments presented in this paper only use HISTOGRAM PRUNING. 5 Iterative Beam Search (Ravi and Knight, 2011b) propose a so called “iterative EM algorithm”. The basic idea is to run a decipherment algorithm—in their case an EM algorithm based approach—on a subset of the vocabulary. After having obtained the results from the restricted vocabulary run, these results are used to initialize a decipherment run with a larger vocabulary. The results from this run will then be used for a further decipherment run with an even Thus the SER can be seen as a weighted form of the MER, emphasizing errors for frequent words. In decipherment experiments, SER will often be lower than MER, since it is often easier to</context>
<context position="22238" citStr="Ravi and Knight, 2011" startWordPosition="4036" endWordPosition="4039">M 39.29 34.80 12 701 10M 19.64 16.18 125 056 5 10k 94.64 96.57 257 100k 10.71 5.39 1 706 1M 8.93 3.19 14 724 10M 8.93 3.19 152 764 6 10k 87.50 84.80 262 100k 94.64 94.61 1 992 1M 8.93 2.70 17 701 10M 7.14 1.96 167 181 Table 2: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for the decipherment of the Zodiac-408 cipher. Runtimes are reported on a 128-core machine. bols from the training corpus before training the actual letter based 4-gram, 5-gram, and 6-gram language model on it. Other than (Ravi and Knight, 2011a) we do not use any word lists and by that avoid any degrees of freedom in how to integrate it into the search process: Only an n-gram language model is used. Figure 4 shows the first parts of the cipher and our best decipherment. Table 2 shows the results of our algorithm on the Zodiac-408 cipher for different language model orders and pruning settings. To summarize: Our final decipherment—for which we only use a 6-gram language model—has a symbol error rate of only 2.0%, which is slightly better than the best decipherment reported in (Ravi and Knight, 2011a). They used an n-gram language mo</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011b. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT), pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Anna Widiger</author>
<author>Camelia Ignat</author>
<author>Tomaˇz Erjavec</author>
<author>Dan Tufis¸</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In</title>
<date>2006</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2142--2147</pages>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis¸, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaˇz Erjavec, and Dan Tufis¸. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 2142–2147. European Language Resources Association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>