<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.99564">
Applying Alternating Structure Optimization
to Word Sense Disambiguation
</title>
<author confidence="0.703332">
Rie Kubota Ando
</author>
<affiliation confidence="0.507505">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.483996">
Yorktown Heights, NY 10598, U.S.A.
</address>
<email confidence="0.94364">
rie1@us.ibm.com
</email>
<sectionHeader confidence="0.996795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999159636363636">
This paper presents a new application of
the recently proposed machine learning
method Alternating Structure Optimiza-
tion (ASO), to word sense disambiguation
(WSD). Given a set of WSD problems
and their respective labeled examples, we
seek to improve overall performance on
that set by using all the labeled exam-
ples (irrespective of target words) for the
entire set in learning a disambiguator for
each individual problem. Thus, in effect,
on each individual problem (e.g., disam-
biguation of “art”) we benefit from train-
ing examples for other problems (e.g.,
disambiguation of “bar”, “canal”, and so
forth). We empirically study the effective
use of ASO for this purpose in the multi-
task and semi-supervised learning config-
urations. Our performance results rival
or exceed those of the previous best sys-
tems on several Senseval lexical sample
task data sets.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996975">
Word sense disambiguation (WSD) is the task of
assigning pre-defined senses to words occurring in
some context. An example is to disambiguate an oc-
currence of “bank” between the “money bank” sense
and the “river bank” sense. Previous studies e.g.,
(Lee and Ng, 2002; Florian and Yarowsky, 2002),
have applied supervised learning techniques to WSD
with success.
A practical issue that arises in supervised WSD
is the paucity of labeled examples (sense-annotated
data) available for training. For example, the train-
ing set of the Senseval-21 English lexical sample
</bodyText>
<footnote confidence="0.981395">
1http://www.cs.unt.edu/-rada/senseval/. WSD systems have
</footnote>
<page confidence="0.995105">
77
</page>
<bodyText confidence="0.999972405405405">
task has only 10 labeled training examples per sense
on average, which is in contrast to nearly 6K training
examples per name class (on average) used for the
CoNLL-2003 named entity chunking shared task2.
One problem is that there are so many words and so
many senses that it is hard to make available a suf-
ficient number of labeled training examples for each
of a large number of target words.
On the other hand, this indicates that the total
number of available labeled examples (irrespective
of target words) can be relatively large. A natural
question to ask is whether we can effectively use all
the labeled examples (irrespective of target words)
for learning on each individual WSD problem.
Based on these observations, we study a new
application of Alternating Structure Optimization
(ASO) (Ando and Zhang, 2005a; Ando and Zhang,
2005b) to WSD. ASO is a recently proposed ma-
chine learning method for learning predictive struc-
ture (i.e., information useful for predictions) shared
by multiple prediction problems via joint empiri-
cal risk minimization. It has been shown that on
several tasks, performance can be significantly im-
proved by a semi-supervised application of ASO,
which obtains useful information from unlabeled
data by learning automatically created prediction
problems. In addition to such semi-supervised learn-
ing, this paper explores ASO multi-task learning,
which learns a number of WSD problems simul-
taneously to exploit the inherent predictive struc-
ture shared by these WSD problems. Thus, in ef-
fect, each individual problem (e.g., disambiguation
of “art”) benefits from labeled training examples for
other problems (e.g., disambiguation of “bar”, dis-
ambiguation of “canal”, and so forth).
The notion of benefiting from training data for
other word senses is not new by itself. For instance,
</bodyText>
<footnote confidence="0.9951745">
been evaluated in the series of Senseval workshops.
2http://www.cnts.ua.ac.be/conll2003/ner/
</footnote>
<note confidence="0.939952">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL -X),
pages 77–84, New York City, June 2006. @c 2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999410166666666">
on the WSD task with respect to WordNet synsets,
Kohomban and Lee (2005) trained classifiers for the
top-level synsets of the WordNet semantic hierar-
chy, consolidating labeled examples associated with
the WordNet sub-trees. To disambiguate test in-
stances, these coarse-grained classifiers are first ap-
plied, and then fine-grained senses are determined
using a heuristic mapping. By contrast, our ap-
proach does not require pre-defined relations among
senses such as the WordNet hierarchy. Rather, we
let the machine learning algorithm ASO automati-
cally and implicitly find relations with respect to the
disambiguation problems (i.e., finding shared pre-
dictive structure). Interestingly, in our experiments,
seemingly unrelated or only loosely related word-
sense pairs help to improve performance.
This paper makes two contributions. First, we
present a new application of ASO to WSD. We em-
pirically study the effective use of ASO and show
that labeled examples of all the words can be effec-
tively exploited in learning each individual disam-
biguator. Second, we report performance results that
rival or exceed the state-of-the-art systems on Sen-
seval lexical sample tasks.
</bodyText>
<subsectionHeader confidence="0.558343">
2 Alternating structure optimization
</subsectionHeader>
<bodyText confidence="0.999232">
This section gives a brief summary of ASO. We first
introduce a standard linear prediction model for a
single task and then extend it to a joint linear model
used by ASO.
</bodyText>
<subsectionHeader confidence="0.99822">
2.1 Standard linear prediction models
</subsectionHeader>
<bodyText confidence="0.906726411764706">
In the standard formulation of supervised learning,
we seek a predictor that maps an input vector (or
feature vector)xEXto the corresponding output
y E Y. For NLP tasks, binary features are often used
– for example, if the word to the left is “money”, set
the corresponding entry of x to 1; otherwise, set it to
0. Ah-way classification problem can be cast ash
binary classification problems, regarding output y =
+1 and y = -1 as “in-class” and “out-of-class”,
respectively.
Predictors based on linear prediction models take
the form: f (x) = wTx, where w is called a weight
vector. A common method to obtain a predictor
�fis regularized empirical risk minimization, which
minimizes an empirical loss of the predictor (with
regularization) on thenlabeled training examples
{(Xi,Yi)}:
</bodyText>
<equation confidence="0.965747">
L(f(Xi),Yi)+r(f)�.(1)
</equation>
<bodyText confidence="0.9990275">
A loss functionL(•)quantifies the difference be-
tween the predictionf(Xi)and the true outputYi,
andr(•)is a regularization term to control the model
complexity.
</bodyText>
<subsectionHeader confidence="0.998992">
2.2 Joint linear models for ASO
</subsectionHeader>
<bodyText confidence="0.994576875">
problems indexed
Considermprediction by f E
{1 ... m}, each with nt samples (Xti, Yt) for i E
{1 ... nt}, and assume that there exists a low-
dimensional predictive structure shared by these m
problems. Ando and Zhang (2005a) extend the
above traditional linear model to a joint linear model
so that a predictor for problem f is in the form:
ft(O x) = wTt x + vTt Ox OOT = I (2)
where I is the identity matrix. wt and vt are
weight vectors specific to each problem `. Predic-
tive structure is parameterized by the structure ma-
trix O shared by all the m predictors. The goal of
this model can also be regarded as learning a com-
mon good feature map Ox used for all the m prob-
lems.
</bodyText>
<subsectionHeader confidence="0.998888">
2.3 ASO algorithm
</subsectionHeader>
<bodyText confidence="0.998566333333333">
Analogous to (1), we computeOand predictors so
that they minimize the empirical risk summed over
all the problems:
</bodyText>
<equation confidence="0.848884">
+r(f`)!.(3)
</equation>
<bodyText confidence="0.999335">
It has been shown in (Ando and Zhang, 2005a) that
the optimization problem (3) has a simple solution
using singular value decomposition (SVD) when we
choose square regularization: r(ft) = A 1 1 wt 1 1 2
where A is a regularization parameter. Let ut =
wt + OTvt . Then (3) becomes the minimization
of the joint empirical risk written as:
</bodyText>
<equation confidence="0.747074">
+allu`—~T&amp;quot;`ll22!�(4)
</equation>
<bodyText confidence="0.997007333333333">
This minimization can be approximately solved by
repeating the following alternating optimization pro-
cedure until a convergence criterion is met:
</bodyText>
<figure confidence="0.890409875">
�f=argmin
f
�i��n
[o,{ ^f`}]=argmin `=1 Xm Xi=1nQ L(f`(~,X`i),Y`i)
0;�fil n`
L(uT`X`i,Y`i)
n`
`=1 Xm Xi=1nQ
</figure>
<page confidence="0.994768">
78
</page>
<note confidence="0.5989182">
Nouns art, authority, bar, bum, chair, channel, child, church, circuit, day, detention, dyke, facility, fatigue, feeling,
grip, hearth, holiday, lady, material, mouth, nation, nature, post, restraint, sense, spade, stress, yew
Verbs begin, call, carry, collaborate, develop, draw, dress, drift, drive, face, ferret, find, keep, leave, live, match,
play, pull, replace, see, serve strike, train, treat, turn, use, wander wash, work
Adjectives blind, colourless, cool, faithful, fine, fit, free, graceful, green, local, natural, oblique, simple, solemn, vital
</note>
<figureCaption confidence="0.983873">
Figure 1: Words to be disambiguated; Senseval-2 English lexical sample task.
</figureCaption>
<listItem confidence="0.9905445">
1. Fix(O,{vt}), and findmpredictors{ut}that
minimizes the joint empirical risk (4).
2. Fixmpredictors{ut}, and find(O,{vt})that
minimizes the joint empirical risk (4).
</listItem>
<bodyText confidence="0.999978047619048">
The first step is equivalent to training m predictors
independently. The second step, which couples all
the predictors, can be done by setting the rows of
O to the most significant left singular vectors of the
predictor (weight) matrix U = [u1,... , ur,,], and
setting vt = Out. That is, the structure matrix O is
computed so that the projection of the predictor ma-
trix U onto the subspace spanned by O’s rows gives
the best approximation (in the least squares sense)
of U for the given row-dimension of O. Thus, in-
tuitively, O captures the commonality of the m pre-
dictors.
ASO has been shown to be useful in its semi-
supervised learning configuration, where the above
algorithm is applied to a number of auxiliary prob-
lems that are automatically created from the unla-
beled data. By contrast, the focus of this paper is the
multi-task learning configuration, where the ASO
algorithm is applied to a number of real problems
with the goal of improving overall performance on
these problems.
</bodyText>
<sectionHeader confidence="0.986111" genericHeader="method">
3 Effective use of ASO on word sense
disambiguation
</sectionHeader>
<bodyText confidence="0.989115526315789">
The essence of ASO is to learn information useful
for prediction (predictive structure) shared by mul-
tiple tasks, assuming the existence of such shared
structure. From this viewpoint, consider the target
words of the Senseval-2 lexical sample task, shown
in Figure 1. Here we have multiple disambiguation
tasks; however, at a first glance, it is not entirely
clear whether these tasks share predictive structure
(or are related to each other). There is no direct se-
mantic relationship (such as synonym or hyponym
relations) among these words.
Local word uni-grams in 5-word window,
context word bi- and tri-grams of (w_2, w_1),
(w+1,w+2),(w_1,w+1),
(w-3,w-2,w-1),(w+1,w+2,w+3),
(w-2,w-1,w+1),(w-1,w+1,w+2).
Syntactic full parser output; see Section 3 for detail.
Global all the words excluding stopwords.
POS uni-, bi-, and tri-grams in 5-word window.
</bodyText>
<figureCaption confidence="0.875133">
Figure 2: Features.wzstands for the word at positioni
relative to the word to be disambiguated. The 5-word win-
dow is[-2,+2]. Local context and POS features are position-
sensitive. Global context features are position insensitive (a bag
of words).
</figureCaption>
<bodyText confidence="0.999929892857143">
The goal of this section is to empirically study
the effective use of ASO for improving overall per-
formance on these seemingly unrelated disambigua-
tion problems. Below we first describe the task set-
ting, features, and algorithms used in our imple-
mentation, and then experiment with the Senseval-
2 English lexical sample data set (with the offi-
cial training / test split) for the development of our
methods. We will then evaluate the methods de-
veloped on the Senseval-2 data set by carrying out
the Senseval-3 tasks, i.e., training on the Senseval-3
training data and then evaluating the results on the
(unseen) Senseval-3 test sets in Section 4.
Task setting In this work, we focus on the Sense-
val lexical sample task. We are given a set of target
words, each of which is associated with several pos-
sible senses, and their labeled instances for training.
Each instance contains an occurrence of one of the
target words and its surrounding words, typically a
few sentences. The task is to assign a sense to each
test instance.
Features We adopt the feature design used by Lee
and Ng (2002), which consists of the following
four types: (1) Local context: n-grams of nearby
words (position sensitive); (2) Global context: all
the words (excluding stopwords) in the given con-
text (position-insensitive; a bag of words); (3) POS:
parts-of-speech n-grams of nearby words; (4) Syn-
</bodyText>
<page confidence="0.991446">
79
</page>
<bodyText confidence="0.9998531875">
tactic relations: syntactic information obtained from
parser output. To generate syntactic relation fea-
tures, we use the Slot Grammar-based full parser
ESG (McCord, 1990). We use as features syntactic
relation types (e.g., subject-of, object-of, and noun
modifier), participants of syntactic relations, and bi-
grams of syntactic relations / participants. Details of
the other three types are shown in Figure 2.
Implementation Our implementation follows
Ando and Zhang (2005a). We use a modifi-
cation of the Huber’s robust loss for regression:
L(p,y)=(max(0,1—py))2ifpy&gt;—1; and—4py
otherwise; with square regularization (A=10-4),
and perform empirical risk minimization by
stochastic gradient descent (SGD) (see e.g., Zhang
(2004)). We perform one ASO iteration.
</bodyText>
<subsectionHeader confidence="0.8477135">
3.1 Exploring the multi-task learning
configuration
</subsectionHeader>
<bodyText confidence="0.999904">
The goal is to effectively apply ASO to the set of
word disambiguation problems so that overall per-
formance is improved. We consider two factors: fea-
ture split and partitioning ofprediction problems.
</bodyText>
<subsectionHeader confidence="0.950025">
3.1.1 Feature split and problem partitioning
</subsectionHeader>
<bodyText confidence="0.9999976">
Our features described above inherently consist of
four feature groups: local context (LC), global con-
text (GC), syntactic relation (SR), and POS features.
To exploit such a natural feature split, we explore the
following extension of the joint linear model:
</bodyText>
<equation confidence="0.985949">
ft(fOjg,x)=wTtx+�jEF vt(j)TOjx(j), (5)
</equation>
<bodyText confidence="0.999088041666667">
where OjOTj = I for j 2 F, F is a set of dis-
joint feature groups, and x(j) (or v,(j)) is a portion
of the feature vector x (or the weight vector vt) cor-
responding to the feature group j, respectively. This
is a slight modification of the extension presented
in (Ando and Zhang, 2005a). Using this model,
ASO computes the structure matrix Oj for each fea-
ture group separately. That is, SVD is applied to
the sub-matrix of the predictor (weight) matrix cor-
responding to each feature groupj, which results
in more focused dimension reduction of the predic-
tor matrix. For example, suppose thatF=fSRg.
Then, we compute the structure matrixOSRfrom
the corresponding sub-matrix of the predictor ma-
trixU, which is the gray region of Figure 3 (a). The
structure matricesOjforj0F(associated with the
white regions in the figure) should be regarded as
being fixed to the zero matrices. Similarly, it is pos-
sible to compute a structure matrix from a subset of
the predictors (such as noun disambiguators only),
as in Figure 3 (b). In this example, we apply the
extension of ASO withF=fSRgto three sets of
problems (disambiguation of nouns, verbs, and ad-
jectives, respectively) separately.
</bodyText>
<figureCaption confidence="0.999303">
Figure 3: Examples of feature split and problem partitioning.
</figureCaption>
<bodyText confidence="0.999783576923077">
To see why such partitioning may be useful for
our WSD problems, consider the disambiguation of
“bank” and the disambiguation of “save”. Since a
“bank” as in “money bank” and a “save” as in “sav-
ing money” may occur in similar global contexts,
certain global context features effective for recog-
nizing the “money bank” sense may be also effective
for disambiguating “save”, and vice versa. However,
with respect to the position-sensitive local context
features, these two disambiguation problems may
not have much in common since, for instance, we
sometimes say “the bank announced”, but we rarely
say “the save announced”. That is, whether prob-
lems share predictive structure may depend on fea-
ture types, and in that case, seeking predictive struc-
ture for each feature group separately may be more
effective. Hence, we experiment with the configu-
rations with and without various feature splits using
the extension of ASO.
Our target words are nouns, verbs, and adjec-
tives. As in the above example of “bank” (noun)
and “save” (verb), the predictive structure of global
context features may be shared by the problems ir-
respective of the parts of speech of the target words.
However, the other types of features may be more
dependent on the target word part of speech. There-
</bodyText>
<figure confidence="0.999104083333333">
LC
GC
SR
POS
OSR
m predictors
predictors
for nouns
predictors
for verbs
predictors
for adjectives
LC
GC
SR
POS
Predictor matrix U O Predictor matrix U
SR,Noun
(b) Partitioned by F = { SR }
and problem types.
(a) Partitioned by features:
F = { SR }
OSR,Adj
OSR,Verb
</figure>
<page confidence="0.961824">
80
</page>
<bodyText confidence="0.999872692307692">
fore, we explore two types of configuration. One
applies ASO to all the disambiguation problems at
once. The other applies ASO separately to each of
the three sets of disambiguation problems (noun dis-
ambiguation problems, verb disambiguation prob-
lems, and adjective disambiguation problems) and
uses the structure matrixOjobtained from the noun
disambiguation problems only for disambiguating
nouns, and so forth.
Thus, we explore combinations of two parame-
ters. One is the set of feature groupsFin the model
(5). The other is the partitioning of disambiguation
problems.
</bodyText>
<subsubsectionHeader confidence="0.595397">
3.1.2 Empirical results
</subsubsectionHeader>
<figureCaption confidence="0.9987845">
Figure 4: F-measure on Senseval-2 English test set. Multi-
task configurations varying feature group set F and problem
partitioning. Performance at the best dimensionality of Oj (in
110,25,50,100,•••}) is shown.
</figureCaption>
<bodyText confidence="0.999977117647059">
In Figure 4, we compare performance on the
Senseval-2 test set produced by training on the
Senseval-2 training set using the various configura-
tions discussed above. As the evaluation metric, we
use the F-measure (micro-averaged)3 returned by the
official Senseval scorer. Our baseline is the standard
single-task configuration using the same loss func-
tion (modified Huber) and the same training algo-
rithm (SGD).
The results are in line with our expectation. To
learn the shared predictive structure of local context
(LC) and syntactic relations (SR), it is more advanta-
geous to apply ASO to each of the three sets of prob-
lems (disambiguation of nouns, verbs, and adjec-
tives, respectively), separately. By contrast, global
context features (GC) can be more effectively ex-
ploited when ASO is applied to all the disambigua-
</bodyText>
<footnote confidence="0.4997425">
3Our precision and recall are always the same since our sys-
tems assign exactly one sense to each instance. That is, our
F-measure is the same as ‘micro-averaged recall’ or ‘accuracy’
used in some of previous studies we will compare with.
</footnote>
<bodyText confidence="0.999949678571428">
tion problems at once. It turned out that the con-
figurationF={POS}does not improve the per-
formance over the baseline. Therefore, we exclude
POS from the feature group setFin the rest of our
experiments. Comparison ofF={LC+SR+GC}
(treating the features of these three types as one
group) andF={LC,SR,GC}indicates that use
of this feature split indeed improves performance.
Among the configurations shown in Figure 4, the
best performance (67.8%) is obtained by applying
ASO to the three sets of problems (corresponding
to nouns, verbs, and adjectives) separately, with the
feature splitF={LC,SR,GC}.
ASO has one parameter, the dimensionality of the
structure matrix Oj (i.e., the number of left singular
vectors to compute). The performance shown in Fig-
ure 4 is the ceiling performance obtained at the best
dimensionality (in {10, 25, 50,100,150, • • • }). In
Figure 5, we show the performance dependency on
Oj’s dimensionality when ASO is applied to all the
problems at once (Figure 5 left), and when ASO is
applied to the set of the noun disambiguation prob-
lems (Figure 5 right). In the left figure, the config-
uration F = { G C } (global context) produces bet-
ter performance at a relatively low dimensionality.
In the other configurations shown in these two fig-
ures, performance is relatively stable as long as the
dimensionality is not too low.
</bodyText>
<figure confidence="0.871146">
0 100 200 300 400 500 0 100 200 300
dimensionality dimensionality
</figure>
<figureCaption confidence="0.9984765">
Figure 5: Left: Applying ASO to all the WSD problems at
once. Right: Applying ASO to noun disambiguation problems
only and testing on the noun disambiguation problems only.x-
axis: dimensionality of0j.
</figureCaption>
<subsectionHeader confidence="0.998577">
3.2 Multi-task learning procedure for WSD
</subsectionHeader>
<bodyText confidence="0.998763">
Based on the above results on the Senseval-2 test set,
we develop the following procedure using the fea-
ture split and problem partitioning shown in Figure
6. Let N, V, and A be sets of disambiguation prob-
lems whose target words are nouns, verbs, and ad-
jectives, respectively. We write 0(j,,) for the struc-
</bodyText>
<figure confidence="0.97934568852459">
Feature group set F {LC+SR+GC}
64.5
Baseline {LC} {GC} {SR}{POS} {LC,SR,GC}
2 3 4 5 8
67.5
66.5
65.5
68
67
66
65
no feature
split
Problem partitioning
all problems at
once
nouns, verbs,
adjectives,
separately
67.5
67
66.5
66
65.5
65
64.5
74
73
72
71
70
69
{LC,GC,SR}
{LC+GC+SR}
{LC}
{GC}
{SR}
baseline
81
predictors predictors predictors
for nouns for verbs for adjectives
ASO multi-task learning (optimum config.)
classifier combination [FY02]
polynomial KPCA [WSC04]
SVM [LN02]
Our single-task baseline
Senseval-2 (2001) best participant
LC
GC
SR
POS
68.1
66.5
65.8
65.4
65.3
64.2
We compute seven structure
matrices Θj,s each from the
seven shaded regions of the
predictor matrix U.
</figure>
<figureCaption confidence="0.998266">
Figure 6: Effective feature split and problem partitioning.
</figureCaption>
<bodyText confidence="0.686141111111111">
ture matrix associated with the feature groupjand
computed from a problem sets. That is, we replace
Ojin (5) withO(j;s).
•Apply ASO to the three sets of disambigua-
tion problems (corresponding to nouns, verbs,
and adjectives), separately, using the extended
model (5) withF={LC,SR}. As a result,
we obtain O(j s) for every (j, s) E {LC, SR} x
{N, V, A}.
</bodyText>
<listItem confidence="0.9745816">
•Apply ASO to all the disambiguation problems
at once using the extended model (5) with F =
{GC} to obtain O(GC;NUVUA).
• For a problem f E P E {N, V, A}, our final
predictor is based on the model:
</listItem>
<equation confidence="0.852669">
Mx)=wT`x+�
(j;s)ET
</equation>
<bodyText confidence="0.994124545454546">
whereT={(LC,P),(SR,P),(GC,NUVU
A)}. We obtain predictorf`by minimizing the
regularized empirical risk with respect tow`
andv`.
We fix the dimension of the structure matrix cor-
responding to global context features to 50. The di-
mensions of the other structure matrices are set to
0.9 times the maximum possible rank to ensure rela-
tively high dimensionality. This procedure produces
68.1%on the Senseval-2 English lexical sample test
set.
</bodyText>
<subsectionHeader confidence="0.98687">
3.3 Previous systems on Senseval-2 data set
</subsectionHeader>
<bodyText confidence="0.639829166666667">
Figure 7 compares our performance with those of
previous best systems on the Senseval-2 English lex-
ical sample test set. Since we used this test set for the
development of our method above, our performance
should be understood as the potential performance.
(In Section 4, we will present evaluation results on
</bodyText>
<figureCaption confidence="0.9987825">
Figure 7: Performance comparison with previous best sys-
tems on Senseval-2 English lexical sample test set. FY02 (Flo-
rian and Yarowsky, 2002), WSC04 (Wu et al., 2004), LN02 (Lee
and Ng, 2002)
</figureCaption>
<bodyText confidence="0.995990923076923">
the unseen Senseval-3 test sets.) Nevertheless, it is
worth noting that our potential performance (68.1%)
exceeds those of the previous best systems.
Our single-task baseline performance is almost
the same as LN02 (Lee and Ng, 2002), which
uses SVM. This is consistent with the fact that we
adopted LN02’s feature design. FY02 (Florian and
Yarowsky, 2002) combines classifiers by linear av-
erage stacking. The best system of the Senseval-2
competition was an early version of FY02. WSC04
used a polynomial kernel via the kernel Principal
Component Analysis (KPCA) method (Sch¨olkopf et
al., 1998) with nearest neighbor classifiers.
</bodyText>
<sectionHeader confidence="0.97216" genericHeader="evaluation">
4 Evaluation on Senseval-3 tasks
</sectionHeader>
<bodyText confidence="0.999655666666667">
In this section, we evaluate the methods developed
on the Senseval-2 data set above on the standard
Senseval-3 lexical sample tasks.
</bodyText>
<subsectionHeader confidence="0.828062">
4.1 Our methods in multi-task and
semi-supervised configurations
</subsectionHeader>
<bodyText confidence="0.99974775">
In addition to the multi-task configuration described
in Section 3.2, we test the following semi-supervised
application of ASO. We first create auxiliary prob-
lems following Ando and Zhang (2005a)’s partially-
supervised strategy (Figure 8) with distinct fea-
ture mapsT1andT2each of which uses one of
{LC,GC,SR}. Then, we apply ASO to these auxil-
iary problems using the feature split and the problem
partitioning described in Section 3.2.
Note that the difference between the multi-task
and semi-supervised configurations is the source of
information. The multi-task configuration utilizes
the label information of the training examples that
are labeled for the rest of the multiple tasks, and
the semi-supervised learning configuration exploits
a large amount of unlabeled data.
</bodyText>
<figure confidence="0.746983">
(j&gt;s)T
v` O(j&gt;s)x(j)
</figure>
<page confidence="0.959471">
82
</page>
<listItem confidence="0.82871675">
1. Train a classifier C, only using feature map IV, on the
labeled data for the target task.
2. Auxiliary problems are to predict the labels assigned by
C, to the unlabeled data, using the other feature map IV 2.
3. Apply ASO to the auxiliary problems to obtain O.
4. Using the joint linear model (2), train the final
predictor by minimizing the empirical risk for fixed O
on the labeled data for the target task.
</listItem>
<figureCaption confidence="0.959191666666667">
Figure 8: Ando and Zhang (2005a)’s ASO semi-supervised
learning method using partially-supervised procedure for creat-
ing relevant auxiliary problems.
</figureCaption>
<table confidence="0.997616625">
#words #train avg #sense avg #train
per word per sense
English 73 8611 10.7 10.0
Senseval-3 data sets
English 57 7860 6.5 21.3
Catalan 27 4469 3.1 53.2
Italian 45 5145 6.2 18.4
Spanish 46 8430 3.3 55.5
</table>
<figureCaption confidence="0.894125333333333">
Figure 9: Data statistics of Senseval-2 English lexical sample
data set (first row) and Senseval-3 data sets. On each data set, #
of test instances is about one half of that of training instances.
</figureCaption>
<subsectionHeader confidence="0.986041">
4.2 Data and evaluation metric
</subsectionHeader>
<bodyText confidence="0.999970125">
We conduct evaluations on four Senseval-3 lexical
sample tasks (English, Catalan, Italian, and Spanish)
using the official training / test splits. Data statis-
tics are shown in Figure 9. On the Spanish, Cata-
lan, and Italian data sets, we use part-of-speech in-
formation (as features) and unlabeled examples (for
semi-supervised learning) provided by the organizer.
Since the English data set was not provided with
these additional resources, we use an in-house POS
tagger trained with the PennTree Bank corpus, and
extract 100K unlabeled examples from the Reuters-
RCV1 corpus. On each language, the number of un-
labeled examples is 5–15 times larger than that of the
labeled training examples. We use syntactic relation
features only for English data set. As in Section 3,
we report micro-averaged F measure.
</bodyText>
<subsectionHeader confidence="0.613789">
4.3 Baseline methods
</subsectionHeader>
<bodyText confidence="0.998804666666667">
In addition to the standard single-task supervised
configuration as in Section 3, we test the following
method as an additional baseline.
Output-based method The goal of our multi-task
learning configuration is to benefit from having the
labeled training examples of a number of words. An
alternative to ASO for this purpose is to use directly
as features the output values of classifiers trained
for disambiguating the other words, which we call
‘output-based method’ (cf. Florian et al. (2003)).
We explore several variations similarly to Section
3.1 and report the ceiling performance.
</bodyText>
<subsectionHeader confidence="0.991717">
4.4 Evaluation results
</subsectionHeader>
<bodyText confidence="0.99987856097561">
Figure 10 shows F-measure results on the four
Senseval-3 data sets using the official training / test
splits. Both ASO multi-task learning and semi-
supervised learning improve performance over the
single-task baseline on all the data sets. The best
performance is achieved when we combine multi-
task learning and semi-supervised learning by using
all the corresponding structure matrices0(j,,)pro-
duced by both multi-task and semi-supervised learn-
ing, in the final predictors. This combined configu-
ration outperforms the single-task supervised base-
line by up to 5.7%.
Performance improvements over the supervised
baseline are relatively small on English and Span-
ish. We conjecture that this is because the supervised
performance is already close to the highest perfor-
mance that automatic methods could achieve. On
these two languages, our (and previous) systems out-
perform inter-human agreement, which is unusual
but can be regarded as an indication that these tasks
are difficult.
The performance of the output-based method
(baseline) is relatively low. This indicates that out-
put values or proposed labels are not expressive
enough to integrate information from other predic-
tors effectively on this task. We conjecture that for
this method to be effective, the problems are re-
quired to be more closely related to each other as
in Florian et al. (2003)’s named entity experiments.
A practical advantage of ASO multi-task learning
over ASO semi-supervised learning is that shorter
computation time is required to produce similar
performance. On this English data set, training
for multi-task learning and semi-supervised learning
takes 15 minutes and 92 minutes, respectively, using
a Pentium-4 3.20GHz computer. The computation
time mostly depends on the amount of the data on
which auxiliary predictors are learned. Since our ex-
periments use unlabeled data 5–15 times larger than
labeled training data, semi-supervised learning takes
longer, accordingly.
</bodyText>
<page confidence="0.984637">
83
</page>
<table confidence="0.662266444444444">
methods English Catalan Italian Spanish
multi-task learning 73.8 (+0.8) 89.5 (+1.5) 63.2 (+4.9) 89.0 (+1.0)
ASO semi-supervised learning 73.5 (+0.5) 88.6 (+0.6) 62.4 (+4.1) 88.9 (+0.9)
multi-task+semi-supervised 74.1 (+1.1) 89.9 (+1.9) 64.0 (+5.7) 89.5 (+1.5)
baselines output-based 73.0 (0.0) 88.3 (+0.3) 58.0 (-0.3) 88.2 (+0.2)
single-task supervised learning 73.0 88.0 58.3 88.0
previous SVM with LSA kernel [GGS05] 73.3 89.0 61.3 88.2
systems Senseval-3 (2004) best systems 72.9 [G04] 85.2 [SGG04] 53.1 [SGG04] 84.2 [SGG04]
inter-annotator agreement 67.3 93.1 89.0 85.3
</table>
<figureCaption confidence="0.9061905">
Figure 10: Performance results on the Senseval-3 lexical sample test sets. Numbers in the parentheses are performance gains
compared with the single-task supervised baseline (italicized). [G04] Grozea (2004); [SGG04] Strapparava et al. (2004).
</figureCaption>
<bodyText confidence="0.999955235294118">
GGS05 combined various kernels, which includes
the LSA kernel that exploits unlabeled data with
global context features. Our implementation of the
LSA kernel with our classifier (and our other fea-
tures) also produced performance similar to that of
GGS05. While the LSA kernel is closely related
to a special case of the semi-supervised application
of ASO (see the discussion of PCA in Ando and
Zhang (2005a)), our approach here is more general
in that we exploit not only unlabeled data and global
context features but also the labeled examples of
other target words and other types of features. G04
achieved high performance on English using regu-
larized least squares with compensation for skewed
class distributions. SGG04 is an early version of
GGS05. Our methods rival or exceed these state-
of-the-art systems on all the data sets.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977764705883">
With the goal of achieving higher WSD perfor-
mance by exploiting all the currently available re-
sources, our focus was the new application of the
ASO algorithm in the multi-task learning configu-
ration, which improves performance by learning a
number of WSD problems simultaneously instead of
training for each individual problem independently.
A key finding is that using ASO with appropriate
feature / problem partitioning, labeled examples of
seemingly unrelated words can be effectively ex-
ploited. Combining ASO multi-task learning with
ASO semi-supervised learning results in further im-
provements. The fact that performance improve-
ments were obtained consistently across several lan-
guages / sense inventories demonstrates that our ap-
proach has broad applicability and hence practical
significance.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99939245">
Rie Kubota Ando and Tong Zhang. 2005a. A framework
for learning predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Research,
6(Nov):1817–1853. An early version was published as IBM
Research Report (2004).
Rie Kubota Ando and Tong Zhang. 2005b. High performance
semi-supervised learning for text chunking. In Proceedings
ofACL-2005.
Radu Florian and David Yarowsky. 2002. Modeling consensus:
Classifier combination for word sense disambiguation. In
Proceedings ofEMNLP-2002.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang.
2003. Named entity recognition through classifier combina-
tion. In Proceedings of CoNLL-2003.
Cristian Grozea. 2004. Finding optimal parameter settings for
high performance word sense diambiguation. In Proceed-
ings ofSenseval-3 Workshop.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning seman-
tic classes for word sense disambiguation. In Proceedings of
ACL-2005.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evalu-
ation of knowledge sources and learning algorithms for word
sense disambiguation. In Proceedings ofEMNLP-2002.
Michael C. McCord. 1990. Slot Grammar: A system for
simpler construction of practical natural language grammars.
Natural Language and Logic: International Scientific Sym-
posium, Lecture Notes in Computer Science, pages 118–145.
Bernhard Sch¨olkopf, Alexander Smola, and Klaus-Rober
M¨uller. 1998. Nonlinear component analysis as a kernel
eigenvalue problem. Neural Computation, 10(5).
Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano. 2004.
Pattern abstraction and term similarity for word sense disam-
biguation: IRST at Senseval-3. In Proceedings ofSenseval-3
Workshop.
Dekai Wu, Weifeng Su, and Marine Carpuat. 2004. A kernel
PCA method for superior word sense disambiguation. In
Proceedings ofACL-2004.
Tong Zhang. 2004. Solving large scale linear prediction prob-
lems using stochastic gradient descent algorithms. In ICML
04, pages 919–926.
</reference>
<page confidence="0.999244">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811305">
<title confidence="0.9928075">Applying Alternating Structure to Word Sense Disambiguation</title>
<author confidence="0.99508">Rie Kubota</author>
<affiliation confidence="0.9207305">IBM T.J. Watson Research Yorktown Heights, NY 10598,</affiliation>
<email confidence="0.997154">rie1@us.ibm.com</email>
<abstract confidence="0.998990260869565">This paper presents a new application of the recently proposed machine learning Structure Optimizato word sense disambiguation (WSD). Given a set of WSD problems and their respective labeled examples, we seek to improve overall performance on that set by using all the labeled examples (irrespective of target words) for the entire set in learning a disambiguator for each individual problem. Thus, in effect, on each individual problem (e.g., disambiguation of “art”) we benefit from training examples for other problems (e.g., disambiguation of “bar”, “canal”, and so forth). We empirically study the effective use of ASO for this purpose in the multitask and semi-supervised learning configurations. Our performance results rival or exceed those of the previous best systems on several Senseval lexical sample task data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="2498" citStr="Ando and Zhang, 2005" startWordPosition="387" endWordPosition="390">ask2. One problem is that there are so many words and so many senses that it is hard to make available a sufficient number of labeled training examples for each of a large number of target words. On the other hand, this indicates that the total number of available labeled examples (irrespective of target words) can be relatively large. A natural question to ask is whether we can effectively use all the labeled examples (irrespective of target words) for learning on each individual WSD problem. Based on these observations, we study a new application of Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a; Ando and Zhang, 2005b) to WSD. ASO is a recently proposed machine learning method for learning predictive structure (i.e., information useful for predictions) shared by multiple prediction problems via joint empirical risk minimization. It has been shown that on several tasks, performance can be significantly improved by a semi-supervised application of ASO, which obtains useful information from unlabeled data by learning automatically created prediction problems. In addition to such semi-supervised learning, this paper explores ASO multi-task learning, which learns a number of WSD problems</context>
<context position="6405" citStr="Ando and Zhang (2005" startWordPosition="986" endWordPosition="989">to obtain a predictor �fis regularized empirical risk minimization, which minimizes an empirical loss of the predictor (with regularization) on thenlabeled training examples {(Xi,Yi)}: L(f(Xi),Yi)+r(f)�.(1) A loss functionL(•)quantifies the difference between the predictionf(Xi)and the true outputYi, andr(•)is a regularization term to control the model complexity. 2.2 Joint linear models for ASO problems indexed Considermprediction by f E {1 ... m}, each with nt samples (Xti, Yt) for i E {1 ... nt}, and assume that there exists a lowdimensional predictive structure shared by these m problems. Ando and Zhang (2005a) extend the above traditional linear model to a joint linear model so that a predictor for problem f is in the form: ft(O x) = wTt x + vTt Ox OOT = I (2) where I is the identity matrix. wt and vt are weight vectors specific to each problem `. Predictive structure is parameterized by the structure matrix O shared by all the m predictors. The goal of this model can also be regarded as learning a common good feature map Ox used for all the m problems. 2.3 ASO algorithm Analogous to (1), we computeOand predictors so that they minimize the empirical risk summed over all the problems: +r(f`)!.(3) </context>
<context position="12408" citStr="Ando and Zhang (2005" startWordPosition="1953" endWordPosition="1956">ords (excluding stopwords) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syn79 tactic relations: syntactic information obtained from parser output. To generate syntactic relation features, we use the Slot Grammar-based full parser ESG (McCord, 1990). We use as features syntactic relation types (e.g., subject-of, object-of, and noun modifier), participants of syntactic relations, and bigrams of syntactic relations / participants. Details of the other three types are shown in Figure 2. Implementation Our implementation follows Ando and Zhang (2005a). We use a modification of the Huber’s robust loss for regression: L(p,y)=(max(0,1—py))2ifpy&gt;—1; and—4py otherwise; with square regularization (A=10-4), and perform empirical risk minimization by stochastic gradient descent (SGD) (see e.g., Zhang (2004)). We perform one ASO iteration. 3.1 Exploring the multi-task learning configuration The goal is to effectively apply ASO to the set of word disambiguation problems so that overall performance is improved. We consider two factors: feature split and partitioning ofprediction problems. 3.1.1 Feature split and problem partitioning Our features de</context>
<context position="23244" citStr="Ando and Zhang (2005" startWordPosition="3697" endWordPosition="3700">l-2 competition was an early version of FY02. WSC04 used a polynomial kernel via the kernel Principal Component Analysis (KPCA) method (Sch¨olkopf et al., 1998) with nearest neighbor classifiers. 4 Evaluation on Senseval-3 tasks In this section, we evaluate the methods developed on the Senseval-2 data set above on the standard Senseval-3 lexical sample tasks. 4.1 Our methods in multi-task and semi-supervised configurations In addition to the multi-task configuration described in Section 3.2, we test the following semi-supervised application of ASO. We first create auxiliary problems following Ando and Zhang (2005a)’s partiallysupervised strategy (Figure 8) with distinct feature mapsT1andT2each of which uses one of {LC,GC,SR}. Then, we apply ASO to these auxiliary problems using the feature split and the problem partitioning described in Section 3.2. Note that the difference between the multi-task and semi-supervised configurations is the source of information. The multi-task configuration utilizes the label information of the training examples that are labeled for the rest of the multiple tasks, and the semi-supervised learning configuration exploits a large amount of unlabeled data. (j&gt;s)T v` O(j&gt;s)x</context>
<context position="29463" citStr="Ando and Zhang (2005" startWordPosition="4659" endWordPosition="4662">s on the Senseval-3 lexical sample test sets. Numbers in the parentheses are performance gains compared with the single-task supervised baseline (italicized). [G04] Grozea (2004); [SGG04] Strapparava et al. (2004). GGS05 combined various kernels, which includes the LSA kernel that exploits unlabeled data with global context features. Our implementation of the LSA kernel with our classifier (and our other features) also produced performance similar to that of GGS05. While the LSA kernel is closely related to a special case of the semi-supervised application of ASO (see the discussion of PCA in Ando and Zhang (2005a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target words and other types of features. G04 achieved high performance on English using regularized least squares with compensation for skewed class distributions. SGG04 is an early version of GGS05. Our methods rival or exceed these stateof-the-art systems on all the data sets. 5 Conclusion With the goal of achieving higher WSD performance by exploiting all the currently available resources, our focus was the new application of the ASO algorithm</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005a. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853. An early version was published as IBM Research Report (2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>High performance semi-supervised learning for text chunking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-2005.</booktitle>
<contexts>
<context position="2498" citStr="Ando and Zhang, 2005" startWordPosition="387" endWordPosition="390">ask2. One problem is that there are so many words and so many senses that it is hard to make available a sufficient number of labeled training examples for each of a large number of target words. On the other hand, this indicates that the total number of available labeled examples (irrespective of target words) can be relatively large. A natural question to ask is whether we can effectively use all the labeled examples (irrespective of target words) for learning on each individual WSD problem. Based on these observations, we study a new application of Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a; Ando and Zhang, 2005b) to WSD. ASO is a recently proposed machine learning method for learning predictive structure (i.e., information useful for predictions) shared by multiple prediction problems via joint empirical risk minimization. It has been shown that on several tasks, performance can be significantly improved by a semi-supervised application of ASO, which obtains useful information from unlabeled data by learning automatically created prediction problems. In addition to such semi-supervised learning, this paper explores ASO multi-task learning, which learns a number of WSD problems</context>
<context position="6405" citStr="Ando and Zhang (2005" startWordPosition="986" endWordPosition="989">to obtain a predictor �fis regularized empirical risk minimization, which minimizes an empirical loss of the predictor (with regularization) on thenlabeled training examples {(Xi,Yi)}: L(f(Xi),Yi)+r(f)�.(1) A loss functionL(•)quantifies the difference between the predictionf(Xi)and the true outputYi, andr(•)is a regularization term to control the model complexity. 2.2 Joint linear models for ASO problems indexed Considermprediction by f E {1 ... m}, each with nt samples (Xti, Yt) for i E {1 ... nt}, and assume that there exists a lowdimensional predictive structure shared by these m problems. Ando and Zhang (2005a) extend the above traditional linear model to a joint linear model so that a predictor for problem f is in the form: ft(O x) = wTt x + vTt Ox OOT = I (2) where I is the identity matrix. wt and vt are weight vectors specific to each problem `. Predictive structure is parameterized by the structure matrix O shared by all the m predictors. The goal of this model can also be regarded as learning a common good feature map Ox used for all the m problems. 2.3 ASO algorithm Analogous to (1), we computeOand predictors so that they minimize the empirical risk summed over all the problems: +r(f`)!.(3) </context>
<context position="12408" citStr="Ando and Zhang (2005" startWordPosition="1953" endWordPosition="1956">ords (excluding stopwords) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syn79 tactic relations: syntactic information obtained from parser output. To generate syntactic relation features, we use the Slot Grammar-based full parser ESG (McCord, 1990). We use as features syntactic relation types (e.g., subject-of, object-of, and noun modifier), participants of syntactic relations, and bigrams of syntactic relations / participants. Details of the other three types are shown in Figure 2. Implementation Our implementation follows Ando and Zhang (2005a). We use a modification of the Huber’s robust loss for regression: L(p,y)=(max(0,1—py))2ifpy&gt;—1; and—4py otherwise; with square regularization (A=10-4), and perform empirical risk minimization by stochastic gradient descent (SGD) (see e.g., Zhang (2004)). We perform one ASO iteration. 3.1 Exploring the multi-task learning configuration The goal is to effectively apply ASO to the set of word disambiguation problems so that overall performance is improved. We consider two factors: feature split and partitioning ofprediction problems. 3.1.1 Feature split and problem partitioning Our features de</context>
<context position="23244" citStr="Ando and Zhang (2005" startWordPosition="3697" endWordPosition="3700">l-2 competition was an early version of FY02. WSC04 used a polynomial kernel via the kernel Principal Component Analysis (KPCA) method (Sch¨olkopf et al., 1998) with nearest neighbor classifiers. 4 Evaluation on Senseval-3 tasks In this section, we evaluate the methods developed on the Senseval-2 data set above on the standard Senseval-3 lexical sample tasks. 4.1 Our methods in multi-task and semi-supervised configurations In addition to the multi-task configuration described in Section 3.2, we test the following semi-supervised application of ASO. We first create auxiliary problems following Ando and Zhang (2005a)’s partiallysupervised strategy (Figure 8) with distinct feature mapsT1andT2each of which uses one of {LC,GC,SR}. Then, we apply ASO to these auxiliary problems using the feature split and the problem partitioning described in Section 3.2. Note that the difference between the multi-task and semi-supervised configurations is the source of information. The multi-task configuration utilizes the label information of the training examples that are labeled for the rest of the multiple tasks, and the semi-supervised learning configuration exploits a large amount of unlabeled data. (j&gt;s)T v` O(j&gt;s)x</context>
<context position="29463" citStr="Ando and Zhang (2005" startWordPosition="4659" endWordPosition="4662">s on the Senseval-3 lexical sample test sets. Numbers in the parentheses are performance gains compared with the single-task supervised baseline (italicized). [G04] Grozea (2004); [SGG04] Strapparava et al. (2004). GGS05 combined various kernels, which includes the LSA kernel that exploits unlabeled data with global context features. Our implementation of the LSA kernel with our classifier (and our other features) also produced performance similar to that of GGS05. While the LSA kernel is closely related to a special case of the semi-supervised application of ASO (see the discussion of PCA in Ando and Zhang (2005a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target words and other types of features. G04 achieved high performance on English using regularized least squares with compensation for skewed class distributions. SGG04 is an early version of GGS05. Our methods rival or exceed these stateof-the-art systems on all the data sets. 5 Conclusion With the goal of achieving higher WSD performance by exploiting all the currently available resources, our focus was the new application of the ASO algorithm</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005b. High performance semi-supervised learning for text chunking. In Proceedings ofACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling consensus: Classifier combination for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP-2002.</booktitle>
<contexts>
<context position="1350" citStr="Florian and Yarowsky, 2002" startWordPosition="205" endWordPosition="208">r problems (e.g., disambiguation of “bar”, “canal”, and so forth). We empirically study the effective use of ASO for this purpose in the multitask and semi-supervised learning configurations. Our performance results rival or exceed those of the previous best systems on several Senseval lexical sample task data sets. 1 Introduction Word sense disambiguation (WSD) is the task of assigning pre-defined senses to words occurring in some context. An example is to disambiguate an occurrence of “bank” between the “money bank” sense and the “river bank” sense. Previous studies e.g., (Lee and Ng, 2002; Florian and Yarowsky, 2002), have applied supervised learning techniques to WSD with success. A practical issue that arises in supervised WSD is the paucity of labeled examples (sense-annotated data) available for training. For example, the training set of the Senseval-21 English lexical sample 1http://www.cs.unt.edu/-rada/senseval/. WSD systems have 77 task has only 10 labeled training examples per sense on average, which is in contrast to nearly 6K training examples per name class (on average) used for the CoNLL-2003 named entity chunking shared task2. One problem is that there are so many words and so many senses tha</context>
<context position="22138" citStr="Florian and Yarowsky, 2002" startWordPosition="3526" endWordPosition="3530">sible rank to ensure relatively high dimensionality. This procedure produces 68.1%on the Senseval-2 English lexical sample test set. 3.3 Previous systems on Senseval-2 data set Figure 7 compares our performance with those of previous best systems on the Senseval-2 English lexical sample test set. Since we used this test set for the development of our method above, our performance should be understood as the potential performance. (In Section 4, we will present evaluation results on Figure 7: Performance comparison with previous best systems on Senseval-2 English lexical sample test set. FY02 (Florian and Yarowsky, 2002), WSC04 (Wu et al., 2004), LN02 (Lee and Ng, 2002) the unseen Senseval-3 test sets.) Nevertheless, it is worth noting that our potential performance (68.1%) exceeds those of the previous best systems. Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. This is consistent with the fact that we adopted LN02’s feature design. FY02 (Florian and Yarowsky, 2002) combines classifiers by linear average stacking. The best system of the Senseval-2 competition was an early version of FY02. WSC04 used a polynomial kernel via the kernel Principal Component An</context>
</contexts>
<marker>Florian, Yarowsky, 2002</marker>
<rawString>Radu Florian and David Yarowsky. 2002. Modeling consensus: Classifier combination for word sense disambiguation. In Proceedings ofEMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="26166" citStr="Florian et al. (2003)" startWordPosition="4168" endWordPosition="4171">tactic relation features only for English data set. As in Section 3, we report micro-averaged F measure. 4.3 Baseline methods In addition to the standard single-task supervised configuration as in Section 3, we test the following method as an additional baseline. Output-based method The goal of our multi-task learning configuration is to benefit from having the labeled training examples of a number of words. An alternative to ASO for this purpose is to use directly as features the output values of classifiers trained for disambiguating the other words, which we call ‘output-based method’ (cf. Florian et al. (2003)). We explore several variations similarly to Section 3.1 and report the ceiling performance. 4.4 Evaluation results Figure 10 shows F-measure results on the four Senseval-3 data sets using the official training / test splits. Both ASO multi-task learning and semisupervised learning improve performance over the single-task baseline on all the data sets. The best performance is achieved when we combine multitask learning and semi-supervised learning by using all the corresponding structure matrices0(j,,)produced by both multi-task and semi-supervised learning, in the final predictors. This comb</context>
<context position="27633" citStr="Florian et al. (2003)" startWordPosition="4393" endWordPosition="4396">already close to the highest performance that automatic methods could achieve. On these two languages, our (and previous) systems outperform inter-human agreement, which is unusual but can be regarded as an indication that these tasks are difficult. The performance of the output-based method (baseline) is relatively low. This indicates that output values or proposed labels are not expressive enough to integrate information from other predictors effectively on this task. We conjecture that for this method to be effective, the problems are required to be more closely related to each other as in Florian et al. (2003)’s named entity experiments. A practical advantage of ASO multi-task learning over ASO semi-supervised learning is that shorter computation time is required to produce similar performance. On this English data set, training for multi-task learning and semi-supervised learning takes 15 minutes and 92 minutes, respectively, using a Pentium-4 3.20GHz computer. The computation time mostly depends on the amount of the data on which auxiliary predictors are learned. Since our experiments use unlabeled data 5–15 times larger than labeled training data, semi-supervised learning takes longer, according</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Grozea</author>
</authors>
<title>Finding optimal parameter settings for high performance word sense diambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings ofSenseval-3 Workshop.</booktitle>
<contexts>
<context position="29021" citStr="Grozea (2004)" startWordPosition="4590" endWordPosition="4591">4.1) 88.9 (+0.9) multi-task+semi-supervised 74.1 (+1.1) 89.9 (+1.9) 64.0 (+5.7) 89.5 (+1.5) baselines output-based 73.0 (0.0) 88.3 (+0.3) 58.0 (-0.3) 88.2 (+0.2) single-task supervised learning 73.0 88.0 58.3 88.0 previous SVM with LSA kernel [GGS05] 73.3 89.0 61.3 88.2 systems Senseval-3 (2004) best systems 72.9 [G04] 85.2 [SGG04] 53.1 [SGG04] 84.2 [SGG04] inter-annotator agreement 67.3 93.1 89.0 85.3 Figure 10: Performance results on the Senseval-3 lexical sample test sets. Numbers in the parentheses are performance gains compared with the single-task supervised baseline (italicized). [G04] Grozea (2004); [SGG04] Strapparava et al. (2004). GGS05 combined various kernels, which includes the LSA kernel that exploits unlabeled data with global context features. Our implementation of the LSA kernel with our classifier (and our other features) also produced performance similar to that of GGS05. While the LSA kernel is closely related to a special case of the semi-supervised application of ASO (see the discussion of PCA in Ando and Zhang (2005a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target wo</context>
</contexts>
<marker>Grozea, 2004</marker>
<rawString>Cristian Grozea. 2004. Finding optimal parameter settings for high performance word sense diambiguation. In Proceedings ofSenseval-3 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Upali S Kohomban</author>
<author>Wee Sun Lee</author>
</authors>
<title>Learning semantic classes for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="3841" citStr="Kohomban and Lee (2005)" startWordPosition="585" endWordPosition="588">ual problem (e.g., disambiguation of “art”) benefits from labeled training examples for other problems (e.g., disambiguation of “bar”, disambiguation of “canal”, and so forth). The notion of benefiting from training data for other word senses is not new by itself. For instance, been evaluated in the series of Senseval workshops. 2http://www.cnts.ua.ac.be/conll2003/ner/ Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL -X), pages 77–84, New York City, June 2006. @c 2006 Association for Computational Linguistics on the WSD task with respect to WordNet synsets, Kohomban and Lee (2005) trained classifiers for the top-level synsets of the WordNet semantic hierarchy, consolidating labeled examples associated with the WordNet sub-trees. To disambiguate test instances, these coarse-grained classifiers are first applied, and then fine-grained senses are determined using a heuristic mapping. By contrast, our approach does not require pre-defined relations among senses such as the WordNet hierarchy. Rather, we let the machine learning algorithm ASO automatically and implicitly find relations with respect to the disambiguation problems (i.e., finding shared predictive structure). I</context>
</contexts>
<marker>Kohomban, Lee, 2005</marker>
<rawString>Upali S. Kohomban and Wee Sun Lee. 2005. Learning semantic classes for word sense disambiguation. In Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP-2002.</booktitle>
<contexts>
<context position="1321" citStr="Lee and Ng, 2002" startWordPosition="201" endWordPosition="204"> examples for other problems (e.g., disambiguation of “bar”, “canal”, and so forth). We empirically study the effective use of ASO for this purpose in the multitask and semi-supervised learning configurations. Our performance results rival or exceed those of the previous best systems on several Senseval lexical sample task data sets. 1 Introduction Word sense disambiguation (WSD) is the task of assigning pre-defined senses to words occurring in some context. An example is to disambiguate an occurrence of “bank” between the “money bank” sense and the “river bank” sense. Previous studies e.g., (Lee and Ng, 2002; Florian and Yarowsky, 2002), have applied supervised learning techniques to WSD with success. A practical issue that arises in supervised WSD is the paucity of labeled examples (sense-annotated data) available for training. For example, the training set of the Senseval-21 English lexical sample 1http://www.cs.unt.edu/-rada/senseval/. WSD systems have 77 task has only 10 labeled training examples per sense on average, which is in contrast to nearly 6K training examples per name class (on average) used for the CoNLL-2003 named entity chunking shared task2. One problem is that there are so many</context>
<context position="11648" citStr="Lee and Ng (2002)" startWordPosition="1841" endWordPosition="1844">ta set by carrying out the Senseval-3 tasks, i.e., training on the Senseval-3 training data and then evaluating the results on the (unseen) Senseval-3 test sets in Section 4. Task setting In this work, we focus on the Senseval lexical sample task. We are given a set of target words, each of which is associated with several possible senses, and their labeled instances for training. Each instance contains an occurrence of one of the target words and its surrounding words, typically a few sentences. The task is to assign a sense to each test instance. Features We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stopwords) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syn79 tactic relations: syntactic information obtained from parser output. To generate syntactic relation features, we use the Slot Grammar-based full parser ESG (McCord, 1990). We use as features syntactic relation types (e.g., subject-of, object-of, and noun modifier), participants of syntactic relations, and bigra</context>
<context position="22188" citStr="Lee and Ng, 2002" startWordPosition="3537" endWordPosition="3540">ocedure produces 68.1%on the Senseval-2 English lexical sample test set. 3.3 Previous systems on Senseval-2 data set Figure 7 compares our performance with those of previous best systems on the Senseval-2 English lexical sample test set. Since we used this test set for the development of our method above, our performance should be understood as the potential performance. (In Section 4, we will present evaluation results on Figure 7: Performance comparison with previous best systems on Senseval-2 English lexical sample test set. FY02 (Florian and Yarowsky, 2002), WSC04 (Wu et al., 2004), LN02 (Lee and Ng, 2002) the unseen Senseval-3 test sets.) Nevertheless, it is worth noting that our potential performance (68.1%) exceeds those of the previous best systems. Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. This is consistent with the fact that we adopted LN02’s feature design. FY02 (Florian and Yarowsky, 2002) combines classifiers by linear average stacking. The best system of the Senseval-2 competition was an early version of FY02. WSC04 used a polynomial kernel via the kernel Principal Component Analysis (KPCA) method (Sch¨olkopf et al., 1998) wit</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings ofEMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
</authors>
<title>Slot Grammar: A system for simpler construction of practical natural language grammars.</title>
<date>1990</date>
<booktitle>Natural Language and Logic: International Scientific Symposium, Lecture Notes in Computer Science,</booktitle>
<pages>118--145</pages>
<contexts>
<context position="12106" citStr="McCord, 1990" startWordPosition="1911" endWordPosition="1912">nding words, typically a few sentences. The task is to assign a sense to each test instance. Features We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stopwords) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syn79 tactic relations: syntactic information obtained from parser output. To generate syntactic relation features, we use the Slot Grammar-based full parser ESG (McCord, 1990). We use as features syntactic relation types (e.g., subject-of, object-of, and noun modifier), participants of syntactic relations, and bigrams of syntactic relations / participants. Details of the other three types are shown in Figure 2. Implementation Our implementation follows Ando and Zhang (2005a). We use a modification of the Huber’s robust loss for regression: L(p,y)=(max(0,1—py))2ifpy&gt;—1; and—4py otherwise; with square regularization (A=10-4), and perform empirical risk minimization by stochastic gradient descent (SGD) (see e.g., Zhang (2004)). We perform one ASO iteration. 3.1 Explor</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael C. McCord. 1990. Slot Grammar: A system for simpler construction of practical natural language grammars. Natural Language and Logic: International Scientific Symposium, Lecture Notes in Computer Science, pages 118–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander Smola</author>
<author>Klaus-Rober M¨uller</author>
</authors>
<title>Nonlinear component analysis as a kernel eigenvalue problem.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>5</issue>
<marker>Sch¨olkopf, Smola, M¨uller, 1998</marker>
<rawString>Bernhard Sch¨olkopf, Alexander Smola, and Klaus-Rober M¨uller. 1998. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation: IRST at Senseval-3.</title>
<date>2004</date>
<booktitle>In Proceedings ofSenseval-3 Workshop.</booktitle>
<contexts>
<context position="29056" citStr="Strapparava et al. (2004)" startWordPosition="4593" endWordPosition="4596">task+semi-supervised 74.1 (+1.1) 89.9 (+1.9) 64.0 (+5.7) 89.5 (+1.5) baselines output-based 73.0 (0.0) 88.3 (+0.3) 58.0 (-0.3) 88.2 (+0.2) single-task supervised learning 73.0 88.0 58.3 88.0 previous SVM with LSA kernel [GGS05] 73.3 89.0 61.3 88.2 systems Senseval-3 (2004) best systems 72.9 [G04] 85.2 [SGG04] 53.1 [SGG04] 84.2 [SGG04] inter-annotator agreement 67.3 93.1 89.0 85.3 Figure 10: Performance results on the Senseval-3 lexical sample test sets. Numbers in the parentheses are performance gains compared with the single-task supervised baseline (italicized). [G04] Grozea (2004); [SGG04] Strapparava et al. (2004). GGS05 combined various kernels, which includes the LSA kernel that exploits unlabeled data with global context features. Our implementation of the LSA kernel with our classifier (and our other features) also produced performance similar to that of GGS05. While the LSA kernel is closely related to a special case of the semi-supervised application of ASO (see the discussion of PCA in Ando and Zhang (2005a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target words and other types of features. G0</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: IRST at Senseval-3. In Proceedings ofSenseval-3 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Weifeng Su</author>
<author>Marine Carpuat</author>
</authors>
<title>A kernel PCA method for superior word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL-2004.</booktitle>
<contexts>
<context position="22163" citStr="Wu et al., 2004" startWordPosition="3532" endWordPosition="3535"> dimensionality. This procedure produces 68.1%on the Senseval-2 English lexical sample test set. 3.3 Previous systems on Senseval-2 data set Figure 7 compares our performance with those of previous best systems on the Senseval-2 English lexical sample test set. Since we used this test set for the development of our method above, our performance should be understood as the potential performance. (In Section 4, we will present evaluation results on Figure 7: Performance comparison with previous best systems on Senseval-2 English lexical sample test set. FY02 (Florian and Yarowsky, 2002), WSC04 (Wu et al., 2004), LN02 (Lee and Ng, 2002) the unseen Senseval-3 test sets.) Nevertheless, it is worth noting that our potential performance (68.1%) exceeds those of the previous best systems. Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. This is consistent with the fact that we adopted LN02’s feature design. FY02 (Florian and Yarowsky, 2002) combines classifiers by linear average stacking. The best system of the Senseval-2 competition was an early version of FY02. WSC04 used a polynomial kernel via the kernel Principal Component Analysis (KPCA) method (Sch</context>
</contexts>
<marker>Wu, Su, Carpuat, 2004</marker>
<rawString>Dekai Wu, Weifeng Su, and Marine Carpuat. 2004. A kernel PCA method for superior word sense disambiguation. In Proceedings ofACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>In ICML 04,</booktitle>
<pages>919--926</pages>
<contexts>
<context position="12663" citStr="Zhang (2004)" startWordPosition="1988" endWordPosition="1989">e the Slot Grammar-based full parser ESG (McCord, 1990). We use as features syntactic relation types (e.g., subject-of, object-of, and noun modifier), participants of syntactic relations, and bigrams of syntactic relations / participants. Details of the other three types are shown in Figure 2. Implementation Our implementation follows Ando and Zhang (2005a). We use a modification of the Huber’s robust loss for regression: L(p,y)=(max(0,1—py))2ifpy&gt;—1; and—4py otherwise; with square regularization (A=10-4), and perform empirical risk minimization by stochastic gradient descent (SGD) (see e.g., Zhang (2004)). We perform one ASO iteration. 3.1 Exploring the multi-task learning configuration The goal is to effectively apply ASO to the set of word disambiguation problems so that overall performance is improved. We consider two factors: feature split and partitioning ofprediction problems. 3.1.1 Feature split and problem partitioning Our features described above inherently consist of four feature groups: local context (LC), global context (GC), syntactic relation (SR), and POS features. To exploit such a natural feature split, we explore the following extension of the joint linear model: ft(fOjg,x)=</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In ICML 04, pages 919–926.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>