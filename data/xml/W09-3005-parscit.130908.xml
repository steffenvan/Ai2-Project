<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997395">
By all these lovely tokens...∗
Merging Conflicting Tokenizations
</title>
<author confidence="0.954905">
Christian Chiarcos, Julia Ritz and Manfred Stede
</author>
<affiliation confidence="0.8146325">
Sonderforschungsbereich 632 “Information Structure”
University of Potsdam
</affiliation>
<address confidence="0.916449">
Karl-Liebknecht-Str. 24-25, 14476 Golm, Germany
</address>
<email confidence="0.998978">
{chiarcos|julia|stede}@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967230769231">
Given the contemporary trend to modular
NLP architectures and multiple annotation
frameworks, the existence of concurrent
tokenizations of the same text represents
a pervasive problem in everyday’s NLP
practice and poses a non-trivial theoretical
problem to the integration of linguistic an-
notations and their interpretability in gen-
eral. This paper describes a solution for
integrating different tokenizations using a
standoff XML format, and discusses the
consequences for the handling of queries
on annotated corpora.
</bodyText>
<sectionHeader confidence="0.998329" genericHeader="keywords">
1 Motivation
</sectionHeader>
<subsectionHeader confidence="0.99963">
1.1 Tokens: Functions and goals
</subsectionHeader>
<bodyText confidence="0.999050857142858">
For most NLP tasks and linguistic annotations,
especially those concerned with syntax (part-of-
speech tagging, chunking, parsing) and the inter-
pretation of syntactic structures (esp., the extrac-
tion of semantic information), tokens represent
the minimal unit of analysis: words (lexemes,
semantic units, partly morphemes) on the one
hand and certain punctuation symbols on the other
hand. From a corpus-linguistic perspective, tokens
also represent the minimal unit of investigation,
the minimal character sequence that can be ad-
dressed in a corpus query (e.g. using search tools
like TIGERSearch (K¨onig and Lezius, 2000) or
CWB (Christ, 1994)). Tokens also constitute the
basis for ‘word’ distance measurements. In many
annotation tools and their corresponding formats,
the order of tokens provides a timeline for the
sequential order of structural elements (MMAX
(M¨uller and Strube, 2006), GENAU (Rehm et al.,
2009), GrAF (Ide and Suderman, 2007), TIGER
XML (K¨onig and Lezius, 2000)). In several multi-
∗Taken from the poem September by Helen Hunt Jackson.
layer formats, tokens also define the absolute po-
sition of annotation elements, and only by refer-
ence to a common token layer, annotations from
different layers can be related with each other
(NITE (Carletta et al., 2003), GENAU).
Thus, by their function, tokens have the fol-
lowing characteristics: (i) tokens are totally or-
dered, (ii) tokens cover the full (annotated portion
of the) primary data, (iii) tokens are the smallest
unit of annotation, and (iv) there is only one sin-
gle privileged token layer. The last aspect is es-
pecially relevant for the study of richly annotated
data, as an integration and serialization of anno-
tations produced by different tools can be estab-
lished only by reference to the token layer. From
a corpus-linguistic perspective, i.e., when focus-
ing on querying of annotated corpora, tokens need
to be well-defined and all information annotated
to a particular text is to be preserved without any
corruption. We argue that for this purpose, char-
acteristic (iii) is to be abandoned, and we will de-
scribe the data format and an algorithm for merg-
ing different tokenizations and their respective an-
notations.
Our goal is a fully automated merging of anno-
tations that refer to different tokenizations (hence-
forth T and T) of the same text. We regard the
following criteria as crucial for this task:
Information preservation. All annotations ap-
plied to the original tokenizations should be pre-
served.
Theoretically well-defined notion of token. It
should be possible to give a plausible list of posi-
tive criteria that define character sequences as to-
kens. Knowledge about the token definition is es-
sential for formulating queries for words, e.g. in a
corpus search interface.
Integrative representation. All annotations that
are consistent with the merged tokenization should
refer to the merged tokenization. This is necessary
in order to query across multiple annotations orig-
</bodyText>
<page confidence="0.984974">
35
</page>
<note confidence="0.9968215">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.977934">
inating from different annotation layers or tools.
Unsupervised merging. The integration of con-
flicting tokenizations should not require manual
interference.
tools for standard NLP tasks, thus, it is the norm
rather than the exception that they disagree in their
tokenization, as shown in ex. (4).
</bodyText>
<subsectionHeader confidence="0.904624">
1.2 Tokenization
</subsectionHeader>
<bodyText confidence="0.996522">
Tokenization is the process of mapping sequences
of characters to sequences of words (cf. Guo
1997). However, different research questions or
applications induce different conceptions of the
term ‘word’. For a shallow morphosyntactic anal-
ysis (part of speech tagging), a ‘simple’ tokeniza-
tion using whitespaces and punctation symbols as
delimiters seems acceptable for the examples in
(1). A full syntactic analysis (parsing), however,
could profit from the aggregation of complex nom-
inals into one token each.
</bodyText>
<listItem confidence="0.988274333333333">
(1) a. department store
b. Herzog-von der Heide1
c. Red Cross/Red Crescent movement
</listItem>
<bodyText confidence="0.99511">
Similarly, examples (2a) and (2b) can be ar-
gued to be treated as one token for (mor-
pho)syntactic analyses, respectively. Despite in-
tervening whitespaces and punctuation symbols,
they are complex instances of the ‘classical’ part-
of-speech adjective. For certain semantic analyses
such as in information extraction, however, it may
be useful to split these compounds in order to ac-
cess the inherent complements (E 605, No. 22).
</bodyText>
<listItem confidence="0.666824">
(2) a. E 605-intoxicated
b. No. 22-rated
</listItem>
<bodyText confidence="0.980817555555556">
Finally, (3) illustrates a morphology-based tok-
enization strategy: the principle of splitting at
morpheme boundaries (Marcus et al., 1993, PTB)
(token boundaries represented by square brack-
ets). Morphological tokenization may help distri-
butional (co-occurrence-based) semantics and/or
parsing; however, the resulting tokens might be
argued as being less intuitive to users of a corpus
search tool.
</bodyText>
<listItem confidence="0.960717">
(3) a. [Mitchell][’s], [they][’ve], [do][n’t]
b. [wo][n’t], [ca][n’t], [ai][n’t]
</listItem>
<bodyText confidence="0.9992926">
These examples show that different applications
(tagging, parsing, information extraction) and the
focus on different levels of description (morphol-
ogy, syntax, semantics) require specialized tok-
enization strategies. When working with multiple
</bodyText>
<footnote confidence="0.872792">
1Double surname consisting of Herzog and von der Heide.
</footnote>
<listItem confidence="0.975944">
(4) doesn’t
a. [does][n’t] (Marcus et al., 1993, PTB)
b. [doesn][’][t] (Brants, 2000, TnT)
</listItem>
<bodyText confidence="0.997198043478261">
When creating a corpus that is annotated at multi-
ple levels and/or using several tools, different tok-
enizations are not always avoidable, as some tools
(automatic NLP tools, but also tools for manual
annotation) have integrated tokenizers. Another
challenge is the representation of token bound-
aries. Commonly, token boundaries are repre-
sented by a line break (‘\n’) or the whitespace
‘character’ (‘ ’) – in which case token-internal
whitespaces are replaced, usually by an under-
score (’ ’) –, thereby corrupting the original data.
This practice makes reconciling/merging the data
a difficult enterprise.
Given this background, we suggest an XML-
based annotation of token boundaries, such that
token boundaries are marked without affecting the
original primary data. In a straightforward XML
model, tokens are represented by XML elements
enclosing primary text slices (c.f. the BNC encod-
ing scheme (Burnard, 2007)). However, treating
tokens as spans of text by means of the XML hier-
archy is impossible for tokenization conflicts as in
(4.a) and (4.b).
</bodyText>
<sectionHeader confidence="0.79954" genericHeader="introduction">
2 Conflicting tokenizations:
</sectionHeader>
<subsectionHeader confidence="0.644363">
Straightforward strategies
</subsectionHeader>
<bodyText confidence="0.779505529411765">
By ‘straightforward strategies’, we mean ap-
proaches that aim to preserve the definition of to-
kens as atomic, minimal, unambiguous units of
annotation when unifying different tokenizations
(henceforth Tl and T2) of the same text. By ‘un-
supervised straightforward strategies’, we mean
tokenization strategies that operate on the primary
data only, without consulting external resources
such as dictionaries or human expertise.
Unsupervised straightforward strategies to the
task include:
1. no merging In a conservative approach, we
could create independent annotation projects for
every tokenization produced, and thus represent
all tokenizations independently. This, however,
rules out any integration or combined evaluation
of annotations to Tl and annotations to T2.
</bodyText>
<page confidence="0.919702">
36
</page>
<listItem confidence="0.842132428571428">
2. normalization Adopt one of the source tok-
enizations, say Tl, as the ‘standard’ tokenization.
Preserve only the information annotated to T2 that
is consistent with Tl. Where tokenization T2 de-
viates from Tl, all annotations to T2 are lost.2
3. maximal tokens For every token boundary
in Tl that is also found in T2, establish a token
boundary in the merged tokenization (cf. Guo’s
1997 ‘critical tokenization’). However, with to-
kens assumed to be the minimal elements of anno-
tation, we lose linguistic analyses of fine-grained
tokens. With respect to (4.a) and (4.b), the max-
imal token would be the whole phrase doesn’t.
Again, this results in a loss of information, as all
annotations applied to does, doesn, n’t, ’ and t re-
fer to units that are smaller than the resulting to-
ken.
4. maximal common substrings For every
token boundary in Tl or T2, establish a token
boundary, thereby producing minimal tokens:
one token for every maximal substring shared
between Tl and T2 (cf. Guo’s 1997 ‘shortest
tokenization’). By defining the original tokens
(‘supertokens’) as annotations spanning over
tokens, all annotations are preserved. However,
the concept of ‘token’ loses its theoretical motiva-
tion; there is no guarantee that maximal common
substrings are meaningful elements in any sense:
</listItem>
<bodyText confidence="0.989386227272727">
The maximum common substring tokenization
of 4.a and 4.b is [does][n][’][t], but [n] is not
a well-defined token. It is neither defined with
respect to morphology (like PTB tokens) nor is
it motivated from orthography (like TnT tokens),
but it is just the remainder of their intersection.
As shown in Table 1, none of the strategies
sketched above fulfills all criteria identified in Sec-
tion 1.1: Avoiding a merging process counteracts
data integration; token normalization and maximal
tokens violate information preservation, and maxi-
mal common substrings violate the requirement to
specify a theoretically well-defined notion of to-
ken.
As an alternative, we propose a formalism for
the lossless integration and representation of con-
2Alternatively, transformation rules to map annotations
from T to T would have to be developed. This does, how-
ever, not guarantee information preservation, and, addition-
ally, it requires manual work, as such transformations are
annotation-specific. Thus, it is not an option for the fully
automated merging of tokenizations.
</bodyText>
<tableCaption confidence="0.860604">
Table 1: Deficits of ‘straightforward’ merging ap-
proaches
</tableCaption>
<bodyText confidence="0.842111666666667">
no normalize max. max. common
merge tokens substrings
information preservation
</bodyText>
<equation confidence="0.998558571428571">
+ − − +
well-defined tokens
+ + (−) −
integrative
− + + +
unsupervised
(+) + + +
</equation>
<bodyText confidence="0.999977588235294">
flicting tokenizations by abandoning the assump-
tion that tokens are an atomic, primitive con-
cept that represents the minimal unit of annota-
tion. Rather, we introduce annotation elements
smaller than the actual token – so-called termi-
nals or terms for short – that are defined accord-
ing to the maximum common substrings strategy
described above.
Then, tokens are defined as nodes that span
over a certain range of terms similar to phrase
nodes that dominate other nodes in syntax annota-
tions. The representation of conflicting tokeniza-
tions, then, requires a format that is capable to
express conflicting hierarchies. For this purpose,
we describe an extension of the PAULA format, a
generic format for text-oriented linguistic annota-
tions based on standoff XML.
</bodyText>
<sectionHeader confidence="0.993411" genericHeader="method">
3 Conflicting tokenizations in the
PAULA format
</sectionHeader>
<subsectionHeader confidence="0.999984">
3.1 Annotation structures in PAULA 1.0
</subsectionHeader>
<bodyText confidence="0.9999744375">
The PAULA format (Dipper, 2005; Dipper and
G¨otze, 2005) is a generic XML format, used as a
pivot format in NLP pipelines (Stede et al., 2006)
and in the web-based corpus interface ANNIS
(Chiarcos et al., 2008). It uses standoff XML rep-
resentations, and is conceptually closely related to
the formats NITE XML (Carletta et al., 2003) and
GraF (Ide and Suderman, 2007).
PAULA was specifically designed to support the
lossless representation of different types of text-
oriented annotations (layer-based/timeline anno-
tations, hierarchical annotations, pointing rela-
tions), optimized for the annotation of multiple
layers, including conflicting hierarchies and sim-
ple addition/deletion routines for annotation lay-
ers. Therefore, primary data is stored in a separate
</bodyText>
<page confidence="0.999638">
37
</page>
<tableCaption confidence="0.996465">
Table 2: PAULA 1.0 data types
</tableCaption>
<bodyText confidence="0.927539765957447">
nodes (structural units of annotation) edges (relational units of annotation, connecting tokens,
markables, structs)
token character spans in the primary data that form the basis
for higher-level annotation dominance relation directed edge between a struct
and its children
markable (spans of) token(s) that can be annotated with lin-
guistic information. Markables represent flat, layer-based pointing relations directed edge between nodes in
annotations defined with respect to the sequence of tokens general (tokens, markables, structs)
as a general timeline.
labels (annotations: node or edge labels)
struct hierarchical structures (DAGs or trees) are formed by
establishing a dominance relation between a struct (e.g., features represent annotations attached to a particular
a phrase) node as parent, and tokens, markables, or other (structural or relational) unit of annotation
struct nodes as children.
file. Multiple annotations are also stored in sepa-
rate files to avoid interference between concurrent
annotations. Annotations refer to the primary data
or to other annotations by means of XLinks and
XPointers.
As types of linguistic annotation, we distinguish
nodes (token, markable, struct), edges (dominance
and pointing relations) and labels (annotations), as
summarized in Table 2. Each type of annotation
is stored in a separate file, so that competing or
ambiguous annotations can be represented in an
encapsulated way.
PAULA 1.0 is already sufficiently expressive for
capturing the data-heterogeneity sketched above,
including the representation of overlapping seg-
ments, intersecting hierarchies, and alternative an-
notations (e.g., for ambiguous annotations), but
only for annotations above the token level. Fur-
ther, PAULA 1.0 relies on the existence of a
unique layer of non-overlapping, atomic tokens as
minimal units of annotation: For all nodes, their
position and sequential order is defined with re-
spect to the absolute position of tokens that they
cover; and for the special case of markables, these
are defined solely in terms of their token range.
Finally, PAULA 1.0 tokens are totally ordered,
they cover the (annotated) primary data com-
pletely, and they are non-overlapping. Only on
this basis, the extension and (token-)distance of
annotated elements can be addressed; and only
by means of unambiguous reference, information
from different layers of annotation can be com-
bined and evaluated.
</bodyText>
<subsectionHeader confidence="0.999058">
3.2 Introducing terminal nodes
</subsectionHeader>
<bodyText confidence="0.99987190625">
In our extension of the PAULA format, we in-
troduce the new concept of term nodes: atomic
terminals that directly point to spans of primary
data. Terms are subject to the same constraints as
tokens in PAULA 1.0 (total order, full coverage,
non-overlapping). So, terms can be used in place
of PAULA 1.0 tokens to define the extension and
position of super-token level and sub-token level
annotation elements.
Markables are then defined with respect to
(spans of) terminal nodes rather than tokens, such
that alternative tokenizations can be expressed as
markables in different layers that differ in their ex-
tensions.
Although terms adopt several functions for-
merly associated with tokens, a privileged token
layer is still required: In many query languages,
including ANNIS-QL (Chiarcos et al., 2008), to-
kens define the application domain of regular ex-
pressions on the primary data. More impor-
tantly, tokens constitute the basis for conventional
(“word”) distance measurements and (“word”)
coverage queries. Consequently, the constraints
on tokens (total order, full coverage and absence
of overlap) remain.
The resulting specifications for structural units
of annotation are summarized in Table 3. Distin-
guishing terminal elements and re-defining the to-
ken layer as a privileged layer of markables al-
lows us to disentangle the technical concept of
‘atomic element’ and ‘token’ as the convention-
ally assumed minimal unit of linguistic analysis.
</bodyText>
<subsectionHeader confidence="0.996248">
3.3 A merging algorithm
</subsectionHeader>
<bodyText confidence="0.9999775">
In order to integrate annotations on tokens, it is
not enough to represent two tokenizations side by
side with reference to the same layer of terminal
nodes. Instead, a privileged token layer is to be es-
tablished and it has to be ensured that annotations
can be queried with reference to the token layer.
</bodyText>
<page confidence="0.998933">
38
</page>
<tableCaption confidence="0.999611">
Table 3: PAULA extensions: revised node types
</tableCaption>
<bodyText confidence="0.958928611111111">
terms specify character spans in the primary data
that form the basis for higher-level annota-
tion
markable defined as above, with terms taking the
place of tokens
structs defined as above, with terms taking the
place of tokens
tokens sub-class of structs that are non-
overlapping, arranged in a total order,
and cover the full primary data
Then, all annotations whose segmentation is con-
sistent with the privileged token layer are directly
linked with tokens.
Alg. 3.1 describes our merging algorithm, and
its application to the four main cases of conflict-
ing tokenization is illustrated in Figure 1.3 The
following section describes its main characteris-
tics and the consequences for querying.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.997698171428572">
Alg. 3.1 produces a PAULA project with one sin-
gle tokenization. So, it is possible to define queries
spanning across annotations with originally differ-
ent tokenization:
Extension and precedence queries are
tokenization-independent: Markables refer to
the term layer, not the tok layer, structs also
(indirectly) dominate term nodes.
Dominance queries for struct nodes and tokens
yield results whenever the struct node dominates
only nodes with tok-compatible source tokeniza-
tion: Structs dominate tok nodes wherever the
original tokenization was consistent with the
privileged tokenization tok (case A and C in Fig.
1).
Distance queries are defined with respect to the
tok layer, and are applicable to all elements that
are are defined with reference to the tok layer (in
figure 1: toka, toka, tokb, tokb in case A; tokab
in case B; toka, tokb, tokab in case C; tokab, tokc
in case D). They are not applicable to elements
that do not refer to the tok layer (B: toka, tokb; D:
toka, tokbc).
3Notation: prim – primary data / tok, term – annota-
tion layers / t ∈ L – t is a node on a layer L / a..b – con-
tinuous span from tok/term a to tok/term b / a, b – list of
tok/term/markable nodes a, b / t = [a] – t is a node (struct,
markable, tok) that points to a node, span or list a
The algorithm is unsupervised, and the token
concept of the output tokenization is well-defined
and consistent (if one of the input tokenizations
is adopted as target tokenization). Also, as shown
below, it is integrative (enabling queries across dif-
ferent tokenizations) and information-preserving
(reversible).
</bodyText>
<subsectionHeader confidence="0.99742">
4.1 Time complexity
</subsectionHeader>
<bodyText confidence="0.999911210526316">
After a PAULA project has been created, the time
complexity of the algorithm is quadratic with re-
spect to the number of characters in the primary
data n. This is due to the total order of tokens:
Step 2 and 3.a are applied once to all original to-
kens from left to right. Step 5 can be reformulated
such that for every terminal node, the relationship
between the directly dominating tok and tok is
checked. Then, Step 5 is also in O(n). In terms of
the number of markables m, the time complexity
in Step 3.b is in O(n m): for every markable, the
corresponding term element is to be found, tak-
ing at most n repositioning operations on the term
layer. Assuming that markables within one layer
are non-overlapping4 and that the number of lay-
ers is bound by some constant c5, then m ≤ n c,
so that 3.b is in O(n c).
For realistic scenarios, the algorithm is thus
quadratic.
</bodyText>
<subsectionHeader confidence="0.933867">
4.2 Reversibility
</subsectionHeader>
<bodyText confidence="0.999975222222222">
The merging algorithm is reversible – and, thus,
lossless – as shown by the splitting algorithm in
Alg. 3.2. For reasons of space, the correctness
of this algorithm cannot be demonstrated here, but
broadly speaking, it just removes every node that
corresponds to an original token of the ‘other’ tok-
enization, plus every node that points to it, so that
only annotations remain that are directly applied
to the target tokenization.
</bodyText>
<subsectionHeader confidence="0.999236">
4.3 Querying merged tokenizations
</subsectionHeader>
<bodyText confidence="0.650161833333333">
We focus in this paper on the merging of analy-
ses with different tokenizations for the purpose of
users querying a corpus across multiple annota-
4Although PAULA supports overlapping markables
within one single layer, even with identical extension, this is
a reasonable assumption: In practice, overlapping markables
within one single layer are rare. More often, there is even a
longer sequence of primary data between one markable of a
particular layer and the next. In our experience, such ‘gaps’
occur much more often than overlapping markables.
5Again, this is a practical simplication. Theoretically, the
number of layers is infinite.
</bodyText>
<page confidence="0.997423">
39
</page>
<figure confidence="0.971636666666666">
Alg. 3.1 Merging different tokenizations
0. assume that we have two annotations analysis and analysis for the same primary data, but with different tokenizations
1. create PAULA 1.0 annotation projects for analysis and analysis with primary data files prim and prim and token
layers tok and tok respectively.
2. harmonize primary data
if prim equals prim, then
(i) rename prim to prim
(ii) set all references in analysis from prim to prim
(iii) create a new annotation project analysis by copying prim and all annotation layers from analysis and analysis
otherwise terminate with error msg
3. harmonize terminal nodes
create a new annotation layer term, then
(a) for all overlapping tokens t E tok and t E tok: identify the maximal common substrings of t and t
for every substring s, create a new element terms pointing to the corresponding character span in the primary data
for every substring s, redefine t and t as markables referring to terms
(b) redefine markable spans as spans of terminal nodes
for every token t = [terms..terms] E tok U tok and every markable m = [w..xty..z]: set m =
[w..xterms..termsy..z]
4. select token layer
rename tok to tok, or rename tok to tok, (cf. the normalization strategy in Sect. 2) or
rename term to tok (cf. the minimal tokens strategy in Sect. 2)
5. token integration
for every original token ot = [a..b] E (tok U tok) \ tok:
if there is a token t E tok such that t = [a..b], then define ot as a struct with ot = [t], else
if there are tokens t, .., tn E tok such that t..tn form a continuous sequence of tokens and t = [a..x] and tn = [y..b],
then define ot as a struct such that ot = [t, .., tn],
otherwise: change nothing
</figure>
<figureCaption confidence="0.999859">
Figure 1: Merging divergent tokenizations
</figureCaption>
<page confidence="0.994835">
40
</page>
<bodyText confidence="0.974958333333333">
Alg. 3.2 Splitting a PAULA annotation project
with two different tokenizations
0. given a PAULA annotation project analysis with token
layer tok, terminal layer term, and two layers l and l
(that may be identical to term or tok) that convey the
information of the original token layers tok and tok
</bodyText>
<listItem confidence="0.955355947368421">
1. create analysis and analysis as copies of analysis
2. if l represents a totally ordered, non-overlapping list of
nodes that cover the primary data completely, then modify
analysis:
a. for every node in l: substitute references to tok by
references to term
b. remove l from analysis
c. if l =� tok, remove tok from analysis
d. for every annotation element (node/relation) e in
analysis that directly or indirectly points to another
node in analysis that is no longer present, remove e
from analysis
e. remove every annotation layer from analysis that
does not contain an annotation element
f. for every markable in l: remove references to term,
define the extension of l nodes directly in terms of
spans of text in prim
g. if l =� term, remove term
3. perform step 2. for l and analysis
</listItem>
<bodyText confidence="0.990328612903226">
tion layers. Although the merging algorithm pro-
duces annotation projects that allow for queries in-
tegrating annotations from analyses with different
tokenization, the structure of the annotations is al-
tered, such that the behaviour of merged and un-
merged PAULA projects may be different. Obvi-
ously, token-level queries must refer to the priv-
ileged tokenization T. Operators querying for
the relative precedence or extension of markables
are not affected: in the merged annotation project,
markables are defined with reference to the layer
term: originally co-extensional elements E and
E (i.e. elements covering the same tokens in the
source tokenization) will also cover the same ter-
minals in the merged project. Distance operators
(e.g. querying for two tokens with distance 2, i.e.
with two tokens in between), however, will oper-
ate on the new privileged tokenization, such that
results from queries on analysis may differ from
those on analysis. Dominance operators are
also affected, as nodes that directly dominated a
token in analysis or analysis now indirectly
dominate it in analysis, with a supertoken as an
intermediate node.
Alg. 3.3 Iterative merging: modifications of Alg.
3.1, step.3
if analysis has a layer of terminal nodes term, then let
T = term, otherwise T = tok
if analysis has a layer of terminal nodes term, then let
T = term, otherwise T = tok
create a new annotation layer term, then
</bodyText>
<listItem confidence="0.757497333333333">
1. for all overlapping terminals/tokens t E T and t E
T: identify the maximal common substrings of t and
t
</listItem>
<bodyText confidence="0.9040184">
for every substring s, create a new element terms
pointing to the corresponding character span in the pri-
mary data
for every substring s, redefine t and t as markables
referring to terms
</bodyText>
<listItem confidence="0.995372666666667">
2. redefine markable spans as spans of terminal nodes
for every node t = [termsy..terms,] E T U T
and every markable m = [w..xty..z]: set
m = [w..xtermsy..terms,y..z]
3. for all original terminals t E TUT: if t is not directly
pointed at, remove t from analysis
</listItem>
<bodyText confidence="0.999776166666667">
Accordingly, queries applicable to PAULA
projects before the merging are not directly appli-
cable to merged PAULA projects. Users are to be
instructed to keep this in mind and to be aware of
the specifications for the merged tokenization and
its derivation.6
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="method">
5 Extensions
</sectionHeader>
<subsectionHeader confidence="0.998761">
5.1 Merging more than two tokenizations
</subsectionHeader>
<bodyText confidence="0.999985714285714">
In the current formulation, Alg. 3.1 is applied to
two PAULA 1.0 projects and generates extended
PAULA annotation projects with a term layer.
The algorithm, however, may be applied itera-
tively, if step 3 is slightly revised, such that ex-
tended PAULA annotation projects can also be
merged, see Alg. 3.3.
</bodyText>
<subsectionHeader confidence="0.998859">
5.2 Annotation integration
</subsectionHeader>
<bodyText confidence="0.999944">
The merging algorithm creates a struct node for
every original token. Although this guarantees re-
versibility, one may consider to remove such re-
dundant structs. Alg. 3.4 proposes an optional
postprocessing step for the merging algorithm.
This step is optional because these operations are
</bodyText>
<footnote confidence="0.990960333333333">
6The information, however, is preserved in the format and
may be addressed by means of queries that, for example, op-
erate on the extension of terminals.
</footnote>
<page confidence="0.999032">
41
</page>
<tableCaption confidence="0.399624333333333">
Alg. 3.4 Annotation integration: Optional post-
processing for merging algorithm
6.a. remove single-token supertoken
</tableCaption>
<bodyText confidence="0.480101933333333">
for every original token ot = [t] ∈ tok ∪ tok with
t ∈ tok: replace all references in analysis to ot by
references to t, remove ot
6.b. merging original token layers tok and tok (if
tok =6 tok and tok =6 tok)
define new ‘super token’ layer stok.
for every ot ∈ tok ∪ tok:
if ot = [t] for some t ∈ tok, then see 6.a
if ot = [t, .., tn] for some t, .., tn ∈ tok, and
there is ot = [t, .., tn] ∈ tok ∪ tok ∪ stok,
then replace all references in analysis to ot by
references to ot, move ot to layer stok, remove
ot from analysis
move all remaining ot ∈ tok ∪ tok to stok, remove
layers tok and tok
</bodyText>
<footnote confidence="0.320172">
6.c. unify higher-level annotations
</footnote>
<construct confidence="0.9297387">
for every markable mark = [term..termn] and
term, .., termn ∈ term:
if there is a markable mark in analysis such
that mark = [term..termn], then replace all
references in analysis to mark by references to
mark, remove mark
for every struct struct = [c, .., cn] that covers ex-
actly the same children as another struct struct =
[c, .., cn], replace all references to struct by refer-
ences to struct, remove struct
</construct>
<bodyText confidence="0.996624333333333">
destructive: We lose the information about the ori-
gin (analysis vs. analysis) of stok elements
and their annotations.
</bodyText>
<sectionHeader confidence="0.996078" genericHeader="conclusions">
6 Summary and Related Reasearch
</sectionHeader>
<bodyText confidence="0.967445463768116">
In this paper, we describe a novel approach for the
integration of conflicting tokenizations, based on
the differentiation between a privileged layer of
tokens and a layer of atomic terminals in a stand-
off XML format: Tokens are defined as structured
units that dominate one or more terminal nodes.
Terminals are atomic units only within the re-
spective annotation project (there is no unit ad-
dressed that is smaller than a terminal). By iter-
ative applications of the merging algorithm, how-
ever, complex terms may be split up in smaller
units, so that they are not atomic in an absolute
sense.
Alternatively, terms could be identified a priori
with the minimal addressable unit available, i.e.,
characters (as in the formalization of tokens as
charspans and charseqs in the ACE information
extraction annotations, Henderson 2000). It is not
clear, however, how a character-based term defini-
tion would deal with sub-character and zero exten-
sion terms: A character-based definition of terms
that represent traces is possible only by corrupt-
ing the primary data.7 Consequently, a character-
based term definition is insufficient unless we re-
strict ourselves to a particular class of languages,
texts and phenomena.
The role of terminals can thus be compared to
timestamps: With reference to a numerical time-
line, it is always possible to define a new event
between two existing timestamps. Formats specif-
ically designed for time-aligned annotations, e.g.,
EXMARaLDA (Schmidt, 2004), however, typi-
cally lack a privileged token layer and a formal
concept of tokens. Instead, tokens, as well as
longer or shorter sequences, are represented as
markables, defined by their extension on the time-
line.
Similarly, GrAF (Ide and Suderman, 2007), al-
though being historically related to PAULA, does
not have a formal concept of a privileged token
layer in the sense of PAULA.8 We do, however,
assume that terminal nodes in GrAF can be com-
pared to PAULA 1.0 tokens.
For conflicting tokenizations, Ide and Suderman
(2007) suggest that ‘dummy’ elements are defined
covering all necessary tokenizations for controver-
sially tokenized stretches of primary data. Such
dummy elements combine the possible tokeniza-
tions for strategies 1 (no merging) and 3 (maxi-
mal tokens), so that the information preservation
deficit of strategy 3 is compensated by strategy 1,
and the integrativity deficit of strategy 1 is com-
pensated by strategy 3 (cf. Table 1). However, to-
kens, if defined in this way, are overlapping and
thus only partially ordered, so that distance opera-
tors are no longer applicable.9
7Similarly, phonological units that are not expressed in
the primary data can be subject to annotations, e.g., short e
and o in various Arabic-based orthographies, e.g., the Ajami
orthography of Hausa. A term with zero extension at the po-
sition of a short vowel can be annotated as having the phono-
logical value e or o without having character status.
8https://www.americannationalcorpus.
org/graf-wiki/wiki/WikiStart#GraphModel,
2009/05/08
9This can be compensated by marking the base segmen-
tation differently from alternative segmentations. In the ab-
stract GrAF model, however, this can be represented only by
means of labels, i.e., annotations. A more consistent con-
</bodyText>
<page confidence="0.997615">
42
</page>
<bodyText confidence="0.999963142857143">
Another problem that arises from the introduc-
tion of dummy nodes is their theoretical status, as
it is not clear how dummy nodes can be distin-
guished from annotation structured on a concep-
tual level. In the PAULA formalization, dummy
nodes are not necessary, so that this ambiguity is
already resolved in the representation.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859901098901">
Thorsten Brants. 2000. TnT A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
ANLP-2000. Seattle, WA.
Lou Burnard (ed.). 2007. Reference Guide
for the British National Corpus (XML Edi-
tion). http://www.natcorp.ox.ac.uk/
XMLedition/URG/bnctags.html.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan
Kilgour, Judy Robertson, and Holger Voormann.
2003. The NITE XML Toolkit: Flexible Annotation
for Multi-modal Language Data. Behavior Research
Methods, Instruments, and Computers 35(3), 353-
363.
Christian Chiarcos, Stefanie Dipper, Michael G¨otze,
Ulf Leser, Anke L¨udeling, Julia Ritz, and Manfred
Stede. 2009. A Flexible Framework for Integrating
Annotations from Different Tools and Tagsets TAL
(Traitement automatique des langues) 49(2).
Oli Christ. 1994. A modular and flexible architec-
ture for an integrated corpus query system. COM-
PLEX’94, Budapest, Hungary.
Stefanie Dipper. 2005. XML-based Stand-off Repre-
sentation and Exploitation ofMulti-Level Linguistic
Annotation. In Rainer Eckstein and Robert Tolks-
dorf (eds:): Proceedings of Berliner XML Tage,
pages 39-50.
Stefanie Dipper and Michael G¨otze. 2005. Accessing
Heterogeneous Linguistic Data — Generic XML-
based Representation and Flexible Visualization. In
Proceedings of the 2nd Language &amp; Technology
Conference 2005, Poznan, Poland, pages 23–30.
Stefanie Dipper, Michael G¨otze. 2006. ANNIS:
Complex Multilevel Annotations in a Linguistic
Database. Proceedings of the 5th Workshop on NLP
and XML (NLPXML-2006): Multi-Dimensional
Markup in Natural Language Processing. Trento,
Italy.
Jin Guo. 1997. Critical Tokenization and its Proper-
ties, Computational Linguistic, 23(4), pp.569-596.
ception would encode structural information on the structural
level, and only linguistic annotation and metadata on the con-
tents level.
John C. Henderson. 2000. A DTD for Reference Key
Annotation of EDT Entities and RDC Relations
in the ACE Evaluations (v. 5.2.0, 2000/01/05),
http://projects.ldc.upenn.edu/ace/
annotation/apf.v5.2.0.dtd (2009/06/04)
Nancy Ide and Keith Suderman. 2007. GrAF: A
Graph-based Format for Linguistic Annotations. In
Proceedings of the Linguistic Annotation Work-
shop,held in conjunction with ACL 2007, Prague,
June 28-29, 1-8.
Esther K¨onig and Wolfgang Lezius. 2000. A descrip-
tion language for syntactically annotated corpora.
In: Proceedings of the COLING Conference, pp.
1056-1060, Saarbr¨ucken, Germany.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics 19, pp.313-330.
Christoph M¨uller and Michael Strube. 2006. Multi-
Level Annotation of Linguistic Data with MMAX2.
In: S. Braun et al. (eds.), Corpus Technology and
Language Pedagogy. New Resources, New Tools,
New Methods. Frankfurt: Peter Lang, 197–214.
Georg Rehm, Oliver Schonefeld, Andreas Witt, Chris-
tian Chiarcos, and Timm Lehmberg. 2009.
SPLICR: A Sustainability Platform for Linguistic
Corpora and Resources. In: Text Resources and
Lexical Knowledge. Selected Papers the 9th Confer-
ence on Natural Language Processing (KONVENS
2008), Berlin, Sept. 30 – Oct. 2, 2008. Mouton de
Gruyter.
Helmut Schmid. 2002. Tokenizing &amp; Tagging. In
L¨udeling, Anke and Kyt¨o, Merja (Hrsg.) Corpus
Linguistics. An International Handbook. (HSK Se-
ries). Mouton de Gryuter, Berlin
Thomas Schmidt. 2004. Transcribing and Annotat-
ing Spoken Language with Exmaralda. Proceedings
of the LREC-workshop on XML Based Richly Anno-
tated Corpora. Lisbon, Portugal. Paris: ELRA.
Manfred Stede, Heike Bieler, Stefanie Dipper, and
Arthit Suriyawongkul. 2006. SUMMaR: Combin-
ing Linguistics and Statistics for Text Summariza-
tion. Proceedings of the 17th European Conference
on Artificial Intelligence (ECAI-06). pp 827-828.
Riva del Garda, Italy.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw
and Linnea Micciulla. 2006. OntoNotes Release
1.0. Linguistic Data Consortium, Philadelphia.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9928605">all these lovely Merging Conflicting Tokenizations</title>
<author confidence="0.982586">Christian Chiarcos</author>
<author confidence="0.982586">Julia Ritz</author>
<author confidence="0.982586">Manfred</author>
<affiliation confidence="0.686327">Sonderforschungsbereich 632 “Information University of</affiliation>
<address confidence="0.290418">Karl-Liebknecht-Str. 24-25, 14476 Golm,</address>
<abstract confidence="0.997395688405798">Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday’s NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora. 1 Motivation 1.1 Tokens: Functions and goals For most NLP tasks and linguistic annotations, especially those concerned with syntax (part-ofspeech tagging, chunking, parsing) and the interpretation of syntactic structures (esp., the extraction of semantic information), tokens represent unit of words (lexemes, semantic units, partly morphemes) on the one hand and certain punctuation symbols on the other hand. From a corpus-linguistic perspective, tokens represent the unit of the minimal character sequence that can be addressed in a corpus query (e.g. using search tools like TIGERSearch (K¨onig and Lezius, 2000) or CWB (Christ, 1994)). Tokens also constitute the for ‘word’ In many annotation tools and their corresponding formats, order of tokens provides a the order of (MMAX (M¨uller and Strube, 2006), GENAU (Rehm et al., 2009), GrAF (Ide and Suderman, 2007), TIGER (K¨onig and Lezius, 2000)). In several multifrom the poem Helen Hunt Jackson. formats, tokens also define the poannotation elements, and only by reference to a common token layer, annotations from different layers can be related with each other (NITE (Carletta et al., 2003), GENAU). Thus, by their function, tokens have the following characteristics: (i) tokens are totally ordered, (ii) tokens cover the full (annotated portion of the) primary data, (iii) tokens are the smallest unit of annotation, and (iv) there is only one single privileged token layer. The last aspect is especially relevant for the study of richly annotated data, as an integration and serialization of annotations produced by different tools can be established only by reference to the token layer. From a corpus-linguistic perspective, i.e., when focusing on querying of annotated corpora, tokens need to be well-defined and all information annotated to a particular text is to be preserved without any corruption. We argue that for this purpose, characteristic (iii) is to be abandoned, and we will describe the data format and an algorithm for merging different tokenizations and their respective annotations. Our goal is a fully automated merging of annotations that refer to different tokenizations (henceand of the same text. We regard the following criteria as crucial for this task: preservation. annotations applied to the original tokenizations should be preserved. well-defined notion of token. should be possible to give a plausible list of positive criteria that define character sequences as tokens. Knowledge about the token definition is essential for formulating queries for words, e.g. in a corpus search interface. representation. annotations that are consistent with the merged tokenization should refer to the merged tokenization. This is necessary order to query across multiple annotations orig- 35 of the Third Linguistic Annotation Workshop, ACL-IJCNLP pages 35–43, Singapore, 6-7 August 2009. ACL and AFNLP inating from different annotation layers or tools. merging. integration of conflicting tokenizations should not require manual interference. tools for standard NLP tasks, thus, it is the norm rather than the exception that they disagree in their tokenization, as shown in ex. (4). 1.2 Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the ‘word’. For a morphosyntactic analof speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in A syntactic analysis however, could profit from the aggregation of complex nominals into one token each. (1) a. department store Herzog-von der c. Red Cross/Red Crescent movement Similarly, examples (2a) and (2b) can be arto be treated as one token for (morrespectively. Despite intervening whitespaces and punctuation symbols, they are complex instances of the ‘classical’ part- For certain analyses such as in information extraction, however, it may be useful to split these compounds in order to acthe inherent complements 605, No. (2) a. E 605-intoxicated b. No. 22-rated (3) illustrates a tokenization strategy: the principle of splitting at morpheme boundaries (Marcus et al., 1993, PTB) (token boundaries represented by square brackets). Morphological tokenization may help distributional (co-occurrence-based) semantics and/or parsing; however, the resulting tokens might be argued as being less intuitive to users of a corpus search tool. (3) a. [Mitchell][’s], [they][’ve], [do][n’t] b. [wo][n’t], [ca][n’t], [ai][n’t] These examples show that different applications (tagging, parsing, information extraction) and the focus on different levels of description (morphology, syntax, semantics) require specialized tokenization strategies. When working with multiple surname consisting of der (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) When creating a corpus that is annotated at multiple levels and/or using several tools, different tokenizations are not always avoidable, as some tools (automatic NLP tools, but also tools for manual annotation) have integrated tokenizers. Another challenge is the representation of token boundaries. Commonly, token boundaries are repreby a line break or the whitespace ‘character’ (‘ ’) – in which case token-internal whitespaces are replaced, usually by an underscore (’ ’) –, thereby corrupting the original data. This practice makes reconciling/merging the data a difficult enterprise. Given this background, we suggest an XMLbased annotation of token boundaries, such that token boundaries are marked without affecting the original primary data. In a straightforward XML model, tokens are represented by XML elements enclosing primary text slices (c.f. the BNC encoding scheme (Burnard, 2007)). However, treating tokens as spans of text by means of the XML hierarchy is impossible for tokenization conflicts as in (4.a) and (4.b). 2 Conflicting tokenizations: Straightforward strategies By ‘straightforward strategies’, we mean approaches that aim to preserve the definition of tokens as atomic, minimal, unambiguous units of annotation when unifying different tokenizations and of the same text. By ‘unsupervised straightforward strategies’, we mean tokenization strategies that operate on the primary data only, without consulting external resources such as dictionaries or human expertise. Unsupervised straightforward strategies to the task include: no merging a conservative approach, we could create independent annotation projects for every tokenization produced, and thus represent all tokenizations independently. This, however, rules out any integration or combined evaluation annotations to and annotations to 36 normalization one of the source toksay as the ‘standard’ tokenization. information annotated to that consistent with Where tokenization defrom all annotations to are maximal tokens every token boundary that is also found in establish a token boundary in the merged tokenization (cf. Guo’s 1997 ‘critical tokenization’). However, with tokens assumed to be the minimal elements of annotation, we lose linguistic analyses of fine-grained tokens. With respect to (4.a) and (4.b), the maxtoken would be the whole phrase Again, this results in a loss of information, as all applied to refer to units that are smaller than the resulting token. maximal common substrings every boundary in or establish a token thereby producing one token for every maximal substring shared and (cf. Guo’s 1997 ‘shortest tokenization’). By defining the original tokens (‘supertokens’) as annotations spanning over tokens, all annotations are preserved. However, the concept of ‘token’ loses its theoretical motivation; there is no guarantee that maximal common substrings are meaningful elements in any sense: The maximum common substring tokenization 4.a and 4.b is but not a well-defined token. It is neither defined with respect to morphology (like PTB tokens) nor is it motivated from orthography (like TnT tokens), but it is just the remainder of their intersection. As shown in Table 1, none of the strategies sketched above fulfills all criteria identified in Section 1.1: Avoiding a merging process counteracts data integration; token normalization and maximal tokens violate information preservation, and maximal common substrings violate the requirement to specify a theoretically well-defined notion of token. As an alternative, we propose a formalism for lossless integration and representation of contransformation rules to map annotations to would have to be developed. This does, however, not guarantee information preservation, and, additionally, it requires manual work, as such transformations are annotation-specific. Thus, it is not an option for the fully automated merging of tokenizations. Table 1: Deficits of ‘straightforward’ merging approaches no normalize max. max. common merge tokens substrings information preservation + − − + well-defined tokens + + (−) − integrative − + + + unsupervised (+) + + + flicting tokenizations by abandoning the assumption that tokens are an atomic, primitive concept that represents the minimal unit of annotation. Rather, we introduce annotation elements than the actual token – so-called termifor short – that are defined according to the maximum common substrings strategy described above. Then, tokens are defined as nodes that span over a certain range of terms similar to phrase nodes that dominate other nodes in syntax annotations. The representation of conflicting tokenizations, then, requires a format that is capable to express conflicting hierarchies. For this purpose, we describe an extension of the PAULA format, a generic format for text-oriented linguistic annotations based on standoff XML. 3 Conflicting tokenizations in the PAULA format 3.1 Annotation structures in PAULA 1.0 The PAULA format (Dipper, 2005; Dipper and G¨otze, 2005) is a generic XML format, used as a pivot format in NLP pipelines (Stede et al., 2006) and in the web-based corpus interface ANNIS (Chiarcos et al., 2008). It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al., 2003) and GraF (Ide and Suderman, 2007). PAULA was specifically designed to support the lossless representation of different types of textoriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including conflicting hierarchies and simple addition/deletion routines for annotation layers. Therefore, primary data is stored in a separate 37 Table 2: PAULA 1.0 data types nodes(structural units of annotation) units of annotation, connecting tokens, markables, structs) spans in the primary data that form the basis higher-level annotation relation edge between a struct and its children of) token(s) that can be annotated with lininformation. Markables represent flat, layer-based relations edge between nodes in annotations defined with respect to the sequence of tokens general (tokens, markables, as a general timeline. labels(annotations: node or edge labels) structures (DAGs or trees) are formed by a dominance relation between a struct (e.g., annotations attached to a particular a phrase) node as parent, and tokens, markables, or other (structural or relational) unit of struct nodes as children. file. Multiple annotations are also stored in separate files to avoid interference between concurrent annotations. Annotations refer to the primary data or to other annotations by means of XLinks and XPointers. As types of linguistic annotation, we distinguish nodes (token, markable, struct), edges (dominance and pointing relations) and labels (annotations), as summarized in Table 2. Each type of annotation is stored in a separate file, so that competing or ambiguous annotations can be represented in an encapsulated way. PAULA 1.0 is already sufficiently expressive for capturing the data-heterogeneity sketched above, including the representation of overlapping segments, intersecting hierarchies, and alternative annotations (e.g., for ambiguous annotations), but for annotations token level. Further, PAULA 1.0 relies on the existence of a unique layer of non-overlapping, atomic tokens as minimal units of annotation: For all nodes, their position and sequential order is defined with respect to the absolute position of tokens that they cover; and for the special case of markables, these are defined solely in terms of their token range. PAULA 1.0 tokens are (annotated) primary data comand they are Only on this basis, the extension and (token-)distance of annotated elements can be addressed; and only by means of unambiguous reference, information from different layers of annotation can be combined and evaluated. 3.2 Introducing terminal nodes In our extension of the PAULA format, we inthe new concept of atomic terminals that directly point to spans of primary subject to the same constraints as tokens in PAULA 1.0 (total order, full coverage, non-overlapping). So, terms can be used in place PAULA 1.0 define the extension and position of super-token level and sub-token level annotation elements. Markables are then defined with respect to (spans of) terminal nodes rather than tokens, such that alternative tokenizations can be expressed as markables in different layers that differ in their extensions. Although terms adopt several functions formerly associated with tokens, a privileged token layer is still required: In many query languages, including ANNIS-QL (Chiarcos et al., 2008), tokens define the application domain of regular expressions on the primary data. More importantly, tokens constitute the basis for conventional (“word”) distance measurements and (“word”) coverage queries. Consequently, the constraints on tokens (total order, full coverage and absence of overlap) remain. The resulting specifications for structural units of annotation are summarized in Table 3. Distinguishing terminal elements and re-defining the token layer as a privileged layer of markables allows us to disentangle the technical concept of ‘atomic element’ and ‘token’ as the conventionally assumed minimal unit of linguistic analysis. 3.3 A merging algorithm In order to integrate annotations on tokens, it is not enough to represent two tokenizations side by side with reference to the same layer of terminal nodes. Instead, a privileged token layer is to be established and it has to be ensured that annotations be queried reference to the token 38 Table 3: PAULA extensions: revised node types character spans in the primary data that form the basis for higher-level annotation as above, with terms taking the place of tokens as above, with terms taking place of tokens of structs that are nonoverlapping, arranged in a total order, and cover the full primary data Then, all annotations whose segmentation is consistent with the privileged token layer are directly linked with tokens. Alg. 3.1 describes our merging algorithm, and its application to the four main cases of conflicttokenization is illustrated in Figure The following section describes its main characteristics and the consequences for querying. 4 Discussion Alg. 3.1 produces a PAULA project with one single tokenization. So, it is possible to define queries spanning across annotations with originally different tokenization: are tokenization-independent: Markables refer to not the structs also dominate for struct nodes and tokens yield results whenever the struct node dominates nodes with source tokeniza- Structs dominate wherever the original tokenization was consistent with the tokenization A and C in Fig. 1). are defined with respect to the and are applicable to all elements that are defined with reference to the (in 1: in case A; case B; in case C; in case D). They are not applicable to elements do not refer to the tok layer (B: D: primary data / term annotalayers / a node on a layer conspan from tok/term tok/term b list of nodes a node (struct, tok) that points to a node, span or list The algorithm is unsupervised, and the token concept of the output tokenization is well-defined and consistent (if one of the input tokenizations is adopted as target tokenization). Also, as shown below, it is integrative (enabling queries across different tokenizations) and information-preserving (reversible). 4.1 Time complexity After a PAULA project has been created, the time complexity of the algorithm is quadratic with respect to the number of characters in the primary This is due to the total order of tokens: Step 2 and 3.a are applied once to all original tokens from left to right. Step 5 can be reformulated that for every the relationship the directly dominating and is Then, Step 5 is also in In terms of number of markables the time complexity Step 3.b is in for every markable, the is to be found, takat most operations on the layer. Assuming that markables within one layer and that the number of layis bound by some constant then that 3.b is in For realistic scenarios, the algorithm is thus quadratic. 4.2 Reversibility The merging algorithm is reversible – and, thus, lossless – as shown by the splitting algorithm in Alg. 3.2. For reasons of space, the correctness of this algorithm cannot be demonstrated here, but broadly speaking, it just removes every node that corresponds to an original token of the ‘other’ tokenization, plus every node that points to it, so that only annotations remain that are directly applied to the target tokenization. 4.3 Querying merged tokenizations We focus in this paper on the merging of analyses with different tokenizations for the purpose of a corpus across multiple annota- PAULA supports overlapping markables within one single layer, even with identical extension, this is a reasonable assumption: In practice, overlapping markables within one single layer are rare. More often, there is even a longer sequence of primary data between one markable of a particular layer and the next. In our experience, such ‘gaps’ occur much more often than overlapping markables. this is a practical simplication. Theoretically, the number of layers is infinite. 39 3.1 different tokenizations assume that we have two annotations and for the same primary data, but with different tokenizations create PAULA 1.0 annotation projects for and with primary data files and and token and respectively. 2. harmonize primary data equals then rename to set all references in from to create a new annotation project copying all annotation layers from and otherwise terminate with error msg 3. harmonize terminal nodes a new annotation layer then for all overlapping tokens E and E identify the maximal common substrings of and every substring create a new element pointing to the corresponding character span in the primary data every substring redefine and as markables referring to (b) redefine markable spans as spans of terminal nodes every token U and every markable set 4. select token layer to or rename to (cf. the normalization strategy in Sect. 2) or the minimal tokens strategy in Sect. 2) 5. token integration every original token U there is a token that then define a struct with else there are tokens .., E that form a continuous sequence of tokens and = = define a struct such that .., otherwise: change nothing Figure 1: Merging divergent tokenizations 40 3.2 a PAULA annotation project with two different tokenizations given a PAULA annotation project token terminal layer and two layers and may be identical to that convey the of the original token layers and create and as copies of if represents a totally ordered, non-overlapping list of nodes that cover the primary data completely, then modify for every node in substitute references to by to remove from if remove from for every annotation element (node/relation) that directly or indirectly points to another in that is no longer present, remove remove every annotation layer from that does not contain an annotation element for every markable in remove references to the extension of nodes directly in terms of of text in if remove perform step 2. for and Although the merging algorithm produces annotation projects that allow for queries integrating annotations from analyses with different tokenization, the structure of the annotations is altered, such that the behaviour of merged and unmerged PAULA projects may be different. Obviously, token-level queries must refer to the privtokenization Operators querying for relative or extension markables are not affected: in the merged annotation project, markables are defined with reference to the layer originally co-extensional elements and (i.e. elements covering the same tokens in the tokenization) will also cover the same terthe merged project. operators (e.g. querying for two tokens with distance 2, i.e. with two tokens in between), however, will operate on the new privileged tokenization, such that from queries on differ from on operators also affected, as nodes that directly dominated a in or now indirectly it in with a supertoken as an intermediate node. 3.3 merging: modifications of Alg. 3.1, step.3 has a layer of terminal nodes then let = otherwise = has a layer of terminal nodes then let = otherwise = a new annotation layer then for all overlapping terminals/tokens E and E identify the maximal common substrings of and every substring create a new element pointing to the corresponding character span in the primary data every substring redefine and as markables to 2. redefine markable spans as spans of terminal nodes every node U every markable set for all original terminals if not directly at, remove Accordingly, queries applicable to PAULA merging are not directly applicable to merged PAULA projects. Users are to be instructed to keep this in mind and to be aware of the specifications for the merged tokenization and 5 Extensions 5.1 Merging more than two tokenizations In the current formulation, Alg. 3.1 is applied to two PAULA 1.0 projects and generates extended annotation projects with a The algorithm, however, may be applied iteratively, if step 3 is slightly revised, such that extended PAULA annotation projects can also be merged, see Alg. 3.3. 5.2 Annotation integration The merging algorithm creates a struct node for every original token. Although this guarantees reversibility, one may consider to remove such redundant structs. Alg. 3.4 proposes an optional postprocessing step for the merging algorithm. This step is optional because these operations are information, however, is preserved in the format and may be addressed by means of queries that, for example, operate on the extension of terminals. 41 3.4 integration: Optional postprocessing for merging algorithm 6.a. remove single-token supertoken every original token ∪ with replace all references in to remove merging original token layers and (if new ‘super token’ layer every ∪ some then see 6.a .., some .., ∈ and is = .., ∪ ∪ replace all references in by to move layer remove from all remaining ∪ to remove and 6.c. unify higher-level annotations every markable = .., ∈ there is a markable in = then replace all in by references to remove every struct = .., covers exthe same children as another struct = .., replace all references to by referto remove destructive: We lose the information about the orivs. of and their annotations. 6 Summary and Related Reasearch In this paper, we describe a novel approach for the integration of conflicting tokenizations, based on the differentiation between a privileged layer of tokens and a layer of atomic terminals in a standoff XML format: Tokens are defined as structured units that dominate one or more terminal nodes. are atomic units only respective annotation project (there is no unit addressed that is smaller than a terminal). By iterative applications of the merging algorithm, however, complex terms may be split up in smaller units, so that they are not atomic in an absolute sense. Alternatively, terms could be identified a priori with the minimal addressable unit available, i.e., characters (as in the formalization of tokens as and in the ACE information extraction annotations, Henderson 2000). It is not clear, however, how a character-based term definition would deal with sub-character and zero extension terms: A character-based definition of terms that represent traces is possible only by corruptthe primary Consequently, a characterbased term definition is insufficient unless we restrict ourselves to a particular class of languages, texts and phenomena. The role of terminals can thus be compared to timestamps: With reference to a numerical timeline, it is always possible to define a new event between two existing timestamps. Formats specifically designed for time-aligned annotations, e.g., EXMARaLDA (Schmidt, 2004), however, typically lack a privileged token layer and a formal concept of tokens. Instead, tokens, as well as longer or shorter sequences, are represented as markables, defined by their extension on the timeline. Similarly, GrAF (Ide and Suderman, 2007), although being historically related to PAULA, does not have a formal concept of a privileged token in the sense of We do, however, assume that terminal nodes in GrAF can be compared to PAULA 1.0 tokens. For conflicting tokenizations, Ide and Suderman (2007) suggest that ‘dummy’ elements are defined covering all necessary tokenizations for controversially tokenized stretches of primary data. Such dummy elements combine the possible tokenizations for strategies 1 (no merging) and 3 (maximal tokens), so that the information preservation deficit of strategy 3 is compensated by strategy 1, and the integrativity deficit of strategy 1 is compensated by strategy 3 (cf. Table 1). However, tokens, if defined in this way, are overlapping and thus only partially ordered, so that distance operaare no longer phonological units that are not expressed in primary data can be subject to annotations, e.g., short various Arabic-based orthographies, e.g., the Ajami orthography of Hausa. A term with zero extension at the position of a short vowel can be annotated as having the phonovalue having character status. 2009/05/08 can be compensated by marking the base segmentation differently from alternative segmentations. In the abstract GrAF model, however, this can be represented only by of labels, i.e., annotations. A more consistent con- 42 Another problem that arises from the introduction of dummy nodes is their theoretical status, as it is not clear how dummy nodes can be distinguished from annotation structured on a conceptual level. In the PAULA formalization, dummy nodes are not necessary, so that this ambiguity is already resolved in the representation.</abstract>
<note confidence="0.888917666666667">References Brants. 2000. A Statistical Part-of- In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000. Seattle, WA. Lou Burnard (ed.). 2007. Reference Guide</note>
<title confidence="0.822924">for the British National Corpus (XML Edi-</title>
<author confidence="0.959696">Jean Carletta</author>
<author confidence="0.959696">Stefan Evert</author>
<author confidence="0.959696">Ulrich Heid</author>
<author confidence="0.959696">Jonathan</author>
<keyword confidence="0.312196333333333">Kilgour, Judy Robertson, and Holger Voormann. NITE XML Toolkit: Flexible Annotation Multi-modal Language Behavior Research</keyword>
<note confidence="0.858612636363636">Methods, Instruments, and Computers 35(3), 353- 363. Christian Chiarcos, Stefanie Dipper, Michael G¨otze, Ulf Leser, Anke L¨udeling, Julia Ritz, and Manfred 2009. Flexible Framework for Integrating from Different Tools and Tagsets (Traitement automatique des langues) 49(2). Christ. 1994. modular and flexible architecfor an integrated corpus query COM- PLEX’94, Budapest, Hungary. Dipper. 2005. Stand-off Repre-</note>
<title confidence="0.685202">sentation and Exploitation ofMulti-Level Linguistic</title>
<author confidence="0.685064">In Rainer Eckstein</author>
<author confidence="0.685064">Robert Tolks-</author>
<note confidence="0.856409666666667">dorf (eds:): Proceedings of Berliner XML Tage, pages 39-50. Dipper and Michael G¨otze. 2005.</note>
<title confidence="0.871529">Heterogeneous Linguistic Data — Generic XML- Representation and Flexible In</title>
<note confidence="0.781785375">Proceedings of the 2nd Language &amp; Technology Conference 2005, Poznan, Poland, pages 23–30. Stefanie Dipper, Michael G¨otze. 2006. ANNIS: Complex Multilevel Annotations in a Linguistic of the 5th Workshop on NLP and XML (NLPXML-2006): Multi-Dimensional in Natural Language Trento, Italy. Guo. 1997. Tokenization and its Proper- Computational Linguistic, 23(4), pp.569-596. ception would encode structural information on the structural level, and only linguistic annotation and metadata on the contents level. C. Henderson. 2000. DTD for Reference Key Annotation of EDT Entities and RDC Relations the ACE Evaluations 5.2.0, 2000/01/05),</note>
<web confidence="0.945625">http://projects.ldc.upenn.edu/ace/</web>
<note confidence="0.9161159">Ide and Keith Suderman. 2007. A Format for Linguistic In Proceedings of the Linguistic Annotation Workshop,held in conjunction with ACL 2007, Prague, June 28-29, 1-8. K¨onig and Wolfgang Lezius. 2000. descriplanguage for syntactically annotated In: Proceedings of the COLING Conference, pp. 1056-1060, Saarbr¨ucken, Germany. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 1993. a large annotated of English: the Penn treebank. Computational Linguistics 19, pp.313-330. M¨uller and Michael Strube. 2006. Multi- Annotation of Linguistic Data with In: S. Braun et al. (eds.), Corpus Technology and Language Pedagogy. New Resources, New Tools, New Methods. Frankfurt: Peter Lang, 197–214. Rehm, Oliver Schonefeld, Andreas Witt, Christian Chiarcos, and Timm Lehmberg.</note>
<title confidence="0.6487565">SPLICR: A Sustainability Platform for Linguistic and In: Text Resources and</title>
<author confidence="0.557493">Selected Papers the th Confer-</author>
<affiliation confidence="0.754753">ence on Natural Language Processing (KONVENS</affiliation>
<address confidence="0.833182">2008), Berlin, Sept. 30 – Oct. 2, 2008. Mouton de</address>
<note confidence="0.645700105263158">Gruyter. Schmid. 2002. &amp; In L¨udeling, Anke and Kyt¨o, Merja (Hrsg.) Corpus Linguistics. An International Handbook. (HSK Series). Mouton de Gryuter, Berlin Thomas Schmidt. 2004. Transcribing and Annotat- Spoken Language with Exmaralda. of the LREC-workshop on XML Based Richly Anno- Lisbon, Portugal. Paris: ELRA. Manfred Stede, Heike Bieler, Stefanie Dipper, and Arthit Suriyawongkul. 2006. SUMMaR: Combining Linguistics and Statistics for Text Summarizaof the 17th European Conference Artificial Intelligence pp 827-828. Riva del Garda, Italy. Ralph Weischedel, Sameer Pradhan, Lance Ramshaw and Linnea Micciulla. 2006. OntoNotes Release 1.0. Linguistic Data Consortium, Philadelphia. 43</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT A Statistical Part-ofSpeech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000.</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="6205" citStr="Brants, 2000" startWordPosition="923" endWordPosition="924">ccurrence-based) semantics and/or parsing; however, the resulting tokens might be argued as being less intuitive to users of a corpus search tool. (3) a. [Mitchell][’s], [they][’ve], [do][n’t] b. [wo][n’t], [ca][n’t], [ai][n’t] These examples show that different applications (tagging, parsing, information extraction) and the focus on different levels of description (morphology, syntax, semantics) require specialized tokenization strategies. When working with multiple 1Double surname consisting of Herzog and von der Heide. (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) When creating a corpus that is annotated at multiple levels and/or using several tools, different tokenizations are not always avoidable, as some tools (automatic NLP tools, but also tools for manual annotation) have integrated tokenizers. Another challenge is the representation of token boundaries. Commonly, token boundaries are represented by a line break (‘\n’) or the whitespace ‘character’ (‘ ’) – in which case token-internal whitespaces are replaced, usually by an underscore (’ ’) –, thereby corrupting the original data. This practice makes reconciling/merging the data a difficult </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT A Statistical Part-ofSpeech Tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<date>2007</date>
<booktitle>Reference Guide for the British National Corpus (XML Edition). http://www.natcorp.ox.ac.uk/ XMLedition/URG/bnctags.html.</booktitle>
<contexts>
<context position="7124" citStr="Burnard, 2007" startWordPosition="1064" endWordPosition="1065">s. Commonly, token boundaries are represented by a line break (‘\n’) or the whitespace ‘character’ (‘ ’) – in which case token-internal whitespaces are replaced, usually by an underscore (’ ’) –, thereby corrupting the original data. This practice makes reconciling/merging the data a difficult enterprise. Given this background, we suggest an XMLbased annotation of token boundaries, such that token boundaries are marked without affecting the original primary data. In a straightforward XML model, tokens are represented by XML elements enclosing primary text slices (c.f. the BNC encoding scheme (Burnard, 2007)). However, treating tokens as spans of text by means of the XML hierarchy is impossible for tokenization conflicts as in (4.a) and (4.b). 2 Conflicting tokenizations: Straightforward strategies By ‘straightforward strategies’, we mean approaches that aim to preserve the definition of tokens as atomic, minimal, unambiguous units of annotation when unifying different tokenizations (henceforth Tl and T2) of the same text. By ‘unsupervised straightforward strategies’, we mean tokenization strategies that operate on the primary data only, without consulting external resources such as dictionaries </context>
</contexts>
<marker>Burnard, 2007</marker>
<rawString>Lou Burnard (ed.). 2007. Reference Guide for the British National Corpus (XML Edition). http://www.natcorp.ox.ac.uk/ XMLedition/URG/bnctags.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Stefan Evert</author>
<author>Ulrich Heid</author>
<author>Jonathan Kilgour</author>
<author>Judy Robertson</author>
<author>Holger Voormann</author>
</authors>
<title>The NITE XML Toolkit: Flexible Annotation for Multi-modal Language Data.</title>
<date>2003</date>
<journal>Behavior Research Methods, Instruments, and Computers</journal>
<volume>35</volume>
<issue>3</issue>
<pages>353--363</pages>
<contexts>
<context position="2134" citStr="Carletta et al., 2003" startWordPosition="302" endWordPosition="305">constitute the basis for ‘word’ distance measurements. In many annotation tools and their corresponding formats, the order of tokens provides a timeline for the sequential order of structural elements (MMAX (M¨uller and Strube, 2006), GENAU (Rehm et al., 2009), GrAF (Ide and Suderman, 2007), TIGER XML (K¨onig and Lezius, 2000)). In several multi∗Taken from the poem September by Helen Hunt Jackson. layer formats, tokens also define the absolute position of annotation elements, and only by reference to a common token layer, annotations from different layers can be related with each other (NITE (Carletta et al., 2003), GENAU). Thus, by their function, tokens have the following characteristics: (i) tokens are totally ordered, (ii) tokens cover the full (annotated portion of the) primary data, (iii) tokens are the smallest unit of annotation, and (iv) there is only one single privileged token layer. The last aspect is especially relevant for the study of richly annotated data, as an integration and serialization of annotations produced by different tools can be established only by reference to the token layer. From a corpus-linguistic perspective, i.e., when focusing on querying of annotated corpora, tokens </context>
<context position="11842" citStr="Carletta et al., 2003" startWordPosition="1800" endWordPosition="1803">rmat that is capable to express conflicting hierarchies. For this purpose, we describe an extension of the PAULA format, a generic format for text-oriented linguistic annotations based on standoff XML. 3 Conflicting tokenizations in the PAULA format 3.1 Annotation structures in PAULA 1.0 The PAULA format (Dipper, 2005; Dipper and G¨otze, 2005) is a generic XML format, used as a pivot format in NLP pipelines (Stede et al., 2006) and in the web-based corpus interface ANNIS (Chiarcos et al., 2008). It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al., 2003) and GraF (Ide and Suderman, 2007). PAULA was specifically designed to support the lossless representation of different types of textoriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including conflicting hierarchies and simple addition/deletion routines for annotation layers. Therefore, primary data is stored in a separate 37 Table 2: PAULA 1.0 data types nodes (structural units of annotation) edges (relational units of annotation, connecting tokens, markables, structs) token character spans </context>
</contexts>
<marker>Carletta, Evert, Heid, Kilgour, Robertson, Voormann, 2003</marker>
<rawString>Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan Kilgour, Judy Robertson, and Holger Voormann. 2003. The NITE XML Toolkit: Flexible Annotation for Multi-modal Language Data. Behavior Research Methods, Instruments, and Computers 35(3), 353-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Chiarcos</author>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
<author>Ulf Leser</author>
<author>Anke L¨udeling</author>
<author>Julia Ritz</author>
<author>Manfred Stede</author>
</authors>
<title>A Flexible Framework for Integrating Annotations from Different Tools and Tagsets TAL (Traitement automatique des langues) 49(2).</title>
<date>2009</date>
<marker>Chiarcos, Dipper, G¨otze, Leser, L¨udeling, Ritz, Stede, 2009</marker>
<rawString>Christian Chiarcos, Stefanie Dipper, Michael G¨otze, Ulf Leser, Anke L¨udeling, Julia Ritz, and Manfred Stede. 2009. A Flexible Framework for Integrating Annotations from Different Tools and Tagsets TAL (Traitement automatique des langues) 49(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oli Christ</author>
</authors>
<title>A modular and flexible architecture for an integrated corpus query system. COMPLEX’94,</title>
<date>1994</date>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1497" citStr="Christ, 1994" startWordPosition="203" endWordPosition="204">tic annotations, especially those concerned with syntax (part-ofspeech tagging, chunking, parsing) and the interpretation of syntactic structures (esp., the extraction of semantic information), tokens represent the minimal unit of analysis: words (lexemes, semantic units, partly morphemes) on the one hand and certain punctuation symbols on the other hand. From a corpus-linguistic perspective, tokens also represent the minimal unit of investigation, the minimal character sequence that can be addressed in a corpus query (e.g. using search tools like TIGERSearch (K¨onig and Lezius, 2000) or CWB (Christ, 1994)). Tokens also constitute the basis for ‘word’ distance measurements. In many annotation tools and their corresponding formats, the order of tokens provides a timeline for the sequential order of structural elements (MMAX (M¨uller and Strube, 2006), GENAU (Rehm et al., 2009), GrAF (Ide and Suderman, 2007), TIGER XML (K¨onig and Lezius, 2000)). In several multi∗Taken from the poem September by Helen Hunt Jackson. layer formats, tokens also define the absolute position of annotation elements, and only by reference to a common token layer, annotations from different layers can be related with eac</context>
</contexts>
<marker>Christ, 1994</marker>
<rawString>Oli Christ. 1994. A modular and flexible architecture for an integrated corpus query system. COMPLEX’94, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
</authors>
<title>XML-based Stand-off Representation and Exploitation ofMulti-Level Linguistic Annotation.</title>
<date>2005</date>
<booktitle>In Rainer Eckstein and Robert Tolksdorf (eds:): Proceedings of Berliner XML Tage,</booktitle>
<pages>39--50</pages>
<contexts>
<context position="11539" citStr="Dipper, 2005" startWordPosition="1750" endWordPosition="1751"> defined according to the maximum common substrings strategy described above. Then, tokens are defined as nodes that span over a certain range of terms similar to phrase nodes that dominate other nodes in syntax annotations. The representation of conflicting tokenizations, then, requires a format that is capable to express conflicting hierarchies. For this purpose, we describe an extension of the PAULA format, a generic format for text-oriented linguistic annotations based on standoff XML. 3 Conflicting tokenizations in the PAULA format 3.1 Annotation structures in PAULA 1.0 The PAULA format (Dipper, 2005; Dipper and G¨otze, 2005) is a generic XML format, used as a pivot format in NLP pipelines (Stede et al., 2006) and in the web-based corpus interface ANNIS (Chiarcos et al., 2008). It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al., 2003) and GraF (Ide and Suderman, 2007). PAULA was specifically designed to support the lossless representation of different types of textoriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including co</context>
</contexts>
<marker>Dipper, 2005</marker>
<rawString>Stefanie Dipper. 2005. XML-based Stand-off Representation and Exploitation ofMulti-Level Linguistic Annotation. In Rainer Eckstein and Robert Tolksdorf (eds:): Proceedings of Berliner XML Tage, pages 39-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
</authors>
<title>Accessing Heterogeneous Linguistic Data — Generic XMLbased Representation and Flexible Visualization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Language &amp; Technology Conference</booktitle>
<pages>23--30</pages>
<location>Poznan, Poland,</location>
<marker>Dipper, G¨otze, 2005</marker>
<rawString>Stefanie Dipper and Michael G¨otze. 2005. Accessing Heterogeneous Linguistic Data — Generic XMLbased Representation and Flexible Visualization. In Proceedings of the 2nd Language &amp; Technology Conference 2005, Poznan, Poland, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
</authors>
<title>ANNIS: Complex Multilevel Annotations in a Linguistic Database.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th Workshop on NLP and XML (NLPXML-2006): Multi-Dimensional Markup in Natural Language Processing.</booktitle>
<location>Trento, Italy.</location>
<marker>Dipper, G¨otze, 2006</marker>
<rawString>Stefanie Dipper, Michael G¨otze. 2006. ANNIS: Complex Multilevel Annotations in a Linguistic Database. Proceedings of the 5th Workshop on NLP and XML (NLPXML-2006): Multi-Dimensional Markup in Natural Language Processing. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Guo</author>
</authors>
<title>Critical Tokenization and its Properties,</title>
<date>1997</date>
<journal>Computational Linguistic,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>569--596</pages>
<contexts>
<context position="4379" citStr="Guo 1997" startWordPosition="659" endWordPosition="660"> order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. tools for standard NLP tasks, thus, it is the norm rather than the exception that they disagree in their tokenization, as shown in ex. (4). 1.2 Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzog-von der Heide1 c. Red Cross/Red Crescent movement Similarly, examples (2a) and (2b) can be argued to be treated as one token for (morpho)syntactic analyse</context>
</contexts>
<marker>Guo, 1997</marker>
<rawString>Jin Guo. 1997. Critical Tokenization and its Properties, Computational Linguistic, 23(4), pp.569-596.</rawString>
</citation>
<citation valid="false">
<title>ception would encode structural information on the structural level, and only linguistic annotation and metadata on the contents level.</title>
<marker></marker>
<rawString>ception would encode structural information on the structural level, and only linguistic annotation and metadata on the contents level.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
</authors>
<title>A DTD for Reference Key Annotation</title>
<date>2000</date>
<booktitle>of EDT Entities and RDC Relations in the ACE Evaluations (v. 5.2.0, 2000/01/05), http://projects.ldc.upenn.edu/ace/ annotation/apf.v5.2.0.dtd</booktitle>
<pages>2009--06</pages>
<contexts>
<context position="29012" citStr="Henderson 2000" startWordPosition="4630" endWordPosition="4631">mat: Tokens are defined as structured units that dominate one or more terminal nodes. Terminals are atomic units only within the respective annotation project (there is no unit addressed that is smaller than a terminal). By iterative applications of the merging algorithm, however, complex terms may be split up in smaller units, so that they are not atomic in an absolute sense. Alternatively, terms could be identified a priori with the minimal addressable unit available, i.e., characters (as in the formalization of tokens as charspans and charseqs in the ACE information extraction annotations, Henderson 2000). It is not clear, however, how a character-based term definition would deal with sub-character and zero extension terms: A character-based definition of terms that represent traces is possible only by corrupting the primary data.7 Consequently, a characterbased term definition is insufficient unless we restrict ourselves to a particular class of languages, texts and phenomena. The role of terminals can thus be compared to timestamps: With reference to a numerical timeline, it is always possible to define a new event between two existing timestamps. Formats specifically designed for time-align</context>
</contexts>
<marker>Henderson, 2000</marker>
<rawString>John C. Henderson. 2000. A DTD for Reference Key Annotation of EDT Entities and RDC Relations in the ACE Evaluations (v. 5.2.0, 2000/01/05), http://projects.ldc.upenn.edu/ace/ annotation/apf.v5.2.0.dtd (2009/06/04)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>GrAF: A Graph-based Format for Linguistic Annotations.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,held in conjunction with ACL 2007,</booktitle>
<pages>1--8</pages>
<location>Prague,</location>
<contexts>
<context position="1803" citStr="Ide and Suderman, 2007" startWordPosition="247" endWordPosition="250">e one hand and certain punctuation symbols on the other hand. From a corpus-linguistic perspective, tokens also represent the minimal unit of investigation, the minimal character sequence that can be addressed in a corpus query (e.g. using search tools like TIGERSearch (K¨onig and Lezius, 2000) or CWB (Christ, 1994)). Tokens also constitute the basis for ‘word’ distance measurements. In many annotation tools and their corresponding formats, the order of tokens provides a timeline for the sequential order of structural elements (MMAX (M¨uller and Strube, 2006), GENAU (Rehm et al., 2009), GrAF (Ide and Suderman, 2007), TIGER XML (K¨onig and Lezius, 2000)). In several multi∗Taken from the poem September by Helen Hunt Jackson. layer formats, tokens also define the absolute position of annotation elements, and only by reference to a common token layer, annotations from different layers can be related with each other (NITE (Carletta et al., 2003), GENAU). Thus, by their function, tokens have the following characteristics: (i) tokens are totally ordered, (ii) tokens cover the full (annotated portion of the) primary data, (iii) tokens are the smallest unit of annotation, and (iv) there is only one single privile</context>
<context position="11876" citStr="Ide and Suderman, 2007" startWordPosition="1806" endWordPosition="1809">onflicting hierarchies. For this purpose, we describe an extension of the PAULA format, a generic format for text-oriented linguistic annotations based on standoff XML. 3 Conflicting tokenizations in the PAULA format 3.1 Annotation structures in PAULA 1.0 The PAULA format (Dipper, 2005; Dipper and G¨otze, 2005) is a generic XML format, used as a pivot format in NLP pipelines (Stede et al., 2006) and in the web-based corpus interface ANNIS (Chiarcos et al., 2008). It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al., 2003) and GraF (Ide and Suderman, 2007). PAULA was specifically designed to support the lossless representation of different types of textoriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including conflicting hierarchies and simple addition/deletion routines for annotation layers. Therefore, primary data is stored in a separate 37 Table 2: PAULA 1.0 data types nodes (structural units of annotation) edges (relational units of annotation, connecting tokens, markables, structs) token character spans in the primary data that form the </context>
<context position="29913" citStr="Ide and Suderman, 2007" startWordPosition="4769" endWordPosition="4772"> is insufficient unless we restrict ourselves to a particular class of languages, texts and phenomena. The role of terminals can thus be compared to timestamps: With reference to a numerical timeline, it is always possible to define a new event between two existing timestamps. Formats specifically designed for time-aligned annotations, e.g., EXMARaLDA (Schmidt, 2004), however, typically lack a privileged token layer and a formal concept of tokens. Instead, tokens, as well as longer or shorter sequences, are represented as markables, defined by their extension on the timeline. Similarly, GrAF (Ide and Suderman, 2007), although being historically related to PAULA, does not have a formal concept of a privileged token layer in the sense of PAULA.8 We do, however, assume that terminal nodes in GrAF can be compared to PAULA 1.0 tokens. For conflicting tokenizations, Ide and Suderman (2007) suggest that ‘dummy’ elements are defined covering all necessary tokenizations for controversially tokenized stretches of primary data. Such dummy elements combine the possible tokenizations for strategies 1 (no merging) and 3 (maximal tokens), so that the information preservation deficit of strategy 3 is compensated by stra</context>
</contexts>
<marker>Ide, Suderman, 2007</marker>
<rawString>Nancy Ide and Keith Suderman. 2007. GrAF: A Graph-based Format for Linguistic Annotations. In Proceedings of the Linguistic Annotation Workshop,held in conjunction with ACL 2007, Prague, June 28-29, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther K¨onig</author>
<author>Wolfgang Lezius</author>
</authors>
<title>A description language for syntactically annotated corpora. In:</title>
<date>2000</date>
<booktitle>Proceedings of the COLING Conference,</booktitle>
<pages>1056--1060</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>K¨onig, Lezius, 2000</marker>
<rawString>Esther K¨onig and Wolfgang Lezius. 2000. A description language for syntactically annotated corpora. In: Proceedings of the COLING Conference, pp. 1056-1060, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="5479" citStr="Marcus et al., 1993" startWordPosition="823" endWordPosition="826">scent movement Similarly, examples (2a) and (2b) can be argued to be treated as one token for (morpho)syntactic analyses, respectively. Despite intervening whitespaces and punctuation symbols, they are complex instances of the ‘classical’ partof-speech adjective. For certain semantic analyses such as in information extraction, however, it may be useful to split these compounds in order to access the inherent complements (E 605, No. 22). (2) a. E 605-intoxicated b. No. 22-rated Finally, (3) illustrates a morphology-based tokenization strategy: the principle of splitting at morpheme boundaries (Marcus et al., 1993, PTB) (token boundaries represented by square brackets). Morphological tokenization may help distributional (co-occurrence-based) semantics and/or parsing; however, the resulting tokens might be argued as being less intuitive to users of a corpus search tool. (3) a. [Mitchell][’s], [they][’ve], [do][n’t] b. [wo][n’t], [ca][n’t], [ai][n’t] These examples show that different applications (tagging, parsing, information extraction) and the focus on different levels of description (morphology, syntax, semantics) require specialized tokenization strategies. When working with multiple 1Double surnam</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics 19, pp.313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>MultiLevel Annotation of Linguistic Data with MMAX2.</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy. New Resources,</booktitle>
<editor>In: S. Braun et al. (eds.),</editor>
<location>New Tools, New Methods. Frankfurt: Peter Lang,</location>
<marker>M¨uller, Strube, 2006</marker>
<rawString>Christoph M¨uller and Michael Strube. 2006. MultiLevel Annotation of Linguistic Data with MMAX2. In: S. Braun et al. (eds.), Corpus Technology and Language Pedagogy. New Resources, New Tools, New Methods. Frankfurt: Peter Lang, 197–214.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Georg Rehm</author>
<author>Oliver Schonefeld</author>
</authors>
<location>Andreas Witt, Chris-</location>
<marker>Rehm, Schonefeld, </marker>
<rawString>Georg Rehm, Oliver Schonefeld, Andreas Witt, Chris-</rawString>
</citation>
<citation valid="true">
<authors>
<author>tian Chiarcos</author>
<author>Timm Lehmberg</author>
</authors>
<title>SPLICR: A Sustainability Platform for Linguistic Corpora and Resources. In:</title>
<date>2009</date>
<booktitle>Text Resources and Lexical Knowledge. Selected Papers the 9th Conference on Natural Language Processing (KONVENS 2008),</booktitle>
<volume>30</volume>
<location>Berlin,</location>
<note>Mouton de Gruyter.</note>
<marker>Chiarcos, Lehmberg, 2009</marker>
<rawString>tian Chiarcos, and Timm Lehmberg. 2009. SPLICR: A Sustainability Platform for Linguistic Corpora and Resources. In: Text Resources and Lexical Knowledge. Selected Papers the 9th Conference on Natural Language Processing (KONVENS 2008), Berlin, Sept. 30 – Oct. 2, 2008. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Tokenizing &amp; Tagging. In L¨udeling, Anke and Kyt¨o, Merja (Hrsg.) Corpus Linguistics. An International Handbook. (HSK Series). Mouton de Gryuter,</title>
<date>2002</date>
<location>Berlin</location>
<marker>Schmid, 2002</marker>
<rawString>Helmut Schmid. 2002. Tokenizing &amp; Tagging. In L¨udeling, Anke and Kyt¨o, Merja (Hrsg.) Corpus Linguistics. An International Handbook. (HSK Series). Mouton de Gryuter, Berlin</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schmidt</author>
</authors>
<title>Transcribing and Annotating Spoken Language with Exmaralda.</title>
<date>2004</date>
<booktitle>Proceedings of the LREC-workshop on XML Based Richly Annotated Corpora.</booktitle>
<publisher>ELRA.</publisher>
<location>Lisbon, Portugal. Paris:</location>
<contexts>
<context position="29659" citStr="Schmidt, 2004" startWordPosition="4730" endWordPosition="4731"> character-based term definition would deal with sub-character and zero extension terms: A character-based definition of terms that represent traces is possible only by corrupting the primary data.7 Consequently, a characterbased term definition is insufficient unless we restrict ourselves to a particular class of languages, texts and phenomena. The role of terminals can thus be compared to timestamps: With reference to a numerical timeline, it is always possible to define a new event between two existing timestamps. Formats specifically designed for time-aligned annotations, e.g., EXMARaLDA (Schmidt, 2004), however, typically lack a privileged token layer and a formal concept of tokens. Instead, tokens, as well as longer or shorter sequences, are represented as markables, defined by their extension on the timeline. Similarly, GrAF (Ide and Suderman, 2007), although being historically related to PAULA, does not have a formal concept of a privileged token layer in the sense of PAULA.8 We do, however, assume that terminal nodes in GrAF can be compared to PAULA 1.0 tokens. For conflicting tokenizations, Ide and Suderman (2007) suggest that ‘dummy’ elements are defined covering all necessary tokeniz</context>
</contexts>
<marker>Schmidt, 2004</marker>
<rawString>Thomas Schmidt. 2004. Transcribing and Annotating Spoken Language with Exmaralda. Proceedings of the LREC-workshop on XML Based Richly Annotated Corpora. Lisbon, Portugal. Paris: ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
<author>Heike Bieler</author>
<author>Stefanie Dipper</author>
<author>Arthit Suriyawongkul</author>
</authors>
<title>SUMMaR: Combining Linguistics and Statistics for Text Summarization.</title>
<date>2006</date>
<booktitle>Proceedings of the 17th European Conference on Artificial Intelligence (ECAI-06).</booktitle>
<pages>827--828</pages>
<location>Riva del Garda, Italy.</location>
<contexts>
<context position="11651" citStr="Stede et al., 2006" startWordPosition="1769" endWordPosition="1772"> nodes that span over a certain range of terms similar to phrase nodes that dominate other nodes in syntax annotations. The representation of conflicting tokenizations, then, requires a format that is capable to express conflicting hierarchies. For this purpose, we describe an extension of the PAULA format, a generic format for text-oriented linguistic annotations based on standoff XML. 3 Conflicting tokenizations in the PAULA format 3.1 Annotation structures in PAULA 1.0 The PAULA format (Dipper, 2005; Dipper and G¨otze, 2005) is a generic XML format, used as a pivot format in NLP pipelines (Stede et al., 2006) and in the web-based corpus interface ANNIS (Chiarcos et al., 2008). It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al., 2003) and GraF (Ide and Suderman, 2007). PAULA was specifically designed to support the lossless representation of different types of textoriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including conflicting hierarchies and simple addition/deletion routines for annotation layers. Therefore, primary data is st</context>
</contexts>
<marker>Stede, Bieler, Dipper, Suriyawongkul, 2006</marker>
<rawString>Manfred Stede, Heike Bieler, Stefanie Dipper, and Arthit Suriyawongkul. 2006. SUMMaR: Combining Linguistics and Statistics for Text Summarization. Proceedings of the 17th European Conference on Artificial Intelligence (ECAI-06). pp 827-828. Riva del Garda, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Linnea Micciulla</author>
</authors>
<title>OntoNotes Release 1.0. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia.</location>
<marker>Weischedel, Pradhan, Ramshaw, Micciulla, 2006</marker>
<rawString>Ralph Weischedel, Sameer Pradhan, Lance Ramshaw and Linnea Micciulla. 2006. OntoNotes Release 1.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>