<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.9641625">
An Efficient Algorithm to Induce Minimum Average Lookahead Grammars
for Incremental LR Parsing
</title>
<author confidence="0.735414">
Dekai WU1 Yihai SHEN
</author>
<email confidence="0.725327">
dekai@cs.ust.hk shenyh@cs.ust.hk
</email>
<note confidence="0.300248">
Human Language Technology Center
</note>
<author confidence="0.319469">
HKUST
</author>
<affiliation confidence="0.998311">
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
</affiliation>
<sectionHeader confidence="0.979977" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999235">
We define a new learning task, minimum average
lookahead grammar induction, with strong poten-
tial implications for incremental parsing in NLP and
cognitive models. Our thesis is that a suitable learn-
ing bias for grammar induction is to minimize the
degree of lookahead required, on the underlying
tenet that language evolution drove grammars to be
efficiently parsable in incremental fashion. The in-
put to the task is an unannotated corpus, plus a non-
deterministic constraining grammar that serves as
an abstract model of environmental constraints con-
firming or rejecting potential parses. The constrain-
ing grammar typically allows ambiguity and is it-
self poorly suited for an incremental parsing model,
since it gives rise to a high degree of nondetermin-
ism in parsing. The learning task, then, is to in-
duce a deterministic LR (k) grammar under which
it is possible to incrementally construct one of the
correct parses for each sentence in the corpus, such
that the average degree of lookahead needed to do
so is minimized. This is a significantly more dif-
ficult optimization problem than merely compiling
LR (k) grammars, since k is not specified in ad-
vance. Clearly, naive approaches to this optimiza-
tion can easily be computationally infeasible. How-
ever, by making combined use of GLR ancestor ta-
bles and incremental LR table construction meth-
ods, we obtain an O(n3 + 2m) greedy approxima-
tion algorithm for this task that is quite efficient in
practice.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9888402">
Marcus’ (1980) Determinism Hypothesis proposed
that natural language can be parsed by a mechanism
that operates “strictly deterministically” in that it
does not simulate a nondeterministic machine. Al-
though the structural details of the deterministic LR
</bodyText>
<footnote confidence="0.993184">
1The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
</footnote>
<bodyText confidence="0.996745326923077">
parsing model we employ in this paper diverge from
those of Marcus, fundamentally we adhere to his
constraints that (1) all syntactic substructures cre-
ated are permanent, which prohibits simulating de-
terminism by backtracking, (2) all syntactic sub-
structures created for a given input must be part of
the output structure assigned to that input, which
prohibits memoizing intermediate results as in dy-
namic programming or beam search, and (3) no
temporary syntactic structures are encoded within
the internal state of the machine, which prohibits
the moving of temporary structures into procedural
codes.
A key issue is that, to give the Determinism Hy-
pothesis teeth, it is necessary to limit the size of the
decision window. Otherwise, it is always possible
to circumvent the constraints simply by increasing
the degree of lookahead or, equivalently, increasing
the buffer size (which we might call the degree of
“look-behind”); either way, increasing the decision
window essentially delays decisions until enough
disambiguating information is seen. In the limit, a
decision window equal to the sentence length ren-
ders the claim of incremental parsing meaningless.
Marcus simply postulated that a maximum buffer
size of three was sufficient. In contrast, our ap-
proach permits greater flexibility and finer grada-
tions, where the average degree of lookahead re-
quired can be minimized with the aim of assisting
grammar induction.
Since Marcus’ work, a significant body of work
on incremental parsing has developed in the sen-
tence processing community, but much of this work
has actually suggested models with an increased
amount of nondeterminism, often with probabilistic
weights (e.g., Narayanan &amp; Jurafsky (1998); Hale
(2001)).
Meanwhile, in the way of formal methods,
Tomita (1986) introduced Generalized LR parsing,
which offers an interesting hybrid of nondetermin-
istic dynamic programming surrounding LR parsing
methods that were originally deterministic.
Additionally, methods for determinizing and
minimizing finite-state machines are well known
(e.g., Mohri (2000), B al &amp; Carton (1968)). How-
ever, such methods (a) do not operate at the context-
free level, (b) do not directly minimize lookahead,
and (c) do not induce grammars under environmen-
tal constraints.
Unfortunately, there has still been relatively lit-
tle work on automatic learning of grammars for de-
terministic parsers to date. Hermjakob &amp; Mooney
(1997) describe a semi-automatic procedure for
learning a deterministic parser from a treebank,
which requires the intervention of a human expert
in the loop to determine appropriate derivation or-
der, to resolve parsing conflicts between certain ac-
tions such as “merge” and “add-into”, and to iden-
tify specific features for disambiguating actions. In
our earlier work we described a deterministic parser
with a fully automatically learned decision algo-
rithm (Wong and Wu, 1999). But unlike our present
work, the decision algorithms in both Hermjakob &amp;
Mooney (1997) and Wong &amp; Wu (1999) are pro-
cedural; there is no explicit representation of the
grammar that can be meaningfully inspected.
Finally, we observe that there are also trainable
stochastic shift-reduce parser models (Briscoe and
Carroll, 1993), which are theoretically related to
shift-reduce parsing, but operate in a highly nonde-
terministic fashion during parsing.
We believe the shortage of learning models for
deterministic parsing is in no small part due to the
difficulty of overcoming computational complexity
barriers in the optimization problems this would in-
volve. Many types of factors need to be optimized
in learning, because deterministic parsing is much
more sensitive to incorrect choice of structural fea-
tures (e.g., categories, rules) than nondeterministic
parsing that employ robustness mechanisms such as
weighted charts.
Consequently, we suggest shifting attention to the
development of new methods that directly address
the problem of optimizing criteria associated with
deterministic parsing, in computationally feasible
ways. In particular, we aim in this paper to develop
a method that efficiently searches for a parser under
a minimum average lookahead cost function.
It should be emphasized that we view the role of
a deterministic parser as one component in a larger
model. A deterministic parsing stage can be ex-
pected to handle most input sentences, but not all.
Other nondeterministic mechanisms will clearly be
needed to handle a minority of cases, the most ob-
vious being garden-path sentences.
In the sections that follow, we first formalize the
learning problem. We then describe an efficient ap-
proximate algorithm for this task. The operation of
this algorithm is illustrated with an example. Fi-
nally, we give an analysis of the complexity charac-
teristics of the algorithm.
2 The minimum average lookahead
(MAL) grammar problem
We formulate the learning task as follows:
</bodyText>
<equation confidence="0.737123">
Definition Given an unannotated corpus
S = {S1, ... , S|S|} plus a constraining gram-
mar GC, the minimum average lookahead grammar
GMAL(S, GC) is defined as
arg minGCGCnbi:parseG(Si)�=0 ˆk (G)
</equation>
<bodyText confidence="0.993579921568627">
where the average lookahead objective function
kˆ (G) is the average (over S) amount of lookahead
that an LR parser for G needs in order to determinis-
tically parse the sample S without any shift-reduce
or reduce-reduce conflicts. If G is ambiguous in the
sense that it generates more than one parse for any
sentence in S, then kˆ (G) = ∞ since a conflict is
unavoidable no matter how much lookahead is used.
In other words, GMAL is the subset of rules of
GC that requires the smallest number of lookaheads
on average so as to make parsing S using this subset
of G deterministic.
Note that the constraining grammar GC by na-
ture is not deterministic. The constraining gram-
mar serves merely as an abstract model of envi-
ronmental constraints that confirm or reject poten-
tial parses. Since such feedback is typically quite
permissive, the constraining grammar typically al-
lows a great deal of ambiguity. This of course ren-
ders the constraining grammar itself poorly suited
for an incremental parsing model, since it gives rise
to a high degree of nondeterminism in parsing. In
other words, we should not expect the constraining
grammar alone to contain sufficient information to
choose a deterministically parsable grammar.
For expository simplicity we assume all gram-
mars are in standard context-free form in the dis-
cussion that follows, although numerous notational
variations, generalizations, and restricted cases are
certainly possible. We note also that, although the
formalization is couched in terms of standard syn-
tactic phrase structures, there is no reason why one
could not employ categories and/or attributes on
parse nodes representing semantic features. Do-
ing so would permit the framework to accommodate
some semantic information in minimizing looka-
head for deterministic parsing, which would be
more realistic from a cognitive modeling stand-
point. (Of course, further extensions to integrate
more complex incremental semantic interpretation
mechanisms into this framework could be explored
as well.)
Finding the minimum average lookahead gram-
mar is clearly a difficult optimization problem. To
compute the value of kˆ (G), one needs the LR table
for that particular G, which is expensive to compute.
Computing the LR table for all G ⊂ GC would be
infeasible. It is a natural conjecture, in fact, that
the problem of learning MAL grammars is NP-hard.
We therefore seek an approximation algorithm with
good performance, as discussed next.
</bodyText>
<sectionHeader confidence="0.710277" genericHeader="method">
3 An efficient approximate algorithm for
</sectionHeader>
<subsectionHeader confidence="0.666263">
learning incremental MAL parsers
</subsectionHeader>
<bodyText confidence="0.999918857142857">
We now describe an approximate method for effi-
ciently learning a MAL grammar. During learning,
the MAL grammar is represented simultaneously as
both a set of standard production rules as well as an
LR parsing table. Thus the learning algorithm out-
puts an explicit declarative rule set together with a
corresponding compiled LR table.
</bodyText>
<subsectionHeader confidence="0.999872">
3.1 Approximating assumptions
</subsectionHeader>
<bodyText confidence="0.999624">
To overcome the obstacles mentioned in the previ-
ous section, we make the following approximations:
</bodyText>
<listItem confidence="0.995052111111111">
1. Incremental approximation for MAL rule set
computation. We assume that the MAL gram-
mar for a given corpus is approximately equal
to the sequential union of the MAL grammar
rules for each sentence in the corpus, where the
set of MAL grammar rules for each sentence is
determined relative to the set of all rules se-
lected from preceding sentences in the corpus.
2. Incremental approximation for LR state set
</listItem>
<bodyText confidence="0.959466411764706">
computation. We assume that the correct set
of LR states for a given set of rules is approx-
imately equal to that obtained by incremen-
tally modifying the LR table and states from
a slightly smaller subset of the rules. (Our ap-
proach exploits the fact that the correct set of
states for LR (k) parsers is always independent
of k.)
Combining these approximation assumptions en-
ables us to utilize a sentence-by-sentence greedy ap-
proach to seeking a MAL grammar. Specifically,
the algorithm iteratively computes a minimum aver-
age lookahead set of rules for each sentence in the
training corpus, accumulating all rules found, while
keeping the LR state set and table updated. The full
algorithm is fairly complex; we discuss its key as-
pects here.
</bodyText>
<subsectionHeader confidence="0.999795">
3.2 Structure of the iteration
</subsectionHeader>
<bodyText confidence="0.999982808510638">
As shown in Figure 1, find MAL parser accepts
as input an unannotated corpus S = {S,,..., S|S|}
plus a constraining grammar GC , and outputs the
LR table for a parser that can deterministically parse
the entire training corpus using a minimum average
lookahead.
The algorithm consists of an initialization step
followed by an iterative loop. In the initialization
step in lines 1–3, we create an empty LR table T,
along with an empty set A of parsing action se-
quences defined as follows. A parsing action se-
quence A (P) for a given parse P is the sequence
of triples (state, input, action) that a shift-reduce
parser follows in order to construct P. At any given
point, T will hold the LR table computed from the
MAL parse of all sentences already processed, and
A will hold the corresponding parsing sequences for
the MAL parses.
Entering the loop, we iteratively augment T by
adding the states arising from the MAL parse F ∗ of
each successive sentence in the training corpus and,
in addition, cache the corresponding parsing action
sequence A (F∗) into the set A. This is done by
first computing a chart for the sentence, in line 4, by
parsing Si under the constraining grammar GC us-
ing the standard Earley (1970) procedure. We then
call find MAL parse in line 5, to compute the parse
that requires minimum average lookahead to resolve
ambiguity. The items and states produced by the
rules in F∗are added to the LR table T by calling
incremental update LR in line 6, and the parsing
action sequence of F∗ is appended to A in line 7.
Note that the indices of the original states in T are
not changed and only items are added into them if
need be so that A is not changed by adding items and
states to T, and there might be new states introduced
which are also indexed.
By definition, the true MAL grammar does not
depend on the order of the sentences the learning al-
gorithm inspects. However, find MAL parser pro-
cesses the example sentences in order, and attempts
to find the MAL grammar sentence by sentence.
The order of the sentences impacts the grammar
produced by the learning algorithm, so it is not guar-
anteed to find the true MAL grammar. However the
approximation is well motivated particularly when
we have large numbers of example sentences.
</bodyText>
<subsectionHeader confidence="0.920438">
3.3 Incrementally updating the LR table
</subsectionHeader>
<bodyText confidence="0.913113">
Given the structure of the loop, it can be seen that
efficient learning of the set of MAL rules cannot be
achieved without a component that can update the
LR table incrementally as each rule is added into the
find MAL parser(S, GC)
</bodyText>
<listItem confidence="0.986349">
1. T +— 0
2. A +— 0
3. i +— 0
4. C +— chart parse(Si, GC)
5. F* +— find MAL parse(C, A, R)
6. T +— incremental update LR(T, F*)
7. append(A, A(F* ))
8. if i &lt; ISI then i +— i + 1; goto 4
</listItem>
<figureCaption confidence="0.997971">
Figure 1: Main algorithm find MAL parser.
</figureCaption>
<bodyText confidence="0.999920652173913">
current MAL grammar. Otherwise, every time a rule
is found to be capable of reducing average looka-
head and therefore is added into the MAL gram-
mar, the LR table must be recomputed from scratch,
which is sufficiently time consuming as to be infea-
sible when trying to learn a MAL grammar with a
realistically large input corpus and/or constraining
grammar.
The incremental update LR function incre-
mentally updates the LR table in an efficient fash-
ion that avoids recomputing the entire table. The
inputs to incremental update LR are a pre-existing
LR table T, and a set of new rules R to be added.
This algorithm is derived from the incremental LR
parser generator algorithm ilalr and is relatively
complex; see Horspool (1988) for details. Histor-
ically, work on incremental parser generators first
concentrated on LL(1) parsers. Fischer (1980) was
first to describe a method for incrementally updat-
ing an LR(1) table. Heering et al.(1990) use the
principle of lazy evaluation to attack the same prob-
lem. Our design of incremental update LR is more
closely related to ilalr for the following reasons:
</bodyText>
<listItem confidence="0.993643538461539">
• ilalr has the property that when updating the
LR table to contain the newly added rules, it
does not change the index of each already ex-
isting state. This is important for our task as
the slightest change in the states might affect
significantly the parsing sequences of the sen-
tences that have already been processed.
• Although worst case complexity for ilalr is ex-
ponential in the number of rules in the gram-
mar, empirically it is quite efficient in practical
use. Heuristics are used to improve the speed
of the algorithm, and as we do not need to com-
pute lookahead sets, the speed of the algorithm
can be further improved.
compute average lookahead(r, A)
1. h +— lookahead(r, A)
2. if Iv1 = (i, s, a1, k1, r1, d1)
• then k = k1 = (m1, l1)
• else k = (0,oc)
3. // note v&apos; = (i&apos;, s&apos;, a&apos;, k&apos;, r&apos;, d&apos;) and k&apos; =
(m&apos;, l&apos;)
4. l&apos;&apos; = m&apos;l&apos;+h
m&apos;+1
5. if l”&lt; l
• then l = l&apos;&apos;
• elsem=m&apos;+1
</listItem>
<figureCaption confidence="0.997064">
Figure 2: Algorithm compute average lookahead.
</figureCaption>
<bodyText confidence="0.9999625">
The method is approximate, and may yield slight,
acceptable deviations from the optimal table. ilalr
is not an exact LR table generator in the sense that
it may create states that should not exist and may
miss some states that should exist. The algorithm
is based on the assumption that most states in the
original LR table occur with the same kernel items
in the updated LR table. Empirically, the assump-
tion is valid as the proportion of superfluous states
is typically only in the 0.1% to 1.3% range.
</bodyText>
<subsectionHeader confidence="0.8438665">
3.4 Finding minimum average lookahead
parses
</subsectionHeader>
<bodyText confidence="0.999322945454546">
The function find MAL parse selects the full parse
F* of a given sentence that requires the least av-
erage lookahead kˆ (A( F )) to resolve any shift-
reduce or reduce-reduce conflicts with a set A of
parsing action sequences, such that F* is a sub-
set of a chart C. The inputs to find MAL parse,
more specifically, are a chart C containing all the
partial parses in the input sentence, and the set A
containing the parsing action sequences of the MAL
parse of all sentences processed so far. The algo-
rithm operates by constructing a graph-structured
stack of the same form as in GLR parsing (Tomita,
1986)(Tomita and Ng, 1991) while simultaneously
computing the minimum average lookahead. Note
that Tomita’s original method for constructing the
graph-structured stack has exponential time com-
plexity O (nP+1), in which n and p are the length of
the sentence and the length of the longest rhs of any
rule in the grammar. As a result, Tomita’s algorithm
achieves O (n3) for grammars in Chomsky normal
form but is potentially exponential when produc-
tions are of unrestricted length, which in practice
is the case with most parsing problems. We fol-
low Kipps (1991) in modifying Tomita’s algorithm
to allow it to run in time proportional to n3 for
grammars with productions of arbitrary length. The
most time consuming part in Tomita’s algorithm is
when reduce actions are executed in which the an-
cestors of the current node have to be found in-
curring time complexity nρ. To avoid this we em-
ploy an ancestor table to keep the ancestors of each
node in the GLR forest which is updated dynam-
ically as the GLR forest is being constructed. This
modification brings down the time complexity of re-
duce actions to n2 in the worst case, and allows the
function build GLR forest to construct the graph-
structured stack in O (n3). Aside from constructing
the graph-structured stack, we compute the average
lookahead for each LR state transition taken during
the construction. Whenever there is a shift or re-
duce action in the algorithm, a new vertex for the
graph-structured stack is generated, and the func-
tion compute average lookahead is called to ascer-
tain the average lookahead of the new vertex. Fi-
nally, reconstruct MAL parse is called to recover
the full parse F* for the MAL parsing action se-
quence.
Figure 2 shows the com-
pute average lookahead function, which es-
timates the average lookahead of a vertex v
generated by an LR state transition r. To facilitate
computations involving average lookahead, we
use a 6-tuple (i, s, a, k, r, d) instead of the more
common triple form (i, s, a) to represent each
vertex in the graph-structured stack, where:
</bodyText>
<listItem confidence="0.9992606875">
• i: The index of the right side of the coverage
of the vertex. The vertices with the same right
side i will be kept in Ui.
• s: The state in which the vertex is in.
• a: The ancestor table of the vertex.
• k: The average lookahead information, in the
form of a pair (m, l) where l is the minimum
average lookahead of all paths leading from the
root to this vertex and m is the number of state
transitions in that MAL path.
• r: The parsing action that generates the ver-
tex along the path that needs minimum average
lookahead. r is a triple (d1, d2, f) denoting ap-
plying the action f on the vertex d1 to generate
the vertex d2.
• d: The unique index of the vertex.
</listItem>
<bodyText confidence="0.978523">
The inputs to compute average lookahead are an
LR state transition r = (d&apos;, d, f) taken in the con-
struction of the graph-structured stack where d&apos; and
</bodyText>
<tableCaption confidence="0.998733">
Table 1: Example constraining grammar.
</tableCaption>
<table confidence="0.998693636363637">
S — *NP VP
VP —* v NP
VP —* v PP
VP —* v
VP —* v p
VP —* v det
PP —* p NP
NP —* NP PP
NP —* n
NP —* det n
VP —* VP n
</table>
<bodyText confidence="0.999703185185185">
d are the index of vertices and f is an action, and the
set A containing the parsing action sequences of the
MAL parse of all sentences processed so far. Let
v = (i, s, a, k, r, d) be the new vertex with index d,
and let v&apos; = (i&apos;, s&apos;, a&apos;, k&apos;, r&apos;, d&apos;) be the vertex with
index d&apos;. The function proceeds by first computing
the lookahead needed to resolve conflicts between r
and A. Next we check whether v is a packed node
and initialize k in v; if not, k is initialized to (0, 0),
and otherwise it is copied from the packed node. We
then compute the average lookahead needed to go
from v&apos; to v and check whether it provides a more
economical way to resolve conflicts. The average
lookahead of a vertex v generated by applying an
action f on vertex v&apos; can be computed from k&apos; of v&apos;
and the lookahead needed to generate v from v&apos;. v
can be generated by applying different actions on
different vertices and k keeps the one that needs
minimum average lookahead and f keeps that ac-
tion.
Finally, the reconstruct MAL parse function
is called after construction of the entire graph-
structured stack is complete in order to recover the
full minimum average lookahead parse tree. We as-
sume the grammar has only one start symbol and
rebuild the parse tree from the state that is labelled
with the start symbol.
</bodyText>
<sectionHeader confidence="0.902348" genericHeader="method">
4 An example
</sectionHeader>
<bodyText confidence="0.9662244">
We now walk through a simplified example so as
to fix ideas and illustrate the operation of the algo-
rithm. Table 1 shows a simple constraining gram-
mar GC which we will use for this example.
Now consider the small training corpus:
</bodyText>
<listItem confidence="0.962966">
1. I did.
2. He went to Africa.
3. I bought a ticket.
</listItem>
<tableCaption confidence="0.9884535">
Table 2: LR state transitions and lookaheads for
sentence 1.
</tableCaption>
<table confidence="0.688002333333333">
[S [NP In ] [VP didv ] ] kˆ
(0, 1, sh, 0) (1, 2, re9, 0) (2, 4, sh, 0)
(4, 5, re4, 0) (5, 3, re1, 0) (3, acc) 0*
</table>
<bodyText confidence="0.998584512195122">
To begin with, find MAL parser considers sen-
tence 1. In this particular case, chart parse(S1, GC)
finds only one valid parse. The GLR forest is built,
giving the LR state transitions and parsing actions
shown in Table 2, where each tuple (d’, d, f, k)
gives the state prior to the action, the state resulting
from the action, the action, and the average looka-
head. Here compute average lookahead determines
that the average lookahead kˆ is 0. From this parse
tree, incremental update LR accepts rules (1), (4),
and (9) and updates the previously empty LR table
T.
Next, find MAL parser considers sentence 2.
Here, chart parse(S1, GC) finds two possible
parses, leading to the LR state transitions and pars-
ing actions shown in Table 3. This time, the average
lookahead calculation is sensitive to the what was
already entered into the LR table T during the pre-
vious step of processing sentence 1. For example,
in the first parse, the fourth transition (4, 6, sh, 1)
requires a lookahead of 1 in order to avoid a shift-
reduce conflict with (4, 5, re4, 0) from sentence 1.
The sixth transition (1, 9, re9, 2) requires a looka-
head of 2. It turns out that the first parse has an aver-
age lookahead of 0.20,while the second parse has an
average lookahead of 0.33. We thus prefer the first
parse tree, calling incremental update LR to further
update the LR table T using rules (3) and (7).
Finally, find MAL parser considers sentence
3. Again, chart parse(S1, GC) finds two possible
parses, leading this time to the LR state transi-
tions and parsing actions shown in Table 4. Vari-
ous lookaheads are again needed to avoid conflicts
with the existing rules in T. The first parse has an
average lookahead of 0.22, and is selected in pref-
erence to the second parse which has an average
lookahead of 0.33. From the first parse tree, in-
cremental update LR accepts rules (2) and (10) to
again update the LR table T.
Thus the final output MAL grammar, requiring a
lookahead of 1, is shown in Table 5.
</bodyText>
<tableCaption confidence="0.9977065">
Table 3: LR state transitions and lookaheads for
sentence 2.
</tableCaption>
<table confidence="0.999802555555556">
[S [NP Hen ] [VP wentv [PP top [NP African ] ] ] ] kˆ
(0, 1, sh, 0) (1, 2, re9, 0) (2, 4, sh, 0)
(4, 6, sh, 1) (6, 1, sh, 0) (1, 9, re9, 1)
(9, 7, re7, 0) (7, 5, re3, 0) (5, 3, re1, 0)
(3, acc) 2/10
[S [NP Hen ] [VP [VP wentv top ] African ] ]
(0, 1, sh, 0) (1, 2, re9, 0) (2, 4, sh, 0)
(4, 6, sh, 1) (6, 5, re5, 0) (5, 8, sh, 1)
(8, 5, re11, 0) (5, 3, re1, 1) (3, acc) 3/9
</table>
<tableCaption confidence="0.9952255">
Table 4: LR state transitions and lookaheads for
sentence 3.
</tableCaption>
<table confidence="0.99973625">
[S [NP In ] [VP boughtv [NP adet ticketn ] ] ] kˆ
(0, 1, sh, 0) (1, 2, re9, 1) (2, 4, sh, 0)
(4, 8, sh, 1) (8, 11, sh, 0) (11, 12, re10, 0)
(12, 5, re12, 0) (5, 3, re1, 0) (3, acc) 2/9
[S [NP In ] [VP [VP boughtv adet ] ticketn ] ]
(0, 1, sh, 0) (1, 2, re9, 1) (2, 4, sh, 0)
(4, 8, sh, 1) (8, 5, re6, 0) (5, 10, sh, 1)
(10, 5, re11, 0) (5, 3, re1, 0) (3, acc) 3/9
</table>
<tableCaption confidence="0.983">
Table 5: Output MAL grammar.
</tableCaption>
<table confidence="0.999709857142857">
S → NP VP
VP → v NP
VP → v PP
VP → v
(7) PP → p NP
NP → n
NP → det n
</table>
<sectionHeader confidence="0.916453" genericHeader="method">
5 Complexity analysis
</sectionHeader>
<subsectionHeader confidence="0.976197">
5.1 Time complexity
</subsectionHeader>
<bodyText confidence="0.999974142857143">
Since the algorithm executes each of its five main
steps once for each sentence in the corpus, the time
complexity of the algorithm is upper bounded by
the sum of the time complexities of those five steps.
Suppose n is the maximum length of any sentence
in the corpus, and m is the number of rules in the
grammar. Then for each of the five steps:
</bodyText>
<listItem confidence="0.697709">
1. chart parse is O W).
2. build GLR forest is O W). As discussed
</listItem>
<bodyText confidence="0.988444527777778">
previously, the use of an ancestor table allows
the graph-structured stack to be built in O(n3)
in the worst case.
3. compute average lookahead is O (n2). As
the number of lookaheads needed by each pars-
ing action is computed by comparing the pars-
ing action with the MAL parsing action se-
quences for all previous sentences, the time
complexity of this function depends on the
maximum length of any sentence that has al-
ready been processed, which is bounded by n.
The dynamic programming method used to lo-
cate the most economical parse in terms of av-
erage lookahead, described above, can be seen
to be quadratic in n.
4. reconstruct MAL parse is O (n2). This is
bounded by the number of LR state transi-
tions in each full parse of the sentence, which
is is O (n2). Note, however, that Tanaka et
al. (1992) propose an enhancement that can re-
construct the parse trees in time linear to n; this
is a direction for future improvement of our al-
gorithm.
5. incremental update LR is O (2m). As with
ilalr, theoretically the worst time complexity is
exponential in the number of rules in the exist-
ing grammar. However, various heuristics can
be employed to make the algorithm quicker,
and in practical experiments the algorithm is
quite fast and precise in producing LR tables,
particularly since m is very small relative to
|S|.
The time complexity of the algorithm for each
sentence is thus O (n3) + O (n3) + O (n2) +
O (n2) + O (2m) which is O(n3 + 2m). Given |S|
sentences in the corpus, the total training time is
</bodyText>
<equation confidence="0.626144">
O (( n3 + 2m) ·|S |).
</equation>
<subsectionHeader confidence="0.998356">
5.2 Space complexity
</subsectionHeader>
<bodyText confidence="0.999283333333333">
As with time complexity, an upper bound on the
space complexity can be obtained from the five
main steps:
</bodyText>
<listItem confidence="0.941714916666667">
1. chart parse is O (n3).
2. build GLR forest is O (n2). The space com-
plexity of both Tomita’s original algorithm and
the modified algorithm is n2.
3. compute average lookahead is O (n2). The
space usage of compute average lookahead di-
rectly corresponds to the dynamic program-
ming structure, like the time complexity.
4. reconstruct MAL parse is O (n). This is
bounded by the number of vertices in the
graph-structured stack, which is is O (n).
5. incremental update LR is O (2m). As with
</listItem>
<bodyText confidence="0.768637428571428">
time complexity, although the worst time com-
plexity is exponential in the number of rules in
the existing grammar, in practice this is not the
major bottleneck.
O �n3�+O �n2�+O �n2�+O (n)+O (2m) which
The space complexity of the algorithm is thus
is again O(n3 + 2m).
</bodyText>
<sectionHeader confidence="0.988253" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999353125">
We have defined a new grammar learning task based
on the concept of a minimum average lookahead
(MAL) objective criterion. This approach provides
an alternative direction for modeling of incremen-
tal parsing: it emphatically avoids increasing the
amount of nondeterminism in the parsing models,
as has been done in across a wide range of recent
models, including probabilized dynamic program-
ming parsers as well as GLR approaches. In con-
trast, the objective here is to learn completely de-
terministic parsers from unannotated corpora, with
loose environmental guidance from nondeterminis-
tic constraining grammars.
Within this context, we have presented a greedy
algorithm for the difficult task of learning approx-
imately MAL grammars for deterministic incre-
mental LR(k) parsers, with a time complexity of
O (( n3 + 2m) · |S |) and a space complexity of
O(n3 + 2m). This algorithm is efficient in prac-
tice, and thus enables a broad range of applications
where degree of lookahead serves as a grammar in-
duction bias.
Numerous future directions are suggested by this
model. One obvious line of work involves experi-
ments varying the types of corpora as well as the nu-
merous parameters within the MAL grammar learn-
ing algorithm, to test predictions against various
modeling criteria. More efficient algorithms and
heuristics could help further increase the applicabil-
ity of the model. In addition, the accuracy of the
model could be strengthened by reducing sensitiv-
ity to some of the approximating assumptions.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999720451612903">
Marie-Pierre Beal and Olivier Carton. Determiniza-
tion of transducers over finite and infinite words.
Theoretical Computer Science, 289(1), 1968.
Ted Briscoe and John Carroll. Generalised prob-
abilistic LR parsing for unification-based gram-
mars. Computational Linguistics, 19(1):25–60,
1993.
Jay Earley. An efficient context-free parsing algo-
rithm. Communications of the Association for
Computing Machinery, 13(2), 1970.
G. Fischer. Incremental LR(1) parser construction
as an aid to syntactical extensibility. Technical
report, Department of Computer Science, Univer-
sity of Dortmund, Federal Republic of Germany,
1980. PhD Dissertation, Tech. Report 102.
John Hale. A probabilistic Earley parser as a psy-
cholinguistic model. In NAACL-2001: Second
Meeting of the North American Chapter of the
Association for Computational Linguistics, 2001.
Jan Heering, Paul Klint, and Jan Rekers. Incremen-
tal generation of parsers. IEEE Transactions on
Software Engineering, 16(12):1344–1351, 1990.
Ulf Hermjakob and Raymond J. Mooney. Learn-
ing parse and translation decisions from examples
with rich context. In ACL/EACL’97: Proceed-
ings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics and 8th Con-
ference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 482–
489, 1997.
R. Nigel Horspool. Incremental generation of LR
parsers. Technical report, University of Victoria,
Victoria, B.C., Canada, 1988. Report DCS-79-
IR.
James R. Kipps. GLR parsing in time O(n3). In
M. Tomita, editor, Generalized LR Parsing, pages
43–60. Kluwer, Boston, 1991.
Mitchell P. Marcus. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press, Cam-
bridge, MA, 1980.
Mehryar Mohri. Minimization algorithms for se-
quential transducers. Theoretical Computer Sci-
ence, 234(1–2):177–201, 2000.
Srini Narayanan and Daniel Jurafsky. Bayesian
models of human sentence processing. In Pro-
ceedings of CogSci-98, 1998.
Hozumi Tanaka, K.G. Suresh, and Koiti Yamada. A
family of generalized LR parsing algorithms us-
ing ancestors table. Technical report, Department
of Computer Science, Tokyo Institute of Technol-
ogy, Tokyo, Japan, 1992. TR92-0019.
Masaru Tomita and See-Kiong Ng. The Generalized
LR parsing algorithm. In Masaru Tomita, edi-
tor, Generalized LR Parsing, pages 1–16. Kluwer,
Boston, 1991.
Masaru Tomita. Efficient Parsing for Natural Lan-
guage. Kluwer, Boston, 1986.
Aboy Wong and Dekai Wu. Learning a
lightweight robust deterministic parser. In EU-
ROSPEECH’99: Sixth European Conference on
Speech Communication and Technology, Bu-
dapest, Sep 1999.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.361794">
<title confidence="0.75839125">An Efficient Algorithm to Induce Minimum Average Lookahead Grammars for Incremental LR Parsing Yihai dekai@cs.ust.hk shenyh@cs.ust.hk</title>
<author confidence="0.815042">Human Language Technology</author>
<affiliation confidence="0.964785">Department of Computer University of Science and Technology, Clear Water Bay, Hong Kong</affiliation>
<abstract confidence="0.996742225806452">We define a new learning task, minimum average lookahead grammar induction, with strong potential implications for incremental parsing in NLP and cognitive models. Our thesis is that a suitable learning bias for grammar induction is to minimize the degree of lookahead required, on the underlying tenet that language evolution drove grammars to be efficiently parsable in incremental fashion. The input to the task is an unannotated corpus, plus a nongrammar serves as an abstract model of environmental constraints confirming or rejecting potential parses. The constraining grammar typically allows ambiguity and is itself poorly suited for an incremental parsing model, since it gives rise to a high degree of nondeterminism in parsing. The learning task, then, is to ina deterministic under which it is possible to incrementally construct one of the correct parses for each sentence in the corpus, such that the average degree of lookahead needed to do so is minimized. This is a significantly more difficult optimization problem than merely compiling since not specified in advance. Clearly, naive approaches to this optimization can easily be computationally infeasible. However, by making combined use of GLR ancestor tables and incremental LR table construction methwe obtain an + approximation algorithm for this task that is quite efficient in practice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marie-Pierre Beal</author>
<author>Olivier Carton</author>
</authors>
<title>Determinization of transducers over finite and infinite words.</title>
<date>1968</date>
<journal>Theoretical Computer Science,</journal>
<volume>289</volume>
<issue>1</issue>
<marker>Beal, Carton, 1968</marker>
<rawString>Marie-Pierre Beal and Olivier Carton. Determinization of transducers over finite and infinite words. Theoretical Computer Science, 289(1), 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing for unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="5422" citStr="Briscoe and Carroll, 1993" startWordPosition="831" endWordPosition="834">ate derivation order, to resolve parsing conflicts between certain actions such as “merge” and “add-into”, and to identify specific features for disambiguating actions. In our earlier work we described a deterministic parser with a fully automatically learned decision algorithm (Wong and Wu, 1999). But unlike our present work, the decision algorithms in both Hermjakob &amp; Mooney (1997) and Wong &amp; Wu (1999) are procedural; there is no explicit representation of the grammar that can be meaningfully inspected. Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly nondeterministic fashion during parsing. We believe the shortage of learning models for deterministic parsing is in no small part due to the difficulty of overcoming computational complexity barriers in the optimization problems this would involve. Many types of factors need to be optimized in learning, because deterministic parsing is much more sensitive to incorrect choice of structural features (e.g., categories, rules) than nondeterministic parsing that employ robustness mechanisms such as weighted charts. C</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. Generalised probabilistic LR parsing for unification-based grammars. Computational Linguistics, 19(1):25–60, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="12660" citStr="Earley (1970)" startWordPosition="2014" endWordPosition="2015">reduce parser follows in order to construct P. At any given point, T will hold the LR table computed from the MAL parse of all sentences already processed, and A will hold the corresponding parsing sequences for the MAL parses. Entering the loop, we iteratively augment T by adding the states arising from the MAL parse F ∗ of each successive sentence in the training corpus and, in addition, cache the corresponding parsing action sequence A (F∗) into the set A. This is done by first computing a chart for the sentence, in line 4, by parsing Si under the constraining grammar GC using the standard Earley (1970) procedure. We then call find MAL parse in line 5, to compute the parse that requires minimum average lookahead to resolve ambiguity. The items and states produced by the rules in F∗are added to the LR table T by calling incremental update LR in line 6, and the parsing action sequence of F∗ is appended to A in line 7. Note that the indices of the original states in T are not changed and only items are added into them if need be so that A is not changed by adding items and states to T, and there might be new states introduced which are also indexed. By definition, the true MAL grammar does not </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2), 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Fischer</author>
</authors>
<title>Incremental LR(1) parser construction as an aid to syntactical extensibility.</title>
<date>1980</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, University of Dortmund, Federal Republic of Germany,</institution>
<contexts>
<context position="15042" citStr="Fischer (1980)" startWordPosition="2448" endWordPosition="2449">ng as to be infeasible when trying to learn a MAL grammar with a realistically large input corpus and/or constraining grammar. The incremental update LR function incrementally updates the LR table in an efficient fashion that avoids recomputing the entire table. The inputs to incremental update LR are a pre-existing LR table T, and a set of new rules R to be added. This algorithm is derived from the incremental LR parser generator algorithm ilalr and is relatively complex; see Horspool (1988) for details. Historically, work on incremental parser generators first concentrated on LL(1) parsers. Fischer (1980) was first to describe a method for incrementally updating an LR(1) table. Heering et al.(1990) use the principle of lazy evaluation to attack the same problem. Our design of incremental update LR is more closely related to ilalr for the following reasons: • ilalr has the property that when updating the LR table to contain the newly added rules, it does not change the index of each already existing state. This is important for our task as the slightest change in the states might affect significantly the parsing sequences of the sentences that have already been processed. • Although worst case </context>
</contexts>
<marker>Fischer, 1980</marker>
<rawString>G. Fischer. Incremental LR(1) parser construction as an aid to syntactical extensibility. Technical report, Department of Computer Science, University of Dortmund, Federal Republic of Germany, 1980. PhD Dissertation, Tech. Report 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In NAACL-2001: Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="3928" citStr="Hale (2001)" startWordPosition="607" endWordPosition="608">ders the claim of incremental parsing meaningless. Marcus simply postulated that a maximum buffer size of three was sufficient. In contrast, our approach permits greater flexibility and finer gradations, where the average degree of lookahead required can be minimized with the aim of assisting grammar induction. Since Marcus’ work, a significant body of work on incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of nondeterminism, often with probabilistic weights (e.g., Narayanan &amp; Jurafsky (1998); Hale (2001)). Meanwhile, in the way of formal methods, Tomita (1986) introduced Generalized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al &amp; Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relatively little work</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. A probabilistic Earley parser as a psycholinguistic model. In NAACL-2001: Second Meeting of the North American Chapter of the Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Heering</author>
<author>Paul Klint</author>
<author>Jan Rekers</author>
</authors>
<title>Incremental generation of parsers.</title>
<date>1990</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>16</volume>
<issue>12</issue>
<marker>Heering, Klint, Rekers, 1990</marker>
<rawString>Jan Heering, Paul Klint, and Jan Rekers. Incremental generation of parsers. IEEE Transactions on Software Engineering, 16(12):1344–1351, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning parse and translation decisions from examples with rich context.</title>
<date>1997</date>
<booktitle>In ACL/EACL’97: Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>482--489</pages>
<contexts>
<context position="4623" citStr="Hermjakob &amp; Mooney (1997)" startWordPosition="708" endWordPosition="711">eneralized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al &amp; Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relatively little work on automatic learning of grammars for deterministic parsers to date. Hermjakob &amp; Mooney (1997) describe a semi-automatic procedure for learning a deterministic parser from a treebank, which requires the intervention of a human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as “merge” and “add-into”, and to identify specific features for disambiguating actions. In our earlier work we described a deterministic parser with a fully automatically learned decision algorithm (Wong and Wu, 1999). But unlike our present work, the decision algorithms in both Hermjakob &amp; Mooney (1997) and Wong &amp; Wu (1999) are procedural; the</context>
</contexts>
<marker>Hermjakob, Mooney, 1997</marker>
<rawString>Ulf Hermjakob and Raymond J. Mooney. Learning parse and translation decisions from examples with rich context. In ACL/EACL’97: Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 482– 489, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nigel</author>
</authors>
<title>Horspool. Incremental generation of LR parsers.</title>
<date>1988</date>
<tech>Technical report,</tech>
<institution>University of Victoria,</institution>
<location>Victoria, B.C.,</location>
<note>Report DCS-79-IR.</note>
<marker>Nigel, 1988</marker>
<rawString>R. Nigel Horspool. Incremental generation of LR parsers. Technical report, University of Victoria, Victoria, B.C., Canada, 1988. Report DCS-79-IR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Kipps</author>
</authors>
<title>GLR parsing in time O(n3).</title>
<date>1991</date>
<booktitle>Generalized LR Parsing,</booktitle>
<pages>43--60</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer,</publisher>
<location>Boston,</location>
<contexts>
<context position="17936" citStr="Kipps (1991)" startWordPosition="2973" endWordPosition="2974">d stack of the same form as in GLR parsing (Tomita, 1986)(Tomita and Ng, 1991) while simultaneously computing the minimum average lookahead. Note that Tomita’s original method for constructing the graph-structured stack has exponential time complexity O (nP+1), in which n and p are the length of the sentence and the length of the longest rhs of any rule in the grammar. As a result, Tomita’s algorithm achieves O (n3) for grammars in Chomsky normal form but is potentially exponential when productions are of unrestricted length, which in practice is the case with most parsing problems. We follow Kipps (1991) in modifying Tomita’s algorithm to allow it to run in time proportional to n3 for grammars with productions of arbitrary length. The most time consuming part in Tomita’s algorithm is when reduce actions are executed in which the ancestors of the current node have to be found incurring time complexity nρ. To avoid this we employ an ancestor table to keep the ancestors of each node in the GLR forest which is updated dynamically as the GLR forest is being constructed. This modification brings down the time complexity of reduce actions to n2 in the worst case, and allows the function build GLR fo</context>
</contexts>
<marker>Kipps, 1991</marker>
<rawString>James R. Kipps. GLR parsing in time O(n3). In M. Tomita, editor, Generalized LR Parsing, pages 43–60. Kluwer, Boston, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Marcus, 1980</marker>
<rawString>Mitchell P. Marcus. A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Minimization algorithms for sequential transducers.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<volume>234</volume>
<issue>1</issue>
<contexts>
<context position="4274" citStr="Mohri (2000)" startWordPosition="652" endWordPosition="653">t body of work on incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of nondeterminism, often with probabilistic weights (e.g., Narayanan &amp; Jurafsky (1998); Hale (2001)). Meanwhile, in the way of formal methods, Tomita (1986) introduced Generalized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al &amp; Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relatively little work on automatic learning of grammars for deterministic parsers to date. Hermjakob &amp; Mooney (1997) describe a semi-automatic procedure for learning a deterministic parser from a treebank, which requires the intervention of a human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions suc</context>
</contexts>
<marker>Mohri, 2000</marker>
<rawString>Mehryar Mohri. Minimization algorithms for sequential transducers. Theoretical Computer Science, 234(1–2):177–201, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Bayesian models of human sentence processing.</title>
<date>1998</date>
<booktitle>In Proceedings of CogSci-98,</booktitle>
<contexts>
<context position="3915" citStr="Narayanan &amp; Jurafsky (1998)" startWordPosition="603" endWordPosition="606">al to the sentence length renders the claim of incremental parsing meaningless. Marcus simply postulated that a maximum buffer size of three was sufficient. In contrast, our approach permits greater flexibility and finer gradations, where the average degree of lookahead required can be minimized with the aim of assisting grammar induction. Since Marcus’ work, a significant body of work on incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of nondeterminism, often with probabilistic weights (e.g., Narayanan &amp; Jurafsky (1998); Hale (2001)). Meanwhile, in the way of formal methods, Tomita (1986) introduced Generalized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al &amp; Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relativel</context>
</contexts>
<marker>Narayanan, Jurafsky, 1998</marker>
<rawString>Srini Narayanan and Daniel Jurafsky. Bayesian models of human sentence processing. In Proceedings of CogSci-98, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hozumi Tanaka</author>
<author>K G Suresh</author>
<author>Koiti Yamada</author>
</authors>
<title>A family of generalized LR parsing algorithms using ancestors table.</title>
<date>1992</date>
<tech>Technical report,</tech>
<pages>92--0019</pages>
<institution>Department of Computer Science, Tokyo Institute of Technology,</institution>
<location>Tokyo,</location>
<contexts>
<context position="26359" citStr="Tanaka et al. (1992)" startWordPosition="4628" endWordPosition="4631"> needed by each parsing action is computed by comparing the parsing action with the MAL parsing action sequences for all previous sentences, the time complexity of this function depends on the maximum length of any sentence that has already been processed, which is bounded by n. The dynamic programming method used to locate the most economical parse in terms of average lookahead, described above, can be seen to be quadratic in n. 4. reconstruct MAL parse is O (n2). This is bounded by the number of LR state transitions in each full parse of the sentence, which is is O (n2). Note, however, that Tanaka et al. (1992) propose an enhancement that can reconstruct the parse trees in time linear to n; this is a direction for future improvement of our algorithm. 5. incremental update LR is O (2m). As with ilalr, theoretically the worst time complexity is exponential in the number of rules in the existing grammar. However, various heuristics can be employed to make the algorithm quicker, and in practical experiments the algorithm is quite fast and precise in producing LR tables, particularly since m is very small relative to |S|. The time complexity of the algorithm for each sentence is thus O (n3) + O (n3) + O </context>
</contexts>
<marker>Tanaka, Suresh, Yamada, 1992</marker>
<rawString>Hozumi Tanaka, K.G. Suresh, and Koiti Yamada. A family of generalized LR parsing algorithms using ancestors table. Technical report, Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan, 1992. TR92-0019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
<author>See-Kiong Ng</author>
</authors>
<title>The Generalized LR parsing algorithm.</title>
<date>1991</date>
<booktitle>In Masaru Tomita, editor, Generalized LR Parsing,</booktitle>
<pages>1--16</pages>
<publisher>Kluwer,</publisher>
<location>Boston,</location>
<contexts>
<context position="17402" citStr="Tomita and Ng, 1991" startWordPosition="2883" endWordPosition="2886"> function find MAL parse selects the full parse F* of a given sentence that requires the least average lookahead kˆ (A( F )) to resolve any shiftreduce or reduce-reduce conflicts with a set A of parsing action sequences, such that F* is a subset of a chart C. The inputs to find MAL parse, more specifically, are a chart C containing all the partial parses in the input sentence, and the set A containing the parsing action sequences of the MAL parse of all sentences processed so far. The algorithm operates by constructing a graph-structured stack of the same form as in GLR parsing (Tomita, 1986)(Tomita and Ng, 1991) while simultaneously computing the minimum average lookahead. Note that Tomita’s original method for constructing the graph-structured stack has exponential time complexity O (nP+1), in which n and p are the length of the sentence and the length of the longest rhs of any rule in the grammar. As a result, Tomita’s algorithm achieves O (n3) for grammars in Chomsky normal form but is potentially exponential when productions are of unrestricted length, which in practice is the case with most parsing problems. We follow Kipps (1991) in modifying Tomita’s algorithm to allow it to run in time propor</context>
</contexts>
<marker>Tomita, Ng, 1991</marker>
<rawString>Masaru Tomita and See-Kiong Ng. The Generalized LR parsing algorithm. In Masaru Tomita, editor, Generalized LR Parsing, pages 1–16. Kluwer, Boston, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwer,</publisher>
<location>Boston,</location>
<contexts>
<context position="3985" citStr="Tomita (1986)" startWordPosition="616" endWordPosition="617">us simply postulated that a maximum buffer size of three was sufficient. In contrast, our approach permits greater flexibility and finer gradations, where the average degree of lookahead required can be minimized with the aim of assisting grammar induction. Since Marcus’ work, a significant body of work on incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of nondeterminism, often with probabilistic weights (e.g., Narayanan &amp; Jurafsky (1998); Hale (2001)). Meanwhile, in the way of formal methods, Tomita (1986) introduced Generalized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al &amp; Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relatively little work on automatic learning of grammars for deterministic pars</context>
<context position="17381" citStr="Tomita, 1986" startWordPosition="2882" endWordPosition="2883">ead parses The function find MAL parse selects the full parse F* of a given sentence that requires the least average lookahead kˆ (A( F )) to resolve any shiftreduce or reduce-reduce conflicts with a set A of parsing action sequences, such that F* is a subset of a chart C. The inputs to find MAL parse, more specifically, are a chart C containing all the partial parses in the input sentence, and the set A containing the parsing action sequences of the MAL parse of all sentences processed so far. The algorithm operates by constructing a graph-structured stack of the same form as in GLR parsing (Tomita, 1986)(Tomita and Ng, 1991) while simultaneously computing the minimum average lookahead. Note that Tomita’s original method for constructing the graph-structured stack has exponential time complexity O (nP+1), in which n and p are the length of the sentence and the length of the longest rhs of any rule in the grammar. As a result, Tomita’s algorithm achieves O (n3) for grammars in Chomsky normal form but is potentially exponential when productions are of unrestricted length, which in practice is the case with most parsing problems. We follow Kipps (1991) in modifying Tomita’s algorithm to allow it </context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Masaru Tomita. Efficient Parsing for Natural Language. Kluwer, Boston, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aboy Wong</author>
<author>Dekai Wu</author>
</authors>
<title>Learning a lightweight robust deterministic parser.</title>
<date>1999</date>
<booktitle>In EUROSPEECH’99: Sixth European Conference on Speech Communication and Technology,</booktitle>
<location>Budapest,</location>
<contexts>
<context position="5094" citStr="Wong and Wu, 1999" startWordPosition="780" endWordPosition="783">unately, there has still been relatively little work on automatic learning of grammars for deterministic parsers to date. Hermjakob &amp; Mooney (1997) describe a semi-automatic procedure for learning a deterministic parser from a treebank, which requires the intervention of a human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as “merge” and “add-into”, and to identify specific features for disambiguating actions. In our earlier work we described a deterministic parser with a fully automatically learned decision algorithm (Wong and Wu, 1999). But unlike our present work, the decision algorithms in both Hermjakob &amp; Mooney (1997) and Wong &amp; Wu (1999) are procedural; there is no explicit representation of the grammar that can be meaningfully inspected. Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly nondeterministic fashion during parsing. We believe the shortage of learning models for deterministic parsing is in no small part due to the difficulty of overcoming computational complexit</context>
<context position="5203" citStr="Wong &amp; Wu (1999)" startWordPosition="799" endWordPosition="802">s to date. Hermjakob &amp; Mooney (1997) describe a semi-automatic procedure for learning a deterministic parser from a treebank, which requires the intervention of a human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as “merge” and “add-into”, and to identify specific features for disambiguating actions. In our earlier work we described a deterministic parser with a fully automatically learned decision algorithm (Wong and Wu, 1999). But unlike our present work, the decision algorithms in both Hermjakob &amp; Mooney (1997) and Wong &amp; Wu (1999) are procedural; there is no explicit representation of the grammar that can be meaningfully inspected. Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly nondeterministic fashion during parsing. We believe the shortage of learning models for deterministic parsing is in no small part due to the difficulty of overcoming computational complexity barriers in the optimization problems this would involve. Many types of factors need to be optimized in lea</context>
</contexts>
<marker>Wong, Wu, 1999</marker>
<rawString>Aboy Wong and Dekai Wu. Learning a lightweight robust deterministic parser. In EUROSPEECH’99: Sixth European Conference on Speech Communication and Technology, Budapest, Sep 1999.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>