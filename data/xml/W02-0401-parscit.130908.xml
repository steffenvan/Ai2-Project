<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001977">
<note confidence="0.991198">
Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
Philadelphia, July 2002, pp. 1-8. Association for Computational Linguistics.
</note>
<title confidence="0.979099">
Using Maximum Entropy for Sentence Extraction
</title>
<author confidence="0.851292">
Miles Osborne
</author>
<email confidence="0.614512">
osborne@cogsci.ed.ac.uk
</email>
<affiliation confidence="0.995682">
Division of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.768368333333333">
2 Buccleuch Place
Edinburgh EH8 9LW
United Kingdom.
</address>
<sectionHeader confidence="0.899377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930888888889">
A maximum entropy classifier can be used
to extract sentences from documents. Ex-
periments using technical documents show
that such a classifier tends to treat features
in a categorical manner. This results in per-
formance that is worse than when extract-
ing sentences using a naive Bayes classifier.
Addition of an optimised prior to the max-
imum entropy classifier improves perfor-
mance over and above that of naive Bayes
(even when naive Bayes is also extended
with a similar prior). Further experiments
show that, should we have at our disposal
extremely informative features, then max-
imum entropy is able to yield excellent re-
sults. Naive Bayes, in contrast, cannot ex-
ploit these features and so fundamentally
limits sentence extraction performance.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998938512">
Sentence extraction the recovery of a given set
of sentences from some document is useful for
tasks such as document summarisation (where the
extracted sentences can form the basis of a summary)
or question-answering (where the extracted sentences
can form the basis of an answer). In this paper, we
concentrate upon extraction of sentences for inclu-
sion into a summary. From a machine learning per-
spective, sentence extraction is interesting because
typically, the number of sentences to be extracted
is a very small fraction of the total number of sen-
tences in the document. Furthermore, those clues
which determine whether a sentence should be ex-
tracted or not tend to be either extremely specific,
or very weak, and furthermore interact together in
non-obvious ways. From a linguistic perspective, the
task is challenging since success hinges upon the abil-
ity to integrate together diverse levels of linguistic
description.
Frequently (see section 6 for examples), sentence
extraction systems are based around simple algo-
rithms which assume independence between those
features used to encode the task. A consequence of
this assumption is that such approaches are funda-
mentally unable to exploit dependencies which pre-
sumably exist in the features that would be present
in an ideal sentence extraction system. This situ-
ation may be acceptable when the features used to
model sentence extraction are simple. However, it
will rapidly become unacceptable when more sophis-
ticated heuristics, with complicated interactions, are
brought to bear upon the problem. For example,
Boguraev and Neff (2000a) argue that the quality of
summarisation can be increased if lexical cohesion
factors (rhetorical devices which help achieve cohe-
sion between related document utterances) are mod-
elled by a sentence extraction system. Clearly such
devices (for example, lexical repetition, ellipsis, co-
reference and so on) all contribute towards the gen-
eral discourse structure of some text and furthermore
are related to each other in non-obvious ways.
Maximum entropy (log-linear) models, on the
other hand, do not make unnecessary independence
assumptions. Within the maximum entropy frame-
work, we are able to optimally integrate together
whatever sources of knowledge we believe potentially
to be useful for the task. Should we use features that
are beneficial, then the model will be able to exploit
this fact. Should we use features that are irrelevant,
then again, the model will be able to notice this, and
effectively ignore them. Models based on maximum
entropy are therefore well suited to the sentence ex-
traction task, and furthermore, yield competitive re-
sults on a variety of language tasks (Ratnaparkhi,
1996; Berger et al., 1996; Charniak, 1999; Nigam et
al., 1999).
In this paper, we outline a conditional maximum
entropy classification model for sentence extraction.
Our model works incrementally, and does not always
need to process the entire document before assign-
ing classification.&apos; It discriminates between those
sentences which should and should not be extracted.
This contrasts with ranking approaches which need
to process the entire document before extracting sen-
tences. Because we model whether a sentence should
be extracted or not in terms of features that are ex-
tracted from the sentence (and its context in the doc-
ument), we do not need to specify the size of the sum-
mary. Again, this contrasts with ranking approaches
which need to specify a priori the summary size.
Our maximum entropy approach for sentence ex-
traction does not come without problems. Using
reasonably standard features, and when extracting
sentences from technical papers, we find that pre-
cision levels are high, but recall is very low. This
arises from the fact that those features which pre-
dict whether a sentence should be extracted tend to
be very specific and occur infrequently. Features for
sentences that should not be extracted tend to be
much more abundant, and so more likely to be seen
in the future. A simple prior probability is shown
to help counter-act this tendency. Using our prior,
we find that the maximum entropy approach is able
to yield results that are better than a naive Bayes
classifier.
Our final set of experiments looks more closely at
the differences between maximum entropy and naive
Bayes. We show that when we have access to an ora-
cle that is able to tell us when to extract a sentence,
then in the situation when that information is en-
coded in dependent features, maximum entropy eas-
ily outperforms naive Bayes. Furthermore, we also
show that even when that information is encoded in
terms of independent features, naive Bayes can be
incapable of fully utilising this information, and so
produces worse results than maximum entropy.2
Incremental classification means that a document is
processed from start-to-finish and decisions are made as
soon as sentences are encountered. Some of our features
(in particular, those which encode sentence position in
a document) do require processing the entire document.
Using such features prevents true incremental process-
ing. However, it is trivial to remove such features and so
ensure true incrementality.
2 A a reviewer commented, under certain circum-
stances, naive Bayes can do well even when there are
strong dependencies within features (Domingos and Paz-
zani, 1997). For example, when the sample size is small,
naive Bayes can be competitive with more sophisticated
approaches such as maximum entropy. Given this, a
fuller comparison of naive Bayes and maximum entropy
for sentence extraction requires considering sample size
in addition to the choice of features.
The rest of this paper is as follows. Section 2
outlines the general framework for sentence extrac-
tion using maximum entropy modelling. Section 3
presents our naive Bayes classifier (which is used as a
comparison with maximum entropy). We then show
in section 4 how both our maximum entropy and
naive Bayes classifiers can be extended with an (op-
timised) prior. The issue of summary size is touched
upon in section 5. Section 6 discusses related work.
We then present our main results (section 7). Fi-
nally, section 8 discusses our results and considers
future work.
</bodyText>
<sectionHeader confidence="0.99656" genericHeader="introduction">
2 Maximum Entropy for Sentence
Extraction
</sectionHeader>
<subsectionHeader confidence="0.995423">
2.1 Conditional Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.9975205">
The parametric form for a conditional maximum en-
tropy model is as follows (Nigam et al., 1999):
</bodyText>
<equation confidence="0.9966955">
P(c 1 s) = Z(s) exp(X 1Aifi(c, s))
i
Z(s) = X exp(X Aifi(c, s)) (2)
c i
</equation>
<bodyText confidence="0.998705833333334">
Here, c is a label (from the set of labels C) and s is
the item we are interested in labelling (from the set
of items S). In our domain, C simply consists of two
labels: one indicating that a sentence should be in
the summary (`keep&apos;), and another label indicating
that the sentence should not be in the summary (`re-
ject&apos;). S consists of a training set of sentences, linked
to their originating documents. This means that we
can recover the position of any given sentence in any
given document.
Within maximum entropy models, the training set
is viewed in terms of a set of features. Each fea-
ture expresses some characteristic of the domain.
For example, a feature might capture the idea that
abstract-worthy sentences contain the words in this
paper. In equation 1, fi(c, s) is a feature. In this pa-
per we restrict ourselves to integer-valued functions.
An example feature might be as follows:
</bodyText>
<footnote confidence="0.99384125">
1 if s contains the phrase
in this paper
and c is the label keep
0 otherwise
</footnote>
<bodyText confidence="0.6096275">
Features are related to each other through weights
(as can be seen in equation 1, where some feature
fi has a weight Ai). Weights are real-valued num-
bers. When a closed form solution cannot be found,
</bodyText>
<equation confidence="0.9543424">
(1)
8&lt;&gt;&gt;
&gt;&gt;:
fi(c, s) =
(3)
</equation>
<bodyText confidence="0.999729666666667">
they are determined by numerical optimisation tech-
niques. In this paper, we use conjugate gradient de-
scent to find the optimal set of weights. Conjugate
Gradient descent converges faster than Improved It-
erative Scaling (Lafferty et al., 1997), and empirically
we find that it is numerically more stable.
</bodyText>
<subsectionHeader confidence="0.998685">
2.2 Maximum Entropy Classification
</subsectionHeader>
<bodyText confidence="0.9997255">
When classifying sentences with maximum entropy,
we use the equation:
</bodyText>
<equation confidence="0.993646">
label(s) = argmaxcECP(c j s) (4)
</equation>
<bodyText confidence="0.986694333333333">
In practice, we are not interested in the probabil-
ity of a label given a sentence. Instead we use the
unnormalised score:
</bodyText>
<equation confidence="0.986001">
label(s) = argmaxcEC exp(X Aifi(c, s)) (5)
i
</equation>
<bodyText confidence="0.999954333333333">
Note that this maximum entropy classifier assumes
a uniform prior. Section 4 shows how a non-uniform
prior is used in place of this uniform prior.
We now present our basic naive Bayes classifier.
Afterwards, we extend this classifier with a non-
uniform prior.
</bodyText>
<sectionHeader confidence="0.996488" genericHeader="method">
3 Naive Bayes Classification
</sectionHeader>
<bodyText confidence="0.999891916666667">
As an alternative to maximum entropy, we also inves-
tigated a naive Bayes classifier. Unlike maximum en-
tropy, naive Bayes assumes features are conditionally
independent of each other. So, comparing the two to-
gether will give an indication of the level of statistical
dependencies which exist between features in the sen-
tence extraction domain. For our experiments, we
used a variant of the multi-variate Bernoulli event
model (McCallum and Nigam, 1998). In particular,
we did not consider features that are absent in some
example. This allows us to avoid summing over all
features in the model for each example. Note that
our maximum entropy model also did not consider
absent features.
Within our naive Bayes approach, the probability
of a label given the sentence is as follows:
As before, s is some sentence, c the label, and gi
is some active feature describing sentence s. Naive
Bayes models can be estimated in a closed form
by simple counting. For features which have zero
counts, we use add-k smoothing (where k is a small
number less than one).
Since the probability of the data (P(s)) is con-
stant:
</bodyText>
<equation confidence="0.9812765">
P(c j s) / P(c) Yn P(gi j c) (7)
i=1
</equation>
<bodyText confidence="0.987545">
If we assume a uniform prior (in which case P(c) is a
constant for all c), this can be further simplified to:
</bodyText>
<equation confidence="0.9784166">
P(c j s) / Yn P(gi j c) (8)
i=1
Our basic naive Bayes classifier is as follows:
label(s) = argmaxcEC Yn P(gi j c) (9)
i=1
</equation>
<bodyText confidence="0.9999595">
As with the maximum entropy classifier, we later
replace the uniform prior with a non-uniform prior.
</bodyText>
<sectionHeader confidence="0.9790935" genericHeader="method">
4 Maximum a Posteriori
Classification
</sectionHeader>
<bodyText confidence="0.999918333333333">
In this section, we show how our classifiers can be
extended with a non-uniform prior. We also describe
how such a prior can be optimised.
</bodyText>
<subsectionHeader confidence="0.999707">
4.1 Adding a non-uniform prior
</subsectionHeader>
<bodyText confidence="0.999845888888889">
Now, the two classifiers mentioned previously (equa-
tions 9 and 5) are both based on maximum likeli-
hood estimation. However, as we describe later, for
sentence extraction, the maximum entropy classifier
tends to over-select labels. In particular, it tends
to reject too many sentences for inclusion into the
summary. So, it it useful to extend the two previous
classifiers with a non-uniform prior. For the naive
Bayes classifier, we have:
</bodyText>
<equation confidence="0.999092">
label(s) = argmaxcECP(c) Yn P(gi j c) (10)
i=1
</equation>
<bodyText confidence="0.99440325">
Here, P(c) is our prior. The probability of the data
(P(s)) is constant and so can be dropped.
For the maximum entropy case, we are not inter-
ested in the actual probability:
</bodyText>
<equation confidence="0.967506">
label(s) = argmaxcECF(c) exp(X Aifi(c, s)) (11)
i
</equation>
<bodyText confidence="0.9993964">
F(c) is a function equivalent to the prior when
using the unnormalised classifier. When this prior
distribution (or equivalent function) is uniform, clas-
sification is as before (namely as outlined in sections
2 and 3), and depends upon the maximum entropy
or naive Bayes component. When the prior is non-
uniform, the classifier behaviour will change. This
prior therefore allows us to affect the performance
of our system. In particular, we can change the
precision-recall balance.
</bodyText>
<equation confidence="0.981847">
P(c)Qn i=1 P(gi j c)
P(c j s) =
(6)
P(s)
</equation>
<subsectionHeader confidence="0.986354">
4.2 Optimising the prior
</subsectionHeader>
<bodyText confidence="0.9999804">
We treat the problem of selecting a prior as an opti-
misation task: select some P(c) (or F(c)) such that
performance, as measured by some objective func-
tion of the overall classifier, is maximised. Since the
choice of objective function is up to us, we can eas-
ily optimise the classifier in any way we decide. For
example, we could optimise for recall by using as our
objective function an f-measure that weighted recall
more highly than precision. In this paper, we opti-
mise the prior using as an objective function the f2
score of the classifier (section 7 details this score).
Our prior therefore does not reflect relative frequen-
cies of labels (as found in some corpus).
We now need to optimise our prior. Brent&apos;s one
dimensional function minimisation method is well
suited to this task (Press et al., 1993), since for a
random variable taking two values, the probability
of one value can be defined in terms of the other
value. Section 7 describes the held-out optimisation
strategy used in our experiments.
Should we decide to use a more elaborate prior (for
example, one which was also sensitive to properties
of documents) then we would need to use a multi-
dimensional function minimisation method.
Note that we have not simultaneously optimised
the likelihood and prior probabilities. This means
that we do not necessarily find the optimal maxi-
mum a posteriori (MAP) solution. It is possible to
integrate into maximum entropy estimation (simple)
conjugate priors that do allow MAP solutions to be
found (Chen and Rosenfeld, 1999). Although it is
an open question whether more complex priors can
be directly integrated, future work ought to consider
the efficacy of such approaches in the context of sum-
marisation.
</bodyText>
<sectionHeader confidence="0.985237" genericHeader="method">
5 Summary size
</sectionHeader>
<bodyText confidence="0.99993275">
Determining the size of the summary is an impor-
tant consideration for summarisation. Frequently,
this is carried out dynamically, and specified by the
user. For example, when there is limited opportu-
nity to display long summaries a user might want a
terse summary. Alternatively, when recall is impor-
tant, a user might prefer a longer summary. Usually,
systems rank all sentences in terms of how abstract-
worthy that are, and then take the top n most highly
ranked sentences. This always requires the size of
summary to be specified.
In our classification framework, sentences are pro-
cessed (largely) independently of each other, and so
there is no direct way of controlling the size of the
summary. Altering the prior will indirectly influence
the summary size. For more direct control over sum-
mary size, we can rank sentences using our classifiers
(we not only label but can also assign label prob-
abilities) and select the top n most highly ranked
sentences.
Within our classification approach, the optimised
prior plays a similar role to the user-defined number
of sentences that a ranking approach might return.
Experiments (not reported here) showed that
ranking sentences using our maximum entropy classi-
fier, and then selecting the top n most highly ranked
sentences produced slightly worse results than when
selecting sentences in terms of classification.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999994470588235">
The summarisation literature is large. Here we con-
sider only a representative sample.
Kupiec et al. (1995) used Naive Bayes for sentence
extraction. They did not consider the role of the
prior, nor did they use Naive Bayes for classifica-
tion. Instead, they used it to rank sentences and
selected the top n sentences. The TEXTRACT
system included a sentence extraction component
that is frequency-based (Boguraev and Neff, 2000b).
Whilst the system uses a wide variety of linguis-
tic cues when scoring sentences, it does not com-
bine these scores in an optimal manner. Also, it
does not consider interactions between the linguistic
cues. Goldstein et al. (1999) used a centroid similar-
ity measure to score sentences. They do not appear
to have optimised their metric, nor do they deal with
statistical dependencies between their features.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.988675666666667">
Summarisation evaluation is a hard task, principally
because the notion of an objective summary is ill-
defined. That aside, in order to compare our various
systems, we used an intrinsic evaluation approach.
Our summaries were evaluated using the standard
f2 score:
</bodyText>
<equation confidence="0.982458222222222">
2pr
f2 =
p + r
where:
r = Recall
p = Precision
j = Number of correct sentences in summary
k = Number of sentences in summary
m = Number of correct sentences in the document
</equation>
<bodyText confidence="0.998011666666667">
A sentence being `correct&apos; means that it was
marked as being somehow important (abstract-
worthy) by a human and labelled `keep&apos; by one of
</bodyText>
<equation confidence="0.997292333333333">
i i
r =
M p = k
</equation>
<bodyText confidence="0.995434066666667">
our classifiers. Summaries produced by our systems
will therefore attempt to mimic the process of select-
ing what it means for a sentence to be important in
a document.
Naturally this premise that an annotator can
decide a priori whether a sentence is abstract-worthy
or not is open to question. That aside, in other
sentence extraction scenarios, it may well be the case
that sentences can be reliably annotated.
The f2 score treats recall and precision equally.
This is a sensible metric to use as we have no a priori
reason to believe in some other non-equal ratio of the
two components.
Our evaluation results are based on the following
approach:
</bodyText>
<listItem confidence="0.985342526315789">
1. Split the set of documents into two disjoint sets
(T1 and T2), with 70 documents in T1 and 10
documents in T2.
2. Further split T1 into two disjoint sets T3 and
T4. T3 is used to train a model, and T4 is
a held-out set. The prior is estimated using
Brent&apos;s line minimisation method, when train-
ing using T3 and evaluating on T4. T3 consisted
of 60 documents and T4 consisted of 10 docu-
ments.
3. Results are then presented using a model trained
on T1, with the prior just found, and evaluated
using T2. T1 is therefore the training set and
T2 is the testing set. Results are also presented
using a flat prior.
4. The whole process is then repeated after ran-
domising the documents. The final results are
then averaged over these n runs. We set n to
40.
</listItem>
<subsectionHeader confidence="0.985773">
7.1 Document set
</subsectionHeader>
<bodyText confidence="0.999935214285714">
For data, we used the same documents that
Teufel (2001) used in her experiments.3 In brief,
these were 80 conference papers, taken from the
Comp-lang preprint archive, and semi-automatically
converted from L�T�,Xto XML. The XML annotated
documents were then additionally manually marked-
up with tags indicating the status of various sen-
tences. This document set is modest in size. On the
other hand, the actual documents are longer than
newswire messages typically used for summarisation
tasks. Also, the documents show variation in style.
For example, some documents are written by non-
native speakers, some by students, some by multiple
authors and so on. Summarisation is therefore hard.
</bodyText>
<footnote confidence="0.9387865">
3A superset of the documents is described in (Teufel
and Moens, 1997).
</footnote>
<bodyText confidence="0.999969181818182">
Here are some properties of the documents. On
average, each document contained 8 sentences that
were marked as being abstract-worthy (standard de-
viation of 3.1). The documents on average each
contained in total 174 sentences (standard deviation
50.7). Here, a `sentence&apos; is either any sequence of
words that happened to be in a title, or else any se-
quence of words in the rest of the document. As can
be seen, the summaries are not uniformly long. Also,
the documents vary considerably in length. Sum-
mary size is therefore not constant.
</bodyText>
<subsectionHeader confidence="0.757698">
7.2 Features
</subsectionHeader>
<bodyText confidence="0.9996125">
We used the following, fairly standard features when
describing all sentences in the documents:
</bodyText>
<listItem confidence="0.999242583333333">
• Word pairs. Word pairs are consecutive words
as found in a sentence. A word pair feature sim-
ply indicates whether a particular word pair is
present. All words were reduced: truncated to
be at most 10 characters long. Stemming (as
for example carried out by the Porter stemmer)
produced worse results. We extracted all word
pairs found in all sentences, and for any given
sentence, found the set of (reduced) word pairs.
• Sentence length. We encoded in three binary
features whether a sentence was less than 6
words in length, whether it was greater than 20
words in length, or whether it was in between
these two ranges. We also used a feature which
encoded whether a previous sentence was less
than 5 words or longer. This captured the idea
that summary sentences tend to follow headings
(which are short).
• Sentence position. Summary sentences tend to
occur either at the start, or the end of a docu-
ment. We used three features: whether a given
sentence was within the first 8 paragraphs of a
document, whether a sentence was in the last
3 paragraphs, or whether the sentence was in
</listItem>
<bodyText confidence="0.9587753125">
a paragraph between these two ranges to en-
code sentence position. Note that this feature
requires the whole document to be processed be-
fore classification can take place.
. (Limited) discourse features. Our features de-
scribed whether a sentence immediately followed
typical headings such as conclusion or introduc-
tion, whether a sentence was at the start of a
paragraph, or whether a sentence followed some
generic heading.
Our features are not exhaustive, and are not designed
to maximise performance. Instead, they are designed
to be typical of those found in sentence extraction
systems. Note that some of our features exploit the
fact that the documents are annotated with struc-
tural information (such as headers etc).
Experiments with removing stop words from docu-
ments resulted in decreased performance. We conjec-
ture that this is because our word pairs are extremely
crude syntax approximations. Removing stop words
from sentences and then creating word pairs makes
these pairs even worse syntax approximations. How-
ever, using stop words increased the number of fea-
tures in our model, and so again reduced perfor-
mance. We therefore compromised between these
two positions, and mapped all stop words to the same
symbol prior to creation of word pair features. We
also found it useful to remove word pairs which con-
sisted solely of stop words. Finally, for maximum
entropy, we deleted any feature that occurred less
than 4 times. Naive Bayes did not benefit from a
frequency-based cutoff.
</bodyText>
<subsectionHeader confidence="0.989326">
7.3 Classifier comparison
</subsectionHeader>
<bodyText confidence="0.998611714285714">
Here we report on our classifiers.
As a baseline model, we simply extracted the first
n sentences from a given document. Figure 1 sum-
marises our results as n varies. In this table, as in all
subsequent tables, P and R are averaged precision
and recall values, whilst F2 is the f2 score of these
averaged values.
</bodyText>
<table confidence="0.996969333333333">
Flat prior Optimised prior
F2 P R F2 P R
8 5 30 20 40 14
25 63 16 36 36 36
28 62 18 39 35 45
35 63 24 42 43 41
</table>
<figureCaption confidence="0.977107">
Figure 2: Results for the maximum entropy model
</figureCaption>
<bodyText confidence="0.9999742">
feature instances in the model, the vast majority are
deeded irrelevant by maximum entropy, and assigned
a zero weight. Only 7086 features (roughly 10% in
total) had non zero weights.
Performance using the optimised prior shows more
balanced results, with an increase in F2 score.
Clearly optimising the prior has helped counter the
categorical behaviour of features in our maximum
entropy classifier.
Figure 3 shows the results we obtained when us-
ing a naive Bayes classifier. As before, the results
show performance with and without the addition of
the optimised prior. Naive Bayes outperforms maxi-
mum entropy when both classifiers do not use a prior.
Performance with and without the prior however, is
worse than the performance of our maximum entropy
classifier with the prior. Evidently, even our rela-
tively simple features interact with each other, and
so approaches such as maximum entropy are required
to fully exploit them.
</bodyText>
<figure confidence="0.927970363636364">
Features
Word pairs
and sent length
and sent position
and discourse
n F2 P R n F2 P R
1 0 0 0 26 16 10 36
6 3 3 2 31 18 12 45
11 19 15 26 36 18 11 53
16 20 16 29 41 17 10 58
21 23 16 38 46 16 9 58
</figure>
<figureCaption confidence="0.970037">
Figure 1: Results for the baseline model
</figureCaption>
<table confidence="0.947413363636364">
Features
Word pairs
and sent length
and sent position
and discourse
Flat prior Optimised prior
F2 P R F2 P R
26 29 23 29 26 32
31 33 28 32 29 35
33 34 33 36 31 43
38 39 37 39 38 40
</table>
<figureCaption confidence="0.986248">
Figure 3: Results for the naive Bayes model
</figureCaption>
<bodyText confidence="0.992645916666667">
Figure 2 shows our results for maximum entropy,
both with and without the prior. Prior optimisation
was with respect to the f2 score. As in subsequent
tables, we show system performance when adding
more and more features.
Performance without the prior is heavily skewed
towards precision. This is because our features are
largely acting categorically: the sheer presence of
some feature is sufficient to influence labelling choice.
Further evidence for this analysis is supported by in-
specting one of the models produced when using the
full set of all feature types. We see that of the 85883
</bodyText>
<subsectionHeader confidence="0.961859">
7.4 Using informative features
</subsectionHeader>
<bodyText confidence="0.999983">
Our previous results showed that maximum entropy
could outperform naive Bayes. However, the differ-
ences, though present, were not large. Clearly, our
feature set was imperfect.4 It is therefore instruc-
tive to see what happens if we had access to an or-
acle who always told us the true status of some un-
seen sentence. To make things more interesting, we
</bodyText>
<footnote confidence="0.984891">
4Another possible reason for the closeness of the re-
sults is the small sample size. There may just not be
enough evidence to reliably estimate dependencies within
the data.
</footnote>
<table confidence="0.997826833333333">
Naive Bayes Maxent
F2 P R F2 P R
30 34 26 32 93 19
35 38 32 99 100 99
40 41 39 100 100 100
43 44 41 99 100 97
</table>
<figureCaption confidence="0.977420666666667">
Figure 4: Results for basic naive Bayes and max-
imum entropy models using dependent informative
features
</figureCaption>
<bodyText confidence="0.6833874">
Features
Word pairs
and sent length
and sent position
and discourse
</bodyText>
<figureCaption confidence="0.694162">
Figure 5: Results for basic naive Bayes and maxi-
mum entropy models using independent informative
features
</figureCaption>
<bodyText confidence="0.999870428571428">
encoded this information in terms of dependent fea-
tures. We simulated this oracle by using two features
which were active whenever a sentence should not
be in the summary; for sentences that should be in-
cluded in the summary, we let either one of those two
features be active, but on a random basis. Our fea-
tures therefore are only informative when the learner
is capable of noting that there are dependencies. We
then repeated our previous maximum entropy and
naive Bayes experiments. Figure 4 summarise our
results.
Unsurprisingly, we see that when features are
highly dependent upon each other, maximum en-
tropy easily outperforms naive Bayes.
Even when we have access to features that are in-
dependent of each other, naive Bayes can still do
worse than maximum entropy. To demonstrate this,
we used a feature that was active whenever a sen-
tence should be in the summary. This feature was
not active on sentences that should not be in the
summary. Figure 5 summarises our results.
As can be seen (figure 5), even when naive Bayes
has access to a perfectly reliable informative feature,
the fact that the other features are not suitably dis-
counted means that performance is worse than that
of maximum entropy. Maximum entropy can dis-
count the other features, and so can take advantage
of reliable features.
</bodyText>
<sectionHeader confidence="0.958628" genericHeader="conclusions">
8 Comments and Future Work
</sectionHeader>
<bodyText confidence="0.999993095238095">
We showed how maximum entropy could be used for
sentence extraction, and in particular, that adding
a prior could deal with the categorical nature of
the features. Maximum entropy, with an opti-
mised prior, did yield marginally better results than
naive Bayes (with and without a similarly optimised
prior). However, the differences were not that great.
Our further experiments with informative features
showed that this lack of difference was probably due
(at least in part) to the actual features used, and not
due to the technique itself.
Our oracle results are an idealisation. A fuller
comparison should use more sophisticated features,
along with more data. As a result of this, we conjec-
ture that should we use a much more sophisticated
feature set, we would expect that the differences be-
tween maximum entropy and naive Bayes would be-
come greater.
Our approach treated sentences largely indepen-
dently of each other. However, abstract-worthy sen-
tences tend to bunch together, particularly at the
beginning and end of a document. We intend cap-
turing this idea by making our approach sequence-
based: future decisions should also be conditioned
on previous choices.
A problem with supervised approaches (such as
ours) is that we need annotated material (Marcu,
1999). This is costly to produce. Future work will
consider weakly supervised approaches (for example
cotraining) as a way of bootstrapping labelled mate-
rial from unlabelled documents (Blum and Mitchell,
1998). Note that there is a close connection between
multi-document summarisation (where many alter-
native documents all consider similar issues) and the
concept of a view in cotraining. We expect that this
redundancy could be exploited as a means of provid-
ing more annotated training material, and so yield
better results.
In summary, maximum entropy can be beneficially
used in sentence extraction. However, one needs to
guard against categorial features. An optimised prior
can provide such help.
</bodyText>
<sectionHeader confidence="0.99189" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999914">
We would like to thank Rob Malouf for supplying the
excellent log-linear estimation code, Simone Teufel
for providing the annotated data, Karen Spark Jones
for a discussion about summarisation, Steve Clark
for spotting textual bugs and the anonymous review-
ers for useful comments.
</bodyText>
<figure confidence="0.927715818181818">
Features
Word pairs
and sent length
and sent position
and discourse
Naive Bayes Maxent
F2 P R F2 P R
84 74 97 25 15 91
85 75 97 100 100 100
84 73 97 100 100 100
84 74 97 100 100 100
</figure>
<sectionHeader confidence="0.760263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657222222223">
Adam Berger, Stephen Della Pietra, and Vin-
cent Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 21{22.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training.
In Proceedings of the Workshop on Computational
Learning Theory. Morgan Kaufmann Publishers.
Branimir K. Boguraev and Mary S. Neff. 2000a. The
effects of analysing cohesion on document sum-
marisation. In Proceedings of the 181h Interna-
tional Conference on Computational Linguistics,
volume 1, pages 76{82, Saarbrucken.
Branmir K. Boguraev and Mary S. Neff. 2000b. Dis-
course Segmentation in Aid of Document Summa-
rization. In Proceedings of the 33 d Hawaii Inter-
national Conference on Systems Science.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS99-12, De-
partment of Computer Science, Brown University.
Stanley F. Chen and Ronald Rosenfeld. 1999.
A Gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple bayesian classifier un-
der zero-one loss. Machine Learning, 29(2-3):103-
130.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mit-
tal, and Jaime G. Carbonell. 1999. Summarizing
text documents: Sentence selection and evaluation
metrics. In Research and Development in Informa-
tion Retrieval, pages 121{128.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A Trainable Document Summarizer. In
Proceedings of the 181h ACM-SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68{73.
J. Lafferty, S. Della Pietra, and V. Della Pietra.
1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 19(4):380-393, April.
Daniel Marcu. 1999. The automatic construction
of large-scale corpora for summarization research.
In Research and Development in Information Re-
trieval, pages 137{144.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classificatio. In
AAAI-98 Workshop on Learning for Text Catego-
rization.
Kamal Nigam, John Lafferty, , and Andrew Mc-
Callum. 1999. Using maximum entropy for text
classification. In IJCAI-99 Workshop on Machine
Learning for Information Filtering,.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1993. Numeri-
cal Recipes in C. the Art of Scientific Computing.
Cambridge University Press, second edition.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Part-Of-Speech Tagger. In Proceed-
ings of Empirical Methods in Natural Lan-
guage, University of Pennsylvania, May. Tagger:
ftp://ftp.cis.upenn.edu/pub/adwait/jmx.
S. Teufel and M. Moens. 1997. Sentence extraction
as a classification task. In ACL/EACL-97 Work-
shop on Intelligent and Scalable Text Summariza-
tion, Madrid, Spain.
Simone Teufel. 2001. Task-Based Evaluation of
Summary Quality: Describing Relationships Be-
tween Scientific Papers. In NAACL Workshop on
Automatic Summarization, Pittsburgh, Pennsyl-
vania, USA, June. Carnegie Mellon University.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.250953">
<note confidence="0.9317655">Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July 2002, pp. 1-8. Association for Computational Linguistics.</note>
<title confidence="0.995015">Using Maximum Entropy for Sentence Extraction</title>
<author confidence="0.900125">Miles</author>
<affiliation confidence="0.971142333333333">Division of University of 2 Buccleuch</affiliation>
<address confidence="0.698751">Edinburgh EH8</address>
<note confidence="0.595823">United Kingdom.</note>
<abstract confidence="0.999440052631579">A maximum entropy classifier can be used to extract sentences from documents. Experiments using technical documents show that such a classifier tends to treat features in a categorical manner. This results in performance that is worse than when extracting sentences using a naive Bayes classifier. Addition of an optimised prior to the maximum entropy classifier improves performance over and above that of naive Bayes (even when naive Bayes is also extended with a similar prior). Further experiments show that, should we have at our disposal extremely informative features, then maximum entropy is able to yield excellent results. Naive Bayes, in contrast, cannot exploit these features and so fundamentally limits sentence extraction performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>21--22</pages>
<contexts>
<context position="3836" citStr="Berger et al., 1996" startWordPosition="587" endWordPosition="590">ecessary independence assumptions. Within the maximum entropy framework, we are able to optimally integrate together whatever sources of knowledge we believe potentially to be useful for the task. Should we use features that are beneficial, then the model will be able to exploit this fact. Should we use features that are irrelevant, then again, the model will be able to notice this, and effectively ignore them. Models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results on a variety of language tasks (Ratnaparkhi, 1996; Berger et al., 1996; Charniak, 1999; Nigam et al., 1999). In this paper, we outline a conditional maximum entropy classification model for sentence extraction. Our model works incrementally, and does not always need to process the entire document before assigning classification.&apos; It discriminates between those sentences which should and should not be extracted. This contrasts with ranking approaches which need to process the entire document before extracting sentences. Because we model whether a sentence should be extracted or not in terms of features that are extracted from the sentence (and its context in the </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 21{22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Computational Learning Theory.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="28787" citStr="Blum and Mitchell, 1998" startWordPosition="4846" endWordPosition="4849">eater. Our approach treated sentences largely independently of each other. However, abstract-worthy sentences tend to bunch together, particularly at the beginning and end of a document. We intend capturing this idea by making our approach sequencebased: future decisions should also be conditioned on previous choices. A problem with supervised approaches (such as ours) is that we need annotated material (Marcu, 1999). This is costly to produce. Future work will consider weakly supervised approaches (for example cotraining) as a way of bootstrapping labelled material from unlabelled documents (Blum and Mitchell, 1998). Note that there is a close connection between multi-document summarisation (where many alternative documents all consider similar issues) and the concept of a view in cotraining. We expect that this redundancy could be exploited as a means of providing more annotated training material, and so yield better results. In summary, maximum entropy can be beneficially used in sentence extraction. However, one needs to guard against categorial features. An optimised prior can provide such help. Acknowledgement We would like to thank Rob Malouf for supplying the excellent log-linear estimation code, </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Workshop on Computational Learning Theory. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir K Boguraev</author>
<author>Mary S Neff</author>
</authors>
<title>The effects of analysing cohesion on document summarisation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 181h International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>76--82</pages>
<contexts>
<context position="2708" citStr="Boguraev and Neff (2000" startWordPosition="410" endWordPosition="413">ence extraction systems are based around simple algorithms which assume independence between those features used to encode the task. A consequence of this assumption is that such approaches are fundamentally unable to exploit dependencies which presumably exist in the features that would be present in an ideal sentence extraction system. This situation may be acceptable when the features used to model sentence extraction are simple. However, it will rapidly become unacceptable when more sophisticated heuristics, with complicated interactions, are brought to bear upon the problem. For example, Boguraev and Neff (2000a) argue that the quality of summarisation can be increased if lexical cohesion factors (rhetorical devices which help achieve cohesion between related document utterances) are modelled by a sentence extraction system. Clearly such devices (for example, lexical repetition, ellipsis, coreference and so on) all contribute towards the general discourse structure of some text and furthermore are related to each other in non-obvious ways. Maximum entropy (log-linear) models, on the other hand, do not make unnecessary independence assumptions. Within the maximum entropy framework, we are able to opt</context>
<context position="16129" citStr="Boguraev and Neff, 2000" startWordPosition="2650" endWordPosition="2653">g our maximum entropy classifier, and then selecting the top n most highly ranked sentences produced slightly worse results than when selecting sentences in terms of classification. 6 Related Work The summarisation literature is large. Here we consider only a representative sample. Kupiec et al. (1995) used Naive Bayes for sentence extraction. They did not consider the role of the prior, nor did they use Naive Bayes for classification. Instead, they used it to rank sentences and selected the top n sentences. The TEXTRACT system included a sentence extraction component that is frequency-based (Boguraev and Neff, 2000b). Whilst the system uses a wide variety of linguistic cues when scoring sentences, it does not combine these scores in an optimal manner. Also, it does not consider interactions between the linguistic cues. Goldstein et al. (1999) used a centroid similarity measure to score sentences. They do not appear to have optimised their metric, nor do they deal with statistical dependencies between their features. 7 Experiments Summarisation evaluation is a hard task, principally because the notion of an objective summary is illdefined. That aside, in order to compare our various systems, we used an i</context>
</contexts>
<marker>Boguraev, Neff, 2000</marker>
<rawString>Branimir K. Boguraev and Mary S. Neff. 2000a. The effects of analysing cohesion on document summarisation. In Proceedings of the 181h International Conference on Computational Linguistics, volume 1, pages 76{82, Saarbrucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branmir K Boguraev</author>
<author>Mary S Neff</author>
</authors>
<title>Discourse Segmentation in Aid of Document Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 33 d Hawaii International Conference on Systems Science.</booktitle>
<contexts>
<context position="2708" citStr="Boguraev and Neff (2000" startWordPosition="410" endWordPosition="413">ence extraction systems are based around simple algorithms which assume independence between those features used to encode the task. A consequence of this assumption is that such approaches are fundamentally unable to exploit dependencies which presumably exist in the features that would be present in an ideal sentence extraction system. This situation may be acceptable when the features used to model sentence extraction are simple. However, it will rapidly become unacceptable when more sophisticated heuristics, with complicated interactions, are brought to bear upon the problem. For example, Boguraev and Neff (2000a) argue that the quality of summarisation can be increased if lexical cohesion factors (rhetorical devices which help achieve cohesion between related document utterances) are modelled by a sentence extraction system. Clearly such devices (for example, lexical repetition, ellipsis, coreference and so on) all contribute towards the general discourse structure of some text and furthermore are related to each other in non-obvious ways. Maximum entropy (log-linear) models, on the other hand, do not make unnecessary independence assumptions. Within the maximum entropy framework, we are able to opt</context>
<context position="16129" citStr="Boguraev and Neff, 2000" startWordPosition="2650" endWordPosition="2653">g our maximum entropy classifier, and then selecting the top n most highly ranked sentences produced slightly worse results than when selecting sentences in terms of classification. 6 Related Work The summarisation literature is large. Here we consider only a representative sample. Kupiec et al. (1995) used Naive Bayes for sentence extraction. They did not consider the role of the prior, nor did they use Naive Bayes for classification. Instead, they used it to rank sentences and selected the top n sentences. The TEXTRACT system included a sentence extraction component that is frequency-based (Boguraev and Neff, 2000b). Whilst the system uses a wide variety of linguistic cues when scoring sentences, it does not combine these scores in an optimal manner. Also, it does not consider interactions between the linguistic cues. Goldstein et al. (1999) used a centroid similarity measure to score sentences. They do not appear to have optimised their metric, nor do they deal with statistical dependencies between their features. 7 Experiments Summarisation evaluation is a hard task, principally because the notion of an objective summary is illdefined. That aside, in order to compare our various systems, we used an i</context>
</contexts>
<marker>Boguraev, Neff, 2000</marker>
<rawString>Branmir K. Boguraev and Mary S. Neff. 2000b. Discourse Segmentation in Aid of Document Summarization. In Proceedings of the 33 d Hawaii International Conference on Systems Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>1999</date>
<tech>Technical Report CS99-12,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="3852" citStr="Charniak, 1999" startWordPosition="591" endWordPosition="592"> assumptions. Within the maximum entropy framework, we are able to optimally integrate together whatever sources of knowledge we believe potentially to be useful for the task. Should we use features that are beneficial, then the model will be able to exploit this fact. Should we use features that are irrelevant, then again, the model will be able to notice this, and effectively ignore them. Models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results on a variety of language tasks (Ratnaparkhi, 1996; Berger et al., 1996; Charniak, 1999; Nigam et al., 1999). In this paper, we outline a conditional maximum entropy classification model for sentence extraction. Our model works incrementally, and does not always need to process the entire document before assigning classification.&apos; It discriminates between those sentences which should and should not be extracted. This contrasts with ranking approaches which need to process the entire document before extracting sentences. Because we model whether a sentence should be extracted or not in terms of features that are extracted from the sentence (and its context in the document), we do</context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A maximum-entropyinspired parser. Technical Report CS99-12, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="14125" citStr="Chen and Rosenfeld, 1999" startWordPosition="2326" endWordPosition="2329"> of the other value. Section 7 describes the held-out optimisation strategy used in our experiments. Should we decide to use a more elaborate prior (for example, one which was also sensitive to properties of documents) then we would need to use a multidimensional function minimisation method. Note that we have not simultaneously optimised the likelihood and prior probabilities. This means that we do not necessarily find the optimal maximum a posteriori (MAP) solution. It is possible to integrate into maximum entropy estimation (simple) conjugate priors that do allow MAP solutions to be found (Chen and Rosenfeld, 1999). Although it is an open question whether more complex priors can be directly integrated, future work ought to consider the efficacy of such approaches in the context of summarisation. 5 Summary size Determining the size of the summary is an important consideration for summarisation. Frequently, this is carried out dynamically, and specified by the user. For example, when there is limited opportunity to display long summaries a user might want a terse summary. Alternatively, when recall is important, a user might prefer a longer summary. Usually, systems rank all sentences in terms of how abst</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Michael J Pazzani</author>
</authors>
<title>On the optimality of the simple bayesian classifier under zero-one loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="6442" citStr="Domingos and Pazzani, 1997" startWordPosition="1009" endWordPosition="1013">o produces worse results than maximum entropy.2 Incremental classification means that a document is processed from start-to-finish and decisions are made as soon as sentences are encountered. Some of our features (in particular, those which encode sentence position in a document) do require processing the entire document. Using such features prevents true incremental processing. However, it is trivial to remove such features and so ensure true incrementality. 2 A a reviewer commented, under certain circumstances, naive Bayes can do well even when there are strong dependencies within features (Domingos and Pazzani, 1997). For example, when the sample size is small, naive Bayes can be competitive with more sophisticated approaches such as maximum entropy. Given this, a fuller comparison of naive Bayes and maximum entropy for sentence extraction requires considering sample size in addition to the choice of features. The rest of this paper is as follows. Section 2 outlines the general framework for sentence extraction using maximum entropy modelling. Section 3 presents our naive Bayes classifier (which is used as a comparison with maximum entropy). We then show in section 4 how both our maximum entropy and naive</context>
</contexts>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>Pedro Domingos and Michael J. Pazzani. 1997. On the optimality of the simple bayesian classifier under zero-one loss. Machine Learning, 29(2-3):103-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu O Mittal</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="16361" citStr="Goldstein et al. (1999)" startWordPosition="2689" endWordPosition="2692">ge. Here we consider only a representative sample. Kupiec et al. (1995) used Naive Bayes for sentence extraction. They did not consider the role of the prior, nor did they use Naive Bayes for classification. Instead, they used it to rank sentences and selected the top n sentences. The TEXTRACT system included a sentence extraction component that is frequency-based (Boguraev and Neff, 2000b). Whilst the system uses a wide variety of linguistic cues when scoring sentences, it does not combine these scores in an optimal manner. Also, it does not consider interactions between the linguistic cues. Goldstein et al. (1999) used a centroid similarity measure to score sentences. They do not appear to have optimised their metric, nor do they deal with statistical dependencies between their features. 7 Experiments Summarisation evaluation is a hard task, principally because the notion of an objective summary is illdefined. That aside, in order to compare our various systems, we used an intrinsic evaluation approach. Our summaries were evaluated using the standard f2 score: 2pr f2 = p + r where: r = Recall p = Precision j = Number of correct sentences in summary k = Number of sentences in summary m = Number of corre</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and Jaime G. Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Research and Development in Information Retrieval, pages 121{128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A Trainable Document Summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 181h ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="15809" citStr="Kupiec et al. (1995)" startWordPosition="2598" endWordPosition="2601">bel but can also assign label probabilities) and select the top n most highly ranked sentences. Within our classification approach, the optimised prior plays a similar role to the user-defined number of sentences that a ranking approach might return. Experiments (not reported here) showed that ranking sentences using our maximum entropy classifier, and then selecting the top n most highly ranked sentences produced slightly worse results than when selecting sentences in terms of classification. 6 Related Work The summarisation literature is large. Here we consider only a representative sample. Kupiec et al. (1995) used Naive Bayes for sentence extraction. They did not consider the role of the prior, nor did they use Naive Bayes for classification. Instead, they used it to rank sentences and selected the top n sentences. The TEXTRACT system included a sentence extraction component that is frequency-based (Boguraev and Neff, 2000b). Whilst the system uses a wide variety of linguistic cues when scoring sentences, it does not combine these scores in an optimal manner. Also, it does not consider interactions between the linguistic cues. Goldstein et al. (1999) used a centroid similarity measure to score sen</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. In Proceedings of the 181h ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 68{73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<contexts>
<context position="8992" citStr="Lafferty et al., 1997" startWordPosition="1453" endWordPosition="1456">elves to integer-valued functions. An example feature might be as follows: 1 if s contains the phrase in this paper and c is the label keep 0 otherwise Features are related to each other through weights (as can be seen in equation 1, where some feature fi has a weight Ai). Weights are real-valued numbers. When a closed form solution cannot be found, (1) 8&lt;&gt;&gt; &gt;&gt;: fi(c, s) = (3) they are determined by numerical optimisation techniques. In this paper, we use conjugate gradient descent to find the optimal set of weights. Conjugate Gradient descent converges faster than Improved Iterative Scaling (Lafferty et al., 1997), and empirically we find that it is numerically more stable. 2.2 Maximum Entropy Classification When classifying sentences with maximum entropy, we use the equation: label(s) = argmaxcECP(c j s) (4) In practice, we are not interested in the probability of a label given a sentence. Instead we use the unnormalised score: label(s) = argmaxcEC exp(X Aifi(c, s)) (5) i Note that this maximum entropy classifier assumes a uniform prior. Section 4 shows how a non-uniform prior is used in place of this uniform prior. We now present our basic naive Bayes classifier. Afterwards, we extend this classifier</context>
</contexts>
<marker>Lafferty, Pietra, Pietra, 1997</marker>
<rawString>J. Lafferty, S. Della Pietra, and V. Della Pietra. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="28583" citStr="Marcu, 1999" startWordPosition="4818" endWordPosition="4819"> data. As a result of this, we conjecture that should we use a much more sophisticated feature set, we would expect that the differences between maximum entropy and naive Bayes would become greater. Our approach treated sentences largely independently of each other. However, abstract-worthy sentences tend to bunch together, particularly at the beginning and end of a document. We intend capturing this idea by making our approach sequencebased: future decisions should also be conditioned on previous choices. A problem with supervised approaches (such as ours) is that we need annotated material (Marcu, 1999). This is costly to produce. Future work will consider weakly supervised approaches (for example cotraining) as a way of bootstrapping labelled material from unlabelled documents (Blum and Mitchell, 1998). Note that there is a close connection between multi-document summarisation (where many alternative documents all consider similar issues) and the concept of a view in cotraining. We expect that this redundancy could be exploited as a means of providing more annotated training material, and so yield better results. In summary, maximum entropy can be beneficially used in sentence extraction. H</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. In Research and Development in Information Retrieval, pages 137{144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classificatio.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="10098" citStr="McCallum and Nigam, 1998" startWordPosition="1631" endWordPosition="1634"> in place of this uniform prior. We now present our basic naive Bayes classifier. Afterwards, we extend this classifier with a nonuniform prior. 3 Naive Bayes Classification As an alternative to maximum entropy, we also investigated a naive Bayes classifier. Unlike maximum entropy, naive Bayes assumes features are conditionally independent of each other. So, comparing the two together will give an indication of the level of statistical dependencies which exist between features in the sentence extraction domain. For our experiments, we used a variant of the multi-variate Bernoulli event model (McCallum and Nigam, 1998). In particular, we did not consider features that are absent in some example. This allows us to avoid summing over all features in the model for each example. Note that our maximum entropy model also did not consider absent features. Within our naive Bayes approach, the probability of a label given the sentence is as follows: As before, s is some sentence, c the label, and gi is some active feature describing sentence s. Naive Bayes models can be estimated in a closed form by simple counting. For features which have zero counts, we use add-k smoothing (where k is a small number less than one)</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. A comparison of event models for naive bayes text classificatio. In AAAI-98 Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>John Lafferty</author>
</authors>
<title>Using maximum entropy for text classification.</title>
<date>1999</date>
<booktitle>In IJCAI-99 Workshop on Machine Learning for Information Filtering,.</booktitle>
<marker>Nigam, Lafferty, 1999</marker>
<rawString>Kamal Nigam, John Lafferty, , and Andrew McCallum. 1999. Using maximum entropy for text classification. In IJCAI-99 Workshop on Machine Learning for Information Filtering,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C. the Art of Scientific Computing.</title>
<date>1993</date>
<publisher>Cambridge University Press,</publisher>
<note>second edition.</note>
<contexts>
<context position="13399" citStr="Press et al., 1993" startWordPosition="2209" endWordPosition="2212">ised. Since the choice of objective function is up to us, we can easily optimise the classifier in any way we decide. For example, we could optimise for recall by using as our objective function an f-measure that weighted recall more highly than precision. In this paper, we optimise the prior using as an objective function the f2 score of the classifier (section 7 details this score). Our prior therefore does not reflect relative frequencies of labels (as found in some corpus). We now need to optimise our prior. Brent&apos;s one dimensional function minimisation method is well suited to this task (Press et al., 1993), since for a random variable taking two values, the probability of one value can be defined in terms of the other value. Section 7 describes the held-out optimisation strategy used in our experiments. Should we decide to use a more elaborate prior (for example, one which was also sensitive to properties of documents) then we would need to use a multidimensional function minimisation method. Note that we have not simultaneously optimised the likelihood and prior probabilities. This means that we do not necessarily find the optimal maximum a posteriori (MAP) solution. It is possible to integrat</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1993</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1993. Numerical Recipes in C. the Art of Scientific Computing. Cambridge University Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part-Of-Speech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of Empirical Methods in Natural</booktitle>
<tech>Tagger: ftp://ftp.cis.upenn.edu/pub/adwait/jmx.</tech>
<institution>Language, University of Pennsylvania,</institution>
<contexts>
<context position="3815" citStr="Ratnaparkhi, 1996" startWordPosition="585" endWordPosition="586">nd, do not make unnecessary independence assumptions. Within the maximum entropy framework, we are able to optimally integrate together whatever sources of knowledge we believe potentially to be useful for the task. Should we use features that are beneficial, then the model will be able to exploit this fact. Should we use features that are irrelevant, then again, the model will be able to notice this, and effectively ignore them. Models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results on a variety of language tasks (Ratnaparkhi, 1996; Berger et al., 1996; Charniak, 1999; Nigam et al., 1999). In this paper, we outline a conditional maximum entropy classification model for sentence extraction. Our model works incrementally, and does not always need to process the entire document before assigning classification.&apos; It discriminates between those sentences which should and should not be extracted. This contrasts with ranking approaches which need to process the entire document before extracting sentences. Because we model whether a sentence should be extracted or not in terms of features that are extracted from the sentence (an</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger. In Proceedings of Empirical Methods in Natural Language, University of Pennsylvania, May. Tagger: ftp://ftp.cis.upenn.edu/pub/adwait/jmx.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="19321" citStr="Teufel and Moens, 1997" startWordPosition="3206" endWordPosition="3209"> the Comp-lang preprint archive, and semi-automatically converted from L�T�,Xto XML. The XML annotated documents were then additionally manually markedup with tags indicating the status of various sentences. This document set is modest in size. On the other hand, the actual documents are longer than newswire messages typically used for summarisation tasks. Also, the documents show variation in style. For example, some documents are written by nonnative speakers, some by students, some by multiple authors and so on. Summarisation is therefore hard. 3A superset of the documents is described in (Teufel and Moens, 1997). Here are some properties of the documents. On average, each document contained 8 sentences that were marked as being abstract-worthy (standard deviation of 3.1). The documents on average each contained in total 174 sentences (standard deviation 50.7). Here, a `sentence&apos; is either any sequence of words that happened to be in a title, or else any sequence of words in the rest of the document. As can be seen, the summaries are not uniformly long. Also, the documents vary considerably in length. Summary size is therefore not constant. 7.2 Features We used the following, fairly standard features </context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>S. Teufel and M. Moens. 1997. Sentence extraction as a classification task. In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Task-Based Evaluation of Summary Quality: Describing Relationships Between Scientific Papers.</title>
<date>2001</date>
<booktitle>In NAACL Workshop on Automatic Summarization,</booktitle>
<institution>Carnegie Mellon University.</institution>
<location>Pittsburgh, Pennsylvania, USA,</location>
<contexts>
<context position="18618" citStr="Teufel (2001)" startWordPosition="3099" endWordPosition="3100">d-out set. The prior is estimated using Brent&apos;s line minimisation method, when training using T3 and evaluating on T4. T3 consisted of 60 documents and T4 consisted of 10 documents. 3. Results are then presented using a model trained on T1, with the prior just found, and evaluated using T2. T1 is therefore the training set and T2 is the testing set. Results are also presented using a flat prior. 4. The whole process is then repeated after randomising the documents. The final results are then averaged over these n runs. We set n to 40. 7.1 Document set For data, we used the same documents that Teufel (2001) used in her experiments.3 In brief, these were 80 conference papers, taken from the Comp-lang preprint archive, and semi-automatically converted from L�T�,Xto XML. The XML annotated documents were then additionally manually markedup with tags indicating the status of various sentences. This document set is modest in size. On the other hand, the actual documents are longer than newswire messages typically used for summarisation tasks. Also, the documents show variation in style. For example, some documents are written by nonnative speakers, some by students, some by multiple authors and so on.</context>
</contexts>
<marker>Teufel, 2001</marker>
<rawString>Simone Teufel. 2001. Task-Based Evaluation of Summary Quality: Describing Relationships Between Scientific Papers. In NAACL Workshop on Automatic Summarization, Pittsburgh, Pennsylvania, USA, June. Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>