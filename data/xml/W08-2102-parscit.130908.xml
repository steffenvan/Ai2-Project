<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.979323">
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing
</title>
<author confidence="0.791748">
Xavier Carreras Michael Collins Terry Koo
</author>
<note confidence="0.639454">
MIT CSAIL, Cambridge, MA 02139, USA
</note>
<email confidence="0.99801">
{carreras,mcollins,maestro}@csail.mit.edu
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938764705882">
We describe a parsing approach that makes use
of the perceptron algorithm, in conjunction with
dynamic programming methods, to recover full
constituent-based parse trees. The formalism allows
a rich set of parse-tree features, including PCFG-
based features, bigram and trigram dependency fea-
tures, and surface features. A severe challenge in
applying such an approach to full syntactic pars-
ing is the efficiency of the parsing algorithms in-
volved. We show that efficient training is feasi-
ble, using a Tree Adjoining Grammar (TAG) based
parsing formalism. A lower-order dependency pars-
ing model is used to restrict the search space of the
full model, thereby making it efficient. Experiments
on the Penn WSJ treebank show that the model
achieves state-of-the-art performance, for both con-
stituent and dependency accuracy.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98367475">
In global linear models (GLMs) for structured pre-
diction, (e.g., (Johnson et al., 1999; Lafferty et al.,
2001; Collins, 2002; Altun et al., 2003; Taskar et
al., 2004)), the optimal label y* for an input x is
</bodyText>
<equation confidence="0.9333755">
y* = arg max w · f(x, y) (1)
yEY(X)
</equation>
<bodyText confidence="0.999651454545454">
where Y(x) is the set of possible labels for the in-
put x; f(x, y) E Rd is a feature vector that rep-
resents the pair (x, y); and w is a parameter vec-
tor. This paper describes a GLM for natural lan-
guage parsing, trained using the averaged percep-
tron. The parser we describe recovers full syntac-
tic representations, similar to those derived by a
probabilistic context-free grammar (PCFG). A key
motivation for the use of GLMs in parsing is that
they allow a great deal of flexibility in the features
which can be included in the definition of f(x, y).
</bodyText>
<note confidence="0.723272">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967254666666667">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.99976272">
A critical problem when training a GLM for
parsing is the computational complexity of the
inference problem. The averaged perceptron re-
quires the training set to be repeatedly decoded
under the model; under even a simple PCFG rep-
resentation, finding the arg max in Eq. 1 requires
O(n3G) time, where n is the length of the sen-
tence, and G is a grammar constant. The average
sentence length in the data set we use (the Penn
WSJ treebank) is over 23 words; the grammar con-
stant G can easily take a value of 1000 or greater.
These factors make exact inference algorithms vir-
tually intractable for training or decoding GLMs
for full syntactic parsing.
As a result, in spite of the potential advantages
of these methods, there has been very little previ-
ous work on applying GLMs for full parsing with-
out the use of fairly severe restrictions or approxi-
mations. For example, the model in (Taskar et al.,
2004) is trained on only sentences of 15 words or
less; reranking models (Collins, 2000; Charniak
and Johnson, 2005) restrict Y(x) to be a small set
of parses from a first-pass parser; see section 1.1
for discussion of other related work.
The following ideas are central to our approach:
</bodyText>
<sectionHeader confidence="0.464077" genericHeader="method">
(1) A TAG-based, splittable grammar. We
</sectionHeader>
<bodyText confidence="0.995523583333333">
describe a novel, TAG-based parsing formalism
that allows full constituent-based trees to be recov-
ered. A driving motivation for our approach comes
from the flexibility of the feature-vector represen-
tations f(x, y) that can be used in the model. The
formalism that we describe allows the incorpora-
tion of: (1) basic PCFG-style features; (2) the
use of features that are sensitive to bigram depen-
dencies between pairs of words; and (3) features
that are sensitive to trigram dependencies. Any
of these feature types can be combined with sur-
face features of the sentence x, in a similar way
</bodyText>
<page confidence="0.969597">
9
</page>
<note confidence="0.8798825">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9–16
Manchester, August 2008
</note>
<bodyText confidence="0.999438482758621">
to the use of surface features in conditional ran-
dom fields (Lafferty et al., 2001). Crucially, in
spite of these relatively rich representations, the
formalism can be parsed efficiently (in O(n4G)
time) using dynamic-programming algorithms de-
scribed by Eisner (2000) (unlike many other TAG-
related approaches, our formalism is “splittable”
in the sense described by Eisner, leading to more
efficient parsing algorithms).
(2) Use of a lower-order model for pruning.
The O(n4G) running time of the TAG parser is
still too expensive for efficient training with the
perceptron. We describe a method that leverages
a simple, first-order dependency parser to restrict
the search space of the TAG parser in training and
testing. The lower-order parser runs in O(n3H)
time where H ≪ G; experiments show that it is
remarkably effective in pruning the search space
of the full TAG parser.
Experiments on the Penn WSJ treebank show
that the model recovers constituent structures with
higher accuracy than the approaches of (Charniak,
2000; Collins, 2000; Petrov and Klein, 2007),
and with a similar level of performance to the
reranking parser of (Charniak and Johnson, 2005).
The model also recovers dependencies with sig-
nificantly higher accuracy than state-of-the-art de-
pendency parsers such as (Koo et al., 2008; Mc-
Donald and Pereira, 2006).
</bodyText>
<subsectionHeader confidence="0.60858">
1.1 Related Work
</subsectionHeader>
<bodyText confidence="0.999943176470588">
Previous work has made use of various restrictions
or approximations that allow efficient training of
GLMs for parsing. This section describes the rela-
tionship between our work and this previous work.
In reranking approaches, a first-pass parser
is used to enumerate a small set of candidate
parses for an input sentence; the reranking model,
which is a GLM, is used to select between these
parses (e.g., (Ratnaparkhi et al., 1994; Johnson et
al., 1999; Collins, 2000; Charniak and Johnson,
2005)). A crucial advantage of our approach is that
it considers a very large set of alternatives in Y(x),
and can thereby avoid search errors that may be
made in the first-pass parser.1
Another approach that allows efficient training
of GLMs is to use simpler syntactic representa-
tions, in particular dependency structures (McDon-
</bodyText>
<footnote confidence="0.9980208">
1Some features used within reranking approaches may be
difficult to incorporate within dynamic programming, but it
is nevertheless useful to make use of GLMs in the dynamic-
programming stage of parsing. Our parser could, of course,
be used as the first-stage parser in a reranking approach.
</footnote>
<bodyText confidence="0.997760172413793">
ald et al., 2005). Dependency parsing can be
implemented in O(n3) time using the algorithms
of Eisner (2000). In this case there is no gram-
mar constant, and parsing is therefore efficient. A
disadvantage of these approaches is that they do
not recover full, constituent-based syntactic struc-
tures; the increased linguistic detail in full syntac-
tic structures may be useful in NLP applications,
or may improve dependency parsing accuracy, as
is the case in our experiments.2
There has been some previous work on GLM
approaches for full syntactic parsing that make use
of dynamic programming. Taskar et al. (2004)
describe a max-margin approach; however, in this
work training sentences were limited to be of 15
words or less. Clark and Curran (2004) describe
a log-linear GLM for CCG parsing, trained on the
Penn treebank. This method makes use of paral-
lelization across an 18 node cluster, together with
up to 25GB of memory used for storage of dy-
namic programming structures for training data.
Clark and Curran (2007) describe a perceptron-
based approach for CCG parsing which is consid-
erably more efficient, and makes use of a super-
tagging model to prune the search space of the full
parsing model. Recent work (Petrov et al., 2007;
Finkel et al., 2008) describes log-linear GLMs ap-
plied to PCFG representations, but does not make
use of dependency features.
</bodyText>
<sectionHeader confidence="0.902631" genericHeader="method">
2 The TAG-Based Parsing Model
</sectionHeader>
<subsectionHeader confidence="0.920831">
2.1 Derivations
</subsectionHeader>
<bodyText confidence="0.999154533333333">
This section describes the idea of derivations in
our parsing formalism. As in context-free gram-
mars or TAGs, a derivation in our approach is a
data structure that specifies the sequence of opera-
tions used in combining basic (elementary) struc-
tures in a grammar, to form a full parse tree. The
parsing formalism we use is related to the tree ad-
joining grammar (TAG) formalisms described in
(Chiang, 2003; Shen and Joshi, 2005). However,
an important difference of our work from this pre-
vious work is that our formalism is defined to be
“splittable”, allowing use of the efficient parsing
algorithms of Eisner (2000).
A derivation in our model is a pair (E, D) where
E is a set of spines, and D is a set of dependencies
</bodyText>
<footnote confidence="0.9997012">
2Note however that the lower-order parser that we use to
restrict the search space of the TAG-based parser is based on
the work of McDonald et al. (2005). See also (Sagae et al.,
2007) for a method that uses a dependency parser to restrict
the search space of a more complex HPSG parser.
</footnote>
<page confidence="0.997813">
10
</page>
<figureCaption confidence="0.999861">
Figure 1: Two example trees.
</figureCaption>
<bodyText confidence="0.958205333333333">
specifying how the spines are combined to form
a parse tree. The spines are similar to elementary
trees in TAG. Some examples are as follows:
</bodyText>
<equation confidence="0.952431333333333">
NP S NP
John VBD cake
ate
</equation>
<bodyText confidence="0.999961230769231">
These structures do not have substitution nodes, as
is common in TAGs.3 Instead, the spines consist
of a lexical anchor together with a series of unary
projections, which usually correspond to different
X-bar levels associated with the anchor.
The operations used to combine spines are sim-
ilar to the TAG operations of adjunction and sis-
ter adjunction. We will call these operations regu-
lar adjunction (r-adjunction) and sister adjunction
(s-adjunction). As one example, the cake spine
shown above can be s-adjoined into the VP node of
the ate spine, to form the tree shown in figure 1(a).
In contrast, if we use the r-adjunction operation to
adjoin the cake tree into the VP node, we get a dif-
ferent structure, which has an additional VP level
created by the r-adjunction operation: the resulting
tree is shown in figure 1(b). The r-adjunction op-
eration is similar to the usual adjunction operation
in TAGs, but has some differences that allow our
grammars to be splittable; see section 2.3 for more
discussion.
We now give formal definitions of the sets E and
D. Take x to be a sentence consisting of n + 1
words, x0 ... xn, where x0 is a special root sym-
bol, which we will denote as *. A derivation for the
input sentence x consists of a pair (E, D), where:
</bodyText>
<listItem confidence="0.68675325">
• E is a set of (n + 1) tuples of the form (i, η),
where i E {0 ... n} is an index of a word in the
sentence, and η is the spine associated with the
word xi. The set E specifies one spine for each
of the (n + 1) words in the sentence. Where it is
3It would be straightforward to extend the approach to in-
clude substitution nodes, and a substitution operation.
clear from context, we will use ηi to refer to the
spine in E corresponding to the i’th word.
• D is a set of n dependencies. Each depen-
dency is a tuple (h, m, l). Here h is the index of
the head-word of the dependency, corresponding
to the spine ηh which contains a node that is being
adjoined into. m is the index of the modifier-word
of the dependency, corresponding to the spine ηm
which is being adjoined into ηh. l is a label.
</listItem>
<bodyText confidence="0.9999715">
The label l is a tuple (POS, A, ηh, ηm, L). ηh and
ηm are the head and modifier spines that are be-
ing combined. POS specifies which node in ηh is
being adjoined into. A is a binary flag specifying
whether the combination operation being used is s-
adjunction or r-adjunction. L is a binary flag spec-
ifying whether or not any “previous” modifier has
been r-adjoined into the position POS in ηh. By a
previous modifier, we mean a modifier m′ that was
adjoined from the same direction as m (i.e., such
that h &lt; m′ &lt; m or m &lt; m′ &lt; h).
It would be sufficient to define l to be the pair
(POS, A)—the inclusion of ηh, ηm and L adds re-
dundant information that can be recovered from
the set E, and other dependencies in D—but it
will be convenient to include this information in
the label. In particular, it is important that given
this definition of l, it is possible to define a func-
tion GRM(l) that maps a label l to a triple of non-
terminals that represents the grammatical relation
between m and h in the dependency structure. For
example, in the tree shown in figure 1(a), the gram-
matical relation between cake and ate is the triple
GRM(l) = (VP VBD NP). In the tree shown in
figure 1(b), the grammatical relation between cake
and ate is the triple GRM(l) = (VP VP NP).
The conditions under which a pair (E, D) forms
a valid derivation for a sentence x are similar to
those in conventional LTAGs. Each (i, η) E E
must be such that η is an elementary tree whose
anchor is the word xi. The dependencies D must
form a directed, projective tree spanning words
0 ... n, with * at the root of this tree, as is also
the case in previous work on discriminative ap-
proches to dependency parsing (McDonald et al.,
2005). We allow any modifier tree ηm to adjoin
into any position in any head tree ηh, but the de-
pendencies D must nevertheless be coherent—for
example they must be consistent with the spines in
E, and they must be nested correctly.4 We will al-
</bodyText>
<footnote confidence="0.9161645">
4For example, closer modifiers to a particular head must
adjoin in at the same or a lower spine position than modifiers
</footnote>
<figure confidence="0.999672789473684">
(a) S (b) S
VP VP
VBD
ate
NP
NN
cake
VP
VBD
ate
NP
NN
cake
VP
NNP
NN
ADVP ADVP
RB RB
quickly luckily
</figure>
<page confidence="0.994111">
11
</page>
<bodyText confidence="0.999375333333333">
low multiple modifier spines to s-adjoin or r-adjoin (a)
into the same node in a head spine; see section 2.3
for more details.
</bodyText>
<subsectionHeader confidence="0.999952">
2.2 A Global Linear Model
</subsectionHeader>
<bodyText confidence="0.9959915">
The model used for parsing with this approach is
a global linear model. For a given sentence x, we
define Y(x) to be the set of valid derivations for x,
where each y E Y(x) is a pair (E, D) as described
in the previous section. A function f maps (x, y)
pairs to feature-vectors f(x, y) E Rd. The param-
eter vector w is also a vector in Rd. Given these
definitions, the optimal derivation for an input sen-
tence x is y* = arg maxyEY(X) w · f(x, y).
We now come to how the feature-vector f(x, y)
is defined in our approach. A simple “first-order”
model would define
</bodyText>
<equation confidence="0.65500725">
f(x, y) = � e(x, (i, ,q)) +
(i,q)EE(y)
ate NN today
cake
</equation>
<figureCaption confidence="0.968981">
Figure 2: Two Example Trees
</figureCaption>
<figure confidence="0.999153547619048">
cake
VP
NP
ate
luckily
(b) S
ADVP
NNP
VP
RB
John
VBD
S
VP
VBD
VP
NP
NP
NN
ADVP
RB
quickly
VP
S
NP VP
NNP
NN
ate
VP
John
ADVP
RB
luckily
NP
NN
today
ADVP
RB
quickly
VBD
VP
NP
</figure>
<bodyText confidence="0.941596866666667">
� d(x, (h, m, l)) (2) Figure 3: An example tree, formed by a combina-
(h,m,l)ED(y) tion of the two structures in figure 2.
Here we use E(y) and D(y) to respectively refer
to the set of spines and dependencies in y. The
function e maps a sentence x paired with a spine
(i, ,q) to a feature vector. The function d maps de-
pendencies within y to feature vectors. This de-
composition is similar to the first-order model of
McDonald et al. (2005), but with the addition of
the e features.
We will extend our model to include higher-
order features, in particular features based on sib-
ling dependencies (McDonald and Pereira, 2006),
and grandparent dependencies, as in (Carreras,
2007). If y = (E, D) is a derivation, then:
</bodyText>
<listItem confidence="0.9948927">
• 5(y) is a set of sibling dependencies. Each
sibling dependency is a tuple (h, m,l, s). For each
(h, m,l, s) E 5 the tuple (h, m, l) is an element of
D; there is one member of 5 for each member of
D. The index s is the index of the word that was
adjoined to the spine for h immediately before m
(or the NULL symbol if no previous adjunction has
taken place).
• G(y) is a set of grandparent dependencies of
type 1. Each type 1 grandparent dependency is a
tuple (h, m,l, g). There is one member of G for
every member of D. The additional information,
the index g, is the index of the word that is the first
modifier to the right of the spine form.
that are further from the head.
• Q(y) is an additional set of grandparent de-
pendencies, of type 2. Each of these dependencies
is a tuple (h, m,l, q). Again, there is one member
of Q for every member of D. The additional infor-
mation, the index q, is the index of the word that is
</listItem>
<bodyText confidence="0.9941015">
the first modifier to the left of the spine for m.
The feature-vector definition then becomes:
</bodyText>
<equation confidence="0.997834875">
f(x, y) = E e(x, (i, η)) +
(i,η)EE(y)
E d(x, (h, m, l)) + s(x, (h, m, l, s)) +
(h,m,l)ED(y) (h,m,l,s)ES(y)
E E
g(x, (h, m, l, g)) + q(x, (h, m, l, q))
(h,m,l,g)EG(y) (h,m,l,q)EQ(y)
(3)
</equation>
<bodyText confidence="0.9999185">
where s, g and q are feature vectors corresponding
to the new, higher-order elements.5
</bodyText>
<subsectionHeader confidence="0.999407">
2.3 Recovering Parse Trees from Derivations
</subsectionHeader>
<bodyText confidence="0.9999966">
As in TAG approaches, there is a mapping from
derivations (E, D) to parse trees (i.e., the type of
trees generated by a context-free grammar). In our
case, we map a spine and its dependencies to a con-
stituent structure by first handling the dependen-
</bodyText>
<footnote confidence="0.937605666666667">
5We also added constituent-boundary features to the
model, which is a simple change that led to small improve-
ments on validation data; for brevity we omit the details.
</footnote>
<page confidence="0.997096">
12
</page>
<bodyText confidence="0.999970947368421">
cies on each side separately and then combining
the left and right sides.
First, it is straightforward to build the con-
stituent structure resulting from multiple adjunc-
tions on the same side of a spine. As one exam-
ple, the structure in figure 2(a) is formed by first
s-adjoining the spine with anchor cake into the VP
node of the spine for ate, then r-adjoining spines
anchored by today and quickly into the same node,
where all three modifier words are to the right of
the head word. Notice that each r-adjunction op-
eration creates a new VP level in the tree, whereas
s-adjunctions do not create a new level. Now con-
sider a tree formed by first r-adjoining a spine for
luckily into the VP node for ate, followed by s-
adjoining the spine for John into the S node, in
both cases where the modifiers are to the left of
the head. In this case the structure that would be
formed is shown in figure 2(b).
Next, consider combining the left and right
structures of a spine. The main issue is how to
handle multiple r-adjunctions or s-adjunctions on
both sides of a node in a spine, because our deriva-
tions do not specify how adjunctions from different
sides embed with each other. In our approach, the
combination operation preserves the height of the
different modifiers from the left and right direc-
tions. To illustrate this, figure 3 shows the result
of combining the two structures in figure 2. The
combination of the left and right modifier struc-
tures has led to flat structures, for example the rule
VP → ADVP VP NP in the above tree.
Note that our r-adjunction operation is different
from the usual adjunction operation in TAGs, in
that “wrapping” adjunctions are not possible, and
r-adjunctions from the left and right directions are
independent from each other; because of this our
grammars are splittable.
</bodyText>
<sectionHeader confidence="0.99749" genericHeader="method">
3 Parsing Algorithms
</sectionHeader>
<subsectionHeader confidence="0.999974">
3.1 Use of Eisner’s Algorithms
</subsectionHeader>
<bodyText confidence="0.999995433333333">
This section describes the algorithm for finding
y∗ = arg maxy∈Y(X) w · f(x, y) where f(x, y) is
defined through either the first-order model (Eq. 2)
or the second-order model (Eq. 3).
For the first-order model, the methods described
in (Eisner, 2000) can be used for the parsing algo-
rithm. In Eisner’s algorithms for dependency pars-
ing each word in the input has left and right finite-
state (weighted) automata, which generate the left
and right modifiers of the word in question. We
make use of this idea of automata, and also make
direct use of the method described in section 4.2 of
(Eisner, 2000) that allows a set of possible senses
for each word in the input string. In our use of
the algorithm, each possible sense for a word cor-
responds to a different possible spine that can be
associated with that word. The left and right au-
tomata are used to keep track of the last position
in the spine that was adjoined into on the left/right
of the head respectively. We can make use of sep-
arate left and right automata—i.e., the grammar is
splittable—because left and right modifiers are ad-
joined independently of each other in the tree. The
extension of Eisner’s algorithm to the second-order
model is similar to the algorithm described in (Car-
reras, 2007), but again with explicit use of word
senses and left/right automata. The resulting algo-
rithms run in O(Gn3) and O(Hn4) time for the
first-order and second-order models respectively,
where G and H are grammar constants.
</bodyText>
<subsectionHeader confidence="0.999197">
3.2 Efficient Parsing
</subsectionHeader>
<bodyText confidence="0.999981689655172">
The efficiency of the parsing algorithm is impor-
tant in applying the parsing model to test sen-
tences, and also when training the model using dis-
criminative methods. The grammar constants G
and H introduced in the previous section are poly-
nomial in factors such as the number of possible
spines in the model, and the number of possible
states in the finite-state automata implicit in the
parsing algorithm. These constants are large, mak-
ing exhaustive parsing very expensive.
To deal with this problem, we use a simple ini-
tial model to prune the search space of the more
complex model. The first-stage model we use
is a first-order dependency model, with labeled
dependencies, as described in (McDonald et al.,
2005). As described shortly, we will use this model
to compute marginal scores for dependencies in
both training and test sentences. A marginal score
µ(x, h, m, l) is a value between 0 and 1 that re-
flects the plausibility of a dependency for sentence
x with head-word xh, modifier word xm, and la-
bel l. In the first-stage pruning model the labels l
are triples of non-terminals representing grammat-
ical relations, as described in section 2.1 of this
paper—for example, one possible label would be
(VP VBD NP), and in general any triple of non-
terminals is possible.
Given a sentence x, and an index m of a word
in that sentence, we define DMAX(x, m) to be the
</bodyText>
<page confidence="0.980694">
13
</page>
<bodyText confidence="0.993281818181818">
highest scoring dependency with m as a modifier:
DMAX(x, m) = max µ(x, h, m, l)
h,l
For a sentence x, we then define the set of allow-
able dependencies to be
to the number of non-terminals in the grammar,
which is far more manageable. We use the algo-
rithm described in (Globerson et al., 2007) to train
the conditional log-linear model; this method was
found to converge to a good model after 10 itera-
tions over the training data.
</bodyText>
<equation confidence="0.966185">
7r(x) = {(h, m, l) : µ(x, h, m, l) &gt; αDMAX(x, m)}
</equation>
<bodyText confidence="0.999894857142857">
where α is a constant dictating the beam size that
is used (in our experiments we used α = 10−6).
The set 7r(x) is used to restrict the set of pos-
sible parses under the full TAG-based model. In
section 2.1 we described how the TAG model has
dependency labels of the form (POS, A, ηh, ηm, L),
and that there is a function GRM that maps labels
of this form to triples of non-terminals. The ba-
sic idea of the pruned search is to only allow de-
pendencies of the form (h, m, (POS, A, ηh, ηm, L))
if the tuple (h, m, GRM((POS, A, ηh, ηm, L))) is a
member of 7r(x), thus reducing the search space
for the parser.
We now turn to how the marginals µ(x, h, m, l)
are defined and computed. A simple approach
would be to use a conditional log-linear model
(Lafferty et al., 2001), with features as defined by
McDonald et al. (2005), to define a distribution
P(y|x) where the parse structures y are depen-
dency structures with labels that are triples of non-
terminals. In this case we could define
</bodyText>
<equation confidence="0.78422">
�µ(x, h, m, l) = P(y|x)
y:(h,m,l)∈y
</equation>
<bodyText confidence="0.974846977777778">
which can be computed with inside-outside style
algorithms, applied to the data structures from
(Eisner, 2000). The complexity of training and ap-
plying such a model is again O(Gn3), where G is
the number of possible labels, and the number of
possible labels (triples of non-terminals) is around
G = 1000 in the case of treebank parsing; this
value for G is still too large for the method to be ef-
ficient. Instead, we train three separate models µ1,
µ2, and µ3 for the three different positions in the
non-terminal triples. We then take µ(x, h, m, l) to
be a product of these three models, for example we
would calculate
µ(x, h, m, (VP VBD NP)) =
µ1(x, h, m, (VP)) x µ2(x, h, m, (VBD))
xµ3(x, h, m, (NP))
Training the three models, and calculating the
marginals, now has a grammar constant equal
Section 2.2 described the use of feature vectors
associated with spines used in a derivation, to-
gether with first-order, sibling, and grandparent
dependencies. The dependency features used in
our experiments are closely related to the features
described in (Carreras, 2007), which are an ex-
tension of the McDonald and Pereira (2006) fea-
tures to cover grandparent dependencies in addi-
tion to first-order and sibling dependencies. The
features take into account the identity of the la-
bels l used in the derivations. The features could
potentially look at any information in the la-
bels, which are of the form (POS, A, ηh, ηm, L),
but in our experiments, we map labels to a pair
(GRM((POS, A, ηh, ηm, L)), A). Thus the label fea-
tures are sensitive only to the triple of non-
terminals corresponding to the grammatical rela-
tion involved in an adjunction, and a binary flag
specifiying whether the operation is s-adjunction
or r-adjunction.
For the spine features e(x, (i, η)), we use fea-
ture templates that are sensitive to the identity of
the spine η, together with contextual features of
the string x. These features consider the iden-
tity of the words and part-of-speech tags in a win-
dow that is centered on xi and spans the range
x(i−2) ... x(i+2).
</bodyText>
<subsectionHeader confidence="0.999578">
4.2 Extracting Derivations from Parse Trees
</subsectionHeader>
<bodyText confidence="0.999953692307692">
In the experiments in this paper, the following
three-step process was used: (1) derivations were
extracted from a training set drawn from the Penn
WSJ treebank, and then used to train a parsing
model; (2) the test data was parsed using the re-
sulting model, giving a derivation for each test
data sentence; (3) the resulting test-data deriva-
tions were mapped back to Penn-treebank style
trees, using the method described in section 2.1.
To achieve step (1), we first apply a set of head-
finding rules which are similar to those described
in (Collins, 1997). Once the head-finding rules
have been applied, it is straightforward to extract
</bodyText>
<sectionHeader confidence="0.587669" genericHeader="method">
4 Implementation Details
4.1 Features
</sectionHeader>
<page confidence="0.961382">
14
</page>
<table confidence="0.999932818181818">
precision recall F1
PPK07 – – 88.3
FKM08 88.2 87.8 88.0
CH2000 89.5 89.6 89.6
CO2000 89.9 89.6 89.8
PK07 90.2 89.9 90.1
this paper 91.4 90.7 91.1
CJ05 – – 91.4
H08 – – 91.7
CO2000(s24) 89.6 88.6 89.1
this paper (s24) 91.1 89.9 90.5
</table>
<tableCaption confidence="0.97816075">
Table 1: Results for different methods. PPK07, FKM08,
CH2000, CO2000, PK07, CJ05 and H08 are results on section
23 of the Penn WSJ treebank, for the models of Petrov et al.
(2007), Finkel et al. (2008), Charniak (2000), Collins (2000),
Petrov and Klein (2007), Charniak and Johnson (2005), and
Huang (2008). (CJ05 is the performance of an updated
model at http://www.cog.brown.edu/mj/software.htm.) “s24”
denotes results on section 24 of the treebank.
</tableCaption>
<table confidence="0.9993705">
s23 s24
KCC08 unlabeled 92.0 91.0
KCC08 labeled 92.5 91.7
this paper 93.5 92.5
</table>
<tableCaption confidence="0.997301">
Table 2: Table showing unlabeled dependency accuracy for
</tableCaption>
<bodyText confidence="0.963647565217391">
sections 23 and 24 of the treebank, using the method of (Ya-
mada and Matsumoto, 2003) to extract dependencies from
parse trees from our model. KCC08 unlabeled is from (Koo
et al., 2008), a model that has previously been shown to have
higher accuracy than (McDonald and Pereira, 2006). KCC08
labeled is the labeled dependency parser from (Koo et al.,
2008); here we only evaluate the unlabeled accuracy.
derivations from the Penn treebank trees.
Note that the mapping from parse trees to
derivations is many-to-one: for example, the ex-
ample trees in section 2.3 have structures that are
as “flat” (have as few levels) as is possible, given
the set D that is involved. Other similar trees,
but with more VP levels, will give the same set
D. However, this issue appears to be benign in the
Penn WSJ treebank. For example, on section 22 of
the treebank, if derivations are first extracted using
the method described in this section, then mapped
back to parse trees using the method described in
section 2.3, the resulting parse trees score 100%
precision and 99.81% recall in labeled constituent
accuracy, indicating that very little information is
lost in this process.
</bodyText>
<subsectionHeader confidence="0.999233">
4.3 Part-of-Speech Tags, and Spines
</subsectionHeader>
<bodyText confidence="0.999862833333333">
Sentences in training, test, and development data
are assumed to have part-of-speech (POS) tags.
POS tags are used for two purposes: (1) in the
features described above; and (2) to limit the set
of allowable spines for each word during parsing.
Specifically, for each POS tag we create a separate
</bodyText>
<table confidence="0.9985554">
1st stage 2nd stage
α active coverage oracle F1 speed F1
10−4 0.07 97.7 97.0 5:15 91.1
10−5 0.16 98.5 97.9 11:45 91.6
10−6 0.34 99.0 98.5 21:50 92.0
</table>
<tableCaption confidence="0.998757">
Table 3: Effect of the beam size, controlled by α, on the
</tableCaption>
<bodyText confidence="0.98871185">
performance of the parser on the development set (1,699 sen-
tences). In each case α refers to the beam size used in both
training and testing the model. “active”: percentage of de-
pendencies that remain in the beam out of the total number of
labeled dependencies (1,000 triple labels times 1,138,167 un-
labeled dependencies); “coverage”: percentage of correct de-
pendencies in the beam out of the total number of correct de-
pendencies. “oracle F1”: maximum achievable score of con-
stituents, given the beam. “speed”: parsing time in min:sec
for the TAG-based model (this figure does not include the time
taken to calculate the marginals using the lower-order model);
“F1”: score of predicted constituents.
dictionary listing the spines that have been seen
with this POS tag in training data; during parsing
we only allow spines that are compatible with this
dictionary. (For test or development data, we used
the part-of-speech tags generated by the parser of
(Collins, 1997). Future work should consider in-
corporating the tagging step within the model; it is
not challenging to extend the model in this way.)
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99997348">
Sections 2-21 of the Penn Wall Street Journal tree-
bank were used as training data in our experiments,
and section 22 was used as a development set. Sec-
tions 23 and 24 were used as test sets. The model
was trained for 20 epochs with the averaged per-
ceptron algorithm, with the development data per-
formance being used to choose the best epoch. Ta-
ble 1 shows the results for the method.
Our experiments show an improvement in per-
formance over the results in (Collins, 2000; Char-
niak, 2000). We would argue that the Collins
(2000) method is considerably more complex than
ours, requiring a first-stage generative model, to-
gether with a reranking approach. The Char-
niak (2000) model is also arguably more com-
plex, again using a carefully constructed genera-
tive model. The accuracy of our approach also
shows some improvement over results in (Petrov
and Klein, 2007). This work makes use of a
PCFG with latent variables that is trained using
a split/merge procedure together with the EM al-
gorithm. This work is in many ways comple-
mentary to ours—for example, it does not make
use of GLMs, dependency features, or of repre-
sentations that go beyond PCFG productions—and
</bodyText>
<page confidence="0.99244">
15
</page>
<bodyText confidence="0.999959454545454">
some combination of the two methods may give
further gains.
Charniak and Johnson (2005), and Huang
(2008), describe approaches that make use of non-
local features in conjunction with the Charniak
(2000) model; future work may consider extend-
ing our approach to include non-local features.
Finally, other recent work (Petrov et al., 2007;
Finkel et al., 2008) has had a similar goal of scal-
ing GLMs to full syntactic parsing. These mod-
els make use of PCFG representations, but do not
explicitly model bigram or trigram dependencies.
The results in this work (88.3%/88.0% F1) are
lower than our F1 score of 91.1%; this is evidence
of the benefits of the richer representations enabled
by our approach.
Table 2 shows the accuracy of the model in
recovering unlabeled dependencies. The method
shows improvements over the method described
in (Koo et al., 2008), which is a state-of-the-art
second-order dependency parser similar to that of
(McDonald and Pereira, 2006), suggesting that the
incorporation of constituent structure can improve
dependency accuracy.
Table 3 shows the effect of the beam-size on the
accuracy and speed of the parser on the develop-
ment set. With the beam setting used in our exper-
iments (α = 10−6), only 0.34% of possible depen-
dencies are considered by the TAG-based model,
but 99% of all correct dependencies are included.
At this beam size the best possible F1 constituent
score is 98.5. Tighter beams lead to faster parsing
times, with slight drops in accuracy.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.997924764705882">
We have described an efficient and accurate parser
for constituent parsing. A key to the approach has
been to use a splittable grammar that allows effi-
cient dynamic programming algorithms, in com-
bination with pruning using a lower-order model.
The method allows relatively easy incorporation of
features; future work should leverage this in pro-
ducing more accurate parsers, and in applying the
parser to different languages or domains.
Acknowledgments X. Carreras was supported by the
Catalan Ministry of Innovation, Universities and Enterprise,
by the GALE program of DARPA, Contract No. HR0011-06-
C-0022, and by a grant from NTT, Agmt. Dtd. 6/21/1998.
T. Koo was funded by NSF grant IIS-0415030. M. Collins
was funded by NSF grant IIS-0347631 and DARPA contract
No. HR0011-06-C-0022. Thanks to Jenny Rose Finkel for
suggesting that we evaluate dependency parsing accuracies.
</bodyText>
<sectionHeader confidence="0.998416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999691911764706">
Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003. Hidden
markov support vector machines. In ICML.
Carreras, X. 2007. Experiments with a higher-order projec-
tive dependency parser. In Proc. EMNLP-CoNLL Shared
Task.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL.
Chiang, D. 2003. Statistical parsing with an automatically
extracted tree adjoining grammar. In Bod, R., R. Scha, and
K. Sima’an, editors, Data Oriented Parsing, pages 299–
316. CSLI Publications.
Clark, S. and J. R. Curran. 2004. Parsing the wsj using ccg
and log-linear models. In Proc. ACL.
Clark, Stephen and James R. Curran. 2007. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser. In
Proc. ACL Workshop on Deep Linguistic Processing.
Collins, M. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. ACL.
Collins, M. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. ICML.
Collins, M. 2002. Discriminative training methods for hid-
den markov models: Theory and experiments with percep-
tron algorithms. In Proc. EMNLP.
Eisner, J. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Bunt, H. C. and A. Nijholt, editors,
New Developments in Natural Language Parsing, pages
29–62. Kluwer Academic Publishers.
Finkel, J. R., A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing. In
Proc. ACL/HLT.
Globerson, A., T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proc. ICML.
Huang, L. 2008. Forest reranking: Discriminative parsing
with non-local features. In Proc. ACL/HLT.
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic unification-based gram-
mars. In Proc. ACL.
Koo, Terry, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL/HLT.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditonal
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In Proc. EACL.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In Proc.
ACL.
Petrov, S. and D. Klein. 2007. Improved inference for unlex-
icalized parsing. In Proc. of HLT-NAACL.
Petrov, S., A. Pauls, and D. Klein. 2007. Discriminative log-
linear grammars with latent variables. In Proc. NIPS.
Ratnaparkhi, A., S. Roukos, and R. Ward. 1994. A maximum
entropy model for parsing. In Proc. ICSLP.
Sagae, Kenji, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Hpsg
parsing with shallow dependency constraints. In Proc.
ACL, pages 624–631.
Shen, L. and A.K. Joshi. 2005. Incremental ltag parsing. In
Proc HLT-EMNLP.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of the
EMNLP-2004.
Yamada, H. and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
</reference>
<page confidence="0.998684">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336448">
<title confidence="0.9932975">TAG, Dynamic Programming, and the for Efficient, Feature-rich Parsing</title>
<author confidence="0.998936">Xavier Carreras Michael Collins Terry Koo</author>
<affiliation confidence="0.362188">MIT CSAIL, Cambridge, MA 02139,</affiliation>
<abstract confidence="0.996706111111111">We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees. The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
</authors>
<title>Hidden markov support vector machines.</title>
<date>2003</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1189" citStr="Altun et al., 2003" startWordPosition="172" endWordPosition="175">roach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is y* = arg max w · f(x, y) (1) yEY(X) where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibility in the features whi</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003. Hidden markov support vector machines. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL Shared Task.</booktitle>
<contexts>
<context position="14899" citStr="Carreras, 2007" startWordPosition="2617" endWordPosition="2618"> a combina(h,m,l)ED(y) tion of the two structures in figure 2. Here we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y. The function e maps a sentence x paired with a spine (i, ,q) to a feature vector. The function d maps dependencies within y to feature vectors. This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007). If y = (E, D) is a derivation, then: • 5(y) is a set of sibling dependencies. Each sibling dependency is a tuple (h, m,l, s). For each (h, m,l, s) E 5 the tuple (h, m, l) is an element of D; there is one member of 5 for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no previous adjunction has taken place). • G(y) is a set of grandparent dependencies of type 1. Each type 1 grandparent dependency is a tuple (h, m,l, g). There is one member of G for every member of D. The additional information, the index g</context>
<context position="19812" citStr="Carreras, 2007" startWordPosition="3514" endWordPosition="3516"> each word in the input string. In our use of the algorithm, each possible sense for a word corresponds to a different possible spine that can be associated with that word. The left and right automata are used to keep track of the last position in the spine that was adjoined into on the left/right of the head respectively. We can make use of separate left and right automata—i.e., the grammar is splittable—because left and right modifiers are adjoined independently of each other in the tree. The extension of Eisner’s algorithm to the second-order model is similar to the algorithm described in (Carreras, 2007), but again with explicit use of word senses and left/right automata. The resulting algorithms run in O(Gn3) and O(Hn4) time for the first-order and second-order models respectively, where G and H are grammar constants. 3.2 Efficient Parsing The efficiency of the parsing algorithm is important in applying the parsing model to test sentences, and also when training the model using discriminative methods. The grammar constants G and H introduced in the previous section are polynomial in factors such as the number of possible spines in the model, and the number of possible states in the finite-st</context>
<context position="23992" citStr="Carreras, 2007" startWordPosition="4268" endWordPosition="4269">and µ3 for the three different positions in the non-terminal triples. We then take µ(x, h, m, l) to be a product of these three models, for example we would calculate µ(x, h, m, (VP VBD NP)) = µ1(x, h, m, (VP)) x µ2(x, h, m, (VBD)) xµ3(x, h, m, (NP)) Training the three models, and calculating the marginals, now has a grammar constant equal Section 2.2 described the use of feature vectors associated with spines used in a derivation, together with first-order, sibling, and grandparent dependencies. The dependency features used in our experiments are closely related to the features described in (Carreras, 2007), which are an extension of the McDonald and Pereira (2006) features to cover grandparent dependencies in addition to first-order and sibling dependencies. The features take into account the identity of the labels l used in the derivations. The features could potentially look at any information in the labels, which are of the form (POS, A, ηh, ηm, L), but in our experiments, we map labels to a pair (GRM((POS, A, ηh, ηm, L)), A). Thus the label features are sensitive only to the triple of nonterminals corresponding to the grammatical relation involved in an adjunction, and a binary flag specifi</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Carreras, X. 2007. Experiments with a higher-order projective dependency parser. In Proc. EMNLP-CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="3029" citStr="Charniak and Johnson, 2005" startWordPosition="492" endWordPosition="495">th in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f(x, y) that can be used in the model. The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of features that are sensitive to bigra</context>
<context position="5122" citStr="Charniak and Johnson, 2005" startWordPosition="828" endWordPosition="831">icient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnap</context>
<context position="26206" citStr="Charniak and Johnson (2005)" startWordPosition="4652" endWordPosition="4655">finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is</context>
<context position="30582" citStr="Charniak and Johnson (2005)" startWordPosition="5387" endWordPosition="5390">with a reranking approach. The Charniak (2000) model is also arguably more complex, again using a carefully constructed generative model. The accuracy of our approach also shows some improvement over results in (Petrov and Klein, 2007). This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions—and 15 some combination of the two methods may give further gains. Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features. Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by ou</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="4985" citStr="Charniak, 2000" startWordPosition="808" endWordPosition="809">ithms). (2) Use of a lower-order model for pruning. The O(n4G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small </context>
<context position="26136" citStr="Charniak (2000)" startWordPosition="4644" endWordPosition="4645">ilar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to ha</context>
<context position="29816" citStr="Charniak, 2000" startWordPosition="5261" endWordPosition="5263"> consider incorporating the tagging step within the model; it is not challenging to extend the model in this way.) 5 Experiments Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set. Sections 23 and 24 were used as test sets. The model was trained for 20 epochs with the averaged perceptron algorithm, with the development data performance being used to choose the best epoch. Table 1 shows the results for the method. Our experiments show an improvement in performance over the results in (Collins, 2000; Charniak, 2000). We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach. The Charniak (2000) model is also arguably more complex, again using a carefully constructed generative model. The accuracy of our approach also shows some improvement over results in (Petrov and Klein, 2007). This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make use of GLMs, dep</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropy-inspired parser. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Statistical parsing with an automatically extracted tree adjoining grammar.</title>
<date>2003</date>
<booktitle>Data Oriented Parsing,</booktitle>
<pages>299--316</pages>
<editor>In Bod, R., R. Scha, and K. Sima’an, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="8230" citStr="Chiang, 2003" startWordPosition="1339" endWordPosition="1340">l. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 2 The TAG-Based Parsing Model 2.1 Derivations This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair (E, D) where E is a set of spines, and D is a set of dependencies 2Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG pars</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>Chiang, D. 2003. Statistical parsing with an automatically extracted tree adjoining grammar. In Bod, R., R. Scha, and K. Sima’an, editors, Data Oriented Parsing, pages 299– 316. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Parsing the wsj using ccg and log-linear models.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7169" citStr="Clark and Curran (2004)" startWordPosition="1157" endWordPosition="1160">ere is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependenc</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, S. and J. R. Curran. 2004. Parsing the wsj using ccg and log-linear models. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Perceptron training for a wide-coverage lexicalized-grammar parser.</title>
<date>2007</date>
<booktitle>In Proc. ACL Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="7439" citStr="Clark and Curran (2007)" startWordPosition="1203" endWordPosition="1206">r may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 2 The TAG-Based Parsing Model 2.1 Derivations This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in com</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, Stephen and James R. Curran. 2007. Perceptron training for a wide-coverage lexicalized-grammar parser. In Proc. ACL Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="25563" citStr="Collins, 1997" startWordPosition="4542" endWordPosition="4543">2) ... x(i+2). 4.2 Extracting Derivations from Parse Trees In the experiments in this paper, the following three-step process was used: (1) derivations were extracted from a training set drawn from the Penn WSJ treebank, and then used to train a parsing model; (2) the test data was parsed using the resulting model, giving a derivation for each test data sentence; (3) the resulting test-data derivations were mapped back to Penn-treebank style trees, using the method described in section 2.1. To achieve step (1), we first apply a set of headfinding rules which are similar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov an</context>
<context position="29181" citStr="Collins, 1997" startWordPosition="5148" endWordPosition="5149">t dependencies in the beam out of the total number of correct dependencies. “oracle F1”: maximum achievable score of constituents, given the beam. “speed”: parsing time in min:sec for the TAG-based model (this figure does not include the time taken to calculate the marginals using the lower-order model); “F1”: score of predicted constituents. dictionary listing the spines that have been seen with this POS tag in training data; during parsing we only allow spines that are compatible with this dictionary. (For test or development data, we used the part-of-speech tags generated by the parser of (Collins, 1997). Future work should consider incorporating the tagging step within the model; it is not challenging to extend the model in this way.) 5 Experiments Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set. Sections 23 and 24 were used as test sets. The model was trained for 20 epochs with the averaged perceptron algorithm, with the development data performance being used to choose the best epoch. Table 1 shows the results for the method. Our experiments show an improvement in performance over the results</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, M. 1997. Three generative, lexicalised models for statistical parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="3000" citStr="Collins, 2000" startWordPosition="490" endWordPosition="491">e sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f(x, y) that can be used in the model. The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of feature</context>
<context position="5000" citStr="Collins, 2000" startWordPosition="810" endWordPosition="811">of a lower-order model for pruning. The O(n4G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidat</context>
<context position="26152" citStr="Collins (2000)" startWordPosition="4646" endWordPosition="4647">cribed in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accura</context>
<context position="29799" citStr="Collins, 2000" startWordPosition="5259" endWordPosition="5260">ure work should consider incorporating the tagging step within the model; it is not challenging to extend the model in this way.) 5 Experiments Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set. Sections 23 and 24 were used as test sets. The model was trained for 20 epochs with the averaged perceptron algorithm, with the development data performance being used to choose the best epoch. Table 1 shows the results for the method. Our experiments show an improvement in performance over the results in (Collins, 2000; Charniak, 2000). We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach. The Charniak (2000) model is also arguably more complex, again using a carefully constructed generative model. The accuracy of our approach also shows some improvement over results in (Petrov and Klein, 2007). This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Collins, M. 2000. Discriminative reranking for natural language parsing. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1169" citStr="Collins, 2002" startWordPosition="170" endWordPosition="171">ing such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is y* = arg max w · f(x, y) (1) yEY(X) where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibility</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>New Developments in Natural Language Parsing,</booktitle>
<pages>29--62</pages>
<editor>In Bunt, H. C. and A. Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="4225" citStr="Eisner (2000)" startWordPosition="686" endWordPosition="687">ensitive to bigram dependencies between pairs of words; and (3) features that are sensitive to trigram dependencies. Any of these feature types can be combined with surface features of the sentence x, in a similar way 9 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9–16 Manchester, August 2008 to the use of surface features in conditional random fields (Lafferty et al., 2001). Crucially, in spite of these relatively rich representations, the formalism can be parsed efficiently (in O(n4G) time) using dynamic-programming algorithms described by Eisner (2000) (unlike many other TAGrelated approaches, our formalism is “splittable” in the sense described by Eisner, leading to more efficient parsing algorithms). (2) Use of a lower-order model for pruning. The O(n4G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full </context>
<context position="6529" citStr="Eisner (2000)" startWordPosition="1055" endWordPosition="1056">thereby avoid search errors that may be made in the first-pass parser.1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon1Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. ald et al., 2005). Dependency parsing can be implemented in O(n3) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 1</context>
<context position="8442" citStr="Eisner (2000)" startWordPosition="1374" endWordPosition="1375">is section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair (E, D) where E is a set of spines, and D is a set of dependencies 2Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 10 Figure 1: Two example trees. specifying how the spines are combined to form a parse tree. The spines are similar to elementary trees in TAG. Some examples are as follows: NP S NP John VBD cake ate These st</context>
<context position="18807" citStr="Eisner, 2000" startWordPosition="3335" endWordPosition="3336">VP → ADVP VP NP in the above tree. Note that our r-adjunction operation is different from the usual adjunction operation in TAGs, in that “wrapping” adjunctions are not possible, and r-adjunctions from the left and right directions are independent from each other; because of this our grammars are splittable. 3 Parsing Algorithms 3.1 Use of Eisner’s Algorithms This section describes the algorithm for finding y∗ = arg maxy∈Y(X) w · f(x, y) where f(x, y) is defined through either the first-order model (Eq. 2) or the second-order model (Eq. 3). For the first-order model, the methods described in (Eisner, 2000) can be used for the parsing algorithm. In Eisner’s algorithms for dependency parsing each word in the input has left and right finitestate (weighted) automata, which generate the left and right modifiers of the word in question. We make use of this idea of automata, and also make direct use of the method described in section 4.2 of (Eisner, 2000) that allows a set of possible senses for each word in the input string. In our use of the algorithm, each possible sense for a word corresponds to a different possible spine that can be associated with that word. The left and right automata are used </context>
<context position="23034" citStr="Eisner, 2000" startWordPosition="4099" endWordPosition="4100">POS, A, ηh, ηm, L))) is a member of 7r(x), thus reducing the search space for the parser. We now turn to how the marginals µ(x, h, m, l) are defined and computed. A simple approach would be to use a conditional log-linear model (Lafferty et al., 2001), with features as defined by McDonald et al. (2005), to define a distribution P(y|x) where the parse structures y are dependency structures with labels that are triples of nonterminals. In this case we could define �µ(x, h, m, l) = P(y|x) y:(h,m,l)∈y which can be computed with inside-outside style algorithms, applied to the data structures from (Eisner, 2000). The complexity of training and applying such a model is again O(Gn3), where G is the number of possible labels, and the number of possible labels (triples of non-terminals) is around G = 1000 in the case of treebank parsing; this value for G is still too large for the method to be efficient. Instead, we train three separate models µ1, µ2, and µ3 for the three different positions in the non-terminal triples. We then take µ(x, h, m, l) to be a product of these three models, for example we would calculate µ(x, h, m, (VP VBD NP)) = µ1(x, h, m, (VP)) x µ2(x, h, m, (VBD)) xµ3(x, h, m, (NP)) Traini</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Eisner, J. 2000. Bilexical grammars and their cubic-time parsing algorithms. In Bunt, H. C. and A. Nijholt, editors, New Developments in Natural Language Parsing, pages 29–62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT.</booktitle>
<contexts>
<context position="7675" citStr="Finkel et al., 2008" startWordPosition="1245" endWordPosition="1248">approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 2 The TAG-Based Parsing Model 2.1 Derivations This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an importan</context>
<context position="26119" citStr="Finkel et al. (2008)" startWordPosition="4640" endWordPosition="4643">ng rules which are similar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously</context>
<context position="30852" citStr="Finkel et al., 2008" startWordPosition="5430" endWordPosition="5433">t variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions—and 15 some combination of the two methods may give further gains. Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features. Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach. Table 2 shows the accuracy of the model in recovering unlabeled dependencies. The method shows improvements over the method described in (Koo et al., 2008), which is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, </context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Finkel, J. R., A. Kleeman, and C. D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proc. ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Exponentiated gradient algorithms for log-linear structured prediction.</title>
<date>2007</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="21718" citStr="Globerson et al., 2007" startWordPosition="3848" endWordPosition="3851">odel the labels l are triples of non-terminals representing grammatical relations, as described in section 2.1 of this paper—for example, one possible label would be (VP VBD NP), and in general any triple of nonterminals is possible. Given a sentence x, and an index m of a word in that sentence, we define DMAX(x, m) to be the 13 highest scoring dependency with m as a modifier: DMAX(x, m) = max µ(x, h, m, l) h,l For a sentence x, we then define the set of allowable dependencies to be to the number of non-terminals in the grammar, which is far more manageable. We use the algorithm described in (Globerson et al., 2007) to train the conditional log-linear model; this method was found to converge to a good model after 10 iterations over the training data. 7r(x) = {(h, m, l) : µ(x, h, m, l) &gt; αDMAX(x, m)} where α is a constant dictating the beam size that is used (in our experiments we used α = 10−6). The set 7r(x) is used to restrict the set of possible parses under the full TAG-based model. In section 2.1 we described how the TAG model has dependency labels of the form (POS, A, ηh, ηm, L), and that there is a function GRM that maps labels of this form to triples of non-terminals. The basic idea of the pruned</context>
</contexts>
<marker>Globerson, Koo, Carreras, Collins, 2007</marker>
<rawString>Globerson, A., T. Koo, X. Carreras, and M. Collins. 2007. Exponentiated gradient algorithms for log-linear structured prediction. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT.</booktitle>
<contexts>
<context position="26224" citStr="Huang (2008)" startWordPosition="4657" endWordPosition="4658">it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled depen</context>
<context position="30600" citStr="Huang (2008)" startWordPosition="5392" endWordPosition="5393">arniak (2000) model is also arguably more complex, again using a carefully constructed generative model. The accuracy of our approach also shows some improvement over results in (Petrov and Klein, 2007). This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions—and 15 some combination of the two methods may give further gains. Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features. Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach. Table </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, L. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic unification-based grammars.</title>
<date>1999</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1131" citStr="Johnson et al., 1999" startWordPosition="162" endWordPosition="165">surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is y* = arg max w · f(x, y) (1) yEY(X) where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that </context>
<context position="5762" citStr="Johnson et al., 1999" startWordPosition="931" endWordPosition="934">overs dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005)). A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser.1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon1Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, </context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999. Estimators for stochastic unification-based grammars. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT.</booktitle>
<contexts>
<context position="5262" citStr="Koo et al., 2008" startWordPosition="849" endWordPosition="852">G parser in training and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005)). A crucial advantage of our approach is that it consid</context>
<context position="26690" citStr="Koo et al., 2008" startWordPosition="4729" endWordPosition="4732"> of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. derivations from the Penn treebank trees. Note that the mapping from parse trees to derivations is many-to-one: for example, the example trees in section 2.3 have structures that are as “flat” (have as few levels) as is possible, given the set D that is involved. Other similar trees, but with more VP levels, will give the same set D. However, this issue appears to be benign in th</context>
<context position="31349" citStr="Koo et al., 2008" startWordPosition="5514" endWordPosition="5517">ending our approach to include non-local features. Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach. Table 2 shows the accuracy of the model in recovering unlabeled dependencies. The method shows improvements over the method described in (Koo et al., 2008), which is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, 2006), suggesting that the incorporation of constituent structure can improve dependency accuracy. Table 3 shows the effect of the beam-size on the accuracy and speed of the parser on the development set. With the beam setting used in our experiments (α = 10−6), only 0.34% of possible dependencies are considered by the TAG-based model, but 99% of all correct dependencies are included. At this beam size the best possible F1 constituent score is 98.5. Tighter beams lead to faster parsing times,</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Koo, Terry, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditonal random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="1154" citStr="Lafferty et al., 2001" startWordPosition="166" endWordPosition="169">vere challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is y* = arg max w · f(x, y) (1) yEY(X) where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal</context>
<context position="4041" citStr="Lafferty et al., 2001" startWordPosition="659" endWordPosition="662">ature-vector representations f(x, y) that can be used in the model. The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of features that are sensitive to bigram dependencies between pairs of words; and (3) features that are sensitive to trigram dependencies. Any of these feature types can be combined with surface features of the sentence x, in a similar way 9 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9–16 Manchester, August 2008 to the use of surface features in conditional random fields (Lafferty et al., 2001). Crucially, in spite of these relatively rich representations, the formalism can be parsed efficiently (in O(n4G) time) using dynamic-programming algorithms described by Eisner (2000) (unlike many other TAGrelated approaches, our formalism is “splittable” in the sense described by Eisner, leading to more efficient parsing algorithms). (2) Use of a lower-order model for pruning. The O(n4G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of t</context>
<context position="22672" citStr="Lafferty et al., 2001" startWordPosition="4037" endWordPosition="4040">ossible parses under the full TAG-based model. In section 2.1 we described how the TAG model has dependency labels of the form (POS, A, ηh, ηm, L), and that there is a function GRM that maps labels of this form to triples of non-terminals. The basic idea of the pruned search is to only allow dependencies of the form (h, m, (POS, A, ηh, ηm, L)) if the tuple (h, m, GRM((POS, A, ηh, ηm, L))) is a member of 7r(x), thus reducing the search space for the parser. We now turn to how the marginals µ(x, h, m, l) are defined and computed. A simple approach would be to use a conditional log-linear model (Lafferty et al., 2001), with features as defined by McDonald et al. (2005), to define a distribution P(y|x) where the parse structures y are dependency structures with labels that are triples of nonterminals. In this case we could define �µ(x, h, m, l) = P(y|x) y:(h,m,l)∈y which can be computed with inside-outside style algorithms, applied to the data structures from (Eisner, 2000). The complexity of training and applying such a model is again O(Gn3), where G is the number of possible labels, and the number of possible labels (triples of non-terminals) is around G = 1000 in the case of treebank parsing; this value </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditonal random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. EACL.</booktitle>
<contexts>
<context position="5291" citStr="McDonald and Pereira, 2006" startWordPosition="853" endWordPosition="857">ng and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005)). A crucial advantage of our approach is that it considers a very large set of alter</context>
<context position="14845" citStr="McDonald and Pereira, 2006" startWordPosition="2608" endWordPosition="2611">D VP NP � d(x, (h, m, l)) (2) Figure 3: An example tree, formed by a combina(h,m,l)ED(y) tion of the two structures in figure 2. Here we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y. The function e maps a sentence x paired with a spine (i, ,q) to a feature vector. The function d maps dependencies within y to feature vectors. This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007). If y = (E, D) is a derivation, then: • 5(y) is a set of sibling dependencies. Each sibling dependency is a tuple (h, m,l, s). For each (h, m,l, s) E 5 the tuple (h, m, l) is an element of D; there is one member of 5 for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no previous adjunction has taken place). • G(y) is a set of grandparent dependencies of type 1. Each type 1 grandparent dependency is a tuple (h, m,l, g). There is one member of G for ever</context>
<context position="24051" citStr="McDonald and Pereira (2006)" startWordPosition="4277" endWordPosition="4280"> non-terminal triples. We then take µ(x, h, m, l) to be a product of these three models, for example we would calculate µ(x, h, m, (VP VBD NP)) = µ1(x, h, m, (VP)) x µ2(x, h, m, (VBD)) xµ3(x, h, m, (NP)) Training the three models, and calculating the marginals, now has a grammar constant equal Section 2.2 described the use of feature vectors associated with spines used in a derivation, together with first-order, sibling, and grandparent dependencies. The dependency features used in our experiments are closely related to the features described in (Carreras, 2007), which are an extension of the McDonald and Pereira (2006) features to cover grandparent dependencies in addition to first-order and sibling dependencies. The features take into account the identity of the labels l used in the derivations. The features could potentially look at any information in the labels, which are of the form (POS, A, ηh, ηm, L), but in our experiments, we map labels to a pair (GRM((POS, A, ηh, ηm, L)), A). Thus the label features are sensitive only to the triple of nonterminals corresponding to the grammatical relation involved in an adjunction, and a binary flag specifiying whether the operation is s-adjunction or r-adjunction.</context>
<context position="26788" citStr="McDonald and Pereira, 2006" startWordPosition="4745" endWordPosition="4748">and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. derivations from the Penn treebank trees. Note that the mapping from parse trees to derivations is many-to-one: for example, the example trees in section 2.3 have structures that are as “flat” (have as few levels) as is possible, given the set D that is involved. Other similar trees, but with more VP levels, will give the same set D. However, this issue appears to be benign in the Penn WSJ treebank. For example, on section 22 of the treebank, if derivations are first extracte</context>
<context position="31457" citStr="McDonald and Pereira, 2006" startWordPosition="5529" endWordPosition="5532">; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach. Table 2 shows the accuracy of the model in recovering unlabeled dependencies. The method shows improvements over the method described in (Koo et al., 2008), which is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, 2006), suggesting that the incorporation of constituent structure can improve dependency accuracy. Table 3 shows the effect of the beam-size on the accuracy and speed of the parser on the development set. With the beam setting used in our experiments (α = 10−6), only 0.34% of possible dependencies are considered by the TAG-based model, but 99% of all correct dependencies are included. At this beam size the best possible F1 constituent score is 98.5. Tighter beams lead to faster parsing times, with slight drops in accuracy. 6 Conclusions We have described an efficient and accurate parser for constit</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>McDonald, R. and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="8699" citStr="McDonald et al. (2005)" startWordPosition="1424" endWordPosition="1427">n a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair (E, D) where E is a set of spines, and D is a set of dependencies 2Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 10 Figure 1: Two example trees. specifying how the spines are combined to form a parse tree. The spines are similar to elementary trees in TAG. Some examples are as follows: NP S NP John VBD cake ate These structures do not have substitution nodes, as is common in TAGs.3 Instead, the spines consist of a lexical anchor together with a series of unary projections, which usually correspond to different X-bar levels associated with the anchor. The operations used t</context>
<context position="12790" citStr="McDonald et al., 2005" startWordPosition="2201" endWordPosition="2204">al relation between cake and ate is the triple GRM(l) = (VP VBD NP). In the tree shown in figure 1(b), the grammatical relation between cake and ate is the triple GRM(l) = (VP VP NP). The conditions under which a pair (E, D) forms a valid derivation for a sentence x are similar to those in conventional LTAGs. Each (i, η) E E must be such that η is an elementary tree whose anchor is the word xi. The dependencies D must form a directed, projective tree spanning words 0 ... n, with * at the root of this tree, as is also the case in previous work on discriminative approches to dependency parsing (McDonald et al., 2005). We allow any modifier tree ηm to adjoin into any position in any head tree ηh, but the dependencies D must nevertheless be coherent—for example they must be consistent with the spines in E, and they must be nested correctly.4 We will al4For example, closer modifiers to a particular head must adjoin in at the same or a lower spine position than modifiers (a) S (b) S VP VP VBD ate NP NN cake VP VBD ate NP NN cake VP NNP NN ADVP ADVP RB RB quickly luckily 11 low multiple modifier spines to s-adjoin or r-adjoin (a) into the same node in a head spine; see section 2.3 for more details. 2.2 A Globa</context>
<context position="14663" citStr="McDonald et al. (2005)" startWordPosition="2578" endWordPosition="2581">mple Trees cake VP NP ate luckily (b) S ADVP NNP VP RB John VBD S VP VBD VP NP NP NN ADVP RB quickly VP S NP VP NNP NN ate VP John ADVP RB luckily NP NN today ADVP RB quickly VBD VP NP � d(x, (h, m, l)) (2) Figure 3: An example tree, formed by a combina(h,m,l)ED(y) tion of the two structures in figure 2. Here we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y. The function e maps a sentence x paired with a spine (i, ,q) to a feature vector. The function d maps dependencies within y to feature vectors. This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007). If y = (E, D) is a derivation, then: • 5(y) is a set of sibling dependencies. Each sibling dependency is a tuple (h, m,l, s). For each (h, m,l, s) E 5 the tuple (h, m, l) is an element of D; there is one member of 5 for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no pre</context>
<context position="20769" citStr="McDonald et al., 2005" startWordPosition="3672" endWordPosition="3675">ntences, and also when training the model using discriminative methods. The grammar constants G and H introduced in the previous section are polynomial in factors such as the number of possible spines in the model, and the number of possible states in the finite-state automata implicit in the parsing algorithm. These constants are large, making exhaustive parsing very expensive. To deal with this problem, we use a simple initial model to prune the search space of the more complex model. The first-stage model we use is a first-order dependency model, with labeled dependencies, as described in (McDonald et al., 2005). As described shortly, we will use this model to compute marginal scores for dependencies in both training and test sentences. A marginal score µ(x, h, m, l) is a value between 0 and 1 that reflects the plausibility of a dependency for sentence x with head-word xh, modifier word xm, and label l. In the first-stage pruning model the labels l are triples of non-terminals representing grammatical relations, as described in section 2.1 of this paper—for example, one possible label would be (VP VBD NP), and in general any triple of nonterminals is possible. Given a sentence x, and an index m of a </context>
<context position="22724" citStr="McDonald et al. (2005)" startWordPosition="4046" endWordPosition="4049">ction 2.1 we described how the TAG model has dependency labels of the form (POS, A, ηh, ηm, L), and that there is a function GRM that maps labels of this form to triples of non-terminals. The basic idea of the pruned search is to only allow dependencies of the form (h, m, (POS, A, ηh, ηm, L)) if the tuple (h, m, GRM((POS, A, ηh, ηm, L))) is a member of 7r(x), thus reducing the search space for the parser. We now turn to how the marginals µ(x, h, m, l) are defined and computed. A simple approach would be to use a conditional log-linear model (Lafferty et al., 2001), with features as defined by McDonald et al. (2005), to define a distribution P(y|x) where the parse structures y are dependency structures with labels that are triples of nonterminals. In this case we could define �µ(x, h, m, l) = P(y|x) y:(h,m,l)∈y which can be computed with inside-outside style algorithms, applied to the data structures from (Eisner, 2000). The complexity of training and applying such a model is again O(Gn3), where G is the number of possible labels, and the number of possible labels (triples of non-terminals) is around G = 1000 in the case of treebank parsing; this value for G is still too large for the method to be effici</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R., K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="5025" citStr="Petrov and Klein, 2007" startWordPosition="812" endWordPosition="815">r model for pruning. The O(n4G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sen</context>
<context position="26177" citStr="Petrov and Klein (2007)" startWordPosition="4648" endWordPosition="4651">ns, 1997). Once the head-finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Per</context>
<context position="30190" citStr="Petrov and Klein, 2007" startWordPosition="5320" endWordPosition="5323">veraged perceptron algorithm, with the development data performance being used to choose the best epoch. Table 1 shows the results for the method. Our experiments show an improvement in performance over the results in (Collins, 2000; Charniak, 2000). We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach. The Charniak (2000) model is also arguably more complex, again using a carefully constructed generative model. The accuracy of our approach also shows some improvement over results in (Petrov and Klein, 2007). This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions—and 15 some combination of the two methods may give further gains. Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features. Finally</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Petrov, S. and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2007</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="7653" citStr="Petrov et al., 2007" startWordPosition="1241" endWordPosition="1244">escribe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 2 The TAG-Based Parsing Model 2.1 Derivations This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005)</context>
<context position="26097" citStr="Petrov et al. (2007)" startWordPosition="4636" endWordPosition="4639">ply a set of headfinding rules which are similar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract 4 Implementation Details 4.1 Features 14 precision recall F1 PPK07 – – 88.3 FKM08 88.2 87.8 88.0 CH2000 89.5 89.6 89.6 CO2000 89.9 89.6 89.8 PK07 90.2 89.9 90.1 this paper 91.4 90.7 91.1 CJ05 – – 91.4 H08 – – 91.7 CO2000(s24) 89.6 88.6 89.1 this paper (s24) 91.1 89.9 90.5 Table 1: Results for different methods. PPK07, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a mod</context>
<context position="30830" citStr="Petrov et al., 2007" startWordPosition="5426" endWordPosition="5429"> of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours—for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions—and 15 some combination of the two methods may give further gains. Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features. Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach. Table 2 shows the accuracy of the model in recovering unlabeled dependencies. The method shows improvements over the method described in (Koo et al., 2008), which is a state-of-the-art second-order dependency parser similar to that of (</context>
</contexts>
<marker>Petrov, Pauls, Klein, 2007</marker>
<rawString>Petrov, S., A. Pauls, and D. Klein. 2007. Discriminative loglinear grammars with latent variables. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
<author>R Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="5740" citStr="Ratnaparkhi et al., 1994" startWordPosition="927" endWordPosition="930"> 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005)). A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser.1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon1Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our par</context>
</contexts>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Ratnaparkhi, A., S. Roukos, and R. Ward. 1994. A maximum entropy model for parsing. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Hpsg parsing with shallow dependency constraints.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>624--631</pages>
<contexts>
<context position="8730" citStr="Sagae et al., 2007" startWordPosition="1430" endWordPosition="1433">tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair (E, D) where E is a set of spines, and D is a set of dependencies 2Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 10 Figure 1: Two example trees. specifying how the spines are combined to form a parse tree. The spines are similar to elementary trees in TAG. Some examples are as follows: NP S NP John VBD cake ate These structures do not have substitution nodes, as is common in TAGs.3 Instead, the spines consist of a lexical anchor together with a series of unary projections, which usually correspond to different X-bar levels associated with the anchor. The operations used to combine spines are similar to</context>
</contexts>
<marker>Sagae, Miyao, Tsujii, 2007</marker>
<rawString>Sagae, Kenji, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Hpsg parsing with shallow dependency constraints. In Proc. ACL, pages 624–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>Incremental ltag parsing.</title>
<date>2005</date>
<booktitle>In Proc HLT-EMNLP.</booktitle>
<contexts>
<context position="8253" citStr="Shen and Joshi, 2005" startWordPosition="1341" endWordPosition="1344"> (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 2 The TAG-Based Parsing Model 2.1 Derivations This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair (E, D) where E is a set of spines, and D is a set of dependencies 2Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 10 Figure 1: Two ex</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>Shen, L. and A.K. Joshi. 2005. Incremental ltag parsing. In Proc HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP-2004.</booktitle>
<contexts>
<context position="1211" citStr="Taskar et al., 2004" startWordPosition="176" endWordPosition="179">tic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is y* = arg max w · f(x, y) (1) yEY(X) where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibility in the features which can be included in </context>
<context position="2918" citStr="Taskar et al., 2004" startWordPosition="474" endWordPosition="477">n3G) time, where n is the length of the sentence, and G is a grammar constant. The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f(x, y) that can be used in the model. The formalism that we describe </context>
<context position="7032" citStr="Taskar et al. (2004)" startWordPosition="1134" endWordPosition="1137">pproach. ald et al., 2005). Dependency parsing can be implemented in O(n3) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, B., D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proceedings of the EMNLP-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. IWPT.</booktitle>
<contexts>
<context position="26590" citStr="Yamada and Matsumoto, 2003" startWordPosition="4711" endWordPosition="4715">, FKM08, CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 s24 KCC08 unlabeled 92.0 91.0 KCC08 labeled 92.5 91.7 this paper 93.5 92.5 Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. derivations from the Penn treebank trees. Note that the mapping from parse trees to derivations is many-to-one: for example, the example trees in section 2.3 have structures that are as “flat” (have as few levels) as is possible, given the set D that is involved. Other similar tree</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, H. and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>