<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.992771">
Embedding Methods for Fine Grained Entity Type Classification
</title>
<author confidence="0.997912">
Dani Yogatama
</author>
<affiliation confidence="0.984955">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.707768">
Pittsburgh, PA 15213
</address>
<email confidence="0.990069">
dyogatama@cs.cmu.edu
</email>
<author confidence="0.957464">
Dan Gillick, Nevena Lazic
</author>
<affiliation confidence="0.920144">
Google Research
</affiliation>
<address confidence="0.75436">
1600 Amphitheatre Parkway
Mountain View, CA 94043
</address>
<email confidence="0.998778">
{dgillick,nevena}@google.com
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990461538462">
We propose a new approach to the task
of fine grained entity type classifications
based on label embeddings that allows for
information sharing among related labels.
Specifically, we learn an embedding for
each label and each feature such that la-
bels which frequently co-occur are close in
the embedded space. We show that it out-
performs state-of-the-art methods on two
fine grained entity-classification bench-
marks and that the model can exploit the
finer-grained labels to improve classifica-
tion of standard coarse types.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888">
Entity type classification is the task of assign-
ing type labels (e.g., person, location,
organization) to mentions of entities in doc-
uments. These types are useful for deeper natural
language analysis such as coreference resolution
(Recasens et al., 2013), relation extraction (Yao et
al., 2010), and downstream applications such as
knowledge base construction (Carlson et al., 2010)
and question answering (Lin et al., 2012).
Standard entity type classification tasks use a
small set of coarse labels, typically fewer than 20
(Hirschman and Chinchor, 1997; Sang and Meul-
der, 2003; Doddington et al., 2004). Recent work
has focused on a much larger set of fine grained
labels (Ling and Weld, 2012; Yosef et al., 2012;
Gillick et al., 2014). Fine grained labels are typ-
ically subtypes of the standard coarse labels (e.g.,
artist is a subtype of person and author is
a subtype of artist), so the label space forms a
tree-structured is-a hierarchy. See Figure 1 for the
label sets used in our experiments. A mention la-
beled with type artist should also be labeled
with all ancestors of artist. Since we allow
mentions to have multiple labels, this is a multi-
label classification task. Multiple labels typically
correspond to a single path in the tree (from root
to a leaf or internal node).
An important aspect of context-dependent fine
grained entity type classification is that mentions
of an entity can have different types depending
on the context. Consider the following example:
Madonna starred as Breathless Mahoney in the
film Dick Tracy. In this context, the most appropri-
ate label for the mention Madonna is actress,
since the sentence talks about her role in a film. In
the majority of other cases, Madonna is likely to
be labeled as a musician.
The main difficulty in fine grained entity type
classification is the absence of labeled training ex-
amples. Training data is typically generated au-
tomatically (e.g. by mapping Freebase labels of
resolved entities), without taking context into ac-
count, so it is common for mentions to have noisy
labels. In our example, the labels for the mention
Madonna would include musician, actress,
author, and potentially others, even though not
all of these labels apply here. Ideally, a fine
grained type classification system should be ro-
bust to such noisy training data, as well as capable
of exploiting relationships between labels during
learning. We describe a model that uses a rank-
ing loss—which tends to be more robust to la-
bel noise—and that learns a joint representation of
features and labels, which allows for information
sharing among related labels.1 A related idea to
learn output representations for multiclass docu-
ment classification and part-of-speech tagging was
considered in Srikumar and Manning (2014). We
show that it outperforms state-of-the-art methods
on two fine grained entity-classification bench-
marks. We also evaluate our model on standard
coarse type classification and find that training em-
bedding models on all fine grained labels gives
better results than training it on just the coarse
</bodyText>
<footnote confidence="0.999039333333333">
1Turian et al. (2010), Collobert et al. (2011), and Qi et
al. (2014) consider representation learning for coarse label
named entity recognition.
</footnote>
<page confidence="0.598498">
291
</page>
<figureCaption confidence="0.3927105">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER.
</figureCaption>
<bodyText confidence="0.708645">
types of interest.
</bodyText>
<sectionHeader confidence="0.979515" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999247296296296">
In this section, we describe our approach, which is
based on the WSABIE (Weston et al., 2011) model.
Notation We use lower case letters to denote
variables, bold lower case letters to denote vectors,
and bold upper case letters to denote matrices. Let
x ∈ RD be the feature vector for a mention, where
D is the number of features and xd is the value of
the d-th feature. Let y ∈ {0,1}T be the corre-
sponding binary label vector, where T is the num-
ber of labels. yt = 1 if and only if the mention
is of type t. We use yt to denote a one-hot binary
vector of size T, where yt = 1 and all other entries
are zero.
Model To leverage the relationships among the
fine grained labels, we would like a model that can
learn an embedding space for labels. Our model,
based on WSABIE, learns to map both feature vec-
tors and labels to a low dimensional space RH
(H is the embedding dimension size) such that
each instance is close to its label(s) in this space;
see Figure 2 for an illustration. Relationships be-
tween labels are captured by their distances in the
embedded space: co-occurring labels tend to be
closer, whereas mutually exclusive labels are fur-
ther apart.
Formally, we are interested in learning the map-
ping functions:
</bodyText>
<equation confidence="0.9988125">
f(x) : RD → RH
∀t ∈ {1, 2, ... , T}, g(yt) : {0,1}T →RH
</equation>
<bodyText confidence="0.9978224">
In this work, we parameterize them as linear func-
tions f(x, A) = Ax and g(yt, B) = Byt, where
A ∈ RH×D and B ∈ RH×T are parameters.
The score of a label t (represented as a one-hot
label vector yt) and a feature vector x is the dot
</bodyText>
<equation confidence="0.50767825">
RH
Ax
Byt
X AT BT Yt
</equation>
<figureCaption confidence="0.8070335">
Figure 2: An illustration of the standard WSABIE model.
x is the feature vector extracted from a mention, and yt is
its label. Here, black cells indicate non-zero and white cells
indicate zero values. The parameters are matrices A and B
which are used to map the feature vector x and the label vec-
tor yt into an embedding space.
</figureCaption>
<bodyText confidence="0.925943888888889">
product between their embeddings:
s(x, yt; A, B) = f(x, A) · g(yt, B) = Ax · Byt
For brevity, we denote this score by s(x, yt). Note
that the total number of parameters is (D+T)×H,
which is typically less than the number of pa-
rameters in standard classification models that use
regular conjunctions of input features with label
classes (e.g., logistic regression) when H &lt; T.
Learning Since we expect the training data to
contain some extraneous labels, we use a ranking
loss to encourage the model to place positive la-
bels above negative labels without competing with
each other. Let Y denote the set of positive labels
for a mention, and let Y¯ denote its complement.
Intuitively, we try to rank labels in Y higher than
labels in ¯Y. Specifically, we use the weighted ap-
proximate pairwise (WARP) loss of Weston et al.
(2011). For a mention {x, y}, the WARP loss is:
</bodyText>
<equation confidence="0.716142">
R(rank(x, yt)) max(1 − s(x, yt) + s(x, y¯t), 0)
</equation>
<bodyText confidence="0.99585575">
where rank(x, yt) is the margin-infused rank of
label t: rank(x, yt) = E¯t∈¯Y ff(1 + s(x, y¯t) &gt;
s(x, yt)), R(rank(x, yt)) is a function that trans-
forms this rank into a weight. In this work, since
</bodyText>
<figure confidence="0.75605175">
E
t∈Y
E
¯t∈¯Y
</figure>
<page confidence="0.987174">
292
</page>
<bodyText confidence="0.9860935">
each mention can have multiple positive labels,
we choose to optimize precision at k by setting
</bodyText>
<equation confidence="0.998579666666667">
1
9t(k) = �k i . Favoring precision over recall in
i=1
</equation>
<bodyText confidence="0.99997032">
fine grained entity type classification makes sense
because if we are not certain about a particular fine
grained label for a mention, we should use its an-
cestor label in the hierarchy.
In order to learn the parameters with this WARP
loss, we use stochastic (sub)gradient descent.
Inference During inference, we consider the
top-k predicted labels, where k is the maximum
depth of the label hierarchy, and greedily remove
labels that are not consistent with other labels (i.e.,
not on the same path of the tree). For example, if
the (ordered) top-k labels are person, artist,
and location, we output only person and
artist as the predicted labels. We use a thresh-
old S such that ˆyt = 1 if s(x, yt) &gt; S and ˆyt = 0
otherwise.
Kernel extension We extend the WSABIE
model to include a weighting function between
each feature and label, similar in spirit to We-
ston et al. (2014). Recall that the WSABIE
scoring function is: s(x, yt) = Ax · Byt =
Ed(Adxd)TBt, where Ad and Bt denote the col-
umn vectors of A and B. We can weight each
(feature, label) pair by a kernel function prior to
computing the embedding:
</bodyText>
<equation confidence="0.6740685">
�s(x, yt) = Kd,t(Adxd)TBt,
d
</equation>
<bodyText confidence="0.9999726875">
where K E RD×T is the kernel matrix. We use
a N-nearest neighbor kernel2 and set Kd,t = 1
if Ad is one of N-nearest neighbors of the label
vector Bt, and Kd,t = 0 otherwise. In all our
experiments, we set N = 200.
To incorporate the kernel weighting function,
we only need to make minor modifications to the
learning procedure. At every iteration, we first
compute the similarity between each feature em-
bedding and each label embedding. For each label
t, we then set the kernel values for the N most
similar features to 1, and the rest to 0 (update K).
We can then follow the learning algorithm for the
standard WSABIE model described above. At in-
ference time, we fix K so this extension is only
slightly slower than the standard model.
</bodyText>
<footnote confidence="0.834643">
2We explored various kernels in preliminary experiments
and found that the nearest neighbor kernel performs the best.
</footnote>
<bodyText confidence="0.999823166666667">
The nearest-neighbor kernel introduces nonlin-
earities to the embedding model. It implicitly
plays the role of a label-dependent feature selector,
learning which features can interact with which la-
bels and turns off potentially noisy features that
are not in the relevant label’s neighborhood.
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999947097560976">
Setup and Baselines We evaluate our methods
on two publicly available datasets that are man-
ually annotated with gold labels for fine grained
entity type classification: GFT (Google Fine
Types; Gillick et al., 2014) and FIGER (Ling and
Weld, 2012). On the GFT dataset, we compare
with state-of-the-art baselines from Gillick et al.
(2014): flat logistic regression (FLAT), an exten-
sion of multiclass logistic regression for multilabel
classification problems; and multiple independent
binary logistic regression (BINARY), one per label
t E {1, 2, ... , T}. On the FIGER dataset, we com-
pare with a state-of-the-art baseline from Ling and
Weld (2012).
We denote the standard embedding method by
WSABIE and its extension by K-WSABIE. We fix
our embedding size to H = 50. We report micro-
averaged precision, recall, and F1-score for each
of the competing methods (this is called Loose Mi-
cro by Ling and Weld). When development data is
available, we use it to tune S by optimizing F1-
score.
Training data Because we have no manually
annotated data, we create training data using the
technique described in Gillick et al. (2014). A set
of 133,000 news documents are automatically an-
notated by a parser, a mention chunker, and an
entity resolver that assigns Freebase types to en-
tites, which we map to fine grained labels. This
approach results in approximately 3 million train-
ing examples which we use to train all the mod-
els evaluated below. The only difference between
models trained for different tasks is the mapping
from Freebase types. See Gillick et al. (2014) for
details.
Table 1 lists the features we use—the same set
as used by Gillick et al. (2014), and very similar to
those used by Ling and Weld. String features are
randomly hashed to a value in 0 to 999,999, which
simplifies feature extraction and adds some addi-
tional regularization (Ganchev and Dredze, 2008).
</bodyText>
<page confidence="0.997339">
293
</page>
<table confidence="0.9992023">
Feature Description Example
Head The syntactic head of the mention phrase “Obama”
Non-head Each non-head word in the mention phrase “Barack”, “H.”
Cluster Word cluster id for the head word “59”
Characters Each character trigram in the mention head “:ob”, “oba”, “bam”, “ama”, “ma:”
Shape The word shape of the words in the mention phrase “Aa A. Aa”
Role Dependency label on the mention head “subj”
Context Words before and after the mention phrase “B:who”, “A:first”
Parent The head’s lexical parent in the dependency tree “picked”
Topic The most likely topic label for the document “politics”
</table>
<tableCaption confidence="0.9971265">
Table 1: List of features used in our experiments, similar to features in Gillick et al. (2014). Features are extracted from each
mention. The example mention in context is ... who Barack H. Obama first picked ....
</tableCaption>
<table confidence="0.9994914">
GFT Dev GFT Test FIGER
Total mentions 6,380 11,324 778
at Level 1 3,934 7,975 568
at Level 2 2,215 2,994 210
at Level 3 251 335 –
</table>
<tableCaption confidence="0.99983">
Table 2: Mention counts in our datasets.
</tableCaption>
<bodyText confidence="0.999765541666667">
GFT evaluation There are T = 86 fine grained
labels in the GFT dataset, as listed in Figure 1. The
four top-level labels are: person, location,
organization, and other; the remaining la-
bels are subtypes of these labels. The maximum
depth of a label is 3. We split the dataset into a
development set (for tuning hyperparameters) and
test set (see Table 2).
The overall experimental results are shown in
Table 3. Embedding methods performed well.
Both WSABIE and K-WSABIE outperformed the
baselines by substantial margins in F1-score,
though the advantage of the kernel version over
the linear version is only marginally significant.
To visualize the learned embeddings, we project
label embeddings down to two dimensions using
PCA in Figure 3. Since there are only 4 top-level
labels here, the fine grained labels are color-coded
according to their top-level labels for readability.
We can see that related labels are clustered to-
gether, and the four major clusters correspond to
to the top-level labels. We note that these first two
components only capture 14% of the total variance
of the full 50-dimensional space.
</bodyText>
<table confidence="0.9996326">
Method P R F1
FLAT 79.22 60.18 68.40
BINARY 80.05 62.20 70.01
WSABIE 80.58 66.20 72.68
K-WSABIE 80.11 67.01 72.98
</table>
<tableCaption confidence="0.96738525">
Table 3: Precision (P), Recall (R), and F1-score on the GFT
test dataset for four competing models. The improvements
for WSABIE and K-WSABIE over both baselines are statisti-
cally significant (p &lt; 0.01).
</tableCaption>
<equation confidence="0.2709545">
-0.4-0.2 OA 0.2 0.4 0.6 0.8
PC1
</equation>
<bodyText confidence="0.956824315789474">
et. See text for details.
evaluation Our second evaluation
dataset is
from Ling and Weld (2012). In
this dataset, there are T = 112 labels organized
in a two-level hierarchy; however, only 102
appear in our training data (see Figure 1, taken
from their paper, for the complete set of labels).
The training labels include 37 top-level labels
(e.g., person, location, product, art,
etc.) and 75 second-level labels (e.g., actor,
city, engine, etc.) The
dataset is much
smaller than the GFT dataset (see Table 2).
Our experimental results are shown in Ta-
ble 4. Again, K-WSABIE performed the best,
followed by the standard WSABIE model. Both
of these methods significantly outperformed Ling
an
</bodyText>
<table confidence="0.956589">
FIGER
FIGER
FIGER
d Weld’s best result.
Method P R F1
Ling an d Weld (2012) – – 69.30
WSABIE 81.85 63.75 71.68
K-WSABIE 82.23 64.55 72.35
location
location
organization
other
person
Figure 3: Two-dimensional projections of label embed-
dings for GFT datas
</table>
<tableCaption confidence="0.858943">
Table 4: Precision (P), Recall (R), and
on the
</tableCaption>
<bodyText confidence="0.823605333333333">
dataset for three competing models. We took the
score from Ling and
best result (no precision and re-
call numbers were reported). The improvements for WSABIE
and K-WSABIE over the baseline are statistically significan
F1-score
</bodyText>
<figure confidence="0.963447106382979">
FIGER
F1
Weld’s
t
(p &lt; 0.01).
other
programming_language
filmart
maladysupernatural
software �an&apos;Furiting
health living_thing
animal�music
spocaf �li1�parly
scientific
currecy
natural_disaster
broadcst
heritage
violent_conflict
acident
celestialsports_league
companY
e[ock-� QQ_teamtion
s
director
religious_leader
doctor
title
athlete military
education political_figure
teacher
music
business
author
artist
person
railway bridge
mfto
city
8 Po
i
6
ewcation
res
legal
-0.6 -0.4 -0.2 0.0 0.2 0.4 0.6
PC2
</figure>
<page confidence="0.995841">
294
</page>
<bodyText confidence="0.999575611111111">
Feature learning We investigate whether hav-
ing a large fine grained label space is helpful in
learning a good representation for feature vec-
tors (recall that WSABIE learns representations for
both feature vectors and labels). We focus on the
task of coarse type classification, where we want
to classify a mention into one of the four top-level
GFT labels. We fix the training mentions and learn
WSABIE embeddings for feature vectors and la-
bels by (1) training only on coarse labels and (2)
training on all labels; we evaluate the models only
on coarse labels. Training with all labels gives
an improvement of about 2 points (F1 score) over
training with just coarse labels, as shown in Ta-
ble 5. This suggests that including additional sub-
type labels can help us learn better feature embed-
dings, even if we are not explicitly interested in the
deeper labels.
</bodyText>
<table confidence="0.994787">
Training labels P R F1
Coarse labels only 82.41 77.87 80.07
All labels 85.18 79.28 82.12
</table>
<tableCaption confidence="0.919975">
Table 5: Comparison of two WSABIE models on coarse
type classification for GFT. The first model only used coarse
</tableCaption>
<bodyText confidence="0.597697">
top-level labels, while the second model was trained on all 86
labels.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9986782">
Design of fine grained label hierarchy Results
at different levels of the hierarchies in Table 6
show that it is more difficult to discriminate among
deeper labels. However, it appears that the depth-
2 FIGER types are easier to discriminate than the
depth-2 (and depth-3) GFT labels. This may sim-
ply be an artifact of the very small FIGER dataset,
but it suggests it may be worthwhile to flatten the
other subtree ini GFT since many of its subtypes
do not obviously share any information.
</bodyText>
<table confidence="0.999011571428571">
GFT P R F1
LEVEL 1 85.22 80.55 82.82
LEVEL 2 56.02 37.14 44.67
LEVEL 3 65.12 7.89 14.07
FIGER P R F1
LEVEL 1 82.82 70.42 76.12
LEVEL 2 68.28 47.14 55.77
</table>
<tableCaption confidence="0.825077">
Table 6: WSABIE model’s Precision (P), Recall (R), and
F1-score at each level of the label hierarchies for GFT (top)
and FIGER (bottom).
</tableCaption>
<sectionHeader confidence="0.999215" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999915857142857">
We introduced embedding methods for fine
grained entity type classifications that outperforms
state-of-the-art methods on benchmark entity-
classification datasets. We showed that these
methods learned reasonable embeddings for fine-
type labels which allowed information sharing
across related labels.
</bodyText>
<sectionHeader confidence="0.997274" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999964666666667">
We thank Andrew McCallum for helpful discus-
sions and anonymous reviewers for feedback on
an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.999419" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999014809523809">
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In Proc. of WSDM.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program tasks, data, and evaluation. In
Proc. of LREC.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In Pro-
ceedings of the ACL08 HLT Workshop on Mobile
Language Processing, pages 19–20.
Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. In
arXiv.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 named entity task definition. In Proc. of MUC-7.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No
noun phrase left behind: Detecting and typing un-
linkable entities. In Proc. of EMNLP-CoNLL.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In Proc. of AAAI.
Yanjun Qi, Sujatha Das G, Ronan Collobert, and Jason
Weston. 2014. Deep learning for character-based
information extraction. In Proc. of ECIR.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proc. of NAACL.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
language-independent named entity recognition. In
Proc. of HLT-NAACL.
Vivek Srikumar and Christopher D. Manning. 2014.
Learning distributed representations for structured
output prediction. In Proc. of NIPS.
</reference>
<page confidence="0.975217">
295
</page>
<reference confidence="0.999467866666667">
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary im-
age annotation. In Proc. of IJCAI.
Jason Weston, Ron Weiss, and Hector Yee. 2014.
Affinity weighted embedding. In Proc. of ICML.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proc. of EMNLP.
Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: Hierarchical type classification for entity
names. In Proc. of COLING.
</reference>
<page confidence="0.998563">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502182">
<title confidence="0.999973">Embedding Methods for Fine Grained Entity Type Classification</title>
<author confidence="0.996537">Dani</author>
<affiliation confidence="0.9489425">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.89473">Pittsburgh, PA</address>
<email confidence="0.999449">dyogatama@cs.cmu.edu</email>
<author confidence="0.988919">Dan Gillick</author>
<author confidence="0.988919">Nevena</author>
<affiliation confidence="0.672446">Google</affiliation>
<address confidence="0.9469725">1600 Amphitheatre Mountain View, CA</address>
<abstract confidence="0.999407142857143">We propose a new approach to the task of fine grained entity type classifications based on label embeddings that allows for information sharing among related labels. Specifically, we learn an embedding for each label and each feature such that labels which frequently co-occur are close in the embedded space. We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard C Wang</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proc. of WSDM.</booktitle>
<contexts>
<context position="1229" citStr="Carlson et al., 2010" startWordPosition="172" endWordPosition="175">d space. We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types. 1 Introduction Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used </context>
</contexts>
<marker>Carlson, Betteridge, Wang, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proc. of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="3959" citStr="Collobert et al. (2011)" startWordPosition="619" endWordPosition="622"> that learns a joint representation of features and labels, which allows for information sharing among related labels.1 A related idea to learn output representations for multiclass document classification and part-of-speech tagging was considered in Srikumar and Manning (2014). We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks. We also evaluate our model on standard coarse type classification and find that training embedding models on all fine grained labels gives better results than training it on just the coarse 1Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER. types of interest. 2 Models In this section, we describe our approach, which is based on the WSABIE (Weston et al., 2011) mod</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction (ACE) program tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="1452" citStr="Doddington et al., 2004" startWordPosition="208" endWordPosition="211">s. 1 Introduction Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used in our experiments. A mention labeled with type artist should also be labeled with all ancestors of artist. Since we allow mentions to have multiple labels, this is a multilabel classification task. Multiple labels typicall</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ACE) program tasks, data, and evaluation. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Mark Dredze</author>
</authors>
<title>Small statistical models by random feature mixing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="11771" citStr="Ganchev and Dredze, 2008" startWordPosition="2002" endWordPosition="2005">signs Freebase types to entites, which we map to fine grained labels. This approach results in approximately 3 million training examples which we use to train all the models evaluated below. The only difference between models trained for different tasks is the mapping from Freebase types. See Gillick et al. (2014) for details. Table 1 lists the features we use—the same set as used by Gillick et al. (2014), and very similar to those used by Ling and Weld. String features are randomly hashed to a value in 0 to 999,999, which simplifies feature extraction and adds some additional regularization (Ganchev and Dredze, 2008). 293 Feature Description Example Head The syntactic head of the mention phrase “Obama” Non-head Each non-head word in the mention phrase “Barack”, “H.” Cluster Word cluster id for the head word “59” Characters Each character trigram in the mention head “:ob”, “oba”, “bam”, “ama”, “ma:” Shape The word shape of the words in the mention phrase “Aa A. Aa” Role Dependency label on the mention head “subj” Context Words before and after the mention phrase “B:who”, “A:first” Parent The head’s lexical parent in the dependency tree “picked” Topic The most likely topic label for the document “politics” </context>
</contexts>
<marker>Ganchev, Dredze, 2008</marker>
<rawString>Kuzman Ganchev and Mark Dredze. 2008. Small statistical models by random feature mixing. In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing, pages 19–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Nevena Lazic</author>
<author>Kuzman Ganchev</author>
<author>Jesse Kirchner</author>
<author>David Huynh</author>
</authors>
<title>Contextdependent fine-grained entity type tagging.</title>
<date>2014</date>
<booktitle>In arXiv.</booktitle>
<contexts>
<context position="1585" citStr="Gillick et al., 2014" startWordPosition="233" endWordPosition="236">entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used in our experiments. A mention labeled with type artist should also be labeled with all ancestors of artist. Since we allow mentions to have multiple labels, this is a multilabel classification task. Multiple labels typically correspond to a single path in the tree (from root to a leaf or internal node). An important aspect of context-dependent fine grain</context>
<context position="4384" citStr="Gillick et al. (2014)" startWordPosition="679" endWordPosition="682">oarse type classification and find that training embedding models on all fine grained labels gives better results than training it on just the coarse 1Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER. types of interest. 2 Models In this section, we describe our approach, which is based on the WSABIE (Weston et al., 2011) model. Notation We use lower case letters to denote variables, bold lower case letters to denote vectors, and bold upper case letters to denote matrices. Let x ∈ RD be the feature vector for a mention, where D is the number of features and xd is the value of the d-th feature. Let y ∈ {0,1}T be the corresponding binary label vector, where T is the number of labels. yt = 1 if and only if the mention is of type t. We use yt to </context>
<context position="10115" citStr="Gillick et al., 2014" startWordPosition="1722" endWordPosition="1725">us kernels in preliminary experiments and found that the nearest neighbor kernel performs the best. The nearest-neighbor kernel introduces nonlinearities to the embedding model. It implicitly plays the role of a label-dependent feature selector, learning which features can interact with which labels and turns off potentially noisy features that are not in the relevant label’s neighborhood. 3 Experiments Setup and Baselines We evaluate our methods on two publicly available datasets that are manually annotated with gold labels for fine grained entity type classification: GFT (Google Fine Types; Gillick et al., 2014) and FIGER (Ling and Weld, 2012). On the GFT dataset, we compare with state-of-the-art baselines from Gillick et al. (2014): flat logistic regression (FLAT), an extension of multiclass logistic regression for multilabel classification problems; and multiple independent binary logistic regression (BINARY), one per label t E {1, 2, ... , T}. On the FIGER dataset, we compare with a state-of-the-art baseline from Ling and Weld (2012). We denote the standard embedding method by WSABIE and its extension by K-WSABIE. We fix our embedding size to H = 50. We report microaveraged precision, recall, and </context>
<context position="11461" citStr="Gillick et al. (2014)" startWordPosition="1948" endWordPosition="1951"> we use it to tune S by optimizing F1- score. Training data Because we have no manually annotated data, we create training data using the technique described in Gillick et al. (2014). A set of 133,000 news documents are automatically annotated by a parser, a mention chunker, and an entity resolver that assigns Freebase types to entites, which we map to fine grained labels. This approach results in approximately 3 million training examples which we use to train all the models evaluated below. The only difference between models trained for different tasks is the mapping from Freebase types. See Gillick et al. (2014) for details. Table 1 lists the features we use—the same set as used by Gillick et al. (2014), and very similar to those used by Ling and Weld. String features are randomly hashed to a value in 0 to 999,999, which simplifies feature extraction and adds some additional regularization (Ganchev and Dredze, 2008). 293 Feature Description Example Head The syntactic head of the mention phrase “Obama” Non-head Each non-head word in the mention phrase “Barack”, “H.” Cluster Word cluster id for the head word “59” Characters Each character trigram in the mention head “:ob”, “oba”, “bam”, “ama”, “ma:” Sh</context>
</contexts>
<marker>Gillick, Lazic, Ganchev, Kirchner, Huynh, 2014</marker>
<rawString>Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Contextdependent fine-grained entity type tagging. In arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<title>MUC7 named entity task definition.</title>
<date>1997</date>
<booktitle>In Proc. of MUC-7.</booktitle>
<contexts>
<context position="1402" citStr="Hirschman and Chinchor, 1997" startWordPosition="199" endWordPosition="202">bels to improve classification of standard coarse types. 1 Introduction Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used in our experiments. A mention labeled with type artist should also be labeled with all ancestors of artist. Since we allow mentions to have multiple labels, this is a multil</context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. MUC7 named entity task definition. In Proc. of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>No noun phrase left behind: Detecting and typing unlinkable entities.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1271" citStr="Lin et al., 2012" startWordPosition="179" endWordPosition="182">the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types. 1 Introduction Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used in our experiments. A mention labeled with</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun phrase left behind: Detecting and typing unlinkable entities. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Daniel S Weld</author>
</authors>
<title>Fine-grained entity recognition.</title>
<date>2012</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="1542" citStr="Ling and Weld, 2012" startWordPosition="225" endWordPosition="228">, location, organization) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used in our experiments. A mention labeled with type artist should also be labeled with all ancestors of artist. Since we allow mentions to have multiple labels, this is a multilabel classification task. Multiple labels typically correspond to a single path in the tree (from root to a leaf or internal node). An impor</context>
<context position="4419" citStr="Ling and Weld (2012)" startWordPosition="684" endWordPosition="687">hat training embedding models on all fine grained labels gives better results than training it on just the coarse 1Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER. types of interest. 2 Models In this section, we describe our approach, which is based on the WSABIE (Weston et al., 2011) model. Notation We use lower case letters to denote variables, bold lower case letters to denote vectors, and bold upper case letters to denote matrices. Let x ∈ RD be the feature vector for a mention, where D is the number of features and xd is the value of the d-th feature. Let y ∈ {0,1}T be the corresponding binary label vector, where T is the number of labels. yt = 1 if and only if the mention is of type t. We use yt to denote a one-hot binary vector of s</context>
<context position="10147" citStr="Ling and Weld, 2012" startWordPosition="1728" endWordPosition="1731">ents and found that the nearest neighbor kernel performs the best. The nearest-neighbor kernel introduces nonlinearities to the embedding model. It implicitly plays the role of a label-dependent feature selector, learning which features can interact with which labels and turns off potentially noisy features that are not in the relevant label’s neighborhood. 3 Experiments Setup and Baselines We evaluate our methods on two publicly available datasets that are manually annotated with gold labels for fine grained entity type classification: GFT (Google Fine Types; Gillick et al., 2014) and FIGER (Ling and Weld, 2012). On the GFT dataset, we compare with state-of-the-art baselines from Gillick et al. (2014): flat logistic regression (FLAT), an extension of multiclass logistic regression for multilabel classification problems; and multiple independent binary logistic regression (BINARY), one per label t E {1, 2, ... , T}. On the FIGER dataset, we compare with a state-of-the-art baseline from Ling and Weld (2012). We denote the standard embedding method by WSABIE and its extension by K-WSABIE. We fix our embedding size to H = 50. We report microaveraged precision, recall, and F1-score for each of the competi</context>
<context position="14318" citStr="Ling and Weld (2012)" startWordPosition="2429" endWordPosition="2432">the four major clusters correspond to to the top-level labels. We note that these first two components only capture 14% of the total variance of the full 50-dimensional space. Method P R F1 FLAT 79.22 60.18 68.40 BINARY 80.05 62.20 70.01 WSABIE 80.58 66.20 72.68 K-WSABIE 80.11 67.01 72.98 Table 3: Precision (P), Recall (R), and F1-score on the GFT test dataset for four competing models. The improvements for WSABIE and K-WSABIE over both baselines are statistically significant (p &lt; 0.01). -0.4-0.2 OA 0.2 0.4 0.6 0.8 PC1 et. See text for details. evaluation Our second evaluation dataset is from Ling and Weld (2012). In this dataset, there are T = 112 labels organized in a two-level hierarchy; however, only 102 appear in our training data (see Figure 1, taken from their paper, for the complete set of labels). The training labels include 37 top-level labels (e.g., person, location, product, art, etc.) and 75 second-level labels (e.g., actor, city, engine, etc.) The dataset is much smaller than the GFT dataset (see Table 2). Our experimental results are shown in Table 4. Again, K-WSABIE performed the best, followed by the standard WSABIE model. Both of these methods significantly outperformed Ling an FIGER</context>
</contexts>
<marker>Ling, Weld, 2012</marker>
<rawString>Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Qi</author>
<author>Sujatha Das G</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Deep learning for character-based information extraction.</title>
<date>2014</date>
<booktitle>In Proc. of ECIR.</booktitle>
<contexts>
<context position="3981" citStr="Qi et al. (2014)" startWordPosition="624" endWordPosition="627">tation of features and labels, which allows for information sharing among related labels.1 A related idea to learn output representations for multiclass document classification and part-of-speech tagging was considered in Srikumar and Manning (2014). We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks. We also evaluate our model on standard coarse type classification and find that training embedding models on all fine grained labels gives better results than training it on just the coarse 1Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER. types of interest. 2 Models In this section, we describe our approach, which is based on the WSABIE (Weston et al., 2011) model. Notation We use lo</context>
</contexts>
<marker>Qi, G, Collobert, Weston, 2014</marker>
<rawString>Yanjun Qi, Sujatha Das G, Ronan Collobert, and Jason Weston. 2014. Deep learning for character-based information extraction. In Proc. of ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: language-independent named entity recognition. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning distributed representations for structured output prediction.</title>
<date>2014</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="3614" citStr="Srikumar and Manning (2014)" startWordPosition="565" endWordPosition="568">author, and potentially others, even though not all of these labels apply here. Ideally, a fine grained type classification system should be robust to such noisy training data, as well as capable of exploiting relationships between labels during learning. We describe a model that uses a ranking loss—which tends to be more robust to label noise—and that learns a joint representation of features and labels, which allows for information sharing among related labels.1 A related idea to learn output representations for multiclass document classification and part-of-speech tagging was considered in Srikumar and Manning (2014). We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks. We also evaluate our model on standard coarse type classification and find that training embedding models on all fine grained labels gives better results than training it on just the coarse 1Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language P</context>
</contexts>
<marker>Srikumar, Manning, 2014</marker>
<rawString>Vivek Srikumar and Christopher D. Manning. 2014. Learning distributed representations for structured output prediction. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3934" citStr="Turian et al. (2010)" startWordPosition="615" endWordPosition="618">ust to label noise—and that learns a joint representation of features and labels, which allows for information sharing among related labels.1 A related idea to learn output representations for multiclass document classification and part-of-speech tagging was considered in Srikumar and Manning (2014). We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks. We also evaluate our model on standard coarse type classification and find that training embedding models on all fine grained labels gives better results than training it on just the coarse 1Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER. types of interest. 2 Models In this section, we describe our approach, which is based on the WSABIE </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="4555" citStr="Weston et al., 2011" startWordPosition="707" endWordPosition="710"> Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition. 291 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291–296, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Label sets for Gillick et al. (2014)—left, GFT—and Ling and Weld (2012)—right, FIGER. types of interest. 2 Models In this section, we describe our approach, which is based on the WSABIE (Weston et al., 2011) model. Notation We use lower case letters to denote variables, bold lower case letters to denote vectors, and bold upper case letters to denote matrices. Let x ∈ RD be the feature vector for a mention, where D is the number of features and xd is the value of the d-th feature. Let y ∈ {0,1}T be the corresponding binary label vector, where T is the number of labels. yt = 1 if and only if the mention is of type t. We use yt to denote a one-hot binary vector of size T, where yt = 1 and all other entries are zero. Model To leverage the relationships among the fine grained labels, we would like a m</context>
<context position="7147" citStr="Weston et al. (2011)" startWordPosition="1196" endWordPosition="1199">han the number of parameters in standard classification models that use regular conjunctions of input features with label classes (e.g., logistic regression) when H &lt; T. Learning Since we expect the training data to contain some extraneous labels, we use a ranking loss to encourage the model to place positive labels above negative labels without competing with each other. Let Y denote the set of positive labels for a mention, and let Y¯ denote its complement. Intuitively, we try to rank labels in Y higher than labels in ¯Y. Specifically, we use the weighted approximate pairwise (WARP) loss of Weston et al. (2011). For a mention {x, y}, the WARP loss is: R(rank(x, yt)) max(1 − s(x, yt) + s(x, y¯t), 0) where rank(x, yt) is the margin-infused rank of label t: rank(x, yt) = E¯t∈¯Y ff(1 + s(x, y¯t) &gt; s(x, yt)), R(rank(x, yt)) is a function that transforms this rank into a weight. In this work, since E t∈Y E ¯t∈¯Y 292 each mention can have multiple positive labels, we choose to optimize precision at k by setting 1 9t(k) = �k i . Favoring precision over recall in i=1 fine grained entity type classification makes sense because if we are not certain about a particular fine grained label for a mention, we shoul</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Ron Weiss</author>
<author>Hector Yee</author>
</authors>
<title>Affinity weighted embedding.</title>
<date>2014</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="8479" citStr="Weston et al. (2014)" startWordPosition="1440" endWordPosition="1444">ic (sub)gradient descent. Inference During inference, we consider the top-k predicted labels, where k is the maximum depth of the label hierarchy, and greedily remove labels that are not consistent with other labels (i.e., not on the same path of the tree). For example, if the (ordered) top-k labels are person, artist, and location, we output only person and artist as the predicted labels. We use a threshold S such that ˆyt = 1 if s(x, yt) &gt; S and ˆyt = 0 otherwise. Kernel extension We extend the WSABIE model to include a weighting function between each feature and label, similar in spirit to Weston et al. (2014). Recall that the WSABIE scoring function is: s(x, yt) = Ax · Byt = Ed(Adxd)TBt, where Ad and Bt denote the column vectors of A and B. We can weight each (feature, label) pair by a kernel function prior to computing the embedding: �s(x, yt) = Kd,t(Adxd)TBt, d where K E RD×T is the kernel matrix. We use a N-nearest neighbor kernel2 and set Kd,t = 1 if Ad is one of N-nearest neighbors of the label vector Bt, and Kd,t = 0 otherwise. In all our experiments, we set N = 200. To incorporate the kernel weighting function, we only need to make minor modifications to the learning procedure. At every ite</context>
</contexts>
<marker>Weston, Weiss, Yee, 2014</marker>
<rawString>Jason Weston, Ron Weiss, and Hector Yee. 2014. Affinity weighted embedding. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1141" citStr="Yao et al., 2010" startWordPosition="160" endWordPosition="163">and each feature such that labels which frequently co-occur are close in the embedded space. We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types. 1 Introduction Entity type classification is the task of assigning type labels (e.g., person, location, organization) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the l</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Amir Yosef</author>
<author>Sandro Bauer</author>
<author>Johannes Hoffart</author>
<author>Marc Spaniol</author>
<author>Gerhard Weikum</author>
</authors>
<title>HYENA: Hierarchical type classification for entity names.</title>
<date>2012</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1562" citStr="Yosef et al., 2012" startWordPosition="229" endWordPosition="232">ion) to mentions of entities in documents. These types are useful for deeper natural language analysis such as coreference resolution (Recasens et al., 2013), relation extraction (Yao et al., 2010), and downstream applications such as knowledge base construction (Carlson et al., 2010) and question answering (Lin et al., 2012). Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 (Hirschman and Chinchor, 1997; Sang and Meulder, 2003; Doddington et al., 2004). Recent work has focused on a much larger set of fine grained labels (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). Fine grained labels are typically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See Figure 1 for the label sets used in our experiments. A mention labeled with type artist should also be labeled with all ancestors of artist. Since we allow mentions to have multiple labels, this is a multilabel classification task. Multiple labels typically correspond to a single path in the tree (from root to a leaf or internal node). An important aspect of conte</context>
</contexts>
<marker>Yosef, Bauer, Hoffart, Spaniol, Weikum, 2012</marker>
<rawString>Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. HYENA: Hierarchical type classification for entity names. In Proc. of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>