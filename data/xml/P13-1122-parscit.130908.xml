<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.864094">
HEADY: News headline abstraction through event pattern clustering
</title>
<author confidence="0.793108">
Enrique Alfonseca Daniele Pighin Guillermo Garrido∗
</author>
<affiliation confidence="0.754492">
Google Inc. Google Inc. NLP &amp; IR Group at UNED
</affiliation>
<email confidence="0.973948">
ealfonseca@google.com biondo@google.com ggarrido@lsi.uned.es
</email>
<sectionHeader confidence="0.994357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975388888889">
This paper presents HEADY: a novel, ab-
stractive approach for headline generation
from news collections. From a web-scale
corpus of English news, we mine syntac-
tic patterns that a Noisy-OR model gener-
alizes into event descriptions. At inference
time, we query the model with the patterns
observed in an unseen news collection,
identify the event that better captures the
gist of the collection and retrieve the most
appropriate pattern to generate a head-
line. HEADY improves over a state-of-the-
art open-domain title abstraction method,
bridging half of the gap that separates
it from extractive methods using human-
generated titles in manual evaluations, and
performs comparably to human-generated
headlines as evaluated with ROUGE.
</bodyText>
<listItem confidence="0.99305705">
• Carmelo and La La Party It Up with Kim and Ciara
• La La Vazquez and Carmelo Anthony: Wedding
Day Bliss
• Carmelo Anthony, actress LaLa Vazquez wed in
NYC
• Stylist to the Stars
• LaLa, Carmelo Set Off Celebrity Wedding Weekend
• Ciara rocks a sexy Versace Spring 2010 mini to
LaLa Vasquez and Carmelo Anthony’s wedding
(photos)
• Lala Vasquez on her wedding dress, cake, reality tv
show and fianc´e, Carmelo Anthony (video)
• VAZQUEZ MARRIES SPORTS STAR AN-
THONY
• Lebron Returns To NYC For Carmelo’s Wedding
• Carmelo Anthony’s stylist dishes on the wedding
• Paul pitching another Big Three with “Melo in
NYC”
• Carmelo Anthony and La La Vazquez Get Married
at Star-Studded Wedding Ceremony
</listItem>
<sectionHeader confidence="0.995094" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960872194444444">
Motivation. News events are rarely reported
only in one way, from a single point of view. Dif-
ferent news agencies will interpret the event in dif-
ferent ways; various countries or locations may
highlight different aspects of it depending on how
they are affected; and opinions and in-depth anal-
yses will be written after the fact.
The variety of contents and styles is both an op-
portunity and a challenge. On the positive side, we
have the same events described in different ways;
this redundancy is useful for summarization, as
the information content reported by the majority
of news sources most likely represents the central
part of the event. On the other hand, variability
and subjectivity can be difficult to isolate. For
some applications it is important to understand,
given a collection of related news articles and re-
∗Work done during an internship at Google Zurich.
Table 1: Headlines observed for a news collection
reporting the same wedding event.
ports, how to formulate in an objective way what
has happened.
As a motivating example, Table 1 shows the dif-
ferent headlines observed in news reporting the
wedding between basketball player Carmelo An-
thony and actress LaLa Vazquez. As can be seen,
there is a wide variety of ways to report the same
event, including different points of view, high-
lighted aspects, and opinionated statements on the
part of the reporter. When presenting this event to
a user in a news-based information retrieval or rec-
ommendation system, different event descriptions
may be more appropriate. For example, a user may
only be interested in objective, informative sum-
maries without any interpretation on the part of
the reporter. In this case, Carmelo Anthony, ac-
</bodyText>
<page confidence="0.844628">
1243
</page>
<note confidence="0.9153365">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1243–1253,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.981735428571429">
tress LaLa Vazquez wed in NYC would be a good
choice.
Goal. Our final goal in this research is to build a
headline generation system that, given a news col-
lection, is able to describe it with the most com-
pact, objective and informative headline. In par-
ticular, we want the system to be able to:
</bodyText>
<listItem confidence="0.92339115">
• Generate headlines in an open-domain, unsu-
pervised way, so that it does not need to rely
on training data which is expensive to pro-
duce.
• Generalize across synonymous expressions
that refer to the same event.
• Do so in an abstractive fashion, to enforce
novelty, objectivity and generality.
In order to advance towards this goal, this paper
explores the following questions:
• What is a good way of using syntactic pat-
terns to represent events for generating head-
lines?
• Can we have satisfactory readability with an
open-domain abstractive approach, not rely-
ing on training data nor on manually pre-
defined generation templates?
• How far can we get in terms of informative-
ness, compared to the human-produced head-
lines, i.e., extractive approaches?
</listItem>
<bodyText confidence="0.999688">
Contributions. In this paper we present
HEADY, which is at the same time a novel system
for abstractive headline generation, and a smooth
clustering of patterns describing the same events.
HEADY is fully open-domain and can scale to
web-sized data. By learning to generalize events
across the boundaries of a single news story or
news collection, HEADY produces compact and
effective headlines that objectively convey the
relevant information.
When compared to a state-of-the-art open-
domain headline abstraction system (Filippova,
2010), the new headlines are statistically signifi-
cantly better both in terms of readability and in-
formativeness. Also, automatic evaluations using
ROUGE, having objective headlines for the news
as references, show that the abstractive headlines
are on par with human-produced headlines.
</bodyText>
<sectionHeader confidence="0.998915" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99765393877551">
Headline generation and summarization.
Most headline generation work in the past has
focused on the problem of single-document sum-
marization: given the main passage of a single
news article, generate a very short summary of
the article. From early in the field, it was pointed
out that a purely extractive approach is not good
enough to generate headlines from the body
text (Banko et al., 2000). Sometimes the most
important information is distributed across several
sentences in the document. More importantly,
quite often, the single sentence selected as the
most informative for the news collection is already
longer than the desired headline size. For this
reason, most early headline generation work fo-
cused on either extracting and reordering n-grams
from the document to be summarized (Banko et
al., 2000), or extracting one or two informative
sentences from the document and performing
linguistically-motivated transformations to them
in order to reduce the summary length (Dorr et
al., 2003). The first approach is not guaranteed
to produce grammatical headlines, whereas the
second approach is tightly tied to the actual
wording found in the document. Single-document
headline generation was also explored at the
Document Understanding Conferences between
2002 and 20041.
In later years, there has been more interest in
problems such as sentence compression (Galley
and McKeown, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Napoles et al., 2011;
Berg-Kirkpatrick et al., 2011), text simplification
(Zhu et al., 2010; Coster and Kauchak, 2011;
Woodsend and Lapata, 2011) and sentence fu-
sion (Barzilay and McKeown, 2005; Wan et al.,
2007; Filippova and Strube, 2008; Elsner and San-
thanam, 2011). All of them have direct applica-
tions for headline generation, as it can be con-
strued as selecting one or a few sentences from
the original document(s), and then reducing them
to the target title size. For example, Wan et al.
(2007) generate novel utterances by combining
Prim’s maximum-spanning-tree algorithm with an
n-gram language model to enforce fluency. Un-
like HEADY, the method by Wan and colleagues
is an extractive method that can summarize single
documents into a sentence, as opposed to generat-
ing a sentence that can stand for a whole collec-
</bodyText>
<footnote confidence="0.992484">
1http://duc.nist.gov/
</footnote>
<page confidence="0.995322">
1244
</page>
<bodyText confidence="0.999334079365079">
tion of news. Filippova (2010) reports a system
that is very close to our settings: the input is a
collection of related news articles, and the system
generates a headline that describes the main event.
This system uses sentence compression techniques
and benefits from the redundancy in the collection.
One difference with respect to HEADY is that it
does not use any syntactic information aside from
part-of-speech tags, and it does not require a train-
ing step. We have used this approach as a baseline
for comparison.
There are not many fully abstractive systems for
news summarization. The few that exist, such as
the work by Genest and Lapalme (2012), rely on
manually written generation templates. In con-
trast, HEADY automatically learns the templates
or headline patterns automatically, which allows it
to work in open-domain settings without relying
on supervision or manual annotations.
Open-domain pattern learning. Pattern learn-
ing for relation extraction is an active area of re-
search that is very related to our problem of event
pattern learning for headline generation. TextRun-
ner (Yates et al., 2007), ReVerb (Fader et al., 2011)
and NELL (Carlson et al., 2010; Mohamed et al.,
2011) are some examples of open-domain systems
that learn surface patterns that express relations
between pairs of entities. PATTY (Nakashole et
al., 2012) generalizes the patterns to also include
syntactic information and ontological (class mem-
bership) constraints. Our patterns are more similar
to the ones used by PATTY, which also produces
clusters of synonymous patterns. The main differ-
ences are that (a) HEADY is not limited to con-
sider patterns expressing relations between pairs
of entities; (b) we identify synonym patterns us-
ing a probabilistic, Bayesian approach that takes
advantage of the multiplicity of news sources re-
porting the same events. Chambers and Jurafsky
(2009) present an unsupervised method for learn-
ing narrative schemas from news, i.e., coherent
sets of events that involve specific entity types (se-
mantic roles). Similarly to them, we move from
the assumptions that 1) utterances involving the
same entity types within the same document (in
our case, a collection of related documents) are
likely describing aspects of the same event, and
2) meaningful representations of the underlying
events can be learned by clustering these utter-
ances in a principled way.
Noisy-OR networks. Noisy-OR Bayesian net-
works (Pearl, 1988) have been applied in the
past to a wide class of large-scale probabilis-
tic inference problems, from the medical do-
main (Middleton et al., 1991; Jaakkola and Jor-
dan, 1999; Onisko et al., 2001), to synthetic
image-decomposition and co-citation data analy-
sis (ˇSingliar and Hauskrecht, 2006). By assum-
ing independence between the causes of the hid-
den variables, noisy-OR models tend to be reli-
able (Friedman and Goldszmidt, 1996) as they re-
quire a relatively small number of parameters to
be estimated (linear with the size of the network).
</bodyText>
<sectionHeader confidence="0.997161" genericHeader="method">
3 Headline generation
</sectionHeader>
<bodyText confidence="0.999932142857143">
In this section, we describe the HEADY system for
news headline abstraction. Our approach takes as
input, for training, a corpus of news articles or-
ganized in news collections. Once the model is
trained, it can generate headlines for new collec-
tions. An outline of HEADY’s main components
follows (details of each component are provided
in Sections 3.1, 3.2 and 3.3):
Pattern extraction. Identify, in each of the news
collections, syntactic patterns connecting k enti-
ties, for k ≥ 1. These will be the candidate pat-
terns expressing events.
Training. Train a Noisy-OR Bayesian network
on the co-occurrence of syntactic patterns. Each
pattern extracted in the previous step is added as
an observed variable, and latent variables are used
to represent the hidden events that generate pat-
terns. An additional noise variable links to every
terminal node, allowing every terminal to be gen-
erated by language background (noise) instead of
by an actual event.
Inference. Generate a headline from an unseen
news collection. First, patterns are extracted using
the pattern extraction procedure mentioned above.
Given the patterns, the posterior probability of the
hidden event variables is estimated. Then, from
the activated hidden events, the likelihood of ev-
ery pattern can be estimated, even if they do not
appear in the collection. The single pattern with
the maximum probability is selected to generate a
new headline from it. Being the product of extra-
news collection generalization, the retrieved pat-
tern is more likely to be objective and informative
than patterns directly observed in the news collec-
tion.
</bodyText>
<page confidence="0.939965">
1245
</page>
<bodyText confidence="0.989085333333333">
Algorithm 1 COLLECTIONTOPATTERNSΨ(N):
N is a repository of news collections, Ψ is a set
of parameters controlling the extraction process.
</bodyText>
<figure confidence="0.585487444444444">
R ← {}
for all N E N do
PREPROCESSDATA(N)
E ← GETRELEVANTENTITIES(Nl)
for all Ei ← COMBINATIONSΨ(E) do
for all n E N do
P ← EXTRACTPATTERNSΨ(n, Ei)
R{N, Ei} ← R{N, Ei} U P
return R
</figure>
<subsectionHeader confidence="0.990727">
3.1 Pattern extraction
</subsectionHeader>
<bodyText confidence="0.995099421052632">
In this section we detail the process for obtain-
ing the event patterns that constitute the building
blocks of learning and inference.
Patterns are extracted from a large repository
N of news collections N1,..., NJArJ. Each news
collection N = {ni} is an unordered collec-
tion of related news, each of which can be seen
as an ordered sequence of sentences, i.e.: n =
[s0,... sJnJ].
Algorithm 1 presents a high-level view of the
pattern extraction process. The different steps are
described below:
PREPROCESSDATA: We start by preprocess-
ing all the news in the news collections with a
standard NLP pipeline: tokenization and sentence
boundary detection (Gillick, 2009), part-of-speech
tagging, dependency parsing (Nivre, 2006), co-
reference resolution (Haghighi and Klein, 2009)
and entity linking based on Wikipedia and Free-
base. Using the Freebase dataset, each entity is
annotated with all its Freebase types (class labels).
In the end, for each entity mentioned in the docu-
ment we have a unique identifier, a list with all its
mentions in the document and a list of class labels
from Freebase.
As a result of this process, we obtain for each
sentence in the corpus a representation as exem-
plified in Figure 1 (1). In this example, the men-
tions of three distinct entities have been identified,
i.e., e1, ... , e3. In the Freebase list of types (class
labels), e1 is a person and a celebrity, and e3 is a
state and a location.
GETRELEVANTENTITIES: For each news col-
lection N we collect the set E of the entities men-
tioned most often within the collection. Next, we
generate the set COMBINATIONSΨ(E) consisting
NNP CC NNP TO VB IN NNP
Portia and Helen to marry in California
</bodyText>
<equation confidence="0.650540666666667">
e1 e2 e3
person actress state
celebrity location
</equation>
<figureCaption confidence="0.860314">
Figure 1: Pattern extraction process from an anno-
</figureCaption>
<bodyText confidence="0.973264303030303">
tated dependency parse. (1): an MST is extracted
from the entity pair e1, e2 (2); nodes are heuristi-
cally added to the MST to enforce grammaticality
(3); entity types are recombined to generate the fi-
nal patterns (4).
of non-empty subsets of E, without repeated en-
tities. The number of entities to consider in each
collection, and the maximum size for the subsets
of entities to consider are meta-parameters embed-
ded in Ψ.2
EXTRACTPATTERNS: For each subset of rel-
evant entities Ei, event patterns are mined from
the articles in the news collection. The process
by which patterns are extracted from a news is
explained in Algorithm 2 and exemplified graphi-
cally in Figure 1 (2–4).
GETMENTIONNODES: Using the dependency
parse T for a sentence s, we first identify the set
of nodes Mi that mention the entities in Ei. If
T does not contain exactly one mention of each
target entity in Ei, then the sentence is ignored.
Otherwise, we obtain the minimum spanning tree
for the nodeset Pi, i.e., the shortest path in the de-
pendency tree connecting all the nodes in Mi (Fig-
ure 1, 2). Pi is the set of nodes around which the
patterns will be constructed.
APPLYHEURISTICS: With very high probabil-
ity, the MST Pi that we obtain does not constitute
a grammatical or useful extrapolation of the origi-
nal sentence s. For example, the MST for the en-
2As our objective is to generate very short titles (under
10 words), we only consider combinations of up to three ele-
ments of E.
</bodyText>
<figure confidence="0.994504393939394">
conj
cc
nsubj
aux prep pobj
root
1
NNP CC NNP TO VB
person and actress to marry
NNP CC NNP TO VB
celebrity and actress to marry
conj 2
conj
cc
nsubj
aux
3
NNP NNP
e1 e2
person actress
celebrity
NNP CC NNP TO VB
e1 and e2 to marry
person actress
celebrity
conj
cc
nsubj
aux
4
conj
cc
nsubj
aux
</figure>
<page confidence="0.963741">
1246
</page>
<bodyText confidence="0.98224425">
Algorithm 2 EXTRACTPATTERNSΨ(n, Ei): n is
the list of sentences in a news article. Sentences
are POS-tagged, dependency parsed and annotated
with respect to a set of entities E ⊇ Ei
</bodyText>
<equation confidence="0.988868777777778">
P ← ∅
for all s ∈ n[0 : 2) do
T ← DEPPARSE(s)
Mi ← GETMENTIONNODES(t, Ei)
if ∃e ∈ Ei, count(e, Mi) =6 1 then continue
Pi ← GETMINIMUMSPANNINGTREEΨ(Mi)
APPLYHEURISTICSΨ(Pi) or continue
P ← P ∪ COMBINEENTITYTYPESΨ(Pi)
return P
</equation>
<bodyText confidence="0.999937185185185">
tity pair he1, e2i in the example does not provide a
good description of the event as it is neither ade-
quate nor fluent. For this reason, we apply a set of
post-processing heuristic transformations that aim
at including a minimal set of meaningful nodes.
These include making sure that both the root of the
clause and its subject appear in the extracted pat-
tern, and that conjunctions between entities should
not be dropped (Figure 1, 3).
COMBINEENTITYTYPES: Finally, a distinct
pattern is generated from each possible combina-
tion of entity type assignments for the participat-
ing entities. (Figure 1, 4).
It is important to note that both at training and
test time, for pattern extraction we only consider
the title and the first sentence of the article body.
The reason is that we want to limit ourselves, in
each news collection, to the most relevant event
reported in the collection, which appears most of
the times in these two sentences. Unlike titles, first
sentences do not extensively use puns or rhetorics
as they tend to be grammatical and informative
rather than catchy.
The patterns mined from the same news collec-
tion and for the same set of entities are grouped
together, and constitute the building blocks of the
clustering algorithm which is described below.
</bodyText>
<subsectionHeader confidence="0.99538">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.996309666666667">
The extracted patterns are used to learn a Noisy-
OR (Pearl, 1988) model by estimating the prob-
ability that each (observed) pattern activates one
or many (hidden) events. Figure 2 represents the
two levels: the hidden event variables at the top,
and the observed pattern variables at the bottom.
An additional noise variable links to every termi-
Figure 2: Probabilistic model. The associations
between latent event variables and observed pat-
tern variables are modeled by noisy-OR gates.
Events are assumed to be marginally independent,
and patterns conditionally independent given the
events.
nal node, allowing all terminals to be generated by
language background (noise) instead of by an ac-
tual event. The associations between latent events
and observed patterns are modeled by noisy-OR
gates.
In this model, the conditional probability of a
hidden event ei given a configuration of observed
patterns p ∈ {0,1}|P |is calculated as:
</bodyText>
<equation confidence="0.9960445">
P(ei = 0  |p) = (1 − qi0) H (1 − qij)pj
j∈πj
⎛ ⎞
�
= exp ⎝−θi0 − θijpj ⎠ ,
j∈πi
</equation>
<bodyText confidence="0.995500388888889">
where πi is the set of active events (i.e., πi =
∪j{pj}  |pj = 1), and qij = P(ei = 1  |pj = 1)
is the estimated probability that the observed pat-
tern pi can, in isolation, activate the event e. The
term qi0 is the so-called “noise” term of the model,
and it accounts for the fact that an observed event
ei might be activated by some pattern that has
never been observed (Jaakkola and Jordan, 1999).
In Algorithm 1, at the end of the process we
group in R[N, Ei] all the patterns extracted from
the same news collection N and entity sub-set Ei.
These groups represent rough clusters of patterns,
that we can use to bootstrap the optimization of
the model parameters θij = − log(1 − qij). We
initiate the training process by randomly selecting
100,000 of these groups, and optimize the weights
of the model through 40 EM (Dempster et al.,
1977) iterations.
</bodyText>
<subsectionHeader confidence="0.953484">
3.3 Inference (generation of new headlines)
</subsectionHeader>
<bodyText confidence="0.999059">
Given an unseen news collection N, the inference
component of HEADY generates a single headline
that captures the main event reported by the news
in N. In order to do so, we first need to select a
</bodyText>
<equation confidence="0.953521">
p1 p2 p3 ... pm
e1 ... en noise
</equation>
<page confidence="0.904436">
1247
</page>
<bodyText confidence="0.999646434782609">
single event-pattern p* that is especially relevant
for N. Having selected p*, in order to generate a
headline it is sufficient to replace the entity place-
holders in p* with the surface forms observed in
N.
To identify p*, we start from the assumption that
the most descriptive event encoded by N must de-
scribe an important situation in which some subset
of the relevant entities E in N are involved.
The basic inference algorithm is a two-
step random walk in the Bayesian network.
Given a set of entities E and sentences n,
EXTRACTPATTERNSΨ(n, E) collects patterns in-
volving those entities. By normalizing the fre-
quency of the extracted patterns, we get a prob-
ability distribution over the observed variables in
the network. A two-step random walk traversing
to the latent event nodes and back to the pattern
nodes allows us to generalize across events. We
call this algorithm INFERENCE(n, E).
In order to decide which is the most relevant set
of events that should appear in the headline, we
use the following procedure:
</bodyText>
<listItem confidence="0.986896533333333">
1. Given the set of entities E mentioned in the
news collection, we consider each entity sub-
set Ei C_ E including up to three entities3.
For each Ei, we run INFERENCE(n, Ei),
which computes a distribution wi over pat-
terns involving the entities in Ei.
2. We invoke again INFERENCE, now using at
the same time all the patterns extracted for
every subset of Ei C_ E. This computes a
probability distribution w over all patterns in-
volving any admissible subset of the entities
mentioned in the collection.
3. Third, we select the entity-specific distribu-
tion that approximates better the overall dis-
tribution
</listItem>
<equation confidence="0.9695675">
w* = arg max
i
</equation>
<bodyText confidence="0.846698714285714">
We assume that the corresponding set of en-
tities Ei are the most central entities in the
collection and therefore any headline should
make sure to mention them all.
3As we noted before, we impose this limitation to keep the
generated headlines relatively short and to limit data sparsity
issues.
</bodyText>
<listItem confidence="0.99689925">
4. Finally, we select the pattern with the highest
weight in w* as the pattern that better cap-
tures the main event reported in the news col-
lection:
</listItem>
<equation confidence="0.943309">
p* = pj  |wj = arg max
j
</equation>
<bodyText confidence="0.999948043478261">
The headline is then produced from p*, replac-
ing placeholders with the entities in the document
from which the pattern was extracted.
While in many cases information about entity
types would be sufficient to decide about the or-
der of the entities in the generated sentences (e.g.,
“[person] married in [location]” for the entity
set {ea = “Mr. Brown”, eb = “Los Angeles”j),
in other cases class assignment can be ambigu-
ous (e.g., “[person] killed [person]” for {ea =
“Mr. A”, eb = “Mr. B”j). To handle these cases,
when extracting patterns for an entity set {ea, ebj,
we keep track of the alphabetical ordering of
the entities, e.g., from a news collection about
“Mr. B” killing “Mr. A” we would produce
patterns such as “[person:2] killed [person:1]” or
“[person:1] was killed by [person:2]” since ea =
“Mr. A” &lt; eb = “Mr. B”. At inference time,
when we query the model with such patterns we
can only activate events whose assignments are
compatible with the entities observed in the text,
making the replacement straightforward and un-
ambiguous.
</bodyText>
<sectionHeader confidence="0.992462" genericHeader="method">
4 Experiment settings
</sectionHeader>
<bodyText confidence="0.9999638">
In our method we use patterns that are fully lex-
icalized (with the exception of entity placehold-
ers) and enriched with syntactic data. Under these
circumstances, the Noisy-OR can effectively gen-
eralize and learn meaningful clusters only if pro-
vided with large amounts of data. To our best
knowledge, available data sets for headline gen-
eration are not large enough to support this kind
of inference.
For this reason, we rely on a corpus of news
crawled from the web between 2008 and 2012
which have been clustered based on closeness in
time and cosine similarity, using the vector-space
model and tf.idf weights. News collections with
less than 5 documents are discarded4, and those
</bodyText>
<footnote confidence="0.9603232">
4There is a very long tail of singleton articles, which do
not offer useful examples of lexical or syntactic variation, and
many very small collections that tend to be especially noisy,
hence the decision to consider only collections with at least 5
documents.
</footnote>
<equation confidence="0.895168">
cos(w, wi)
wj*
</equation>
<page confidence="0.956331">
1248
</page>
<bodyText confidence="0.999963076923077">
larger than 50 documents are capped, by randomly
picking 50 documents from the collection5. The
total number of news collections after clustering
is 1.7 million. From this set, we have set aside
a few hundred collections that will remain unseen
until the final evaluation.
As we have no development set, we have done
no tuning of the parameters for pattern extraction
nor for the Bayesian network training (100,000 la-
tent variables to represent the different events, 40
EM iterations, as mentioned in Section 3.2). The
EM iterations on the noisy-OR were distributed
across 30 machines with 16 GB of memory each.
</bodyText>
<subsectionHeader confidence="0.995768">
4.1 Systems used
</subsectionHeader>
<bodyText confidence="0.999915">
One of the questions we wanted to answer in
this research was whether it was possible to ob-
tain the same quality with automatically abstracted
headlines as with human-generated headlines. For
every news collection we have as many human-
generated headlines as documents. To decide
which human-generated headline should be used
in this comparison, we used three different meth-
ods that pick one of the collection headlines:
</bodyText>
<listItem confidence="0.859454">
• Latest headline: selects the headline from
</listItem>
<bodyText confidence="0.9926125">
the latest document in the collection. Intu-
itively this should be the most relevant one
for news about sport matches and competi-
tions, where the earlier headlines offer pre-
views and predictions, and the later headlines
report who won and the final scores.
</bodyText>
<listItem confidence="0.6254748">
• Most frequent headline: some headlines
are repeated across the collection, and this
method chooses the most frequent one. If
there are several with the same frequency,
one is taken at random6.
• TopicSum: we use TopicSum (Haghighi and
Vanderwende, 2009), a 3-layer hierarchical
topic model, to infer the language model that
is most central for the collection. The news
title that has the smallest Kullback-Leibler
</listItem>
<footnote confidence="0.7774211">
5Even though we did not run any experiment to find an
optimal value for this parameter, 50 documents seems like
a reasonable choice to avoid redundancy while allowing for
considerable lexical and syntactic variation.
6The most frequent headline only has a tie in 6 collections
in the whole test set. In 5 cases two headlines are tied at fre-
quencies around 4, and in one case three headlines are tied at
frequency 2. All six are large collections with 50 news arti-
cles, so this baseline is significantly different from a random
baseline.
</footnote>
<table confidence="0.999089714285714">
R-1 R-2 R-SU4
HEADY 0.3565 0.1903 0.1966
Most frequent pattern 0.3560 0.1864 0.1959
TopicSum 0.3594 0.1821 0.1935
MSC 0.3470 0.1765 0.1855
Most frequent headline 0.3177 0.1401 0.1668
Latest headline 0.2814 0.1191 0.1425
</table>
<tableCaption confidence="0.935014333333333">
Table 2: Results from the automatic evaluation,
sorted according to the ROUGE-2 and ROUGE-
SU4 scores.
</tableCaption>
<bodyText confidence="0.998660461538462">
divergence with respect the collection lan-
guage model is the one chosen.
A headline generation system that addresses
the same application as ours is (Filippova, 2010),
which generates a graph from the collection sen-
tences and selects the shortest path between the
begin and the end node traversing words in the
same order in which they were found in the orig-
inal documents. We have used this system, called
Multi-Sentence Compression (MSC), for compar-
isons.
Finally, in order to understand whether the
noisy-OR Bayesian network is useful for general-
izing across patterns into latent events, we added a
baseline that extracts all patterns from the test col-
lection following the same COLLECTIONTOPAT-
TERNS algorithm (including the application of the
linguistically motivated heuristics), and then pro-
duces a headline straightaway from the most fre-
quent pattern extracted. In other words, the only
difference with respect to HEADY is that in this
case no generalization through the Noisy-OR net-
work is carried out, and that headlines are gen-
erated from patterns directly observed in the test
news collections. We call this system Most fre-
quent pattern.
</bodyText>
<subsectionHeader confidence="0.998249">
4.2 Annotation activities
</subsectionHeader>
<bodyText confidence="0.999961272727273">
In order to evaluate HEADY’s performance, we
carried out two annotation activities.
First, from the set of collections that we had
set aside at the beginning, we randomly chose 50
collections for which all the systems could gen-
erate an output, and we asked raters to manually
write titles for them. As this is not a simple task
to be crowdsourced, for this evaluation we relied
on eight trained raters. We collected between four
and five reference titles for each of the fifty news
collections, to be used to compare the headline
</bodyText>
<page confidence="0.976903">
1249
</page>
<table confidence="0.999656285714286">
Readability Informativeness
TopicSum 4.86 4.63
Most freq. headline †$4.61 †$✸4.43
Latest headline †$4.55 † 4.00
HEADY † 4.28 † 3.75
Most freq. pattern † 3.95 † 3.82
MSC 3.00 3.05
</table>
<tableCaption confidence="0.999419">
Table 3: Results from the manual evaluation. At
</tableCaption>
<bodyText confidence="0.980092090909091">
95% confidence, TopicSum is significantly better
than all others for readability, and only indistin-
guishable from the most frequent pattern for in-
formativeness. For the rest, ✸ means being signifi-
cantly better than HEADY, ‡ than the most frequent
pattern, and † than MSC.
generation methods using automatic summariza-
tion metrics.
Then, we took the output of the systems for the
50 test collections and asked human raters to eval-
uate the headlines:
</bodyText>
<listItem confidence="0.780207285714286">
1. Raters were shown one headline and asked to
rate it in terms of readability on a 5-point
Likert scale. In the instructions, the raters
were provided with examples of ungrammat-
ical and grammatical titles to guide them in
this annotation.
2. After the previous rating is done, raters were
</listItem>
<bodyText confidence="0.958815736842105">
shown a selection of five documents from the
collection, and they were asked to judge the
informativeness of the previous headline for
the news in the collection, again on a 5-point
Likert scale.
This second annotation was carried out by inde-
pendent raters in a crowd-sourcing setting. The
raters did not have any involvement with the in-
ception of the model or the writing of the pa-
per. They did not know that the headlines they
were rating were generated according to differ-
ent methods. We measured inter-judge agreement
on the Likert-scale annotations using their Intra-
Class Correlation (ICC) (Cicchetti, 1994). The
ICC for readability is 0.76 (0.95 confidence in-
terval [0.71, 0.80]), and for informativeness it is
0.67 (0.95 confidence interval [0.60, 0.73]). This
means strong agreement for readability, and mod-
erate agreement for informativeness.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.993889018867925">
The COLLECTIONTOPATTERNS algorithm was
run on the training set, producing a 230 million
event patterns. Patterns that were obtained from
the same collection and involving the same entities
were grouped together, for a total of 1.7 million
pattern collections. The pattern groups are used to
bootstrap the Noisy-OR model training. Training
the HEADY model that we used for the evaluation
took around six hours on 30 cores.
Table 2 shows the results of the comparison
of the headline generation systems using ROUGE
(R-1, R-2 and R-SU4) (Lin, 2004) with the col-
lected references. According to Owczarzak et
al. (2012), ROUGE is still a competitive met-
ric that correlates well with human judgements
for ranking summarizers. The significance tests
for ROUGE are performed using bootstrap resam-
pling and a graphical significance test (Minka,
2002). The human annotators that created the
references for this evaluation were explicitly in-
structed to write objective titles, which is the kind
of headlines that the abstractive systems aim at
generating. It is common to see real headlines
that are catchy, joking, or with a double mean-
ing, and therefore they use a different vocabulary
than objective titles that simply mention what hap-
pened. TopicSum sometimes selects objective ti-
tles amongst the human-made titles and that is
why it also scores very well with the ROUGE
scores. But the other two criteria for choosing
human-made headlines select non-objective titles
much more often, and this lowers their perfor-
mance when measured with ROUGE with respect
to the objective references.
Table 3 lists the results of the manual evaluation
of readability and informativeness of the generated
headlines. The first result that we can see is the
difference in the rankings between the two evalu-
ations. Part of this difference might be due to the
fact that ROUGE is not as good for discriminating
between human-made and automatic summaries.
In fact, in the DUC competitions, the gap between
human summaries and automatic summaries was
also more apparent in the manual evaluations than
using ROUGE. Another part of the observed dif-
ference may be due to the design of the evalua-
tion. The manual evaluation is asking raters to
judge whether real, human-written titles that were
actually used for those news are grammatical and
informative. As could be expected, as these are
published titles, the real titles score very good on
the manual evaluation.
Some other interesting results are:
</bodyText>
<page confidence="0.923604">
1250
</page>
<table confidence="0.998077384615385">
Model Generated title
TopicSum Modern Family’s Eric Stonestreet laughs off
Charlize Theron rumours
MSC Modern Family star Eric Stonestreet is dating
Charlize Theron.
Latest headline Eric laughs off Theron dating rumours
Frequent pattern Eric Stonestreetjokes about Charlize relationship
Frequent headline Charlize Theron dating Modern Family star
HEADY Eric Stonestreet not dating Charlize Theron
TopicSum McFadzean rescues point for Crawley Town
MSC Crawley side challenging for a point against Old-
ham Athletic.
Latest headline Reds midfielder victim of racist tweet
Frequent pattern Kyle McFadzean fired a equaliser Crawley were
made
Frequent headline Latics halt Crawley charge
HEADY Kyle McFadzean rescues point for Crawley Town
F.C.
TopicSum UCI to strip Lance Armstrong of his 7 Tour titles
MSC The international cycling union said today.
Latest headline Letters: elderly drivers and Lance Armstrong
Frequent pattern Lance Armstrong stripped of Tour de France ti-
tles
Frequent headline Today in the news: third debate is tonight
HEADY Lance Armstrong was stripped of Tour de France
titles
</table>
<tableCaption confidence="0.993024">
Table 4: A comparison of the titles generated by
</tableCaption>
<bodyText confidence="0.980555891891892">
the different models for three news collections.
• Amongst the automatic systems, HEADY per-
formed better than MSC, with statistical sig-
nificance at 95% for all the metrics. Head-
lines based on the most frequent patterns
were better than MSC for all metrics but
ROUGE-2.
• The most frequent pattern baseline and
HEADY have comparable performance across
all the metrics (not statistically significantly
different), although HEADY has slightly bet-
ter scores for all metrics except for informa-
tiveness.
While we do not take any step to explicitly
model stylistic variation, estimating the weights
of the Noisy-OR network turns out to be a very
effective way of filtering out sensational wording
to the advantage of plainer, more objective style.
This may not clearly emerge from the evaluation,
as we did not explicitly ask the raters to annotate
the items based on their objectivity, but a manual
inspection of the clusters suggests that the gener-
alization is working in the right direction.
Table 4 presents a selection of outputs produced
by the six models for three different news collec-
tions. The first example shows a news collection
containing news about a rumour that was imme-
diately denied. In the second example, HEADY
generalization improves over the most frequent
pattern. In the third case, HEADY generates a
good title from a noisy collection (containing dif-
ferent but related events). The examples also
show that TopicSum is very effective in selecting
a good human-generated headline for each collec-
tion. This opens the possibility of using TopicSum
to automatically generate ROUGE references for
future evaluations of abstractive methods.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999987387096774">
We have presented HEADY, an abstractive head-
line generation system based on the generaliza-
tion of syntactic patterns by means of a Noisy-OR
Bayesian network. We evaluated the model both
automatically and through human annotations.
HEADY performs significantly better than a state-
of-the-art open domain abstractive model (Filip-
pova, 2010) in all evaluations, and is in par with
human-generated headlines in terms of ROUGE
scores. We have shown that it is possible to
achieve high quality generation of news headlines
in an open-domain, unsupervised setting by suc-
cessfully exploiting syntactic and ontological in-
formation. The system relies on a standard NLP
pipeline, requires no manual data annotation and
can effectively scale to web-sized corpora.
For feature work, we plan to improve all compo-
nents of HEADY in order to fill in the gap with the
human-generated titles in terms of readability and
informativeness. One of the directions in which
we plan to move is the removal of the syntac-
tic heuristics that currently enforce pattern well-
formedness and to automatically learn the neces-
sary transformations from the data.
Two other lines of work that we plan to explore
are the possibility of personalizing the headlines
to user interests (as stored in user profiles or ex-
pressed as user queries), and to investigate further
applications of the Bayesian network of event pat-
terns, such as its use for relation extraction and
knowledge base population.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999930444444444">
The research leading to these results has received
funding from: the EU’s 7th Framework Pro-
gramme (FP7/2007-2013) under grant agreement
number 257790; the Spanish Ministry of Science
and Innovation’s project Holopedia (TIN2010-
21128-C02); and the Regional Government of
Madrid’s MA2VICMR (S2009/TIC1542). We
would like to thank Katja Filippova and the anony-
mous reviewers for their insightful comments.
</bodyText>
<page confidence="0.98821">
1251
</page>
<sectionHeader confidence="0.983688" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897392857143">
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’00, pages 318–325. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
328.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481–490. Association for
Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), pages 3–3.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and Their
Participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume
2, pages 602–610.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal ofArtificialIntelli-
gence Research, 31(1):399–429.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637–674.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using Wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1–9. Association for Computational
Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B.
Rubi. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1–38.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1–8. Association for Computational Lin-
guistics.
Micha Elsner and Deepak Santhanam. 2011. Learn-
ing to fuse disparate sentences. In Proceedings of
the Workshop on Monolingual Text-To-Text Gener-
ation, pages 54–63. Association for Computational
Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 177–185. As-
sociation for Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 322–330. Association
for Computational Linguistics.
Nir Friedman and Moises Goldszmidt. 1996. Learning
Bayesian networks with local structure. In Proceed-
ings of the Twelfth Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-96),
pages 252–262, San Francisco, CA. Morgan Kauf-
mann.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 180–187.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, short papers.
Association for Computational Linguistics.
Dan Gillick. 2009. Sentence boundary detection and
the problem with the us. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 241–244. Association for
Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1152–1161. Asso-
ciation for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370. Association for
Computational Linguistics.
</reference>
<page confidence="0.833761">
1252
</page>
<reference confidence="0.999412759493671">
Tommi S. Jaakkola and Michael I. Jordan. 1999.
Variational probabilistic inference and the QMR-
DT Network. Journal of Artificial Intelligence Re-
search, 10:291–322.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Blackford Middleton, Michael Shwe, David Hecker-
man, Max Henrion, Eric Horvitz, Harold Lehmann,
and Gregory Cooper. 1991. Probabilistic diag-
nosis using a reformulation of the INTERNIST-
1/QMR knowledge base. I. The probabilistic model
and inference algorithms. Methods of information in
medicine, 30(4):241–255, October.
Tom Minka. 2002. Judging Significance from Error
Bars. CM U Tech R eport.
Thahir P Mohamed, Estevam R Hruschka Jr, and
Tom M Mitchell. 2011. Discovering relations be-
tween noun categories. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1447–1455. Association for Com-
putational Linguistics.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84–90. Association for Computa-
tional Linguistics.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technol-
ogy. Springer.
Agnieszka Onisko, Marek J. Druzdzel, and Hanna Wa-
syluk. 2001. Learning Bayesian network parame-
ters from small data sets: application of Noisy-OR
gates. International Journal of Approximated Rea-
soning, 27(2):165–182.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of the NAACL-HLT 2012 Work-
shop on Evaluation Metrics and System Comparison
for Automatic Summarization, pages 1–9. Associa-
tion for Computational Linguistics.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks ofplausible inference. Mor-
gan Kaufmann.
Tom´a&amp;quot;s &amp;quot;Singliar and Milo&amp;quot;s Hauskrecht. 2006. Noisy-or
component analysis and its application to link analy-
sis. J. Mach. Learn. Res., 7:2189–2213, December.
Stephen Wan, Robert Dale, Mark Dras, and C´ecile
Paris. 2007. Global Revision in Summarisation:
Generating Novel Sentences with Prim’s Algorithm.
In Proceedings of PACLING 2007 - 10th Conference
of the Pacific Association for Computational Lin-
guistics.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409–420. Association
for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
25–26. Association for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics, pages 1353–1361.
</reference>
<page confidence="0.974043">
1253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.004599">
<title confidence="0.997989">News headline abstraction through event pattern clustering</title>
<author confidence="0.962715">Alfonseca Daniele Pighin Guillermo</author>
<affiliation confidence="0.759898">Google Inc. Google Inc. NLP &amp; IR Group at</affiliation>
<email confidence="0.721177">ealfonseca@google.combiondo@google.comggarrido@lsi.uned.es</email>
<abstract confidence="0.99705352631579">paper presents a novel, abfor headline generation from news collections. From a web-scale corpus of English news, we mine syntactic patterns that a Noisy-OR model generalizes into event descriptions. At inference time, we query the model with the patterns observed in an unseen news collection, identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a headover a state-of-theart open-domain title abstraction method, bridging half of the gap that separates it from extractive methods using humangenerated titles in manual evaluations, and performs comparably to human-generated headlines as evaluated with ROUGE.</abstract>
<title confidence="0.642377">Carmelo and La La Party It Up with Kim and Ciara</title>
<author confidence="0.7214755">La La Vazquez</author>
<author confidence="0.7214755">Carmelo Anthony Wedding Day Bliss</author>
<abstract confidence="0.37191725">Carmelo Anthony, actress LaLa Vazquez wed in NYC • Stylist to the Stars • LaLa, Carmelo Set Off Celebrity Wedding Weekend • Ciara rocks a sexy Versace Spring 2010 mini to LaLa Vasquez and Carmelo Anthony’s wedding (photos) • Lala Vasquez on her wedding dress, cake, reality tv show and fianc´e, Carmelo Anthony (video) • VAZQUEZ MARRIES SPORTS STAR AN- THONY • Lebron Returns To NYC For Carmelo’s Wedding • Carmelo Anthony’s stylist dishes on the wedding • Paul pitching another Big Three with “Melo in NYC” • Carmelo Anthony and La La Vazquez Get Married</abstract>
<intro confidence="0.503745">at Star-Studded Wedding Ceremony</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu O Mittal</author>
<author>Michael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>318--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5852" citStr="Banko et al., 2000" startWordPosition="937" endWordPosition="940">f readability and informativeness. Also, automatic evaluations using ROUGE, having objective headlines for the news as references, show that the abstractive headlines are on par with human-produced headlines. 2 Related work Headline generation and summarization. Most headline generation work in the past has focused on the problem of single-document summarization: given the main passage of a single news article, generate a very short summary of the article. From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al., 2000). Sometimes the most important information is distributed across several sentences in the document. More importantly, quite often, the single sentence selected as the most informative for the news collection is already longer than the desired headline size. For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al.,</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu O. Mittal, and Michael J. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, ACL ’00, pages 318–325. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>328</pages>
<contexts>
<context position="7094" citStr="Barzilay and McKeown, 2005" startWordPosition="1123" endWordPosition="1126">t approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summarize single documents into a sentence, as opposed to generating a sentence tha</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>481--490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6953" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="1101" endWordPosition="1104"> the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the meth</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 481–490. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI</booktitle>
<pages>3--3</pages>
<contexts>
<context position="8926" citStr="Carlson et al., 2010" startWordPosition="1421" endWordPosition="1424">ve systems for news summarization. The few that exist, such as the work by Genest and Lapalme (2012), rely on manually written generation templates. In contrast, HEADY automatically learns the templates or headline patterns automatically, which allows it to work in open-domain settings without relying on supervision or manual annotations. Open-domain pattern learning. Pattern learning for relation extraction is an active area of research that is very related to our problem of event pattern learning for headline generation. TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011) and NELL (Carlson et al., 2010; Mohamed et al., 2011) are some examples of open-domain systems that learn surface patterns that express relations between pairs of entities. PATTY (Nakashole et al., 2012) generalizes the patterns to also include syntactic information and ontological (class membership) constraints. Our patterns are more similar to the ones used by PATTY, which also produces clusters of synonymous patterns. The main differences are that (a) HEADY is not limited to consider patterns expressing relations between pairs of entities; (b) we identify synonym patterns using a probabilistic, Bayesian approach that ta</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010), pages 3–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Schemas and Their Participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>602--610</pages>
<contexts>
<context position="9631" citStr="Chambers and Jurafsky (2009)" startWordPosition="1529" endWordPosition="1532">urface patterns that express relations between pairs of entities. PATTY (Nakashole et al., 2012) generalizes the patterns to also include syntactic information and ontological (class membership) constraints. Our patterns are more similar to the ones used by PATTY, which also produces clusters of synonymous patterns. The main differences are that (a) HEADY is not limited to consider patterns expressing relations between pairs of entities; (b) we identify synonym patterns using a probabilistic, Bayesian approach that takes advantage of the multiplicity of news sources reporting the same events. Chambers and Jurafsky (2009) present an unsupervised method for learning narrative schemas from news, i.e., coherent sets of events that involve specific entity types (semantic roles). Similarly to them, we move from the assumptions that 1) utterances involving the same entity types within the same document (in our case, a collection of related documents) are likely describing aspects of the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a </context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and Their Participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Domenic V Cicchetti</author>
</authors>
<title>Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology.</title>
<date>1994</date>
<journal>Psychological Assessment,</journal>
<volume>6</volume>
<issue>4</issue>
<contexts>
<context position="30131" citStr="Cicchetti, 1994" startWordPosition="5006" endWordPosition="5007">ere shown a selection of five documents from the collection, and they were asked to judge the informativeness of the previous headline for the news in the collection, again on a 5-point Likert scale. This second annotation was carried out by independent raters in a crowd-sourcing setting. The raters did not have any involvement with the inception of the model or the writing of the paper. They did not know that the headlines they were rating were generated according to different methods. We measured inter-judge agreement on the Likert-scale annotations using their IntraClass Correlation (ICC) (Cicchetti, 1994). The ICC for readability is 0.76 (0.95 confidence interval [0.71, 0.80]), and for informativeness it is 0.67 (0.95 confidence interval [0.60, 0.73]). This means strong agreement for readability, and moderate agreement for informativeness. 5 Results The COLLECTIONTOPATTERNS algorithm was run on the training set, producing a 230 million event patterns. Patterns that were obtained from the same collection and involving the same entities were grouped together, for a total of 1.7 million pattern collections. The pattern groups are used to bootstrap the Noisy-OR model training. Training the HEADY m</context>
</contexts>
<marker>Cicchetti, 1994</marker>
<rawString>Domenic V Cicchetti. 1994. Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology. Psychological Assessment, 6(4):284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal ofArtificialIntelligence Research,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="6876" citStr="Clarke and Lapata, 2008" startWordPosition="1089" endWordPosition="1092">nko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algor</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal ofArtificialIntelligence Research, 31(1):399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>34--637</pages>
<contexts>
<context position="6899" citStr="Cohn and Lapata, 2009" startWordPosition="1093" endWordPosition="1096">racting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram lan</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research, 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Learning to simplify sentences using Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7018" citStr="Coster and Kauchak, 2011" startWordPosition="1111" endWordPosition="1114">o them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summari</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011. Learning to simplify sentences using Wikipedia. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubi</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="19785" citStr="Dempster et al., 1977" startWordPosition="3268" endWordPosition="3271">led “noise” term of the model, and it accounts for the fact that an observed event ei might be activated by some pattern that has never been observed (Jaakkola and Jordan, 1999). In Algorithm 1, at the end of the process we group in R[N, Ei] all the patterns extracted from the same news collection N and entity sub-set Ei. These groups represent rough clusters of patterns, that we can use to bootstrap the optimization of the model parameters θij = − log(1 − qij). We initiate the training process by randomly selecting 100,000 of these groups, and optimize the weights of the model through 40 EM (Dempster et al., 1977) iterations. 3.3 Inference (generation of new headlines) Given an unseen news collection N, the inference component of HEADY generates a single headline that captures the main event reported by the news in N. In order to do so, we first need to select a p1 p2 p3 ... pm e1 ... en noise 1247 single event-pattern p* that is especially relevant for N. Having selected p*, in order to generate a headline it is sufficient to replace the entity placeholders in p* with the surface forms observed in N. To identify p*, we start from the assumption that the most descriptive event encoded by N must describ</context>
</contexts>
<marker>Dempster, Laird, Rubi, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubi. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLTNAACL 03 on Text summarization workshop-Volume 5,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6458" citStr="Dorr et al., 2003" startWordPosition="1027" endWordPosition="1030">t al., 2000). Sometimes the most important information is distributed across several sentences in the document. More importantly, quite often, the single sentence selected as the most informative for the news collection is already longer than the desired headline size. For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentenc</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLTNAACL 03 on Text summarization workshop-Volume 5, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Deepak Santhanam</author>
</authors>
<title>Learning to fuse disparate sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>54--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7169" citStr="Elsner and Santhanam, 2011" startWordPosition="1135" endWordPosition="1139"> second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summarize single documents into a sentence, as opposed to generating a sentence that can stand for a whole collec1http://duc.nist.gov/ 1244 tion of news. Fili</context>
</contexts>
<marker>Elsner, Santhanam, 2011</marker>
<rawString>Micha Elsner and Deepak Santhanam. 2011. Learning to fuse disparate sentences. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 54–63. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8895" citStr="Fader et al., 2011" startWordPosition="1415" endWordPosition="1418">e are not many fully abstractive systems for news summarization. The few that exist, such as the work by Genest and Lapalme (2012), rely on manually written generation templates. In contrast, HEADY automatically learns the templates or headline patterns automatically, which allows it to work in open-domain settings without relying on supervision or manual annotations. Open-domain pattern learning. Pattern learning for relation extraction is an active area of research that is very related to our problem of event pattern learning for headline generation. TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011) and NELL (Carlson et al., 2010; Mohamed et al., 2011) are some examples of open-domain systems that learn surface patterns that express relations between pairs of entities. PATTY (Nakashole et al., 2012) generalizes the patterns to also include syntactic information and ontological (class membership) constraints. Our patterns are more similar to the ones used by PATTY, which also produces clusters of synonymous patterns. The main differences are that (a) HEADY is not limited to consider patterns expressing relations between pairs of entities; (b) we identify synonym patterns using a probabili</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>177--185</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7140" citStr="Filippova and Strube, 2008" startWordPosition="1131" endWordPosition="1134">tical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summarize single documents into a sentence, as opposed to generating a sentence that can stand for a whole collec1http://duc.nist</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 177–185. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-sentence compression: Finding shortest paths in word graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>322--330</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5159" citStr="Filippova, 2010" startWordPosition="830" endWordPosition="831">iveness, compared to the human-produced headlines, i.e., extractive approaches? Contributions. In this paper we present HEADY, which is at the same time a novel system for abstractive headline generation, and a smooth clustering of patterns describing the same events. HEADY is fully open-domain and can scale to web-sized data. By learning to generalize events across the boundaries of a single news story or news collection, HEADY produces compact and effective headlines that objectively convey the relevant information. When compared to a state-of-the-art opendomain headline abstraction system (Filippova, 2010), the new headlines are statistically significantly better both in terms of readability and informativeness. Also, automatic evaluations using ROUGE, having objective headlines for the news as references, show that the abstractive headlines are on par with human-produced headlines. 2 Related work Headline generation and summarization. Most headline generation work in the past has focused on the problem of single-document summarization: given the main passage of a single news article, generate a very short summary of the article. From early in the field, it was pointed out that a purely extract</context>
<context position="7781" citStr="Filippova (2010)" startWordPosition="1238" endWordPosition="1239">011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summarize single documents into a sentence, as opposed to generating a sentence that can stand for a whole collec1http://duc.nist.gov/ 1244 tion of news. Filippova (2010) reports a system that is very close to our settings: the input is a collection of related news articles, and the system generates a headline that describes the main event. This system uses sentence compression techniques and benefits from the redundancy in the collection. One difference with respect to HEADY is that it does not use any syntactic information aside from part-of-speech tags, and it does not require a training step. We have used this approach as a baseline for comparison. There are not many fully abstractive systems for news summarization. The few that exist, such as the work by </context>
<context position="27010" citStr="Filippova, 2010" startWordPosition="4490" endWordPosition="4491">ency 2. All six are large collections with 50 news articles, so this baseline is significantly different from a random baseline. R-1 R-2 R-SU4 HEADY 0.3565 0.1903 0.1966 Most frequent pattern 0.3560 0.1864 0.1959 TopicSum 0.3594 0.1821 0.1935 MSC 0.3470 0.1765 0.1855 Most frequent headline 0.3177 0.1401 0.1668 Latest headline 0.2814 0.1191 0.1425 Table 2: Results from the automatic evaluation, sorted according to the ROUGE-2 and ROUGESU4 scores. divergence with respect the collection language model is the one chosen. A headline generation system that addresses the same application as ours is (Filippova, 2010), which generates a graph from the collection sentences and selects the shortest path between the begin and the end node traversing words in the same order in which they were found in the original documents. We have used this system, called Multi-Sentence Compression (MSC), for comparisons. Finally, in order to understand whether the noisy-OR Bayesian network is useful for generalizing across patterns into latent events, we added a baseline that extracts all patterns from the test collection following the same COLLECTIONTOPATTERNS algorithm (including the application of the linguistically moti</context>
<context position="36000" citStr="Filippova, 2010" startWordPosition="5926" endWordPosition="5928">events). The examples also show that TopicSum is very effective in selecting a good human-generated headline for each collection. This opens the possibility of using TopicSum to automatically generate ROUGE references for future evaluations of abstractive methods. 6 Conclusions We have presented HEADY, an abstractive headline generation system based on the generalization of syntactic patterns by means of a Noisy-OR Bayesian network. We evaluated the model both automatically and through human annotations. HEADY performs significantly better than a stateof-the-art open domain abstractive model (Filippova, 2010) in all evaluations, and is in par with human-generated headlines in terms of ROUGE scores. We have shown that it is possible to achieve high quality generation of news headlines in an open-domain, unsupervised setting by successfully exploiting syntactic and ontological information. The system relies on a standard NLP pipeline, requires no manual data annotation and can effectively scale to web-sized corpora. For feature work, we plan to improve all components of HEADY in order to fill in the gap with the human-generated titles in terms of readability and informativeness. One of the direction</context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-sentence compression: Finding shortest paths in word graphs. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 322–330. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nir Friedman</author>
<author>Moises Goldszmidt</author>
</authors>
<title>Learning Bayesian networks with local structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the Twelfth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-96),</booktitle>
<pages>252--262</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="10622" citStr="Friedman and Goldszmidt, 1996" startWordPosition="1686" endWordPosition="1689">f the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a wide class of large-scale probabilistic inference problems, from the medical domain (Middleton et al., 1991; Jaakkola and Jordan, 1999; Onisko et al., 2001), to synthetic image-decomposition and co-citation data analysis (ˇSingliar and Hauskrecht, 2006). By assuming independence between the causes of the hidden variables, noisy-OR models tend to be reliable (Friedman and Goldszmidt, 1996) as they require a relatively small number of parameters to be estimated (linear with the size of the network). 3 Headline generation In this section, we describe the HEADY system for news headline abstraction. Our approach takes as input, for training, a corpus of news articles organized in news collections. Once the model is trained, it can generate headlines for new collections. An outline of HEADY’s main components follows (details of each component are provided in Sections 3.1, 3.2 and 3.3): Pattern extraction. Identify, in each of the news collections, syntactic patterns connecting k ent</context>
</contexts>
<marker>Friedman, Goldszmidt, 1996</marker>
<rawString>Nir Friedman and Moises Goldszmidt. 1996. Learning Bayesian networks with local structure. In Proceedings of the Twelfth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-96), pages 252–262, San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>180--187</pages>
<contexts>
<context position="6851" citStr="Galley and McKeown, 2007" startWordPosition="1085" endWordPosition="1088">ument to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s ma</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 180–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
</authors>
<title>Fully abstractive approach to guided summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, short papers. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8406" citStr="Genest and Lapalme (2012)" startWordPosition="1341" endWordPosition="1344"> reports a system that is very close to our settings: the input is a collection of related news articles, and the system generates a headline that describes the main event. This system uses sentence compression techniques and benefits from the redundancy in the collection. One difference with respect to HEADY is that it does not use any syntactic information aside from part-of-speech tags, and it does not require a training step. We have used this approach as a baseline for comparison. There are not many fully abstractive systems for news summarization. The few that exist, such as the work by Genest and Lapalme (2012), rely on manually written generation templates. In contrast, HEADY automatically learns the templates or headline patterns automatically, which allows it to work in open-domain settings without relying on supervision or manual annotations. Open-domain pattern learning. Pattern learning for relation extraction is an active area of research that is very related to our problem of event pattern learning for headline generation. TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011) and NELL (Carlson et al., 2010; Mohamed et al., 2011) are some examples of open-domain systems that learn surf</context>
</contexts>
<marker>Genest, Lapalme, 2012</marker>
<rawString>Pierre-Etienne Genest and Guy Lapalme. 2012. Fully abstractive approach to guided summarization. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, short papers. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
</authors>
<title>Sentence boundary detection and the problem with the us.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>241--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13373" citStr="Gillick, 2009" startWordPosition="2137" endWordPosition="2138">g the event patterns that constitute the building blocks of learning and inference. Patterns are extracted from a large repository N of news collections N1,..., NJArJ. Each news collection N = {ni} is an unordered collection of related news, each of which can be seen as an ordered sequence of sentences, i.e.: n = [s0,... sJnJ]. Algorithm 1 presents a high-level view of the pattern extraction process. The different steps are described below: PREPROCESSDATA: We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. Using the Freebase dataset, each entity is annotated with all its Freebase types (class labels). In the end, for each entity mentioned in the document we have a unique identifier, a list with all its mentions in the document and a list of class labels from Freebase. As a result of this process, we obtain for each sentence in the corpus a representation as exemplified in Figure 1 (1). In this example, the mentions of three distinct ent</context>
</contexts>
<marker>Gillick, 2009</marker>
<rawString>Dan Gillick. 2009. Sentence boundary detection and the problem with the us. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 241–244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13482" citStr="Haghighi and Klein, 2009" startWordPosition="2148" endWordPosition="2151">xtracted from a large repository N of news collections N1,..., NJArJ. Each news collection N = {ni} is an unordered collection of related news, each of which can be seen as an ordered sequence of sentences, i.e.: n = [s0,... sJnJ]. Algorithm 1 presents a high-level view of the pattern extraction process. The different steps are described below: PREPROCESSDATA: We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. Using the Freebase dataset, each entity is annotated with all its Freebase types (class labels). In the end, for each entity mentioned in the document we have a unique identifier, a list with all its mentions in the document and a list of class labels from Freebase. As a result of this process, we obtain for each sentence in the corpus a representation as exemplified in Figure 1 (1). In this example, the mentions of three distinct entities have been identified, i.e., e1, ... , e3. In the Freebase list of types (class labels), e1 is a person </context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1152–1161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25825" citStr="Haghighi and Vanderwende, 2009" startWordPosition="4296" endWordPosition="4299">we used three different methods that pick one of the collection headlines: • Latest headline: selects the headline from the latest document in the collection. Intuitively this should be the most relevant one for news about sport matches and competitions, where the earlier headlines offer previews and predictions, and the later headlines report who won and the final scores. • Most frequent headline: some headlines are repeated across the collection, and this method chooses the most frequent one. If there are several with the same frequency, one is taken at random6. • TopicSum: we use TopicSum (Haghighi and Vanderwende, 2009), a 3-layer hierarchical topic model, to infer the language model that is most central for the collection. The news title that has the smallest Kullback-Leibler 5Even though we did not run any experiment to find an optimal value for this parameter, 50 documents seems like a reasonable choice to avoid redundancy while allowing for considerable lexical and syntactic variation. 6The most frequent headline only has a tie in 6 collections in the whole test set. In 5 cases two headlines are tied at frequencies around 4, and in one case three headlines are tied at frequency 2. All six are large colle</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi S Jaakkola</author>
<author>Michael I Jordan</author>
</authors>
<title>Variational probabilistic inference and the QMRDT Network.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>10--291</pages>
<contexts>
<context position="10365" citStr="Jaakkola and Jordan, 1999" startWordPosition="1646" endWordPosition="1650">involve specific entity types (semantic roles). Similarly to them, we move from the assumptions that 1) utterances involving the same entity types within the same document (in our case, a collection of related documents) are likely describing aspects of the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a wide class of large-scale probabilistic inference problems, from the medical domain (Middleton et al., 1991; Jaakkola and Jordan, 1999; Onisko et al., 2001), to synthetic image-decomposition and co-citation data analysis (ˇSingliar and Hauskrecht, 2006). By assuming independence between the causes of the hidden variables, noisy-OR models tend to be reliable (Friedman and Goldszmidt, 1996) as they require a relatively small number of parameters to be estimated (linear with the size of the network). 3 Headline generation In this section, we describe the HEADY system for news headline abstraction. Our approach takes as input, for training, a corpus of news articles organized in news collections. Once the model is trained, it ca</context>
<context position="19340" citStr="Jaakkola and Jordan, 1999" startWordPosition="3189" endWordPosition="3192">sy-OR gates. In this model, the conditional probability of a hidden event ei given a configuration of observed patterns p ∈ {0,1}|P |is calculated as: P(ei = 0 |p) = (1 − qi0) H (1 − qij)pj j∈πj ⎛ ⎞ � = exp ⎝−θi0 − θijpj ⎠ , j∈πi where πi is the set of active events (i.e., πi = ∪j{pj} |pj = 1), and qij = P(ei = 1 |pj = 1) is the estimated probability that the observed pattern pi can, in isolation, activate the event e. The term qi0 is the so-called “noise” term of the model, and it accounts for the fact that an observed event ei might be activated by some pattern that has never been observed (Jaakkola and Jordan, 1999). In Algorithm 1, at the end of the process we group in R[N, Ei] all the patterns extracted from the same news collection N and entity sub-set Ei. These groups represent rough clusters of patterns, that we can use to bootstrap the optimization of the model parameters θij = − log(1 − qij). We initiate the training process by randomly selecting 100,000 of these groups, and optimize the weights of the model through 40 EM (Dempster et al., 1977) iterations. 3.3 Inference (generation of new headlines) Given an unseen news collection N, the inference component of HEADY generates a single headline th</context>
</contexts>
<marker>Jaakkola, Jordan, 1999</marker>
<rawString>Tommi S. Jaakkola and Michael I. Jordan. 1999. Variational probabilistic inference and the QMRDT Network. Journal of Artificial Intelligence Research, 10:291–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="30926" citStr="Lin, 2004" startWordPosition="5131" endWordPosition="5132">adability, and moderate agreement for informativeness. 5 Results The COLLECTIONTOPATTERNS algorithm was run on the training set, producing a 230 million event patterns. Patterns that were obtained from the same collection and involving the same entities were grouped together, for a total of 1.7 million pattern collections. The pattern groups are used to bootstrap the Noisy-OR model training. Training the HEADY model that we used for the evaluation took around six hours on 30 cores. Table 2 shows the results of the comparison of the headline generation systems using ROUGE (R-1, R-2 and R-SU4) (Lin, 2004) with the collected references. According to Owczarzak et al. (2012), ROUGE is still a competitive metric that correlates well with human judgements for ranking summarizers. The significance tests for ROUGE are performed using bootstrap resampling and a graphical significance test (Minka, 2002). The human annotators that created the references for this evaluation were explicitly instructed to write objective titles, which is the kind of headlines that the abstractive systems aim at generating. It is common to see real headlines that are catchy, joking, or with a double meaning, and therefore t</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blackford Middleton</author>
<author>Michael Shwe</author>
<author>David Heckerman</author>
<author>Max Henrion</author>
<author>Eric Horvitz</author>
<author>Harold Lehmann</author>
<author>Gregory Cooper</author>
</authors>
<title>Probabilistic diagnosis using a reformulation of the INTERNIST1/QMR knowledge base. I. The probabilistic model and inference algorithms. Methods of information in medicine,</title>
<date>1991</date>
<pages>30--4</pages>
<contexts>
<context position="10338" citStr="Middleton et al., 1991" startWordPosition="1642" endWordPosition="1645">ent sets of events that involve specific entity types (semantic roles). Similarly to them, we move from the assumptions that 1) utterances involving the same entity types within the same document (in our case, a collection of related documents) are likely describing aspects of the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a wide class of large-scale probabilistic inference problems, from the medical domain (Middleton et al., 1991; Jaakkola and Jordan, 1999; Onisko et al., 2001), to synthetic image-decomposition and co-citation data analysis (ˇSingliar and Hauskrecht, 2006). By assuming independence between the causes of the hidden variables, noisy-OR models tend to be reliable (Friedman and Goldszmidt, 1996) as they require a relatively small number of parameters to be estimated (linear with the size of the network). 3 Headline generation In this section, we describe the HEADY system for news headline abstraction. Our approach takes as input, for training, a corpus of news articles organized in news collections. Once </context>
</contexts>
<marker>Middleton, Shwe, Heckerman, Henrion, Horvitz, Lehmann, Cooper, 1991</marker>
<rawString>Blackford Middleton, Michael Shwe, David Heckerman, Max Henrion, Eric Horvitz, Harold Lehmann, and Gregory Cooper. 1991. Probabilistic diagnosis using a reformulation of the INTERNIST1/QMR knowledge base. I. The probabilistic model and inference algorithms. Methods of information in medicine, 30(4):241–255, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Minka</author>
</authors>
<title>Judging Significance from Error Bars.</title>
<date>2002</date>
<tech>CM U Tech R eport.</tech>
<contexts>
<context position="31221" citStr="Minka, 2002" startWordPosition="5176" endWordPosition="5177">.7 million pattern collections. The pattern groups are used to bootstrap the Noisy-OR model training. Training the HEADY model that we used for the evaluation took around six hours on 30 cores. Table 2 shows the results of the comparison of the headline generation systems using ROUGE (R-1, R-2 and R-SU4) (Lin, 2004) with the collected references. According to Owczarzak et al. (2012), ROUGE is still a competitive metric that correlates well with human judgements for ranking summarizers. The significance tests for ROUGE are performed using bootstrap resampling and a graphical significance test (Minka, 2002). The human annotators that created the references for this evaluation were explicitly instructed to write objective titles, which is the kind of headlines that the abstractive systems aim at generating. It is common to see real headlines that are catchy, joking, or with a double meaning, and therefore they use a different vocabulary than objective titles that simply mention what happened. TopicSum sometimes selects objective titles amongst the human-made titles and that is why it also scores very well with the ROUGE scores. But the other two criteria for choosing human-made headlines select n</context>
</contexts>
<marker>Minka, 2002</marker>
<rawString>Tom Minka. 2002. Judging Significance from Error Bars. CM U Tech R eport.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thahir P Mohamed</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Discovering relations between noun categories.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1447--1455</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8949" citStr="Mohamed et al., 2011" startWordPosition="1425" endWordPosition="1428">mmarization. The few that exist, such as the work by Genest and Lapalme (2012), rely on manually written generation templates. In contrast, HEADY automatically learns the templates or headline patterns automatically, which allows it to work in open-domain settings without relying on supervision or manual annotations. Open-domain pattern learning. Pattern learning for relation extraction is an active area of research that is very related to our problem of event pattern learning for headline generation. TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011) and NELL (Carlson et al., 2010; Mohamed et al., 2011) are some examples of open-domain systems that learn surface patterns that express relations between pairs of entities. PATTY (Nakashole et al., 2012) generalizes the patterns to also include syntactic information and ontological (class membership) constraints. Our patterns are more similar to the ones used by PATTY, which also produces clusters of synonymous patterns. The main differences are that (a) HEADY is not limited to consider patterns expressing relations between pairs of entities; (b) we identify synonym patterns using a probabilistic, Bayesian approach that takes advantage of the mu</context>
</contexts>
<marker>Mohamed, Jr, Mitchell, 2011</marker>
<rawString>Thahir P Mohamed, Estevam R Hruschka Jr, and Tom M Mitchell. 2011. Discovering relations between noun categories. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1447–1455. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<pages>12</pages>
<contexts>
<context position="9099" citStr="Nakashole et al., 2012" startWordPosition="1447" endWordPosition="1450">tomatically learns the templates or headline patterns automatically, which allows it to work in open-domain settings without relying on supervision or manual annotations. Open-domain pattern learning. Pattern learning for relation extraction is an active area of research that is very related to our problem of event pattern learning for headline generation. TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011) and NELL (Carlson et al., 2010; Mohamed et al., 2011) are some examples of open-domain systems that learn surface patterns that express relations between pairs of entities. PATTY (Nakashole et al., 2012) generalizes the patterns to also include syntactic information and ontological (class membership) constraints. Our patterns are more similar to the ones used by PATTY, which also produces clusters of synonymous patterns. The main differences are that (a) HEADY is not limited to consider patterns expressing relations between pairs of entities; (b) we identify synonym patterns using a probabilistic, Bayesian approach that takes advantage of the multiplicity of news sources reporting the same events. Chambers and Jurafsky (2009) present an unsupervised method for learning narrative schemas from </context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns with semantic types. EMNLP12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Chris Callison-Burch</author>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Paraphrastic sentence compression with a character-based metric: Tightening without deletion.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>84--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Napoles, Callison-Burch, Ganitkevitch, Van Durme, 2011</marker>
<rawString>Courtney Napoles, Chris Callison-Burch, Juri Ganitkevitch, and Benjamin Van Durme. 2011. Paraphrastic sentence compression with a character-based metric: Tightening without deletion. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 84–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing,</title>
<date>2006</date>
<journal>Text, Speech and Language</journal>
<volume>34</volume>
<publisher>Technology. Springer.</publisher>
<contexts>
<context position="13431" citStr="Nivre, 2006" startWordPosition="2143" endWordPosition="2144">learning and inference. Patterns are extracted from a large repository N of news collections N1,..., NJArJ. Each news collection N = {ni} is an unordered collection of related news, each of which can be seen as an ordered sequence of sentences, i.e.: n = [s0,... sJnJ]. Algorithm 1 presents a high-level view of the pattern extraction process. The different steps are described below: PREPROCESSDATA: We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. Using the Freebase dataset, each entity is annotated with all its Freebase types (class labels). In the end, for each entity mentioned in the document we have a unique identifier, a list with all its mentions in the document and a list of class labels from Freebase. As a result of this process, we obtain for each sentence in the corpus a representation as exemplified in Figure 1 (1). In this example, the mentions of three distinct entities have been identified, i.e., e1, ... , e3. In the Fre</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing, volume 34 of Text, Speech and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnieszka Onisko</author>
<author>Marek J Druzdzel</author>
<author>Hanna Wasyluk</author>
</authors>
<title>Learning Bayesian network parameters from small data sets: application of Noisy-OR gates.</title>
<date>2001</date>
<journal>International Journal of Approximated Reasoning,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="10387" citStr="Onisko et al., 2001" startWordPosition="1651" endWordPosition="1654">es (semantic roles). Similarly to them, we move from the assumptions that 1) utterances involving the same entity types within the same document (in our case, a collection of related documents) are likely describing aspects of the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a wide class of large-scale probabilistic inference problems, from the medical domain (Middleton et al., 1991; Jaakkola and Jordan, 1999; Onisko et al., 2001), to synthetic image-decomposition and co-citation data analysis (ˇSingliar and Hauskrecht, 2006). By assuming independence between the causes of the hidden variables, noisy-OR models tend to be reliable (Friedman and Goldszmidt, 1996) as they require a relatively small number of parameters to be estimated (linear with the size of the network). 3 Headline generation In this section, we describe the HEADY system for news headline abstraction. Our approach takes as input, for training, a corpus of news articles organized in news collections. Once the model is trained, it can generate headlines f</context>
</contexts>
<marker>Onisko, Druzdzel, Wasyluk, 2001</marker>
<rawString>Agnieszka Onisko, Marek J. Druzdzel, and Hanna Wasyluk. 2001. Learning Bayesian network parameters from small data sets: application of Noisy-OR gates. International Journal of Approximated Reasoning, 27(2):165–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
<author>Ani Nenkova</author>
</authors>
<title>An assessment of the accuracy of automatic evaluation in summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT 2012 Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30994" citStr="Owczarzak et al. (2012)" startWordPosition="5140" endWordPosition="5143">5 Results The COLLECTIONTOPATTERNS algorithm was run on the training set, producing a 230 million event patterns. Patterns that were obtained from the same collection and involving the same entities were grouped together, for a total of 1.7 million pattern collections. The pattern groups are used to bootstrap the Noisy-OR model training. Training the HEADY model that we used for the evaluation took around six hours on 30 cores. Table 2 shows the results of the comparison of the headline generation systems using ROUGE (R-1, R-2 and R-SU4) (Lin, 2004) with the collected references. According to Owczarzak et al. (2012), ROUGE is still a competitive metric that correlates well with human judgements for ranking summarizers. The significance tests for ROUGE are performed using bootstrap resampling and a graphical significance test (Minka, 2002). The human annotators that created the references for this evaluation were explicitly instructed to write objective titles, which is the kind of headlines that the abstractive systems aim at generating. It is common to see real headlines that are catchy, joking, or with a double meaning, and therefore they use a different vocabulary than objective titles that simply men</context>
</contexts>
<marker>Owczarzak, Conroy, Dang, Nenkova, 2012</marker>
<rawString>Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In Proceedings of the NAACL-HLT 2012 Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic reasoning in intelligent systems: networks ofplausible inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10195" citStr="Pearl, 1988" startWordPosition="1619" endWordPosition="1620">ng the same events. Chambers and Jurafsky (2009) present an unsupervised method for learning narrative schemas from news, i.e., coherent sets of events that involve specific entity types (semantic roles). Similarly to them, we move from the assumptions that 1) utterances involving the same entity types within the same document (in our case, a collection of related documents) are likely describing aspects of the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a wide class of large-scale probabilistic inference problems, from the medical domain (Middleton et al., 1991; Jaakkola and Jordan, 1999; Onisko et al., 2001), to synthetic image-decomposition and co-citation data analysis (ˇSingliar and Hauskrecht, 2006). By assuming independence between the causes of the hidden variables, noisy-OR models tend to be reliable (Friedman and Goldszmidt, 1996) as they require a relatively small number of parameters to be estimated (linear with the size of the network). 3 Headline generation In this section, we describe the HEADY </context>
<context position="17998" citStr="Pearl, 1988" startWordPosition="2955" endWordPosition="2956">f the article body. The reason is that we want to limit ourselves, in each news collection, to the most relevant event reported in the collection, which appears most of the times in these two sentences. Unlike titles, first sentences do not extensively use puns or rhetorics as they tend to be grammatical and informative rather than catchy. The patterns mined from the same news collection and for the same set of entities are grouped together, and constitute the building blocks of the clustering algorithm which is described below. 3.2 Training The extracted patterns are used to learn a NoisyOR (Pearl, 1988) model by estimating the probability that each (observed) pattern activates one or many (hidden) events. Figure 2 represents the two levels: the hidden event variables at the top, and the observed pattern variables at the bottom. An additional noise variable links to every termiFigure 2: Probabilistic model. The associations between latent event variables and observed pattern variables are modeled by noisy-OR gates. Events are assumed to be marginally independent, and patterns conditionally independent given the events. nal node, allowing all terminals to be generated by language background (n</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic reasoning in intelligent systems: networks ofplausible inference. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Singliar</author>
<author>Milos Hauskrecht</author>
</authors>
<title>Noisy-or component analysis and its application to link analysis.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>7--2189</pages>
<contexts>
<context position="10484" citStr="Singliar and Hauskrecht, 2006" startWordPosition="1663" endWordPosition="1666"> involving the same entity types within the same document (in our case, a collection of related documents) are likely describing aspects of the same event, and 2) meaningful representations of the underlying events can be learned by clustering these utterances in a principled way. Noisy-OR networks. Noisy-OR Bayesian networks (Pearl, 1988) have been applied in the past to a wide class of large-scale probabilistic inference problems, from the medical domain (Middleton et al., 1991; Jaakkola and Jordan, 1999; Onisko et al., 2001), to synthetic image-decomposition and co-citation data analysis (ˇSingliar and Hauskrecht, 2006). By assuming independence between the causes of the hidden variables, noisy-OR models tend to be reliable (Friedman and Goldszmidt, 1996) as they require a relatively small number of parameters to be estimated (linear with the size of the network). 3 Headline generation In this section, we describe the HEADY system for news headline abstraction. Our approach takes as input, for training, a corpus of news articles organized in news collections. Once the model is trained, it can generate headlines for new collections. An outline of HEADY’s main components follows (details of each component are </context>
</contexts>
<marker>Singliar, Hauskrecht, 2006</marker>
<rawString>Tom´a&amp;quot;s &amp;quot;Singliar and Milo&amp;quot;s Hauskrecht. 2006. Noisy-or component analysis and its application to link analysis. J. Mach. Learn. Res., 7:2189–2213, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Robert Dale</author>
<author>Mark Dras</author>
<author>C´ecile Paris</author>
</authors>
<title>Global Revision in Summarisation: Generating Novel Sentences with Prim’s Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of PACLING 2007 - 10th Conference of the Pacific Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7112" citStr="Wan et al., 2007" startWordPosition="1127" endWordPosition="1130"> to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summarize single documents into a sentence, as opposed to generating a sentence that can stand for a </context>
</contexts>
<marker>Wan, Dale, Dras, Paris, 2007</marker>
<rawString>Stephen Wan, Robert Dale, Mark Dras, and C´ecile Paris. 2007. Global Revision in Summarisation: Generating Novel Sentences with Prim’s Algorithm. In Proceedings of PACLING 2007 - 10th Conference of the Pacific Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>409--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7046" citStr="Woodsend and Lapata, 2011" startWordPosition="1115" endWordPosition="1118">the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extractive method that can summarize single documents into a s</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 409–420. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Michael Cafarella</author>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
<author>Matthew Broadhead</author>
<author>Stephen Soderland</author>
</authors>
<title>TextRunner: Open information extraction on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,</booktitle>
<pages>25--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8866" citStr="Yates et al., 2007" startWordPosition="1410" endWordPosition="1413">baseline for comparison. There are not many fully abstractive systems for news summarization. The few that exist, such as the work by Genest and Lapalme (2012), rely on manually written generation templates. In contrast, HEADY automatically learns the templates or headline patterns automatically, which allows it to work in open-domain settings without relying on supervision or manual annotations. Open-domain pattern learning. Pattern learning for relation extraction is an active area of research that is very related to our problem of event pattern learning for headline generation. TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011) and NELL (Carlson et al., 2010; Mohamed et al., 2011) are some examples of open-domain systems that learn surface patterns that express relations between pairs of entities. PATTY (Nakashole et al., 2012) generalizes the patterns to also include syntactic information and ontological (class membership) constraints. Our patterns are more similar to the ones used by PATTY, which also produces clusters of synonymous patterns. The main differences are that (a) HEADY is not limited to consider patterns expressing relations between pairs of entities; (b) we identify synon</context>
</contexts>
<marker>Yates, Cafarella, Banko, Etzioni, Broadhead, Soderland, 2007</marker>
<rawString>Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. TextRunner: Open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 25–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of The 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1353--1361</pages>
<contexts>
<context position="6992" citStr="Zhu et al., 2010" startWordPosition="1107" endWordPosition="1110"> transformations to them in order to reduce the summary length (Dorr et al., 2003). The first approach is not guaranteed to produce grammatical headlines, whereas the second approach is tightly tied to the actual wording found in the document. Single-document headline generation was also explored at the Document Understanding Conferences between 2002 and 20041. In later years, there has been more interest in problems such as sentence compression (Galley and McKeown, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Napoles et al., 2011; Berg-Kirkpatrick et al., 2011), text simplification (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011) and sentence fusion (Barzilay and McKeown, 2005; Wan et al., 2007; Filippova and Strube, 2008; Elsner and Santhanam, 2011). All of them have direct applications for headline generation, as it can be construed as selecting one or a few sentences from the original document(s), and then reducing them to the target title size. For example, Wan et al. (2007) generate novel utterances by combining Prim’s maximum-spanning-tree algorithm with an n-gram language model to enforce fluency. Unlike HEADY, the method by Wan and colleagues is an extracti</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of The 23rd International Conference on Computational Linguistics, pages 1353–1361.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>