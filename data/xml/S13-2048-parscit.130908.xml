<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023029">
<title confidence="0.948511">
UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis
</title>
<author confidence="0.980702">
Torsten Zesch† Omer Levy§ Iryna Gurevych† Ido Dagan§
</author>
<affiliation confidence="0.894921666666667">
† Ubiquitous Knowledge Processing Lab § Natural Language Processing Lab
Computer Science Department Computer Science Department
Technische Universit¨at Darmstadt Bar-Ilan University
</affiliation>
<sectionHeader confidence="0.971986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999491875">
Our system combines text similarity measures
with a textual entailment system. In the main
task, we focused on the influence of lexical-
ized versus unlexicalized features, and how
they affect performance on unseen questions
and domains. We also participated in the pi-
lot partial entailment task, where our system
significantly outperforms a strong baseline.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870326923077">
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (Dzikovska
et al., 2013) brings together two important dimen-
sions of Natural Language Processing: real-world
applications and semantic inference technologies.
The challenge focuses on the domain of middle-
school quizzes, and attempts to emulate the metic-
ulous marking process that teachers do on a daily
basis. Given a question, a reference answer, and a
student’s answer, the task is to determine whether
the student answered correctly. While this is not
a new task in itself, the challenge focuses on em-
ploying textual entailment technologies as the back-
bone of this educational application. As a conse-
quence, we formalize the question “Did the student
answer correctly?” as “Can the reference answer be
inferred from the student’s answer?”. This question
can (hopefully) be answered by a textual entailment
system (Dagan et al., 2009).
The challenge contains two tasks: In the main
task, the system must analyze each answer as a
whole. There are three settings, where each one de-
fines “correct” in a different resolution. The highest-
resolution setting defines five different classes or
“correctness values”: correct, partially correct, con-
tradictory, irrelevant, non-domain. In the pilot task,
critical elements of the answer need to be analyzed
separately. Each such element is called a facet, and
is defined as a pair of words that are critical in an-
swering the question. As there is a substantial dif-
ference between the two tasks, we designed sibling
architectures for each task, and divide the main part
of the paper accordingly.
Our goal is to provide a robust architecture for stu-
dent response analysis, that can generalize and per-
form well in multiple domains. Moreover, we are
interested in evaluating how well general-purpose
technologies will perform in this setting. We there-
fore approach the challenge by combining two such
technologies: DKPro Similarity –an extensive suite
of text similarity measures– that has been success-
fully applied in other settings like the SemEval 2012
task on semantic textual similarity (B¨ar et al., 2012a)
or reuse detection (B¨ar et al., 2012b).
BIUTEE, the Bar-Ilan University Textual Entail-
ment Engine (Stern and Dagan, 2011), which has
shown state-of-the-art performance on recognizing
textual entailment challenges. Our systems use both
technologies to extract features, and combine them
in a supervised model. Indeed, this approach works
relatively well (with respect to other entries in the
challenge), especially in unseen domains.
</bodyText>
<sectionHeader confidence="0.984365" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.989031">
2.1 Text Similarity
</subsectionHeader>
<bodyText confidence="0.994473">
Text similarity is a bidirectional, continuous func-
tion which operates on pairs of texts of any length
and returns a numeric score of how similar one text
is to the other. In previous work (Mihalcea et al.,
</bodyText>
<page confidence="0.965785">
285
</page>
<bodyText confidence="0.990923411764706">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 285–289, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
2006; Gabrilovich and Markovitch, 2007; Landauer
et al., 1998), only a single text similarity measure
has typically been applied to text pairs. However,
as recent work (B¨ar et al., 2012a; B¨ar et al., 2012b)
has shown, text similarity computation can be much
improved when a variety of measures are combined.
In recent years, UKP lab at TU Darmstadt has de-
veloped DKPro Similarity1, an open source toolkit
for analyzing text similarity. It is part of the
DKPro framework for natural language processing
(Gurevych et al., 2007). DKPro Similarity excels
at the tasks of measuring semantic textual simi-
larity (STS) and detecting text reuse (DTR), hav-
ing achieved the best performance in previous chal-
lenges (B¨ar et al., 2012a; B¨ar et al., 2012b).
</bodyText>
<subsectionHeader confidence="0.99659">
2.2 Textual Entailment
</subsectionHeader>
<bodyText confidence="0.99997715">
The textual entailment paradigm is a generic frame-
work for applied semantic inference (Dagan et al.,
2009). The most prevalent task of textual entailment
is to recognize whether the meaning of a target nat-
ural language statement (H for hypothesis) can be
inferred from another piece of text (T for text). Ap-
parently, this core task underlies semantic inference
in many text applications. The task of analyzing stu-
dent responses is one such example. By assigning
the student’s answer as T and the reference answer
as H, we are basically asking whether one can in-
fer the correct (reference) answer from the student’s
response. In recent years, Bar-Ilan University has
developed BIUTEE (Stern and Dagan, 2011), an ex-
tensive textual entailment recognition engine. BI-
UTEE tries to convert T (represented as a depen-
dency tree) to H. It does so by applying a series of
knowledge-based transformations, such as synonym
substitution, active-passive conversion, and more.
BIUTEE is publicly available as open source.2
</bodyText>
<sectionHeader confidence="0.976697" genericHeader="method">
3 Main Task
</sectionHeader>
<bodyText confidence="0.978415555555556">
In this section, we explain how we approached the
main task, in which the system needs to analyze each
answer as a whole. After describing our system’s ar-
chitecture, we explain how we selected training data
for the different scenarios in the main task. We then
1code.google.com/p/dkpro-similarity-asl
2cs.biu.ac.il/˜nlp/downloads/biutee
provide the details for each submitted run, and fi-
nally, our empirical results.
</bodyText>
<subsectionHeader confidence="0.998531">
3.1 System Description
</subsectionHeader>
<bodyText confidence="0.999652888888889">
We build a system based on the Apache UIMA
framework (Ferrucci and Lally, 2004) and DKPro
Lab (Eckart de Castilho and Gurevych, 2011). We
use DKPro Core3 for preprocessing. Specifically,
we used the default DKPro segmenter, TreeTagger
POS tagger and chunker, Jazzy Spell Checker, and
the Stanford parser.4 We trained a supervised model
(Naive Bayes) using Weka (Hall et al., 2009) with
feature extraction based on clearTK (Ogren et al.,
2008). The following features have been used:
BOW features Bag-of-word features are based on
the assumption that certain words need to appear in
a correct answer. We used a mixture of token uni-
grams, bigrams, and trigrams, where each n-gram is
a binary feature that can either be true or false for a
document.5 Additionally, we also used the number
of tokens in the student answer as another feature in
this group.
Syntactic Features We extend BOW features
with syntactic functions by adding dependency and
phrase n-grams. Dependency n-grams are combina-
tions of two tokens and their dependency relation.
Phrase n-grams are combinations of the main verb
and the noun phrase left and right of the verb. In
both cases, we use the 10 most frequent n-grams.
Basic Similarity Features This group of features
computes the similarity between the reference an-
swer and the student answer. In case there is more
than one reference answer, we compute all pairwise
similarity scores and add the minimum, maximum,
average, and median similarity.6
Semantic Similarity Features are very similar to
the basic similarity features, except that we use se-
mantic similarity measures in order to bridge a pos-
sible vocabulary gap between the student and refer-
ence answer. We use the ESA measure (Gabrilovich
</bodyText>
<footnote confidence="0.894073333333333">
3code.google.com/p/dkpro-core-asl/
4DKPro Core v1.4.0, TreeTagger models v20130204.0,
Stanford parser PCFG model v20120709.0.
5Using the 750 most frequent n-grams gave good results on
the training set, so we also used this number for the test runs.
6As basic similarity measures, we use greedy string tiling
(Wise, 1996) with n = 3, longest common subsequence and
longest common substring (Allison and Dix, 1986), and word
n-gram containment(Lyon et al., 2001) with n = 2.
</footnote>
<page confidence="0.996656">
286
</page>
<bodyText confidence="0.999727307692307">
and Markovitch, 2007) based on concept vectors
build from WordNet, Wiktionary, and Wikipedia.
Spelling Features As spelling errors might be in-
dicative of the answer quality, we use the number of
spelling errors normalized by the text length as an
additional feature.
Entailment Features We run BIUTEE (Stern and
Dagan, 2011) on the test instance (as T) with each
reference answer (as H), which results in an array
of numerical entailment confidence values. If there
is more than one reference answer, we compute all
pairwise confidence scores and add the minimum,
maximum, average, and median confidence.
</bodyText>
<subsectionHeader confidence="0.99665">
3.2 Data Selection Regime
</subsectionHeader>
<bodyText confidence="0.999139428571428">
There are three scenarios under which our system
is expected to perform. For each one, we chose (a-
priori) a different set of examples for training.
Unseen Answers Classify new answers to famil-
iar questions. Train on instances that have the same
question as the test instance.
Unseen Questions Classify new answers to un-
seen (but related) questions. Partition the questions
according to their IDs, creating sets of related ques-
tions, and then train on all the instances that share
the same partition as the test instance.
Unseen Domains Classify new answers to unseen
questions from unseen domains. Use all available
training data from the same dataset.
</bodyText>
<subsectionHeader confidence="0.995857">
3.3 Submitted Runs
</subsectionHeader>
<bodyText confidence="0.995865529411765">
The runs represent the different levels of lexicaliza-
tion of the model which we expect to have strong
influence in each scenario:
Run 1 uses all features as described above. We
expect the BOW features to be helpful for the Un-
seen Answers setting, but to be misleading for un-
seen questions or domains, as the same word indi-
cating a correct answer for one question might cor-
respond to a wrong answer for another question.
Run 2 uses only non-lexicalized features, i.e. all
features except the BOW and syntactic features, in
order to assess the impact of the lexicalization that
overfits on the topic of the questions. We expect this
run to be less sensitive to the topic changes in the
Unseen Questions and Unseen Domains settings.
Run 3 uses only the basic similarity and the en-
tailment features. It should indicate the baseline per-
</bodyText>
<table confidence="0.985350454545455">
Unseen Unseen Unseen
Task Run Answers Questions Domains
1 .734 .678 .671
2-way 2 .665 .644 .677
3 .662 .625 .677
1 .670 .573 .572
3-way 2 .595 .561 .577
3 .574 .540 .576
1 .590 .376 .407
5-way 2 .495 .397 .371
3 .461 .394 .376
</table>
<tableCaption confidence="0.9939495">
Table 1: Main task performance for the SciEntsBank test
set. We show weighted average Fl for the three scenarios.
</tableCaption>
<table confidence="0.996838833333333">
Cor. Par Con. Irr. Non.
Correct 903 463 164 309 78
Partially Correct 219 261 93 333 80
Contradictory 61 126 91 103 36
Irrelevant 209 229 119 476 189
Non-Domain 0 0 0 2 18
</table>
<tableCaption confidence="0.84654">
Table 2: Confusion matrix of Run 1 in the 5-way Unseen
Domains scenario. The vertical axis is the real class, the
horizontal axis is the predicted class.
</tableCaption>
<bodyText confidence="0.9962435">
formance that can be expected without targeting the
system towards a certain topic.
</bodyText>
<subsectionHeader confidence="0.974048">
3.4 Empirical Results
</subsectionHeader>
<bodyText confidence="0.99995585">
Table 1 shows the Fl-measure (weighted average)
of the three runs. As was expected for the Unseen
Answers scenario, Run 1 using a lexicalized model
outperformed the other two runs. However, in the
other scenarios Run 1 is not significantly better, as
lexicalized features do not have the same impact if
the question or the domain changes.
Table 2 shows the confusion matrix of Run 1 in
the 5-way Unseen Domains scenario. The Correct
category was classified quite reliably, but the Irrele-
vant category was especially hard. While the refer-
ence answer provides some clues for what is correct
or incorrect, the range of things that are “irrelevant”
for a given question is potentially very big and thus
cannot be easily learned. We also see that the system
ability to distinguish Correct and Partially Correct
answers need to be improved.
It is difficult to provide an exact assessment of our
system’s performance (with respect to other systems
in the challenge), since there are multiple tasks, sce-
</bodyText>
<page confidence="0.991403">
287
</page>
<bodyText confidence="0.999376">
narios, datasets, and even metrics. However, we can
safely say that our system performed above average
in most settings, and showed competitive results in
the Unseen Domains scenario.
</bodyText>
<sectionHeader confidence="0.978277" genericHeader="method">
4 Pilot Task
</sectionHeader>
<bodyText confidence="0.999735333333333">
In the pilot task each facet needs to be analyzed sep-
arately, which requires some changes in the system
architecture.
</bodyText>
<subsectionHeader confidence="0.992175">
4.1 System Description
</subsectionHeader>
<bodyText confidence="0.999981866666667">
We segmented and lemmatized the input data using
the default DKPro segmenter and the TreeTagger
lemmatizer. The partial entailment system is com-
posed of three methods: Exact, WordNet, and BI-
UTEE. These were combined in different combina-
tions to form the different runs.
Exact In this baseline method, we represent a
student answer as a bag-of-words containing all to-
kens and lemmas appearing in the text. Lemmas
are used to account for minor morphological dif-
ferences, such as tense or plurality. We then check
whether both facet words appear in the set.
WordNet checks whether both facet words, or
their semantically related words, appear in the stu-
dent’s answer. We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term as matched if the similarity
score exceeds a certain threshold (0.9, empirically
determined on the training set).
BIUTEE processes dependency trees instead of
bags of words. We therefore added a pre-processing
stage that extracts a path in the dependency parse
that represents the facet. This is done by first pars-
ing the entire reference answer, and then locating the
two nodes mentioned in the facet. We then find their
lowest common ancestor (LCA), and extract the path
from the facet’s first word to the second through the
LCA. BIUTEE can now be given the student an-
swer and the pre-processed facet, and try to recog-
nize whether the former entails the latter.
</bodyText>
<subsectionHeader confidence="0.995054">
4.2 Submitted Runs
</subsectionHeader>
<bodyText confidence="0.9996058">
In preliminary experiments using the provided train-
ing data, we found that the very simple Exact Match
baseline performed surprisingly well, with 0.96 pre-
cision and 0.32 recall on positive class instances (ex-
pressed facets). We therefore decided to use this fea-
</bodyText>
<table confidence="0.9673175">
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
Run 1 .756 .710 .760
Run 2 .782 .765 .816
Run 3 .744 .733 .770
</table>
<tableCaption confidence="0.998301">
Table 3: Pilot task performance across different scenar-
ios. The scores are Fl-measures (weighted average).
</tableCaption>
<bodyText confidence="0.592915">
ture as an initial filter, and employ the others for
classifying the “harder” cases. Training BIUTEE
only on these cases, while dismissing easy ones, im-
proved our system’s performance significantly.
Run 1: Exact OR WordNet This is essentially
just the WordNet feature on its own, because every
instance that Exact classifies as positive is also pos-
itive by WordNet.
Run 2: Exact OR (BIUTEE AND WordNet) If
the instance is non-trivial, this configuration requires
that both BIUTEE and WordNet Match agree on pos-
itive classification. Equivalent to the majority rule.
Run 3: Exact OR BIUTEE BIUTEE increases
recall of expressed facets at the expense of precision.
</bodyText>
<subsectionHeader confidence="0.999576">
4.3 Empirical Results
</subsectionHeader>
<bodyText confidence="0.999978909090909">
Table 3 shows the F1-measure (weighted average) of
each run in each scenario, including Exact Match as
a quite strong baseline. In the majority of cases, Run
2 that combines entailment and WordNet-based lex-
ical matching, significantly outperformed the other
two. It is interesting to note that the systems’ perfor-
mance does not degrade in “harder” scenarios; this is
a result of the non-lexicalized nature of our methods.
Unfortunately, our system was the only submission
in this track, so we do not have any means to perform
relative comparison.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999813555555556">
We combined semantic textual similarity with tex-
tual entailment to solve the problem of student re-
sponse analysis. Though our features were not tai-
lored for this task, they proved quite indicative, and
adapted well to unseen domains. We believe that ad-
ditional generic features and knowledge resources
are the best way to improve on our results, while
retaining the same robustness and generality as our
current architecture.
</bodyText>
<page confidence="0.996293">
288
</page>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999480285714286">
This work has been supported by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant No.
I/82806, and by the European Community’s Seventh Frame-
work Programme (FP7/2007-2013) under grant agreement no.
287923 (EXCITEMENT). We would like to thank the Minerva
Foundation for facilitating this cooperation with a short term
research grant.
</bodyText>
<sectionHeader confidence="0.998454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979762967032967">
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305–310.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012a. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation and the 1st Joint Confer-
ence on Lexical and Computational Semantics, pages
435–440, June.
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2012b.
Text reuse detection using a composition of text sim-
ilarity measures. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING 2012), pages 167–184, December.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rationale,
evaluation and approaches. Natural Language Engi-
neering, 15(4):i–xvii.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A lightweight framework for reproducible parame-
ter sweeping in information retrieval. In Proceed-
ings of the 2011 workshop on Data infrastructurEs for
supporting information retrieval evaluation (DESIRE
’11), New York, NY, USA. ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3-4):327–348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence (IJCAI2007), pages 1606–1611.
Iryna Gurevych, Max M¨uhlh¨auser, Christof M¨uller,
J¨urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
based on UIMA. In Proceedings of the 1st Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technology,
T¨ubingen, Germany, April.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2&amp;3):259–284.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
6th Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 118–125,
Pittsburgh, PA USA.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775–780, Boston, MA.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA Toolkit for Statistical Nat-
ural Language Processing. In Towards Enhanced
Interoperability for Large HLT Systems: UIMA for
NLP workshop at Language Resources and Evaluation
Conference (LREC).
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence (IJCAI 1995), pages 448–453.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Processing
(RANLP 2011), pages 455–462.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE Technical Sympo-
sium on Computer Science Education (SIGCSE 1996),
pages 130–134, Philadelphia, PA.
</reference>
<page confidence="0.99864">
289
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616331">
<title confidence="0.995064">UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis</title>
<author confidence="0.622551">Knowledge Processing Lab Language Processing Lab</author>
<affiliation confidence="0.999376">Computer Science Department Computer Science Department Technische Universit¨at Darmstadt Bar-Ilan University</affiliation>
<abstract confidence="0.998989">Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm. Information Processing Letters,</title>
<date>1986</date>
<pages>23--305</pages>
<contexts>
<context position="8116" citStr="Allison and Dix, 1986" startWordPosition="1260" endWordPosition="1263">ilar to the basic similarity features, except that we use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3code.google.com/p/dkpro-core-asl/ 4DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 286 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-common-subsequence algorithm. Information Processing Letters, 23:305–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation and the 1st Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012a. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the 6th International Workshop on Semantic Evaluation and the 1st Joint Conference on Lexical and Computational Semantics, pages 435–440, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Text reuse detection using a composition of text similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>167--184</pages>
<marker>B¨ar, Zesch, Gurevych, 2012</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2012b. Text reuse detection using a composition of text similarity measures. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 167–184, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rationale, evaluation and approaches.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="1610" citStr="Dagan et al., 2009" startWordPosition="234" endWordPosition="237">s, and attempts to emulate the meticulous marking process that teachers do on a daily basis. Given a question, a reference answer, and a student’s answer, the task is to determine whether the student answered correctly. While this is not a new task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application. As a consequence, we formalize the question “Did the student answer correctly?” as “Can the reference answer be inferred from the student’s answer?”. This question can (hopefully) be answered by a textual entailment system (Dagan et al., 2009). The challenge contains two tasks: In the main task, the system must analyze each answer as a whole. There are three settings, where each one defines “correct” in a different resolution. The highestresolution setting defines five different classes or “correctness values”: correct, partially correct, contradictory, irrelevant, non-domain. In the pilot task, critical elements of the answer need to be analyzed separately. Each such element is called a facet, and is defined as a pair of words that are critical in answering the question. As there is a substantial difference between the two tasks, </context>
<context position="4629" citStr="Dagan et al., 2009" startWordPosition="706" endWordPosition="709">ch improved when a variety of measures are combined. In recent years, UKP lab at TU Darmstadt has developed DKPro Similarity1, an open source toolkit for analyzing text similarity. It is part of the DKPro framework for natural language processing (Gurevych et al., 2007). DKPro Similarity excels at the tasks of measuring semantic textual similarity (STS) and detecting text reuse (DTR), having achieved the best performance in previous challenges (B¨ar et al., 2012a; B¨ar et al., 2012b). 2.2 Textual Entailment The textual entailment paradigm is a generic framework for applied semantic inference (Dagan et al., 2009). The most prevalent task of textual entailment is to recognize whether the meaning of a target natural language statement (H for hypothesis) can be inferred from another piece of text (T for text). Apparently, this core task underlies semantic inference in many text applications. The task of analyzing student responses is one such example. By assigning the student’s answer as T and the reference answer as H, we are basically asking whether one can infer the correct (reference) answer from the student’s response. In recent years, Bar-Ilan University has developed BIUTEE (Stern and Dagan, 2011)</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rationale, evaluation and approaches. Natural Language Engineering, 15(4):i–xvii.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="798" citStr="Dzikovska et al., 2013" startWordPosition="107" endWordPosition="110">anguage Processing Lab Computer Science Department Computer Science Department Technische Universit¨at Darmstadt Bar-Ilan University Abstract Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline. 1 Introduction The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013) brings together two important dimensions of Natural Language Processing: real-world applications and semantic inference technologies. The challenge focuses on the domain of middleschool quizzes, and attempts to emulate the meticulous marking process that teachers do on a daily basis. Given a question, a reference answer, and a student’s answer, the task is to determine whether the student answered correctly. While this is not a new task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application. As a consequence, we formalize </context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Clark, Dagan, Dang, 2013</marker>
<rawString>Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
</authors>
<title>A lightweight framework for reproducible parameter sweeping in information retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 workshop on Data infrastructurEs for supporting information retrieval evaluation (DESIRE ’11),</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>de Castilho, Gurevych, 2011</marker>
<rawString>Richard Eckart de Castilho and Iryna Gurevych. 2011. A lightweight framework for reproducible parameter sweeping in information retrieval. In Proceedings of the 2011 workshop on Data infrastructurEs for supporting information retrieval evaluation (DESIRE ’11), New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<pages>10--3</pages>
<contexts>
<context position="6062" citStr="Ferrucci and Lally, 2004" startWordPosition="932" endWordPosition="935">stitution, active-passive conversion, and more. BIUTEE is publicly available as open source.2 3 Main Task In this section, we explain how we approached the main task, in which the system needs to analyze each answer as a whole. After describing our system’s architecture, we explain how we selected training data for the different scenarios in the main task. We then 1code.google.com/p/dkpro-similarity-asl 2cs.biu.ac.il/˜nlp/downloads/biutee provide the details for each submitted run, and finally, our empirical results. 3.1 System Description We build a system based on the Apache UIMA framework (Ferrucci and Lally, 2004) and DKPro Lab (Eckart de Castilho and Gurevych, 2011). We use DKPro Core3 for preprocessing. Specifically, we used the default DKPro segmenter, TreeTagger POS tagger and chunker, Jazzy Spell Checker, and the Stanford parser.4 We trained a supervised model (Naive Bayes) using Weka (Hall et al., 2009) with feature extraction based on clearTK (Ogren et al., 2008). The following features have been used: BOW features Bag-of-word features are based on the assumption that certain words need to appear in a correct answer. We used a mixture of token unigrams, bigrams, and trigrams, where each n-gram i</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An architectural approach to unstructured information processing in the corporate research environment. Natural Language Engineering, 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI2007),</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="3791" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="569" endWordPosition="572">ks relatively well (with respect to other entries in the challenge), especially in unseen domains. 2 Background 2.1 Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 285–289, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2006; Gabrilovich and Markovitch, 2007; Landauer et al., 1998), only a single text similarity measure has typically been applied to text pairs. However, as recent work (B¨ar et al., 2012a; B¨ar et al., 2012b) has shown, text similarity computation can be much improved when a variety of measures are combined. In recent years, UKP lab at TU Darmstadt has developed DKPro Similarity1, an open source toolkit for analyzing text similarity. It is part of the DKPro framework for natural language processing (Gurevych et al., 2007). DKPro Similarity excels at the tasks of measuring semantic textual similarity (STS) and detecting text reuse </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI2007), pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
<author>Christof M¨uller</author>
<author>J¨urgen Steimle</author>
<author>Markus Weimer</author>
<author>Torsten Zesch</author>
</authors>
<title>Darmstadt Knowledge Processing Repository based on UIMA.</title>
<date>2007</date>
<booktitle>In Proceedings of the 1st Workshop on Unstructured Information Management Architecture at Biannual Conference of the Society for Computational Linguistics and Language Technology,</booktitle>
<location>T¨ubingen, Germany,</location>
<marker>Gurevych, M¨uhlh¨auser, M¨uller, Steimle, Weimer, Zesch, 2007</marker>
<rawString>Iryna Gurevych, Max M¨uhlh¨auser, Christof M¨uller, J¨urgen Steimle, Markus Weimer, and Torsten Zesch. 2007. Darmstadt Knowledge Processing Repository based on UIMA. In Proceedings of the 1st Workshop on Unstructured Information Management Architecture at Biannual Conference of the Society for Computational Linguistics and Language Technology, T¨ubingen, Germany, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="6363" citStr="Hall et al., 2009" startWordPosition="979" endWordPosition="982">data for the different scenarios in the main task. We then 1code.google.com/p/dkpro-similarity-asl 2cs.biu.ac.il/˜nlp/downloads/biutee provide the details for each submitted run, and finally, our empirical results. 3.1 System Description We build a system based on the Apache UIMA framework (Ferrucci and Lally, 2004) and DKPro Lab (Eckart de Castilho and Gurevych, 2011). We use DKPro Core3 for preprocessing. Specifically, we used the default DKPro segmenter, TreeTagger POS tagger and chunker, Jazzy Spell Checker, and the Stanford parser.4 We trained a supervised model (Naive Bayes) using Weka (Hall et al., 2009) with feature extraction based on clearTK (Ogren et al., 2008). The following features have been used: BOW features Bag-of-word features are based on the assumption that certain words need to appear in a correct answer. We used a mixture of token unigrams, bigrams, and trigrams, where each n-gram is a binary feature that can either be true or false for a document.5 Additionally, we also used the number of tokens in the student answer as another feature in this group. Syntactic Features We extend BOW features with syntactic functions by adding dependency and phrase n-grams. Dependency n-grams a</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="3815" citStr="Landauer et al., 1998" startWordPosition="573" endWordPosition="576">o other entries in the challenge), especially in unseen domains. 2 Background 2.1 Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 285–289, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2006; Gabrilovich and Markovitch, 2007; Landauer et al., 1998), only a single text similarity measure has typically been applied to text pairs. However, as recent work (B¨ar et al., 2012a; B¨ar et al., 2012b) has shown, text similarity computation can be much improved when a variety of measures are combined. In recent years, UKP lab at TU Darmstadt has developed DKPro Similarity1, an open source toolkit for analyzing text similarity. It is part of the DKPro framework for natural language processing (Gurevych et al., 2007). DKPro Similarity excels at the tasks of measuring semantic textual similarity (STS) and detecting text reuse (DTR), having achieved t</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25(2&amp;3):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>118--125</pages>
<location>Pittsburgh, PA USA.</location>
<contexts>
<context position="8164" citStr="Lyon et al., 2001" startWordPosition="1267" endWordPosition="1270">e use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3code.google.com/p/dkpro-core-asl/ 4DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 286 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence scores and add the minimum, maximum, average, a</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), pages 118–125, Pittsburgh, PA USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<location>Boston, MA.</location>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence, pages 775–780, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven Bethard</author>
</authors>
<title>ClearTK: A UIMA Toolkit for Statistical Natural Language Processing.</title>
<date>2008</date>
<booktitle>In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</booktitle>
<contexts>
<context position="6425" citStr="Ogren et al., 2008" startWordPosition="989" endWordPosition="992">ode.google.com/p/dkpro-similarity-asl 2cs.biu.ac.il/˜nlp/downloads/biutee provide the details for each submitted run, and finally, our empirical results. 3.1 System Description We build a system based on the Apache UIMA framework (Ferrucci and Lally, 2004) and DKPro Lab (Eckart de Castilho and Gurevych, 2011). We use DKPro Core3 for preprocessing. Specifically, we used the default DKPro segmenter, TreeTagger POS tagger and chunker, Jazzy Spell Checker, and the Stanford parser.4 We trained a supervised model (Naive Bayes) using Weka (Hall et al., 2009) with feature extraction based on clearTK (Ogren et al., 2008). The following features have been used: BOW features Bag-of-word features are based on the assumption that certain words need to appear in a correct answer. We used a mixture of token unigrams, bigrams, and trigrams, where each n-gram is a binary feature that can either be true or false for a document.5 Additionally, we also used the number of tokens in the student answer as another feature in this group. Syntactic Features We extend BOW features with syntactic functions by adding dependency and phrase n-grams. Dependency n-grams are combinations of two tokens and their dependency relation. P</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA Toolkit for Statistical Natural Language Processing. In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>448--453</pages>
<contexts>
<context position="13170" citStr="Resnik, 1995" startWordPosition="2117" endWordPosition="2118">stem is composed of three methods: Exact, WordNet, and BIUTEE. These were combined in different combinations to form the different runs. Exact In this baseline method, we represent a student answer as a bag-of-words containing all tokens and lemmas appearing in the text. Lemmas are used to account for minor morphological differences, such as tense or plurality. We then check whether both facet words appear in the set. WordNet checks whether both facet words, or their semantically related words, appear in the student’s answer. We use WordNet (Fellbaum, 1998) with the Resnik similarity measure (Resnik, 1995) and count a facet term as matched if the similarity score exceeds a certain threshold (0.9, empirically determined on the training set). BIUTEE processes dependency trees instead of bags of words. We therefore added a pre-processing stage that extracts a path in the dependency parse that represents the facet. This is done by first parsing the entire reference answer, and then locating the two nodes mentioned in the facet. We then find their lowest common ancestor (LCA), and extract the path from the facet’s first word to the second through the LCA. BIUTEE can now be given the student answer a</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI 1995), pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Ido Dagan</author>
</authors>
<title>A confidence model for syntactically-motivated entailment proofs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 8th International Conference on Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>455--462</pages>
<contexts>
<context position="2946" citStr="Stern and Dagan, 2011" startWordPosition="447" endWordPosition="450"> provide a robust architecture for student response analysis, that can generalize and perform well in multiple domains. Moreover, we are interested in evaluating how well general-purpose technologies will perform in this setting. We therefore approach the challenge by combining two such technologies: DKPro Similarity –an extensive suite of text similarity measures– that has been successfully applied in other settings like the SemEval 2012 task on semantic textual similarity (B¨ar et al., 2012a) or reuse detection (B¨ar et al., 2012b). BIUTEE, the Bar-Ilan University Textual Entailment Engine (Stern and Dagan, 2011), which has shown state-of-the-art performance on recognizing textual entailment challenges. Our systems use both technologies to extract features, and combine them in a supervised model. Indeed, this approach works relatively well (with respect to other entries in the challenge), especially in unseen domains. 2 Background 2.1 Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Compu</context>
<context position="5229" citStr="Stern and Dagan, 2011" startWordPosition="805" endWordPosition="808">e (Dagan et al., 2009). The most prevalent task of textual entailment is to recognize whether the meaning of a target natural language statement (H for hypothesis) can be inferred from another piece of text (T for text). Apparently, this core task underlies semantic inference in many text applications. The task of analyzing student responses is one such example. By assigning the student’s answer as T and the reference answer as H, we are basically asking whether one can infer the correct (reference) answer from the student’s response. In recent years, Bar-Ilan University has developed BIUTEE (Stern and Dagan, 2011), an extensive textual entailment recognition engine. BIUTEE tries to convert T (represented as a dependency tree) to H. It does so by applying a series of knowledge-based transformations, such as synonym substitution, active-passive conversion, and more. BIUTEE is publicly available as open source.2 3 Main Task In this section, we explain how we approached the main task, in which the system needs to analyze each answer as a whole. After describing our system’s architecture, we explain how we selected training data for the different scenarios in the main task. We then 1code.google.com/p/dkpro-</context>
<context position="8505" citStr="Stern and Dagan, 2011" startWordPosition="1323" endWordPosition="1326">s on the training set, so we also used this number for the test runs. 6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 286 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence scores and add the minimum, maximum, average, and median confidence. 3.2 Data Selection Regime There are three scenarios under which our system is expected to perform. For each one, we chose (apriori) a different set of examples for training. Unseen Answers Classify new answers to familiar questions. Train on instances that have the same question as the test instance. Unseen Questions </context>
</contexts>
<marker>Stern, Dagan, 2011</marker>
<rawString>Asher Stern and Ido Dagan. 2011. A confidence model for syntactically-motivated entailment proofs. In Proceedings of the 8th International Conference on Recent Advances in Natural Language Processing (RANLP 2011), pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>YAP3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of the 27th SIGCSE Technical Symposium on Computer Science Education (SIGCSE</booktitle>
<pages>130--134</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8024" citStr="Wise, 1996" startWordPosition="1247" endWordPosition="1248">ximum, average, and median similarity.6 Semantic Similarity Features are very similar to the basic similarity features, except that we use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3code.google.com/p/dkpro-core-asl/ 4DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 286 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T) with each reference answer (as H), which results in an array of numerical entailment confi</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J. Wise. 1996. YAP3: Improved detection of similarities in computer program and other texts. In Proceedings of the 27th SIGCSE Technical Symposium on Computer Science Education (SIGCSE 1996), pages 130–134, Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>