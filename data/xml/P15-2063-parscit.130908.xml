<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.950738">
Language Identification and Modeling in Specialized Hardware
</title>
<author confidence="0.830347">
Kenneth Heafield*,† Rohan Kshirsagar* Santiago Barona*
</author>
<note confidence="0.604247">
* Bloomberg L.P. † University of Edinburgh
731 Lexington Ave. 10 Crichton Street
</note>
<address confidence="0.883679">
New York, NY 10022 USA Edinburgh EH8 9AB, UK
</address>
<email confidence="0.99515">
{kheafield,rkshirsagar2,sbarona}@bloomberg.net
</email>
<sectionHeader confidence="0.993751" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969125">
We repurpose network security hardware
to perform language identification and lan-
guage modeling tasks. The hardware is
a deterministic pushdown transducer since
it executes regular expressions and has a
stack. One core is 2.4 times as fast at lan-
guage identification and 1.8 to 6 times as
fast at part-of-speech language modeling.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930470588235">
Larger data sizes and more detailed models have
led to adoption of specialized hardware for natural
language processing. Graphics processing units
(GPUs) are the most common, with applications
to neural networks (Oh and Jung, 2004) and pars-
ing (Johnson, 2011). Field-programmable gate ar-
rays (FPGAs) are faster and more customizable,
so grammars can be encoded in gates (Ciressan
et al., 2000). In this work, we go further down
the hardware hierarchy by performing language
identification and language modeling tasks on an
application-specific integrated circuit designed for
network monitoring.
The hardware is programmable with regular ex-
pressions and access to a stack. It is therefore a de-
terministic pushdown transducer. Prior work used
the hardware mostly as intended, by scanning hard
drive contents against a small set of patterns for
digital forensics purposes (Lee et al., 2008). The
purposes of this paper are to introduce the natural
language processing community to the hardware
and evaluate performance.
We chose the related tasks of language identi-
fication and language modeling because they do
not easily map to regular expressions. Fast lan-
guage classification is essential to using the web
as a corpus (Smith et al., 2013) and packages com-
pete on speed (Lui and Baldwin, 2012). Exten-
sive literature on fast language models comprises
a strong baseline (Stolcke, 2002; Federico et al.,
2008; Heafield, 2011; Yasuhara et al., 2013). In
both cases, matches are frequent, which differs
from network security and forensics applications
where matches are rare.
</bodyText>
<sectionHeader confidence="0.999397" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.990114296296296">
Automata have been emulated on CPUs with
AT&amp;T FSM (Mohri et al., 2000) and OpenFST
(Allauzen et al., 2007), on GPUs (Rudomin et al.,
2005; He et al., 2015), and on FPGAs (Sidhu and
Prasanna, 2001; Lin et al., 2006; Korenek, 2010).
These are candidates for the ASIC we use. In par-
ticular, gappy pattern matching (He et al., 2015)
maps directly to regular expressions.
GPUs have recently been applied to the re-
lated problem of parsing (Johnson, 2011; Yi et al.,
2011). These operate largely by turning a sparse
parsing problem into a highly-parallel dense prob-
lem (Canny et al., 2013) and by clustering similar
workloads (Hall et al., 2014). Since the hardware
used in this paper is a deterministic pushdown au-
tomaton, parsing ambiguous natural language is
theoretically impossible without using the CPU as
an oracle. Hall et al. (2014) rely on communi-
cation between the CPU and GPU, albeit for ef-
ficiency reasons rather than out of necessity.
Work on efficiently querying backoff language
models (Katz, 1987) has diverged from a finite
state representation. DALM (Yasuhara et al.,
2013) is an efficient trie-based representation us-
ing double arrays while KenLM (Heafield, 2011)
has traditional tries and a linear probing hash ta-
ble. We use the fastest baselines from both.
</bodyText>
<sectionHeader confidence="0.997162" genericHeader="method">
3 Programming Model
</sectionHeader>
<bodyText confidence="0.99893775">
The fundamental programming unit is a POSIX
regular expression including repetition, line
boundaries, and trailing context. For example,
a[bc] matches “ab” and “ac”.
</bodyText>
<page confidence="0.952896">
384
</page>
<bodyText confidence="0.901961923076923">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 384–389,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
When an expression matches, the hardware can
output a constant to the CPU, output the span
matched, push a symbol onto the stack, pop from
the stack, or halt. There is little meaning to
the order in which the expressions appear in the
program. All expressions are able to match at
any time, but can condition on the top of the
stack. This is similar to the flex tool (Lesk and
Schmidt, 1975), which refers to stack symbols as
start conditions.
</bodyText>
<sectionHeader confidence="0.986524" genericHeader="method">
4 Language Identification
</sectionHeader>
<bodyText confidence="0.999962142857143">
We exactly replicate the model of langid.py
(Lui and Baldwin, 2012) to identify 97 languages.
Their Naive Bayes model has 7,480 features fi,
each of which is a string of up to four bytes (Lui
and Baldwin, 2011). Inference amounts to collect-
ing the count ci of each feature and computing the
most likely language l given model p.
</bodyText>
<equation confidence="0.9937415">
l∗ = argmax
l
</equation>
<bodyText confidence="0.999954619047619">
We use the hardware to find all instances of fea-
tures in the input. Feature strings are converted
to literal regular expressions. When the hardware
matches the expression for feature fi, it outputs
the unique feature index i. Since the hardware
has no user-accessible arithmetic, the CPU accu-
mulates feature counts ci in an array and performs
subsequent modeling steps. The baseline emulates
automata on the CPU (Aho and Corasick, 1975).
Often the input is a collection of documents,
each of which should be classified independently.
To separate documents, we have the hardware
match document boundaries, such as newlines,
and output a special value. Since the hardware
natively reports matches in order by start position
(then by end position), the special value acts as
a delimiter between documents that the CPU can
detect. This removes the need to reconcile docu-
ment offsets on the CPU and saves bus bandwidth
since the hardware can be configured to not report
offsets.
</bodyText>
<sectionHeader confidence="0.994332" genericHeader="method">
5 Language Model Probability
</sectionHeader>
<bodyText confidence="0.6381782">
The task is to compute the language model prob-
ability p of some text w. Backoff models (Katz,
1987) memorize probability for seen n–grams and
charge a backoff penalty b for unseen n–grams.
p(wn  |wn−1
</bodyText>
<footnote confidence="0.76836925">
1 ) if wn1 is seen
p(wn  |wn−1
2 )b(wn−1
1 ) o.w.
</footnote>
<subsectionHeader confidence="0.9961">
5.1 Optimizing the Task
</subsectionHeader>
<bodyText confidence="0.9985912">
The backoff algorithm normally requires stor-
ing probability p and backoff b with each seen
n–gram. However, Heafield et al. (2012) used
telescoping series to prove that probability and
backoff can be collapsed into a single function q
</bodyText>
<equation confidence="0.991013">
|w1 n−1 n i
n
q(wn= p(wn|w)nZn−
1Hi b(()
</equation>
<bodyText confidence="0.99971375">
This preserves sentence-level probabilities.1
Because the hardware lacks user-accessible
arithmetic, terms are sent to the CPU. Sending just
q for each token instead of p and various backoffs
b reduces communication and CPU workload. We
also benefit from a simplified query procedure: for
each word, match as much context as possible then
return the corresponding value q.
</bodyText>
<subsectionHeader confidence="0.996951">
5.2 Greedy Matching
</subsectionHeader>
<bodyText confidence="0.988367137931035">
Language models are greedy in the sense that, for
every word, they match as much leading context
as possible. We map this onto greedy regular ex-
pressions, which match as much trailing context as
possible, by reversing the input and n–grams.2
Unlike language identification, we run the hard-
ware in a greedy mode that scans until a match
is found, reports the longest such match, and re-
sumes scanning afterwards. The trailing context
operator / allows fine-grained control over the off-
set where scanning resumes. Given two regular
expressions r and s, the trailing context expres-
sion r/s matches rs as if they were concatenated,
but scanning resumes after r. For example, if the
language model contains n–gram “This is a”, then
we create regular expression
&amp;quot; a&amp;quot;/&amp;quot; is This &amp;quot;
where the quotes ensure that spaces are interpreted
literally. Scanning resumes at the space before the
next word: “ is”. Because greedy mode suppresses
shorter matches, only the longest n–gram will be
reported. The CPU can then sum log q values asso-
ciated with each expression without regard to po-
sition.
Unknown words are detected by matching a
space: &amp;quot; &amp;quot;. Vocabulary words will greedily
1Technically, q is off by the constant b(&lt;s&gt;) due to con-
ditioning on &lt;s&gt;. We account for this at the end of sentence,
re-defining q(&lt;/s&gt;  |wn−1
</bodyText>
<footnote confidence="0.6979678">
1 ) ← q(&lt;/s&gt;  |wn−1
1 )b(&lt;s&gt;).
Doing so saves one output per sentence.
2For exposition, we show words in reverse order. The
implementation reverses bytes.
</footnote>
<equation confidence="0.9952482">
p(l) � p(fi|l)ci
i
�
p(wn  |wn−1
1 ) =
</equation>
<page confidence="0.987252">
385
</page>
<subsectionHeader confidence="0.292502">
Rule Value Purpose
</subsectionHeader>
<listItem confidence="0.8772052">
&amp;quot; a&amp;quot;/&amp;quot; in &amp;quot; q(a  |in) Normal query
&amp;quot; &amp;quot; q(&lt;unk&gt;) Unknown word
&amp;quot; in&amp;quot;/&amp;quot; \n&amp;quot; q(in  |&lt;s&gt;) Sentence begin
&amp;quot; \n&amp;quot;/&amp;quot; &amp;quot; q(&lt;/s&gt;) Sentence end
&amp;quot; \n&amp;quot;/&amp;quot; in &amp;quot; q(&lt;/s&gt;  |in) Sentence end
</listItem>
<tableCaption confidence="0.814528">
Table 1: Example regular expressions, including
</tableCaption>
<table confidence="0.775797888888889">
the special rules for the unknown word and sen-
tence boundaries. We rely on the newline \n in
lieu of sentence boundary tokens &lt;s&gt; and &lt;/s&gt;.
Model Platform 1 core 5 cores
Hardware 160.34 608.41
langid C 64.57 279.18
Java 25.53 102.72
Python 2.90 12.63
CLD2 C++ 12.39 30.15
</table>
<tableCaption confidence="0.999295">
Table 2: Language identification speed in MB/s.
</tableCaption>
<bodyText confidence="0.999493666666667">
match their own regular expression, which begins
with a space. This space also prevents matching
inside an unknown word (e.g. “Ugrasena” should
not match “a”). The tokenizer is expected to re-
move duplicate spaces and add them at line bound-
aries. Table 1 shows key expressions.
Instead of strings, we can match vocabulary in-
dices. Spaces are unnecessary since indices have
fixed length and the unknown word has an index.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999868444444444">
We benchmarked a Tarari T2540 PCI express de-
vice from 2011 against several CPU baselines. It
has 2 GB of DDR2 RAM and 5 cores. A single-
threaded CPU program controls the device and
performs arithmetic. The program scaled linearly
to control four devices, so it is not a bottleneck.
Wall clock time, except loading, is the minimum
from three runs on an otherwise-idle machine.
Models and input were in RAM before each run.
</bodyText>
<subsectionHeader confidence="0.993158">
6.1 Language Identification
</subsectionHeader>
<bodyText confidence="0.998672222222222">
The langid.py model is 88.6–99.2% accurate
(Lui and Baldwin, 2012). We tested the origi-
nal Python, a Java implementation that “should be
faster than anything else out there” (Weiss, 2013),
a C implementation (Lui, 2014), and our replica in
hardware. We also tested CLD2 (Sites, 2013) writ-
ten in C++, which has a different model that was
less accurate on 4 of 6 languages selected from
Europarl (Koehn, 2005). Time includes the costs
</bodyText>
<table confidence="0.978028333333333">
Lines Tokens Ken DA 1 core 5 cores
100 2.6 · 103 37.8 40.3 6.6 2.1
1000 2.2 · 104 42.4 43.6 16.2 10.7
10000 2.6 · 105 53.9 55.7 46.2 42.0
100000 2.8 · 106 78.6 85.3 91.3 93.6
305263 8.6 · 106 92.9 105.6 97.0 91.8
</table>
<tableCaption confidence="0.87069225">
Table 3: Seconds to compute perplexity on strings.
The hardware was tested with 1 core and 5 cores.
of feature extraction and modeling.
Table 2 reports speed measured on a 9.6 GB text
</tableCaption>
<bodyText confidence="0.927989777777778">
file created by concatenating the 2013 News Crawl
corpora for English, French, German, Hindi,
Spanish, and Russian (Bojar et al., 2014). One
hardware core is 2.48 times as fast as the fastest
CPU program. Using five cores instead of one
yielded speed improvements of 3.8x on hardware
and 4.3x on a 16-core CPU. The hardware per-
forms decently on this task, likely because the 1
MB binary transition table mostly fits in cache.
</bodyText>
<subsectionHeader confidence="0.998589">
6.2 Language Modeling
</subsectionHeader>
<bodyText confidence="0.999921666666667">
We benchmarked against the fastest reported lan-
guage models, DALM’s reverse trie (Yasuhara et
al., 2013) and KenLM’s linear probing (Heafield,
2011). Both use stateful queries. For sur-
face strings, time includes the cost of vocabulary
lookup. For vocabulary identifiers, we converted
words to bytes then timed custom query programs.
Unpruned models were trained on the En-
glish side of the French–English MultiUN corpus
(Eisele and Chen, 2010). Perplexity was computed
on 2.6 GB of tokenized text from the 2013 English
News Crawl (Bojar et al., 2014).
</bodyText>
<subsectionHeader confidence="0.937074">
6.2.1 Surface Strings
</subsectionHeader>
<bodyText confidence="0.999904857142857">
We tested trigram language models trained on var-
ious amounts of data before reaching a software-
imposed limit of 4.2 million regular expressions.3
Figure 1 and Table 3 show total query time as a
function of training data size while Figure 2 shows
model size. DALM model size includes the entire
directory.
Cache effects are evident: the hardware binary
format is much larger because it stores a generic
table. Queries are fast for tiny models but become
slower than the CPU. Multiple cores do not help
for larger models because they share the cache and
memory bus. Since the hardware operates at the
byte level and there is an average of 5.34 bytes
</bodyText>
<footnote confidence="0.99144">
3Intel is working to remove this restriction.
</footnote>
<page confidence="0.975662">
386
387
</page>
<figure confidence="0.997243333333333">
DALM
KenLM
5 cores
1 core
104 105 106 107
Tokens of training data
</figure>
<figureCaption confidence="0.993497">
Figure 1: Time to compute perplexity on strings.
</figureCaption>
<figure confidence="0.974479066666667">
Binary file size (MB)
800
600
400
300
700
500
200
100
0
Hard
DALM
KenLM
0 1 2 3 4 5 6 7 8 9
Millions of tokens of training data
</figure>
<figureCaption confidence="0.999969">
Figure 2: Size of the models on strings.
</figureCaption>
<bodyText confidence="0.99736675">
per word, random memory accesses happen more
often than in CPU-based models that operate on
words. We then set out to determine if the hard-
ware runs faster when each word is a byte.
</bodyText>
<subsubsectionHeader confidence="0.909038">
6.2.2 Vocabulary Indices
</subsubsectionHeader>
<bodyText confidence="0.999868166666667">
Class-based language models are often used
alongside lexical language models to form gener-
alizations. We tested a 5–gram language model
over CoNLL part-of-speech tags from MITIE
(King, 2014). There are fewer than 256 unique
tags, fitting into a byte per word. We also cre-
ated special KenLM and DALM query programs
that read byte-encoded input. Figure 3 and Ta-
ble 4 show total time while model sizes are shown
in Figure 4. Performance plateaus for very small
models, which is more clearly shown by plotting
speed in Figure 5.
</bodyText>
<figure confidence="0.9966385">
KenLM
DALM
1 core
5 cores
102 103 104 105 106 107 108 109
Tokens of training data
</figure>
<figureCaption confidence="0.941347">
Figure 3: Time to compute perplexity on bytes.
</figureCaption>
<figure confidence="0.976587285714286">
Binary file size (MB)
140
120
100
80
60
40
20
0
Hard
KenLM
DALM
0 50 100 150 200 250 300 350 400
Millions of tokens of training data
</figure>
<figureCaption confidence="0.996536">
Figure 4: Size of the models on bytes.
</figureCaption>
<figure confidence="0.998386533333333">
102 103 104 105 106 107 108 109
Tokens of training data
5 cores
1 core
DALM
KenLM
800
700
600
500
400
300
200
100
0
</figure>
<figureCaption confidence="0.99518">
Figure 5: Speed, in words per microsecond, to
compute perplexity on bytes.
</figureCaption>
<figure confidence="0.997156263157895">
Words per microsecond excluding loading
120
100
80
60
40
20
Real seconds excluding loading
0
80
70
60
50
40
30
20
10
Real seconds excluding loading
0
</figure>
<table confidence="0.973897285714286">
Lines Tokens Ken DA 1 core 5 cores
100 2.6 · 103 38.0 24.1 3.4 0.9
1000 2.3 · 104 46.1 27.5 7.5 5.0
10000 2.7 · 105 53.9 33.4 15.7 10.7
100000 2.9 · 106 57.5 34.2 21.1 19.3
1000000 2.9 · 107 65.2 35.4 22.1 20.7
13000000 3.7 · 108 73.0 42.9 23.3 22.0
</table>
<tableCaption confidence="0.807652">
Table 4: Seconds to compute perplexity on bytes.
The hardware was tested with 1 core and 5 cores.
</tableCaption>
<bodyText confidence="0.999924928571428">
The hardware is faster for all training data sizes
we tested. For tiny models, one core is initially 6
times as fast one CPU core while larger models are
1.8 times as fast as the CPU. For small models, the
hardware appears to hitting another limit, perhaps
the speed at which a core can output matches. This
is not a CPU or PCI bus limitation because five
cores are faster than one core, by a factor of 4.67.
Model growth is sublinear because novel POS
n–grams are limited. The hardware binary image
is 3.4 times as large as DALM, compared with
7.2 times as large for the lexical model. We at-
tribute this to denser transition tables that result
from model saturation.
</bodyText>
<sectionHeader confidence="0.971806" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999238">
We thank Intel and Xanadata for numerous con-
sultations and for providing access to a machine
with four devices. Intel markets the hardware as
a regular expression processor, not a deterministic
pushdown automaton.
</bodyText>
<sectionHeader confidence="0.99216" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99998">
Language identification and language modeling
entail scanning that can be offloaded to regular ex-
pression hardware. The hardware works best for
small models, such as those used in language iden-
tification. Like CPUs, random memory accesses
are slow. We believe it will be useful for web-
scale extraction problems, where language identi-
fication and coarse language modeling are used to
filter large amounts of data. We plan to investigate
a new hardware version that Intel is preparing.
</bodyText>
<sectionHeader confidence="0.993941" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99944605">
Alfred V. Aho and Margaret J. Corasick. 1975. Ef-
ficient string matching: An aid to bibliographic
search. Commun. ACM, 18(6):333–340, June.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11–23. Springer.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 12–58, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
John Canny, David Hall, and Dan Klein. 2013. A
multi-teraflop constituency parser using GPUs. In
Proceedings of EMNLP, pages 1898–1907.
Cristian Ciressan, Eduardo Sanchez, Martin Rajman,
and Jean-Cedric Chappelier. 2000. An fpga-based
coprocessor for the parsing of context-free gram-
mars. In Field-Programmable Custom Computing
Machines, Annual IEEE Symposium on, pages 236–
236. IEEE Computer Society.
Andreas Eisele and Yu Chen. 2010. MultiUN: A
multilingual corpus from United Nation documents.
In Daniel Tapias, Mike Rosner, Stelios Piperidis,
Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid
Choukri, and Nicoletta Calzolari, editors, Proceed-
ings of the Seventh conference on International
Language Resources and Evaluation, pages 2868–
2872. European Language Resources Association
(ELRA), 5.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
David Hall, Taylor Berg-Kirkpatrick, John Canny, and
Dan Klein. 2014. Sparser, better, faster GPU pars-
ing. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics,
pages 208–217, June.
Hua He, Jimmy Lin, and Adam Lopez. 2015. Gappy
pattern matching on GPUs for on-demand extrac-
tion of hierarchical translation grammars. Transac-
tions of the Association for Computational Linguis-
tics, 3:87–100.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-efficient
storage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.992737">
388
</page>
<reference confidence="0.998681619047619">
Mark Johnson. 2011. Parsing in parallel on multiple
cores and GPUs. In Proceedings of the Australasian
Language Technology Association Workshop 2011,
pages 29–37, December.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, ASSP-35(3):400–
401, March.
Davis E. King. 2014. MITIE: MIT information
extraction, January. https://github.com/
mit-nlp/MITIE.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Jan Korenek. 2010. Fast regular expression matching
using FPGA. Information Sciences and Technolo-
gies Bulletin of the ACM Slovakia, 2(2):103–111.
Jooyoung Lee, Sungkyong Un, and Dowon Hong.
2008. High-speed search using Tarari content pro-
cessor in digital forensics. Digital Investigation,
5:S91–S95.
Michael E Lesk and Eric Schmidt. 1975. Lex: A lexi-
cal analyzer generator, July.
Cheng-Hung Lin, Chih-Tsun Huang, Chang-Ping
Jiang, and Shih-Chieh Chang. 2006. Optimization
of regular expression pattern matching circuits on
FPGA. In Design, Automation and Test in Europe,
2006. DATE’06. Proceedings, volume 2, pages 1–6.
IEEE.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 553–561,
Chiang Mai, Thailand, November.
Marco Lui and Timothy Baldwin. 2012.
langid.py: An off-the-shelf language iden-
tification tool. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics, pages 25–30, Jeju, Republic of Korea,
July.
Marco Lui. 2014. Pure C natural language iden-
tifier with support for 97 languages. https://
github.com/saffsd/langid.c.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17–32.
Kyoung-Su Oh and Keechul Jung. 2004. GPU imple-
mentation of neural networks. Pattern Recognition,
37(6):1311–1314.
Isaac Rudom´ın, Erik Mill´an, and Benjam´ın Hern´andez.
2005. Fragment shaders for agent animation using
finite state machines. Simulation Modelling Prac-
tice and Theory, 13(8):741–751.
Reetinder Sidhu and Viktor K Prasanna. 2001. Fast
regular expression matching using FPGAs. In Field-
Programmable Custom Computing Machines, 2001.
FCCM’01. The 9th Annual IEEE Symposium on,
pages 227–238. IEEE.
Dick Sites. 2013. Compact language detection 2.
https://code.google.com/p/cld2/.
Jason R. Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the common crawl. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901–904.
Dawid Weiss. 2013. Java port of langid.py
(language identifier). https://github.com/
carrotsearch/langid-java.
Makoto Yasuhara, Toru Tanaka, Jun-ya Norimatsu, and
Mikio Yamamoto. 2013. An efficient language
model using double-array structures. In Proceed-
ings of EMNLP, pages 222–232, October.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel CKY parsing on
GPUs. In Proceedings of the 12th International
Conference on Parsing Technologies, pages 175–
185. Association for Computational Linguistics.
</reference>
<page confidence="0.999153">
389
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686799">
<title confidence="0.998192">Language Identification and Modeling in Specialized Hardware</title>
<author confidence="0.917731">Santiago</author>
<affiliation confidence="0.999794">L.P. of Edinburgh</affiliation>
<address confidence="0.879608">731 Lexington Ave. 10 Crichton Street New York, NY 10022 USA Edinburgh EH8 9AB,</address>
<abstract confidence="0.998296333333333">We repurpose network security hardware to perform language identification and language modeling tasks. The hardware is a deterministic pushdown transducer since it executes regular expressions and has a stack. One core is 2.4 times as fast at language identification and 1.8 to 6 times as fast at part-of-speech language modeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Margaret J Corasick</author>
</authors>
<title>Efficient string matching: An aid to bibliographic search.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="5218" citStr="Aho and Corasick, 1975" startWordPosition="827" endWordPosition="830">s a string of up to four bytes (Lui and Baldwin, 2011). Inference amounts to collecting the count ci of each feature and computing the most likely language l given model p. l∗ = argmax l We use the hardware to find all instances of features in the input. Feature strings are converted to literal regular expressions. When the hardware matches the expression for feature fi, it outputs the unique feature index i. Since the hardware has no user-accessible arithmetic, the CPU accumulates feature counts ci in an array and performs subsequent modeling steps. The baseline emulates automata on the CPU (Aho and Corasick, 1975). Often the input is a collection of documents, each of which should be classified independently. To separate documents, we have the hardware match document boundaries, such as newlines, and output a special value. Since the hardware natively reports matches in order by start position (then by end position), the special value acts as a delimiter between documents that the CPU can detect. This removes the need to reconcile document offsets on the CPU and saves bus bandwidth since the hardware can be configured to not report offsets. 5 Language Model Probability The task is to compute the langua</context>
</contexts>
<marker>Aho, Corasick, 1975</marker>
<rawString>Alfred V. Aho and Margaret J. Corasick. 1975. Efficient string matching: An aid to bibliographic search. Commun. ACM, 18(6):333–340, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>Openfst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Implementation and Application of Automata,</booktitle>
<pages>11--23</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2337" citStr="Allauzen et al., 2007" startWordPosition="353" endWordPosition="356"> and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In Implementation and Application of Automata, pages 11–23. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
</authors>
<title>Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="10630" citStr="Bojar et al., 2014" startWordPosition="1760" endWordPosition="1763">rate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs Lines Tokens Ken DA 1 core 5 cores 100 2.6 · 103 37.8 40.3 6.6 2.1 1000 2.2 · 104 42.4 43.6 16.2 10.7 10000 2.6 · 105 53.9 55.7 46.2 42.0 100000 2.8 · 106 78.6 85.3 91.3 93.6 305263 8.6 · 106 92.9 105.6 97.0 91.8 Table 3: Seconds to compute perplexity on strings. The hardware was tested with 1 core and 5 cores. of feature extraction and modeling. Table 2 reports speed measured on a 9.6 GB text file created by concatenating the 2013 News Crawl corpora for English, French, German, Hindi, Spanish, and Russian (Bojar et al., 2014). One hardware core is 2.48 times as fast as the fastest CPU program. Using five cores instead of one yielded speed improvements of 3.8x on hardware and 4.3x on a 16-core CPU. The hardware performs decently on this task, likely because the 1 MB binary transition table mostly fits in cache. 6.2 Language Modeling We benchmarked against the fastest reported language models, DALM’s reverse trie (Yasuhara et al., 2013) and KenLM’s linear probing (Heafield, 2011). Both use stateful queries. For surface strings, time includes the cost of vocabulary lookup. For vocabulary identifiers, we converted wor</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Canny</author>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>A multi-teraflop constituency parser using GPUs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1898--1907</pages>
<contexts>
<context position="2813" citStr="Canny et al., 2013" startWordPosition="436" endWordPosition="439">ere matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has t</context>
</contexts>
<marker>Canny, Hall, Klein, 2013</marker>
<rawString>John Canny, David Hall, and Dan Klein. 2013. A multi-teraflop constituency parser using GPUs. In Proceedings of EMNLP, pages 1898–1907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Ciressan</author>
<author>Eduardo Sanchez</author>
<author>Martin Rajman</author>
<author>Jean-Cedric Chappelier</author>
</authors>
<title>An fpga-based coprocessor for the parsing of context-free grammars.</title>
<date>2000</date>
<booktitle>In Field-Programmable Custom Computing Machines, Annual IEEE Symposium on,</booktitle>
<pages>236--236</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="1037" citStr="Ciressan et al., 2000" startWordPosition="148" endWordPosition="151">s a deterministic pushdown transducer since it executes regular expressions and has a stack. One core is 2.4 times as fast at language identification and 1.8 to 6 times as fast at part-of-speech language modeling. 1 Introduction Larger data sizes and more detailed models have led to adoption of specialized hardware for natural language processing. Graphics processing units (GPUs) are the most common, with applications to neural networks (Oh and Jung, 2004) and parsing (Johnson, 2011). Field-programmable gate arrays (FPGAs) are faster and more customizable, so grammars can be encoded in gates (Ciressan et al., 2000). In this work, we go further down the hardware hierarchy by performing language identification and language modeling tasks on an application-specific integrated circuit designed for network monitoring. The hardware is programmable with regular expressions and access to a stack. It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware a</context>
</contexts>
<marker>Ciressan, Sanchez, Rajman, Chappelier, 2000</marker>
<rawString>Cristian Ciressan, Eduardo Sanchez, Martin Rajman, and Jean-Cedric Chappelier. 2000. An fpga-based coprocessor for the parsing of context-free grammars. In Field-Programmable Custom Computing Machines, Annual IEEE Symposium on, pages 236– 236. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>MultiUN: A multilingual corpus from United Nation documents.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation,</booktitle>
<pages>2868--2872</pages>
<editor>In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari, editors,</editor>
<contexts>
<context position="11385" citStr="Eisele and Chen, 2010" startWordPosition="1883" endWordPosition="1886">.8x on hardware and 4.3x on a 16-core CPU. The hardware performs decently on this task, likely because the 1 MB binary transition table mostly fits in cache. 6.2 Language Modeling We benchmarked against the fastest reported language models, DALM’s reverse trie (Yasuhara et al., 2013) and KenLM’s linear probing (Heafield, 2011). Both use stateful queries. For surface strings, time includes the cost of vocabulary lookup. For vocabulary identifiers, we converted words to bytes then timed custom query programs. Unpruned models were trained on the English side of the French–English MultiUN corpus (Eisele and Chen, 2010). Perplexity was computed on 2.6 GB of tokenized text from the 2013 English News Crawl (Bojar et al., 2014). 6.2.1 Surface Strings We tested trigram language models trained on various amounts of data before reaching a softwareimposed limit of 4.2 million regular expressions.3 Figure 1 and Table 3 show total query time as a function of training data size while Figure 2 shows model size. DALM model size includes the entire directory. Cache effects are evident: the hardware binary format is much larger because it stores a generic table. Queries are fast for tiny models but become slower than the </context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. MultiUN: A multilingual corpus from United Nation documents. In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation, pages 2868– 2872. European Language Resources Association (ELRA), 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="2050" citStr="Federico et al., 2008" startWordPosition="307" endWordPosition="310">g hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related probl</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>John Canny</author>
<author>Dan Klein</author>
</authors>
<title>Sparser, better, faster GPU parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>208--217</pages>
<contexts>
<context position="2869" citStr="Hall et al., 2014" startWordPosition="445" endWordPosition="448">mulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has traditional tries and a linear probing hash table. We use</context>
</contexts>
<marker>Hall, Berg-Kirkpatrick, Canny, Klein, 2014</marker>
<rawString>David Hall, Taylor Berg-Kirkpatrick, John Canny, and Dan Klein. 2014. Sparser, better, faster GPU parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 208–217, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua He</author>
<author>Jimmy Lin</author>
<author>Adam Lopez</author>
</authors>
<title>Gappy pattern matching on GPUs for on-demand extraction of hierarchical translation grammars.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--87</pages>
<contexts>
<context position="2386" citStr="He et al., 2015" startWordPosition="363" endWordPosition="366">o regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is </context>
</contexts>
<marker>He, Lin, Lopez, 2015</marker>
<rawString>Hua He, Jimmy Lin, and Adam Lopez. 2015. Gappy pattern matching on GPUs for on-demand extraction of hierarchical translation grammars. Transactions of the Association for Computational Linguistics, 3:87–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Language model rest costs and space-efficient storage.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="6188" citStr="Heafield et al. (2012)" startWordPosition="992" endWordPosition="995">between documents that the CPU can detect. This removes the need to reconcile document offsets on the CPU and saves bus bandwidth since the hardware can be configured to not report offsets. 5 Language Model Probability The task is to compute the language model probability p of some text w. Backoff models (Katz, 1987) memorize probability for seen n–grams and charge a backoff penalty b for unseen n–grams. p(wn |wn−1 1 ) if wn1 is seen p(wn |wn−1 2 )b(wn−1 1 ) o.w. 5.1 Optimizing the Task The backoff algorithm normally requires storing probability p and backoff b with each seen n–gram. However, Heafield et al. (2012) used telescoping series to prove that probability and backoff can be collapsed into a single function q |w1 n−1 n i n q(wn= p(wn|w)nZn− 1Hi b(() This preserves sentence-level probabilities.1 Because the hardware lacks user-accessible arithmetic, terms are sent to the CPU. Sending just q for each token instead of p and various backoffs b reduces communication and CPU workload. We also benefit from a simplified query procedure: for each word, match as much context as possible then return the corresponding value q. 5.2 Greedy Matching Language models are greedy in the sense that, for every word,</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2012</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012. Language model rest costs and space-efficient storage. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="2066" citStr="Heafield, 2011" startWordPosition="311" endWordPosition="312">gainst a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (J</context>
<context position="3407" citStr="Heafield, 2011" startWordPosition="531" endWordPosition="532"> (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has traditional tries and a linear probing hash table. We use the fastest baselines from both. 3 Programming Model The fundamental programming unit is a POSIX regular expression including repetition, line boundaries, and trailing context. For example, a[bc] matches “ab” and “ac”. 384 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 384–389, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics When an expression matches, the hardwa</context>
<context position="11091" citStr="Heafield, 2011" startWordPosition="1839" endWordPosition="1840">d on a 9.6 GB text file created by concatenating the 2013 News Crawl corpora for English, French, German, Hindi, Spanish, and Russian (Bojar et al., 2014). One hardware core is 2.48 times as fast as the fastest CPU program. Using five cores instead of one yielded speed improvements of 3.8x on hardware and 4.3x on a 16-core CPU. The hardware performs decently on this task, likely because the 1 MB binary transition table mostly fits in cache. 6.2 Language Modeling We benchmarked against the fastest reported language models, DALM’s reverse trie (Yasuhara et al., 2013) and KenLM’s linear probing (Heafield, 2011). Both use stateful queries. For surface strings, time includes the cost of vocabulary lookup. For vocabulary identifiers, we converted words to bytes then timed custom query programs. Unpruned models were trained on the English side of the French–English MultiUN corpus (Eisele and Chen, 2010). Perplexity was computed on 2.6 GB of tokenized text from the 2013 English News Crawl (Bojar et al., 2014). 6.2.1 Surface Strings We tested trigram language models trained on various amounts of data before reaching a softwareimposed limit of 4.2 million regular expressions.3 Figure 1 and Table 3 show tot</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing in parallel on multiple cores and GPUs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>29--37</pages>
<contexts>
<context position="903" citStr="Johnson, 2011" startWordPosition="129" endWordPosition="130">Abstract We repurpose network security hardware to perform language identification and language modeling tasks. The hardware is a deterministic pushdown transducer since it executes regular expressions and has a stack. One core is 2.4 times as fast at language identification and 1.8 to 6 times as fast at part-of-speech language modeling. 1 Introduction Larger data sizes and more detailed models have led to adoption of specialized hardware for natural language processing. Graphics processing units (GPUs) are the most common, with applications to neural networks (Oh and Jung, 2004) and parsing (Johnson, 2011). Field-programmable gate arrays (FPGAs) are faster and more customizable, so grammars can be encoded in gates (Ciressan et al., 2000). In this work, we go further down the hardware hierarchy by performing language identification and language modeling tasks on an application-specific integrated circuit designed for network monitoring. The hardware is programmable with regular expressions and access to a stack. It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensic</context>
<context position="2678" citStr="Johnson, 2011" startWordPosition="415" endWordPosition="416">1; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state repres</context>
</contexts>
<marker>Johnson, 2011</marker>
<rawString>Mark Johnson. 2011. Parsing in parallel on multiple cores and GPUs. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 29–37, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>401</pages>
<contexts>
<context position="3238" citStr="Katz, 1987" startWordPosition="506" endWordPosition="507">ed to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has traditional tries and a linear probing hash table. We use the fastest baselines from both. 3 Programming Model The fundamental programming unit is a POSIX regular expression including repetition, line boundaries, and trailing context. For example, a[bc] matches “ab” and “ac”. 384 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan</context>
<context position="5884" citStr="Katz, 1987" startWordPosition="940" endWordPosition="941"> which should be classified independently. To separate documents, we have the hardware match document boundaries, such as newlines, and output a special value. Since the hardware natively reports matches in order by start position (then by end position), the special value acts as a delimiter between documents that the CPU can detect. This removes the need to reconcile document offsets on the CPU and saves bus bandwidth since the hardware can be configured to not report offsets. 5 Language Model Probability The task is to compute the language model probability p of some text w. Backoff models (Katz, 1987) memorize probability for seen n–grams and charge a backoff penalty b for unseen n–grams. p(wn |wn−1 1 ) if wn1 is seen p(wn |wn−1 2 )b(wn−1 1 ) o.w. 5.1 Optimizing the Task The backoff algorithm normally requires storing probability p and backoff b with each seen n–gram. However, Heafield et al. (2012) used telescoping series to prove that probability and backoff can be collapsed into a single function q |w1 n−1 n i n q(wn= p(wn|w)nZn− 1Hi b(() This preserves sentence-level probabilities.1 Because the hardware lacks user-accessible arithmetic, terms are sent to the CPU. Sending just q for eac</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-35(3):400– 401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davis E King</author>
</authors>
<date>2014</date>
<booktitle>MITIE: MIT information extraction,</booktitle>
<note>https://github.com/ mit-nlp/MITIE.</note>
<contexts>
<context position="12899" citStr="King, 2014" startWordPosition="2156" endWordPosition="2157">re 1: Time to compute perplexity on strings. Binary file size (MB) 800 600 400 300 700 500 200 100 0 Hard DALM KenLM 0 1 2 3 4 5 6 7 8 9 Millions of tokens of training data Figure 2: Size of the models on strings. per word, random memory accesses happen more often than in CPU-based models that operate on words. We then set out to determine if the hardware runs faster when each word is a byte. 6.2.2 Vocabulary Indices Class-based language models are often used alongside lexical language models to form generalizations. We tested a 5–gram language model over CoNLL part-of-speech tags from MITIE (King, 2014). There are fewer than 256 unique tags, fitting into a byte per word. We also created special KenLM and DALM query programs that read byte-encoded input. Figure 3 and Table 4 show total time while model sizes are shown in Figure 4. Performance plateaus for very small models, which is more clearly shown by plotting speed in Figure 5. KenLM DALM 1 core 5 cores 102 103 104 105 106 107 108 109 Tokens of training data Figure 3: Time to compute perplexity on bytes. Binary file size (MB) 140 120 100 80 60 40 20 0 Hard KenLM DALM 0 50 100 150 200 250 300 350 400 Millions of tokens of training data Fig</context>
</contexts>
<marker>King, 2014</marker>
<rawString>Davis E. King. 2014. MITIE: MIT information extraction, January. https://github.com/ mit-nlp/MITIE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="10072" citStr="Koehn, 2005" startWordPosition="1656" endWordPosition="1657"> devices, so it is not a bottleneck. Wall clock time, except loading, is the minimum from three runs on an otherwise-idle machine. Models and input were in RAM before each run. 6.1 Language Identification The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the original Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) written in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs Lines Tokens Ken DA 1 core 5 cores 100 2.6 · 103 37.8 40.3 6.6 2.1 1000 2.2 · 104 42.4 43.6 16.2 10.7 10000 2.6 · 105 53.9 55.7 46.2 42.0 100000 2.8 · 106 78.6 85.3 91.3 93.6 305263 8.6 · 106 92.9 105.6 97.0 91.8 Table 3: Seconds to compute perplexity on strings. The hardware was tested with 1 core and 5 cores. of feature extraction and modeling. Table 2 reports speed measured on a 9.6 GB text file created by concatenating the 2013 News Crawl corpora for English, French, German, Hindi, Spanish, and Russian (Bojar et al., 2014). One hardware core is 2.48 times as fast </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Korenek</author>
</authors>
<title>Fast regular expression matching using FPGA.</title>
<date>2010</date>
<journal>Information Sciences and Technologies Bulletin of the ACM Slovakia,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2460" citStr="Korenek, 2010" startWordPosition="378" endWordPosition="379">e web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (</context>
</contexts>
<marker>Korenek, 2010</marker>
<rawString>Jan Korenek. 2010. Fast regular expression matching using FPGA. Information Sciences and Technologies Bulletin of the ACM Slovakia, 2(2):103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jooyoung Lee</author>
<author>Sungkyong Un</author>
<author>Dowon Hong</author>
</authors>
<title>High-speed search using Tarari content processor in digital forensics.</title>
<date>2008</date>
<booktitle>Digital Investigation, 5:S91–S95.</booktitle>
<contexts>
<context position="1532" citStr="Lee et al., 2008" startWordPosition="223" endWordPosition="226">rammable gate arrays (FPGAs) are faster and more customizable, so grammars can be encoded in gates (Ciressan et al., 2000). In this work, we go further down the hardware hierarchy by performing language identification and language modeling tasks on an application-specific integrated circuit designed for network monitoring. The hardware is programmable with regular expressions and access to a stack. It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, whi</context>
</contexts>
<marker>Lee, Un, Hong, 2008</marker>
<rawString>Jooyoung Lee, Sungkyong Un, and Dowon Hong. 2008. High-speed search using Tarari content processor in digital forensics. Digital Investigation, 5:S91–S95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Lesk</author>
<author>Eric Schmidt</author>
</authors>
<title>Lex: A lexical analyzer generator,</title>
<date>1975</date>
<contexts>
<context position="4360" citStr="Lesk and Schmidt, 1975" startWordPosition="682" endWordPosition="685">e Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 384–389, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics When an expression matches, the hardware can output a constant to the CPU, output the span matched, push a symbol onto the stack, pop from the stack, or halt. There is little meaning to the order in which the expressions appear in the program. All expressions are able to match at any time, but can condition on the top of the stack. This is similar to the flex tool (Lesk and Schmidt, 1975), which refers to stack symbols as start conditions. 4 Language Identification We exactly replicate the model of langid.py (Lui and Baldwin, 2012) to identify 97 languages. Their Naive Bayes model has 7,480 features fi, each of which is a string of up to four bytes (Lui and Baldwin, 2011). Inference amounts to collecting the count ci of each feature and computing the most likely language l given model p. l∗ = argmax l We use the hardware to find all instances of features in the input. Feature strings are converted to literal regular expressions. When the hardware matches the expression for fea</context>
</contexts>
<marker>Lesk, Schmidt, 1975</marker>
<rawString>Michael E Lesk and Eric Schmidt. 1975. Lex: A lexical analyzer generator, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng-Hung Lin</author>
<author>Chih-Tsun Huang</author>
<author>Chang-Ping Jiang</author>
<author>Shih-Chieh Chang</author>
</authors>
<title>Optimization of regular expression pattern matching circuits on FPGA.</title>
<date>2006</date>
<booktitle>In Design, Automation and Test in Europe, 2006. DATE’06. Proceedings,</booktitle>
<volume>2</volume>
<pages>1--6</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2444" citStr="Lin et al., 2006" startWordPosition="374" endWordPosition="377">ential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracl</context>
</contexts>
<marker>Lin, Huang, Jiang, Chang, 2006</marker>
<rawString>Cheng-Hung Lin, Chih-Tsun Huang, Chang-Ping Jiang, and Shih-Chieh Chang. 2006. Optimization of regular expression pattern matching circuits on FPGA. In Design, Automation and Test in Europe, 2006. DATE’06. Proceedings, volume 2, pages 1–6. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Cross-domain feature selection for language identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>553--561</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="4649" citStr="Lui and Baldwin, 2011" startWordPosition="731" endWordPosition="734">tant to the CPU, output the span matched, push a symbol onto the stack, pop from the stack, or halt. There is little meaning to the order in which the expressions appear in the program. All expressions are able to match at any time, but can condition on the top of the stack. This is similar to the flex tool (Lesk and Schmidt, 1975), which refers to stack symbols as start conditions. 4 Language Identification We exactly replicate the model of langid.py (Lui and Baldwin, 2012) to identify 97 languages. Their Naive Bayes model has 7,480 features fi, each of which is a string of up to four bytes (Lui and Baldwin, 2011). Inference amounts to collecting the count ci of each feature and computing the most likely language l given model p. l∗ = argmax l We use the hardware to find all instances of features in the input. Feature strings are converted to literal regular expressions. When the hardware matches the expression for feature fi, it outputs the unique feature index i. Since the hardware has no user-accessible arithmetic, the CPU accumulates feature counts ci in an array and performs subsequent modeling steps. The baseline emulates automata on the CPU (Aho and Corasick, 1975). Often the input is a collecti</context>
</contexts>
<marker>Lui, Baldwin, 2011</marker>
<rawString>Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 553–561, Chiang Mai, Thailand, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--30</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="1938" citStr="Lui and Baldwin, 2012" startWordPosition="290" endWordPosition="293"> It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern match</context>
<context position="4506" citStr="Lui and Baldwin, 2012" startWordPosition="704" endWordPosition="707">, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics When an expression matches, the hardware can output a constant to the CPU, output the span matched, push a symbol onto the stack, pop from the stack, or halt. There is little meaning to the order in which the expressions appear in the program. All expressions are able to match at any time, but can condition on the top of the stack. This is similar to the flex tool (Lesk and Schmidt, 1975), which refers to stack symbols as start conditions. 4 Language Identification We exactly replicate the model of langid.py (Lui and Baldwin, 2012) to identify 97 languages. Their Naive Bayes model has 7,480 features fi, each of which is a string of up to four bytes (Lui and Baldwin, 2011). Inference amounts to collecting the count ci of each feature and computing the most likely language l given model p. l∗ = argmax l We use the hardware to find all instances of features in the input. Feature strings are converted to literal regular expressions. When the hardware matches the expression for feature fi, it outputs the unique feature index i. Since the hardware has no user-accessible arithmetic, the CPU accumulates feature counts ci in an </context>
<context position="9731" citStr="Lui and Baldwin, 2012" startWordPosition="1595" endWordPosition="1598">s. Spaces are unnecessary since indices have fixed length and the unknown word has an index. 6 Experiments We benchmarked a Tarari T2540 PCI express device from 2011 against several CPU baselines. It has 2 GB of DDR2 RAM and 5 cores. A singlethreaded CPU program controls the device and performs arithmetic. The program scaled linearly to control four devices, so it is not a bottleneck. Wall clock time, except loading, is the minimum from three runs on an otherwise-idle machine. Models and input were in RAM before each run. 6.1 Language Identification The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the original Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) written in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs Lines Tokens Ken DA 1 core 5 cores 100 2.6 · 103 37.8 40.3 6.6 2.1 1000 2.2 · 104 42.4 43.6 16.2 10.7 10000 2.6 · 105 53.9 55.7 46.2 42.0 100000 2.8 · 106 78.6 85.3 91.3 93.6 305263 8.6 · 106 92.9 105.6 97.0 91.8 Table 3: Seconds to </context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25–30, Jeju, Republic of Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
</authors>
<title>Pure C natural language identifier with support for 97 languages. https:// github.com/saffsd/langid.c.</title>
<date>2014</date>
<contexts>
<context position="9884" citStr="Lui, 2014" startWordPosition="1622" endWordPosition="1623"> against several CPU baselines. It has 2 GB of DDR2 RAM and 5 cores. A singlethreaded CPU program controls the device and performs arithmetic. The program scaled linearly to control four devices, so it is not a bottleneck. Wall clock time, except loading, is the minimum from three runs on an otherwise-idle machine. Models and input were in RAM before each run. 6.1 Language Identification The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the original Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) written in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs Lines Tokens Ken DA 1 core 5 cores 100 2.6 · 103 37.8 40.3 6.6 2.1 1000 2.2 · 104 42.4 43.6 16.2 10.7 10000 2.6 · 105 53.9 55.7 46.2 42.0 100000 2.8 · 106 78.6 85.3 91.3 93.6 305263 8.6 · 106 92.9 105.6 97.0 91.8 Table 3: Seconds to compute perplexity on strings. The hardware was tested with 1 core and 5 cores. of feature extraction and modeling. Table 2 reports speed measured on a 9</context>
</contexts>
<marker>Lui, 2014</marker>
<rawString>Marco Lui. 2014. Pure C natural language identifier with support for 97 languages. https:// github.com/saffsd/langid.c.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>The design principles of a weighted finitestate transducer library.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<volume>231</volume>
<issue>1</issue>
<contexts>
<context position="2301" citStr="Mohri et al., 2000" startWordPosition="347" endWordPosition="350"> tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in thi</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2000. The design principles of a weighted finitestate transducer library. Theoretical Computer Science, 231(1):17–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyoung-Su Oh</author>
<author>Keechul Jung</author>
</authors>
<title>GPU implementation of neural networks.</title>
<date>2004</date>
<journal>Pattern Recognition,</journal>
<volume>37</volume>
<issue>6</issue>
<contexts>
<context position="875" citStr="Oh and Jung, 2004" startWordPosition="122" endWordPosition="125">irsagar2,sbarona}@bloomberg.net Abstract We repurpose network security hardware to perform language identification and language modeling tasks. The hardware is a deterministic pushdown transducer since it executes regular expressions and has a stack. One core is 2.4 times as fast at language identification and 1.8 to 6 times as fast at part-of-speech language modeling. 1 Introduction Larger data sizes and more detailed models have led to adoption of specialized hardware for natural language processing. Graphics processing units (GPUs) are the most common, with applications to neural networks (Oh and Jung, 2004) and parsing (Johnson, 2011). Field-programmable gate arrays (FPGAs) are faster and more customizable, so grammars can be encoded in gates (Ciressan et al., 2000). In this work, we go further down the hardware hierarchy by performing language identification and language modeling tasks on an application-specific integrated circuit designed for network monitoring. The hardware is programmable with regular expressions and access to a stack. It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of p</context>
</contexts>
<marker>Oh, Jung, 2004</marker>
<rawString>Kyoung-Su Oh and Keechul Jung. 2004. GPU implementation of neural networks. Pattern Recognition, 37(6):1311–1314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac Rudom´ın</author>
<author>Erik Mill´an</author>
<author>Benjam´ın Hern´andez</author>
</authors>
<title>Fragment shaders for agent animation using finite state machines. Simulation Modelling Practice and Theory,</title>
<date>2005</date>
<marker>Rudom´ın, Mill´an, Hern´andez, 2005</marker>
<rawString>Isaac Rudom´ın, Erik Mill´an, and Benjam´ın Hern´andez. 2005. Fragment shaders for agent animation using finite state machines. Simulation Modelling Practice and Theory, 13(8):741–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reetinder Sidhu</author>
<author>Viktor K Prasanna</author>
</authors>
<title>Fast regular expression matching using FPGAs.</title>
<date>2001</date>
<booktitle>In FieldProgrammable Custom Computing Machines,</booktitle>
<pages>227--238</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2426" citStr="Sidhu and Prasanna, 2001" startWordPosition="370" endWordPosition="373">uage classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using t</context>
</contexts>
<marker>Sidhu, Prasanna, 2001</marker>
<rawString>Reetinder Sidhu and Viktor K Prasanna. 2001. Fast regular expression matching using FPGAs. In FieldProgrammable Custom Computing Machines, 2001. FCCM’01. The 9th Annual IEEE Symposium on, pages 227–238. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Sites</author>
</authors>
<title>Compact language detection 2.</title>
<date>2013</date>
<note>https://code.google.com/p/cld2/.</note>
<contexts>
<context position="9948" citStr="Sites, 2013" startWordPosition="1633" endWordPosition="1634">cores. A singlethreaded CPU program controls the device and performs arithmetic. The program scaled linearly to control four devices, so it is not a bottleneck. Wall clock time, except loading, is the minimum from three runs on an otherwise-idle machine. Models and input were in RAM before each run. 6.1 Language Identification The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the original Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) written in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs Lines Tokens Ken DA 1 core 5 cores 100 2.6 · 103 37.8 40.3 6.6 2.1 1000 2.2 · 104 42.4 43.6 16.2 10.7 10000 2.6 · 105 53.9 55.7 46.2 42.0 100000 2.8 · 106 78.6 85.3 91.3 93.6 305263 8.6 · 106 92.9 105.6 97.0 91.8 Table 3: Seconds to compute perplexity on strings. The hardware was tested with 1 core and 5 cores. of feature extraction and modeling. Table 2 reports speed measured on a 9.6 GB text file created by concatenating the 2013 News Crawl cor</context>
</contexts>
<marker>Sites, 2013</marker>
<rawString>Dick Sites. 2013. Compact language detection 2. https://code.google.com/p/cld2/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Herve Saint-Amand</author>
<author>Magdalena Plamada</author>
<author>Philipp Koehn</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Dirt cheap web-scale parallel text from the common crawl.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1884" citStr="Smith et al., 2013" startWordPosition="280" endWordPosition="283">ble with regular expressions and access to a stack. It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates f</context>
</contexts>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez. 2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="2027" citStr="Stolcke, 2002" startWordPosition="305" endWordPosition="306">ded, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been appli</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawid Weiss</author>
</authors>
<title>Java port of langid.py (language identifier).</title>
<date>2013</date>
<note>https://github.com/ carrotsearch/langid-java.</note>
<contexts>
<context position="9852" citStr="Weiss, 2013" startWordPosition="1617" endWordPosition="1618">T2540 PCI express device from 2011 against several CPU baselines. It has 2 GB of DDR2 RAM and 5 cores. A singlethreaded CPU program controls the device and performs arithmetic. The program scaled linearly to control four devices, so it is not a bottleneck. Wall clock time, except loading, is the minimum from three runs on an otherwise-idle machine. Models and input were in RAM before each run. 6.1 Language Identification The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the original Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) written in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs Lines Tokens Ken DA 1 core 5 cores 100 2.6 · 103 37.8 40.3 6.6 2.1 1000 2.2 · 104 42.4 43.6 16.2 10.7 10000 2.6 · 105 53.9 55.7 46.2 42.0 100000 2.8 · 106 78.6 85.3 91.3 93.6 305263 8.6 · 106 92.9 105.6 97.0 91.8 Table 3: Seconds to compute perplexity on strings. The hardware was tested with 1 core and 5 cores. of feature extraction and modeling. Table</context>
</contexts>
<marker>Weiss, 2013</marker>
<rawString>Dawid Weiss. 2013. Java port of langid.py (language identifier). https://github.com/ carrotsearch/langid-java.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Yasuhara</author>
<author>Toru Tanaka</author>
<author>Jun-ya Norimatsu</author>
<author>Mikio Yamamoto</author>
</authors>
<title>An efficient language model using double-array structures.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>222--232</pages>
<contexts>
<context position="2090" citStr="Yasuhara et al., 2013" startWordPosition="313" endWordPosition="316">et of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises a strong baseline (Stolcke, 2002; Federico et al., 2008; Heafield, 2011; Yasuhara et al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al.,</context>
<context position="3316" citStr="Yasuhara et al., 2013" startWordPosition="516" endWordPosition="519">1). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has traditional tries and a linear probing hash table. We use the fastest baselines from both. 3 Programming Model The fundamental programming unit is a POSIX regular expression including repetition, line boundaries, and trailing context. For example, a[bc] matches “ab” and “ac”. 384 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 384–389, Beijing, China, July 26-31, 20</context>
<context position="11047" citStr="Yasuhara et al., 2013" startWordPosition="1831" endWordPosition="1834">raction and modeling. Table 2 reports speed measured on a 9.6 GB text file created by concatenating the 2013 News Crawl corpora for English, French, German, Hindi, Spanish, and Russian (Bojar et al., 2014). One hardware core is 2.48 times as fast as the fastest CPU program. Using five cores instead of one yielded speed improvements of 3.8x on hardware and 4.3x on a 16-core CPU. The hardware performs decently on this task, likely because the 1 MB binary transition table mostly fits in cache. 6.2 Language Modeling We benchmarked against the fastest reported language models, DALM’s reverse trie (Yasuhara et al., 2013) and KenLM’s linear probing (Heafield, 2011). Both use stateful queries. For surface strings, time includes the cost of vocabulary lookup. For vocabulary identifiers, we converted words to bytes then timed custom query programs. Unpruned models were trained on the English side of the French–English MultiUN corpus (Eisele and Chen, 2010). Perplexity was computed on 2.6 GB of tokenized text from the 2013 English News Crawl (Bojar et al., 2014). 6.2.1 Surface Strings We tested trigram language models trained on various amounts of data before reaching a softwareimposed limit of 4.2 million regular</context>
</contexts>
<marker>Yasuhara, Tanaka, Norimatsu, Yamamoto, 2013</marker>
<rawString>Makoto Yasuhara, Toru Tanaka, Jun-ya Norimatsu, and Mikio Yamamoto. 2013. An efficient language model using double-array structures. In Proceedings of EMNLP, pages 222–232, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngmin Yi</author>
<author>Chao-Yue Lai</author>
<author>Slav Petrov</author>
<author>Kurt Keutzer</author>
</authors>
<title>Efficient parallel CKY parsing on GPUs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>175--185</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2696" citStr="Yi et al., 2011" startWordPosition="417" endWordPosition="420">al., 2013). In both cases, matches are frequent, which differs from network security and forensics applications where matches are rare. 2 Related Work Automata have been emulated on CPUs with AT&amp;T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudomin et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Ya</context>
</contexts>
<marker>Yi, Lai, Petrov, Keutzer, 2011</marker>
<rawString>Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt Keutzer. 2011. Efficient parallel CKY parsing on GPUs. In Proceedings of the 12th International Conference on Parsing Technologies, pages 175– 185. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>