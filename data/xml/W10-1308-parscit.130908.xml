<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000342">
<title confidence="0.999817">
A Multimodal Vocabulary for Augmentative and Alternative Communi-
cation from Sound/Image Label Datasets
</title>
<author confidence="0.973291">
Xiaojuan Ma Christiane Fellbaum Perry R. Cook
</author>
<affiliation confidence="0.964884">
Princeton University
</affiliation>
<address confidence="0.601885">
35 Olden St. Princeton, NJ 08544, USA
</address>
<email confidence="0.997396">
{xm,fellbaum,prc}@princeton.edu
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999125851851852">
Existing Augmentative and Alternative Com-
munication vocabularies assign multimodal
stimuli to words with multiple meanings. The
ambiguity hampers the vocabulary effective-
ness when used by people with language dis-
abilities. For example, the noun &amp;quot;a missing
letter&amp;quot; may refer to a character or a written
message, and each corresponds to a different
picture. A vocabulary with images and sounds
unambiguously linked to words can better
eliminate misunderstanding and assist com-
munication for people with language disorders.
We explore a new approach of creating such a
vocabulary via automatically assigning se-
mantically unambiguous groups of synonyms
to sound and image labels. We propose an un-
supervised word sense disambiguation (WSD)
voting algorithm, which combines different
semantic relatedness measures. Our voting al-
gorithm achieved over 80% accuracy with a
sound label dataset, which significantly out-
performs WSD with individual measures. We
also explore the use of human judgments of
evocation between members of concept pairs,
in the label disambiguation task. Results show
that evocation achieves similar performance to
most of the existing relatedness measures.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857392156863">
In natural languages, a word form may refer to dif-
ferent meanings. For instance, the word &amp;quot;fly&amp;quot;
means &amp;quot;travel through the air&amp;quot; in context like &amp;quot;fly
to New York,&amp;quot; while it refers to an insect in the
phrase &amp;quot;a fly on the trashcan.&amp;quot; Speakers determine
the appropriate sense of a polysemous word based
on the context. However, people with language
disorders and access/retrieval problems, may have
great difficulty in understanding words individual-
ly or in a context. To overcome such language bar-
riers, visual and auditory representations are intro-
duced to help illustrate concepts (Ma et al.,
2009a)(Ma et al., 2010). For example, a person
with a language disability can tell the word &amp;quot;fly&amp;quot;
refers to &amp;quot;travel through the air&amp;quot; when he sees a
plane in the image (rather than an insect); likewise
he can distinguish the meaning of &amp;quot;fly&amp;quot; given the
plane engine sound vs. the insect buzzing sound.
This approach has been employed in Augmentative
and Alternative Communication (AAC), in the
form of multimodal vocabularies in assistive de-
vices (Steele et al. 1989)(Lingraphica, 2010).
However, current AAC vocabularies assign vis-
ual stimuli to words instead of specific meanings,
and thus bring in ambiguity when a user with lan-
guage disability tries to comprehend and commu-
nicate a concept. For example, for the word &amp;quot;fly,&amp;quot;
Lingraphica only has an icon showing a plane and
a flock of birds flying. Confusion arises when a
sentence like &amp;quot;I want to kill the fly (the insect)&amp;quot; is
explained using the airplane/bird icon. Similarly, it
will lead to miscommunication if the sound of keys
jingling is used to express &amp;quot;a key is missing&amp;quot; when
the person intends to refer to a key on the keyboard.
People with language impairment are relying on
the AAC vocabularies for language access, and any
ambiguity may result in communication failure.
To address this problem, we propose building a
semantic multimodal AAC vocabulary with visual
and auditory representations expressing concepts
rather than words (Figure 1), as the backbone of
the language assistant system for people with
aphasia (Ma et al. 2009b). Our work is exploratory
with the following innovations: 1) we target the
insufficiency of current assistive vocabularies by
resolving ambiguity; 2) we enrich concept invento-
ry and connect concepts through language, envi-
ronmental sounds, and images (little research has
looked into conveying concepts through natural
nonspeech sounds); and 3) our vocabulary has a
dynamic scalable semantic network structure rather
</bodyText>
<page confidence="0.990876">
62
</page>
<note confidence="0.9964615">
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 62–70,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999801">
Figure 1. Disambiguated AAC multimedia vocabulary; dash arrows are semantic relations between concepts.
</figureCaption>
<bodyText confidence="0.999358818181818">
than simply grouping words into categories as
conventional assistive devices do.
One intuitive way to build a disambiguated mul-
timodal vocabulary is to manually assign meanings
to each word in the existing vocabulary. However,
the task is time consuming with poor scalability —
no new multimedia representations are generated
for concepts that are missing in the vocabulary.
ImageNet (Jia et al., 2009) was constructed by
people verifying the assignment of web images to
given synonym sets (synsets). ImageNet has over
nine million images linked to about 15 thousands
noun synsets in WordNet (Fellbaum, 1998). De-
spite the huge human effort, ImageNet, with the
goal of creating a computer vision database, does
not yet include all the most commonly used words
across different parts of speech. It is not yet suita-
ble for a language support application.
We explore a new approach for generating a vo-
cabulary with concept to sound/image associations,
that is, conducting word sense disambiguation
(WSD) techniques used in Natural Language
Processing on sound/image label datasets. For ex-
ample, the labels &amp;quot;car, drive, fast&amp;quot; for the sound
&amp;quot;car — passing.wav&amp;quot; are assigned to synsets &amp;quot;car: a
motor vehicle,&amp;quot; &amp;quot;drive: operate or control a ve-
hicle,&amp;quot; and &amp;quot;fast: quickly or rapidly&amp;quot; via WSD. It
means the sound &amp;quot;car — passing.wav&amp;quot; can be used
to depict those concepts. This approach is viable
because the words in the sound/image labels were
shown to evoke one another based on the audito-
ry/visual content, and their meanings can be identi-
fied by considering all the tags generated for a
given sound or image as a context. With the avail-
ability of large sound/image label datasets, the vo-
cabulary created from WSD can be easily
expanded.
A variety of WSD methods (e.g. knowledge-
based methods (Lesk, 1986), unsupervised me-
thods (Lin, 1997), semi-supervised methods
(Hearst, 1991) (Yarowsky, 1995), and supervised
methods (Novischi et al., 2007)) were developed
and evaluated with corpus data and other text doc-
uments like webpages. Compared to the text data
that WSD methods work with, labels for sounds
and images have unique characteristics. The labels
are a bag of words related to the visual/auditory
content; there is no syntactic or part of speech in-
formation, nor are the words necessarily contextual
neighbors. For example, contexts suggest land-
scape senses for the word pair &amp;quot;bank&amp;quot; and &amp;quot;water&amp;quot;,
whereas in an image, a person may drink water
inside a bank building. Furthermore, few annotated
image or sound label datasets are available, making
it hard to apply supervised or semi-supervised
WSD methods.
To efficiently and effectively create a disambi-
guated multimodal vocabulary, we need to achieve
two goals. First, optimize the accuracy of the WSD
algorithm to minimize the work required for ma-
nual checking and correction afterwards. Second,
construct a semantic network across different parts
of speech, and thus explore linking semantic rela-
tedness measures that can capture aspects different
from existing ones. In this paper, we target the first
goal by proposing an unsupervised sense disam-
</bodyText>
<page confidence="0.999013">
63
</page>
<bodyText confidence="0.999816086956522">
biguation algorithm combining a variety of seman-
tic relatedness measures. We chose an unsuper-
vised method because of the lack of a large
manually annotated gold standard. The measure-
combined voting algorithm presented here draws
advantages from different semantic relatedness
measures and has them vote for the best-fitting
sense to assign to a label. Evaluation shows that
the voting algorithm significantly exceeds WSD
with each individual measure.
To approach the second goal, we proposed and
tested a semantic relatedness measure called evo-
cation (Boyd-Graber et al., 2006) in disambigua-
tion of sound/image labels. Evocation measures
human judgements of relatedness between a di-
rected concepts pair. It provides cross parts of
speech evocativeness information which supple-
ments most of the knowledge-based semantic rela-
tedness measures. Evaluation results showed that
the performance of WSD with evocation is no
worse than most of the relatedness measures that
we applied, despite the relatively small size of the
current evocation dataset.
</bodyText>
<sectionHeader confidence="0.914151" genericHeader="method">
2 Dataset: Semantic Labels for Environ-
</sectionHeader>
<subsectionHeader confidence="0.58949">
mental Sounds and Images
</subsectionHeader>
<bodyText confidence="0.9998895">
Our ultimate goal is to create an AAC vocabulary
of associations between environmental sounds and
images and groups of synonymous words that are
relevant to the content. We are working with two
datasets of human labels for multimedia data,
SoundNet and the Peekaboom dataset.
</bodyText>
<subsectionHeader confidence="0.993671">
2.1 SoundNet Sound Label Dataset
</subsectionHeader>
<bodyText confidence="0.999963">
The SoundNet Dataset (Ma, Fellbaum, and Cook,
2009) consists of 327 environmental &amp;quot;soundnails&amp;quot;
(5-second audio clips) each with semantic labels
collected from participants via a large scale Ama-
zon Mechanical Turk (AMT) study. The sound-
nails cover a wide range of auditory scenes, from
vehicle (e.g. car starting), mechanical tools (e.g.
handsaw) and electrical devices (e.g. TV), to natu-
ral phenomena (e.g. rain), animals (e.g. a dog bark-
ing), and human sounds (e.g. a baby crying). In the
AMT study, participants were asked to generate
tags for each soundnail labeling its source, possible
location, and actions involved in making the sound.
Each soundnail was labeled by over 100 people.
The tags were clustered into meaning units that
SoundNet refers to as &amp;quot;sense sets.&amp;quot; A sense set in-
cludes a set of words with similar meanings. For
instance, for the soundnail pre-labeled &amp;quot;bag, zipO-
pen&amp;quot; which is the sound of opening the zipper of a
bag, the following sense sets were generated:
</bodyText>
<listItem confidence="0.962598666666667">
(a) &amp;quot;zipper&amp;quot; {zipper, zip up, zip, unzip};
(b) &amp;quot;bag&amp;quot; {bag, duffle bag, nylon bag, suitcase,
luggage, backpack, purse, pack, briefcase};
(c) &amp;quot;house&amp;quot; {house, home, building}, and
(d) &amp;quot;clothes&amp;quot; {clothes, jacket, coat, pants, jeans,
dress, garment}.
</listItem>
<bodyText confidence="0.999971789473684">
The word in bold is was judged by SoundNet to
be the best representative of the sense set, and oth-
er words, possibly belonging to different parts of
speech are included in the curly brackets enclosing
the sense sets. SoundNet uses sense sets rather than
single words because 1) people may use different
words to describe the same underlying concept,
(e.g. &amp;quot;baby&amp;quot; and &amp;quot;infant;&amp;quot; &amp;quot;rain&amp;quot; as a noun and as a
verb); 2) people cannot draw fine distinctions be-
tween objects and events that generate similar
sounds, and thus may come up with different but
related categories (e.g. &amp;quot;plate,&amp;quot; &amp;quot;cup,&amp;quot; and &amp;quot;bowl&amp;quot;
for the dish clinking sound); and 3) people may
perceive objects and events that are not explicitly
presented in the sound very differently (e.g. &amp;quot;bag&amp;quot;
vs. &amp;quot;clothes&amp;quot; for the sound made by a zipper). In
this experiment, only sense sets (labels) that were
generated by at least 25% of the labelers were
used.
In our disambiguation experiment, two kinds of
contexts were explored. In the Context 1 scheme,
each label is treated separately: all its members
plus the representatives of the other sense sets are
considered. Take the soundnail &amp;quot;bag, zipOpen&amp;quot; as
an example. The context for disambiguating label
(a) &amp;quot;zipper&amp;quot; {zipper, zip up, zip, unzip} is:
zipper, zip up, zip, unzip, bag, house, clothes.
The context for label (d) &amp;quot;clothes&amp;quot; {clothes, jacket,
coat, pants, jeans, dress, garment} is:
clothes, jacket, coat, pants, jeans, dress, garment,
zipper, bag, house.
In the Context 1 scheme, all representative
words will be disambiguated multiple times. The
final result will be the synset that gets the most
votes. In the Context 2 scheme, as for the image
dataset described below, all members from each
sense set are put together to create the context, and
each word is disambiguated only once.
</bodyText>
<subsectionHeader confidence="0.92954">
2.2 Peekaboom Image Label Dataset
</subsectionHeader>
<page confidence="0.979577">
64
</page>
<bodyText confidence="0.999966192307692">
The ESP Game Dataset (Von Ahn and Dabbish,
2004) contains a large number of web images and
human labels produced via an online game. For
example, an image of a glass of hard liquor is la-
beled &amp;quot;full, shot, alcohol, clear, drink, glass, beve-
rage.&amp;quot; The Peekaboom Game (Von Ahn et al.,
2006) is the successor of the ESP Game. In our
experiment, part of the Peekaboom Dataset (3,086
images) was used. For each image, all the labels
together form the context for sense disambigua-
tion.
The Peekaboom labels are noisier than the
SoundNet labels for several reasons. First, random
objects may appear in a picture and thus be in-
cluded in the labels. For example, an image is la-
beled &amp;quot;computer, shark&amp;quot; because there is a shark
picture on the computer screen. Second, texts in
the images are often included in the labels. For
example, the word &amp;quot;green&amp;quot; is one of the labels for
an image with a street sign &amp;quot;Green St.&amp;quot; Third, the
Peekaboom labels are not stemmed, which adds
another layer of ambiguity. For example, the labels
&amp;quot;bridge, building&amp;quot; could refer to a building event
or to a built entity. In the experiment, all labels for
an image are used in their unstemmed form to con-
struct the context for WSD.
</bodyText>
<sectionHeader confidence="0.7604445" genericHeader="method">
3 Evocation and .ther Semantic Related-
ness Measures
</sectionHeader>
<bodyText confidence="0.99998275">
A set of measures were selected to assess the rela-
tedness between possible senses of words in the
sound/image labels. Apart from existing methods,
an additional measure, evocation, is introduced.
</bodyText>
<subsectionHeader confidence="0.988377">
3.1 Evocation
</subsectionHeader>
<bodyText confidence="0.99997296969697">
Evocation (Boyd-Graber et al., 2006) measures
concept similarity based on human judgment. It is
a directed measure, with evocation(synset A, syn-
set B) defined as how much synset A brings to
mind synset B. The evocation dataset has been ex-
tended to scores for 100,000 directed synset pairs
(Nikolova et al., 2009).
The evocation data were collected independently
of WordNet or corpus data. We propose the use of
evocation in WSD for image and sound labels for
the following reasons. First, the sound and image
labels are generated based on human perception of
the content and common knowledge. In SoundNet
in particular, many of the evoked labels reflected
the most obvious objects or events in a sound
scene. For example, &amp;quot;bag&amp;quot; and &amp;quot;coat&amp;quot; were evoked
from the zipper soundnail. In this case, the evoca-
tion score may be a good evaluation of the related-
ness between the labels. Second, evocation
assesses relatedness of concepts across different
parts of speech, which is suitable for identifying
image and sound labels containing nouns, verbs,
adjectives, adverbs, etc.
This paper is a first attempt to compare the ef-
fectiveness of the use of evocation measure in
sense disambiguation to the conventional, relative-
ly better tested similarity measures, in the context
of assigning synsets to sound/image labels. Consi-
dering that the evocation dataset is small in size
and susceptible to noise given the method by
which it was collected, we have not yet incorpo-
rated evocation into the measure-combined voting
algorithm described in the Section 4.
</bodyText>
<subsectionHeader confidence="0.998145">
3.2 Semantic Relatedness Measures
</subsectionHeader>
<bodyText confidence="0.988588">
Nine measures of semantic relatedness1 between
synsets are used in the experiment, both as contri-
butors to the voting algorithm and as baselines for
comparison, including:
</bodyText>
<listItem confidence="0.996612142857143">
1) WordNet path based measures.
• &amp;quot;path&amp;quot; — shortest path length between syn-
sets, inversely proportional to the number
of nodes on the path.
• &amp;quot;wup&amp;quot; (Wu and Palmer, 1994) — ratio of the
depth of the Least Common Subsumer
(LCS) to the depths of two synsets in the
Wordnet taxonomy.
• &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) —
considering the length of the shortest path
between two synsets to the depth of the
WordNet taxonomy.
2) Information and content based measures.
• &amp;quot;res&amp;quot; (Resnik, 1995) — the informational
content (IC) of a given corpus of the LCS
between two synsets.
• &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the
LCS to the IC of the two synsets.
• &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely
proportional to the difference between the
IC of the two synsets and the IC of the LCS.
</listItem>
<footnote confidence="0.874449">
1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the
WSD process with over five context words, and thus, is not
included in the experiment.
</footnote>
<page confidence="0.995611">
65
</page>
<listItem confidence="0.997945875">
3) WordNet definition based measures.
• &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) —
overlaps in the definitions of two synsets.
• &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006)
— cosine of the angle between the co-
occurrence vector computed from the defi-
nitions around the two synsets.
• &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are
</listItem>
<bodyText confidence="0.979351705882353">
computed from definition pairs separately.
The computation of the relatedness scores using
measures listed above were carried out by codes
from the WordNet::Similarity (Pedersen et al.,
2004) and WordNet::SenseRelate projects (Peder-
sen and Kolhatkar, 2009). In contrast to Word-
Net::SenseRelated, which employs only one
similarity measure in the WSD process, this paper
proposes a strategy of having several semantic re-
latedness measures vote for the best synset for each
word. The voting algorithm intends to improve
WSD performance by combining conclusions from
various measures to eliminate a false result. Since
there is no syntax among the words generated for a
sound/image, they should all be considered for
WSD. Thus, the width of the context window is the
total number of words in the context.
</bodyText>
<sectionHeader confidence="0.949273" genericHeader="method">
4 Label Sense Disambiguation Algorithm
</sectionHeader>
<figureCaption confidence="0.999655">
Figure 2. Measure-Combined Voting Algorithm.
</figureCaption>
<bodyText confidence="0.9871113">
Figure 2 shows the overall process of the measure-
combined voting algorithm for disambiguating
sound/image labels. After the context for WSD is
generated, the process is divided into two steps. In
Step I, the relatedness scores of each sense of a
word based on the context is computed by each
measure separately. Step II combines results from
all measures and generates the disambiguated syn-
sets for all words in the sound/image labels. Evo-
cation did not participate in Step II.
</bodyText>
<subsectionHeader confidence="0.5935645">
4.1 Step I: Generate Candidate Synsets Based
on Individual Measures
</subsectionHeader>
<bodyText confidence="0.999969428571428">
Given the context of M words (w1, ..., wM), and K
relatedness measures (k = 1, ..., K), the task is to
assign each word wj (j = 1, ..., M) to the synset
sx,wj that is the most appropriate within the context.
Here, the word wj has Nj synsets, denoted as sn,wj (n
= 1, ..., Nj). Step I is to calculate the relatedness
score for each synset of each word in the context.
</bodyText>
<equation confidence="0.998608125">
� �
≠
(si w,) = max ( measurek(si,w � sn,w, ))
=
� �
1,..., �
=
� �
</equation>
<bodyText confidence="0.9891335">
The evocation score between two sysnets sa, sb is
the maximum of the directed evocation ratings.
</bodyText>
<equation confidence="0.996040857142857">
(sa, sb) = max(evocation(sa , sb ), evocation(sb, sa ))
� �
≠
(si wj) = ∑ max (score(si,w. , sn m ))
n=1,...,Nm evocation
=
� �
</equation>
<bodyText confidence="0.9907595">
The synset that evocation assigns to word j is the
one with the highest score.
</bodyText>
<equation confidence="0.855182">
S = S if score(s. w) = max (score(si,W ))
wj x,wj , evocation,i i=1,...,Nj evocation J
</equation>
<subsectionHeader confidence="0.908603">
4.2 Step II: Vote for the Best Candidate
</subsectionHeader>
<bodyText confidence="0.997045">
Three voting schemes were tested, including un-
weighted simple votes, weighted votes among top
candidates, and weighted votes among all synsets.
1) Unweighted Simple Votes
Synset sn,wj of word wj gets a vote from related-
ness measure k if its scorek is the maximum among
all the synsets for wj, and it becomes the candidate
synset for wj elected by measure k (Ck,wj):
</bodyText>
<equation confidence="0.9908405">
1, if scorek (sx , w, ) = max (scorek (Si w ))
k
else
candidate, (s, ) = sx, , if votek (sx, ) = 1
</equation>
<bodyText confidence="0.996243285714286">
The candidate list for word wj (candidates(Swj))
is the union of all candidate synsets elected by in-
dividual relatedness measures.
candidates(swj ) = union(candidatek (swj ))
For each candidate in the list, the votes from all
measures are calculated. The one receiving the
most votes becomes the proposed synset for wj.
</bodyText>
<equation confidence="0.585702333333333">
K
voteCount (si,wj ) = ∑ votek (si,wj )
k=1
scorek
score
evocation
score
evocation
vote
 


(sx,w ) �
0,
i=1 711.
</equation>
<page confidence="0.782501">
66
</page>
<bodyText confidence="0.893465333333333">
The evaluation of WSD with evocation and the
measure-combined voting algorithm was carried
out primarily on the SoundNet label dataset be-
</bodyText>
<figureCaption confidence="0.9894145">
Figure 3. Accuracy rate at word and sound level in comparison among evocation, voting, and nine individual
sense similarity measures.
</figureCaption>
<figure confidence="0.94510725">
sw, = Sx,wi , i
voteCount
(sx ,w.) = max (voteCount(si
�
J
E s &apos;Candidates(s,,,J,)
J„,,
2) Weighted Votes among Top Candidates
</figure>
<bodyText confidence="0.9998515">
The weighted voting scheme avoids a situation
where the false results win by a very small margin.
The weight under relatedness measure k for si,wj is
calculated as the relative score to the maximum
scorek among all synsets for word wj. It suggests
how big of a difference in relatedness score of any
given synset is to the highest score among all the
possible synsets for the target word.
</bodyText>
<equation confidence="0.964945">
weightk (Sx , ,�) = scorek (Sx , ,�) l max (scorek (Si , , ))
i=1,...,Nj,J
</equation>
<bodyText confidence="0.9998052">
The weighted votes synset si,wj receives over all
measures is the sum of its weight under individual
measure. In voting scheme 2, the synset from the
candidate list which gets the highest weighted
votes becomes the winner.
</bodyText>
<equation confidence="0.987833375">
K
weightedVote(si w. ) = ∑ weightk (si w. )
J
1
k=
SwJ =sxwJ,IJ
weighted iVote(sx w.) = max ( weightedVote(s;,w. ))
,
</equation>
<listItem confidence="0.501968">
3) Weighted Votes among All Synsets
</listItem>
<bodyText confidence="0.967107666666667">
Voting scheme 3 differs from 2 in that the synset
from all synsets for word wj which gets the highest
weighted votes is the proposed synset for wj.
</bodyText>
<equation confidence="0.99064025">
sw; = sx,w; , if
weightedVote(sx , J w) = max (weightedVote(si w. ))
J
i=1 V.
</equation>
<bodyText confidence="0.999984272727273">
cause of the availability of ground truth data.
SoundNet provides manual annotation for 1,553
different words for 327 soundnails (e.g. the word
&amp;quot;road&amp;quot; appears in 41 sounds).
The accuracy rate (precision) was computed for
each WSD method. The sound level accuracy of a
WSDk is the average percentage of correct sense
assignments over the 327 sounds. The word level
accuracy is the mean over 1553 distinctive words.
Accuracy rates of different measures at both level
accepted the null hypothesis in homogeneity test.
</bodyText>
<equation confidence="0.977449375">
327
(WSD,) = (∑ (%correctness);) l 327
����� �����
− i=1
1553
���� �����
− w=1
(WSD,) = (∑ (%correctness)w) /1553
</equation>
<bodyText confidence="0.99998325">
Due to the lack of ground truth in the Peekaboom
dataset, we only computed the overlap between the
WSD result of 3,086 images from the voting algo-
rithm, evocation and each relatedness measures.
</bodyText>
<subsectionHeader confidence="0.671618">
5.1 .verall Comparison across WSD me-
thods with Various Relatedness Measures
</subsectionHeader>
<sectionHeader confidence="0.952358" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999720375">
Figures 3 show the overall comparison among dif-
ferent methods at both sound level and word level.
It suggests that the performance of the evocation
measure in sense disambiguation is as good as the
path-based and context-based measures. The defi-
nition-based measures (&amp;quot;lesk&amp;quot; and &amp;quot;vector&amp;quot;) are
significantly better than other measures if used in-
dividually (similar to (Patwardhan et al.2003)).
</bodyText>
<figure confidence="0.92305675">
J s,,,,j∈can&amp;dntes(s,,j)
J
accuracy
accuracy
</figure>
<page confidence="0.993152">
67
</page>
<bodyText confidence="0.9823775">
However, the voting algorithms proposed in this
work significantly outperformed each individual
</bodyText>
<note confidence="0.584687">
5.2 Performance of the Voting Algorithm
</note>
<figureCaption confidence="0.9695906">
Figure 4 shows the histogram (distribution) for the
Figure 4. Histogram of accuracy rate at sound (327, left) and word level (1553, right) among different measures,
contexts, and voting schemes. EVC1 = Evocation (Context 1); SR11 = Voting (Context 1, voting scheme 1).
Figure 5. Percentage of sense disambiguation results overlap between voting algorithm, evocation, and individ-
ual sense relatedness measures at image (3,086 images) and sound (327 sounds) level.
measure based on ANOVA results. At sound level,
Context 1: (F(12, 20176) = 102.92, p &lt; 0.001);
Context 2: (F(12, 4238) = 89.42, p &lt; 0.001). At
word level, Context 1: (F(12, 20176) = 68.78, p &lt;
0.001); Context 2: (F(12, 4238) = 60.72, p &lt; 0.001).
</figureCaption>
<bodyText confidence="0.999964518518518">
The scheme of composing context (Section 2.1)
has significant impact on the accuracy, with Con-
text 1 (taking all members in the related sense set
and representatives from the others) outperforming
Context 2 (taking all words in all sense sets) at the
word level (F(1, 40352) = 20.19, p &lt; 0.001). The
influence of context scheme is not significant at the
sound level (F(1, 8476) = 0.35, p = 0.5546). The
interaction between measures and context schemes
is not significant, indicating that accuracy differ-
ences are similar regardless of context construction.
accuracy rate at sound and word levels. We see
that for the voting algorithm, the accuracy rates are
greater than 0.7 for most of the sounds, and greater
than 0.9 for majority of the words to disambiguate.
Figure 5 show the percentage of sense disam-
biguation results overlapping between voting algo-
rithm and individual relatedness measures. Note
that any two methods may come up with different
correct results (e.g. &amp;quot;lesk&amp;quot; assigned &amp;quot;chirp&amp;quot; as &amp;quot;a
sharp sound&amp;quot; while the voting algorithm assigned
&amp;quot;chirp&amp;quot; as &amp;quot;making a sharp sound&amp;quot;). This indicates
the change of the contribution of each relatedness
measures in different voting schemes. In the simple
voting scheme, more disambiguation results came
from the &amp;quot;path,&amp;quot; &amp;quot;wup,&amp;quot; and &amp;quot;lch&amp;quot; (the WordNet
path based measures), while the weighted voting
</bodyText>
<page confidence="0.997925">
68
</page>
<bodyText confidence="0.999576875">
scheme took more of the recommendations from
&amp;quot;lesk,&amp;quot; &amp;quot;lin,&amp;quot; and &amp;quot;jcn&amp;quot; (context and definition
based measures) into consideration. At the sound
level, there is no significant accuracy difference
measure may be closer to the definition-based
measures than path and content based measures.
For the SoundNet dataset, 34% to 44% of evoca-
tion WSD results overlap with that of other meas-
</bodyText>
<figureCaption confidence="0.969377">
Figure 6. Percentage of WSD results overlap between evocation and various relatedness measures.
</figureCaption>
<bodyText confidence="0.999992375">
among the three voting schemes, and the influence
of the context composition is similar. However, at
the word level (Figure 3), the weighted voting
schemes significantly outperformed the simple vot-
ing scheme (F(2, 9312) = 5.20, p = 0.0055), and all
of them have significantly better accuracy when
the context contains mainly members from the
same sense set (F(1, 9312) = 4.79, p = 0.0287).
</bodyText>
<subsectionHeader confidence="0.997804">
5.3 Performance of WSD with Evocation
</subsectionHeader>
<bodyText confidence="0.999980259259259">
As shown in Figures 3, the performance of the
evocation measure is not significantly different
from path-based and some context-based measures
at sound level, including &amp;quot;path,&amp;quot; &amp;quot;wup,&amp;quot; &amp;quot;lch,&amp;quot;
&amp;quot;res,&amp;quot; &amp;quot;lin,&amp;quot; and &amp;quot;jcn&amp;quot; (for Context 1, F(6, 2282) =
2.0582, p = 0.0551; for Context 2, F(6, 2282) =
1.6679, p = 0.1249); and is significantly better than
the vector�pairs measure (for Context 1, F(1, 652)
= 61.37, p &lt; 0.001; for Context 2, F(1, 652) =
36.47, p &lt; 0.001). At the word level, the perfor-
mance of the evocation measure is not significantly
different from that of measures including &amp;quot;path,&amp;quot;
&amp;quot;wup,&amp;quot; &amp;quot;lch,&amp;quot; &amp;quot;res&amp;quot; (F(4, 7760) = 0.39, p = 0.8135),
and &amp;quot;lin,&amp;quot; &amp;quot;jcn,&amp;quot; and &amp;quot;vector�pairs&amp;quot; (F(3, 6208) =
1.52, p = 0.2077). Figure 8 (SoundNet) and Figure
9 (Peekaboom) show the percentage of synset as-
signment overlap between evocation and the other
nine relatedness measures. The overlap with &amp;quot;lesk&amp;quot;
and &amp;quot;vector&amp;quot; are significantly higher than that with
the other measures (F(8, 5877) = 34.67, p &lt; 0.001).
It suggests that evocation as a semantic relatedness
ures; for the Peekaboom dataset, the overlap is
25% to 35% (Figure 6). Given that evocation per-
formed similarly in accuracy to most of other
measures with relatively low overlap in WSD re-
sults, evocation may capture different aspects of
semantic relatedness from existing measures.
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999994857142857">
We explored the construction of a sense disambi-
guated semantic AAC multimodal vocabulary from
sound/image label datasets. Two WSD approaches
are introduced to assign specific meanings to envi-
ronmental sound and image labels, and further
create concept-sound/image associations. The
measure-combined voting algorithm targets the
accuracy of WSD and achieves significantly better
performance than each relatedness measure indivi-
dually. Our second approach applies a new rela-
tedness measure, evocation. Evocation achieves
similar performance to most of the existing rela-
tedness measures with sound labels. Results sug-
gest that evocation provides different semantic
information from current measures.
Future work includes: 1) expanding the evoca-
tion dataset and investigating the potential im-
provement in its WSD accuracy; 2) incorporating
the extended evocation dataset into the voting al-
gorithm; 3) exploring additional information such
as image and sound similarity to help with WSD.
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<page confidence="0.997871">
69
</page>
<bodyText confidence="0.999182333333333">
We thank the Kimberley and Frank H. Moss &apos;71
Princeton SEAS Research Fund for supporting our
project.
</bodyText>
<sectionHeader confidence="0.989878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997372018348624">
Satanjeev Banerjee and Ted Pedersen. 2002. An
Adapted Lesk Algorithm for Word Sense Disambig-
uation Using WordNet. 3rd
Proceedings of the Inter-
national Conference on Intelligent Text Processing
and Computational Linguistics.
Jordan Boyd-Graber, Christaine Fellbaum, Daniel
Osherson, and Robert Schapire. 2006. Adding Dense,
Weighted Connections to WordNet. Proceedings of
the Thirds International WordNet Conference.
Jia Deng, Wei Dong, Richard Socher, Li -J. Li, Kai Li
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hie-
rarchical Image Database. Proceedings of the IEEE
Computer Vision and Pattern Recognition (CVPR).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Marti Hearst. 1991. Noun Homograph Disambiguation
Using Local Context in Large Text Corpora. Proc. of
7th
the Annual Conference of the University of Water-
loo Center for the New OED and Text Research.
Graeme Hirst and David St. Onge. 1998. Lexical Chains
as Representations of Context for the Detection and
Correction of Malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database.
Jay Jiang and David Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy.
Proceedings on International Conference on Re-
search in Computational Linguistics.
Claudia Leacock and Martin Chodorow. 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification. In Christiane Fellbaum, editor,
WordNet: An Electronic Lexical Database.
Michael Lesk. 1986. Automatic Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. Proceedings of
SIGDOC&apos;86.
Dekang Lin. 1997. Using Syntactic Dependency as a
Local Context to Resolve Word Sense Ambiguity.
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 64-71.
Lingraphica. http://www.aphasia.com/. 2010.
Xiaojuan Ma, Christiane Fellbaum. and Perry Cook.
2010. SoundNet: Investigating a Language Com-
posed of Environmental Sounds. In Proc. CHI 2010.
Xiaojuan Ma, Jordan Boy-Graber, Sonya Nikolova, and
Perry Cook. 2009a. Speaking Through Pictures: Im-
ages vs. Icons. Proceedings ofASSETS09.
Xiaojuan Ma, Sonya Nikolova and Perry Cook. 2009b.
W2ANE: When Words Are Not Enough - Online
Multimedia Language Assistant for People with
Aphasia. Proceedings ofACM Multimedia 2009.
Sonya Nikolova, Jordan Boyd-Graber, and Christiane
Fellbaum. 2009. Collecting Semantic Similarity Rat-
ings to Connect Concepts in Assistive Communica-
tion Tools (in press). Modelling, Learning and
Processing of Text-Technological Data Structures,
Springer Studies in Computational Intelligence.
Adrian Novischi, Muirathnam Srikanth, and Andrew
Bennett. 2007. Lcc-wsd: System Description for
English Coarse Grained All Words Task at SemEval
2007. 4th
Proceedings of the International Workshop
on Semantic Evaluations(SemEval-2007), pp 223-226.
Siddharth Patwardhan, Satanjeev Benerjee and Ted Pe-
dersen. Using Measures of Semantic Relatedness for
Word Sense Disambiguation. 2003. Proceeding of
CICLing2003, pp. 241-257.
Siddharth Patwardhan and Ted Pedersen Using Word-
Net Based Context Vectors to Estimate the Semantic
Relatedness of Concepts. 2006. Proceedings of the
EACL 2006 Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pp. 1-8
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WorNet::Similarity - Measuring the Re-
latedness of Concepts. Proceedings of Human
Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics Demonstrations, pp. 38-41.
Ted Pedersen and Varada Kolhatkar. 2009. Word-
Net::SenseRelate::AllWords - A Broad Coverage
Word Sense Tagger that Maximimizes Semantic Re-
latedness. Proceedings of Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics Dem-
onstrations, pp. 17-20.
Philip Resnik. 1995. Using Information Content to Eva-
luate Semantic Similarity in a Taxonomy. Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence.
Richard Steele, Michael Weinrich, Robert Wertz, Gloria
Carlson, and Maria Kleczewska. Computer-based
visual communication in aphasia. Neuropsychologia.
27(4): pp 409-26. 1989.
Luis von Ahn, Laura Dabbish. 2004. Labeling images
with a computer game. Proceedings of the SIGCHI
conference on Human factors in computing systems,
p.319-326.
Luis von Ahn, Ruoran Liu, Manuel Blum. 2006 Peeka-
boom: a game for locating objects in images. Pro-
ceedings of the SIGCHI conference on Human
Factors in computing systems.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. Proc. ofACL, pp 133-138.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Proceed-
ings of the 33rd Annual Meeting on Association For
Computational Linguistics.
</reference>
<page confidence="0.998475">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984437">
<title confidence="0.998645">A Multimodal Vocabulary for Augmentative and Alternative Communication from Sound/Image Label Datasets</title>
<author confidence="0.999799">Xiaojuan Ma Christiane Fellbaum Perry R Cook</author>
<affiliation confidence="0.999996">Princeton University</affiliation>
<address confidence="0.99757">35 Olden St. Princeton, NJ 08544,</address>
<email confidence="0.999846">xm@princeton.edu</email>
<email confidence="0.999846">fellbaum@princeton.edu</email>
<email confidence="0.999846">prc@princeton.edu</email>
<abstract confidence="0.999623214285714">Existing Augmentative and Alternative Communication vocabularies assign multimodal stimuli to words with multiple meanings. The ambiguity hampers the vocabulary effectiveness when used by people with language disabilities. For example, the noun &amp;quot;a missing letter&amp;quot; may refer to a character or a written message, and each corresponds to a different picture. A vocabulary with images and sounds unambiguously linked to words can better eliminate misunderstanding and assist communication for people with language disorders. We explore a new approach of creating such a vocabulary via automatically assigning semantically unambiguous groups of synonyms to sound and image labels. We propose an unsupervised word sense disambiguation (WSD) voting algorithm, which combines different semantic relatedness measures. Our voting algorithm achieved over 80% accuracy with a sound label dataset, which significantly outperforms WSD with individual measures. We also explore the use of human judgments of evocation between members of concept pairs, in the label disambiguation task. Results show that evocation achieves similar performance to most of the existing relatedness measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2002</date>
<booktitle>3rd Proceedings of the International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="16055" citStr="Banerjee and Pedersen, 2002" startWordPosition="2572" endWordPosition="2575">the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions around the two synsets. • &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are computed from definition pairs separately. The computation of the relatedness scores using measures listed above were carried out by codes from the WordNet::Similarity (Pedersen et al., 2004) and WordNet::SenseRelate projects (Pedersen and Kolhatkar, 2009). In contrast to WordNet::SenseRelated, which employs only one similarity measure in the WSD process,</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. 3rd Proceedings of the International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Christaine Fellbaum</author>
<author>Daniel Osherson</author>
<author>Robert Schapire</author>
</authors>
<title>Adding Dense, Weighted Connections to WordNet.</title>
<date>2006</date>
<booktitle>Proceedings of the Thirds International WordNet Conference.</booktitle>
<contexts>
<context position="7929" citStr="Boyd-Graber et al., 2006" startWordPosition="1227" endWordPosition="1230">goal by proposing an unsupervised sense disam63 biguation algorithm combining a variety of semantic relatedness measures. We chose an unsupervised method because of the lack of a large manually annotated gold standard. The measurecombined voting algorithm presented here draws advantages from different semantic relatedness measures and has them vote for the best-fitting sense to assign to a label. Evaluation shows that the voting algorithm significantly exceeds WSD with each individual measure. To approach the second goal, we proposed and tested a semantic relatedness measure called evocation (Boyd-Graber et al., 2006) in disambiguation of sound/image labels. Evocation measures human judgements of relatedness between a directed concepts pair. It provides cross parts of speech evocativeness information which supplements most of the knowledge-based semantic relatedness measures. Evaluation results showed that the performance of WSD with evocation is no worse than most of the relatedness measures that we applied, despite the relatively small size of the current evocation dataset. 2 Dataset: Semantic Labels for Environmental Sounds and Images Our ultimate goal is to create an AAC vocabulary of associations betw</context>
<context position="13326" citStr="Boyd-Graber et al., 2006" startWordPosition="2114" endWordPosition="2117">mage with a street sign &amp;quot;Green St.&amp;quot; Third, the Peekaboom labels are not stemmed, which adds another layer of ambiguity. For example, the labels &amp;quot;bridge, building&amp;quot; could refer to a building event or to a built entity. In the experiment, all labels for an image are used in their unstemmed form to construct the context for WSD. 3 Evocation and .ther Semantic Relatedness Measures A set of measures were selected to assess the relatedness between possible senses of words in the sound/image labels. Apart from existing methods, an additional measure, evocation, is introduced. 3.1 Evocation Evocation (Boyd-Graber et al., 2006) measures concept similarity based on human judgment. It is a directed measure, with evocation(synset A, synset B) defined as how much synset A brings to mind synset B. The evocation dataset has been extended to scores for 100,000 directed synset pairs (Nikolova et al., 2009). The evocation data were collected independently of WordNet or corpus data. We propose the use of evocation in WSD for image and sound labels for the following reasons. First, the sound and image labels are generated based on human perception of the content and common knowledge. In SoundNet in particular, many of the evok</context>
</contexts>
<marker>Boyd-Graber, Fellbaum, Osherson, Schapire, 2006</marker>
<rawString>Jordan Boyd-Graber, Christaine Fellbaum, Daniel Osherson, and Robert Schapire. 2006. Adding Dense, Weighted Connections to WordNet. Proceedings of the Thirds International WordNet Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li -J Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet: A Large-Scale Hierarchical Image Database.</title>
<date>2009</date>
<booktitle>Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR).</booktitle>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li -J. Li, Kai Li and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Noun Homograph Disambiguation Using Local Context in Large Text Corpora.</title>
<date>1991</date>
<booktitle>Proc. of 7th</booktitle>
<contexts>
<context position="6120" citStr="Hearst, 1991" startWordPosition="948" endWordPosition="949">ickly or rapidly&amp;quot; via WSD. It means the sound &amp;quot;car — passing.wav&amp;quot; can be used to depict those concepts. This approach is viable because the words in the sound/image labels were shown to evoke one another based on the auditory/visual content, and their meanings can be identified by considering all the tags generated for a given sound or image as a context. With the availability of large sound/image label datasets, the vocabulary created from WSD can be easily expanded. A variety of WSD methods (e.g. knowledgebased methods (Lesk, 1986), unsupervised methods (Lin, 1997), semi-supervised methods (Hearst, 1991) (Yarowsky, 1995), and supervised methods (Novischi et al., 2007)) were developed and evaluated with corpus data and other text documents like webpages. Compared to the text data that WSD methods work with, labels for sounds and images have unique characteristics. The labels are a bag of words related to the visual/auditory content; there is no syntactic or part of speech information, nor are the words necessarily contextual neighbors. For example, contexts suggest landscape senses for the word pair &amp;quot;bank&amp;quot; and &amp;quot;water&amp;quot;, whereas in an image, a person may drink water inside a bank building. Furth</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Marti Hearst. 1991. Noun Homograph Disambiguation Using Local Context in Large Text Corpora. Proc. of 7th</rawString>
</citation>
<citation valid="false">
<booktitle>the Annual Conference of the University of Waterloo Center for the New OED and Text Research.</booktitle>
<marker></marker>
<rawString>the Annual Conference of the University of Waterloo Center for the New OED and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Onge</author>
</authors>
<date>1998</date>
<booktitle>Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database.</booktitle>
<contexts>
<context position="15861" citStr="Onge, 1998" startWordPosition="2543" endWordPosition="2544">n Subsumer (LCS) to the depths of two synsets in the Wordnet taxonomy. • &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) — considering the length of the shortest path between two synsets to the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions around the two synsets. • &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are computed from definition pairs separately. The computation of the relatedness scores using measures listed above were carried out by codes from the WordNet::Simila</context>
</contexts>
<marker>Onge, 1998</marker>
<rawString>Graeme Hirst and David St. Onge. 1998. Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>Proceedings on International Conference on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="15727" citStr="Jiang and Conrath, 1997" startWordPosition="2516" endWordPosition="2519">h between synsets, inversely proportional to the number of nodes on the path. • &amp;quot;wup&amp;quot; (Wu and Palmer, 1994) — ratio of the depth of the Least Common Subsumer (LCS) to the depths of two synsets in the Wordnet taxonomy. • &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) — considering the length of the shortest path between two synsets to the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions around the two synsets. • &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are computed from definition pair</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. Proceedings on International Conference on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining Local Context and WordNet Similarity for Word Sense Identification.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database.</booktitle>
<editor>In Christiane Fellbaum, editor,</editor>
<contexts>
<context position="15357" citStr="Leacock and Chodorow, 1998" startWordPosition="2447" endWordPosition="2450">ve not yet incorporated evocation into the measure-combined voting algorithm described in the Section 4. 3.2 Semantic Relatedness Measures Nine measures of semantic relatedness1 between synsets are used in the experiment, both as contributors to the voting algorithm and as baselines for comparison, including: 1) WordNet path based measures. • &amp;quot;path&amp;quot; — shortest path length between synsets, inversely proportional to the number of nodes on the path. • &amp;quot;wup&amp;quot; (Wu and Palmer, 1994) — ratio of the depth of the Least Common Subsumer (LCS) to the depths of two synsets in the Wordnet taxonomy. • &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) — considering the length of the shortest path between two synsets to the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining Local Context and WordNet Similarity for Word Sense Identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>Proceedings of SIGDOC&apos;86.</booktitle>
<contexts>
<context position="6046" citStr="Lesk, 1986" startWordPosition="939" endWordPosition="940">: a motor vehicle,&amp;quot; &amp;quot;drive: operate or control a vehicle,&amp;quot; and &amp;quot;fast: quickly or rapidly&amp;quot; via WSD. It means the sound &amp;quot;car — passing.wav&amp;quot; can be used to depict those concepts. This approach is viable because the words in the sound/image labels were shown to evoke one another based on the auditory/visual content, and their meanings can be identified by considering all the tags generated for a given sound or image as a context. With the availability of large sound/image label datasets, the vocabulary created from WSD can be easily expanded. A variety of WSD methods (e.g. knowledgebased methods (Lesk, 1986), unsupervised methods (Lin, 1997), semi-supervised methods (Hearst, 1991) (Yarowsky, 1995), and supervised methods (Novischi et al., 2007)) were developed and evaluated with corpus data and other text documents like webpages. Compared to the text data that WSD methods work with, labels for sounds and images have unique characteristics. The labels are a bag of words related to the visual/auditory content; there is no syntactic or part of speech information, nor are the words necessarily contextual neighbors. For example, contexts suggest landscape senses for the word pair &amp;quot;bank&amp;quot; and &amp;quot;water&amp;quot;, w</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. Proceedings of SIGDOC&apos;86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using Syntactic Dependency as a Local Context to Resolve Word Sense Ambiguity.</title>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>64--71</pages>
<location>Lingraphica. http://www.aphasia.com/.</location>
<contexts>
<context position="6080" citStr="Lin, 1997" startWordPosition="944" endWordPosition="945"> or control a vehicle,&amp;quot; and &amp;quot;fast: quickly or rapidly&amp;quot; via WSD. It means the sound &amp;quot;car — passing.wav&amp;quot; can be used to depict those concepts. This approach is viable because the words in the sound/image labels were shown to evoke one another based on the auditory/visual content, and their meanings can be identified by considering all the tags generated for a given sound or image as a context. With the availability of large sound/image label datasets, the vocabulary created from WSD can be easily expanded. A variety of WSD methods (e.g. knowledgebased methods (Lesk, 1986), unsupervised methods (Lin, 1997), semi-supervised methods (Hearst, 1991) (Yarowsky, 1995), and supervised methods (Novischi et al., 2007)) were developed and evaluated with corpus data and other text documents like webpages. Compared to the text data that WSD methods work with, labels for sounds and images have unique characteristics. The labels are a bag of words related to the visual/auditory content; there is no syntactic or part of speech information, nor are the words necessarily contextual neighbors. For example, contexts suggest landscape senses for the word pair &amp;quot;bank&amp;quot; and &amp;quot;water&amp;quot;, whereas in an image, a person may d</context>
<context position="15630" citStr="Lin, 1997" startWordPosition="2496" endWordPosition="2497">mparison, including: 1) WordNet path based measures. • &amp;quot;path&amp;quot; — shortest path length between synsets, inversely proportional to the number of nodes on the path. • &amp;quot;wup&amp;quot; (Wu and Palmer, 1994) — ratio of the depth of the Least Common Subsumer (LCS) to the depths of two synsets in the Wordnet taxonomy. • &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) — considering the length of the shortest path between two synsets to the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions a</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Dekang Lin. 1997. Using Syntactic Dependency as a Local Context to Resolve Word Sense Ambiguity. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pp. 64-71. Lingraphica. http://www.aphasia.com/. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Perry Cook</author>
</authors>
<title>SoundNet: Investigating a Language Composed of Environmental Sounds.</title>
<date>2010</date>
<booktitle>In Proc. CHI</booktitle>
<marker>Cook, 2010</marker>
<rawString>Xiaojuan Ma, Christiane Fellbaum. and Perry Cook. 2010. SoundNet: Investigating a Language Composed of Environmental Sounds. In Proc. CHI 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojuan Ma</author>
<author>Jordan Boy-Graber</author>
<author>Sonya Nikolova</author>
<author>Perry Cook</author>
</authors>
<title>Speaking Through Pictures: Images vs. Icons.</title>
<date>2009</date>
<booktitle>Proceedings ofASSETS09.</booktitle>
<contexts>
<context position="2033" citStr="Ma et al., 2009" startWordPosition="299" endWordPosition="302">res. 1 Introduction In natural languages, a word form may refer to different meanings. For instance, the word &amp;quot;fly&amp;quot; means &amp;quot;travel through the air&amp;quot; in context like &amp;quot;fly to New York,&amp;quot; while it refers to an insect in the phrase &amp;quot;a fly on the trashcan.&amp;quot; Speakers determine the appropriate sense of a polysemous word based on the context. However, people with language disorders and access/retrieval problems, may have great difficulty in understanding words individually or in a context. To overcome such language barriers, visual and auditory representations are introduced to help illustrate concepts (Ma et al., 2009a)(Ma et al., 2010). For example, a person with a language disability can tell the word &amp;quot;fly&amp;quot; refers to &amp;quot;travel through the air&amp;quot; when he sees a plane in the image (rather than an insect); likewise he can distinguish the meaning of &amp;quot;fly&amp;quot; given the plane engine sound vs. the insect buzzing sound. This approach has been employed in Augmentative and Alternative Communication (AAC), in the form of multimodal vocabularies in assistive devices (Steele et al. 1989)(Lingraphica, 2010). However, current AAC vocabularies assign visual stimuli to words instead of specific meanings, and thus bring in ambig</context>
<context position="3524" citStr="Ma et al. 2009" startWordPosition="544" endWordPosition="547">plained using the airplane/bird icon. Similarly, it will lead to miscommunication if the sound of keys jingling is used to express &amp;quot;a key is missing&amp;quot; when the person intends to refer to a key on the keyboard. People with language impairment are relying on the AAC vocabularies for language access, and any ambiguity may result in communication failure. To address this problem, we propose building a semantic multimodal AAC vocabulary with visual and auditory representations expressing concepts rather than words (Figure 1), as the backbone of the language assistant system for people with aphasia (Ma et al. 2009b). Our work is exploratory with the following innovations: 1) we target the insufficiency of current assistive vocabularies by resolving ambiguity; 2) we enrich concept inventory and connect concepts through language, environmental sounds, and images (little research has looked into conveying concepts through natural nonspeech sounds); and 3) our vocabulary has a dynamic scalable semantic network structure rather 62 Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 62–70, Los Angeles, California, June 2010. c�2010 Association for Co</context>
</contexts>
<marker>Ma, Boy-Graber, Nikolova, Cook, 2009</marker>
<rawString>Xiaojuan Ma, Jordan Boy-Graber, Sonya Nikolova, and Perry Cook. 2009a. Speaking Through Pictures: Images vs. Icons. Proceedings ofASSETS09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojuan Ma</author>
<author>Sonya Nikolova</author>
<author>Perry Cook</author>
</authors>
<title>W2ANE: When Words Are Not Enough - Online Multimedia Language Assistant for People with Aphasia.</title>
<date>2009</date>
<booktitle>Proceedings ofACM Multimedia</booktitle>
<contexts>
<context position="2033" citStr="Ma et al., 2009" startWordPosition="299" endWordPosition="302">res. 1 Introduction In natural languages, a word form may refer to different meanings. For instance, the word &amp;quot;fly&amp;quot; means &amp;quot;travel through the air&amp;quot; in context like &amp;quot;fly to New York,&amp;quot; while it refers to an insect in the phrase &amp;quot;a fly on the trashcan.&amp;quot; Speakers determine the appropriate sense of a polysemous word based on the context. However, people with language disorders and access/retrieval problems, may have great difficulty in understanding words individually or in a context. To overcome such language barriers, visual and auditory representations are introduced to help illustrate concepts (Ma et al., 2009a)(Ma et al., 2010). For example, a person with a language disability can tell the word &amp;quot;fly&amp;quot; refers to &amp;quot;travel through the air&amp;quot; when he sees a plane in the image (rather than an insect); likewise he can distinguish the meaning of &amp;quot;fly&amp;quot; given the plane engine sound vs. the insect buzzing sound. This approach has been employed in Augmentative and Alternative Communication (AAC), in the form of multimodal vocabularies in assistive devices (Steele et al. 1989)(Lingraphica, 2010). However, current AAC vocabularies assign visual stimuli to words instead of specific meanings, and thus bring in ambig</context>
<context position="3524" citStr="Ma et al. 2009" startWordPosition="544" endWordPosition="547">plained using the airplane/bird icon. Similarly, it will lead to miscommunication if the sound of keys jingling is used to express &amp;quot;a key is missing&amp;quot; when the person intends to refer to a key on the keyboard. People with language impairment are relying on the AAC vocabularies for language access, and any ambiguity may result in communication failure. To address this problem, we propose building a semantic multimodal AAC vocabulary with visual and auditory representations expressing concepts rather than words (Figure 1), as the backbone of the language assistant system for people with aphasia (Ma et al. 2009b). Our work is exploratory with the following innovations: 1) we target the insufficiency of current assistive vocabularies by resolving ambiguity; 2) we enrich concept inventory and connect concepts through language, environmental sounds, and images (little research has looked into conveying concepts through natural nonspeech sounds); and 3) our vocabulary has a dynamic scalable semantic network structure rather 62 Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 62–70, Los Angeles, California, June 2010. c�2010 Association for Co</context>
</contexts>
<marker>Ma, Nikolova, Cook, 2009</marker>
<rawString>Xiaojuan Ma, Sonya Nikolova and Perry Cook. 2009b. W2ANE: When Words Are Not Enough - Online Multimedia Language Assistant for People with Aphasia. Proceedings ofACM Multimedia 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonya Nikolova</author>
<author>Jordan Boyd-Graber</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Collecting Semantic Similarity Ratings to Connect Concepts</title>
<date>2009</date>
<booktitle>in Assistive Communication Tools (in press). Modelling, Learning and Processing of Text-Technological Data Structures, Springer Studies in Computational Intelligence.</booktitle>
<contexts>
<context position="13602" citStr="Nikolova et al., 2009" startWordPosition="2161" endWordPosition="2164">nstemmed form to construct the context for WSD. 3 Evocation and .ther Semantic Relatedness Measures A set of measures were selected to assess the relatedness between possible senses of words in the sound/image labels. Apart from existing methods, an additional measure, evocation, is introduced. 3.1 Evocation Evocation (Boyd-Graber et al., 2006) measures concept similarity based on human judgment. It is a directed measure, with evocation(synset A, synset B) defined as how much synset A brings to mind synset B. The evocation dataset has been extended to scores for 100,000 directed synset pairs (Nikolova et al., 2009). The evocation data were collected independently of WordNet or corpus data. We propose the use of evocation in WSD for image and sound labels for the following reasons. First, the sound and image labels are generated based on human perception of the content and common knowledge. In SoundNet in particular, many of the evoked labels reflected the most obvious objects or events in a sound scene. For example, &amp;quot;bag&amp;quot; and &amp;quot;coat&amp;quot; were evoked from the zipper soundnail. In this case, the evocation score may be a good evaluation of the relatedness between the labels. Second, evocation assesses relatedne</context>
</contexts>
<marker>Nikolova, Boyd-Graber, Fellbaum, 2009</marker>
<rawString>Sonya Nikolova, Jordan Boyd-Graber, and Christiane Fellbaum. 2009. Collecting Semantic Similarity Ratings to Connect Concepts in Assistive Communication Tools (in press). Modelling, Learning and Processing of Text-Technological Data Structures, Springer Studies in Computational Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Novischi</author>
<author>Muirathnam Srikanth</author>
<author>Andrew Bennett</author>
</authors>
<title>Lcc-wsd: System Description for English Coarse Grained All Words Task at SemEval</title>
<date>2007</date>
<booktitle>4th Proceedings of the International Workshop on Semantic Evaluations(SemEval-2007),</booktitle>
<pages>223--226</pages>
<contexts>
<context position="6185" citStr="Novischi et al., 2007" startWordPosition="955" endWordPosition="958">sing.wav&amp;quot; can be used to depict those concepts. This approach is viable because the words in the sound/image labels were shown to evoke one another based on the auditory/visual content, and their meanings can be identified by considering all the tags generated for a given sound or image as a context. With the availability of large sound/image label datasets, the vocabulary created from WSD can be easily expanded. A variety of WSD methods (e.g. knowledgebased methods (Lesk, 1986), unsupervised methods (Lin, 1997), semi-supervised methods (Hearst, 1991) (Yarowsky, 1995), and supervised methods (Novischi et al., 2007)) were developed and evaluated with corpus data and other text documents like webpages. Compared to the text data that WSD methods work with, labels for sounds and images have unique characteristics. The labels are a bag of words related to the visual/auditory content; there is no syntactic or part of speech information, nor are the words necessarily contextual neighbors. For example, contexts suggest landscape senses for the word pair &amp;quot;bank&amp;quot; and &amp;quot;water&amp;quot;, whereas in an image, a person may drink water inside a bank building. Furthermore, few annotated image or sound label datasets are available</context>
</contexts>
<marker>Novischi, Srikanth, Bennett, 2007</marker>
<rawString>Adrian Novischi, Muirathnam Srikanth, and Andrew Bennett. 2007. Lcc-wsd: System Description for English Coarse Grained All Words Task at SemEval 2007. 4th Proceedings of the International Workshop on Semantic Evaluations(SemEval-2007), pp 223-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
</authors>
<title>Satanjeev Benerjee and Ted Pedersen. Using Measures of Semantic Relatedness for Word Sense Disambiguation.</title>
<date>2003</date>
<booktitle>Proceeding of CICLing2003,</booktitle>
<pages>241--257</pages>
<marker>Patwardhan, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Benerjee and Ted Pedersen. Using Measures of Semantic Relatedness for Word Sense Disambiguation. 2003. Proceeding of CICLing2003, pp. 241-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNet Based Context Vectors to Estimate the Semantic Relatedness of Concepts.</title>
<date>2006</date>
<booktitle>Proceedings of the EACL 2006 Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="16144" citStr="Patwardhan and Pedersen, 2006" startWordPosition="2586" endWordPosition="2589">Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions around the two synsets. • &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are computed from definition pairs separately. The computation of the relatedness scores using measures listed above were carried out by codes from the WordNet::Similarity (Pedersen et al., 2004) and WordNet::SenseRelate projects (Pedersen and Kolhatkar, 2009). In contrast to WordNet::SenseRelated, which employs only one similarity measure in the WSD process, this paper proposes a strategy of having several semantic relatedness measures vote for </context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen Using WordNet Based Context Vectors to Estimate the Semantic Relatedness of Concepts. 2006. Proceedings of the EACL 2006 Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholinguistics Together, pp. 1-8</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WorNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Demonstrations,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="16489" citStr="Pedersen et al., 2004" startWordPosition="2638" endWordPosition="2641">sively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions around the two synsets. • &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are computed from definition pairs separately. The computation of the relatedness scores using measures listed above were carried out by codes from the WordNet::Similarity (Pedersen et al., 2004) and WordNet::SenseRelate projects (Pedersen and Kolhatkar, 2009). In contrast to WordNet::SenseRelated, which employs only one similarity measure in the WSD process, this paper proposes a strategy of having several semantic relatedness measures vote for the best synset for each word. The voting algorithm intends to improve WSD performance by combining conclusions from various measures to eliminate a false result. Since there is no syntax among the words generated for a sound/image, they should all be considered for WSD. Thus, the width of the context window is the total number of words in the</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WorNet::Similarity - Measuring the Relatedness of Concepts. Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Demonstrations, pp. 38-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Varada Kolhatkar</author>
</authors>
<title>WordNet::SenseRelate::AllWords - A Broad Coverage Word Sense Tagger that Maximimizes Semantic Relatedness.</title>
<date>2009</date>
<booktitle>Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Demonstrations,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="16554" citStr="Pedersen and Kolhatkar, 2009" startWordPosition="2645" endWordPosition="2649">ords, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan and Pedersen, 2006) — cosine of the angle between the cooccurrence vector computed from the definitions around the two synsets. • &amp;quot;vector�pairs&amp;quot; — co-occurrence vectors are computed from definition pairs separately. The computation of the relatedness scores using measures listed above were carried out by codes from the WordNet::Similarity (Pedersen et al., 2004) and WordNet::SenseRelate projects (Pedersen and Kolhatkar, 2009). In contrast to WordNet::SenseRelated, which employs only one similarity measure in the WSD process, this paper proposes a strategy of having several semantic relatedness measures vote for the best synset for each word. The voting algorithm intends to improve WSD performance by combining conclusions from various measures to eliminate a false result. Since there is no syntax among the words generated for a sound/image, they should all be considered for WSD. Thus, the width of the context window is the total number of words in the context. 4 Label Sense Disambiguation Algorithm Figure 2. Measur</context>
</contexts>
<marker>Pedersen, Kolhatkar, 2009</marker>
<rawString>Ted Pedersen and Varada Kolhatkar. 2009. WordNet::SenseRelate::AllWords - A Broad Coverage Word Sense Tagger that Maximimizes Semantic Relatedness. Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Demonstrations, pp. 17-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>Proceedings of the 14th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="15527" citStr="Resnik, 1995" startWordPosition="2477" endWordPosition="2478">n synsets are used in the experiment, both as contributors to the voting algorithm and as baselines for comparison, including: 1) WordNet path based measures. • &amp;quot;path&amp;quot; — shortest path length between synsets, inversely proportional to the number of nodes on the path. • &amp;quot;wup&amp;quot; (Wu and Palmer, 1994) — ratio of the depth of the Least Common Subsumer (LCS) to the depths of two synsets in the Wordnet taxonomy. • &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) — considering the length of the shortest path between two synsets to the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and the IC of the LCS. 1 &amp;quot;hso&amp;quot; (Hirst and St-Onge, 1998) extensively slows down the WSD process with over five context words, and thus, is not included in the experiment. 65 3) WordNet definition based measures. • &amp;quot;lesk&amp;quot; (Banerjee and Pedersen, 2002) — overlaps in the definitions of two synsets. • &amp;quot;vector&amp;quot; (Patwardhan an</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. Proceedings of the 14th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Steele</author>
<author>Michael Weinrich</author>
<author>Robert Wertz</author>
<author>Gloria Carlson</author>
<author>Maria Kleczewska</author>
</authors>
<title>Computer-based visual communication in aphasia.</title>
<date>1989</date>
<journal>Neuropsychologia.</journal>
<volume>27</volume>
<issue>4</issue>
<pages>409--26</pages>
<contexts>
<context position="2494" citStr="Steele et al. 1989" startWordPosition="376" endWordPosition="379">ividually or in a context. To overcome such language barriers, visual and auditory representations are introduced to help illustrate concepts (Ma et al., 2009a)(Ma et al., 2010). For example, a person with a language disability can tell the word &amp;quot;fly&amp;quot; refers to &amp;quot;travel through the air&amp;quot; when he sees a plane in the image (rather than an insect); likewise he can distinguish the meaning of &amp;quot;fly&amp;quot; given the plane engine sound vs. the insect buzzing sound. This approach has been employed in Augmentative and Alternative Communication (AAC), in the form of multimodal vocabularies in assistive devices (Steele et al. 1989)(Lingraphica, 2010). However, current AAC vocabularies assign visual stimuli to words instead of specific meanings, and thus bring in ambiguity when a user with language disability tries to comprehend and communicate a concept. For example, for the word &amp;quot;fly,&amp;quot; Lingraphica only has an icon showing a plane and a flock of birds flying. Confusion arises when a sentence like &amp;quot;I want to kill the fly (the insect)&amp;quot; is explained using the airplane/bird icon. Similarly, it will lead to miscommunication if the sound of keys jingling is used to express &amp;quot;a key is missing&amp;quot; when the person intends to refer t</context>
</contexts>
<marker>Steele, Weinrich, Wertz, Carlson, Kleczewska, 1989</marker>
<rawString>Richard Steele, Michael Weinrich, Robert Wertz, Gloria Carlson, and Maria Kleczewska. Computer-based visual communication in aphasia. Neuropsychologia. 27(4): pp 409-26. 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn, Laura Dabbish. 2004. Labeling images with a computer game. Proceedings of the SIGCHI conference on Human factors in computing systems, p.319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Ruoran Liu</author>
<author>Manuel Blum</author>
</authors>
<title>Peekaboom: a game for locating objects in images.</title>
<date>2006</date>
<booktitle>Proceedings of the SIGCHI conference on Human Factors in computing systems.</booktitle>
<marker>von Ahn, Liu, Blum, 2006</marker>
<rawString>Luis von Ahn, Ruoran Liu, Manuel Blum. 2006 Peekaboom: a game for locating objects in images. Proceedings of the SIGCHI conference on Human Factors in computing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>Proc. ofACL,</booktitle>
<pages>133--138</pages>
<contexts>
<context position="15210" citStr="Wu and Palmer, 1994" startWordPosition="2420" endWordPosition="2423">e labels. Considering that the evocation dataset is small in size and susceptible to noise given the method by which it was collected, we have not yet incorporated evocation into the measure-combined voting algorithm described in the Section 4. 3.2 Semantic Relatedness Measures Nine measures of semantic relatedness1 between synsets are used in the experiment, both as contributors to the voting algorithm and as baselines for comparison, including: 1) WordNet path based measures. • &amp;quot;path&amp;quot; — shortest path length between synsets, inversely proportional to the number of nodes on the path. • &amp;quot;wup&amp;quot; (Wu and Palmer, 1994) — ratio of the depth of the Least Common Subsumer (LCS) to the depths of two synsets in the Wordnet taxonomy. • &amp;quot;lch&amp;quot; (Leacock and Chodorow, 1998) — considering the length of the shortest path between two synsets to the depth of the WordNet taxonomy. 2) Information and content based measures. • &amp;quot;res&amp;quot; (Resnik, 1995) — the informational content (IC) of a given corpus of the LCS between two synsets. • &amp;quot;lin&amp;quot; (Lin, 1997) — the ratio of the IC of the LCS to the IC of the two synsets. • &amp;quot;jcn&amp;quot; (Jiang and Conrath, 1997) — inversely proportional to the difference between the IC of the two synsets and t</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. Proc. ofACL, pp 133-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting on Association For Computational Linguistics.</booktitle>
<contexts>
<context position="6137" citStr="Yarowsky, 1995" startWordPosition="950" endWordPosition="951">y&amp;quot; via WSD. It means the sound &amp;quot;car — passing.wav&amp;quot; can be used to depict those concepts. This approach is viable because the words in the sound/image labels were shown to evoke one another based on the auditory/visual content, and their meanings can be identified by considering all the tags generated for a given sound or image as a context. With the availability of large sound/image label datasets, the vocabulary created from WSD can be easily expanded. A variety of WSD methods (e.g. knowledgebased methods (Lesk, 1986), unsupervised methods (Lin, 1997), semi-supervised methods (Hearst, 1991) (Yarowsky, 1995), and supervised methods (Novischi et al., 2007)) were developed and evaluated with corpus data and other text documents like webpages. Compared to the text data that WSD methods work with, labels for sounds and images have unique characteristics. The labels are a bag of words related to the visual/auditory content; there is no syntactic or part of speech information, nor are the words necessarily contextual neighbors. For example, contexts suggest landscape senses for the word pair &amp;quot;bank&amp;quot; and &amp;quot;water&amp;quot;, whereas in an image, a person may drink water inside a bank building. Furthermore, few annot</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. Proceedings of the 33rd Annual Meeting on Association For Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>