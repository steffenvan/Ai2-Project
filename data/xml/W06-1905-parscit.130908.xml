<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.996137">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<title confidence="0.9983565">
Keyword Translation Accuracy and Cross-Lingual Question
Answering in Chinese and Japanese
</title>
<author confidence="0.991476">
Teruko Mitamura Mengqiu Wang Hideki Shima Frank Lin
</author>
<affiliation confidence="0.788746">
Carnegie Mellon Carnegie Mellon Carnegie Mellon Carnegie Mellon
University University University University
Pittsburgh, PA USA Pittsburgh, PA USA Pittsburgh, PA USA Pittsburgh, PA USA
</affiliation>
<email confidence="0.998021">
teruko@cs.cmu.edu mengqiu@cs.cmu.edu hideki@cs.cmu.edu frank+@cs.cmu.edu
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999432">
In this paper, we describe the extension
of an existing monolingual QA system
for English-to-Chinese and English-to-
Japanese cross-lingual question answer-
ing (CLQA). We also attempt to charac-
terize the influence of translation on
CLQA performance through experimen-
tal evaluation and analysis. The paper
also describes some language-specific is-
sues for keyword translation in CLQA.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980505882353">
The JAVELIN system is a modular, extensible
architecture for building question-answering
(QA) systems (Nyberg, et al., 2005). Since the
JAVELIN architecture is language-independent,
we extended the original English version of
JAVELIN for cross-language question answering
(CLQA) in Chinese and Japanese. The same
overall architecture was used for both systems,
allowing us to compare the performance of the
two systems. In this paper, we describe how we
extended the monolingual system for CLQA (see
Section 3). Keyword translation is a crucial ele-
ment of the system; we describe our translation
module in Section 3.2. In Section 4, we evaluate
the end-to-end CLQA systems using three differ-
ent translation methods. Language-specific
translation issues are discussed in Section 5.
</bodyText>
<sectionHeader confidence="0.984798" genericHeader="introduction">
2 Javelin Architecture
</sectionHeader>
<bodyText confidence="0.9999185">
The JAVELIN system is composed of four main
modules: the Question Analyzer (QA), Retrieval
Strategist (RS), Information eXtractor (IX) and
Answer Generator (AG). Inputs to the system are
processed by these modules in the order listed
above. The QA module is responsible for parsing
the input question, assigning the appropriate an-
swer type to the question, and producing a set of
keywords. The RS module is responsible for
finding documents containing answers to the
question, using keywords produced by the QA
module. The IX module finds and extracts an-
swers from the documents based on the answer
type, and then produces a ranked list of answer
candidates. The AG module normalizes and clus-
ters the answer candidates to rerank and generate
a final ranked list. The overall monolingual ar-
chitecture is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="method">
3 Extension for Cross-Lingual QA
</sectionHeader>
<bodyText confidence="0.999957">
Because of JAVELIN’s modular design, signifi-
cant changes to the monolingual architecture
were not required. We customized the system in
order to handle Unicode characters and “plug in”
cross-lingual components and resources.
For the Question Analyzer, we created the
Keyword Translator, a sub-module for translat-
ing keywords. The Retrieval Strategist was
adapted to search in multilingual corpora. The
Information Extractors use language-independent
extraction algorithms. The Answer Generator
uses language-specific sub-modules for normali-
zation, and a language-independent algorithm for
answer ranking. The overall cross-lingual archi-
tecture is shown in Figure 2. The rest of this sec-
tion explains the details of each module.
</bodyText>
<subsectionHeader confidence="0.999121">
3.1 Question Analyzer
</subsectionHeader>
<bodyText confidence="0.999723333333333">
The Question Analyzer (QA) is responsible for
extracting information from the input question in
order to formulate a representation of the
</bodyText>
<page confidence="0.999509">
31
</page>
<note confidence="0.6523715">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
Figure1: Javelin Monolingual Architecture Figure2: Javelin Architecture with Cross-Lingual
</note>
<sectionHeader confidence="0.483269" genericHeader="method">
Extension
</sectionHeader>
<bodyText confidence="0.99997988">
information required to answer the question.
Input questions are processed using the RASP
parser (Korhonen and Briscoe, 2004), and the
module output contains three main components:
a) selected keywords; b) the answer type (e.g.
numeric-expression, person-name, location); and
c) the answer subtype (e.g. author, river, city).
The selected keywords are words or phrases
which are expected to appear in documents with
correct answers. In order to reduce noise in the
document retrieval phase, we use stop-word lists
to eliminate high-frequency terms; for example,
the term “old” is not included as a keyword for
“how-old” questions.
We extended the QA module with a keyword
translation sub-module, so that translated key-
words can be used to retrieve documents from
multilingual corpora. This straightforward ap-
proach has been used by many other CLQA sys-
tems. An alternative approach is to first translate
the whole question sentence from English to the
target language, and then analyze the translated
question. Our reasons for favoring keyword
translation are two-fold. First, to translate the
question to the target language and analyze it, we
would have to replace the English NLP compo-
nents in the Question Analyzer with their coun-
terparts for the target language. In contrast, key-
word translation decouples the question analysis
from the translation, and requires no language
specific resources during question analysis. The
second reason is that machine translation is not
perfect, and therefore the resulting translation(s)
for the question may be incomplete or ungram-
matical, thus adding to the complexity of the
analysis task. One could argue that when trans-
lating the full sentence instead of just the key-
words, we can better utilize state-of-art machine
translation techniques because more context in-
formation is available. But for our application, an
accurate translation of functional words (such as
prepositions or conjunctions) is less important.
We focus more on words that carry more content
information, such as verbs and nouns. We will
present more detail on the use of contextual in-
formation for disambiguation in the next section.
In some recent work (Kwok, 2005, Mori and
Kawagishi, 2005), researchers have combined
these two approaches, but to date no studies have
compared their effectiveness.
</bodyText>
<subsectionHeader confidence="0.999243">
3.2 Translation Module
</subsectionHeader>
<bodyText confidence="0.999655064516129">
The Translation Module (TM) is used by the QA
module to translate keywords into the language
of the target corpus. Instead of combining multi-
ple translation candidates with a disjunctive
query operator (Isozaki et al., 2005), the TM se-
lects the best combination of translated keywords
from several sources: Machine Readable Dic-
tionaries (MRDs), Machine Translation systems
(MTs) and Web-mining-Based Keyword Trans-
lators (WBMTs) (Nagata et al., 2001, Li et al.,
2003). For translation from English to Japanese,
we used two MRDs, eight MTs and one WBMT.
If none of them return a translation, the word is
transliterated into kana for Japanese (for details
on transliteration, see Section 5.2). For transla-
tion from English to Chinese, we used one MRD,
three MTs and one WBMT. After gathering all
possible translations for every keyword, the TM
uses a noisy channel model to select the best
combination of translated keywords. The TM
estimates model statistics using the World Wide
Web. Details of the translation selection method
are described in the rest of this subsection.
The Noisy Channel Model: In the noisy channel
model, an undistorted signal passes through a
noisy channel and becomes distorted. Given the
distorted signal, we are to find the original, un-
distorted signal. IBM applied the noisy channel
model idea to translation of sentences from
aligned parallel corpora, where the source lan-
guage sentence is the distorted signal, and the
</bodyText>
<page confidence="0.997935">
32
</page>
<note confidence="0.854189">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.994029545454545">
target language sentence is the original signal
(Brown et al., 1990). We adopt this model for
disambiguating keyword translation, with the
source language keyword terms as the distorted
signal and the target language terms as the origi-
nal signal. The TM&apos;s job is to find the target lan-
guage terms given the source language terms, by
finding the probability of the target language
terms given the source language terms P(T|S).
Using Bayes&apos; Rule, we can break the equation
down to several components:
</bodyText>
<equation confidence="0.998644">
P(T  |S) = P(T) ⋅ P(S  |T)
P(S)
</equation>
<bodyText confidence="0.999754333333333">
Because we are comparing probabilities of dif-
ferent translations of the same source keyword
terms, we can simplify the problem to be:
</bodyText>
<equation confidence="0.985996">
P(T  |S) = P(T) ⋅ P(S  |T)
</equation>
<bodyText confidence="0.97817">
We can now reduce the equation to two compo-
nents. P(T) is the language model and P(S|T) is
the translation model. If we assume independ-
ence among the translations of individual terms,
we can represent the translation probability of a
keyword by the product of the probabilities of
the individual term translations:
i
Estimating Probabilities using the World
Wide Web: For estimating the probabilities of
the translation model and the language model,
we chose to gather statistics from the World
Wide Web. There are three advantages in utiliz-
ing the web for gathering translation statistics: 1)
it contains documents written in many different
languages, 2) it has high coverage of virtually all
types of words and phrases, and 3) it is con-
stantly updated. However, we also note that the
web contains a lot of noisy data, and building up
web statistics is time-consuming unless one has
direct access to a web search index.
Estimating Translation Model Probabilities:
We make an assumption that terms that are trans-
lations of each other co-occur more often in
mixed-language web pages than terms that are
not translations of each other. This assumption is
analogous to Turney’s work on the co-
occurrence of synonyms (Turney, 2001). We
then define the translation probability of each
keyword translation as:
j
Where si is the i-th term in the source language
and ti,j is the j-th translation candidate for si. Let
hits be a number of web pages retrieved from a
certain search engine. co(si, t i,j) is the hits given
a query si and ti,j., where log is applied to adjust
the count so that translation probabilities can still
be comparable at higher counts.
Estimating Language Model Probabilities: In
estimating the language model, we simply obtain
hits given a conjunction of all the candidate
terms in the target language, and divide that
count by the sum of the occurrences of the indi-
vidual terms:
( ) 1 2
</bodyText>
<equation confidence="0.992918142857143">
co t t t
( , ,... n
P T =
∑o t
( )
i
i
</equation>
<bodyText confidence="0.99848905">
The final score of a translation candidate for a
query is the product of the translation model
score P(S|T) and the language model score P(T).
Smoothing and Pruning: As with most statisti-
cal calculations in language technologies, there is
a data sparseness problem when calculating the
language model score. Also, because statistics
are gathered real-time by accessing a remote
search engine via internet, it can take a long time
to process a single query when there is a large
number of translation candidates. We describe
methods for smoothing the language model and
pruning the set of translation candidates below.
The data sparseness problem occurs when
there are many terms in the query, and the terms
are relatively rare keywords. When calculating
the language model score, it is possible that none
of the translation candidates appear on any web
page. To address this issue, we propose a &amp;quot;mov-
ing-window smoothing&amp;quot; algorithm:
</bodyText>
<listItem confidence="0.912941333333333">
• When the target keyword co-occurrence
count with n keywords is below a set
threshold for all of the translation candi-
</listItem>
<bodyText confidence="0.99752475">
dates, we use a moving window of size
n-1 that &amp;quot;moves&amp;quot; through the keywords
in sequence, splitting the set of keywords
into two sets, each with n-1 keywords.
</bodyText>
<equation confidence="0.806296">
P(S  |T) P ( s i  |t i )
= ∏
</equation>
<listItem confidence="0.9705">
• If the co-occurrence count of all of these
sets of keywords is above the threshold,
return the product of the language model
</listItem>
<equation confidence="0.996615333333334">
Rs i|ti j ) ,
log(co(si , ti
∑
j
))
log(co(si, ti
,
j ))
)
</equation>
<page confidence="0.975347">
33
</page>
<note confidence="0.788083">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.796406">
score of these two sets as the language
model score.
</bodyText>
<listItem confidence="0.892659">
• If not, decrease the window and repeat
until either all of the split sets are above
the threshold or n = 1.
</listItem>
<bodyText confidence="0.998751571428571">
The moving window smoothing technique
gradually relaxes the search constraint without
losing the &amp;quot;connectivity&amp;quot; of keywords (there is
always overlap in the split parts) before finally
backing off to just the individual keywords.
However, there are two issues worth noting with
this approach:
</bodyText>
<listItem confidence="0.999761">
1. &amp;quot;Moving-window smoothing&amp;quot; assumes
that keywords that are next to each other
are also more semantically related,
which may not always be the case.
2. &amp;quot;Moving-window smoothing&amp;quot; tends to
give the keywords near the middle of the
question more weight, which may not be
desirable.
2. Late Pruning: We prune possible transla-
</listItem>
<bodyText confidence="0.9375664">
tion candidates of the entire set of key-
words after calculating translation prob-
abilities. Since the calculation of the
translation probabilities requires little
access to the web, we can calculate only
the language model score for the top N
candidates with the highest translation
score and prune the rest.
An Example of English to Chinese Keyword
Translation Selection: Suppose we translate the
following question from English to Chinese.
&amp;quot;What if Bush leaves Iraq?&amp;quot;
Three keywords are extracted: “Bush”,
“leaves”, and “Iraq.” Using two MT systems and
an MRD, we obtain the following translations:
</bodyText>
<table confidence="0.98274175">
i=1 i=2 i=3
Source Bush leaves Iraq
Target j=1 灌木 离去 伊拉克
Target j=2 布什 叶子
</table>
<tableCaption confidence="0.999685">
Table 1. E-C Keyword Translation
</tableCaption>
<bodyText confidence="0.999394105263158">
A better smoothing technique may be used
with trying all possible &amp;quot;splits&amp;quot; at each stage, but
this would greatly increase the time cost. There-
fore, we chose the moving-window smoothing as
a trade-off between a more robust smoothing
technique that tries all possible split combina-
tions and no smoothing at all.
The set of possible translation candidates is
produced by creating all possible combinations
of the translations of individual keywords. For a
question with n keywords and an average of m
possible translations per keyword, the number of
possible combinations is mn. This quickly be-
comes intractable as we have to access a search
engine at least mn times just for the language
model score. Therefore, pruning is needed to cut
down the number of translation candidates. We
prune possible translation candidates twice dur-
ing each run, using early and late pruning:
</bodyText>
<listItem confidence="0.764927">
1. Early Pruning: We prune possible trans-
</listItem>
<bodyText confidence="0.968243333333333">
lations of the individual keywords before
combining them to make all possible
translations of a query. We use a very
simple pruning heuristic based on target
word frequency using a word frequency
list. Very rare translations produced by a
resource are not considered.
&amp;quot;Bush&amp;quot; and &amp;quot;leaves&amp;quot; both have two transla-
tions because they are ambiguous keywords,
while &amp;quot;Iraq&amp;quot; is unambiguous. Translation (1,1)
means bush as in a shrub, and translation (1,2)
refers to the person named Bush. Translation
(2,1) is the verb &amp;quot;to go away&amp;quot;, and translation
(2,2) is the noun for leaf. Note that we would like
translation (1,2) and translation (2,1) because
they match the sense of the word intended by the
user. Now we can create all possible combina-
tions of the keywords in the target language:
</bodyText>
<table confidence="0.774433714285714">
&amp;quot;灌木 离去 伊拉克&amp;quot;
&amp;quot;灌木 叶子 伊拉克&amp;quot;
&amp;quot;布什 离去 伊拉克&amp;quot;
&amp;quot;布什 叶子 伊拉克&amp;quot;
Query &amp;quot;Bush&amp;quot; &amp;quot;Bush&amp;quot; &amp;quot;leaves&amp;quot; &amp;quot;leaves&amp;quot; &amp;quot;Iraq&amp;quot;
&amp;quot;灌木&amp;quot; &amp;quot;布什&amp;quot; &amp;quot;离去&amp;quot; &amp;quot;叶子&amp;quot; &amp;quot;伊拉克&amp;quot;
hits 3790 41100 5780 7240 24500
</table>
<tableCaption confidence="0.998885">
Table 2. Translation Pair Page Counts
</tableCaption>
<table confidence="0.9996696">
Candidate Translation Score
&amp;quot;灌木 离去 伊拉克&amp;quot; 0.215615
&amp;quot;灌木 叶子 伊拉克&amp;quot; 0.221219
&amp;quot;布什 离去 伊拉克&amp;quot; 0.277970
&amp;quot;布什 叶子 伊拉克&amp;quot; 0.285195
</table>
<tableCaption confidence="0.997862">
Table 3. Translation Scores
</tableCaption>
<page confidence="0.990328">
34
</page>
<note confidence="0.926786">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.996780857142857">
By calculating hits, we obtain the statistics and
the translation scores shown in Table 2 and 3.
Now we can proceed to use the search engine to
obtain language model statistics, which we use to
obtain the language model. Then, together with
the translation model score, we calculate the
overall score1.
</bodyText>
<table confidence="0.991454">
Query 灌木 布什 离去 叶子 伊拉克
hits 428K 459K 1490K 1100K 9590K
</table>
<tableCaption confidence="0.989458">
Table 4. Individual Term Page Counts
</tableCaption>
<table confidence="0.9995216">
Query hits
&amp;quot;灌木 离去 伊拉克&amp;quot; 1200
&amp;quot;灌木 叶子 伊拉克&amp;quot; 455
&amp;quot;布什 离去 伊拉克&amp;quot; 17300
&amp;quot;布什 叶子 伊拉克&amp;quot; 2410
</table>
<tableCaption confidence="0.987749">
Table 5. Target Language Query Page Counts
</tableCaption>
<table confidence="0.9983924">
Cand Translation Language Overall
灌木离去伊拉克 2.1562E-1 1.0428E-4 2.2483E-5
灌木叶子伊拉克 2.2122E-1 4.0925E-5 9.0533E-6
布什离去伊拉克 2.7797E-1 1.4993E-3 4.1675E-4
布什叶子伊拉克 2.8520E-1 2.1616E-4 6.1649E-5
</table>
<tableCaption confidence="0.7850305">
Table 6. Translation Score, Language Model
Score, and Overall Score
</tableCaption>
<bodyText confidence="0.99975625">
As shown in Table 6, we select the most prob-
able combination of translated keywords with the
highest overall score (the third candidate), which
is the correct translation of the English keywords.
</bodyText>
<subsectionHeader confidence="0.99854">
3.3 Retrieval Strategies
</subsectionHeader>
<bodyText confidence="0.9007095">
The Retrieval Strategist (RS) module retrieves
documents from a corpus in response to a query.
For document retrieval, the RS uses the Lemur
3.0 toolkit (Ogilvie and Callan, 2001). Lemur
supports structured queries using operators such
as Boolean AND, Synonym, Ordered/Un-
Ordered Window and NOT. An example of a
structured query is shown below:
1 For simplicity, we don’t apply smoothing
and pruning.
</bodyText>
<equation confidence="0.993263333333333">
#BAND( #OD4(邪馬台国 王朝)
女王
#SYN(*organization *person) )
</equation>
<bodyText confidence="0.999900636363636">
In formulating a structured query, the RS uses an
incremental relaxation technique, starting from
an initial query that is highly constrained; the
algorithm searches for all the keywords and data
types in close proximity to each other. The prior-
ity is based on a function of the likely answer
type, keyword type (word, proper name, or
phrase) and the inverse document frequency of
each keyword. The query is gradually relaxed
until the desired number of relevant documents is
retrieved.
</bodyText>
<subsectionHeader confidence="0.819346">
3.4 Information Extraction
</subsectionHeader>
<bodyText confidence="0.989274586206897">
In the JAVELIN system, the Information Ex-
tractor (IX) is not a single module that uses one
extraction algorithm; rather, it is an abstract in-
terface which allows different information ex-
tractor implementations to be plugged into
JAVELIN. These different extractors can be used
to produce different results for comparison, or
the results of running them all in parallel can be
merged. Here we will describe just one of the
extractors, the one which is currently the best
algorithm in our CLQA experiment: the Light IX.
The Light IX module uses simple, distance-
based algorithms to find a named entity that
matches the expected answer type and is “clos-
est” to all the keywords according to some dis-
tance measure. The algorithm considers as an-
swer candidates only those terms that are tagged
as named entities which match the desired an-
swer type. The score for an answer candidate a
is calculated as follows:
Score (a) = α • OccScore (a) + Q • DistScore a )
(
where α + R = 1, OccScore is the occurrence
score and DistScore is the distance score. Both
OccScore and DistScore return a number be-
tween zero and one, and likewise Score returns a
number between zero and one. Usually, α is
much smaller than R. The occurrence score for-
mula is:
</bodyText>
<equation confidence="0.98646075">
n
� i=1 ( )
Exist k i
n
</equation>
<bodyText confidence="0.999973">
where a is the answer candidate and ki is the i-th
keyword, and n is the number of keywords. Exist
returns 1 if the i-th keyword exists in the docu-
ment, and 0 otherwise. The distance score for
</bodyText>
<equation confidence="0.657902">
OccScore(a)
</equation>
<page confidence="0.991469">
35
</page>
<note confidence="0.923556">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.995881">
each answer candidate is calculated according to
the following formula:
</bodyText>
<equation confidence="0.967436">
DistScore(a)
n
</equation>
<bodyText confidence="0.999560888888889">
This formula produces a score between zero
and one. If the i-th keyword does not exist in a
document, the equation inside the summation
will return zero. If the i-th keyword appears more
than once in the document, the one closest to the
answer candidate is considered. An additional
restriction is that the answer candidate cannot be
one of the keywords. The Dist function is the
distance measure, which has two definitions:
</bodyText>
<listItem confidence="0.606111">
1. Dist(a, b) = TokensApart(a, b)
2. Dist a b = TokensApart a b
</listItem>
<equation confidence="0.737546">
( , ) log( ( , ))
</equation>
<bodyText confidence="0.999980863636364">
The first definition simply counts the number
of tokens between two terms. The second defini-
tion is a logarithmic measure. The function re-
turns the number of tokens from a to b; if a and b
are adjacent, the count is 1; if a and b are sepa-
rated by one token, the count is 2, and so on. A
token can either be a character or a word; for the
E-C, we used character-based tokenization,
whereas for the E-J, we use word-based tokeni-
zation. By heuristics obtained from training re-
sults, we used the linear Dist measure for E-C
and logarithmic Dist measure for E-J in the
evaluation.
This algorithm is a simple statistical approach
which requires no language-specific external
tools beyond word segmentation and a named-
entity tagger. It is not as sophisticated as other
approaches which perform deep linguistic analy-
sis, but one advantage is faster adaptation to mul-
tiple languages. In our experiments, this simple
algorithm performs at the same level as a FST-
based approach (Nyberg, et al. 2005).
</bodyText>
<subsectionHeader confidence="0.996169">
3.5 Answer Generator
</subsectionHeader>
<bodyText confidence="0.999030538461538">
The task of the Answer Generator (AG) module
is to produce a ranked list of answer candidates
from the IX output. The AG is designed to nor-
malize answer candidates by resolving represen-
tational differences (e.g. in how numbers, dates,
etc. are expressed in text). This canonicalization
makes it possible to combine answer candidates
that differ only in surface form.
Even though the AG module plays an impor-
tant role in JAVELIN, we did not use its full po-
tential in our E-C and E-J systems, since we
lacked some language-specific resources re-
quired for multilingual answer merging.
</bodyText>
<sectionHeader confidence="0.9542825" genericHeader="method">
4 Evaluation and Effect of Translation
Accuracy
</sectionHeader>
<bodyText confidence="0.999663375">
To evaluate the effect of translation accuracy on
the overall performance of the CLQA system, we
conducted several experiments using different
translation methods. Three different runs were
carried out for both the E-C and E-J systems,
using the same 200-question test set and the
document corpora provided by the NTCIR
CLQA task. The first run was a fully automatic
run using the original translation module in the
CLQA system; the result is exactly same as the
one we submitted to NTCIR5 CLQA. For the
second run, we manually translated the keywords
that were selected by the Question Analyzer
module. This translation was done by looking at
only the selected keywords, but not the original
question. For both E-C and E-J tasks, the NTCIR
organizers provided the translations for the Eng-
lish questions, which we assume are the gold-
standard translations. Taking advantage of this
resource, in the third run we simply looked up
the corresponding term for each English keyword
from the gold-standard translation of the ques-
tion. The results for these runs are shown in Ta-
ble 7 and 8 below.
</bodyText>
<table confidence="0.999783">
Translation Top1 Top1+U
Accuracy
Run 1 69.3% 15 (7.5%) 23 (11.5%)
Run 2 85.5% 16 (8.0%) 31 (15.5%)
Run 3 100% 18 (9.0%) 38 (19.0%)
</table>
<tableCaption confidence="0.986855">
Table 7. Effect of Translation (E-C)
</tableCaption>
<table confidence="0.97854725">
Top1 Top1+U
Run 1 54.2% 20 (10.0%) 25 (12.5%)
Run 2 81.2% 19 (9.5%) 30 (15.0%)
Run 3 100% 18 (9.0%) 31 (15.5%)
</table>
<tableCaption confidence="0.999419">
Table 8. Effect of Translation (E-J)
</tableCaption>
<bodyText confidence="0.999533222222222">
We found that in the NTCIR task, the sup-
ported/correct document set was not complete.
Some answers judged as unsupported were in-
deed well supported, but the supporting docu-
ment did not appear in NTCIR&apos;s correct docu-
ment set. Therefore, we think the Top1+U col-
umn is more informative for this evaluation.
From Table 7 and 8, it is obvious that the overall
performance increases as translation accuracy
</bodyText>
<table confidence="0.748614333333333">
En = 1
i
= 1 ( , )
Dist a k i
Translation
Accuracy
</table>
<page confidence="0.992491">
36
</page>
<note confidence="0.954298">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.999901766666667">
increases. From Run1 to Run2, we eliminated all
the overt translation errors produced by the sys-
tem, and also corrected word-sense errors. Then
from Run2 to Run3, we made different lexical
choices among the seemingly all correct transla-
tions of a word. This type of inappropriateness
cannot be classified as an error, but it makes a
difference in QA systems, especially at the docu-
ment retrieval stage. For example, the phrase
&amp;quot;Kyoto Protocol&amp;quot; can have two valid transla-
tions: 京都協議 or 京都議定書. Both translations
would be understandable to a human, but the sec-
ond translation will appear much more frequently
than the first one in the document set. This type
of lexical choice is hard to make, because we
would need either subtle domain-specific knowl-
edge, or knowledge about the target corpus; nei-
ther is easily obtainable.
Comparing Run 1 and 3 in Table 8, we see
that improving keyword translation had less
overall impact on the E-J system. Information
extraction (including named entity identification)
did not perform as well in E-J. We also com-
pared the translation effect on cross-lingual
document retrieval (Figure 3). As we can see,
Run 3 retrieved supporting documents more fre-
quently in rank 1 than in Run 1 or 2. From these
preliminary investigations, it would seem that
information extraction and/or answer generation
must be improved for English-Japanese CLQA.
</bodyText>
<figureCaption confidence="0.610319">
Figure3: Comparison of three runs: Cross-lingual
document retrieval performance in E-J
</figureCaption>
<sectionHeader confidence="0.994072" genericHeader="method">
5 Translation Issues
</sectionHeader>
<bodyText confidence="0.994695">
In this section, we discuss language specific key-
word translation issues for Chinese and Japanese
CLQA.
</bodyText>
<subsectionHeader confidence="0.833721">
5.1 Chinese
</subsectionHeader>
<bodyText confidence="0.990089675675675">
One prominent problem in Chinese keyword
translation is word sense disambiguation. In
question answering systems, the translation re-
sults are used directly in information retrieval,
which exhibits a high dependency on the lexical
form of a word but not so much on the meaning.
In other words, having a different lexical form
from the corresponding term in corpora is the
same as having a wrong translation. For exam-
ple, to translate the word “bury” into Chinese,
our system gives a translation of tff, which
means “bury” as the action of digging a hole,
hiding some items in the hole and then covering
it with earth. But the desired translation, as it
appears in the document is 葬 , which means
“bury” too, but specifically for burial in funerals.
Even more challenging are regional language
differences. In our system, for example, the cor-
pora are newswire articles written in Traditional
Chinese from Taiwan, and if we use an MT sys-
tem that produces translations in Simplified Chi-
nese followed by conversion to Traditional Chi-
nese, we may run into problems. The MT system
generates Simplified Chinese translations first,
which may suggest that the translation resources
it uses were written in Simplified Chinese and
originate from mainland China. In mainland
China and in Taiwan, people commonly use dif-
ferent words for describing the same thing, espe-
cially for proper nouns like foreign names. Table
9 lists some examples. Therefore if the MT sys-
tem generates its output using text from
mainland China, it may produce a different word
than the one used in Taiwan, which may not ap-
pear in the corpora. This could lead to failure in
document retrieval.
English Mainland China Taiwan
</bodyText>
<equation confidence="0.983897333333333">
樂隊 樂團
電腦遊戲 電玩
吉尼斯世界紀錄 金氏世界紀錄
稻田裏的守望者 麥田捕手
奈爾森 納爾遜
塞林格 沙林傑
人類狂牛症 賈庫氏症
呂克 貝松 盧貝松
帕瓦洛蒂 帕華洛帝
</equation>
<tableCaption confidence="0.927921">
Table 9. Different Translation in Chinese
</tableCaption>
<subsectionHeader confidence="0.931216">
5.2 Japanese
</subsectionHeader>
<bodyText confidence="0.99922975">
Representational Gaps: One of the advantages
of using structured queries and automatic query
formulation in the RS is that the system is able to
handle slight representational gaps between a
</bodyText>
<figure confidence="0.99557725">
Band
Computer Game
World Guinness
Record
The Catcher in
the Rye
Nelson
Salinger
Creutzfeldt
Jakob Disease
Luc Besson
Pavarotti
</figure>
<page confidence="0.99352">
37
</page>
<note confidence="0.93979">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.999420581395349">
translated query and corresponding target words
in the corpus.
For example, Werner Spies appears as ヴェル
ナー ・ シュピース in our Japanese preproc-
essed corpus and therefore ヴェルナー シュピー
ス, which is missing a dot between last and first
name, is a wrong translation if our retrieval
module only allows exact match. Lemur supports
an Ordered Distance Operator where the terms
within a #ODN operator must be found within N
words of each other in the text in order to con-
tribute to the document&apos;s belief value. This en-
ables us to bridge the representational gaps; such
as when #OD1(ヴェルナー シュピース) does not
match any words in the corpus, #OD2(ヴェルナー
シュピース) is formulated in the next step in or-
der to capture ヴェルナー ・ シュピース.
Transliteration in WBMT: After detecting
Japanese nouns written in romaji (e.g. Funaba-
shi), we transliterated them into hiragana for a
better result in WBMT. This is because we are
assuming higher positive co-occurrence between
kana and kanji (i.e. ふなばし and 船橋) than be-
tween romaji and kanji (i.e. funabashi and 船橋).
When there are multiple transliteration candi-
dates, we iterate through each candidate.
Document Retrieval in Kana: Suppose we are
going to transliterate Yusuke. This romaji can be
mapped to kana characters with relatively less
ambiguity (i.e. ゆすけ, ゆうすけ), when com-
pared to their subsequent transliteration to kanji
(i.e. 雄介, 祐介, 佑介, 勇介, 雄輔 etc.). Therefore,
indexing kana readings in the corpus and query-
ing in kana is sometimes a useful technique for
CLQA, given the difficulty in converting romaji
to kana and romaji to kanji.
To implement this approach, the Japanese cor-
pus was first preprocessed by annotating named
entities and by chunking morphemes. Then, we
annotated a kana reading for each named entity.
At query time, if there is no translation found
from other resources, the TM transliterates ro-
maji to kana as a back-off strategy.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999951333333333">
We described how we extended an existing
monolingual (English) system for CLQA (Eng-
lish to Chinese and English to Japanese), includ-
ing a translation disambiguation technique
which uses a noisy channel model with probabil-
ity estimations using web as corpora. We dis-
cussed the influence of translation accuracy on
CLQA by presenting experimental results and
analysis. We concluded by introducing some
language-specific issues for keyword translation
from English to Chinese and Japanese which we
hope to address in ongoing research.
</bodyText>
<sectionHeader confidence="0.996518" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9532815">
This work is supported by the Advanced Re-
search and Development Activity (ARDA)’s
Advanced Question Answering for Intelligent
(AQUAINT) Program.
</bodyText>
<sectionHeader confidence="0.995662" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865076923077">
Brown, P., J. Cocke, S.D. Pietra, V.D. Pietra, F.
Jelinek., J. Lafferty, R. Mercer, and P. Roossin.
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):38—45.
Isozaki, H., K. Sudoh and H. Tsukada. 2005. NTT’s
Japanese-English Cross-Language Question An-
swering System. In Proceedings of the NTCIR
Workshop 5 Meeting, pages 186-193.
Korhonen, A. and E. Briscoe. 2004. Extended Lexi-
cal-Semantic Classification of English Verbs. Pro-
ceedings of the HLT/NAACL &apos;04 Workshop on
Computational Lexical Semantics, pages 38-45.
Kwok, K., P. Deng, N. Dinstl and S. Choi. 2005.
NTCIR-5 English-Chinese Cross Language Ques-
tion-Answering Experiments using PIRCS. In Pro-
ceedings of the NTCIR Workshop 5 Meeting.
Li, Hang, Yunbo Cao, and Cong Li. 2003. Using Bi-
lingual Web Data To Mine and Rank Translations,
IEEE Intelligent Systems 18(4), pages 54-59.
Mori, T. and M. Kawagishi. 2005. A Method of Cross
Language Question-Answering Based on Machine
Translation and Transliteration. In Proceedings of
the NTCIR Workshop 5 Meeting.
Nagata, N., T. Saito, and K. Suzuki. 2001. Using the
Web as a Bilingual Dictionary, In Proceedings of
ACL 2001 Workshop Data-Driven Methods in Ma-
chine Translation, pages 95-102
Nyberg, E., R. Frederking, T. Mitamura, J. M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita,
V. Pedro, A. Schlaikjer. 2005. JAVELIN I and II in
TREC2005. In Proceedings of TREC 2005.
Ogilvie, P. and J. Callan. 2001. Experiments Using
the Lemur Toolkit. In Proceedings of the 2001 Text
REtrieval Conference (TREC 2001), pages 103-
108.
Turney, P.D. 2001, Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL, Proceedings of the
Twelfth European Conference on Machine Learn-
ing, pages 491-502.
</reference>
<page confidence="0.999352">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342246">
<note confidence="0.586189">EACL 2006 Workshop on Multilingual Question Answering - MLQA06</note>
<title confidence="0.9119415">Keyword Translation Accuracy and Cross-Lingual Answering in Chinese and Japanese</title>
<author confidence="0.999716">Teruko Mitamura Mengqiu Wang Hideki Shima Frank Lin</author>
<affiliation confidence="0.9085255">Carnegie Mellon Carnegie Mellon Carnegie Mellon Carnegie Mellon University University University University</affiliation>
<address confidence="0.993439">Pittsburgh, PA USA Pittsburgh, PA USA Pittsburgh, PA USA Pittsburgh, PA USA</address>
<email confidence="0.999785">teruko@cs.cmu.edumengqiu@cs.cmu.eduhideki@cs.cmu.edufrank+@cs.cmu.edu</email>
<abstract confidence="0.990906">In this paper, we describe the extension of an existing monolingual QA system for English-to-Chinese and English-to- Japanese cross-lingual question answering (CLQA). We also attempt to characterize the influence of translation on CLQA performance through experimental evaluation and analysis. The paper also describes some language-specific issues for keyword translation in CLQA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R Mercer</author>
<author>P Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="7501" citStr="Brown et al., 1990" startWordPosition="1129" endWordPosition="1132"> the World Wide Web. Details of the translation selection method are described in the rest of this subsection. The Noisy Channel Model: In the noisy channel model, an undistorted signal passes through a noisy channel and becomes distorted. Given the distorted signal, we are to find the original, undistorted signal. IBM applied the noisy channel model idea to translation of sentences from aligned parallel corpora, where the source language sentence is the distorted signal, and the 32 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 target language sentence is the original signal (Brown et al., 1990). We adopt this model for disambiguating keyword translation, with the source language keyword terms as the distorted signal and the target language terms as the original signal. The TM&apos;s job is to find the target language terms given the source language terms, by finding the probability of the target language terms given the source language terms P(T|S). Using Bayes&apos; Rule, we can break the equation down to several components: P(T |S) = P(T) ⋅ P(S |T) P(S) Because we are comparing probabilities of different translations of the same source keyword terms, we can simplify the problem to be: P(T |</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, P., J. Cocke, S.D. Pietra, V.D. Pietra, F. Jelinek., J. Lafferty, R. Mercer, and P. Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):38—45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>K Sudoh</author>
<author>H Tsukada</author>
</authors>
<title>NTT’s Japanese-English Cross-Language Question Answering System.</title>
<date>2005</date>
<booktitle>In Proceedings of the NTCIR Workshop 5 Meeting,</booktitle>
<pages>186--193</pages>
<contexts>
<context position="6144" citStr="Isozaki et al., 2005" startWordPosition="912" endWordPosition="915">s less important. We focus more on words that carry more content information, such as verbs and nouns. We will present more detail on the use of contextual information for disambiguation in the next section. In some recent work (Kwok, 2005, Mori and Kawagishi, 2005), researchers have combined these two approaches, but to date no studies have compared their effectiveness. 3.2 Translation Module The Translation Module (TM) is used by the QA module to translate keywords into the language of the target corpus. Instead of combining multiple translation candidates with a disjunctive query operator (Isozaki et al., 2005), the TM selects the best combination of translated keywords from several sources: Machine Readable Dictionaries (MRDs), Machine Translation systems (MTs) and Web-mining-Based Keyword Translators (WBMTs) (Nagata et al., 2001, Li et al., 2003). For translation from English to Japanese, we used two MRDs, eight MTs and one WBMT. If none of them return a translation, the word is transliterated into kana for Japanese (for details on transliteration, see Section 5.2). For translation from English to Chinese, we used one MRD, three MTs and one WBMT. After gathering all possible translations for every</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, 2005</marker>
<rawString>Isozaki, H., K. Sudoh and H. Tsukada. 2005. NTT’s Japanese-English Cross-Language Question Answering System. In Proceedings of the NTCIR Workshop 5 Meeting, pages 186-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>E Briscoe</author>
</authors>
<title>Extended Lexical-Semantic Classification of English Verbs.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT/NAACL &apos;04 Workshop on Computational Lexical Semantics,</booktitle>
<pages>38--45</pages>
<contexts>
<context position="3706" citStr="Korhonen and Briscoe, 2004" startWordPosition="533" endWordPosition="536">language-independent algorithm for answer ranking. The overall cross-lingual architecture is shown in Figure 2. The rest of this section explains the details of each module. 3.1 Question Analyzer The Question Analyzer (QA) is responsible for extracting information from the input question in order to formulate a representation of the 31 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Figure1: Javelin Monolingual Architecture Figure2: Javelin Architecture with Cross-Lingual Extension information required to answer the question. Input questions are processed using the RASP parser (Korhonen and Briscoe, 2004), and the module output contains three main components: a) selected keywords; b) the answer type (e.g. numeric-expression, person-name, location); and c) the answer subtype (e.g. author, river, city). The selected keywords are words or phrases which are expected to appear in documents with correct answers. In order to reduce noise in the document retrieval phase, we use stop-word lists to eliminate high-frequency terms; for example, the term “old” is not included as a keyword for “how-old” questions. We extended the QA module with a keyword translation sub-module, so that translated keywords c</context>
</contexts>
<marker>Korhonen, Briscoe, 2004</marker>
<rawString>Korhonen, A. and E. Briscoe. 2004. Extended Lexical-Semantic Classification of English Verbs. Proceedings of the HLT/NAACL &apos;04 Workshop on Computational Lexical Semantics, pages 38-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kwok</author>
<author>P Deng</author>
<author>N Dinstl</author>
<author>S Choi</author>
</authors>
<title>NTCIR-5 English-Chinese Cross Language Question-Answering Experiments using PIRCS.</title>
<date>2005</date>
<booktitle>In Proceedings of the NTCIR Workshop 5 Meeting.</booktitle>
<marker>Kwok, Deng, Dinstl, Choi, 2005</marker>
<rawString>Kwok, K., P. Deng, N. Dinstl and S. Choi. 2005. NTCIR-5 English-Chinese Cross Language Question-Answering Experiments using PIRCS. In Proceedings of the NTCIR Workshop 5 Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Yunbo Cao</author>
<author>Cong Li</author>
</authors>
<title>Using Bilingual Web Data To Mine and Rank Translations,</title>
<date>2003</date>
<journal>IEEE Intelligent Systems</journal>
<volume>18</volume>
<issue>4</issue>
<pages>54--59</pages>
<contexts>
<context position="6386" citStr="Li et al., 2003" startWordPosition="948" endWordPosition="951">nd Kawagishi, 2005), researchers have combined these two approaches, but to date no studies have compared their effectiveness. 3.2 Translation Module The Translation Module (TM) is used by the QA module to translate keywords into the language of the target corpus. Instead of combining multiple translation candidates with a disjunctive query operator (Isozaki et al., 2005), the TM selects the best combination of translated keywords from several sources: Machine Readable Dictionaries (MRDs), Machine Translation systems (MTs) and Web-mining-Based Keyword Translators (WBMTs) (Nagata et al., 2001, Li et al., 2003). For translation from English to Japanese, we used two MRDs, eight MTs and one WBMT. If none of them return a translation, the word is transliterated into kana for Japanese (for details on transliteration, see Section 5.2). For translation from English to Chinese, we used one MRD, three MTs and one WBMT. After gathering all possible translations for every keyword, the TM uses a noisy channel model to select the best combination of translated keywords. The TM estimates model statistics using the World Wide Web. Details of the translation selection method are described in the rest of this subse</context>
</contexts>
<marker>Li, Cao, Li, 2003</marker>
<rawString>Li, Hang, Yunbo Cao, and Cong Li. 2003. Using Bilingual Web Data To Mine and Rank Translations, IEEE Intelligent Systems 18(4), pages 54-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mori</author>
<author>M Kawagishi</author>
</authors>
<title>A Method of Cross Language Question-Answering Based on Machine Translation and Transliteration.</title>
<date>2005</date>
<booktitle>In Proceedings of the NTCIR Workshop 5 Meeting.</booktitle>
<contexts>
<context position="5789" citStr="Mori and Kawagishi, 2005" startWordPosition="858" endWordPosition="861">hus adding to the complexity of the analysis task. One could argue that when translating the full sentence instead of just the keywords, we can better utilize state-of-art machine translation techniques because more context information is available. But for our application, an accurate translation of functional words (such as prepositions or conjunctions) is less important. We focus more on words that carry more content information, such as verbs and nouns. We will present more detail on the use of contextual information for disambiguation in the next section. In some recent work (Kwok, 2005, Mori and Kawagishi, 2005), researchers have combined these two approaches, but to date no studies have compared their effectiveness. 3.2 Translation Module The Translation Module (TM) is used by the QA module to translate keywords into the language of the target corpus. Instead of combining multiple translation candidates with a disjunctive query operator (Isozaki et al., 2005), the TM selects the best combination of translated keywords from several sources: Machine Readable Dictionaries (MRDs), Machine Translation systems (MTs) and Web-mining-Based Keyword Translators (WBMTs) (Nagata et al., 2001, Li et al., 2003). F</context>
</contexts>
<marker>Mori, Kawagishi, 2005</marker>
<rawString>Mori, T. and M. Kawagishi. 2005. A Method of Cross Language Question-Answering Based on Machine Translation and Transliteration. In Proceedings of the NTCIR Workshop 5 Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nagata</author>
<author>T Saito</author>
<author>K Suzuki</author>
</authors>
<title>Using the Web as a Bilingual Dictionary,</title>
<date>2001</date>
<booktitle>In Proceedings of ACL 2001 Workshop Data-Driven Methods in Machine Translation,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="6368" citStr="Nagata et al., 2001" startWordPosition="944" endWordPosition="947">k (Kwok, 2005, Mori and Kawagishi, 2005), researchers have combined these two approaches, but to date no studies have compared their effectiveness. 3.2 Translation Module The Translation Module (TM) is used by the QA module to translate keywords into the language of the target corpus. Instead of combining multiple translation candidates with a disjunctive query operator (Isozaki et al., 2005), the TM selects the best combination of translated keywords from several sources: Machine Readable Dictionaries (MRDs), Machine Translation systems (MTs) and Web-mining-Based Keyword Translators (WBMTs) (Nagata et al., 2001, Li et al., 2003). For translation from English to Japanese, we used two MRDs, eight MTs and one WBMT. If none of them return a translation, the word is transliterated into kana for Japanese (for details on transliteration, see Section 5.2). For translation from English to Chinese, we used one MRD, three MTs and one WBMT. After gathering all possible translations for every keyword, the TM uses a noisy channel model to select the best combination of translated keywords. The TM estimates model statistics using the World Wide Web. Details of the translation selection method are described in the </context>
</contexts>
<marker>Nagata, Saito, Suzuki, 2001</marker>
<rawString>Nagata, N., T. Saito, and K. Suzuki. 2001. Using the Web as a Bilingual Dictionary, In Proceedings of ACL 2001 Workshop Data-Driven Methods in Machine Translation, pages 95-102</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>R Frederking</author>
<author>T Mitamura</author>
<author>J M Bilotti</author>
<author>K Hannan</author>
<author>L Hiyakumoto</author>
<author>J Ko</author>
<author>F Lin</author>
<author>L Lita</author>
<author>V Pedro</author>
<author>A Schlaikjer</author>
</authors>
<title>JAVELIN I and II in TREC2005.</title>
<date>2005</date>
<booktitle>In Proceedings of TREC</booktitle>
<contexts>
<context position="990" citStr="Nyberg, et al., 2005" startWordPosition="127" endWordPosition="130"> Pittsburgh, PA USA teruko@cs.cmu.edu mengqiu@cs.cmu.edu hideki@cs.cmu.edu frank+@cs.cmu.edu Abstract In this paper, we describe the extension of an existing monolingual QA system for English-to-Chinese and English-toJapanese cross-lingual question answering (CLQA). We also attempt to characterize the influence of translation on CLQA performance through experimental evaluation and analysis. The paper also describes some language-specific issues for keyword translation in CLQA. 1 Introduction The JAVELIN system is a modular, extensible architecture for building question-answering (QA) systems (Nyberg, et al., 2005). Since the JAVELIN architecture is language-independent, we extended the original English version of JAVELIN for cross-language question answering (CLQA) in Chinese and Japanese. The same overall architecture was used for both systems, allowing us to compare the performance of the two systems. In this paper, we describe how we extended the monolingual system for CLQA (see Section 3). Keyword translation is a crucial element of the system; we describe our translation module in Section 3.2. In Section 4, we evaluate the end-to-end CLQA systems using three different translation methods. Language</context>
<context position="20192" citStr="Nyberg, et al. 2005" startWordPosition="3293" endWordPosition="3296">okenization, whereas for the E-J, we use word-based tokenization. By heuristics obtained from training results, we used the linear Dist measure for E-C and logarithmic Dist measure for E-J in the evaluation. This algorithm is a simple statistical approach which requires no language-specific external tools beyond word segmentation and a namedentity tagger. It is not as sophisticated as other approaches which perform deep linguistic analysis, but one advantage is faster adaptation to multiple languages. In our experiments, this simple algorithm performs at the same level as a FSTbased approach (Nyberg, et al. 2005). 3.5 Answer Generator The task of the Answer Generator (AG) module is to produce a ranked list of answer candidates from the IX output. The AG is designed to normalize answer candidates by resolving representational differences (e.g. in how numbers, dates, etc. are expressed in text). This canonicalization makes it possible to combine answer candidates that differ only in surface form. Even though the AG module plays an important role in JAVELIN, we did not use its full potential in our E-C and E-J systems, since we lacked some language-specific resources required for multilingual answer merg</context>
</contexts>
<marker>Nyberg, Frederking, Mitamura, Bilotti, Hannan, Hiyakumoto, Ko, Lin, Lita, Pedro, Schlaikjer, 2005</marker>
<rawString>Nyberg, E., R. Frederking, T. Mitamura, J. M. Bilotti, K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita, V. Pedro, A. Schlaikjer. 2005. JAVELIN I and II in TREC2005. In Proceedings of TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ogilvie</author>
<author>J Callan</author>
</authors>
<title>Experiments Using the Lemur Toolkit.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Text REtrieval Conference (TREC</booktitle>
<pages>103--108</pages>
<contexts>
<context position="16291" citStr="Ogilvie and Callan, 2001" startWordPosition="2613" endWordPosition="2616">rall 灌木离去伊拉克 2.1562E-1 1.0428E-4 2.2483E-5 灌木叶子伊拉克 2.2122E-1 4.0925E-5 9.0533E-6 布什离去伊拉克 2.7797E-1 1.4993E-3 4.1675E-4 布什叶子伊拉克 2.8520E-1 2.1616E-4 6.1649E-5 Table 6. Translation Score, Language Model Score, and Overall Score As shown in Table 6, we select the most probable combination of translated keywords with the highest overall score (the third candidate), which is the correct translation of the English keywords. 3.3 Retrieval Strategies The Retrieval Strategist (RS) module retrieves documents from a corpus in response to a query. For document retrieval, the RS uses the Lemur 3.0 toolkit (Ogilvie and Callan, 2001). Lemur supports structured queries using operators such as Boolean AND, Synonym, Ordered/UnOrdered Window and NOT. An example of a structured query is shown below: 1 For simplicity, we don’t apply smoothing and pruning. #BAND( #OD4(邪馬台国 王朝) 女王 #SYN(*organization *person) ) In formulating a structured query, the RS uses an incremental relaxation technique, starting from an initial query that is highly constrained; the algorithm searches for all the keywords and data types in close proximity to each other. The priority is based on a function of the likely answer type, keyword type (word, proper</context>
</contexts>
<marker>Ogilvie, Callan, 2001</marker>
<rawString>Ogilvie, P. and J. Callan. 2001. Experiments Using the Lemur Toolkit. In Proceedings of the 2001 Text REtrieval Conference (TREC 2001), pages 103-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL,</title>
<date>2001</date>
<booktitle>Proceedings of the Twelfth European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="9347" citStr="Turney, 2001" startWordPosition="1441" endWordPosition="1442">ts written in many different languages, 2) it has high coverage of virtually all types of words and phrases, and 3) it is constantly updated. However, we also note that the web contains a lot of noisy data, and building up web statistics is time-consuming unless one has direct access to a web search index. Estimating Translation Model Probabilities: We make an assumption that terms that are translations of each other co-occur more often in mixed-language web pages than terms that are not translations of each other. This assumption is analogous to Turney’s work on the cooccurrence of synonyms (Turney, 2001). We then define the translation probability of each keyword translation as: j Where si is the i-th term in the source language and ti,j is the j-th translation candidate for si. Let hits be a number of web pages retrieved from a certain search engine. co(si, t i,j) is the hits given a query si and ti,j., where log is applied to adjust the count so that translation probabilities can still be comparable at higher counts. Estimating Language Model Probabilities: In estimating the language model, we simply obtain hits given a conjunction of all the candidate terms in the target language, and divi</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, P.D. 2001, Mining the Web for synonyms: PMI-IR versus LSA on TOEFL, Proceedings of the Twelfth European Conference on Machine Learning, pages 491-502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>