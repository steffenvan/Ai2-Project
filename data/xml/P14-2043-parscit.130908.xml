<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.9938955">
Part-of-Speech Tagging using Conditional Random Fields: Exploiting
Sub-Label Dependencies for Improved Accuracy
</title>
<author confidence="0.9179">
Miikka Silfverberga Teemu Ruokolainenb Krister Lindéna Mikko Kurimob
</author>
<affiliation confidence="0.765558">
a Department of Modern Languages, University of Helsinki,
firstname.lastname@helsinki.fi
b Department of Signal Processing and Acoustics, Aalto University,
</affiliation>
<email confidence="0.988498">
firstname.lastname@aalto.fi
</email>
<sectionHeader confidence="0.994405" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888642857143">
We discuss part-of-speech (POS) tagging
in presence of large, fine-grained la-
bel sets using conditional random fields
(CRFs). We propose improving tagging
accuracy by utilizing dependencies within
sub-components of the fine-grained labels.
These sub-label dependencies are incor-
porated into the CRF model via a (rela-
tively) straightforward feature extraction
scheme. Experiments on five languages
show that the approach can yield signifi-
cant improvement in tagging accuracy in
case the labels have sufficiently rich inner
structure.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999846333333334">
We discuss part-of-speech (POS) tagging using
the well-known conditional random field (CRF)
model introduced originally by Lafferty et al.
(2001). Our focus is on scenarios, in which the
POS labels have a rich inner structure. For exam-
ple, consider
</bodyText>
<equation confidence="0.805292666666667">
PRON+1SG V+NON3SG+PRES N+SG
ham ,
I like
</equation>
<bodyText confidence="0.999975848484849">
where the compound labels PRON+1SG,
V+NON3SG+PRES, and N+SG stand for pro-
noun first person singular, verb non-third singular
present tense, and noun singular, respectively.
Fine-grained labels occur frequently in mor-
phologically complex languages (Erjavec, 2010;
Haverinen et al., 2013).
We propose improving tagging accuracy by uti-
lizing dependencies within the sub-labels (PRON,
1SG, V, NON3SG, N, and SG in the above ex-
ample) of the compound labels. From a technical
perspective, we accomplish this by making use of
the fundamental ability of the CRFs to incorporate
arbitrarily defined feature functions. The newly-
defined features are expected to alleviate data spar-
sity problems caused by the fine-grained labels.
Despite the (relative) simplicity of the approach,
we are unaware of previous work exploiting the
sub-labels to the extent presented here.
We present experiments on five languages (En-
glish, Finnish, Czech, Estonian, and Romanian)
with varying POS annotation granularity. By uti-
lizing the sub-labels, we gain significant improve-
ment in model accuracy given a sufficiently fine-
grained label set. Moreover, our results indi-
cate that exploiting the sub-labels can yield larger
improvements in tagging compared to increasing
model order.
The rest of the paper is organized as follows.
Section 2 describes the methodology. Experimen-
tal setup and results are presented in Section 3.
Section 4 discusses related work. Lastly, we pro-
vide conclusions on the work in Section 5.
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.994082">
2.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.971838666666667">
The (unnormalized) CRF model (Lafferty et al.,
2001) for a sentence x = (x1, ... , x|x|) and a POS
sequence y = (y1, ... , y|x|) is defined as
</bodyText>
<equation confidence="0.958188">
( )
exp w&apos;φ(yi−n, . . . , yi, x, i) ,
(1)
</equation>
<bodyText confidence="0.99502475">
where n denotes the model order, w the model pa-
rameter vector, and φ the feature extraction func-
tion. We denote the tag set as Y, that is, yi E Y
for i E 1... |x|.
</bodyText>
<subsectionHeader confidence="0.999137">
2.2 Baseline Feature Set
</subsectionHeader>
<bodyText confidence="0.5806582">
We first describe our baseline feature set
�φj(yi−1, yi,x, i)1|φ|
j=1 by defining emission and
transition features. The emission feature set as-
sociates properties of the sentence position i with
</bodyText>
<equation confidence="0.936203333333333">
|x|
p(y |x;w) a H
i=n
</equation>
<page confidence="0.945579">
259
</page>
<bodyText confidence="0.931310333333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 259–264,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
{PRON, 1, SG}. (Label partitions employed in
the experiments are described in Section 3.2.) We
denote the set of all sub-label components as S.
Subsequently, instead of defining only (2), we
additionally associate the feature functions X with
all sub-labels s E S by defining
the corresponding label as
</bodyText>
<equation confidence="0.85368">
{χj(x,i)1(yi = y0 i)|j E 1 ... |X|,by0 i E Y},
(2)
</equation>
<bodyText confidence="0.997675">
where the function 1(q) returns one if and only if
the proposition q is true and zero otherwise, that is
</bodyText>
<equation confidence="0.99089725">
0) _ f 1 if yi = y0
1
(yi = yi l 0 otherwise , (3)
and X = {χj(x, i)}|X |
</equation>
<bodyText confidence="0.98523325">
j=1 is the set of functions
characterizing the word position i. Following the
classic work of Ratnaparkhi (1996), our X com-
prises simple binary functions:
</bodyText>
<listItem confidence="0.987241166666667">
1. Bias (always active irrespective of input).
2. Word forms xi−2, ... , xi+2.
3. Prefixes and suffixes of the word form xi up
to length δsuf = 4.
4. If the word form xi contains (one or more)
capital letter, hyphen, dash, or digit.
</listItem>
<bodyText confidence="0.994923666666667">
Binary functions have a return value of either zero
(inactive) or one (active). Meanwhile, the transi-
tion features
</bodyText>
<equation confidence="0.991492">
{1(yi−k = y0i−k) ... 1(yi = y0i) |
y0i−k, ... , y0i E Y , bk E 1... n} (4)
</equation>
<bodyText confidence="0.999958">
capture dependencies between adjacent labels ir-
respective of the input x.
</bodyText>
<subsectionHeader confidence="0.826058">
2.2.1 Expanded Feature Set Leveraging
Sub-Label Dependencies
</subsectionHeader>
<bodyText confidence="0.972907590909091">
The baseline feature set described above can yield
a high tagging accuracy given a conveniently sim-
ple label set, exemplified by the tagging results
of Collins (2002) on the Penn Treebank (Mar-
cus et al., 1993). (Note that conditional random
fields correspond to discriminatively trained hid-
den Markov models and Collins (2002) employs
the latter terminology.) However, it does to some
extent overlook some beneficial dependency infor-
mation in case the labels have a rich sub-structure.
In what follows, we describe expanded feature sets
which explicitly model the sub-label dependen-
cies.
We begin by defining a function P(yi) which
partitions any label yi into its sub-label compo-
nents and returns them in an unordered set. For
example, we could define P(PRON+1+SG) =
{χj(x, i)1(s E P(yi))  |bj E 1... |X  |, bs E S},
(5)
where 1(s E P(yi)) returns one in case s is in
P(yi) and zero otherwise. Second, we exploit sub-
label transitions using features
</bodyText>
<equation confidence="0.9707685">
{1(si−k E P(yi−k)) ... 1(si E P(yi)) |
bsi−k, ... , si E S , bk E 1... m}. (6)
</equation>
<bodyText confidence="0.999724625">
Note that we define the sub-label transitions up
to order m, 1 &lt; m &lt; n, that is, an nth-order
CRF model is not obliged to utilize sub-label tran-
sitions all the way up to order n. This is be-
cause employing high-order sub-label transitions
may potentially cause overfitting to training data
due to substantially increased number of features
(equivalent to the number of model parameters,
|w |= |φ|). For example, in a second-order
(n = 2) model, it might be beneficial to em-
ploy the sub-label emission feature set (5) and
first-order sub-label transitions while discarding
second-order sub-label transitions. (See the exper-
imental results presented in Section 3.)
In the remainder of this paper, we use the fol-
lowing notations.
</bodyText>
<listItem confidence="0.999918333333333">
1. A standard CRF model incorporating (2) and
(4) is denoted as CRF(n,-).
2. A CRF model incorporating (2), (4), and (5)
is denoted as CRF(n,0).
3. A CRF model incorporating (2), (4), (5), and
(6) is denoted as CRF(n,m).
</listItem>
<subsectionHeader confidence="0.997197">
2.3 On Linguistic Intuition
</subsectionHeader>
<bodyText confidence="0.9999944">
This section aims to provide some intuition on the
types of linguistic phenomena that can be captured
by the expanded feature set. To this end, we con-
sider an example on the plural number in Finnish.
First, consider the plural nominative word form
kissat (cats) where the plural number is denoted
by the 1-suffix -t. Then, by employing the features
(2), the suffix -t is associated solely with the com-
pound label NOMINATIVE+PLURAL. However,
by incorporating the expanded feature set (5), -t
</bodyText>
<page confidence="0.960955">
260
</page>
<bodyText confidence="0.999919681818182">
will also be associated to the sub-label PLURAL.
This can be useful because, in Finnish, also adjec-
tives and numerals are inflected according to num-
ber and denote the plural number with the suffix
-t (Hakulinen et al., 2004, §79). Therefore, one
can exploit -t to predict the plural number also in
words such as mustat (plural of black) with a com-
pound analysis ADJECTIVE+PLURAL.
Second, consider the number agreement (con-
gruence). For example, in the sentence fragment
mustat kissat juoksevat (black cats are running),
the words mustat and kissat share the plural num-
ber. In other words, the analyses of both mustat
and kissat are required to contain the sub-label
PLURAL. This short-span dependency between
sub-labels will be captured by a first-order sub-
label transition feature included in (6).
Lastly, we note that the feature expansion sets
(5) and (6) will, naturally, capture any short-span
dependencies within the sub-labels irrespective if
the dependencies have a clear linguistic interpre-
tation or not.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999684">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.991040791666667">
For a quick overview of the data sets, see Table 1.
Penn Treebank. The English Penn Treebank
(Marcus et al., 1993) is divided into 25 sections
of newswire text extracted from the Wall Street
Journal. We split the data into training, develop-
ment, and test sets using the sections 0-18, 19-21,
and 22-24, according to the standardly applied di-
vision introduced by Collins (2002).
Turku Depedency Treebank. The Finnish
Turku Depedendency Treebank (Haverinen et al.,
2013) contains text from 10 different domains.
The treebank does not have default partition to
training and test sets. Therefore, from each 10
consecutive sentences, we assign the 9th and 10th
to the development set and the test set, respec-
tively. The remaining sentences are assigned to
the training set.
Multext-East. The third data we consider is the
multilingual Multext-East (Erjavec, 2010) corpus,
from which we utilize the Czech, Estonian and Ro-
manian sections. The corpus corresponds to trans-
lations of the novel 1984 by George Orwell. We
apply the same data splits as for Turku Depen-
dency Treebank.
</bodyText>
<table confidence="0.9657455">
lang. train. dev. test tags train. tags
Eng 38,219 5,527 5,462 45 45
Rom 5,216 652 652 405 391
Est 5,183 648 647 413 408
Cze 5,402 675 675 955 908
Fin 5,043 630 630 2,355 2,141
</table>
<tableCaption confidence="0.6472952">
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
</tableCaption>
<subsectionHeader confidence="0.999412">
3.2 Label Partitions
</subsectionHeader>
<bodyText confidence="0.999961157894737">
This section describes the employed compound la-
bel splits. The label splits for all data sets are sub-
mitted as data file attachments. All the splits are
performed a priori to model learning, that is, we
do not try to optimize them on the development
sets.
The POS labels in the Penn Treebank are split
in a way which captures relevant inflectional cat-
egories, such as tense and number. Consider, for
example, the split for the present tense third sin-
gular verb label P(VBZ) _ {VB, Z}.
In the Turku Dependency Treebank, each
morphological tag consists of sub-labels mark-
ing word-class, relevant inflectional categories,
and their respective values. Each inflec-
tional category, such as case or tense, com-
bined with its value, such as nominative or
present, constitutes one sub-label. Consider,
for example, the split for the singular, adessive
noun P(N+CASE_ADE+NUM_SG) _ {POS_N,
CASE_ADE, NUM_SG}.
The labeling scheme employed in the Multext-
East data set represents a considerably different
annotation approach compared to the Penn and
Turku Treebanks. Each morphological analysis is
a sequence of feature markers, for example Pw3–
r. The first feature marker (P) denotes word class
and the rest (w, 3, and r) encode values of inflec-
tional categories relevant for that word class. A
feature marker may correspond to several differ-
ent values depending on word class and its posi-
tion in the analysis. Therefore it becomes rather
difficult to split the labels into similar pairs of in-
flectional category and value as we are able to do
for the Turku Dependency Treebank. Since the in-
terpretation of a feature marker depends on its po-
sition in the analysis and the word class, the mark-
ers have to be numbered and appended with the
</bodyText>
<page confidence="0.979357">
261
</page>
<bodyText confidence="0.493445">
word class marker. For example, consider the split
P(Pw3–r) = {0 : P, 1 : Pw, 2 : P3, 5 : Pr}.
</bodyText>
<subsectionHeader confidence="0.885342">
3.3 CRF Model Specification
</subsectionHeader>
<bodyText confidence="0.9999849">
We perform experiments using first-order and
second-order CRFs with zeroth-order and first-
order sub-label features. Using the notation
introduced in Section 2, the employed mod-
els are CRF(1,-), CRF(1,1), CRF(2,-), CRF(2,0),
and CRF(2,1). We do not report results us-
ing CRF(2,2) since, based on preliminary exper-
iments, this model overfits on all languages.
The CRF model parameters are estimated using
the averaged perceptron algorithm (Collins, 2002).
The model parameters are initialized with a zero
vector. We evaluate the latest averaged parameters
on the held-out development set after each pass
over the training data and terminate training if no
improvement in accuracy is obtained during three
last passes. The best-performing parameters are
then applied on the test instances.
We accelerate the perceptron learning using
beam search (Zhang and Clark, 2011). The beam
width, b, is optimized separately for each lan-
guage on the development sets by considering b =
1, 2, 4, 8, 16, 32, 64,128 until the model accuracy
does not improve by at least 0.01 (absolute).
Development and test instances are decoded us-
ing Viterbi search in combination with the tag dic-
tionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
</bodyText>
<subsectionHeader confidence="0.988115">
3.4 Software and Hardware
</subsectionHeader>
<bodyText confidence="0.99996075">
The experiments are run on a standard desktop
computer (Intel Xeon E5450 with 3.00 GHz and
64 GB of memory). The methods discussed in
Section 2 are implemented in C++.
</bodyText>
<sectionHeader confidence="0.899378" genericHeader="evaluation">
3.5 Results
</sectionHeader>
<bodyText confidence="0.999978611111111">
The obtained tagging accuracies and training
times are presented in Table 2. The times in-
clude running the averaged perceptron algorithm
and evaluation of the development sets. The col-
umn labeled it. corresponds to the number of
passes over the training data made by the percep-
tron algorithm before termination. We summarize
the results as follows.
First, compared to standard feature extraction
approach, employing the sub-label transition fea-
tures resulted in improved accuracy on all lan-
guages apart from English. The differences were
statistically significant on Czech, Estonian, and
Finnish. (We establish statistical significance
(with confidence level 0.95) using the standard 1-
sided Wilcoxon signed-rank test performed on 10
randomly divided, non-overlapping subsets of the
complete test sets.) This results supports the in-
tuition that the sub-label features should be most
useful in presence of large, fine-grained label sets,
in which case the learning is most affected by data
sparsity.
Second, on all languages apart from English,
employing a first-order model with sub-label fea-
tures yielded higher accuracy compared to a
second-order model with standard features. The
differences were again statistically significant on
Czech, Estonian, and Finnish. This result suggests
that, compared to increasing model order, exploit-
ing the sub-label dependencies can be a preferable
approach to improve the tagging accuracy.
Third, applying the expanded feature set in-
evitably causes some increase in the computa-
tional cost of model estimation. However, as
shown by the running times, this increase is not
prohibitive.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999900869565217">
In this section, we compare the approach pre-
sented in Section 2 to two prior systems which at-
tempt to utilize sub-label dependencies in a similar
manner.
Smith et al. (2005) use a CRF-based system
for tagging Czech, in which they utilize expanded
emission features similar to our (5). However, they
do not utilize the full expanded transition features
(6). More specifically, instead of utilizing a sin-
gle chain as in our approach, Smith et al. employ
five parallel structured chains. One of the chains
models the sequence of word-class labels such as
noun and adjective. The other four chains model
gender, number, case, and lemma sequences, re-
spectively. Therefore, in contrast to our approach,
their system does not capture cross-dependencies
between inflectional categories, such as the de-
pendence between the word-class and case of ad-
jacent words. Unsurprisingly, Smith et al. fail
to achieve improvement over a generative HMM-
based POS tagger of Hajiˇc (2001). Meanwhile,
our system outperforms the generative trigram tag-
ger HunPos (Halácsy et al., 2007) which is an im-
</bodyText>
<page confidence="0.990882">
262
</page>
<table confidence="0.999516444444445">
model it. time (min) acc. OOV.
English
CRF(1, -) 8 9 97.04 88.65
CRF(1, 0) 6 17 97.02 88.44
CRF(1, 1) 8 22 97.02 88.82
CRF(2, -) 9 15 97.18 88.82
CRF(2, 0) 11 36 97.17 89.23
CRF(2, 1) 8 27 97.15 89.04
Romanian
CRF(1, -) 14 29 97.03 85.01
CRF(1, 0) 13 68 96.96 84.59
CRF(1, 1) 16 146 97.24 85.94
CRF(2, -) 7 19 97.08 85.21
CRF(2, 0) 18 99 97.02 85.42
CRF(2, 1) 12 118 97.29 86.25
Estonian
CRF(1, -) 15 28 93.39 78.66
CRF(1, 0) 17 66 93.81 80.44
CRF(1, 1) 13 129 93.77 79.37
CRF(2, -) 15 30 93.48 77.13
CRF(2, 0) 13 53 93.78 79.60
CRF(2, 1) 16 105 94.01 79.53
Czech
CRF(1, -) 6 28 89.28 70.90
CRF(1, 0) 10 112 89.94 74.44
CRF(1, 1) 10 365 90.78 76.83
CRF(2, -) 19 91 89.81 72.44
CRF(2, 0) 13 203 90.35 76.37
CRF(2, 1) 24 936 91.00 77.75
Finnish
CRF(1, -) 10 80 87.37 59.29
CRF(1, 0) 13 249 88.58 63.46
CRF(1, 1) 12 474 88.41 62.63
CRF(2, -) 11 106 86.74 56.96
CRF(2, 0) 13 272 88.52 63.46
CRF(2, 1) 12 331 88.68 63.62
</table>
<tableCaption confidence="0.997817">
Table 2: Results.
</tableCaption>
<bodyText confidence="0.911707">
proved open-source implementation of the well-
known TnT tagger of Brants (2000). The obtained
HunPos results are presented in Table 3.
</bodyText>
<table confidence="0.4210705">
Eng Rom Est Cze Fin
HunPos 96.58 96.96 92.76 89.57 85.77
</table>
<tableCaption confidence="0.9634625">
Table 3: Results using a generative HMM-based
HunPos tagger of Halacsy et al. (2007).
</tableCaption>
<bodyText confidence="0.9990645625">
Ceau¸su (2006) uses a maximum entropy
Markov model (MEMM) based system for tag-
ging Romanian which utilizes transitional behav-
ior between sub-labels similarly to our feature set
(6). However, in addition to ignoring the most in-
formative emission-type features (5), Ceau¸su em-
beds the MEMMs into the tiered tagging frame-
work of Tufis (1999). In tiered tagging, the full
morphological analyses are mapped into a coarser
tag set and a tagger is trained for this reduced tag
set. Subsequent to decoding, the coarser tags are
mapped into the original fine-grained morpholog-
ical analyses. There are several problems associ-
ated with this tiered tagging approach. First, the
success of the approach is highly dependent on a
well designed coarse label set. Consequently, it
requires intimate knowledge of the tag set and lan-
guage. Meanwhile, our model can be set up with
relatively little prior knowledge of the language
or the tagging scheme (see Section 3.2). More-
over, a conversion to a coarser label set is neces-
sarily lossy (at least for OOV words) and poten-
tially results in reduced accuracy since recovering
the original fine-grained tags from the coarse tags
may induce errors. Indeed, the accuracy 96.56, re-
ported by Ceau¸su on the Romanian section of the
Multext-East data set, is substantially lower than
the accuracy 97.29 we obtain. These accuracies
were obtained using identical sized training and
test sets (although direct comparison is impossible
because Ceau¸su uses a non-documented random
split).
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999990785714286">
We studied improving the accuracy of CRF-based
POS tagging by exploiting sub-label dependency
structure. The dependencies were included in the
CRF model using a relatively straightforward fea-
ture expansion scheme. Experiments on five lan-
guages showed that the approach can yield signif-
icant improvement in tagging accuracy given suf-
ficiently fine-grained label sets.
In future work, we aim to perform a more
fine-grained error analysis to gain a better under-
standing where the improvement in accuracy takes
place. One could also attempt to optimize the
compound label splits to maximize prediction ac-
curacy instead of applying a priori partitions.
</bodyText>
<sectionHeader confidence="0.996176" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996945166666667">
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the grant no
251170 (Finnish Centre of Excellence Program
(2012-2017)). We would like to thank the anony-
mous reviewers for their useful comments.
</bodyText>
<page confidence="0.994512">
263
</page>
<bodyText confidence="0.8977625">
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
475–482.
</bodyText>
<sectionHeader confidence="0.85234" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.965870116666667">
Thorsten Brants. 2000. Tnt: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing,
pages 224–231.
A. Ceausu. 2006. Maximum entropy tiered tagging.
In The 11th ESSLI Student session, pages 173–179.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1–8.
Toma˘z Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10).
Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva, and
Vladimír Petkeviˇc. 2001. Serial combination of
rules and statistics: A case study in czech tagging.
In Proceedings of the 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 268–
275.
Auli Hakulinen, Maria Vilkuna, Riitta Korhonen, Vesa
Koivisto, Tarja Riitta Heinonen, and Irja Alho.
2004. Iso suomen kielioppi. Suomalaisen Kirjal-
lisuuden Seura, Helsinki, Finland.
Péter Halácsy, András Kornai, and Csaba Oravecz.
2007. Hunpos: An open source trigram tagger. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 209–212.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missilä,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguis-
tics, 19(2):313–330.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natu-
ral language processing, volume 1, pages 133–142.
Philadelphia, PA.
Dan Tufis. 1999. Tiered tagging and combined lan-
guage models classifiers. In Proceedings of the Sec-
ond International Workshop on Text, Speech and Di-
alogue, pages 28–33.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
</reference>
<page confidence="0.998213">
264
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.096487">
<title confidence="0.988766">Part-of-Speech Tagging using Conditional Random Fields:</title>
<author confidence="0.34685">Sub-Label Dependencies for Improved Accuracy</author>
<abstract confidence="0.884384368421053">of Modern Languages, University of firstname.lastname@helsinki.fi of Signal Processing and Acoustics, Aalto firstname.lastname@aalto.fi Abstract We discuss part-of-speech (POS) tagging in presence of large, fine-grained label sets using conditional random fields (CRFs). We propose improving tagging accuracy by utilizing dependencies within sub-components of the fine-grained labels. These sub-label dependencies are incorporated into the CRF model via a (relatively) straightforward feature extraction scheme. Experiments on five languages show that the approach can yield significant improvement in tagging accuracy in case the labels have sufficiently rich inner structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Tnt: A statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="17030" citStr="Brants (2000)" startWordPosition="2816" endWordPosition="2817">5 28 93.39 78.66 CRF(1, 0) 17 66 93.81 80.44 CRF(1, 1) 13 129 93.77 79.37 CRF(2, -) 15 30 93.48 77.13 CRF(2, 0) 13 53 93.78 79.60 CRF(2, 1) 16 105 94.01 79.53 Czech CRF(1, -) 6 28 89.28 70.90 CRF(1, 0) 10 112 89.94 74.44 CRF(1, 1) 10 365 90.78 76.83 CRF(2, -) 19 91 89.81 72.44 CRF(2, 0) 13 203 90.35 76.37 CRF(2, 1) 24 936 91.00 77.75 Finnish CRF(1, -) 10 80 87.37 59.29 CRF(1, 0) 13 249 88.58 63.46 CRF(1, 1) 12 474 88.41 62.63 CRF(2, -) 11 106 86.74 56.96 CRF(2, 0) 13 272 88.52 63.46 CRF(2, 1) 12 331 88.68 63.62 Table 2: Results. proved open-source implementation of the wellknown TnT tagger of Brants (2000). The obtained HunPos results are presented in Table 3. Eng Rom Est Cze Fin HunPos 96.58 96.96 92.76 89.57 85.77 Table 3: Results using a generative HMM-based HunPos tagger of Halacsy et al. (2007). Ceau¸su (2006) uses a maximum entropy Markov model (MEMM) based system for tagging Romanian which utilizes transitional behavior between sub-labels similarly to our feature set (6). However, in addition to ignoring the most informative emission-type features (5), Ceau¸su embeds the MEMMs into the tiered tagging framework of Tufis (1999). In tiered tagging, the full morphological analyses are mapped</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. Tnt: A statistical part-ofspeech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ceausu</author>
</authors>
<title>Maximum entropy tiered tagging.</title>
<date>2006</date>
<booktitle>In The 11th ESSLI Student session,</booktitle>
<pages>173--179</pages>
<marker>Ceausu, 2006</marker>
<rawString>A. Ceausu. 2006. Maximum entropy tiered tagging. In The 11th ESSLI Student session, pages 173–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<volume>10</volume>
<pages>1--8</pages>
<contexts>
<context position="4956" citStr="Collins (2002)" startWordPosition="797" endWordPosition="798"> word form xi up to length δsuf = 4. 4. If the word form xi contains (one or more) capital letter, hyphen, dash, or digit. Binary functions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features {1(yi−k = y0i−k) ... 1(yi = y0i) | y0i−k, ... , y0i E Y , bk E 1... n} (4) capture dependencies between adjacent labels irrespective of the input x. 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies The baseline feature set described above can yield a high tagging accuracy given a conveniently simple label set, exemplified by the tagging results of Collins (2002) on the Penn Treebank (Marcus et al., 1993). (Note that conditional random fields correspond to discriminatively trained hidden Markov models and Collins (2002) employs the latter terminology.) However, it does to some extent overlook some beneficial dependency information in case the labels have a rich sub-structure. In what follows, we describe expanded feature sets which explicitly model the sub-label dependencies. We begin by defining a function P(yi) which partitions any label yi into its sub-label components and returns them in an unordered set. For example, we could define P(PRON+1+SG) </context>
<context position="8700" citStr="Collins (2002)" startWordPosition="1426" endWordPosition="1427">Lastly, we note that the feature expansion sets (5) and (6) will, naturally, capture any short-span dependencies within the sub-labels irrespective if the dependencies have a clear linguistic interpretation or not. 3 Experiments 3.1 Data For a quick overview of the data sets, see Table 1. Penn Treebank. The English Penn Treebank (Marcus et al., 1993) is divided into 25 sections of newswire text extracted from the Wall Street Journal. We split the data into training, development, and test sets using the sections 0-18, 19-21, and 22-24, according to the standardly applied division introduced by Collins (2002). Turku Depedency Treebank. The Finnish Turku Depedendency Treebank (Haverinen et al., 2013) contains text from 10 different domains. The treebank does not have default partition to training and test sets. Therefore, from each 10 consecutive sentences, we assign the 9th and 10th to the development set and the test set, respectively. The remaining sentences are assigned to the training set. Multext-East. The third data we consider is the multilingual Multext-East (Erjavec, 2010) corpus, from which we utilize the Czech, Estonian and Romanian sections. The corpus corresponds to translations of th</context>
<context position="12153" citStr="Collins, 2002" startWordPosition="2001" endWordPosition="2002">o be numbered and appended with the 261 word class marker. For example, consider the split P(Pw3–r) = {0 : P, 1 : Pw, 2 : P3, 5 : Pr}. 3.3 CRF Model Specification We perform experiments using first-order and second-order CRFs with zeroth-order and firstorder sub-label features. Using the notation introduced in Section 2, the employed models are CRF(1,-), CRF(1,1), CRF(2,-), CRF(2,0), and CRF(2,1). We do not report results using CRF(2,2) since, based on preliminary experiments, this model overfits on all languages. The CRF model parameters are estimated using the averaged perceptron algorithm (Collins, 2002). The model parameters are initialized with a zero vector. We evaluate the latest averaged parameters on the held-out development set after each pass over the training data and terminate training if no improvement in accuracy is obtained during three last passes. The best-performing parameters are then applied on the test instances. We accelerate the perceptron learning using beam search (Zhang and Clark, 2011). The beam width, b, is optimized separately for each language on the development sets by considering b = 1, 2, 4, 8, 16, 32, 64,128 until the model accuracy does not improve by at least</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), volume 10, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toma˘z Erjavec</author>
</authors>
<title>Multext-east version 4: Multilingual morphosyntactic specifications, lexicons and corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="1473" citStr="Erjavec, 2010" startWordPosition="196" endWordPosition="197">labels have sufficiently rich inner structure. 1 Introduction We discuss part-of-speech (POS) tagging using the well-known conditional random field (CRF) model introduced originally by Lafferty et al. (2001). Our focus is on scenarios, in which the POS labels have a rich inner structure. For example, consider PRON+1SG V+NON3SG+PRES N+SG ham , I like where the compound labels PRON+1SG, V+NON3SG+PRES, and N+SG stand for pronoun first person singular, verb non-third singular present tense, and noun singular, respectively. Fine-grained labels occur frequently in morphologically complex languages (Erjavec, 2010; Haverinen et al., 2013). We propose improving tagging accuracy by utilizing dependencies within the sub-labels (PRON, 1SG, V, NON3SG, N, and SG in the above example) of the compound labels. From a technical perspective, we accomplish this by making use of the fundamental ability of the CRFs to incorporate arbitrarily defined feature functions. The newlydefined features are expected to alleviate data sparsity problems caused by the fine-grained labels. Despite the (relative) simplicity of the approach, we are unaware of previous work exploiting the sub-labels to the extent presented here. We </context>
<context position="9182" citStr="Erjavec, 2010" startWordPosition="1499" endWordPosition="1500">, and test sets using the sections 0-18, 19-21, and 22-24, according to the standardly applied division introduced by Collins (2002). Turku Depedency Treebank. The Finnish Turku Depedendency Treebank (Haverinen et al., 2013) contains text from 10 different domains. The treebank does not have default partition to training and test sets. Therefore, from each 10 consecutive sentences, we assign the 9th and 10th to the development set and the test set, respectively. The remaining sentences are assigned to the training set. Multext-East. The third data we consider is the multilingual Multext-East (Erjavec, 2010) corpus, from which we utilize the Czech, Estonian and Romanian sections. The corpus corresponds to translations of the novel 1984 by George Orwell. We apply the same data splits as for Turku Dependency Treebank. lang. train. dev. test tags train. tags Eng 38,219 5,527 5,462 45 45 Rom 5,216 652 652 405 391 Est 5,183 648 647 413 408 Cze 5,402 675 675 955 908 Fin 5,043 630 630 2,355 2,141 Table 1: Overview on data. The training (train.), development (dev.) and test set sizes are given in sentences. The columns titled tags and train. tags correspond to total number of tags in the data set and num</context>
</contexts>
<marker>Erjavec, 2010</marker>
<rawString>Toma˘z Erjavec. 2010. Multext-east version 4: Multilingual morphosyntactic specifications, lexicons and corpora. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
<author>Karel Oliva</author>
<author>Vladimír Petkeviˇc</author>
</authors>
<title>Serial combination of rules and statistics: A case study in czech tagging.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<marker>Hajiˇc, Krbec, Kvˇetoˇn, Oliva, Petkeviˇc, 2001</marker>
<rawString>Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva, and Vladimír Petkeviˇc. 2001. Serial combination of rules and statistics: A case study in czech tagging. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 268– 275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Auli Hakulinen</author>
<author>Maria Vilkuna</author>
</authors>
<title>Riitta Korhonen, Vesa Koivisto, Tarja Riitta Heinonen, and Irja Alho.</title>
<date>2004</date>
<location>Helsinki, Finland.</location>
<marker>Hakulinen, Vilkuna, 2004</marker>
<rawString>Auli Hakulinen, Maria Vilkuna, Riitta Korhonen, Vesa Koivisto, Tarja Riitta Heinonen, and Irja Alho. 2004. Iso suomen kielioppi. Suomalaisen Kirjallisuuden Seura, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Péter Halácsy</author>
<author>András Kornai</author>
<author>Csaba Oravecz</author>
</authors>
<title>Hunpos: An open source trigram tagger.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>209--212</pages>
<contexts>
<context position="15999" citStr="Halácsy et al., 2007" startWordPosition="2603" endWordPosition="2606">et al. employ five parallel structured chains. One of the chains models the sequence of word-class labels such as noun and adjective. The other four chains model gender, number, case, and lemma sequences, respectively. Therefore, in contrast to our approach, their system does not capture cross-dependencies between inflectional categories, such as the dependence between the word-class and case of adjacent words. Unsurprisingly, Smith et al. fail to achieve improvement over a generative HMMbased POS tagger of Hajiˇc (2001). Meanwhile, our system outperforms the generative trigram tagger HunPos (Halácsy et al., 2007) which is an im262 model it. time (min) acc. OOV. English CRF(1, -) 8 9 97.04 88.65 CRF(1, 0) 6 17 97.02 88.44 CRF(1, 1) 8 22 97.02 88.82 CRF(2, -) 9 15 97.18 88.82 CRF(2, 0) 11 36 97.17 89.23 CRF(2, 1) 8 27 97.15 89.04 Romanian CRF(1, -) 14 29 97.03 85.01 CRF(1, 0) 13 68 96.96 84.59 CRF(1, 1) 16 146 97.24 85.94 CRF(2, -) 7 19 97.08 85.21 CRF(2, 0) 18 99 97.02 85.42 CRF(2, 1) 12 118 97.29 86.25 Estonian CRF(1, -) 15 28 93.39 78.66 CRF(1, 0) 17 66 93.81 80.44 CRF(1, 1) 13 129 93.77 79.37 CRF(2, -) 15 30 93.48 77.13 CRF(2, 0) 13 53 93.78 79.60 CRF(2, 1) 16 105 94.01 79.53 Czech CRF(1, -) 6 28 89</context>
</contexts>
<marker>Halácsy, Kornai, Oravecz, 2007</marker>
<rawString>Péter Halácsy, András Kornai, and Csaba Oravecz. 2007. Hunpos: An open source trigram tagger. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 209–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katri Haverinen</author>
<author>Jenna Nyblom</author>
</authors>
<title>Timo Viljanen, Veronika Laippala, Samuel Kohonen,</title>
<date>2013</date>
<location>Anna Missilä, Stina Ojala, Tapio</location>
<marker>Haverinen, Nyblom, 2013</marker>
<rawString>Katri Haverinen, Jenna Nyblom, Timo Viljanen, Veronika Laippala, Samuel Kohonen, Anna Missilä, Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013. Building the essential resources for Finnish: the Turku Dependency Treebank. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1067" citStr="Lafferty et al. (2001)" startWordPosition="133" endWordPosition="136">ned label sets using conditional random fields (CRFs). We propose improving tagging accuracy by utilizing dependencies within sub-components of the fine-grained labels. These sub-label dependencies are incorporated into the CRF model via a (relatively) straightforward feature extraction scheme. Experiments on five languages show that the approach can yield significant improvement in tagging accuracy in case the labels have sufficiently rich inner structure. 1 Introduction We discuss part-of-speech (POS) tagging using the well-known conditional random field (CRF) model introduced originally by Lafferty et al. (2001). Our focus is on scenarios, in which the POS labels have a rich inner structure. For example, consider PRON+1SG V+NON3SG+PRES N+SG ham , I like where the compound labels PRON+1SG, V+NON3SG+PRES, and N+SG stand for pronoun first person singular, verb non-third singular present tense, and noun singular, respectively. Fine-grained labels occur frequently in morphologically complex languages (Erjavec, 2010; Haverinen et al., 2013). We propose improving tagging accuracy by utilizing dependencies within the sub-labels (PRON, 1SG, V, NON3SG, N, and SG in the above example) of the compound labels. Fr</context>
<context position="2790" citStr="Lafferty et al., 2001" startWordPosition="399" endWordPosition="402">rying POS annotation granularity. By utilizing the sub-labels, we gain significant improvement in model accuracy given a sufficiently finegrained label set. Moreover, our results indicate that exploiting the sub-labels can yield larger improvements in tagging compared to increasing model order. The rest of the paper is organized as follows. Section 2 describes the methodology. Experimental setup and results are presented in Section 3. Section 4 discusses related work. Lastly, we provide conclusions on the work in Section 5. 2 Methods 2.1 Conditional Random Fields The (unnormalized) CRF model (Lafferty et al., 2001) for a sentence x = (x1, ... , x|x|) and a POS sequence y = (y1, ... , y|x|) is defined as ( ) exp w&apos;φ(yi−n, . . . , yi, x, i) , (1) where n denotes the model order, w the model parameter vector, and φ the feature extraction function. We denote the tag set as Y, that is, yi E Y for i E 1... |x|. 2.2 Baseline Feature Set We first describe our baseline feature set �φj(yi−1, yi,x, i)1|φ| j=1 by defining emission and transition features. The emission feature set associates properties of the sentence position i with |x| p(y |x;w) a H i=n 259 Proceedings of the 52nd Annual Meeting of the Association</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="4999" citStr="Marcus et al., 1993" startWordPosition="803" endWordPosition="807">4. If the word form xi contains (one or more) capital letter, hyphen, dash, or digit. Binary functions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features {1(yi−k = y0i−k) ... 1(yi = y0i) | y0i−k, ... , y0i E Y , bk E 1... n} (4) capture dependencies between adjacent labels irrespective of the input x. 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies The baseline feature set described above can yield a high tagging accuracy given a conveniently simple label set, exemplified by the tagging results of Collins (2002) on the Penn Treebank (Marcus et al., 1993). (Note that conditional random fields correspond to discriminatively trained hidden Markov models and Collins (2002) employs the latter terminology.) However, it does to some extent overlook some beneficial dependency information in case the labels have a rich sub-structure. In what follows, we describe expanded feature sets which explicitly model the sub-label dependencies. We begin by defining a function P(yi) which partitions any label yi into its sub-label components and returns them in an unordered set. For example, we could define P(PRON+1+SG) = {χj(x, i)1(s E P(yi)) |bj E 1... |X |, bs</context>
<context position="8438" citStr="Marcus et al., 1993" startWordPosition="1381" endWordPosition="1384">ustat and kissat share the plural number. In other words, the analyses of both mustat and kissat are required to contain the sub-label PLURAL. This short-span dependency between sub-labels will be captured by a first-order sublabel transition feature included in (6). Lastly, we note that the feature expansion sets (5) and (6) will, naturally, capture any short-span dependencies within the sub-labels irrespective if the dependencies have a clear linguistic interpretation or not. 3 Experiments 3.1 Data For a quick overview of the data sets, see Table 1. Penn Treebank. The English Penn Treebank (Marcus et al., 1993) is divided into 25 sections of newswire text extracted from the Wall Street Journal. We split the data into training, development, and test sets using the sections 0-18, 19-21, and 22-24, according to the standardly applied division introduced by Collins (2002). Turku Depedency Treebank. The Finnish Turku Depedendency Treebank (Haverinen et al., 2013) contains text from 10 different domains. The treebank does not have default partition to training and test sets. Therefore, from each 10 consecutive sentences, we assign the 9th and 10th to the development set and the test set, respectively. The</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<volume>1</volume>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4189" citStr="Ratnaparkhi (1996)" startWordPosition="662" endWordPosition="663">tions employed in the experiments are described in Section 3.2.) We denote the set of all sub-label components as S. Subsequently, instead of defining only (2), we additionally associate the feature functions X with all sub-labels s E S by defining the corresponding label as {χj(x,i)1(yi = y0 i)|j E 1 ... |X|,by0 i E Y}, (2) where the function 1(q) returns one if and only if the proposition q is true and zero otherwise, that is 0) _ f 1 if yi = y0 1 (yi = yi l 0 otherwise , (3) and X = {χj(x, i)}|X | j=1 is the set of functions characterizing the word position i. Following the classic work of Ratnaparkhi (1996), our X comprises simple binary functions: 1. Bias (always active irrespective of input). 2. Word forms xi−2, ... , xi+2. 3. Prefixes and suffixes of the word form xi up to length δsuf = 4. 4. If the word form xi contains (one or more) capital letter, hyphen, dash, or digit. Binary functions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features {1(yi−k = y0i−k) ... 1(yi = y0i) | y0i−k, ... , y0i E Y , bk E 1... n} (4) capture dependencies between adjacent labels irrespective of the input x. 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies</context>
<context position="12904" citStr="Ratnaparkhi (1996)" startWordPosition="2123" endWordPosition="2124"> after each pass over the training data and terminate training if no improvement in accuracy is obtained during three last passes. The best-performing parameters are then applied on the test instances. We accelerate the perceptron learning using beam search (Zhang and Clark, 2011). The beam width, b, is optimized separately for each language on the development sets by considering b = 1, 2, 4, 8, 16, 32, 64,128 until the model accuracy does not improve by at least 0.01 (absolute). Development and test instances are decoded using Viterbi search in combination with the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 Software and Hardware The experiments are run on a standard desktop computer (Intel Xeon E5450 with 3.00 GHz and 64 GB of memory). The methods discussed in Section 2 are implemented in C++. 3.5 Results The obtained tagging accuracies and training times are presented in Table 2. The times include running the averaged perceptron algorithm and evaluation of the development sets. The column labeled it. co</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the conference on empirical methods in natural language processing, volume 1, pages 133–142. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis</author>
</authors>
<title>Tiered tagging and combined language models classifiers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Second International Workshop on Text, Speech and Dialogue,</booktitle>
<pages>28--33</pages>
<contexts>
<context position="17567" citStr="Tufis (1999)" startWordPosition="2904" endWordPosition="2905">ved open-source implementation of the wellknown TnT tagger of Brants (2000). The obtained HunPos results are presented in Table 3. Eng Rom Est Cze Fin HunPos 96.58 96.96 92.76 89.57 85.77 Table 3: Results using a generative HMM-based HunPos tagger of Halacsy et al. (2007). Ceau¸su (2006) uses a maximum entropy Markov model (MEMM) based system for tagging Romanian which utilizes transitional behavior between sub-labels similarly to our feature set (6). However, in addition to ignoring the most informative emission-type features (5), Ceau¸su embeds the MEMMs into the tiered tagging framework of Tufis (1999). In tiered tagging, the full morphological analyses are mapped into a coarser tag set and a tagger is trained for this reduced tag set. Subsequent to decoding, the coarser tags are mapped into the original fine-grained morphological analyses. There are several problems associated with this tiered tagging approach. First, the success of the approach is highly dependent on a well designed coarse label set. Consequently, it requires intimate knowledge of the tag set and language. Meanwhile, our model can be set up with relatively little prior knowledge of the language or the tagging scheme (see </context>
</contexts>
<marker>Tufis, 1999</marker>
<rawString>Dan Tufis. 1999. Tiered tagging and combined language models classifiers. In Proceedings of the Second International Workshop on Text, Speech and Dialogue, pages 28–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="12567" citStr="Zhang and Clark, 2011" startWordPosition="2062" endWordPosition="2065"> not report results using CRF(2,2) since, based on preliminary experiments, this model overfits on all languages. The CRF model parameters are estimated using the averaged perceptron algorithm (Collins, 2002). The model parameters are initialized with a zero vector. We evaluate the latest averaged parameters on the held-out development set after each pass over the training data and terminate training if no improvement in accuracy is obtained during three last passes. The best-performing parameters are then applied on the test instances. We accelerate the perceptron learning using beam search (Zhang and Clark, 2011). The beam width, b, is optimized separately for each language on the development sets by considering b = 1, 2, 4, 8, 16, 32, 64,128 until the model accuracy does not improve by at least 0.01 (absolute). Development and test instances are decoded using Viterbi search in combination with the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 Software and Hardware The experiments are run on a standard desktop</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>