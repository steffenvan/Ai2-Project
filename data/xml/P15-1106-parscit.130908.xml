<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000201">
<title confidence="0.979652">
Driving ROVER with Segment-based ASR Quality Estimation
</title>
<author confidence="0.930192">
Shahab Jalalvand(1 2), Matteo Negri(1), Daniele Falavigna(1), Marco Turchi(1)
</author>
<affiliation confidence="0.778265">
(1) FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2) University of Trento, Italy
</affiliation>
<email confidence="0.984385">
{jalalvand,negri,falavi,turchi}@fbk.eu
</email>
<sectionHeader confidence="0.993434" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999439">
ROVER is a widely used method to
combine the output of multiple auto-
matic speech recognition (ASR) systems.
Though effective, the basic approach and
its variants suffer from potential draw-
backs: i) their results depend on the order
in which the hypotheses are used to feed
the combination process, ii) when applied
to combine long hypotheses, they disre-
gard possible differences in transcription
quality at local level, iii) they often rely on
word confidence information. We address
these issues by proposing a segment-based
ROVER in which hypothesis ranking is
obtained from a confidence-independent
ASR quality estimation method. Our re-
sults on English data from the IWSLT2012
and IWSLT2013 evaluation campaigns
significantly outperform standard ROVER
and approximate two strong oracles.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840290909091">
In automatic speech recognition (ASR), the com-
bination of transcription hypotheses produced by
multiple systems usually leads to significant word
error rate (WER) reductions compared to the out-
put of each individual system. Systems’ diversity
and complementarity have been exploited in dif-
ferent ways to synthetically obtain more accurate
transcriptions. Recognizer output voting error re-
duction – ROVER (Fiscus, 1997), the most widely
used method, performs hypothesis fusion in two
steps. First, the 1-best transcriptions from multi-
ple systems are aligned by means of dynamic pro-
gramming to build a single, minimal word tran-
sition network. Then, the resulting network is
searched to select the best scoring word at each
node. The final hypothesis is constructed via a ma-
jority voting mechanism and, if available, by using
word confidence measures.
This general strategy has been improved in sev-
eral ways but, despite their proven effectiveness,
ROVER and its variants have three potential draw-
backs. The first one is intrinsic to their implemen-
tation: the fusion process starts from one of the
input hypotheses, which is used as “skeleton” for
the greedy alignment of the others. The order in
which the hypotheses are used to feed the process
can hence determine significant variations in the
WER of the resulting combination. This calls for
automatic methods for ranking the hypotheses
to initialise and carry on the fusion process.
The second drawback is inherent to the way
ROVER is usually run: the fusion process is typi-
cally fed with transcriptions of entire audio record-
ings (lasting up to hours). With this level of granu-
larity, the skeleton used as basis for the alignment
may consist of long transcriptions whose quality
can considerably vary at local level. For instance,
the worst transcription of an entire audio recording
(globally) could be the best one for some passages
(locally). This calls for solutions capable to op-
erate at higher granularity levels (e.g. segments
lasting up to few seconds) to better exploit the
local diversity of the combined transcriptions.
The third drawback relates to the applicability
of ROVER-like fusion methods: their common
trait is the reliance on information about the in-
ner workings of the combined systems. Indeed,
the standard voting scheme with confidence scores
is usually much more reliable than the simpler
frequency-based voting. The access to confidence
scores, however, is a too rigid constraint in ap-
plication scenarios where the hypotheses to be
combined come from unknown (“black-box”) sys-
tems.1 This calls for confidence-independent fu-
sion methods.
</bodyText>
<footnote confidence="0.99407275">
1One example, among the many possible ones, is the sce-
nario in which an array of microphones (e.g. in a room or a
vehicle) sends input to one or more commercial ASR systems
which do not provide confidence information.
</footnote>
<page confidence="0.867421">
1095
</page>
<note confidence="0.982629">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1095–1105,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.6060812">
L3 L4 L5 L6 L7 L8
SysO 12.2 11.7 11.8 11.9 12.1 12.1
InSysO 19.8 16.6 15.1 13.9 13.4 13.3
SegO 10.5 11.0 11.4 11.6 11.7 11.7
InSegO 22.9 19.6 17.4 15.8 14.4 13.0
</table>
<tableCaption confidence="0.7421055">
Table 1: Motivation: the influence of hypothesis
order and granularity on standard ROVER results.
</tableCaption>
<bodyText confidence="0.997882666666667">
The impact of the first two issues is evident from
the figures provided in Table 1. The results refer to
the WER achieved by different “oracles” obtained
from the output of eight ASR systems that partici-
pated in the IWSLT2013 campaign (Cettolo et al.,
2013).2 Such oracles combine:
</bodyText>
<listItem confidence="0.99525">
• Different numbers of transcriptions (from
three – L3 to eight – L8);
• At different granularity levels (whole utter-
ance – SysO and segment – SegO);
• In different orders (best to worst – SysO,
SegO and inverse – InSysO, InSegO).
</listItem>
<bodyText confidence="0.993407038461539">
As shown in the table, the gap between
utterance-based (SysO) and segment-based
(SegO) is evident at all levels: WER differences
vary from 0.3 (11.9 vs. 11.6 at L6) to 1.7 points
(12.2 vs. 10.5 at L3). Another gap is evident
between best-to-worst and inverse rankings,
with WER differences up to 7.6 points at whole
utterance level (SysO vs. InSysO at L3) and
12.4 points at segment level (SegO vs. InSegO
at L3). Another interesting observation is that
top results (i.e. lower WER) are obtained when
combining a subset of the outputs (respectively
four at utterance level and three at segment level).
Referring to this analysis, the goal of computing
ROVER based on hypothesis ranking at higher
granularity levels is well motivated.
A crucial need to achieve this goal is the avail-
ability of a confidence-independent method to pre-
dict the quality of ASR transcriptions at segment
level. This “quality estimation” (QE) task has
been recently addressed in (Negri et al., 2014;
C. de Souza et al., 2015) as a supervised regres-
sion problem in which transcriptions’ WER is pre-
dicted without having access to reference tran-
scripts.3 Different feature sets have been evalu-
ated, showing that even with those extracted only
</bodyText>
<footnote confidence="0.92324525">
2Details about this dataset will be provided in Section 6.1.
3This formulation is very similar to the machine transla-
tion counterpart of the task (Specia et al., 2009; Mehdad et
al., 2012; Turchi et al., 2014; C. de Souza et al., 2014).
</footnote>
<bodyText confidence="0.999917315789474">
from the signal and the transcription (i.e. disre-
garding information about the decoding process)
the prediction error is sufficiently low to open to
real applications. However, though promising, ex-
perimental results stem from an intrinsic evalua-
tion in which QE is only addressed in isolation.
By applying it to inform ROVER, we pro-
pose for the first time an application-oriented ex-
trinsic evaluation of ASR QE (our first contri-
bution). To this aim, we extend previous ASR
QE methods with new features (second contribu-
tion), and report significant improvements over
standard ROVER on a shared dataset (third con-
tribution). For the sake of brevity, our compar-
ison is performed only against standard ROVER
and in “black-box” conditions. However it’s worth
remarking that our approach can be straightfor-
wardly applied to any ROVER-like variant and, if
available, by exploiting confidence features.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.975048366666667">
This paper gathers three main research strands to-
gether: ASR system combination, ASR quality es-
timation and machine-learned ranking.
Fiscus (1997) proposed ROVER as an approach
to produce a composite ASR output. The basic ap-
proach has been extended in several ways. N-Best
ROVER (Stolcke et al., 2000) improves the orig-
inal method by combining multiple alternatives
from each combined system. Schwenk and Gau-
vain (2000) exploit a secondary language model
to rescore the final n-best hypotheses generated by
ROVER. iROVER (Hillard et al., 2007) exploits a
classifier to choose the system that is most likely to
be correct at each word location. cROVER (Abida
et al., 2011) integrates a semantic pre-filtering step
in which the word transition network is scanned to
flag and eliminate erroneous words to facilitate the
voting. Other approaches to ASR system combi-
nation make use of word lattices or confusion net-
works (Mangu, 2000; Li et al., 2002; Evermann
and Woodland, 2000; Hoffmeister et al., 2006;
Bougares et al., 2013, inter alia). Note that all
these combination methods require to have access
to the inner structure of the ASR decoder, while
ASR systems, especially the commercial ones, of-
ten do not provide this information.
ASR quality estimation allows us to overcome
this problem and obtain confidence-independent
estimates of ASR output quality. Based on the
positive intrinsic evaluation results reported in
</bodyText>
<page confidence="0.99001">
1096
</page>
<bodyText confidence="0.999076071428571">
(Negri et al., 2014; C. de Souza et al., 2015), here
we extend the approach with new features and per-
form an extrinsic evaluation in a real application
scenario. Our new features are inspired by re-
search on ASR error detection at word level (Gold-
water et al., 2010; Pellegrini and Trancoso, 2010).
Machine-learned ranking (MLR) or learning to
rank (Hang, 2011) is widely used in information
retrieval to order the answers to a user’s query
(Cao et al., 2007; McFee and Lanckriet, 2010;
McSherry and Najork, 2008). We use it to order
the transcription hypotheses produced by multiple
ASR systems and feed ROVER with the resulting
ranked lists.
</bodyText>
<sectionHeader confidence="0.990975" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.997576666666667">
Given an utterance and a set of M transcription
hypotheses produced by M different (possibly un-
known) ASR systems, our goal is to:
</bodyText>
<listItem confidence="0.998324083333333">
1. Split the utterance into segments (ideally at
sentence level);
2. For each segment, automatically estimate the
quality (e.g. in terms of WER) of the corre-
sponding M (segment-level) hypotheses;
3. Use the estimates to rank the hypotheses and
feed ROVER based on the ranking;
4. Reconstruct the entire utterance transcrip-
tion by concatenating the combined segment-
level transcriptions produced by ROVER;
5. Measure the overall WER differences against
standard ROVER and other oracles.
</listItem>
<bodyText confidence="0.9999186">
Step 1 is performed by a start-end point detection
module based on signal energy, which is followed
by a segment classification module based on Gaus-
sian Mixture Models similar to (Cettolo and Fed-
erico, 2000). Although the comparison with al-
ternative splitting methods might lead to different
results, this is not the main focus of the paper and
is left as future work. Steps 2–4, instead, repre-
sent the core of our contribution and are described
in the following sections.
</bodyText>
<sectionHeader confidence="0.989518" genericHeader="method">
4 Segment-based QE-informed ROVER
</sectionHeader>
<bodyText confidence="0.999753633333333">
ROVER uses iterative dynamic programming to
build a word transition network (WTN) from mul-
tiple ASR output hypotheses. The resulting WTN
can be seen as a confusion network with an equal
number of word arc hypotheses (one for each ASR
system entering the combination) in each corre-
spondence slot. The best word sequence is deter-
mined from the WTN via majority voting among
the words in each slot. Most of the extensions of
ROVER, such as iROVER (Hillard et al., 2007),
cROVER (Abida et al., 2011) and the one de-
scribed in (Zhang and Rudnicky, 2006), aim to
learn a scoring function that allows improving the
reordering of words inside each slot. In particu-
lar, iROVER reorders the words in each slot by
means of a classifier trained with features that
characterize the individual ASR systems. This ap-
proach, however, needs first to properly normal-
ize the word lattices generated by each system, in
order to exhibit the same vocabulary and similar
densities, and to generate a unified segmentation
for joining the lattices.
In a similar way, motivated by the analysis
shown in Table 1, our method applies reordering
of the ASR hypotheses at segment level. However,
differently from iROVER, it does not require to
access the inner components of the decoders (e.g.
word lattices or word confidences), nor to apply
pre-processing steps that can distort the outputs of
individual ASR components.
</bodyText>
<equation confidence="0.920836333333333">
this is my mobile A mobile phone can change ................................................ Thank you
T1A T2A TAn
T1B T2B TnB
T1C T2C TnC
SRV(T1A, T1B, T1C) SRV(T2A, T2C, T2B) SRV(TnB, TnA, TnC)
concat(SRV)
</equation>
<figureCaption confidence="0.998569">
Figure 1: Segment-based ROVER
</figureCaption>
<bodyText confidence="0.9848285">
Figure 1 illustrates the difference between stan-
dard ROVER (RV, shown at the rightmost verti-
cal) which works at the utterance level (lasting
up to few hours) and the segment-based ROVER
(SRV, shown at the bottom horizontal) that works
at the segment level (lasting up to few seconds).
RV keeps the order of the systems static along
the whole utterance (A � B � C, i.e. system
A has generated a better transcription than sys-
tem B which, in turn is better than system C) for
all the segments RV (Tin, TBn, Tin). SRV, in-
stead, dynamically changes the system order from
</bodyText>
<equation confidence="0.790343">
RV(T1..nA,T1..nB, T1..nC)
</equation>
<page confidence="0.904761">
1097
</page>
<bodyText confidence="0.999975736842105">
one segment to the other. For example, the system
order for the first segment is A � B � C, while
for the next segment it is A � C � B. Our hy-
pothesis is that, with a proper segment-based rank-
ing, SRV will result in lower WER scores than RV.
Note that, as depicted in Figure 1, segment-
based ROVER requires that all the ASR systems
share a common segmentation. This is easy to ob-
tain by force-aligning the transcriptions of each
system with a given segmentation (e.g. one ran-
domly chosen among those employed by each
ASR system).
In this paper we approach segment-level ASR
QE as a supervised learning task, by comparing
two alternative strategies: ranking by regression
(Section 4.1) and machine-learned ranking (Sec-
tion 4.2). Both methods rely on the features used
in (Negri et al., 2014), extended with a new set of
word-level features described in Section 5.
</bodyText>
<subsectionHeader confidence="0.998636">
4.1 Ranking by regression (RR)
</subsectionHeader>
<bodyText confidence="0.999201657142857">
The first ranking strategy is based on training a re-
gressor on a set of (signal, transcription, WER)
triples, and use it to predict the WER score for
new, unseen (signal, transcription) test instances.
Then, based on the predicted WERs, a ranked list
is produced for each segment to feed ROVER.
To train the regressor, we are given N seg-
ments (Si,1 &lt; i &lt; N), their automatic transcrip-
tions ({Ti1 ... T i M}i=N
i=1 ) produced by M ASR sys-
tems, and manual references from which the true
WERs ({TWi1 ...TW i M}i=N
i=1 ) can be computed
for each segment i. The whole set of train-
ing data is hence represented by instances: I =
{(Si,Tij,TWij),1 &lt; j &lt; M,1 &lt; i &lt; N }.
Training is performed with two alternative strate-
gies, which differ in the amount of training data
used. The first one, RR1, employs the whole train-
ing set I. The second one, RR2, uses only one
transcription for each segment, randomly chosen
from the M available. In this case, the training set
becomes: I� = {(Si,Tij,TWij),1 &lt; i &lt; N, j =
rnd(M)} where rnd(M) is a random number be-
tween 1 to M. In practice, RR2 learns from a
smaller but more diverse training set compared to
RR1. On the one side, in fact, RR1 deals with
a larger number of training instances (M times
more), but the feature vectors will share the same
values for the features extracted from the signal of
each utterance. On the other side, RR2 reduces the
size of the training set I� down to 1M of I, but only
one feature vector is extracted for each utterance.
The unpredictable effect of such differences on QE
results motivates experiments with both methods.
</bodyText>
<subsectionHeader confidence="0.993919">
4.2 Machine-learned ranking (MLR)
</subsectionHeader>
<bodyText confidence="0.999942421052632">
The second strategy relies on directly training
a ranking model from a set of instances I =
{(Si,Tij,TRji),1 &lt; i &lt; N,1 &lt; j &lt; M }, where
Si and Tij respectively represent segments and
transcriptions, and TRji represents “true ranks”
computed from the corresponding reference WER
values TWij. That is, given two transcriptions, Tij
and Tik and the true WERs, then TRji -� TRki, if
TWij &lt; TWik.
It is worth to note that MLR, differently from
the two regression methods described above, per-
forms a pairwise comparison between the seg-
ment candidates. That is, for each pair of seg-
ment transcriptions, the algorithm processes their
corresponding feature vectors against each other
and decides to place one transcription ahead of the
other, as long as returning a score for this decision.
Based on this score, the algorithm is then able to
rank more than two candidates.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.99999025">
We use two sets of features. One consists of the
basic features described in (Negri et al., 2014); the
other includes several word-based features specif-
ically introduced for our ranking task.
</bodyText>
<subsectionHeader confidence="0.995437">
5.1 Basic features
</subsectionHeader>
<bodyText confidence="0.999972176470588">
Basic features can be further divided in three
groups:
Signal features (16 in total) aim to capture the
difficulty to transcribe a given input by looking
at the signal as a whole. They are obtained by
analyzing the audio waveform with a window of
20ms at a frame rate of 10ms. For each analysed
window, 12 Mel Frequency Cepstral Coefficients
(MFCCs) are evaluated (MFCC of order 0 is dis-
carded) plus log energy. Then, to form the signal
feature vector for each given segment, we com-
pute the mean/min/max values of raw energy, as
well as the mean MFCCs values and total segment
duration.
Hybrid features (26) provide a more fine-
grained way to capture the difficulty of transcrib-
ing the signal. They are computed based on
</bodyText>
<page confidence="0.971">
1098
</page>
<bodyText confidence="0.995961111111111">
the forced alignment between the M given auto-
matic transcriptions of each segment and the cor-
responding acoustic observations obtained from
raw features. For each transcription hypothesis
hybrid features are: signal to noise ratio (SNR),
mean/min/max noise energy, mean/min/max word
energy, (max word - min noise) energy, number of
silences (#sil), #sil per second, number of words
(#wrd) per second, #sil, total duration of words
</bodyText>
<subsubsectionHeader confidence="0.526492">
#wrd
</subsubsectionHeader>
<bodyText confidence="0.999948285714286">
(Dwrd), total duration of silences (Dsil), mean du-
ration of words, mean duration of silences, Dsil Dwrd,
Dwrd − Dsil, standard deviation (std) of word -
dution, std of silence duration, mean/std/min/max
of pitch4, number of hesitations, frequency of hes-
itations.
Textual features (10) aim to capture the plausi-
bility (i.e. the fluency) of a transcription. For each
hypothesis textual features are: number of words,
LM log probability, LM log probability of part of
speech (POS), log perplexity, LM log perplexity
of POS, percentage (%) of numbers, % of tokens
which do not contain only “[a-z]”, % of content
words, % of nouns, % of verbs.
</bodyText>
<subsectionHeader confidence="0.98569">
5.2 Word-based features
</subsectionHeader>
<bodyText confidence="0.999976434782609">
To compensate the absence of ASR confidence in-
formation, we also designed a set of “word-based”
features inspired by previous approaches to ASR
error detection (Chieu and Ng, 2002; Pellegrini
and Trancoso, 2010; Goldwater et al., 2010; Tam
et al., 2014). They aim to capture words’ pronun-
ciation difficulty, which is determined by the num-
ber of lexical neighbors (similar pronunciations)
and the types of phonemes that form the words.
From the ASR error detection field we also borrow
additional language model features based on re-
current neural network language model (RNNLM)
probability (Mikolov et al., 2010).
Word-based features (22) are: POS tag and
score of the previous/current/next words (6),
RNNLM probabilities (2) given by models
trained on in-domain and out-of-domain data, in-
domain/out-of-domain 4-gram LM probability (2),
number of phoneme classes (including fricatives,
liquids, nasals, stops and vowels) (5), number of
homophones (1), number of lexical neighbors (1)
and binary features answering the three questions:
“is stop word?” (1), “is before/after repetition?”
</bodyText>
<footnote confidence="0.962002">
4Pitch features have been computed with the Praat soft-
ware tool (Boersma and Weenink, 2005).
</footnote>
<table confidence="0.999815333333333">
Dataset duration sent token voc talks
tst2012 1h45m 1,124 19.2k 2.8k 11
tst2013 4h50m 2,246 41.6k 5.6k 28
</table>
<tableCaption confidence="0.921689666666667">
Table 2: Dataset statistics: duration, number of
sentences, number of tokens, vocabulary size,
number of talks.
</tableCaption>
<table confidence="0.999959666666666">
System tst2012 tst2013
FBK 16.8 23.2
KIT 12.7 14.4
MITLL 13.3 15.9
NAIST – 16.2
NICT 12.4 13.5
PRKE – 27.2
RWTH 13.6 16.0
UEDIN 14.4 22.1
</table>
<tableCaption confidence="0.994152">
Table 3: Official WER[%] scores of the partic-
</tableCaption>
<bodyText confidence="0.955713">
ipants in the IWSLT2012 and IWSLT2013 ASR
evaluations.
(2), “is before/after silence?” (2). Since the ASR
hypotheses of a given segment might contain dif-
ferent numbers of words, we average the values of
the word-based features for each hypothesis.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="method">
6 Experimental setup
</sectionHeader>
<bodyText confidence="0.999964">
In this section we illustrate the audio data used in
our experiments, the methods used to inform and
run ROVER, the evaluation metric and the signifi-
cance testing method applied.
</bodyText>
<subsectionHeader confidence="0.98296">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999873722222222">
We experiment with two sets of speech recordings
collected from English TED talks and used for
the 2012 (IWSLT2012) and 2013 (IWSLT2013)
editions of the International Workshop on Spo-
ken Language Translation (Federico et al., 2012;
Cettolo et al., 2013). Statistics for both datasets
are shown in Table 2. Six teams participated
in the 2012 evaluation: FBK, KIT, MITLL,
NICT, RWTH and UEDIN. Two more competi-
tors, NAIST and PRKE, took part in the 2013 edi-
tion of the campaign. The related WERs are re-
ported in Table 3. For detailed system descrip-
tions we refer the reader to the IWSLT20125 and
IWSLT20136 proceedings.
In the experiments, we used tst2012 for train-
ing with 4-fold cross-validation, and tst2013 for
testing purposes. Note that cross-validation was
applied ensuring that a given speaker does not ap-
</bodyText>
<footnote confidence="0.9999925">
5http://workshop2012.iwslt.org
6http://workshop2013.iwslt.org
</footnote>
<page confidence="0.997383">
1099
</page>
<bodyText confidence="0.999979785714286">
pear simultaneously both in the training and vali-
dation sets. The same condition holds for the test
set: speakers in tst2012 do not occur in tst2013.
These conditions, and the use of two different
sets of talks (acquired in different IWSLT editions
and transcribed by different sets of ASR systems),
make our task particularly difficult and guarantee
the congruence with real-life scenarios in which
training and test data are totally independent.
As previously mentioned, a common segmenta-
tion needs to be shared among the various ASR
components. To do this we decided to use the
one provided by our internal ASR system, and to
force-align to it all the other ones.
</bodyText>
<subsectionHeader confidence="0.998082">
6.2 Terms of comparison
</subsectionHeader>
<bodyText confidence="0.999613433333333">
We compare our segment-based QE-informed
ROVER against three methods that differ in the
granularity of the combined hypotheses and in the
way they are ranked:
Random ROVER. It is obtained by averaging
the results of 100 runs of standard, system-level
ROVER (i.e. the WTN is obtained by com-
bining transcriptions of the whole utterance) in
which the systems to be combined are ranked ran-
domly. Note that this is the only possible way
to run ROVER in absence of information about
the reliability of the combined systems. Random
ROVER is the standard fusion method adopted
in IWSLT2013 to produce the final transcriptions
that are sent to the machine translation phase.
System-based Oracle (SysO). It is obtained
by computing the standard, system-level ROVER
based on the true system ranking (i.e. the actual
ranking of the IWSLT2013 participants). We con-
sider it as an oracle since the true ranking repre-
sents prior knowledge about systems’ reliability
which is not available in real testing conditions.
Segment-based Oracle (SegO). It is obtained
by computing ROVER at segment-level, using the
true system ranking for each segment. Also this
oracle relies on information about systems’ rank-
ing (at a higher granularity level), which is not
available in real testing conditions. As shown in
Table 1, this is the strongest term of comparison
and actually represents out upper bound.
</bodyText>
<subsectionHeader confidence="0.998663">
6.3 Evaluation metric and significance test
</subsectionHeader>
<bodyText confidence="0.999882714285714">
As usually done in ASR evaluation, performance
results are measured in terms of WER.7 Our
segment-based, QE-informed ROVER is hence
compared against the other methods based on the
WER computed on the test set (tst2013).
To measure if two methods produce statistically
different results, we run the matched-pairs signif-
icance test (Gillick and Cox, 1989). It is based
on averaging the differences between the number
of errors (insertions, deletions and substitutions)
produced by the two approaches for the individual
segments. If the average falls in the [-0.05,+0.05]
interval, then the global WER difference between
the two methods is not statistically significant.
In terms of results’ significance tests, our suc-
cess criteria are: i) a statistically significant im-
provement over random ROVER, and ii) non-
significant differences with respect to the two
strong oracles. For the sake of comparison, we
define three symbols for the evaluation results re-
ported in Table 4:
</bodyText>
<listItem confidence="0.997432">
1. “†” indicates that the corresponding WER
score is not significantly different from ran-
dom ROVER (a negative result);
2. “•” indicates that the WER score is not sig-
nificantly different from the system-based
ROVER oracle (a positive result);
3. “Y” indicates that the WER score is not sig-
nificantly different from the segment-based
ROVER oracle (the best result).
</listItem>
<subsectionHeader confidence="0.994536">
6.4 Ranking Models
</subsectionHeader>
<bodyText confidence="0.999986363636364">
Ranking by regression (see Section 4.1) is per-
formed using the implementation of the extremely
randomized trees algorithm (Geurts et al., 2006)
provided by the Scikit-learn package (Pedregosa
et al., 2011). Extra-trees are a tree-based ensemble
method for supervised classification and regres-
sion, which we successfully used in the past both
for MT (de Souza et al., 2013) and ASR quality
estimation (Negri et al., 2014). The model used
for machine learned ranking (see Section 4.2) is
based on the implementation of the random forest
</bodyText>
<footnote confidence="0.5616662">
7The word error rate is the minimum edit distance be-
tween an hypothesis and the reference transcription. Edit dis-
tance is calculated as the number of edits (word insertions,
deletions, substitutions) divided by the number of words in
the reference. Lower WERs (↓) indicate better transcriptions.
</footnote>
<page confidence="0.9584">
1100
</page>
<table confidence="0.999874230769231">
method-number of combined systems L3 L4 L5 L6 L7 L8
Random ROVER 14.6 13.7 13.2 12.8 12.7 12.4
SegO 10.5 11.0 11.4 11.6 11.7 11.7
SysO 12.2 11.7 11.8 11.9 12.1 12.1
RR1 +Basic 13.9 13.1 12.6 12.4 12.4 12.3 † •
RR1 +WordBased 14.0 13.0 12.5 12.2 12.3 • 12.3 † •
RR1 +Basic+WordBased 14.0 13.0 12.5 12.2 12.3 • 12.3 † •
RR2 +Basic 13.8 13.0 12.6 12.4 12.3 • 12.3 † •
RR2 +WordBased 14.2 13.1 12.7 12.4 12.5 † 12.4 † •
RR2 +Basic+WordBased 13.7 12.8 12.4 12.2 12.2 • 12.2 † •
MLR +Basic 12.9 12.4 12.3 12.1 • 12.3 12.2 † •
MLR +WordBased 12.4 • 12.1 12.0 12.0 • 12.2 • 12.2 † •
MLR +Basic+WordBased 12.4 • 12.1 12.0 • 11.9 • * 12.2 • 12.2 † •
</table>
<tableCaption confidence="0.888908">
Table 4: WER[%] (↓) of random, oracle and QE-informed ROVERs. The symbols assigned to some
scores indicate their statistical significance (p G 0.05 computed with the matched-pairs test). In particu-
</tableCaption>
<bodyText confidence="0.930696304347826">
lar: “†” = the result is not statistically different from random ROVER; “•” = the result is not statistically
different from SysO; “*” the result is not statistically different from SegO.
ensemble method (Breiman, 2001) provided in the
RankLib library.8
As mentioned in Section 6.1, all the ranking
models are trained in 4-fold cross validation. RR1
uses all the instances in tst2012 (i.e. 1,124 seg-
ments transcribed by 6 ASR systems, which re-
sults in a total of 6,744 training instances). RR2
uses only one instance per segment, which is ran-
domly selected among the 6 automatic transcrip-
tions available in tst2012 (resulting in a total of
1,124 training instances). Similar to RR1, MLR
uses all the instances in tst2012 (6,744 in total).
The learning parameters of each model (number
of bags, number of trees per bag, number of leaves
per tree and minimum number of instances per
leaf) are tuned by maximising Mean Average Pre-
cision as the objective function (Hang, 2011).
All the models are trained using the ba-
sic features (+Basic), the word-based ones
(+WordBased) and their combination (+Ba-
sic+WordBased).
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="evaluation">
7 Results and discussion
</sectionHeader>
<bodyText confidence="0.998651">
Table 4 reports the WER results obtained on
tst2013 by ROVER methods fed with: different
numbers of hypotheses (from 3 to 8), at different
granularity levels (whole utterance vs. segment),
ranked with different models (random, RR1, RR2
and MLR) trained with different sets of features
</bodyText>
<footnote confidence="0.895092">
8http://sourceforge.net/p/lemur/wiki/
RankLib/
</footnote>
<bodyText confidence="0.974392258064516">
(Basic, WordBased, Basic+WordBased).
The first three rows present the results achieved
by our terms of comparison: random ROVER,
the segment-based oracle (SegO) and the system-
based oracle (SysO). As anticipated when moti-
vating our work (see Table 1), the WER achieved
by SegO is always lower than the scores achieved
by SysO. Note also that the performance of SegO
decreases as the number of combined hypotheses
increases, due to the introduction in the input of
progressively worse transcripts. Instead, SysO ex-
hibits a less coherent behaviour, with close WER
values at all levels, and a minimum in correspon-
dence of column L4 (the combination of four
transcriptions of the whole utterance). We inter-
pret these results as a further motivation for our
work: feeding ROVER with a good ranking that
exploits local (segment-level) differences between
the combined hypotheses seems to be more reli-
able than relying on system-level ranks based on
global WER scores. A theoretical analysis of the
relation between the diversity of the combined hy-
potheses and ROVER results is presented in (Au-
dhkhasi et al., 2014). In light of this analysis,
our results open an interesting issue concerning
the trade-offs between optimal hypothesis ranking
and their (local) diversity. We initially explore this
problem in Section 7.1, but leave for future work
a more systematic investigation.
Rows 4-6 show the results achieved by RR1
(ranking by regression, trained with all the tran-
</bodyText>
<page confidence="0.985994">
1101
</page>
<bodyText confidence="0.999930946428572">
scriptions for each input segment). When trained
only with basic features, it always outperforms
random ROVER. At L8 the gain is not statisti-
cally significant but, at the same time, also the
WER difference with SysO is not significant. Note
that, proceeding from L3 to L8, the WER differ-
ence between RR1+Basic and random ROVER de-
creases from 0.7 to 0.1. This can be explained by
the fact that when the number of candidates in-
creases, then the role of majority voting dominates
the role of hypothesis ranking. Similar trends are
shown by all other approaches, including the or-
acles. RR1+WordBased slightly improves over
RR1+Basic, indicating the possible usefulness of
this new set of features. However, when used
in combination (RR1+Basic+WordBased), the two
feature sets do not yield further WER reductions.
Nevertheless, what is worth to remark is that at L7
and L8 the distance from Sys0 is not statistically
significant (a positive result).
As shown in rows 7-9, the situation changes
with RR2 (ranking by regression, trained with one
transcription per segment). When trained with the
combined feature sets (RR2+Basic+WordBased),
the model always leads to slight WER reductions
over RR2+Basic. Also in this case, the gains over
random ROVER are consistent (they range from
0.9 at L3 to 0.2 at L8), and the difference with re-
spect to SysO is not statistically significant at L7
and L8 (a positive result).
As shown in rows 10-12, results are further im-
proved by MLR. Except for L8, the improvement
over random ROVER is statistically significant,
large and consistent with all feature sets. The
WER reduction obtained by MLR+Basic varies
from 1.7 to 0.2 WER points, indicating a higher ef-
fectiveness of machine-learned ranking compared
to ranking by regression. MLR+WordBased pro-
duces further WER reductions, with differences
with SysO that become statistically not-significant
at four levels (L3, L6, L7 and L8). Finally, when
trained with the combined feature sets, the ranking
model leads to the lowest WER scores. Notice-
ably, such results are not only on par with SysO
(the difference is statistically significant only at
L4), but in one case (L6) they even reach those
of SegO, the strongest competitor (best result).
Overall, as evidenced by the L8 column, when
the number of input components becomes large
our QE-informed approaches are not significantly
better than random ROVER and SysO. This raises
the need of a stopping criterion to avoid entering
useless inputs into the ROVER combination. To-
gether with the trade-off between ranking perfor-
mance and hypotheses’ diversity, this represents
an interesting topic for future work.
</bodyText>
<subsectionHeader confidence="0.989662">
7.1 The role of hypotheses’ diversity
</subsectionHeader>
<bodyText confidence="0.999974875">
To gain further insights on our results, and as
a first step along the research directions previ-
ously outlined, we analysed the relation between
ROVER results and hypotheses’ diversity. To this
aim, Figure 2 plots the WER of our best method
(MLR+Basic+WordBased) and the two oracles
as a function of hypotheses’ diversity at L6, for
which we obtain the best results.
</bodyText>
<figure confidence="0.986286909090909">
30
25
20
occurences[%]
SegO
SysO
MLR+Basic+
WordBased
0 0
10 20 30 40 50 60 70 80 90 100
Level of diversity (MAX WER[%] − MIN WER[%])
</figure>
<figureCaption confidence="0.963038">
Figure 2: Results on tst2013 of the oracles and our
best model, as functions of hypotheses’ diversity.
</figureCaption>
<bodyText confidence="0.997747">
Diversity is measured by computing the difference
between the maximum and the minimum WERs
of the input transcriptions. All the segments are
then grouped with regard to this difference. For
example 10 on the x-axis refers to the group of
segments whose diversities lay in the interval of
[0,10); 20 refers to the segments whose diversities
are in [10,20) and consequently, 100 represents the
segments whose diversities lay in [90,100]. This
latter means that for each segment there is at least
one transcription that is perfect or close to perfec-
tion, and one that is (almost completely) wrong.
For segments with diversity smaller than 70, the
performance of the system-based oracle (line with
circle marks) and our segment-level QE-informed
ROVER (line with triangle marks) is almost iden-
tical. Instead, for segments with a “high” level
of diversity (in the interval [70,100]), our method
significantly outperforms the system-based oracle.
With a maximum gain larger than 3 WER points, it
approaches the strong segment-based oracle (line
</bodyText>
<figure confidence="0.993455272727273">
15
WER[%]
10
05
30
25
20
15
10
Occurences[%]
5
</figure>
<page confidence="0.998252">
1102
</page>
<bodyText confidence="0.999959105263158">
with asterisk marks). Remarkably, for diversity
values in the interval [90,100], our method is able
to halve the distance that separates the two oracles.
The considerable WER reductions observed for
diversity values larger than 70 shed new light on
the global results reported in Table 4. The fact that
such performance gains are hidden in the global
scores can be explained by looking at the dashed
line in Figure 2, which shows the percentage of
segments belonging to each diversity level. As
it can be observed, the vast majority of the seg-
ments (∼95%) falls in diversity bins in the inter-
val [10,70). The large WER reductions obtained
on the few remaining segments are definitely not
enough to boost global results. Overall, this find-
ing suggests that our segment-level QE-informed
ROVER can fully unfold its potential in applica-
tion scenarios featuring high diversity among the
transcriptions.
</bodyText>
<subsectionHeader confidence="0.999703">
7.2 Prediction of overall ranks
</subsectionHeader>
<bodyText confidence="0.999932171428571">
Since our results strongly depend on the reliabil-
ity of hypothesis ranking, our final analysis fo-
cuses on the correlation between QE-based rank-
ing methods and the “true” ranks used as prior
knowledge by the system-based oracle (the official
ranking of the IWSLT2013 participants). In order
to predict the overall IWSLT2013 ranking, we first
run our QE models on each segment. Systems are
then ordered based on the average ranking score
received by their transcriptions. Finally, the alter-
native QE-based methods (RR1, RR2 and MLR)
are compared by measuring their Spearman corre-
lation with the TRUE systems’ order.
Table 5 reports the resulting rankings and the
corresponding correlation with the true, official
one. Among all the possible combinations (8 fac-
torial), our two best methods (RR2 and MLR) re-
sult in a systems’ ordering with high correlation
with the official IWSLT2013 ranking. In particu-
lar, MLR achieves correlation of 0.905 with three
out of eight systems (1, 2 and 8) that are correctly
positioned. The correlation values of the differ-
ent approaches reflect the performance reported
in Table 4, in which the WER achieved by us-
ing MLR is usually better than the ones obtained
from RR1 and RR2. It is interesting to note in
the last column of Table 5 that the ranking errors
are represented by switches between systems with
similar WERs, while it seems easier to discrimi-
nate between systems with more distant WER val-
ues. This consideration is in line with the findings
of Section 7.1 concerning the higher potential of
segment-level QE-informed ROVER in scenarios
featuring a higher diversity between the combined
systems.
</bodyText>
<table confidence="0.9996171">
tst2013 WER TRUE RR1 RR2 MLR
NICT 13.5 1 6 2 1
KIT 14.4 2 3 4 2
MITLL 15.9 3 1 1 4
RWTH 16.0 4 2 3 5
NAIST 16.2 5 5 5 3
UEDIN 22.1 6 8 8 7
FBK 23.2 7 4 6 6
PRKE 27.2 8 7 7 8
Spearman correlation 0.429 0.809 0.905
</table>
<tableCaption confidence="0.9958355">
Table 5: True and predicted IWSLT2013 system
ranks (correct predictions are shown in bold).
</tableCaption>
<sectionHeader confidence="0.997362" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999962625">
We presented a novel approach to improve the
combination of multiple automatic transcription
hypotheses using ROVER. Our method is based on
informing the fusion process with accurate word
error rate predictions obtained from ASR quality
estimation models. First, to exploit the possible
local diversity among the combined hypotheses,
it performs quality prediction and ranking at seg-
ment level. Then, the predicted ranks for each
segment are used to feed ROVER. Finally, the
combined hypotheses are concatenated to recon-
struct the entire utterance transcription. To rank
predictions, we compared two different regression
models with a machine-learned ranking method.
We carried out experiments on a set of English
TED talks collected for two editions of the IWSLT
ASR evaluation campaign. Results show that our
segment-level QE-informed ROVER outperforms
the standard random ROVER and performs on par
(differences are not statistically significant) with
a system-based ROVER oracle that exploits prior
knowledge about systems’ reliability. Moreover,
compared to a very strong segment-based ROVER
oracle, in one case the performance of our method
is not statistically different. These results are par-
ticularly encouraging, especially in light of the
fact that our approach does not exploit confidence
information related to the internal behaviour of the
ASR decoders. Overall, this represents the first
confirmation, obtained in an extrinsic evaluation
setting, of the good potential of reference-free and
system-agnostic ASR quality estimation.
</bodyText>
<page confidence="0.987291">
1103
</page>
<sectionHeader confidence="0.989916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999556018181818">
Kacem Abida, Fakhri Karray, and Wafa Abida. 2011.
cROVER: Improving ROVER using Automatic Er-
ror Detection. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, (ICASSP 2011), pages 1753–1756,
Prague, Czech Republic, May.
Kartik Audhkhasi, Andreas M Zavou, Panayiotis G
Georgiou, and Shrikanth S Narayanan. 2014. The-
oretical analysis of diversity in an ensemble of au-
tomatic speech recognition systems. IEEE/ACM
Transactions on Audio, Speech &amp; Language Pro-
cessing, 22(3):711–726.
Paul Boersma and David Weenink. 2005. Praat: Doing
Phonetics by Computer (Version 4.3.01). Retrieved
from http://www.praat.org/.
Fethi Bougares, Del´eglise, Est`eve Paul, Yannick, and
Mickael Rouvier. 2013. LIUM ASR System for
Etape French Evaluation Campaign: Experiments
on System Combination using Open-source Recog-
nizers. In Proceedings of the 16th International
Conference on Text, Speech, and Dialogue, pages
319–326, Pilsen, Czech Republic, September.
Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5–32.
Jos´e G. C. de Souza, Marco Turchi, and Matteo Ne-
gri. 2014. Machine Translation Quality Estimation
Across Domains. In Proceedings of the 25th Inter-
national Conference on Computational Linguistics
(COLING 2014): Technical Papers, pages 409–420,
Dublin, Ireland, August.
Jos´e G. C. de Souza, Hamed Zamani, Matteo Negri,
Marco Turchi, and Daniele Falavigna. 2015. Mul-
titask Learning for Adaptive Quality Estimation of
Automatically Transcribed Utterances. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies (NAACL
HLT 2015), Denver, Colorado, USA.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to Rank: from Pairwise
Approach to Listwise Approach. In Proceedings
of the 24th International Conference on Machine
learning (ICML-07), pages 129–136, Corvalis, Ore-
gon, USA.
Mauro Cettolo and Marcello Federico. 2000. Model
Selection Criteria for Acoustic Segmentation. In
ASR2000-Automatic Speech Recognition: Chal-
lenges for the new Millenium ISCA Tutorial and Re-
search Workshop (ITRW).
Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa
Bentivogli, and Marcello Federico. 2013. Report on
the 10th IWSLT Evaluation Campaign. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation (IWSLT 2013), Heidelberg, Ger-
many, December.
Hai Leong Chieu and Hwee Tou Ng. 2002. Named
Entity Recognition: A Maximum Entropy Approach
Using Global Information. In Proceedings of the
19th International Conference on Computational
Linguistics - Volume 1, COLING ’02, pages 1–7,
Taipei, Taiwan.
Jos´e G. C. de Souza, Christian Buck, Marco Turchi,
and Matteo Negri. 2013. FBK-UEdin participation
to the WMT13 quality estimation shared task. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 352–358, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Gunnar Evermann and PC Woodland. 2000. Pos-
terior Probability Decoding, Confidence Estimation
and System Combination. In Proceedings of NIST
Speech Transcription Workshop, volume 27, College
Park, MD, USA.
Marcello Federico, Luisa Bentivogli, Michael Paul,
and Sebastian St¨uker. 2012. Overview of the
IWSLT 2012 Evaluation Campaign. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation (IWSLT 2012), pages 11–27,
Hong Kong, December.
Jonathan G Fiscus. 1997. A Post-processing Sys-
tem to Yield Reduced Word Error Rates: Recog-
nizer Output Voting Error Reduction (ROVER). In
Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 347–
354, Santa Barbara, CA, USA. IEEE.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine learn-
ing, 63(1):3–42.
Laurence Gillick and Stephen J Cox. 1989. Some Sta-
tistical Issues in the Comparison of Speech Recog-
nition Algorithms. In Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing, (ICASSP 1989), pages 532–535,
Glasgow, Scotland.
Sharon Goldwater, Dan Jurafsky, and Christopher D
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181–200.
LI Hang. 2011. A short introduction to learning to
rank. IEICE TRANSACTIONS on Information and
Systems, 94(10):1854–1862.
Dustin Hillard, Bjoern Hoffmeister, Mari Ostendorf,
Ralf Schlueter, and Hermann Ney. 2007. iROVER:
Improving System Combination with Classification.
In Human Language Technologies 2007: The Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics; Companion
Volume, Short Papers, pages 65–68, Rochester, New
York, April.
</reference>
<page confidence="0.944722">
1104
</page>
<reference confidence="0.999807528089888">
Bj¨orn Hoffmeister, Tobias Klein, Ralf Schl¨uter, and
Hermann Ney. 2006. Frame Based System Com-
bination and a Comparison with Weighted ROVER
and CNC. In Proceedings of the International
Conference on Spoken Language Processing (Inter-
speech 2006 — ICSLP), pages 537–540, Pittsburgh,
PA, USA.
Xiang Li, Rita Singh, and Richard M. Stern. 2002.
Lattice Combination for Improved Speech Recogni-
tion. In Proceedings of the International Conference
of Spoken Language Processing, Denver, CO, USA,
September.
Lidia Mangu. 2000. Finding Consensus in Speech
Recognition. John Hopkins University. PhD The-
sis.
Brian McFee and Gert R Lanckriet. 2010. Metric
Learning to Rank. In Proceedings of the 27th Inter-
national Conference on Machine Learning (ICML-
10), pages 775–782, Haifa, Israel, June.
Frank McSherry and Marc Najork. 2008. Comput-
ing information retrieval performance measures ef-
ficiently in the presence of tied scores. In Advances
in information retrieval, pages 414–421. Springer.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012. Match without a Referee: Evaluating
MT Adequacy without Reference Translations. In
Proceedings of the Machine Translation Workshop
(WMT2012), pages 171–180, June.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network Based Language Model. In
Proceedings of INTERSPEECH 2010, 11th Annual
Conference of the International Speech Communi-
cation Association, pages 1045–1048, Makuhari,
Chiba, Japan, September.
Matteo Negri, Marco Turchi, Jos´e G. C. de Souza,
and Falavigna Daniele. 2014. Quality Estimation
for Automatic Speech Recognition. In Proceedings
of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 1813–1823, Dublin, Ireland, August.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. The Journal of Ma-
chine Learning Research, 12:2825–2830.
Thomas Pellegrini and Isabel Trancoso. 2010. Im-
proving ASR Error Detection with Non-decoder
Based Features. In Proceedings of INTERSPEECH
2010, 11th Annual Conference of the International
Speech Communication Association, pages 1950–
1953, Makuhari, Chiba, Japan, September.
Holger Schwenk and Jean-Luc Gauvain. 2000. Im-
proved ROVER using Language Model Information.
In ASR2000-Automatic Speech Recognition: Chal-
lenges for the new Millenium ISCA Tutorial and Re-
search Workshop (ITRW).
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Es-
timating the Sentence-Level Quality of Machine
Translation Systems. In Proceedings of the 13th
Annual Conference of the European Association
for Machine Translation (EAMT’09), pages 28–35,
Barcelona, Spain.
Andreas Stolcke, Harry Bratt, John Butzberger, Ho-
racio Franco, Venkata Ramana Gadde, Madelaine
Plauche, Colleen Richey, Elizabeth Shriberg, Kemal
Sonmez, F Weng, and Jing Zheng. 2000. The SRI
march 2000 HUB5 conversational speech transcrip-
tion system.
Yik-Cheung Tam, Yun Lei, Jing Zheng, and Wen
Wang. 2014. ASR Error Detection using Recur-
rent Neural Network Language Model and Comple-
mentary ASR. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, (ICASSP 2014), pages 2312–2316, Flo-
rence, Italy, May.
Marco Turchi, Antonios Anastasopoulos, Jos´e G. C. de
Souza, and Matteo Negri. 2014. Adaptive Qual-
ity Estimation for Machine Translation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 710–720, Baltimore, Maryland, June.
Rong Zhang and Alexander I. Rudnicky. 2006. In-
vestigations of Issues for Using Multiple Acoustic
Models to Improve Continuous Speech Recognition.
In Proceedings of INTERSPEECH, Pittsburgh, PA,
USA, September.
</reference>
<page confidence="0.995565">
1105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913737">
<title confidence="0.999947">Driving ROVER with Segment-based ASR Quality Estimation</title>
<author confidence="0.999673">Matteo Daniele Marco</author>
<affiliation confidence="0.9510245">(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, (2)University of Trento,</affiliation>
<abstract confidence="0.999720714285714">ROVER is a widely used method to combine the output of multiple automatic speech recognition (ASR) systems. Though effective, the basic approach and its variants suffer from potential drawresults depend on the order in which the hypotheses are used to feed combination process, applied to combine long hypotheses, they disregard possible differences in transcription at local level, often rely on word confidence information. We address these issues by proposing a segment-based ROVER in which hypothesis ranking is obtained from a confidence-independent ASR quality estimation method. Our results on English data from the IWSLT2012 and IWSLT2013 evaluation campaigns significantly outperform standard ROVER and approximate two strong oracles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kacem Abida</author>
<author>Fakhri Karray</author>
<author>Wafa Abida</author>
</authors>
<title>cROVER: Improving ROVER using Automatic Error Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>1753--1756</pages>
<location>ICASSP</location>
<contexts>
<context position="8003" citStr="Abida et al., 2011" startWordPosition="1270" endWordPosition="1273">her: ASR system combination, ASR quality estimation and machine-learned ranking. Fiscus (1997) proposed ROVER as an approach to produce a composite ASR output. The basic approach has been extended in several ways. N-Best ROVER (Stolcke et al., 2000) improves the original method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., 2013, inter alia). Note that all these combination methods require to have access to the inner structure of the ASR decoder, while ASR systems, especially the commercial ones, often do not provide this information. ASR quality estimation allows us to o</context>
<context position="11029" citStr="Abida et al., 2011" startWordPosition="1766" endWordPosition="1769">tead, represent the core of our contribution and are described in the following sections. 4 Segment-based QE-informed ROVER ROVER uses iterative dynamic programming to build a word transition network (WTN) from multiple ASR output hypotheses. The resulting WTN can be seen as a confusion network with an equal number of word arc hypotheses (one for each ASR system entering the combination) in each correspondence slot. The best word sequence is determined from the WTN via majority voting among the words in each slot. Most of the extensions of ROVER, such as iROVER (Hillard et al., 2007), cROVER (Abida et al., 2011) and the one described in (Zhang and Rudnicky, 2006), aim to learn a scoring function that allows improving the reordering of words inside each slot. In particular, iROVER reorders the words in each slot by means of a classifier trained with features that characterize the individual ASR systems. This approach, however, needs first to properly normalize the word lattices generated by each system, in order to exhibit the same vocabulary and similar densities, and to generate a unified segmentation for joining the lattices. In a similar way, motivated by the analysis shown in Table 1, our method </context>
</contexts>
<marker>Abida, Karray, Abida, 2011</marker>
<rawString>Kacem Abida, Fakhri Karray, and Wafa Abida. 2011. cROVER: Improving ROVER using Automatic Error Detection. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP 2011), pages 1753–1756, Prague, Czech Republic, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kartik Audhkhasi</author>
<author>Andreas M Zavou</author>
<author>Panayiotis G Georgiou</author>
<author>Shrikanth S Narayanan</author>
</authors>
<title>Theoretical analysis of diversity in an ensemble of automatic speech recognition systems.</title>
<date>2014</date>
<journal>IEEE/ACM Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="28818" citStr="Audhkhasi et al., 2014" startWordPosition="4720" endWordPosition="4724">ranscripts. Instead, SysO exhibits a less coherent behaviour, with close WER values at all levels, and a minimum in correspondence of column L4 (the combination of four transcriptions of the whole utterance). We interpret these results as a further motivation for our work: feeding ROVER with a good ranking that exploits local (segment-level) differences between the combined hypotheses seems to be more reliable than relying on system-level ranks based on global WER scores. A theoretical analysis of the relation between the diversity of the combined hypotheses and ROVER results is presented in (Audhkhasi et al., 2014). In light of this analysis, our results open an interesting issue concerning the trade-offs between optimal hypothesis ranking and their (local) diversity. We initially explore this problem in Section 7.1, but leave for future work a more systematic investigation. Rows 4-6 show the results achieved by RR1 (ranking by regression, trained with all the tran1101 scriptions for each input segment). When trained only with basic features, it always outperforms random ROVER. At L8 the gain is not statistically significant but, at the same time, also the WER difference with SysO is not significant. No</context>
</contexts>
<marker>Audhkhasi, Zavou, Georgiou, Narayanan, 2014</marker>
<rawString>Kartik Audhkhasi, Andreas M Zavou, Panayiotis G Georgiou, and Shrikanth S Narayanan. 2014. Theoretical analysis of diversity in an ensemble of automatic speech recognition systems. IEEE/ACM Transactions on Audio, Speech &amp; Language Processing, 22(3):711–726.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>David Weenink</author>
</authors>
<title>Praat: Doing Phonetics by Computer (Version 4.3.01). Retrieved from http://www.praat.org/.</title>
<date>2005</date>
<contexts>
<context position="19363" citStr="Boersma and Weenink, 2005" startWordPosition="3174" endWordPosition="3177">ork language model (RNNLM) probability (Mikolov et al., 2010). Word-based features (22) are: POS tag and score of the previous/current/next words (6), RNNLM probabilities (2) given by models trained on in-domain and out-of-domain data, indomain/out-of-domain 4-gram LM probability (2), number of phoneme classes (including fricatives, liquids, nasals, stops and vowels) (5), number of homophones (1), number of lexical neighbors (1) and binary features answering the three questions: “is stop word?” (1), “is before/after repetition?” 4Pitch features have been computed with the Praat software tool (Boersma and Weenink, 2005). Dataset duration sent token voc talks tst2012 1h45m 1,124 19.2k 2.8k 11 tst2013 4h50m 2,246 41.6k 5.6k 28 Table 2: Dataset statistics: duration, number of sentences, number of tokens, vocabulary size, number of talks. System tst2012 tst2013 FBK 16.8 23.2 KIT 12.7 14.4 MITLL 13.3 15.9 NAIST – 16.2 NICT 12.4 13.5 PRKE – 27.2 RWTH 13.6 16.0 UEDIN 14.4 22.1 Table 3: Official WER[%] scores of the participants in the IWSLT2012 and IWSLT2013 ASR evaluations. (2), “is before/after silence?” (2). Since the ASR hypotheses of a given segment might contain different numbers of words, we average the valu</context>
</contexts>
<marker>Boersma, Weenink, 2005</marker>
<rawString>Paul Boersma and David Weenink. 2005. Praat: Doing Phonetics by Computer (Version 4.3.01). Retrieved from http://www.praat.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fethi Bougares</author>
<author>Est`eve Paul Del´eglise</author>
<author>Yannick</author>
<author>Mickael Rouvier</author>
</authors>
<title>LIUM ASR System for Etape French Evaluation Campaign: Experiments on System Combination using Open-source Recognizers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 16th International Conference on Text, Speech, and Dialogue,</booktitle>
<pages>319--326</pages>
<location>Pilsen, Czech Republic,</location>
<marker>Bougares, Del´eglise, Yannick, Rouvier, 2013</marker>
<rawString>Fethi Bougares, Del´eglise, Est`eve Paul, Yannick, and Mickael Rouvier. 2013. LIUM ASR System for Etape French Evaluation Campaign: Experiments on System Combination using Open-source Recognizers. In Proceedings of the 16th International Conference on Text, Speech, and Dialogue, pages 319–326, Pilsen, Czech Republic, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="26462" citStr="Breiman, 2001" startWordPosition="4345" endWordPosition="4346">12.8 12.4 12.2 12.2 • 12.2 † • MLR +Basic 12.9 12.4 12.3 12.1 • 12.3 12.2 † • MLR +WordBased 12.4 • 12.1 12.0 12.0 • 12.2 • 12.2 † • MLR +Basic+WordBased 12.4 • 12.1 12.0 • 11.9 • * 12.2 • 12.2 † • Table 4: WER[%] (↓) of random, oracle and QE-informed ROVERs. The symbols assigned to some scores indicate their statistical significance (p G 0.05 computed with the matched-pairs test). In particular: “†” = the result is not statistically different from random ROVER; “•” = the result is not statistically different from SysO; “*” the result is not statistically different from SegO. ensemble method (Breiman, 2001) provided in the RankLib library.8 As mentioned in Section 6.1, all the ranking models are trained in 4-fold cross validation. RR1 uses all the instances in tst2012 (i.e. 1,124 segments transcribed by 6 ASR systems, which results in a total of 6,744 training instances). RR2 uses only one instance per segment, which is randomly selected among the 6 automatic transcriptions available in tst2012 (resulting in a total of 1,124 training instances). Similar to RR1, MLR uses all the instances in tst2012 (6,744 in total). The learning parameters of each model (number of bags, number of trees per bag, </context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>Machine Translation Quality Estimation Across Domains.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014): Technical Papers,</booktitle>
<pages>409--420</pages>
<location>Dublin, Ireland,</location>
<marker>de Souza, Turchi, Negri, 2014</marker>
<rawString>Jos´e G. C. de Souza, Marco Turchi, and Matteo Negri. 2014. Machine Translation Quality Estimation Across Domains. In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014): Technical Papers, pages 409–420, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Hamed Zamani</author>
<author>Matteo Negri</author>
<author>Marco Turchi</author>
<author>Daniele Falavigna</author>
</authors>
<title>Multitask Learning for Adaptive Quality Estimation of Automatically Transcribed Utterances.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015),</booktitle>
<location>Denver, Colorado, USA.</location>
<marker>de Souza, Zamani, Negri, Turchi, Falavigna, 2015</marker>
<rawString>Jos´e G. C. de Souza, Hamed Zamani, Matteo Negri, Marco Turchi, and Daniele Falavigna. 2015. Multitask Learning for Adaptive Quality Estimation of Automatically Transcribed Utterances. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015), Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhe Cao</author>
<author>Tao Qin</author>
<author>Tie-Yan Liu</author>
<author>Ming-Feng Tsai</author>
<author>Hang Li</author>
</authors>
<title>Learning to Rank: from Pairwise Approach to Listwise Approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine learning (ICML-07),</booktitle>
<pages>129--136</pages>
<location>Corvalis, Oregon, USA.</location>
<contexts>
<context position="9216" citStr="Cao et al., 2007" startWordPosition="1468" endWordPosition="1471">o overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real application scenario. Our new features are inspired by research on ASR error detection at word level (Goldwater et al., 2010; Pellegrini and Trancoso, 2010). Machine-learned ranking (MLR) or learning to rank (Hang, 2011) is widely used in information retrieval to order the answers to a user’s query (Cao et al., 2007; McFee and Lanckriet, 2010; McSherry and Najork, 2008). We use it to order the transcription hypotheses produced by multiple ASR systems and feed ROVER with the resulting ranked lists. 3 Method Given an utterance and a set of M transcription hypotheses produced by M different (possibly unknown) ASR systems, our goal is to: 1. Split the utterance into segments (ideally at sentence level); 2. For each segment, automatically estimate the quality (e.g. in terms of WER) of the corresponding M (segment-level) hypotheses; 3. Use the estimates to rank the hypotheses and feed ROVER based on the rankin</context>
</contexts>
<marker>Cao, Qin, Liu, Tsai, Li, 2007</marker>
<rawString>Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: from Pairwise Approach to Listwise Approach. In Proceedings of the 24th International Conference on Machine learning (ICML-07), pages 129–136, Corvalis, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Marcello Federico</author>
</authors>
<title>Model Selection Criteria for Acoustic Segmentation.</title>
<date>2000</date>
<booktitle>In ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW).</booktitle>
<contexts>
<context position="10234" citStr="Cettolo and Federico, 2000" startWordPosition="1630" endWordPosition="1634"> 2. For each segment, automatically estimate the quality (e.g. in terms of WER) of the corresponding M (segment-level) hypotheses; 3. Use the estimates to rank the hypotheses and feed ROVER based on the ranking; 4. Reconstruct the entire utterance transcription by concatenating the combined segmentlevel transcriptions produced by ROVER; 5. Measure the overall WER differences against standard ROVER and other oracles. Step 1 is performed by a start-end point detection module based on signal energy, which is followed by a segment classification module based on Gaussian Mixture Models similar to (Cettolo and Federico, 2000). Although the comparison with alternative splitting methods might lead to different results, this is not the main focus of the paper and is left as future work. Steps 2–4, instead, represent the core of our contribution and are described in the following sections. 4 Segment-based QE-informed ROVER ROVER uses iterative dynamic programming to build a word transition network (WTN) from multiple ASR output hypotheses. The resulting WTN can be seen as a confusion network with an equal number of word arc hypotheses (one for each ASR system entering the combination) in each correspondence slot. The </context>
</contexts>
<marker>Cettolo, Federico, 2000</marker>
<rawString>Mauro Cettolo and Marcello Federico. 2000. Model Selection Criteria for Acoustic Segmentation. In ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Jan Niehues</author>
<author>Sebastian St¨uker</author>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
</authors>
<title>Report on the 10th IWSLT Evaluation Campaign.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT 2013),</booktitle>
<location>Heidelberg, Germany,</location>
<marker>Cettolo, Niehues, St¨uker, Bentivogli, Federico, 2013</marker>
<rawString>Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th IWSLT Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT 2013), Heidelberg, Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named Entity Recognition: A Maximum Entropy Approach Using Global Information.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING ’02,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="18366" citStr="Chieu and Ng, 2002" startWordPosition="3025" endWordPosition="3028">hesitations, frequency of hesitations. Textual features (10) aim to capture the plausibility (i.e. the fluency) of a transcription. For each hypothesis textual features are: number of words, LM log probability, LM log probability of part of speech (POS), log perplexity, LM log perplexity of POS, percentage (%) of numbers, % of tokens which do not contain only “[a-z]”, % of content words, % of nouns, % of verbs. 5.2 Word-based features To compensate the absence of ASR confidence information, we also designed a set of “word-based” features inspired by previous approaches to ASR error detection (Chieu and Ng, 2002; Pellegrini and Trancoso, 2010; Goldwater et al., 2010; Tam et al., 2014). They aim to capture words’ pronunciation difficulty, which is determined by the number of lexical neighbors (similar pronunciations) and the types of phonemes that form the words. From the ASR error detection field we also borrow additional language model features based on recurrent neural network language model (RNNLM) probability (Mikolov et al., 2010). Word-based features (22) are: POS tag and score of the previous/current/next words (6), RNNLM probabilities (2) given by models trained on in-domain and out-of-domain</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2002. Named Entity Recognition: A Maximum Entropy Approach Using Global Information. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING ’02, pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Christian Buck</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>FBK-UEdin participation to the WMT13 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>352--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>de Souza, Buck, Turchi, Negri, 2013</marker>
<rawString>Jos´e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013. FBK-UEdin participation to the WMT13 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352–358, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunnar Evermann</author>
<author>PC Woodland</author>
</authors>
<title>Posterior Probability Decoding, Confidence Estimation and System Combination.</title>
<date>2000</date>
<booktitle>In Proceedings of NIST Speech Transcription Workshop,</booktitle>
<volume>27</volume>
<location>College Park, MD, USA.</location>
<contexts>
<context position="8306" citStr="Evermann and Woodland, 2000" startWordPosition="1319" endWordPosition="1322">tiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., 2013, inter alia). Note that all these combination methods require to have access to the inner structure of the ASR decoder, while ASR systems, especially the commercial ones, often do not provide this information. ASR quality estimation allows us to overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real applica</context>
</contexts>
<marker>Evermann, Woodland, 2000</marker>
<rawString>Gunnar Evermann and PC Woodland. 2000. Posterior Probability Decoding, Confidence Estimation and System Combination. In Proceedings of NIST Speech Transcription Workshop, volume 27, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Luisa Bentivogli</author>
<author>Michael Paul</author>
<author>Sebastian St¨uker</author>
</authors>
<title>Evaluation Campaign.</title>
<date>2012</date>
<journal>Overview of the IWSLT</journal>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT 2012),</booktitle>
<pages>11--27</pages>
<location>Hong Kong,</location>
<marker>Federico, Bentivogli, Paul, St¨uker, 2012</marker>
<rawString>Marcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian St¨uker. 2012. Overview of the IWSLT 2012 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT 2012), pages 11–27, Hong Kong, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A Post-processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>347--354</pages>
<publisher>IEEE.</publisher>
<location>Santa Barbara, CA, USA.</location>
<contexts>
<context position="1512" citStr="Fiscus, 1997" startWordPosition="214" endWordPosition="215">ty estimation method. Our results on English data from the IWSLT2012 and IWSLT2013 evaluation campaigns significantly outperform standard ROVER and approximate two strong oracles. 1 Introduction In automatic speech recognition (ASR), the combination of transcription hypotheses produced by multiple systems usually leads to significant word error rate (WER) reductions compared to the output of each individual system. Systems’ diversity and complementarity have been exploited in different ways to synthetically obtain more accurate transcriptions. Recognizer output voting error reduction – ROVER (Fiscus, 1997), the most widely used method, performs hypothesis fusion in two steps. First, the 1-best transcriptions from multiple systems are aligned by means of dynamic programming to build a single, minimal word transition network. Then, the resulting network is searched to select the best scoring word at each node. The final hypothesis is constructed via a majority voting mechanism and, if available, by using word confidence measures. This general strategy has been improved in several ways but, despite their proven effectiveness, ROVER and its variants have three potential drawbacks. The first one is </context>
<context position="7478" citStr="Fiscus (1997)" startWordPosition="1185" endWordPosition="1186">s aim, we extend previous ASR QE methods with new features (second contribution), and report significant improvements over standard ROVER on a shared dataset (third contribution). For the sake of brevity, our comparison is performed only against standard ROVER and in “black-box” conditions. However it’s worth remarking that our approach can be straightforwardly applied to any ROVER-like variant and, if available, by exploiting confidence features. 2 Related work This paper gathers three main research strands together: ASR system combination, ASR quality estimation and machine-learned ranking. Fiscus (1997) proposed ROVER as an approach to produce a composite ASR output. The basic approach has been extended in several ways. N-Best ROVER (Stolcke et al., 2000) improves the original method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition netw</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G Fiscus. 1997. A Post-processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER). In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 347– 354, Santa Barbara, CA, USA. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Geurts</author>
<author>Damien Ernst</author>
<author>Louis Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="24713" citStr="Geurts et al., 2006" startWordPosition="4032" endWordPosition="4035"> sake of comparison, we define three symbols for the evaluation results reported in Table 4: 1. “†” indicates that the corresponding WER score is not significantly different from random ROVER (a negative result); 2. “•” indicates that the WER score is not significantly different from the system-based ROVER oracle (a positive result); 3. “Y” indicates that the WER score is not significantly different from the segment-based ROVER oracle (the best result). 6.4 Ranking Models Ranking by regression (see Section 4.1) is performed using the implementation of the extremely randomized trees algorithm (Geurts et al., 2006) provided by the Scikit-learn package (Pedregosa et al., 2011). Extra-trees are a tree-based ensemble method for supervised classification and regression, which we successfully used in the past both for MT (de Souza et al., 2013) and ASR quality estimation (Negri et al., 2014). The model used for machine learned ranking (see Section 4.2) is based on the implementation of the random forest 7The word error rate is the minimum edit distance between an hypothesis and the reference transcription. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions) divided </context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Gillick</author>
<author>Stephen J Cox</author>
</authors>
<title>Some Statistical Issues in the Comparison of Speech Recognition Algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>532--535</pages>
<location>ICASSP</location>
<contexts>
<context position="23567" citStr="Gillick and Cox, 1989" startWordPosition="3852" endWordPosition="3855">on information about systems’ ranking (at a higher granularity level), which is not available in real testing conditions. As shown in Table 1, this is the strongest term of comparison and actually represents out upper bound. 6.3 Evaluation metric and significance test As usually done in ASR evaluation, performance results are measured in terms of WER.7 Our segment-based, QE-informed ROVER is hence compared against the other methods based on the WER computed on the test set (tst2013). To measure if two methods produce statistically different results, we run the matched-pairs significance test (Gillick and Cox, 1989). It is based on averaging the differences between the number of errors (insertions, deletions and substitutions) produced by the two approaches for the individual segments. If the average falls in the [-0.05,+0.05] interval, then the global WER difference between the two methods is not statistically significant. In terms of results’ significance tests, our success criteria are: i) a statistically significant improvement over random ROVER, and ii) nonsignificant differences with respect to the two strong oracles. For the sake of comparison, we define three symbols for the evaluation results re</context>
</contexts>
<marker>Gillick, Cox, 1989</marker>
<rawString>Laurence Gillick and Stephen J Cox. 1989. Some Statistical Issues in the Comparison of Speech Recognition Algorithms. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP 1989), pages 532–535, Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Which words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="9023" citStr="Goldwater et al., 2010" startWordPosition="1436" endWordPosition="1440"> methods require to have access to the inner structure of the ASR decoder, while ASR systems, especially the commercial ones, often do not provide this information. ASR quality estimation allows us to overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real application scenario. Our new features are inspired by research on ASR error detection at word level (Goldwater et al., 2010; Pellegrini and Trancoso, 2010). Machine-learned ranking (MLR) or learning to rank (Hang, 2011) is widely used in information retrieval to order the answers to a user’s query (Cao et al., 2007; McFee and Lanckriet, 2010; McSherry and Najork, 2008). We use it to order the transcription hypotheses produced by multiple ASR systems and feed ROVER with the resulting ranked lists. 3 Method Given an utterance and a set of M transcription hypotheses produced by M different (possibly unknown) ASR systems, our goal is to: 1. Split the utterance into segments (ideally at sentence level); 2. For each seg</context>
<context position="18421" citStr="Goldwater et al., 2010" startWordPosition="3033" endWordPosition="3036">ures (10) aim to capture the plausibility (i.e. the fluency) of a transcription. For each hypothesis textual features are: number of words, LM log probability, LM log probability of part of speech (POS), log perplexity, LM log perplexity of POS, percentage (%) of numbers, % of tokens which do not contain only “[a-z]”, % of content words, % of nouns, % of verbs. 5.2 Word-based features To compensate the absence of ASR confidence information, we also designed a set of “word-based” features inspired by previous approaches to ASR error detection (Chieu and Ng, 2002; Pellegrini and Trancoso, 2010; Goldwater et al., 2010; Tam et al., 2014). They aim to capture words’ pronunciation difficulty, which is determined by the number of lexical neighbors (similar pronunciations) and the types of phonemes that form the words. From the ASR error detection field we also borrow additional language model features based on recurrent neural network language model (RNNLM) probability (Mikolov et al., 2010). Word-based features (22) are: POS tag and score of the previous/current/next words (6), RNNLM probabilities (2) given by models trained on in-domain and out-of-domain data, indomain/out-of-domain 4-gram LM probability (2)</context>
</contexts>
<marker>Goldwater, Jurafsky, Manning, 2010</marker>
<rawString>Sharon Goldwater, Dan Jurafsky, and Christopher D Manning. 2010. Which words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates. Speech Communication, 52(3):181–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LI Hang</author>
</authors>
<title>A short introduction to learning to rank.</title>
<date>2011</date>
<journal>IEICE TRANSACTIONS on Information and Systems,</journal>
<volume>94</volume>
<issue>10</issue>
<contexts>
<context position="9119" citStr="Hang, 2011" startWordPosition="1452" endWordPosition="1453">e commercial ones, often do not provide this information. ASR quality estimation allows us to overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real application scenario. Our new features are inspired by research on ASR error detection at word level (Goldwater et al., 2010; Pellegrini and Trancoso, 2010). Machine-learned ranking (MLR) or learning to rank (Hang, 2011) is widely used in information retrieval to order the answers to a user’s query (Cao et al., 2007; McFee and Lanckriet, 2010; McSherry and Najork, 2008). We use it to order the transcription hypotheses produced by multiple ASR systems and feed ROVER with the resulting ranked lists. 3 Method Given an utterance and a set of M transcription hypotheses produced by M different (possibly unknown) ASR systems, our goal is to: 1. Split the utterance into segments (ideally at sentence level); 2. For each segment, automatically estimate the quality (e.g. in terms of WER) of the corresponding M (segment-</context>
<context position="27215" citStr="Hang, 2011" startWordPosition="4473" endWordPosition="4474">e instances in tst2012 (i.e. 1,124 segments transcribed by 6 ASR systems, which results in a total of 6,744 training instances). RR2 uses only one instance per segment, which is randomly selected among the 6 automatic transcriptions available in tst2012 (resulting in a total of 1,124 training instances). Similar to RR1, MLR uses all the instances in tst2012 (6,744 in total). The learning parameters of each model (number of bags, number of trees per bag, number of leaves per tree and minimum number of instances per leaf) are tuned by maximising Mean Average Precision as the objective function (Hang, 2011). All the models are trained using the basic features (+Basic), the word-based ones (+WordBased) and their combination (+Basic+WordBased). 7 Results and discussion Table 4 reports the WER results obtained on tst2013 by ROVER methods fed with: different numbers of hypotheses (from 3 to 8), at different granularity levels (whole utterance vs. segment), ranked with different models (random, RR1, RR2 and MLR) trained with different sets of features 8http://sourceforge.net/p/lemur/wiki/ RankLib/ (Basic, WordBased, Basic+WordBased). The first three rows present the results achieved by our terms of c</context>
</contexts>
<marker>Hang, 2011</marker>
<rawString>LI Hang. 2011. A short introduction to learning to rank. IEICE TRANSACTIONS on Information and Systems, 94(10):1854–1862.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Bjoern Hoffmeister</author>
<author>Mari Ostendorf</author>
<author>Ralf Schlueter</author>
<author>Hermann Ney</author>
</authors>
<title>iROVER: Improving System Combination with Classification.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>65--68</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="7875" citStr="Hillard et al., 2007" startWordPosition="1247" endWordPosition="1250"> variant and, if available, by exploiting confidence features. 2 Related work This paper gathers three main research strands together: ASR system combination, ASR quality estimation and machine-learned ranking. Fiscus (1997) proposed ROVER as an approach to produce a composite ASR output. The basic approach has been extended in several ways. N-Best ROVER (Stolcke et al., 2000) improves the original method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., 2013, inter alia). Note that all these combination methods require to have access to the inner structure of the ASR decoder,</context>
<context position="11000" citStr="Hillard et al., 2007" startWordPosition="1761" endWordPosition="1764"> as future work. Steps 2–4, instead, represent the core of our contribution and are described in the following sections. 4 Segment-based QE-informed ROVER ROVER uses iterative dynamic programming to build a word transition network (WTN) from multiple ASR output hypotheses. The resulting WTN can be seen as a confusion network with an equal number of word arc hypotheses (one for each ASR system entering the combination) in each correspondence slot. The best word sequence is determined from the WTN via majority voting among the words in each slot. Most of the extensions of ROVER, such as iROVER (Hillard et al., 2007), cROVER (Abida et al., 2011) and the one described in (Zhang and Rudnicky, 2006), aim to learn a scoring function that allows improving the reordering of words inside each slot. In particular, iROVER reorders the words in each slot by means of a classifier trained with features that characterize the individual ASR systems. This approach, however, needs first to properly normalize the word lattices generated by each system, in order to exhibit the same vocabulary and similar densities, and to generate a unified segmentation for joining the lattices. In a similar way, motivated by the analysis </context>
</contexts>
<marker>Hillard, Hoffmeister, Ostendorf, Schlueter, Ney, 2007</marker>
<rawString>Dustin Hillard, Bjoern Hoffmeister, Mari Ostendorf, Ralf Schlueter, and Hermann Ney. 2007. iROVER: Improving System Combination with Classification. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 65–68, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bj¨orn Hoffmeister</author>
<author>Tobias Klein</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>Frame Based System Combination and a Comparison with Weighted ROVER and CNC.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (Interspeech 2006 — ICSLP),</booktitle>
<pages>537--540</pages>
<location>Pittsburgh, PA, USA.</location>
<marker>Hoffmeister, Klein, Schl¨uter, Ney, 2006</marker>
<rawString>Bj¨orn Hoffmeister, Tobias Klein, Ralf Schl¨uter, and Hermann Ney. 2006. Frame Based System Combination and a Comparison with Weighted ROVER and CNC. In Proceedings of the International Conference on Spoken Language Processing (Interspeech 2006 — ICSLP), pages 537–540, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Li</author>
<author>Rita Singh</author>
<author>Richard M Stern</author>
</authors>
<title>Lattice Combination for Improved Speech Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference of Spoken Language Processing,</booktitle>
<location>Denver, CO, USA,</location>
<contexts>
<context position="8277" citStr="Li et al., 2002" startWordPosition="1315" endWordPosition="1318"> by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., 2013, inter alia). Note that all these combination methods require to have access to the inner structure of the ASR decoder, while ASR systems, especially the commercial ones, often do not provide this information. ASR quality estimation allows us to overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic</context>
</contexts>
<marker>Li, Singh, Stern, 2002</marker>
<rawString>Xiang Li, Rita Singh, and Richard M. Stern. 2002. Lattice Combination for Improved Speech Recognition. In Proceedings of the International Conference of Spoken Language Processing, Denver, CO, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
</authors>
<title>Finding Consensus in Speech Recognition.</title>
<date>2000</date>
<tech>PhD Thesis.</tech>
<institution>John Hopkins University.</institution>
<contexts>
<context position="8260" citStr="Mangu, 2000" startWordPosition="1313" endWordPosition="1314">iginal method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., 2013, inter alia). Note that all these combination methods require to have access to the inner structure of the ASR decoder, while ASR systems, especially the commercial ones, often do not provide this information. ASR quality estimation allows us to overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and per</context>
</contexts>
<marker>Mangu, 2000</marker>
<rawString>Lidia Mangu. 2000. Finding Consensus in Speech Recognition. John Hopkins University. PhD Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian McFee</author>
<author>Gert R Lanckriet</author>
</authors>
<title>Metric Learning to Rank.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning (ICML10),</booktitle>
<pages>775--782</pages>
<location>Haifa, Israel,</location>
<contexts>
<context position="9243" citStr="McFee and Lanckriet, 2010" startWordPosition="1472" endWordPosition="1475">oblem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real application scenario. Our new features are inspired by research on ASR error detection at word level (Goldwater et al., 2010; Pellegrini and Trancoso, 2010). Machine-learned ranking (MLR) or learning to rank (Hang, 2011) is widely used in information retrieval to order the answers to a user’s query (Cao et al., 2007; McFee and Lanckriet, 2010; McSherry and Najork, 2008). We use it to order the transcription hypotheses produced by multiple ASR systems and feed ROVER with the resulting ranked lists. 3 Method Given an utterance and a set of M transcription hypotheses produced by M different (possibly unknown) ASR systems, our goal is to: 1. Split the utterance into segments (ideally at sentence level); 2. For each segment, automatically estimate the quality (e.g. in terms of WER) of the corresponding M (segment-level) hypotheses; 3. Use the estimates to rank the hypotheses and feed ROVER based on the ranking; 4. Reconstruct the entir</context>
</contexts>
<marker>McFee, Lanckriet, 2010</marker>
<rawString>Brian McFee and Gert R Lanckriet. 2010. Metric Learning to Rank. In Proceedings of the 27th International Conference on Machine Learning (ICML10), pages 775–782, Haifa, Israel, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank McSherry</author>
<author>Marc Najork</author>
</authors>
<title>Computing information retrieval performance measures efficiently in the presence of tied scores.</title>
<date>2008</date>
<booktitle>In Advances in information retrieval,</booktitle>
<pages>414--421</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9271" citStr="McSherry and Najork, 2008" startWordPosition="1476" endWordPosition="1479">-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real application scenario. Our new features are inspired by research on ASR error detection at word level (Goldwater et al., 2010; Pellegrini and Trancoso, 2010). Machine-learned ranking (MLR) or learning to rank (Hang, 2011) is widely used in information retrieval to order the answers to a user’s query (Cao et al., 2007; McFee and Lanckriet, 2010; McSherry and Najork, 2008). We use it to order the transcription hypotheses produced by multiple ASR systems and feed ROVER with the resulting ranked lists. 3 Method Given an utterance and a set of M transcription hypotheses produced by M different (possibly unknown) ASR systems, our goal is to: 1. Split the utterance into segments (ideally at sentence level); 2. For each segment, automatically estimate the quality (e.g. in terms of WER) of the corresponding M (segment-level) hypotheses; 3. Use the estimates to rank the hypotheses and feed ROVER based on the ranking; 4. Reconstruct the entire utterance transcription by</context>
</contexts>
<marker>McSherry, Najork, 2008</marker>
<rawString>Frank McSherry and Marc Najork. 2008. Computing information retrieval performance measures efficiently in the presence of tied scores. In Advances in information retrieval, pages 414–421. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Match without a Referee: Evaluating MT Adequacy without Reference Translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Machine Translation Workshop (WMT2012),</booktitle>
<pages>171--180</pages>
<contexts>
<context position="6372" citStr="Mehdad et al., 2012" startWordPosition="1009" endWordPosition="1012">ility of a confidence-independent method to predict the quality of ASR transcriptions at segment level. This “quality estimation” (QE) task has been recently addressed in (Negri et al., 2014; C. de Souza et al., 2015) as a supervised regression problem in which transcriptions’ WER is predicted without having access to reference transcripts.3 Different feature sets have been evaluated, showing that even with those extracted only 2Details about this dataset will be provided in Section 6.1. 3This formulation is very similar to the machine translation counterpart of the task (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014; C. de Souza et al., 2014). from the signal and the transcription (i.e. disregarding information about the decoding process) the prediction error is sufficiently low to open to real applications. However, though promising, experimental results stem from an intrinsic evaluation in which QE is only addressed in isolation. By applying it to inform ROVER, we propose for the first time an application-oriented extrinsic evaluation of ASR QE (our first contribution). To this aim, we extend previous ASR QE methods with new features (second contribution), and report significant im</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012), pages 171–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent Neural Network Based Language Model.</title>
<date>2010</date>
<booktitle>In Proceedings of INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent Neural Network Based Language Model. In Proceedings of INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, pages 1045–1048, Makuhari, Chiba, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Marco Turchi</author>
<author>Jos´e G C de Souza</author>
<author>Falavigna Daniele</author>
</authors>
<title>Quality Estimation for Automatic Speech Recognition.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1813--1823</pages>
<location>Dublin, Ireland,</location>
<marker>Negri, Turchi, de Souza, Daniele, 2014</marker>
<rawString>Matteo Negri, Marco Turchi, Jos´e G. C. de Souza, and Falavigna Daniele. 2014. Quality Estimation for Automatic Speech Recognition. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1813–1823, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Pellegrini</author>
<author>Isabel Trancoso</author>
</authors>
<title>Improving ASR Error Detection with Non-decoder Based Features.</title>
<date>2010</date>
<booktitle>In Proceedings of INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>pages</pages>
<location>Makuhari, Chiba, Japan,</location>
<contexts>
<context position="9055" citStr="Pellegrini and Trancoso, 2010" startWordPosition="1441" endWordPosition="1444"> access to the inner structure of the ASR decoder, while ASR systems, especially the commercial ones, often do not provide this information. ASR quality estimation allows us to overcome this problem and obtain confidence-independent estimates of ASR output quality. Based on the positive intrinsic evaluation results reported in 1096 (Negri et al., 2014; C. de Souza et al., 2015), here we extend the approach with new features and perform an extrinsic evaluation in a real application scenario. Our new features are inspired by research on ASR error detection at word level (Goldwater et al., 2010; Pellegrini and Trancoso, 2010). Machine-learned ranking (MLR) or learning to rank (Hang, 2011) is widely used in information retrieval to order the answers to a user’s query (Cao et al., 2007; McFee and Lanckriet, 2010; McSherry and Najork, 2008). We use it to order the transcription hypotheses produced by multiple ASR systems and feed ROVER with the resulting ranked lists. 3 Method Given an utterance and a set of M transcription hypotheses produced by M different (possibly unknown) ASR systems, our goal is to: 1. Split the utterance into segments (ideally at sentence level); 2. For each segment, automatically estimate the</context>
<context position="18397" citStr="Pellegrini and Trancoso, 2010" startWordPosition="3029" endWordPosition="3032">cy of hesitations. Textual features (10) aim to capture the plausibility (i.e. the fluency) of a transcription. For each hypothesis textual features are: number of words, LM log probability, LM log probability of part of speech (POS), log perplexity, LM log perplexity of POS, percentage (%) of numbers, % of tokens which do not contain only “[a-z]”, % of content words, % of nouns, % of verbs. 5.2 Word-based features To compensate the absence of ASR confidence information, we also designed a set of “word-based” features inspired by previous approaches to ASR error detection (Chieu and Ng, 2002; Pellegrini and Trancoso, 2010; Goldwater et al., 2010; Tam et al., 2014). They aim to capture words’ pronunciation difficulty, which is determined by the number of lexical neighbors (similar pronunciations) and the types of phonemes that form the words. From the ASR error detection field we also borrow additional language model features based on recurrent neural network language model (RNNLM) probability (Mikolov et al., 2010). Word-based features (22) are: POS tag and score of the previous/current/next words (6), RNNLM probabilities (2) given by models trained on in-domain and out-of-domain data, indomain/out-of-domain 4</context>
</contexts>
<marker>Pellegrini, Trancoso, 2010</marker>
<rawString>Thomas Pellegrini and Isabel Trancoso. 2010. Improving ASR Error Detection with Non-decoder Based Features. In Proceedings of INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, pages 1950– 1953, Makuhari, Chiba, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Improved ROVER using Language Model Information.</title>
<date>2000</date>
<booktitle>In ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW).</booktitle>
<contexts>
<context position="7751" citStr="Schwenk and Gauvain (2000)" startWordPosition="1227" endWordPosition="1231"> and in “black-box” conditions. However it’s worth remarking that our approach can be straightforwardly applied to any ROVER-like variant and, if available, by exploiting confidence features. 2 Related work This paper gathers three main research strands together: ASR system combination, ASR quality estimation and machine-learned ranking. Fiscus (1997) proposed ROVER as an approach to produce a composite ASR output. The basic approach has been extended in several ways. N-Best ROVER (Stolcke et al., 2000) improves the original method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., </context>
</contexts>
<marker>Schwenk, Gauvain, 2000</marker>
<rawString>Holger Schwenk and Jean-Luc Gauvain. 2000. Improved ROVER using Language Model Information. In ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the Sentence-Level Quality of Machine Translation Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09),</booktitle>
<pages>28--35</pages>
<location>Barcelona,</location>
<contexts>
<context position="6351" citStr="Specia et al., 2009" startWordPosition="1005" endWordPosition="1008">s goal is the availability of a confidence-independent method to predict the quality of ASR transcriptions at segment level. This “quality estimation” (QE) task has been recently addressed in (Negri et al., 2014; C. de Souza et al., 2015) as a supervised regression problem in which transcriptions’ WER is predicted without having access to reference transcripts.3 Different feature sets have been evaluated, showing that even with those extracted only 2Details about this dataset will be provided in Section 6.1. 3This formulation is very similar to the machine translation counterpart of the task (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014; C. de Souza et al., 2014). from the signal and the transcription (i.e. disregarding information about the decoding process) the prediction error is sufficiently low to open to real applications. However, though promising, experimental results stem from an intrinsic evaluation in which QE is only addressed in isolation. By applying it to inform ROVER, we propose for the first time an application-oriented extrinsic evaluation of ASR QE (our first contribution). To this aim, we extend previous ASR QE methods with new features (second contribution), and </context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09), pages 28–35, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Harry Bratt</author>
<author>John Butzberger</author>
<author>Horacio Franco</author>
</authors>
<title>HUB5 conversational speech transcription system.</title>
<date>2000</date>
<booktitle>The SRI</booktitle>
<institution>Venkata Ramana Gadde, Madelaine Plauche, Colleen Richey, Elizabeth Shriberg, Kemal</institution>
<contexts>
<context position="7633" citStr="Stolcke et al., 2000" startWordPosition="1210" endWordPosition="1213">ed dataset (third contribution). For the sake of brevity, our comparison is performed only against standard ROVER and in “black-box” conditions. However it’s worth remarking that our approach can be straightforwardly applied to any ROVER-like variant and, if available, by exploiting confidence features. 2 Related work This paper gathers three main research strands together: ASR system combination, ASR quality estimation and machine-learned ranking. Fiscus (1997) proposed ROVER as an approach to produce a composite ASR output. The basic approach has been extended in several ways. N-Best ROVER (Stolcke et al., 2000) improves the original method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or conf</context>
</contexts>
<marker>Stolcke, Bratt, Butzberger, Franco, 2000</marker>
<rawString>Andreas Stolcke, Harry Bratt, John Butzberger, Horacio Franco, Venkata Ramana Gadde, Madelaine Plauche, Colleen Richey, Elizabeth Shriberg, Kemal Sonmez, F Weng, and Jing Zheng. 2000. The SRI march 2000 HUB5 conversational speech transcription system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Yun Lei</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
</authors>
<title>ASR Error Detection using Recurrent Neural Network Language Model and Complementary ASR.</title>
<date>2014</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP 2014),</booktitle>
<pages>2312--2316</pages>
<location>Florence, Italy,</location>
<contexts>
<context position="18440" citStr="Tam et al., 2014" startWordPosition="3037" endWordPosition="3040"> the plausibility (i.e. the fluency) of a transcription. For each hypothesis textual features are: number of words, LM log probability, LM log probability of part of speech (POS), log perplexity, LM log perplexity of POS, percentage (%) of numbers, % of tokens which do not contain only “[a-z]”, % of content words, % of nouns, % of verbs. 5.2 Word-based features To compensate the absence of ASR confidence information, we also designed a set of “word-based” features inspired by previous approaches to ASR error detection (Chieu and Ng, 2002; Pellegrini and Trancoso, 2010; Goldwater et al., 2010; Tam et al., 2014). They aim to capture words’ pronunciation difficulty, which is determined by the number of lexical neighbors (similar pronunciations) and the types of phonemes that form the words. From the ASR error detection field we also borrow additional language model features based on recurrent neural network language model (RNNLM) probability (Mikolov et al., 2010). Word-based features (22) are: POS tag and score of the previous/current/next words (6), RNNLM probabilities (2) given by models trained on in-domain and out-of-domain data, indomain/out-of-domain 4-gram LM probability (2), number of phoneme</context>
</contexts>
<marker>Tam, Lei, Zheng, Wang, 2014</marker>
<rawString>Yik-Cheung Tam, Yun Lei, Jing Zheng, and Wen Wang. 2014. ASR Error Detection using Recurrent Neural Network Language Model and Complementary ASR. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP 2014), pages 2312–2316, Florence, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Antonios Anastasopoulos</author>
<author>Jos´e G C de Souza</author>
<author>Matteo Negri</author>
</authors>
<title>Adaptive Quality Estimation for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>710--720</pages>
<location>Baltimore, Maryland,</location>
<marker>Turchi, Anastasopoulos, de Souza, Negri, 2014</marker>
<rawString>Marco Turchi, Antonios Anastasopoulos, Jos´e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estimation for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 710–720, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zhang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Investigations of Issues for Using Multiple Acoustic Models to Improve Continuous Speech Recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="11081" citStr="Zhang and Rudnicky, 2006" startWordPosition="1776" endWordPosition="1779">nd are described in the following sections. 4 Segment-based QE-informed ROVER ROVER uses iterative dynamic programming to build a word transition network (WTN) from multiple ASR output hypotheses. The resulting WTN can be seen as a confusion network with an equal number of word arc hypotheses (one for each ASR system entering the combination) in each correspondence slot. The best word sequence is determined from the WTN via majority voting among the words in each slot. Most of the extensions of ROVER, such as iROVER (Hillard et al., 2007), cROVER (Abida et al., 2011) and the one described in (Zhang and Rudnicky, 2006), aim to learn a scoring function that allows improving the reordering of words inside each slot. In particular, iROVER reorders the words in each slot by means of a classifier trained with features that characterize the individual ASR systems. This approach, however, needs first to properly normalize the word lattices generated by each system, in order to exhibit the same vocabulary and similar densities, and to generate a unified segmentation for joining the lattices. In a similar way, motivated by the analysis shown in Table 1, our method applies reordering of the ASR hypotheses at segment </context>
</contexts>
<marker>Zhang, Rudnicky, 2006</marker>
<rawString>Rong Zhang and Alexander I. Rudnicky. 2006. Investigations of Issues for Using Multiple Acoustic Models to Improve Continuous Speech Recognition. In Proceedings of INTERSPEECH, Pittsburgh, PA, USA, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>