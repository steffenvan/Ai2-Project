<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.997037">
A Statistical NLG Framework for Aggregated Planning and Realization
</title>
<author confidence="0.992663">
Ravi Kondadadi∗, Blake Howald and Frank Schilder
</author>
<affiliation confidence="0.919311">
Thomson Reuters, Research &amp; Development
</affiliation>
<address confidence="0.785085">
610 Opperman Drive, Eagan, MN 55123
</address>
<email confidence="0.99475">
firstname.lastname@thomsonreuters.com
</email>
<sectionHeader confidence="0.993792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915722222222">
We present a hybrid natural language gen-
eration (NLG) system that consolidates
macro and micro planning and surface re-
alization tasks into one statistical learn-
ing process. Our novel approach is based
on deriving a template bank automatically
from a corpus of texts from a target do-
main. First, we identify domain specific
entity tags and Discourse Representation
Structures on a per sentence basis. Each
sentence is then organized into semanti-
cally similar groups (representing a do-
main specific concept) by k-means cluster-
ing. After this semi-automatic processing
(human review of cluster assignments), a
number of corpus–level statistics are com-
piled and used as features by a ranking
SVM to develop model weights from a
training corpus. At generation time, a set
of input data, the collection of semanti-
cally organized templates, and the model
weights are used to select optimal tem-
plates. Our system is evaluated with au-
tomatic, non–expert crowdsourced and ex-
pert evaluation metrics. We also introduce
a novel automatic metric – syntactic vari-
ability – that represents linguistic variation
as a measure of unique template sequences
across a collection of automatically gener-
ated documents. The metrics for generated
weather and biography texts fall within ac-
ceptable ranges. In sum, we argue that our
statistical approach to NLG reduces the
need for complicated knowledge-based ar-
chitectures and readily adapts to different
domains with reduced development time.
</bodyText>
<footnote confidence="0.369614">
∗*Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
</footnote>
<sectionHeader confidence="0.996935" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999329948717949">
NLG is the process of generating natural-sounding
text from non-linguistic inputs. A typical NLG
system contains three main components: (1) Doc-
ument (Macro) Planning - deciding what content
should be realized in the output and how it should
be structured; (2) Sentence (Micro) planning -
generating a detailed sentence specification and
selecting appropriate referring expressions; and
(3) Surface Realization - generating the final text
after applying morphological modifications based
on syntactic rules (see e.g., Bateman and Zock
(2003), Reiter and Dale (2000) and McKeown
(1985)). However, document planning is arguably
one of the most crucial components of an NLG
system and is responsible for making the texts ex-
press the desired communicative goal in a coher-
ent structure. If the document planning stage fails,
the communicative goal of the generated text will
not be met even if the other two stages are perfect.
While most traditional systems simplify develop-
ment by using a pipelined approach where (1-3)
are executed in a sequence, this can result in er-
rors at one stage propagating to successive stages
(see e.g., Robin and McKeown (1996)). We pro-
pose a hybrid framework that combines (1-3) by
converting data to text in one single process.
Most NLG systems fall into two broad
categories: knowledge-based and statistical.
Knowledge-based systems heavily depend on hav-
ing domain expertise to come up with hand-
crafted rules at each stage of a pipeline. Although
knowledge-based systems can produce high qual-
ity text, they are (1) very expensive to build, in-
volving a lot of discussion with the end users of the
system for the document planning stage alone; (2)
have limited linguistic coverage, as it is time con-
suming to capture linguistic variation; and (3) one
has to start from scratch for each new domain be-
cause the developed components cannot be reused.
</bodyText>
<page confidence="0.924867">
1406
</page>
<note confidence="0.9133475">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1406–1415,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999917078431373">
Statistical systems, on the other hand, are fairly
inexpensive, more adaptable and rely on having
historical data for the given domain. Coverage is
likely to be high if more historical data is avail-
able. The main disadvantage with statistical sys-
tems is that they are more prone to errors and the
output text may not be coherent as there are less
constraints on the generated text.
Our framework is a hybrid of statistical and
template-based systems. Many knowledge-based
systems use templates to generate text. A tem-
plate structure contains “gaps” that are filled to
generate the output. The idea is to create a lot
of templates from the historical data and select
the right template based on some constraints. To
the best of our knowledge, this is the first hy-
brid statistical-template-based system that com-
bines all three stages of NLG. Experiments with
different variants of our system (for biography and
weather subject matter domains) demonstrate that
our system generates reasonable texts.
Also, in addition to the standard metrics used
to evaluate NLG systems (e.g., BLEU, NIST, etc.),
we present a unique text evaluation metric called
syntactic variability to measure the linguistic vari-
ation of generated texts. This metric applies to the
document collection level and is based on com-
puting the number of unique template sequences
among all the generated texts. A higher number
indicates the texts are more variable and natural-
sounding whereas a lower number shows they are
more redundant. We argue that this metric is use-
ful for evaluating template-based systems and for
any type of text generation for domains where lin-
guistic variability is favored (e.g., the user is ex-
pected to go through more than one document in
the same session).
The main contributions of this paper are (1) A
statistical NLG system that combines document
and sentence planning and surface realization into
one single process; and (2) A new metric – syntac-
tic variability – is proposed to measure the syntac-
tic and morphological variability of the generated
texts. We believe this is the first work to propose
an automatic metric to measure linguistic variabil-
ity of generated texts in NLG.
Section 2 provides an overview of related work
on NLG. We present our main system in Section 3.
The system is evaluated and discussed in Section
4. Finally, we conclude in Section 5 and point out
future directions of research.
</bodyText>
<sectionHeader confidence="0.943563" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99998148">
Typically, knowledge-based NLG systems are im-
plemented by rules and, as mentioned above, have
a pipelined architecture for the document and
sentence planning stages and surface realization
(Hovy, 1993; Moore and Paris, 1993). However,
document planning is arguably the most impor-
tant task (Sripada et al., 2001). It follows that ap-
proaches to document planning are rule-based as
well and, concomitantly, are usually domain spe-
cific. For example, Bouayad-Agha, et al. (2011)
proposed document planning based on an ontol-
ogy knowledge base to generate football sum-
maries. For rule–based systems, rules exist for
selecting content to grammatical choices to post-
processing (e.g., pronoun generation). These rules
are often tailored to a given system, with input
from multiple experts; consequently, there is a
high associated development cost (e.g., 12 person
months for the SUMTIME-METEO system (Belz,
2007)).
Statistical approaches can reduce extensive de-
velopment time by relying on corpus data to
“learn” rules for one or more components of an
NLG system (Langkilde and Knight, 1998). For
example, Duboue and McKeown (2003) proposed
a statistical approach to extract content selection
rules for biography descriptions. Further, statisti-
cal approaches should be more adaptable to differ-
ent domains than their rule-based equivalents (An-
geli et al., 2012). For example, Barzilay and Lap-
ata (2005) formulated content selection as a clas-
sification task to produce football summaries and
Kelly et al. (2009) extended Barzilay and Lapata’s
approach for generating match reports for cricket.
The present work builds on Howald et al.
(2013) where, in a given corpus, a combination of
domain specific named entity tagging and cluster-
ing sentences (based on semantic predicates) were
used to generate templates. However, while the
system consolidated both sentence planning and
surface realization with this approach (described
in more detail in Section 3), the document plan
was given via the input data and sequencing infor-
mation was present in training documents. For the
present research, we introduce a similar method
that leverages the distributions of document–level
features in the training corpus to incorporate a
statistical document planning component. Con-
sequently, we are able to create a streamlined
statistical NLG architecture that balances natural
</bodyText>
<page confidence="0.989711">
1407
</page>
<bodyText confidence="0.98597">
human–like variability with appropriate and accu-
rate information.
</bodyText>
<sectionHeader confidence="0.997458" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9989538">
In order to generate text for a given domain our
system runs input data through a statistical ranking
model to select a sequence of templates that best
fit the input data (E). In order to build the rank-
ing model, our system takes historical data (cor-
pus) for the domain through four components: (A)
preprocessing; (B) “conceptual unit” creation; (C)
collecting statistics; and (D) ranking model build-
ing (summarized in Figure 1). In this section, we
describe each component in detail.
</bodyText>
<figureCaption confidence="0.998939">
Figure 1: System Architecture.
</figureCaption>
<subsectionHeader confidence="0.997941">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999014">
The first component processes the given corpus to
extract templates. We assume that each document
in the corpus is classified to a specific domain.
Preprocessing involves uncovering the underlying
semantic structure of the corpus and using this as
a foundation for template creation (Lu et al., 2009;
Lu and Ng, 2011; Konstas and Lapata, 2012).
We first split each document in the corpus into
sentences and create a shallow Discourse Repre-
sentation Structure (following Discourse Repre-
sentation Theory (Kamp and Reyle, 1993)) of each
sentence. The DRS consists of semantic predi-
cates and named entity tags. We use Boxer se-
mantic analyzer (Bos, 2008) to extract semantic
predicates such as EVENT or DATE. In parallel,
domain specific named entity tags are identified
and, in conjunction with the semantic predicates,
are used to create templates. We developed the
named-entity tagger for the weather domain our-
selves. To tag entities in the biography domain,
we used OpenCalais (www.opencalais.com). For
example, in the biography in (1), the conceptual
meaning (semantic predicates and domain-specific
entities) of sentences (a-b) are represented in (c-d).
The corresponding templates are showing in (e-f).
</bodyText>
<listItem confidence="0.894576">
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing Di-
rector of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor’s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING  |TITLE  |PERSON  |COMPANY  |DATE
d. HOLDS  |DEGREE  |SUBJECT  |INSTITUTION |EVENT
</listItem>
<subsubsectionHeader confidence="0.500417">
Templates
</subsubsectionHeader>
<listItem confidence="0.8613245">
e. [person] has been serving as [title] of the [company]
since [date].
f. [person] holds a [degree] in [subject] from [institution]
and a [degree] from [institution].
</listItem>
<bodyText confidence="0.999771">
The outputs of the preprocessing stage are the tem-
plate bank and predicate information for each tem-
plate in the corpus.1
</bodyText>
<subsectionHeader confidence="0.999802">
3.2 Creating Conceptual Units
</subsectionHeader>
<bodyText confidence="0.999972941176471">
The next step is to create conceptual units for the
corpus by clustering templates. This is a semi-
automatic process where we use the predicate in-
formation for each template to compute similar-
ity between templates. We use k-means clustering
with k (equivalent to the number of semantic con-
cepts in the domain) set to an arbitrarily high value
(100) to over-generate (using the WEKA toolkit
(Witten and Frank, 2005)). This allows for easier
manual verification of the generated clusters and
we merge them if necessary. We assign a unique
identifier called a CuId (Conceptual Unit Identi-
fier) to each cluster, which represents a “concep-
tual unit”. We associate each template in the cor-
pus to a corresponding CuId. For example, in (2),
using the templates in (1e-f), the identified named
entities are assigned to a clustered CuId (2a-b).
</bodyText>
<listItem confidence="0.9862622">
(2) Conceptual Units
a. {CuId : 000} – [person] has been serving as [title] of the
[company] since [date].
b. {CuId : 001} – [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
</listItem>
<bodyText confidence="0.996479307692308">
At this stage, we will have a set of conceptual
units with corresponding template collections (see
Howald et al. (2013) for a further explanation of
Sections 3.1-3.2).
1A similar approach to the clustering of semantic content
is found in Duboue and McKeown (2003), where text with
stopwords removed were used as semantic input. Boxer pro-
vides a similar representation with the addition of domain
general tags. However, to contrast our work from Duboue
and McKeown, which focused on content selection, we are
focused on learning templates from the semantic representa-
tions for the complete generation system (covering content
selection, aggregation, sentence and document planning).
</bodyText>
<page confidence="0.957465">
1408
</page>
<subsectionHeader confidence="0.998387">
3.3 Collecting Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999676">
After identifying the different conceptual units and
the template bank, we collect a number of statistics
from the corpus:
</bodyText>
<listItem confidence="0.998737952380952">
• Frequency distribution of templates overall and per po-
sition
• Frequency distribution of CuIds overall and per posi-
tion
• Average number of entity tags by CuId as well as the
entity distribution by CuId
• Average number of entity tags by position as well as
the entity distribution by position
• Average number of words per CuId.
• Average number of words per CuId and position com-
bination.
• Average number of words per position
• Frequency distribution of the main verbs by position
• Frequency distribution of CuId sequences (bigrams and
trigrams only) overall and per position
• Frequency distribution of template sequences (bigrams
and trigrams only) overall and per position
• Frequency distribution of entity tag sequences overall
and per position
• The average, minimum, maximum number of CuIds
across all documents
</listItem>
<bodyText confidence="0.999800666666667">
As discussed in the next section, these statistics
are turned into features used for building a ranking
model in the next component.
</bodyText>
<subsectionHeader confidence="0.99932">
3.4 Building a ranking model
</subsectionHeader>
<bodyText confidence="0.99995576">
The core component of our system is a statistical
model that ranks a set of templates for a given
position (sentence 1, sentence 2, ..., sentence n)
based on the input data. The input data in our
tasks was extracted from a training document; this
serves as a temporary surrogate to a database. The
task is to learn the ranks of all the templates from
all CuIds at each position.
To generate the training data, we first filter the
templates that have named entity tags not specified
in the input data. This will make sure the gener-
ated text does not have any unfilled entity tags. We
then rank templates according to the Levenshtein
edit distance (Levenshtein, 1966) from the tem-
plate corresponding to the current sentence in the
training document (using the top 10 ranked tem-
plates in training for ease of processing effort). We
experimented with other ranking schemes such as
entity-based similarity (similarity between entity
sequences in the templates) and a combination of
edit-distance based and entity-based similarities.
We obtained better results with edit distance. For
each template, we generate the following features
to build the ranking model. Most of the features
are based on the corpus statistics mentioned above.
</bodyText>
<listItem confidence="0.974302325581395">
• CuId given position: This is a binary feature where
the current CuId is either the same as the most frequent
CuId for the position (1) or not (0).
• Overlap of named entities: Number of common enti-
ties between current CuId and most likely CuId for the
position
• Prior template: Probability of the sequence of tem-
plates selected at the previous position and the current
template (iterated for the last three positions).
• Prior CuId: Probability of the sequence of the CuId
selected at the previous position and the current CuId
(iterated for the last three positions).
• Difference in number of words: Absolute difference
between number of words for current template and av-
erage number of words for the CuId
• Difference in number of words given position: Ab-
solute difference between number of words for cur-
rent template and average number of words for CuId
at given position
• Percentage of unused data: This feature represents
the portion of the unused input so far.
• Difference in number of named entities: Absolute
difference between the number of named entities in the
current template and the average number of named en-
tities for the current position
• Most frequent verb for the position: Binary valued
feature where the main verb of the template belongs to
the most frequent verb group given the position is either
the same (1) or not (0).
• Average number of words used: Ratio of number of
words in the generated text so far to the average number
of words.
• Average number of entities: Ratio of number of
named entities in the generated text so far to the av-
erage number of named entities.
• Most likely CuId given position and previous CuId:
Binary feature indicating if the current CuId is most
likely given the position and the previous CuId.
• Similarity between the most likely template in CuId
and current template: Edit distance between the cur-
rent template and the most likely template for the cur-
rent CuId.
• Similarity between the most likely template in CuId
</listItem>
<bodyText confidence="0.790152571428571">
given position and current template: Edit distance
between the current template and the most likely tem-
plate for the current CuId at the current position.
We used a linear kernel for a ranking SVM
(Joachims, 2002) (cost set to total queries) to learn
the weights associated with each feature for the
different domains.
</bodyText>
<subsectionHeader confidence="0.662123">
3.5 Generation
</subsectionHeader>
<bodyText confidence="0.999976363636364">
At generation time, our system has a set of in-
put data, a semantically organized template bank
(collection of templates organized by CuId) and a
model from training on the documents for a given
domain. We first filter out those templates that
contain a named entity tag not present in the in-
put data. Then, we compute a score for each of the
remaining templates from the feature values and
the feature weights from the model. The template
with the highest overall score is selected and filled
with matching entity tags from the input data and
</bodyText>
<page confidence="0.983279">
1409
</page>
<bodyText confidence="0.9891645">
appended to the generated text.
Before generating the next sentence, we track
those entities used in the initial sentence gener-
ation and decide to either remove those entities
from the input data or keep the entity for one or
more additional sentence generations. For exam-
ple, in the biography discourses, the name of the
person may occur only once in the input data, but
it may be useful for creating good texts to have
that person’s name available for subsequent gen-
erations. To illustrate in (3), if we remove James
Smithton from the input data after the initial gen-
eration, an irrelevant sentence (d) is generated as
the input data will only have one company after
the removal of James Smithton and the model will
only select a template with one company. If we
keep James Smithton, then the generations in (a-b)
are more cohesive.
</bodyText>
<table confidence="0.454677333333333">
(3) Use more than once
a. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
b. Previously, Mr. Smithton was CFO of the Keyes
Development Group.
Use once and remove
c. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
d. Keyes Development Group is a venture capital firm.
</table>
<bodyText confidence="0.997820913043478">
Deciding on what type of entities and how to
remove them is different for each domain. For ex-
ample, some entities are very unique to a text and
should not be made available for subsequent gen-
erations as doing so would lead to unwanted re-
dundancies (e.g., mentioning the name of current
company in a biography discourse more than once
as in (3)) and some entities are general and should
be retained. Our system possesses the ability to
monitor the data usage from historical data and we
can set parameters (based on the distribution of en-
tities) on the usage to ensure coherent generations
for a given domain.
Once the input data has been modified (i.e., an
entity have been removed, replaced or retained),
it serves as the new input data for the next sen-
tence generation. This process repeats until reach-
ing the minimum number of sentences for the do-
main (determined from the training corpus statis-
tic) and then continues until all of the remaining
input data is consumed (and not to exceed the pre-
determined maximum number of sentences, also
determined from the training corpus statistic).
</bodyText>
<sectionHeader confidence="0.982689" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999993538461538">
In this section, we first discuss the corpus data
used to train and generate texts. Then, the re-
sults of both automatic and human evaluations of
our system’s generations against the original and
baseline texts are considered as a means of de-
termining performance. For all experiments re-
ported in this section, the baseline system selects
the most frequent conceptual unit at the given po-
sition, chooses the most likely template for the
conceptual unit, and fills the template with input
data. The above process is repeated until the num-
ber of sentences is less than or equal to the average
number of sentences for the given domain.
</bodyText>
<subsectionHeader confidence="0.947763">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9995478125">
We ran our system on two different domains: cor-
porate officer and director biographies and off-
shore oil rig weather reports from the SUMTIME-
METEO corpus ((Reiter et al., 2005)). The biogra-
phy domain includes 1150 texts ranging from 3-17
sentences and the weather domain includes 1045
weather reports ranging from 1-6 sentences.2 We
used a training-test(generation) split of 70/30.
(4) provides generation comparisons for the
system ( DocSys), baseline ( DocBase) and orig-
inal ( DocOrig) randomly selected text snippets
from each domain. The variability of the gener-
ated texts ranges from a close similarity to slightly
shorter - not an uncommon (Belz and Reiter,
2006), but not necessarily detrimental, observation
for NLG systems (van Deemter et al., 2005).
</bodyText>
<figure confidence="0.6572568125">
(4) Weather DocOrig
a. Another weak cold front will move ne to Cornwall by later
Friday.
Weather DocSys
b. Another weak cold front will move ne to Cornwall during
Friday.
Weather DocBase
c. Another weak cold front from ne through the Cornwall will
remain slow moving.
Bio DocOrig
d. He previously served as Director of Sales Planning and
Manager of Loan Center.
Bio DocSys
e. He previously served as Director of Sales in Loan Center
of the Company.
Bio DocBase
</figure>
<footnote confidence="0.9159955">
2The SUMTIME-METEO project is a common bench
mark in NLG. However, we provide no comparison between
our system and SUMTIME-METEO as our system utilized the
generated forecasts from SUMTIME-METEO’s system as the
historical data. We cannot compare with other statistical gen-
eration systems like (Belz, 2007) as they only focussed on the
part of the forecasts the predicts wind characteristics whereas
our system generates the complete forecasts.
</footnote>
<page confidence="0.98635">
1410
</page>
<bodyText confidence="0.961022769230769">
f. He previously served as Director of Sales of the Company.
The DocSys and DocBase generations are
largely grammatical and coherent on the surface
with some variance, but there are graded semantic
variations (e.g., Director of Sales Planning vs. Di-
rector of Sales (4g-h) and move ne to Cornwall vs.
from ne through the Cornwall). Both automatic
and human evaluations are required in NLG to de-
termine the impact of these variances on the under-
standability of the texts in general (non-experts)
and as they are representative of particular subject
matter domains (experts). The following sections
discuss the evaluation results.
</bodyText>
<subsectionHeader confidence="0.962059">
4.2 Automatic Metrics
</subsectionHeader>
<bodyText confidence="0.999988515151515">
We used BLEU–4 (Papineni et al., 2002), METEOR
(v.1.3) (Denkowski and Lavie, 2011) to evaluate
the texts at document level. Both BLEU–4 and
METEOR originate from machine translation re-
search. BLEU–4 measures the degree of 4-gram
overlap between documents. METEOR uses a un-
igram weighted f–score less a penalty based on
chunking dissimilarity. These metrics only eval-
uate the text on a document level but fail to iden-
tify “syntactic repetitiveness” across documents in
a document collection. This is an important char-
acteristic of a document collection to avoid banal-
ity. To address this issue, we propose a new auto-
matic metric called syntactic variability. In order
to compute this metric, each document should be
represented as a sequence of templates by associ-
ating each sentence in the document with a tem-
plate in the template bank. Syntactic variability is
defined as the percentage of unique template se-
quences across all generated documents. It ranges
between 0 and 1. A higher value indicates that
more documents in the collection are linguistically
different from each other and a value closer to zero
shows that most of documents have the similar
language despite different input data.3
As indicated in Figure 2, the BLEU-4 scores are
low for all DocSys and DocBase generations (as
compared to DocOrig) for each domain. How-
ever, the METEOR scores, while low overall (rang-
ing from .201-.437) are noticeably increased over
BLEU-4 (which ranges from .199-.320).
Given the nature of each metric, the results in-
dicate that the generated and baseline texts have
</bodyText>
<footnote confidence="0.88679325">
3Of course, syntactic and semantic repetitiveness could be
captured by syntactic variability, but only if this is the nature
of the underlying historical data - financial texts tend to be
fairly repetitive.
</footnote>
<figureCaption confidence="0.999872">
Figure 2: Automatic Evaluations.
</figureCaption>
<bodyText confidence="0.999959571428571">
very different surface realizations compared to the
originals (low BLEU-4), but are still capturing the
content of the originals (higher METEOR). Both
BLEU–4 and METEOR measure the similarity of
the generated text to the original text, but fail to
penalize repetitiveness across texts, which is ad-
dressed by the syntactic variability metric. There
is no statistically significant difference between
DocSys and DocBase generations for METEOR
and BLEU–4.4 However, there is a statistically
significant difference in the syntactic variability
metric for both domains (weather - x2=137.16,
d.f.=1, p&lt;.0001; biography - x2=96.641, d.f.=1,
p&lt;.0001) - the variability of the DocSys gener-
ations is greater than the DocBase generations,
which shows that texts generated by our system
are more variable than the baseline texts.
The use of automatic metrics is a common eval-
uation method in NLG, but they must be recon-
ciled against non–expert and expert level evalua-
tions.
</bodyText>
<subsectionHeader confidence="0.994893">
4.3 Non-Expert Human Evaluations
</subsectionHeader>
<bodyText confidence="0.99990925">
Two sets of crowdsourced human evaluation tasks
(run on CrowdFlower) were constructed to com-
pare against the automatic metrics: (1) an under-
standability evaluation of the entire text on a three-
point scale: Fluent = no grammatical or infor-
mative barriers; Understandable = some gram-
matical or informative barriers; Disfluent = sig-
nificant grammatical or informative barriers; and
(2) a sentence–level preference between sentence
pairs (e.g., “Do you prefer Sentence A (from Do-
cOrig) or the corresponding Sentence B (from
DocBase/DocSys)”).
</bodyText>
<footnote confidence="0.779832333333333">
4BLEU–4: weather- x2=1.418, d.f.=1, p=.230; biography
- x2=0.311, d.f.=1, p=.354. METEOR: weather - x2=1.016,
d.f.=1, p=.313; biography - x2=0.851, d.f.=1, p=.354.
</footnote>
<page confidence="0.992434">
1411
</page>
<bodyText confidence="0.999732142857143">
Over 100 native English speakers contributed,
each one restricted to providing no more than
50 responses and only after they successfully an-
swered 4 “gold data” questions correctly. We also
omitted those evaluators with a disproportionately
high response rate. No other data was collected on
the contributors (although geographic data (coun-
try, region, city) and IP addresses were available).
For the sentence–level preference task, the pair or-
derings were randomized to prevent click bias.
For the text–understandability task, 40 docu-
ments were chosen at random from the DocOrig
test set along with the corresponding 40 Doc-
Sys and DocBase generations (240 documents to-
tal/120 for each domain). 8 judgments per doc-
ument were solicited from the crowd (1920 to-
tal judgments, 69.51 average agreement) and are
summarized in Figures 3 and 4 (biography and
weather respectively).
If the system is performing well and the rank-
ing model is actually contributing to increased
performance, the accepted trend should be that
the DocOrig texts are more fluent and preferred
compared to both the DocSys and DocBase sys-
tems. However, the differences between DocOrig
and DocSys will not be significant, the differences
between DocOrig and DocBase and DocSys and
DocBase will be significantly different.
</bodyText>
<figureCaption confidence="0.995568">
Figure 3: Biography Text Evaluations.
</figureCaption>
<bodyText confidence="0.998452151515151">
Focusing on fluency ratings, it is expected that
the DocOrig generations will have the highest flu-
ency (as they are human generated). Further, if the
DocSys is performing well, it is expected that the
fluency rating will be less than the DocOrig and
higher than DocBase. Figure 3, which shows the
biography text evaluations, demonstrates this ac-
ceptable distribution of performances.
For the weather discourses, as evident from
Figure 4, the acceptable trend holds between the
DocSys and DocBase generations, and the Doc-
Sys generation fluency is actually slightly higher
than DocOrig. This is possibly because the Do-
cOrig texts are from a particular subject matter -
weather forecasts for offshore oil rigs in the U.K.
- which may be difficult for people in general to
understand. Nonetheless, the demonstrated trend
is favorable to our system.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - x2=.347,
d.f.=1, p=.555; DocOrig vs. DocBase - x2=.090,
d.f.=1, p=.764; DocSys vs. DocBase - x2=.790,
d.f.=1, p=.373). While this is a good result for
comparing DocOrig and DocSys generations, it is
not for the other pairs. However, numerically, the
trend is in the right direction despite not being
able to demonstrate significance. For biography,
the trend fits nicely both numerically and in terms
of statistical significance (DocOrig vs. DocSys -
x2=5.094, d.f.=1, p=.024; DocOrig vs. DocBase -
x2=35.171, d.f.=1, p&lt;.0001; DocSys vs. DocBase
- x2=14.000, d.f.=1, p&lt;.0001).
</bodyText>
<figureCaption confidence="0.993726">
Figure 4: Weather Text Evaluations.
</figureCaption>
<bodyText confidence="0.9999412">
For the sentence preference task, equivalent
sentences across the 120 documents were chosen
at random (80 sentences from biography and 74
sentences from weather). 8 judgments per com-
parison were solicited from the crowd (3758 to-
tal judgments, 75.87 average agreement) and are
summarized in Figures 5 and 6 (biography and
weather, respectively).
Similar to the text–understandability task, an
acceptable performance pattern should include the
DocOrig texts being preferred to both DocSys and
DocBase generations and the DocSys generation
preferred to the DocBase. The closer the Doc-
Sys generation is to the DocOrig, the better Doc-
Sys is performing. The biography domain illus-
</bodyText>
<page confidence="0.996604">
1412
</page>
<figureCaption confidence="0.999589">
Figure 5: Biography Sentence Evaluations. Figure 6: Weather Sentence Evaluations.
</figureCaption>
<bodyText confidence="0.999709189189189">
trates this scenario (Figure 5) where the results are
similar to the text-understandability experiments.
In contrast, for weather domain, sentences from
DocBase system were preferred to our system’s
(Figure 6). We looked at the cases where the
preferences were in favor of DocBase. It appears
that because of high syntactic variability, our sys-
tem can produce quite complex sentences where as
the non-experts seem to prefer shorter and simpler
sentences because of the complexity of the text.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - x2=6.48,
d.f.=1, p=.011; DocOrig vs. DocBase - x2=.720,
d.f.=1, p=.396; DocSys vs. DocBase - x2=.720,
d.f.=1, p=.396). The trend is different compared to
the fluency metric above in that the DocBase sys-
tem is outperforming the DocOrig generations to
an almost statistically significant difference - the
remaining comparisons follow the trend. We be-
lieve that this is for similar reasons stated above
- i.e., the generation may be a more digestible
version of a technical document. More problem-
atic is the results of the biography evaluations.
Here there is a statistically significant difference
between the DocSys and DocOrig and no sta-
tistically significant difference between the Doc-
Sys and DocBase generations (DocOrig vs. Doc-
Sys - x2=76.880, d.f.=1, p&lt;.0001; DocOrig vs.
DocBase - x2=38.720, d.f.=1, p&lt;.0001; DocSys
vs. DocBase - x2=.720, d.f.=1, p=.396). Again,
this distribution of preferences is numerically sim-
ilar to the trend we would like to see, but the sta-
tistical significance indicates that there is some
ground to make up. Expert evaluations are po-
tentially informative for identifying specific short-
comings and how best to address them.
</bodyText>
<subsectionHeader confidence="0.887391">
4.4 Expert Human Evaluations
</subsectionHeader>
<bodyText confidence="0.999912971428571">
We performed expert evaluations for the biogra-
phy domain only as we do not have access to
weather experts. The four biography reviewers are
journalists who write short biographies for news
archives.
For the biography domain, evaluations of the
texts were largely similar to the evaluations of
the non-expert crowd (76.22 average agreement
for the sentence–preference task and 72.95 for the
text–understandability task). For example, the dis-
fluent ratings were highest for the DocBase gen-
erations and lowest for the DocOrig generations.
Also, the fluent ratings were highest for the Do-
cOrig generations, and while the combined flu-
ent and understandable are higher for DocSys as
compared to DocBase, the DocBase generations
had a 10% higher fluent score (58.22%) as com-
pared to the DocSys fluent score (47.97%). Based
on notes from the reviewers, the succinctness of
the the DocBase generations are preferred in some
ways as they are in keeping with certain editorial
standards. This is further reflected in the sentence
preferences being 70% in favor of the DocBase
generations as compared to the DocSys genera-
tions (all other sentence comparisons were consis-
tent with the non-expert crowd).
These expert evaluations provide much needed
clarity to the NLG process. Overall, our system
is generating clearly acceptable texts. Further,
there are enough parameters inherent in the system
to tune to different domain expectations. This is
an encouraging result considering that no experts
were involved in the development of the system -
a key contrast to many other existing (especially
rule-based) NLG systems.
</bodyText>
<page confidence="0.984091">
1413
</page>
<sectionHeader confidence="0.996498" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999978229166667">
We have presented a hybrid (template-based and
statistical), single–staged NLG system that gen-
erates natural sounding texts and is domain–
adaptable. Our experiments with both ex-
perts and non–experts demonstrate that the
system-generated texts are comparable to human–
authored texts. The development time to adapt
our system to new domains is small compared to
other NLG systems; around a week to adapt the
system to weather and biography domains. Most
of the development time was spent on creating the
domain-specific entity taggers for the weather do-
main. The development time would be reduced to
hours if the historical data for a domain is readily
available with the corresponding input data.
The main limitation of our system is that it re-
quires significant historical data. Our system does
consolidate many traditional components (macro-
and micro-planning, lexical choice and aggrega-
tion),5 but the system cannot be applied to the do-
mains with no historical data. The quality and the
linguistic variability of the generated text is di-
rectly proportional to the amount of historical data
available.
We also presented a new automatic metric to
evaluate generated texts at document collection
level to identify boilerplate texts. This metric
computes “syntactic repetitiveness” by counting
the number of unique template sequences across
the given document collection.
Future work will focus on extending our frame-
work by adding additional features to the model
that could improve the quality of the generated
text. For example, most NLG pipelines have a
separate component responsible for referring ex-
pression generation (Krahmer and van Deemter,
2012). While we address the associated concern
of data consumption in Section 3.5, we currently
do not have any features that would handle refer-
ring expression generation. We believe that this
is possible by identifying referring expressions in
templates and adding features to the model to give
higher scores to the templates having relevant re-
ferring expressions. We also would like to inves-
tigate using all the top-scored templates instead
of the highest-scoring template. This would help
achieve better syntactic-variability scores by pro-
ducing more natural-sounding texts.
</bodyText>
<footnote confidence="0.956812">
5Lexical choice and aggregation are “handled” insofar as
their existence in the historical data.
</footnote>
<sectionHeader confidence="0.992922" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999847125">
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragment, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
</bodyText>
<sectionHeader confidence="0.998496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999452071428572">
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502–512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331–338.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284–304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL’06), pages 313–320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT’07), pages
164–171.
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2011. Content selection from an ontology-
based knowledge base for the generation of foot-
ball summaries. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), pages 72–81.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85–91.
</reference>
<page confidence="0.872486">
1414
</page>
<reference confidence="0.999692788888889">
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of the
2003 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2003), pages 2003–
2007.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341–385.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in
statistical nlg. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143–154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Colin Kelly, Ann Copestake, and Nikiforos Karama-
nis. 2009. Investigating content selection for lan-
guage generation using machine learning. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG), pages 130–137.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369–
378.
Emiel Krahmer and Kees van Deemter. 2012. Com-
putational generation of referring expression: A sur-
vey. Computational Linguistics, 38(1):173–218.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’98),
pages 704–710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707–710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611–1622.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proceedings of the 2009 Conference
on Empirical Methods for Natural Language Pro-
cessing (EMNLP 2009), pages 400–409.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Johanna D. Moore and Cecile L. Paris. 1993. Planning
text for advisory dialogues: Capturing intentional
and rhetorical information. Computational Linguis-
tics, 19(4):651–694.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL’02), pages 311–318.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137–
169.
Jacques Robin and Kathy McKeown. 1996. Exmpiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and
Jin Yu. 2001. A two-stage model for content
determination. In Proceedings of the 8th Euro-
pean Workshop on Natural Language Generation
(ENLG), pages 1–8.
Kees van Deemter, Mari¨et Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15–24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
</reference>
<page confidence="0.992953">
1415
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788502">
<title confidence="0.999795">A Statistical NLG Framework for Aggregated Planning and Realization</title>
<author confidence="0.998168">Blake Howald</author>
<author confidence="0.998168">Frank</author>
<affiliation confidence="0.975367">Thomson Reuters, Research &amp;</affiliation>
<address confidence="0.987346">610 Opperman Drive, Eagan, MN</address>
<email confidence="0.999724">firstname.lastname@thomsonreuters.com</email>
<abstract confidence="0.999661837837838">We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a dospecific concept) by clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce novel automatic metric – varithat represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time.</abstract>
<note confidence="0.868294">Kondadadi is now affiliated with Nuance Communications, Inc.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods for Natural Language Processing (EMNLP</booktitle>
<pages>502--512</pages>
<contexts>
<context position="7626" citStr="Angeli et al., 2012" startWordPosition="1196" endWordPosition="1200"> given system, with input from multiple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) extended Barzilay and Lapata’s approach for generating match reports for cricket. The present work builds on Howald et al. (2013) where, in a given corpus, a combination of domain specific named entity tagging and clustering sentences (based on semantic predicates) were used to generate templates. However, while the system consolidated both sentence planning and surface realization with this approach (described in more detail in Section 3), the d</context>
</contexts>
<marker>Angeli, Liang, Klein, 2012</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2012. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods for Natural Language Processing (EMNLP 2010), pages 502–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Conference on Empirical Methods for Natural Language Processing (EMNLP</booktitle>
<pages>331--338</pages>
<contexts>
<context position="7667" citStr="Barzilay and Lapata (2005)" startWordPosition="1203" endWordPosition="1207">iple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) extended Barzilay and Lapata’s approach for generating match reports for cricket. The present work builds on Howald et al. (2013) where, in a given corpus, a combination of domain specific named entity tagging and clustering sentences (based on semantic predicates) were used to generate templates. However, while the system consolidated both sentence planning and surface realization with this approach (described in more detail in Section 3), the document plan was given via the input data</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of the 2005 Conference on Empirical Methods for Natural Language Processing (EMNLP 2005), pages 331–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
<author>Michael Zock</author>
</authors>
<title>Natural language generation.</title>
<date>2003</date>
<booktitle>Oxford Handbook of Computational Linguistics, Research in Computational Semantics,</booktitle>
<pages>284--304</pages>
<editor>In R. Mitkov, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="2331" citStr="Bateman and Zock (2003)" startWordPosition="344" endWordPosition="347">∗*Ravi Kondadadi is now affiliated with Nuance Communications, Inc. 1 Introduction NLG is the process of generating natural-sounding text from non-linguistic inputs. A typical NLG system contains three main components: (1) Document (Macro) Planning - deciding what content should be realized in the output and how it should be structured; (2) Sentence (Micro) planning - generating a detailed sentence specification and selecting appropriate referring expressions; and (3) Surface Realization - generating the final text after applying morphological modifications based on syntactic rules (see e.g., Bateman and Zock (2003), Reiter and Dale (2000) and McKeown (1985)). However, document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure. If the document planning stage fails, the communicative goal of the generated text will not be met even if the other two stages are perfect. While most traditional systems simplify development by using a pipelined approach where (1-3) are executed in a sequence, this can result in errors at one stage propagating to successive stages (see e.g., Robin and McK</context>
</contexts>
<marker>Bateman, Zock, 2003</marker>
<rawString>John Bateman and Michael Zock. 2003. Natural language generation. In R. Mitkov, editor, Oxford Handbook of Computational Linguistics, Research in Computational Semantics, pages 284–304. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Association for Computational Linguistics (EACL’06),</booktitle>
<pages>313--320</pages>
<contexts>
<context position="21622" citStr="Belz and Reiter, 2006" startWordPosition="3509" endWordPosition="3512">rporate officer and director biographies and offshore oil rig weather reports from the SUMTIMEMETEO corpus ((Reiter et al., 2005)). The biography domain includes 1150 texts ranging from 3-17 sentences and the weather domain includes 1045 weather reports ranging from 1-6 sentences.2 We used a training-test(generation) split of 70/30. (4) provides generation comparisons for the system ( DocSys), baseline ( DocBase) and original ( DocOrig) randomly selected text snippets from each domain. The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al., 2005). (4) Weather DocOrig a. Another weak cold front will move ne to Cornwall by later Friday. Weather DocSys b. Another weak cold front will move ne to Cornwall during Friday. Weather DocBase c. Another weak cold front from ne through the Cornwall will remain slow moving. Bio DocOrig d. He previously served as Director of Sales Planning and Manager of Loan Center. Bio DocSys e. He previously served as Director of Sales in Loan Center of the Company. Bio DocBase 2The SUMTIME-METEO project is a common bench mar</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of the European Association for Computational Linguistics (EACL’06), pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Probabilistic generation of weather forecast texts.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT’07),</booktitle>
<pages>164--171</pages>
<contexts>
<context position="7178" citStr="Belz, 2007" startWordPosition="1131" endWordPosition="1132">ipada et al., 2001). It follows that approaches to document planning are rule-based as well and, concomitantly, are usually domain specific. For example, Bouayad-Agha, et al. (2011) proposed document planning based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) ex</context>
<context position="22479" citStr="Belz, 2007" startWordPosition="3650" endWordPosition="3651"> during Friday. Weather DocBase c. Another weak cold front from ne through the Cornwall will remain slow moving. Bio DocOrig d. He previously served as Director of Sales Planning and Manager of Loan Center. Bio DocSys e. He previously served as Director of Sales in Loan Center of the Company. Bio DocBase 2The SUMTIME-METEO project is a common bench mark in NLG. However, we provide no comparison between our system and SUMTIME-METEO as our system utilized the generated forecasts from SUMTIME-METEO’s system as the historical data. We cannot compare with other statistical generation systems like (Belz, 2007) as they only focussed on the part of the forecasts the predicts wind characteristics whereas our system generates the complete forecasts. 1410 f. He previously served as Director of Sales of the Company. The DocSys and DocBase generations are largely grammatical and coherent on the surface with some variance, but there are graded semantic variations (e.g., Director of Sales Planning vs. Director of Sales (4g-h) and move ne to Cornwall vs. from ne through the Cornwall). Both automatic and human evaluations are required in NLG to determine the impact of these variances on the understandability </context>
</contexts>
<marker>Belz, 2007</marker>
<rawString>Anja Belz. 2007. Probabilistic generation of weather forecast texts. In Proceedings of Human Language Technologies 2007: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT’07), pages 164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer. In</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>277--286</pages>
<editor>J. Bos and R. Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="9896" citStr="Bos, 2008" startWordPosition="1552" endWordPosition="1553"> corpus to extract templates. We assume that each document in the corpus is classified to a specific domain. Preprocessing involves uncovering the underlying semantic structure of the corpus and using this as a foundation for template creation (Lu et al., 2009; Lu and Ng, 2011; Konstas and Lapata, 2012). We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence. The DRS consists of semantic predicates and named entity tags. We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. We developed the named-entity tagger for the weather domain ourselves. To tag entities in the biography domain, we used OpenCalais (www.opencalais.com). For example, in the biography in (1), the conceptual meaning (semantic predicates and domain-specific entities) of sentences (a-b) are represented in (c-d). The corresponding templates are showing in (e-f). (1) Sentence a. Mr. Mitsutaka Kambe has bee</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In J. Bos and R. Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadjet Bouayad-Agha</author>
<author>Gerard Casamayor</author>
<author>Leo Wanner</author>
</authors>
<title>Content selection from an ontologybased knowledge base for the generation of football summaries.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>72--81</pages>
<contexts>
<context position="6748" citStr="Bouayad-Agha, et al. (2011)" startWordPosition="1065" endWordPosition="1068">ection 3. The system is evaluated and discussed in Section 4. Finally, we conclude in Section 5 and point out future directions of research. 2 Background Typically, knowledge-based NLG systems are implemented by rules and, as mentioned above, have a pipelined architecture for the document and sentence planning stages and surface realization (Hovy, 1993; Moore and Paris, 1993). However, document planning is arguably the most important task (Sripada et al., 2001). It follows that approaches to document planning are rule-based as well and, concomitantly, are usually domain specific. For example, Bouayad-Agha, et al. (2011) proposed document planning based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knig</context>
</contexts>
<marker>Bouayad-Agha, Casamayor, Wanner, 2011</marker>
<rawString>Nadjet Bouayad-Agha, Gerard Casamayor, and Leo Wanner. 2011. Content selection from an ontologybased knowledge base for the generation of football summaries. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 72–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation,</booktitle>
<pages>85--91</pages>
<contexts>
<context position="23355" citStr="Denkowski and Lavie, 2011" startWordPosition="3786" endWordPosition="3789">y grammatical and coherent on the surface with some variance, but there are graded semantic variations (e.g., Director of Sales Planning vs. Director of Sales (4g-h) and move ne to Cornwall vs. from ne through the Cornwall). Both automatic and human evaluations are required in NLG to determine the impact of these variances on the understandability of the texts in general (non-experts) and as they are representative of particular subject matter domains (experts). The following sections discuss the evaluation results. 4.2 Automatic Metrics We used BLEU–4 (Papineni et al., 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. Both BLEU–4 and METEOR originate from machine translation research. BLEU–4 measures the degree of 4-gram overlap between documents. METEOR uses a unigram weighted f–score less a penalty based on chunking dissimilarity. These metrics only evaluate the text on a document level but fail to identify “syntactic repetitiveness” across documents in a document collection. This is an important characteristic of a document collection to avoid banality. To address this issue, we propose a new automatic metric called syntactic variability. In order to compute this</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation, pages 85–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Statistical acquisition of content selection rules for natural language generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods for Natural Language Processing (EMNLP</booktitle>
<pages>pages</pages>
<contexts>
<context position="7397" citStr="Duboue and McKeown (2003)" startWordPosition="1164" endWordPosition="1167">ing based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) extended Barzilay and Lapata’s approach for generating match reports for cricket. The present work builds on Howald et al. (2013) where, in a given corpus, a combination of domain specific named entity tagging and cluster</context>
<context position="12384" citStr="Duboue and McKeown (2003)" startWordPosition="1952" endWordPosition="1955"> corresponding CuId. For example, in (2), using the templates in (1e-f), the identified named entities are assigned to a clustered CuId (2a-b). (2) Conceptual Units a. {CuId : 000} – [person] has been serving as [title] of the [company] since [date]. b. {CuId : 001} – [person] holds a [degree] in [subject] from [institution] and a [degree] from [institution]. At this stage, we will have a set of conceptual units with corresponding template collections (see Howald et al. (2013) for a further explanation of Sections 3.1-3.2). 1A similar approach to the clustering of semantic content is found in Duboue and McKeown (2003), where text with stopwords removed were used as semantic input. Boxer provides a similar representation with the addition of domain general tags. However, to contrast our work from Duboue and McKeown, which focused on content selection, we are focused on learning templates from the semantic representations for the complete generation system (covering content selection, aggregation, sentence and document planning). 1408 3.3 Collecting Corpus Statistics After identifying the different conceptual units and the template bank, we collect a number of statistics from the corpus: • Frequency distribu</context>
</contexts>
<marker>Duboue, McKeown, 2003</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Proceedings of the 2003 Conference on Empirical Methods for Natural Language Processing (EMNLP 2003), pages 2003– 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--341</pages>
<contexts>
<context position="6475" citStr="Hovy, 1993" startWordPosition="1024" endWordPosition="1025">logical variability of the generated texts. We believe this is the first work to propose an automatic metric to measure linguistic variability of generated texts in NLG. Section 2 provides an overview of related work on NLG. We present our main system in Section 3. The system is evaluated and discussed in Section 4. Finally, we conclude in Section 5 and point out future directions of research. 2 Background Typically, knowledge-based NLG systems are implemented by rules and, as mentioned above, have a pipelined architecture for the document and sentence planning stages and surface realization (Hovy, 1993; Moore and Paris, 1993). However, document planning is arguably the most important task (Sripada et al., 2001). It follows that approaches to document planning are rule-based as well and, concomitantly, are usually domain specific. For example, Bouayad-Agha, et al. (2011) proposed document planning based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there </context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Eduard H. Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence, 63:341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blake Howald</author>
<author>Ravi Kondadadi</author>
<author>Frank Schilder</author>
</authors>
<title>Domain adaptable semantic clustering in statistical nlg.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013),</booktitle>
<pages>143--154</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="7905" citStr="Howald et al. (2013)" startWordPosition="1241" endWordPosition="1244">for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) extended Barzilay and Lapata’s approach for generating match reports for cricket. The present work builds on Howald et al. (2013) where, in a given corpus, a combination of domain specific named entity tagging and clustering sentences (based on semantic predicates) were used to generate templates. However, while the system consolidated both sentence planning and surface realization with this approach (described in more detail in Section 3), the document plan was given via the input data and sequencing information was present in training documents. For the present research, we introduce a similar method that leverages the distributions of document–level features in the training corpus to incorporate a statistical documen</context>
<context position="12240" citStr="Howald et al. (2013)" startWordPosition="1929" endWordPosition="1932">ed a CuId (Conceptual Unit Identifier) to each cluster, which represents a “conceptual unit”. We associate each template in the corpus to a corresponding CuId. For example, in (2), using the templates in (1e-f), the identified named entities are assigned to a clustered CuId (2a-b). (2) Conceptual Units a. {CuId : 000} – [person] has been serving as [title] of the [company] since [date]. b. {CuId : 001} – [person] holds a [degree] in [subject] from [institution] and a [degree] from [institution]. At this stage, we will have a set of conceptual units with corresponding template collections (see Howald et al. (2013) for a further explanation of Sections 3.1-3.2). 1A similar approach to the clustering of semantic content is found in Duboue and McKeown (2003), where text with stopwords removed were used as semantic input. Boxer provides a similar representation with the addition of domain general tags. However, to contrast our work from Duboue and McKeown, which focused on content selection, we are focused on learning templates from the semantic representations for the complete generation system (covering content selection, aggregation, sentence and document planning). 1408 3.3 Collecting Corpus Statistics</context>
</contexts>
<marker>Howald, Kondadadi, Schilder, 2013</marker>
<rawString>Blake Howald, Ravi Kondadadi, and Frank Schilder. 2013. Domain adaptable semantic clustering in statistical nlg. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), pages 143–154. Association for Computational Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Learning to Classify Text Using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="17379" citStr="Joachims, 2002" startWordPosition="2787" endWordPosition="2788">er of named entities. • Most likely CuId given position and previous CuId: Binary feature indicating if the current CuId is most likely given the position and the previous CuId. • Similarity between the most likely template in CuId and current template: Edit distance between the current template and the most likely template for the current CuId. • Similarity between the most likely template in CuId given position and current template: Edit distance between the current template and the most likely template for the current CuId at the current position. We used a linear kernel for a ranking SVM (Joachims, 2002) (cost set to total queries) to learn the weights associated with each feature for the different domains. 3.5 Generation At generation time, our system has a set of input data, a semantically organized template bank (collection of templates organized by CuId) and a model from training on the documents for a given domain. We first filter out those templates that contain a named entity tag not present in the input data. Then, we compute a score for each of the remaining templates from the feature values and the feature weights from the model. The template with the highest overall score is select</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Learning to Classify Text Using Support Vector Machines. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="9771" citStr="Kamp and Reyle, 1993" startWordPosition="1528" endWordPosition="1531"> section, we describe each component in detail. Figure 1: System Architecture. 3.1 Preprocessing The first component processes the given corpus to extract templates. We assume that each document in the corpus is classified to a specific domain. Preprocessing involves uncovering the underlying semantic structure of the corpus and using this as a foundation for template creation (Lu et al., 2009; Lu and Ng, 2011; Konstas and Lapata, 2012). We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence. The DRS consists of semantic predicates and named entity tags. We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. We developed the named-entity tagger for the weather domain ourselves. To tag entities in the biography domain, we used OpenCalais (www.opencalais.com). For example, in the biography in (1), the conceptual meaning (semantic predicates and domain-specific entities) of sentences </context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Kelly</author>
<author>Ann Copestake</author>
<author>Nikiforos Karamanis</author>
</authors>
<title>Investigating content selection for language generation using machine learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>130--137</pages>
<contexts>
<context position="7775" citStr="Kelly et al. (2009)" startWordPosition="1221" endWordPosition="1224">EO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) extended Barzilay and Lapata’s approach for generating match reports for cricket. The present work builds on Howald et al. (2013) where, in a given corpus, a combination of domain specific named entity tagging and clustering sentences (based on semantic predicates) were used to generate templates. However, while the system consolidated both sentence planning and surface realization with this approach (described in more detail in Section 3), the document plan was given via the input data and sequencing information was present in training documents. For the present research, we introduce a simi</context>
</contexts>
<marker>Kelly, Copestake, Karamanis, 2009</marker>
<rawString>Colin Kelly, Ann Copestake, and Nikiforos Karamanis. 2009. Investigating content selection for language generation using machine learning. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG), pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Conceptto-text generation via discriminative reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>369--378</pages>
<contexts>
<context position="9590" citStr="Konstas and Lapata, 2012" startWordPosition="1501" endWordPosition="1504">s) for the domain through four components: (A) preprocessing; (B) “conceptual unit” creation; (C) collecting statistics; and (D) ranking model building (summarized in Figure 1). In this section, we describe each component in detail. Figure 1: System Architecture. 3.1 Preprocessing The first component processes the given corpus to extract templates. We assume that each document in the corpus is classified to a specific domain. Preprocessing involves uncovering the underlying semantic structure of the corpus and using this as a foundation for template creation (Lu et al., 2009; Lu and Ng, 2011; Konstas and Lapata, 2012). We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence. The DRS consists of semantic predicates and named entity tags. We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. We developed the named-entity tagger for the weather domain ourselves. To tag entities in the bio</context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Ioannis Konstas and Mirella Lapata. 2012. Conceptto-text generation via discriminative reranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 369– 378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expression: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expression: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL’98),</booktitle>
<pages>704--710</pages>
<contexts>
<context position="7357" citStr="Langkilde and Knight, 1998" startWordPosition="1158" endWordPosition="1161">gha, et al. (2011) proposed document planning based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Statistical approaches can reduce extensive development time by relying on corpus data to “learn” rules for one or more components of an NLG system (Langkilde and Knight, 1998). For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. Further, statistical approaches should be more adaptable to different domains than their rule-based equivalents (Angeli et al., 2012). For example, Barzilay and Lapata (2005) formulated content selection as a classification task to produce football summaries and Kelly et al. (2009) extended Barzilay and Lapata’s approach for generating match reports for cricket. The present work builds on Howald et al. (2013) where, in a given corpus, a combination of domain s</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL’98), pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="14617" citStr="Levenshtein, 1966" startWordPosition="2320" endWordPosition="2321">al model that ranks a set of templates for a given position (sentence 1, sentence 2, ..., sentence n) based on the input data. The input data in our tasks was extracted from a training document; this serves as a temporary surrogate to a database. The task is to learn the ranks of all the templates from all CuIds at each position. To generate the training data, we first filter the templates that have named entity tags not specified in the input data. This will make sure the generated text does not have any unfilled entity tags. We then rank templates according to the Levenshtein edit distance (Levenshtein, 1966) from the template corresponding to the current sentence in the training document (using the top 10 ranked templates in training for ease of processing effort). We experimented with other ranking schemes such as entity-based similarity (similarity between entity sequences in the templates) and a combination of edit-distance based and entity-based similarities. We obtained better results with edit distance. For each template, we generate the following features to build the ranking model. Most of the features are based on the corpus statistics mentioned above. • CuId given position: This is a bi</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A probabilistic forest-to-string model for language generation from typed lambda calculus expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods for Natural Language Processing (EMNLP</booktitle>
<pages>1611--1622</pages>
<contexts>
<context position="9563" citStr="Lu and Ng, 2011" startWordPosition="1497" endWordPosition="1500">rical data (corpus) for the domain through four components: (A) preprocessing; (B) “conceptual unit” creation; (C) collecting statistics; and (D) ranking model building (summarized in Figure 1). In this section, we describe each component in detail. Figure 1: System Architecture. 3.1 Preprocessing The first component processes the given corpus to extract templates. We assume that each document in the corpus is classified to a specific domain. Preprocessing involves uncovering the underlying semantic structure of the corpus and using this as a foundation for template creation (Lu et al., 2009; Lu and Ng, 2011; Konstas and Lapata, 2012). We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence. The DRS consists of semantic predicates and named entity tags. We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. We developed the named-entity tagger for the weather domain ourselves.</context>
</contexts>
<marker>Lu, Ng, 2011</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the 2011 Conference on Empirical Methods for Natural Language Processing (EMNLP 2011), pages 1611–1622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
</authors>
<title>Natural language generation with tree conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods for Natural Language Processing (EMNLP</booktitle>
<pages>400--409</pages>
<contexts>
<context position="9546" citStr="Lu et al., 2009" startWordPosition="1493" endWordPosition="1496">ystem takes historical data (corpus) for the domain through four components: (A) preprocessing; (B) “conceptual unit” creation; (C) collecting statistics; and (D) ranking model building (summarized in Figure 1). In this section, we describe each component in detail. Figure 1: System Architecture. 3.1 Preprocessing The first component processes the given corpus to extract templates. We assume that each document in the corpus is classified to a specific domain. Preprocessing involves uncovering the underlying semantic structure of the corpus and using this as a foundation for template creation (Lu et al., 2009; Lu and Ng, 2011; Konstas and Lapata, 2012). We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence. The DRS consists of semantic predicates and named entity tags. We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. We developed the named-entity tagger for the weather </context>
</contexts>
<marker>Lu, Ng, Lee, 2009</marker>
<rawString>Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Natural language generation with tree conditional random fields. In Proceedings of the 2009 Conference on Empirical Methods for Natural Language Processing (EMNLP 2009), pages 400–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2374" citStr="McKeown (1985)" startWordPosition="353" endWordPosition="354">nications, Inc. 1 Introduction NLG is the process of generating natural-sounding text from non-linguistic inputs. A typical NLG system contains three main components: (1) Document (Macro) Planning - deciding what content should be realized in the output and how it should be structured; (2) Sentence (Micro) planning - generating a detailed sentence specification and selecting appropriate referring expressions; and (3) Surface Realization - generating the final text after applying morphological modifications based on syntactic rules (see e.g., Bateman and Zock (2003), Reiter and Dale (2000) and McKeown (1985)). However, document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure. If the document planning stage fails, the communicative goal of the generated text will not be met even if the other two stages are perfect. While most traditional systems simplify development by using a pipelined approach where (1-3) are executed in a sequence, this can result in errors at one stage propagating to successive stages (see e.g., Robin and McKeown (1996)). We propose a hybrid framework</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R. McKeown. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>Cecile L Paris</author>
</authors>
<title>Planning text for advisory dialogues: Capturing intentional and rhetorical information.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="6499" citStr="Moore and Paris, 1993" startWordPosition="1026" endWordPosition="1029">ability of the generated texts. We believe this is the first work to propose an automatic metric to measure linguistic variability of generated texts in NLG. Section 2 provides an overview of related work on NLG. We present our main system in Section 3. The system is evaluated and discussed in Section 4. Finally, we conclude in Section 5 and point out future directions of research. 2 Background Typically, knowledge-based NLG systems are implemented by rules and, as mentioned above, have a pipelined architecture for the document and sentence planning stages and surface realization (Hovy, 1993; Moore and Paris, 1993). However, document planning is arguably the most important task (Sripada et al., 2001). It follows that approaches to document planning are rule-based as well and, concomitantly, are usually domain specific. For example, Bouayad-Agha, et al. (2011) proposed document planning based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there is a high associated dev</context>
</contexts>
<marker>Moore, Paris, 1993</marker>
<rawString>Johanna D. Moore and Cecile L. Paris. 1993. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19(4):651–694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Slim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="23311" citStr="Papineni et al., 2002" startWordPosition="3780" endWordPosition="3783">ocSys and DocBase generations are largely grammatical and coherent on the surface with some variance, but there are graded semantic variations (e.g., Director of Sales Planning vs. Director of Sales (4g-h) and move ne to Cornwall vs. from ne through the Cornwall). Both automatic and human evaluations are required in NLG to determine the impact of these variances on the understandability of the texts in general (non-experts) and as they are representative of particular subject matter domains (experts). The following sections discuss the evaluation results. 4.2 Automatic Metrics We used BLEU–4 (Papineni et al., 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. Both BLEU–4 and METEOR originate from machine translation research. BLEU–4 measures the degree of 4-gram overlap between documents. METEOR uses a unigram weighted f–score less a penalty based on chunking dissimilarity. These metrics only evaluate the text on a document level but fail to identify “syntactic repetitiveness” across documents in a document collection. This is an important characteristic of a document collection to avoid banality. To address this issue, we propose a new automatic metric called syn</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Slim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2355" citStr="Reiter and Dale (2000)" startWordPosition="348" endWordPosition="351">ffiliated with Nuance Communications, Inc. 1 Introduction NLG is the process of generating natural-sounding text from non-linguistic inputs. A typical NLG system contains three main components: (1) Document (Macro) Planning - deciding what content should be realized in the output and how it should be structured; (2) Sentence (Micro) planning - generating a detailed sentence specification and selecting appropriate referring expressions; and (3) Surface Realization - generating the final text after applying morphological modifications based on syntactic rules (see e.g., Bateman and Zock (2003), Reiter and Dale (2000) and McKeown (1985)). However, document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure. If the document planning stage fails, the communicative goal of the generated text will not be met even if the other two stages are perfect. While most traditional systems simplify development by using a pipelined approach where (1-3) are executed in a sequence, this can result in errors at one stage propagating to successive stages (see e.g., Robin and McKeown (1996)). We propose</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<pages>169</pages>
<contexts>
<context position="21129" citStr="Reiter et al., 2005" startWordPosition="3432" endWordPosition="3435">aseline texts are considered as a means of determining performance. For all experiments reported in this section, the baseline system selects the most frequent conceptual unit at the given position, chooses the most likely template for the conceptual unit, and fills the template with input data. The above process is repeated until the number of sentences is less than or equal to the average number of sentences for the given domain. 4.1 Data We ran our system on two different domains: corporate officer and director biographies and offshore oil rig weather reports from the SUMTIMEMETEO corpus ((Reiter et al., 2005)). The biography domain includes 1150 texts ranging from 3-17 sentences and the weather domain includes 1045 weather reports ranging from 1-6 sentences.2 We used a training-test(generation) split of 70/30. (4) provides generation comparisons for the system ( DocSys), baseline ( DocBase) and original ( DocOrig) randomly selected text snippets from each domain. The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al., 2005). (4) Weather DocO</context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, 2005</marker>
<rawString>Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin Yu. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
<author>Kathy McKeown</author>
</authors>
<title>Exmpirically designing and evaluating a new revision-based model for summary generation.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>85--1</pages>
<contexts>
<context position="2942" citStr="Robin and McKeown (1996)" startWordPosition="447" endWordPosition="450">d Zock (2003), Reiter and Dale (2000) and McKeown (1985)). However, document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure. If the document planning stage fails, the communicative goal of the generated text will not be met even if the other two stages are perfect. While most traditional systems simplify development by using a pipelined approach where (1-3) are executed in a sequence, this can result in errors at one stage propagating to successive stages (see e.g., Robin and McKeown (1996)). We propose a hybrid framework that combines (1-3) by converting data to text in one single process. Most NLG systems fall into two broad categories: knowledge-based and statistical. Knowledge-based systems heavily depend on having domain expertise to come up with handcrafted rules at each stage of a pipeline. Although knowledge-based systems can produce high quality text, they are (1) very expensive to build, involving a lot of discussion with the end users of the system for the document planning stage alone; (2) have limited linguistic coverage, as it is time consuming to capture linguisti</context>
</contexts>
<marker>Robin, McKeown, 1996</marker>
<rawString>Jacques Robin and Kathy McKeown. 1996. Exmpirically designing and evaluating a new revision-based model for summary generation. Artificial Intelligence, 85(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>A two-stage model for content determination.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6586" citStr="Sripada et al., 2001" startWordPosition="1040" endWordPosition="1043">c metric to measure linguistic variability of generated texts in NLG. Section 2 provides an overview of related work on NLG. We present our main system in Section 3. The system is evaluated and discussed in Section 4. Finally, we conclude in Section 5 and point out future directions of research. 2 Background Typically, knowledge-based NLG systems are implemented by rules and, as mentioned above, have a pipelined architecture for the document and sentence planning stages and surface realization (Hovy, 1993; Moore and Paris, 1993). However, document planning is arguably the most important task (Sripada et al., 2001). It follows that approaches to document planning are rule-based as well and, concomitantly, are usually domain specific. For example, Bouayad-Agha, et al. (2011) proposed document planning based on an ontology knowledge base to generate football summaries. For rule–based systems, rules exist for selecting content to grammatical choices to postprocessing (e.g., pronoun generation). These rules are often tailored to a given system, with input from multiple experts; consequently, there is a high associated development cost (e.g., 12 person months for the SUMTIME-METEO system (Belz, 2007)). Stati</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2001</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2001. A two-stage model for content determination. In Proceedings of the 8th European Workshop on Natural Language Generation (ENLG), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Mari¨et Theune</author>
<author>Emiel Krahmer</author>
</authors>
<title>Real vs. template-based natural language generation: a false opposition?</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>van Deemter, Theune, Krahmer, 2005</marker>
<rawString>Kees van Deemter, Mari¨et Theune, and Emiel Krahmer. 2005. Real vs. template-based natural language generation: a false opposition? Computational Linguistics, 31(1):15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Techniques with Java Implementation (2nd Ed.).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="11482" citStr="Witten and Frank, 2005" startWordPosition="1803" endWordPosition="1806">ect] from [institution] and a [degree] from [institution]. The outputs of the preprocessing stage are the template bank and predicate information for each template in the corpus.1 3.2 Creating Conceptual Units The next step is to create conceptual units for the corpus by clustering templates. This is a semiautomatic process where we use the predicate information for each template to compute similarity between templates. We use k-means clustering with k (equivalent to the number of semantic concepts in the domain) set to an arbitrarily high value (100) to over-generate (using the WEKA toolkit (Witten and Frank, 2005)). This allows for easier manual verification of the generated clusters and we merge them if necessary. We assign a unique identifier called a CuId (Conceptual Unit Identifier) to each cluster, which represents a “conceptual unit”. We associate each template in the corpus to a corresponding CuId. For example, in (2), using the templates in (1e-f), the identified named entities are assigned to a clustered CuId (2a-b). (2) Conceptual Units a. {CuId : 000} – [person] has been serving as [title] of the [company] since [date]. b. {CuId : 001} – [person] holds a [degree] in [subject] from [instituti</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Techniques with Java Implementation (2nd Ed.). Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>