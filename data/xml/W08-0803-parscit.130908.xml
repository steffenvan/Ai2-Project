<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.998504">
Information extraction using finite state automata and syllable n-grams
in a mobile environment
</title>
<author confidence="0.781392">
Choong-Nyoung Seon Harksoo Kim Jungyun Seo
</author>
<affiliation confidence="0.565385666666667">
Computer Science and Engi- Computer and Communications Computer Science and Engi-
neering Engineering neering
Sogang University Kangwon National University Sogang University
</affiliation>
<address confidence="0.689085">
Seoul, Korea Chuncheon, Korea Seoul, Korea
</address>
<email confidence="0.991842">
wilowisp@gmail.com nlpdrkim@kangwon.ac.kr seojy@sogang.ac.kr
</email>
<sectionHeader confidence="0.995564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999813058823529">
We propose an information extraction system
that is designed for mobile devices with low
hardware resources. The proposed system ex-
tracts temporal instances (dates and times) and
named instances (locations and topics) from
Korean short messages in an appointment
management domain. To efficiently extract
temporal instances with limited numbers of
surface forms, the proposed system uses well-
refined finite state automata. To effectively
extract various surface forms of named in-
stances with low hardware resources, the pro-
posed system uses a modified HMM based on
syllable n-grams. In the experiment on in-
stance boundary labeling, the proposed system
showed better performances than traditional
classifiers.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857333333333">
Recently, many people access various multi-media
contents using mobile devices such as a cellular
phone and a PDA (personal digital assistant). Ac-
cordingly, users’ requests on NLP (natural lan-
guage processing) are increasing because they
want to easily and simply look up the multi-media
contents. Information extraction is one of useful
applications in NLP that helps users to easily
access core information in a large amount of free
texts. Unfortunately, it is not easy to implement an
information extraction system in mobile devices
because target texts include many morphological
variations (e.g. blank omission, typos, word ab-
breviation) and mobile devices have many hard-
ware limitations (e.g. a small volume of a main
</bodyText>
<page confidence="0.985312">
13
</page>
<bodyText confidence="0.958247422222222">
memory and the absence of an arithmetic logic unit
for floating-point calculation)
There are some researches on information ex-
traction from short messages in a mobile device,
and Cooper’s research (Cooper, 2005) is represent-
ative. Cooper predefined various syntactic patterns
with placeholders and matched an input message
against the syntactic patterns. Then, he extracted
texts in the placeholders and assigned them the
attribute name of the placeholders. This method
has some advantages like easy implementation and
fast response time. However, it is inadequate to
apply Cooper’s method to languages with partially-
free word-order like Korean and Japanese because
a huge amount of syntactic patterns should be pre-
defined according as the degree of freedom on
word order increases. Kang (2004) proposed a
NLIDB (natural language interface to database)
system using lightweight shallow NLP techniques.
Kang raised problems of deep NLP techniques
such as low portability and error-proneness. Kang
proposed a lightweight approach to natural lan-
guage interfaces, where translation knowledge is
semi-automatically acquired and user questions are
only syntactically analyzed. Although Kang’s me-
thod showed good performances in spite of using
shallow NLP techniques, it is difficult to apply his
method to mobile devices because his method still
needs a morphological analyzer with a large size of
dictionary. In this paper, we propose an informa-
tion extraction system that is designed for mobile
devices with low hardware resources. The pro-
posed system extracts appointment-related infor-
mation (i.e. dates, times, locations, and topics)
from Korean short messages.
This paper is organized as follows. In section 2,
we proposed an information extraction system for
a mobile device in an appointment domain. In sec-
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 13–18,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
tion 3, we explain experimental setup and report
some experimental results. Finally, we draw some
conclusions in section 4.
2 Lightweight information extraction sys-
tem
</bodyText>
<figureCaption confidence="0.987045333333333">
Figure 1 shows an overall architecture of the pro-
posed system.
Figure 1. The system architecture
</figureCaption>
<bodyText confidence="0.999950333333333">
As shown in Figure 1, the proposed system con-
sists of an extraction part and a normalization part.
In the extraction part, the proposed system first
extracts temporal instance candidates (i.e. dates
and times) using FSA (finite-state automata). Then,
the proposed system extracts named instance can-
didates (i.e. locations and topics) using syllable n-
grams. Finally, the proposed system ranks the ex-
tracted instances and selects the highest one per
target category. In the normalization part, the pro-
posed system converts the temporal instances into
suitable forms.
</bodyText>
<subsectionHeader confidence="0.951067">
2.1 Information extraction using finite state
automata
</subsectionHeader>
<bodyText confidence="0.999951571428571">
Although short messages in an appointment do-
main often include many incorrect words, temporal
instances like dates and times are expressed as cor-
rect as possible because they are very important to
appointment management. In addition, temporal
instances are expressed in tractable numbers of
surface forms in order to make message receivers
easily be understood. In MUC-7, these kinds of
temporal instances are called TIMEX (Chinchor,
1998), and it has known that TIMEX can be easily
extracted by using FSA (Srihari, 2001). Based on
these previous works, the proposed system extracts
temporal instances from short messages by using
FSA, as shown in Figure 2.
</bodyText>
<figureCaption confidence="0.990226">
Figure 2. An example of FSA for date extraction
</figureCaption>
<subsectionHeader confidence="0.9372085">
2.2 Information extraction using statistical
syllable n-grams
</subsectionHeader>
<bodyText confidence="0.999995944444445">
Unlike dates and times, locations and topics not
only have various surface forms, but also their
constituent words are not included in a closed set.
In MUC-7, these kinds of named entities are called
NAMEX (Chinchor, 1998), and many researches
on NAMEX have been performed by using rules
and statistics. Generally, rule-based methods show
high precisions but they have a weak point that it is
hard to maintain a system when new words are
continuously added to the system (Goh, 2003). Sta-
tistical methods guarantee reasonable perfor-
mances but they need large-scale language
resource and complex floating point operations.
Therefore, it is not suitable to apply previous tradi-
tional approaches to mobile devices with many
hardware limitations. To resolve this problem, we
propose a statistical model based on syllable n-
grams, as shown in Figure 3.
</bodyText>
<figureCaption confidence="0.953628">
Figure 3. Statistical information extraction
</figureCaption>
<page confidence="0.992551">
14
</page>
<bodyText confidence="0.999043833333333">
The extraction of named instances has two kinds of
problems; a instance boundary detection problem
and a category assigning problem. If we can use a
conventional morphological analyzer, the instance
boundary detection problem is not big. However, it
is not easy to use a morphological analyzer in a
mobile device because of hardware limitations and
users’ writing habitations. Users often ignore word
spacing and this habitation lowers the performance
of the morphological analyzer. To resolve this
problem, we adopt a syllable n-gram model that
performs well in word boundary detection for lan-
guages like Chinese with no spacing between
words (Goh, 2003; Ha, 2004). We first define 9
labels that represent boundaries of named instance
candidates by adopting BIO (begin, inner, and out-
er) annotation scheme, as shown in Table 1 (Hong,
2005; Uchimoto, 2000).
</bodyText>
<table confidence="0.495301428571429">
Tag Description Tag Description
LB Begin of a location TB Begin of a topic
LI Inner of a location TI Inner of a topic
LE End of a location TE End of a topic
LS A single-syllable loca- TS A single-syllable
tion topic
OT Other syllable
</table>
<tableCaption confidence="0.997098">
Table 1. The definition of instance boundary labels
</tableCaption>
<bodyText confidence="0.992340916666667">
Then, based on a modified HMM (hidden Markov
model), the proposed system assigns boundary la-
bels to each syllable in an input message, as fol-
lows.
Let S1,n denote a message which consists of a
sequence of n syllable, s1, s2,..., sn , and let L1,n de-
note the boundary label sequence, l1, l2,..., ln , of S1,n .
Then, the label annotation problem can be formally
defined as finding L1,n which is the result of Equa-
tion (1).
size pieces about which we can collect statistics, as
shown in Equation (2).
</bodyText>
<equation confidence="0.9976893">
n
( , ) ∏ (2)
P L S = P s l s P l l s
(  |, ) (  |, )
1, 1,
n n 1, 1, 1
i i i− i i
1, 1 1, 1
− i−
i=1
</equation>
<bodyText confidence="0.9998592">
We simplify Equation (2) by making the following
two assumptions: one is that the current boundary
label is only dependent on the previous boundary
label, and the other is that current boundary label is
affected by its contextual features.
</bodyText>
<equation confidence="0.996835166666667">
n
PL S = ∏P s l P l l
* (3)
( , ) (  |) (  |)
1,n 1,n i i i i−1
i=1
</equation>
<bodyText confidence="0.9934365">
In Equation (3), P*(si |li) is a modified observation
probability that is adopted from a class probability
in naïve Bayesian classification (Zheng, 1998) as
shown in Equation (4). The reason why we modify
an original observation probability P(si |li) in
HMM is its sparseness that is caused by a size li-
mitation of training corpus in a mobile environ-
ment.
</bodyText>
<equation confidence="0.983061857142857">
1
*
P s l
(  |) = P li
( ) (  |)
i i ∏= P sij li
Z j 1
</equation>
<bodyText confidence="0.9983155">
In Equation (4), f is the number of contextual fea-
tures, and sij is the jth feature of the ith syllable. Z
is a normalizing factor. Table 2 shows the contex-
tual features that the proposed system uses.
</bodyText>
<table confidence="0.965606875">
Feature Composition Meaning
si1 si The current syllable
si2 si−1si The previous syllable and the
current syllable
si3 si si+1 The current syllable and the
next syllable
f
(4)
</table>
<tableCaption confidence="0.998218">
Table 2. The composition of contextual features
</tableCaption>
<equation confidence="0.374055333333333">
def
L(S1,n ) = arg max P(L1,n  |S1,n)
L1,n
</equation>
<bodyText confidence="0.999656545454545">
In Equation (1), we dropped P(S1,n) as it is constant
for all L1, n . Next, we break Equation (1) into bite-
In Equation (1), the max scores are calculated by
using the well-known Viterbi algorithm (Forney,
1973).
After performing instance boundary labeling,
the proposed system extracts syllable sequences
labeled with the same named categories. For ex-
ample, if a syllable sequence is labeled with ‘TS
OT LB LI LI’, the proposed system extracts the
sub-sequence of syllables labeled with ‘LB LI LI’,
</bodyText>
<equation confidence="0.966171380952381">
,
(1)
)
n
S1,
P L
( 1, n
= arg max
n
P(S1,n )
L1,
,
)
n
= arg max
n
S1,
P L
( 1,
n
L1,
</equation>
<page confidence="0.946799">
15
</page>
<figureCaption confidence="0.541285777777778">
as a location candidate. Then, the proposed system stances into machine-manageable forms like
ranks the extracted instance candidates by using ‘20080124’. However, the normalization is not
some information such as position, length, and a easy because temporal instances often include the
degree of completion, as shown in Equation (5). relative information like ‘this Sunday’ and ‘after
Rank(NIi ) = a • Position +b • Lengthi + y • Completion (5) two days’. To resolve this problem, the proposed
In Equation (5), Positioni means the distance from system converts relative temporal instances into
the beginning of input message to the ith named absolute temporal instances by using a message
instance candidate NIi. In Korean, important words arrival time. For example, if a message includes
tend to appear in the latter part of a message. the temporal instance ‘after two days’, the pro-
Therefore, we assume that the latter part an in- posed system checks arrival time information of
stance candidate appears in, the more important the the message. Then, the proposed system adds a
instance candidate is. Lengthi means the length of an date in the arrival time information to two days.
instance candidate. We assume that the longer an Figure 5 shows an example of date normalization.
instance candidate include is, the more informative
the instance candidate is. Completioni means whether
a sequence of instance boundary labels is complete.
We assume that instance candidates with complete
label sequences are more informative. To check the
degree of completion, the proposed system uses
FSA, as shown in Figure 4. In the training corpus,
every transition is legal. Therefore most of candi-
dates were satisfied the completion condition.
However, sometimes the completion condition is
not satisfied, when the candidate was extracted
from the boundary of a sentence. Accordingly the
condition gave an effect to the rank.
Figure 5. An example of date normalization
</figureCaption>
<sectionHeader confidence="0.631032" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.668681">
3.1 Data sets and experimental settings
</subsectionHeader>
<bodyText confidence="0.957717052631579">
We collected 6,190 short messages simulated in an
appointment scheduling domain. These messages
contain 4,686 locations and 4,836 topics. Each
message is manually annotated with the boundary
labels in Table 1. The manual annotation was done
by 2 graduate students majoring in natural lan-
guage processing and post-processed by a student
in a doctoral course for consistency. In order to
experiment the proposed system, we divided the
annotated messages into the training corpus and
the testing corpus by a ratio of four (4,952 messag-
es) to one (1,238 messages). Then, we performed
5-fold cross validation and used a precision, a re-
call rate, and a F1-measure as performance meas-
ures. In this paper, we did not evaluate the
performances on the temporal instance extraction
because performances of the proposed method are
fully dependent on the coverage of pre-constructed
FSA.
</bodyText>
<subsectionHeader confidence="0.612014">
3.2 Experimental results
</subsectionHeader>
<figureCaption confidence="0.792412727272727">
Figure 4. The FSA for checking label completion
In the experiments, we set a , b , and y to 1, 2,
and 10, respectively.
2.3 Normalization of temporal instances
It is inadequate for the proposed system to use the
extracted temporal instances as database instances
without any processing because the temporal in-
stances consist of various forms of human-readable
strings like ‘January 24, 2008’. Therefore, the pro-
posed system should normalize the temporal in-
16
</figureCaption>
<bodyText confidence="0.9545615">
To choose the proper size of language models in a
mobile environment, we evaluated performance
variations of the proposed system, as shown in
Figure 6.
</bodyText>
<figureCaption confidence="0.9454135">
Figure 6. The performance variations according to the
size of language models
</figureCaption>
<bodyText confidence="0.99912075">
As shown in Figure 6, the system using syllable
unigrams showed much lower performances than
the systems using syllable bigrams or syllable tri-
grams.
</bodyText>
<table confidence="0.997979333333333">
Bigram Trigram
# of features 54,711 158,525
Size of DB 1.33M 2.83M
</table>
<tableCaption confidence="0.999123">
Table 3. Space requirements of language models.
</tableCaption>
<bodyText confidence="0.971174">
However, as shown in the Table 3, although the
number of syllable trigrams was three times larger
than the number of syllable bigrams, the difference
of performances between the system using syllable
bigrams and the system using syllable trigrams was
not big (about 1%). Based on this experimental
result, we conclude that the combination of sylla-
ble unigrams and syllable bigrams, as shown in
Table 2, is the most suitable language model for
mobile devices with low hardware resources.
To evaluate the proposed system, we calculated
two types of performances. One is boundary labe-
ling performances that measure whether the pro-
posed system can correctly annotate a test corpus
with boundary labels in Table 1. The other is ex-
traction performances that measure whether the
proposed system can correctly extract named in-
stances from a test corpus by using Equation (5).
Table 4 shows the boundary labeling performances
of the proposed system in comparisons with those
of representative classifiers.
</bodyText>
<table confidence="0.999675857142857">
Model Precision Recall F1-
rate measure
NB 62.74% 75.17% 68.34%
SVM 67.29% 67.58% 67.37%
CRF 70.98% 66.27% 68.45%
Proposed 74.81% 77.20% 75.91%
system
</table>
<tableCaption confidence="0.749806">
Table 4. The comparison of boundary labeling perfor-
mances
</tableCaption>
<bodyText confidence="0.970966933333333">
In Table 4, NB is a classifier using naïve Bayesian
statistics, and SVM is a classifier using a support
vector machine. CRF is a classifier using condi-
tional random fields. As shown in Table 4, the
proposed system outperformed the comparative
models in all measures. Based on this fact, we
think that the modified HMM may be more effec-
tive in a labeling sequence problem.
Table 5 shows the extraction performances of the
proposed system. In Table 5, the reason why the
performances on the topic extraction are lower is
that topic instances can consist of more various
syllables (e.g. the topic instance, ‘a meeting in
Samsung Research Center’, includes the location,
‘Samsung Research Center’).
</bodyText>
<table confidence="0.9991055">
Category Precision Recall rate F1-
measure
Location 79.37% 76.33% 77.78%
Topic 58.54% 55.20% 56.72%
</table>
<tableCaption confidence="0.99971">
Table 5. The extraction performances
</tableCaption>
<bodyText confidence="0.556936857142857">
Table 6 shows performance variations according as
the parameters in Equation (5) are changed. As
shown in Table 6, the differences between perfor-
mances are not big, and the proposed model
showed the best performance at (a=1, 0=2, Y=5) or
(a=1, 0=2, Y=10). On the basis of this experiments,
we set a, 0, and Y to 1, 2, and 5, respectively.
</bodyText>
<table confidence="0.9982135">
Precision of Recall rate F1-measure
(,,) Location of Location of Location
(1,1,1) 79.23% 76.20% 77.65%
(1,1,5) 79.28% 76.24% 77.69%
</table>
<page confidence="0.615396">
17
</page>
<table confidence="0.9999721">
(1,1,10) 79.30% 76.26% 77.71%
(1,2,5) 79.37% 76.33% 77.78%
(1,2,10) 79.37% 76.33% 77.78%
(,,) Precision of Recall rate F1-measure
Topic of Topic of Topic
(1,1,1) 58.09% 54.76% 56.28%
(1,1,5) 58.09% 54.76% 56.28%
(1,1,10) 58.11% 54.78% 56.30%
(1,2,5) 58.54% 55.20% 56.72%
(1,2,10) 58.54% 55.20% 56.72%
</table>
<tableCaption confidence="0.9845">
Table 6. The performance variations according to para-
meter changes
</tableCaption>
<bodyText confidence="0.999683857142857">
To evaluate usefulness of the proposed model in a
real mobile phone environment, we measured an
average response time of 100 short messages in a
mobile phone with XSCALE PXA270 CPU,
51.26MB memory, and Windows mobile 5.0. We
obtained an average response time of 0.0532
seconds.
</bodyText>
<sectionHeader confidence="0.995759" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999958">
We proposed an information extraction system for
a mobile device in an appointment management
domain. The proposed system efficiently extracts
temporal instances with limited numbers of surface
forms by using FSA. To effectively extract various
surface forms of named instances with low hard-
ware resources, the proposed system uses a mod-
ified HMM based on syllable n-grams. In the
experiment on instance boundary labeling, the pro-
posed system outperformed traditional classifiers
that showed good performances in a labeling se-
quence problem. On the experimental basis, we
think that the proposed method is very suitable for
information extraction applications with many
hardware limitations.
</bodyText>
<sectionHeader confidence="0.990602" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995596">
This research (paper) was funded by Samsung
Electronics.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="references">
5 Reference
</sectionHeader>
<reference confidence="0.99905819047619">
Chooi Ling Goh, Masayuki Asahara, Yuji Matsumoto.
2003. Chinese unknown word identification using
character-based tagging and chunking. Proceedings
of ACL-2003 Interactive Posters and Demonstrations,
197-200.
G. David Forney, JR. 1973. The Viterbi Algorithm Pro-
ceedings of the IEEE, 61(3):268-278.
Hong Shen, Anoop Sarkar. 2005. Voting Between Mul-
tiple Data Representations for Text Chunking. Cana-
dian Conference on AI 2005. 389-400.
In-Su Kang, Seung-Hoon Na, Jong-Hyeok Lee, Gijoo
Yang. 2004. Lightweight Natural Language Database
Interfaces. Proceedings of the 9th International Con-
ference on Application of Natural Language to In-
formation Systems. 76-88.
Juhong Ha, Yu Zheng, Byeongchang Kim, Gary Geun-
bae Lee, Yoon-Suk Seong. 2004. High Speed Un-
known Word Prediction Using Support Vector
Machine for Chinese Text-to-Speech Systems.
IJCNLP:509-517
Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hiromi
Ozaku, and Hitoshi Isahara. Named Entity Extraction
Based on A Maximum Entropy Model and Trans-
formation Rules. In Proceedings of the 38th Annual
Meeting of Association for Computational Linguis-
tics
Nancy A. Chinchor. 1998. MUC-7 named entity task
definition, Proceedings of the Seventh Message Un-
derstanding Conference.
Richard Cooper, Sajjad Ali, Chenlan Bi, 2005. Extract-
ing Information from Short Messages, NLDB 2005,
LNCS 3513, pp. 388-391.
Rohini Srihari, Cheng Niu, Wei Li. 2001. A hybrid ap-
proach for named entity and sub-type tagging. In
Proc. 6th Applied Natural Language Processing Con-
ference.
Zijian Zheng. Naive Bayesian classifier committees.
Proceedings of the 10th European Conference on
Machine Learning. Berlin: Springet-Verlag (1998)
196-207.
SVM_light: http://svmlight.joachims.org/
CRF++: http://crfpp.sourceforge.net/
</reference>
<page confidence="0.999278">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.356767">
<title confidence="0.990759">Information extraction using finite state automata and syllable n-grams in a mobile environment</title>
<author confidence="0.983205">Seon Harksoo Kim Seo</author>
<affiliation confidence="0.856303666666667">Science and Engi- Computer and Communications Computer Science and Engineering Engineering neering Sogang University Kangwon National University Sogang University</affiliation>
<author confidence="0.569661">Korea Chuncheon Seoul</author>
<author confidence="0.569661">Korea Seoul</author>
<abstract confidence="0.996942722222222">We propose an information extraction system that is designed for mobile devices with low hardware resources. The proposed system extracts temporal instances (dates and times) and named instances (locations and topics) from Korean short messages in an appointment management domain. To efficiently extract temporal instances with limited numbers of surface forms, the proposed system uses wellrefined finite state automata. To effectively extract various surface forms of named instances with low hardware resources, the proposed system uses a modified HMM based on In the experiment on instance boundary labeling, the proposed system showed better performances than traditional classifiers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chooi Ling Goh</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chinese unknown word identification using character-based tagging and chunking.</title>
<date>2003</date>
<booktitle>Proceedings of ACL-2003 Interactive Posters and Demonstrations,</booktitle>
<pages>197--200</pages>
<marker>Goh, Asahara, Matsumoto, 2003</marker>
<rawString>Chooi Ling Goh, Masayuki Asahara, Yuji Matsumoto. 2003. Chinese unknown word identification using character-based tagging and chunking. Proceedings of ACL-2003 Interactive Posters and Demonstrations, 197-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G David Forney</author>
</authors>
<date>1973</date>
<booktitle>The Viterbi Algorithm Proceedings of the IEEE,</booktitle>
<pages>61--3</pages>
<contexts>
<context position="9528" citStr="Forney, 1973" startWordPosition="1550" endWordPosition="1551">j is the jth feature of the ith syllable. Z is a normalizing factor. Table 2 shows the contextual features that the proposed system uses. Feature Composition Meaning si1 si The current syllable si2 si−1si The previous syllable and the current syllable si3 si si+1 The current syllable and the next syllable f (4) Table 2. The composition of contextual features def L(S1,n ) = arg max P(L1,n |S1,n) L1,n In Equation (1), we dropped P(S1,n) as it is constant for all L1, n . Next, we break Equation (1) into biteIn Equation (1), the max scores are calculated by using the well-known Viterbi algorithm (Forney, 1973). After performing instance boundary labeling, the proposed system extracts syllable sequences labeled with the same named categories. For example, if a syllable sequence is labeled with ‘TS OT LB LI LI’, the proposed system extracts the sub-sequence of syllables labeled with ‘LB LI LI’, , (1) ) n S1, P L ( 1, n = arg max n P(S1,n ) L1, , ) n = arg max n S1, P L ( 1, n L1, 15 as a location candidate. Then, the proposed system stances into machine-manageable forms like ranks the extracted instance candidates by using ‘20080124’. However, the normalization is not some information such as positio</context>
</contexts>
<marker>Forney, 1973</marker>
<rawString>G. David Forney, JR. 1973. The Viterbi Algorithm Proceedings of the IEEE, 61(3):268-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Shen</author>
<author>Anoop Sarkar</author>
</authors>
<title>Voting Between Multiple Data Representations for Text Chunking. Canadian Conference on AI</title>
<date>2005</date>
<pages>389--400</pages>
<marker>Shen, Sarkar, 2005</marker>
<rawString>Hong Shen, Anoop Sarkar. 2005. Voting Between Multiple Data Representations for Text Chunking. Canadian Conference on AI 2005. 389-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In-Su Kang</author>
<author>Seung-Hoon Na</author>
<author>Jong-Hyeok Lee</author>
<author>Gijoo Yang</author>
</authors>
<title>Lightweight Natural Language Database Interfaces.</title>
<date>2004</date>
<booktitle>Proceedings of the 9th International Conference on Application of Natural Language to Information Systems.</booktitle>
<pages>76--88</pages>
<marker>Kang, Na, Lee, Yang, 2004</marker>
<rawString>In-Su Kang, Seung-Hoon Na, Jong-Hyeok Lee, Gijoo Yang. 2004. Lightweight Natural Language Database Interfaces. Proceedings of the 9th International Conference on Application of Natural Language to Information Systems. 76-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juhong Ha</author>
<author>Yu Zheng</author>
<author>Byeongchang Kim</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Yoon-Suk Seong.</title>
<date>2004</date>
<pages>509--517</pages>
<marker>Ha, Zheng, Kim, Lee, 2004</marker>
<rawString>Juhong Ha, Yu Zheng, Byeongchang Kim, Gary Geunbae Lee, Yoon-Suk Seong. 2004. High Speed Unknown Word Prediction Using Support Vector Machine for Chinese Text-to-Speech Systems. IJCNLP:509-517</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Masaki Murata</author>
<author>Hiromi Ozaku</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules.</title>
<booktitle>In Proceedings of the 38th Annual Meeting of Association for Computational Linguistics</booktitle>
<marker>Uchimoto, Ma, Murata, Ozaku, Isahara, </marker>
<rawString>Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hiromi Ozaku, and Hitoshi Isahara. Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules. In Proceedings of the 38th Annual Meeting of Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy A Chinchor</author>
</authors>
<title>MUC-7 named entity task definition,</title>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference.</booktitle>
<contexts>
<context position="5171" citStr="Chinchor, 1998" startWordPosition="762" endWordPosition="763">per target category. In the normalization part, the proposed system converts the temporal instances into suitable forms. 2.1 Information extraction using finite state automata Although short messages in an appointment domain often include many incorrect words, temporal instances like dates and times are expressed as correct as possible because they are very important to appointment management. In addition, temporal instances are expressed in tractable numbers of surface forms in order to make message receivers easily be understood. In MUC-7, these kinds of temporal instances are called TIMEX (Chinchor, 1998), and it has known that TIMEX can be easily extracted by using FSA (Srihari, 2001). Based on these previous works, the proposed system extracts temporal instances from short messages by using FSA, as shown in Figure 2. Figure 2. An example of FSA for date extraction 2.2 Information extraction using statistical syllable n-grams Unlike dates and times, locations and topics not only have various surface forms, but also their constituent words are not included in a closed set. In MUC-7, these kinds of named entities are called NAMEX (Chinchor, 1998), and many researches on NAMEX have been performe</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy A. Chinchor. 1998. MUC-7 named entity task definition, Proceedings of the Seventh Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Cooper</author>
<author>Sajjad Ali</author>
<author>Chenlan Bi</author>
</authors>
<date>2005</date>
<booktitle>Extracting Information from Short Messages, NLDB 2005, LNCS 3513,</booktitle>
<pages>388--391</pages>
<marker>Cooper, Ali, Bi, 2005</marker>
<rawString>Richard Cooper, Sajjad Ali, Chenlan Bi, 2005. Extracting Information from Short Messages, NLDB 2005, LNCS 3513, pp. 388-391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohini Srihari</author>
<author>Cheng Niu</author>
<author>Wei Li</author>
</authors>
<title>A hybrid approach for named entity and sub-type tagging.</title>
<date>2001</date>
<booktitle>In Proc. 6th Applied Natural Language Processing Conference.</booktitle>
<marker>Srihari, Niu, Li, 2001</marker>
<rawString>Rohini Srihari, Cheng Niu, Wei Li. 2001. A hybrid approach for named entity and sub-type tagging. In Proc. 6th Applied Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zijian Zheng</author>
</authors>
<title>Naive Bayesian classifier committees.</title>
<date>1998</date>
<booktitle>Proceedings of the 10th European Conference on Machine Learning.</booktitle>
<pages>196--207</pages>
<publisher>Springet-Verlag</publisher>
<location>Berlin:</location>
<contexts>
<context position="8596" citStr="Zheng, 1998" startWordPosition="1369" endWordPosition="1370">we can collect statistics, as shown in Equation (2). n ( , ) ∏ (2) P L S = P s l s P l l s ( |, ) ( |, ) 1, 1, n n 1, 1, 1 i i i− i i 1, 1 1, 1 − i− i=1 We simplify Equation (2) by making the following two assumptions: one is that the current boundary label is only dependent on the previous boundary label, and the other is that current boundary label is affected by its contextual features. n PL S = ∏P s l P l l * (3) ( , ) ( |) ( |) 1,n 1,n i i i i−1 i=1 In Equation (3), P*(si |li) is a modified observation probability that is adopted from a class probability in naïve Bayesian classification (Zheng, 1998) as shown in Equation (4). The reason why we modify an original observation probability P(si |li) in HMM is its sparseness that is caused by a size limitation of training corpus in a mobile environment. 1 * P s l ( |) = P li ( ) ( |) i i ∏= P sij li Z j 1 In Equation (4), f is the number of contextual features, and sij is the jth feature of the ith syllable. Z is a normalizing factor. Table 2 shows the contextual features that the proposed system uses. Feature Composition Meaning si1 si The current syllable si2 si−1si The previous syllable and the current syllable si3 si si+1 The current sylla</context>
</contexts>
<marker>Zheng, 1998</marker>
<rawString>Zijian Zheng. Naive Bayesian classifier committees. Proceedings of the 10th European Conference on Machine Learning. Berlin: Springet-Verlag (1998) 196-207.</rawString>
</citation>
<citation valid="false">
<note>SVM_light: http://svmlight.joachims.org/ CRF++: http://crfpp.sourceforge.net/</note>
<marker></marker>
<rawString>SVM_light: http://svmlight.joachims.org/ CRF++: http://crfpp.sourceforge.net/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>