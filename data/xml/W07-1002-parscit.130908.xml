<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004945">
<title confidence="0.989005">
Determining the Syntactic Structure of Medical Terms in Clinical Notes
</title>
<author confidence="0.985131">
Bridget T. McInnes Ted Pedersen Serguei V. Pakhomov
</author>
<affiliation confidence="0.999445333333333">
Dept. of Computer Science Dept. of Computer Science Dept. of Pharmaceutical Care
and Engineering University of Minnesota Duluth and Health Systems Center
University of Minnesota Duluth, MN, 55812 for Health Informatics
</affiliation>
<address confidence="0.7187125">
Minneapolis, MN, 55455 tpederse@d.umn.edu University of Minnesota
bthomson@cs.umn.edu Minneapolis, MN, 55455
</address>
<email confidence="0.991257">
pakh0002@umn.edu
</email>
<sectionHeader confidence="0.99549" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999787235294118">
This paper demonstrates a method for de-
termining the syntactic structure of medi-
cal terms. We use a model-fitting method
based on the Log Likelihood Ratio to clas-
sify three-word medical terms as right or
left-branching. We validate this method by
computing the agreement between the clas-
sification produced by the method and man-
ually annotated classifications. The results
show an agreement of 75% - 83%. This
method may be used effectively to enable
a wide range of applications that depend
on the semantic interpretation of medical
terms including automatic mapping of terms
to standardized vocabularies and induction
of terminologies from unstructured medical
text.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984008">
Most medical concepts are expressed via a domain
specific terminology that can either be explicitly
agreed upon or extracted empirically from domain
specific text. Regardless of how it is constructed,
a terminology serves as a foundation for informa-
tion encoding, processing and exchange in a special-
ized sub-language such as medicine. Concepts in the
medical domain are encoded through a variety of lin-
guistic forms, the most typical and widely accepted
is the noun phrase (NP). In some even further spe-
cialized subdomains within medicine, such as nurs-
ing and surgery, an argument can be made that some
concepts are represented by an entire predication
rather than encapsulated within a single nominal-
ized expression. For example, in order to describe
someone’s ability to lift objects 5 pounds or heav-
ier above their head, it may be necessary to use a
term consisting of a predicate such as [LIFT] and a
set of arguments corresponding to various thematic
roles such as &lt;PATIENT&gt; and &lt;PATH&gt; (Ruggieri
et al., 2004). In this paper, we address typical med-
ical terms encoded as noun phrases (NPs) that are
often structurally ambiguous, as in Example 1, and
discuss a case for extending the proposed method to
non-nominalized terms as well.
</bodyText>
<equation confidence="0.977042">
small1 bowel2 obstruction3 (1)
</equation>
<bodyText confidence="0.544775">
The NP in Example 1 can have at least two interpre-
tations depending on the syntactic analysis:
</bodyText>
<equation confidence="0.999614">
[[small1 bowel2] obstruction3] (2)
[small1 [bowel2 obstruction3]] (3)
</equation>
<bodyText confidence="0.995581642857143">
The term in Example 2 denotes an obstruction in
the small bowel, which is a diagnosable disorder;
whereas, the term in Example 3 refers to a small un-
specified obstruction in the bowel.
Unlike the truly ambiguous general English cases
such as the classical “American History Professor”
where the appropriate interpretation depends on the
context, medical terms, such as in Example 1, tend
to have only one appropriate interpretation. The
context, in this case, is the discourse domain of
medicine. From the standpoint of the English lan-
guage, the interpretation that follows from Example
3 is certainly plausible, but unlikely in the context
of a medical term. The syntax of a term only shows
</bodyText>
<page confidence="0.965633">
9
</page>
<note confidence="0.738103">
BioNLP 2007: Biological, translational, and clinical language processing, pages 9–16,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999924928571429">
what interpretations are possible without restricting
them to any particular one. From the syntactic anal-
ysis, we know that the term in Example 1 has the po-
tential for being ambiguous; however, we also know
that it does have an intended interpretation by virtue
of being an entry term in a standardized terminology
with a unique identifier anchoring its meaning. What
we do not know is which syntactic structure gen-
erated that interpretation. Being able to determine
the structure consistent with the intended interpreta-
tion of a clinical term can improve the analysis of
unrestricted medical text and subsequently improve
the accuracy of Natural Language Processing (NLP)
tasks that depend on semantic interpretation.
To address this problem, we propose to use a
model-fitting method which utilizes an existing sta-
tistical measure, the Log Likelihood Ratio. We val-
idate the application of this method on a corpus
of manually annotated noun-phrase-based medical
terms. First, we present previous work on structural
ambiguity resolution. Second, we describe the Log
Likelihood Ratio and then its application to deter-
mining the structure of medical terms. Third, we
describe the training corpus and discuss the compi-
lation of a test set of medical terms and human ex-
pert annotation of those terms. Last, we present the
results of a preliminary validation of the method and
discuss several possible future directions.
</bodyText>
<sectionHeader confidence="0.993934" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999896246153846">
The problem of resolving structural ambiguity has
been previously addressed in the computational lin-
guistics literature. There are multiple approaches
ranging from purely statistical (Ratnaparkhi, 1998),
to hybrid approaches that take into account the lexi-
cal semantics of the verb (Hindle and Rooth, 1993),
to corpus-based, which is the approach discussed
in this paper. (Marcus, 1980) presents an early ex-
ample of a corpus-based approach to syntactic am-
biguity resolution. One type of structural ambigu-
ity that has received much attention has to do with
nominal compounds as seen in the work of (Resnik,
1993), (Resnik and Hearst, 1993), (Pustejovsky et
al., 1993), and (Lauer, 1995).
(Lauer, 1995) points out that the existing ap-
proaches to resolving the ambiguity of noun phrases
fall roughly into two camps: adjacency and de-
pendency. The proponents of the adjacency model
((Liberman and Sproat, 1992), (Resnik, 1993) and
(Pustejovsky et al., 1993)) argue that, given a three
word noun phrase XYZ, there are two possible an-
alyzes [[XY]Z] and [X[YZ]]. The correct analysis
is chosen based on the “acceptability” of the adja-
cent bigrams A[XY] and A[YZ]. If A[XY] is more
acceptable than A[YZ], then the left-branching anal-
ysis [[XY]Z] is preferred.
(Lauer and Dras, 1994) and (Lauer, 1995) address
the issue of structural ambiguity by developing a de-
pendency model where instead of computing the ac-
ceptability of A[YZ] one would compute the accept-
ability of A[XZ]. (Lauer, 1995) argues that the de-
pendency model is not only more intuitive than the
adjacency model, but also yields better results. (La-
pata and Keller, 2004) results also support this as-
sertion.
The difference between the approaches within the
two models is the computation of acceptability. Pro-
posals for computing acceptability (or preference)
include raw frequency counts ((Evans and Zhai,
1996) and (Lapata and Keller, 2004)), Latent Se-
mantic Indexing ((Buckeridge and Sutcliffe, 2002))
and statistical measures of association ((Lapata et
al., 1999) and (Nakov and Hearst, 2005)).
One of the main problems with using frequency
counts or statistical methods for structural ambigu-
ity resolution is the sparseness of data; however,
(Resnik and Hearst, 1993) used conceptual associa-
tions (associations between groups of terms deemed
to form conceptual units) in order to alleviate this
problem. (Lapata and Keller, 2004) use the doc-
ument counts returned by WWW search engines.
(Nakov and Hearst, 2005) use the x2 measure based
on statistics obtained from WWW search engines to
compute values to determine acceptability of a syn-
tactic analysis for nominal compounds. This method
is tested using a set of general English nominal com-
pounds developed by (Lauer, 1995) as well as a set
of nominal compounds extracted from MEDLINE
abstracts.
The novel contribution of our study is in demon-
strating and validating a corpus-based method for
determining the syntactic structure of medical terms
that relies on using the statistical measure of asso-
ciation, the Log Likelihood Ratio, described in the
following section.
</bodyText>
<page confidence="0.998958">
10
</page>
<sectionHeader confidence="0.978113" genericHeader="method">
3 Log Likelihood Ratio
</sectionHeader>
<bodyText confidence="0.999705791666667">
The Log Likelihood Ratio (G2) is a “goodness of
fit” statistic first proposed by (Wilks, 1938) to test if
a given piece of data is a sample from a set of data
with a specific distribution described by a hypothe-
sized model. It was later applied by (Dunning, 1993)
as a way to determine if a sequence of N words (N-
gram) came from an independently distributed sam-
ple.
(Pedersen et al., 1996) pointed out that there ex-
ists theoretical assumptions underlying the G2 mea-
sure that were being violated therefore making them
unreliable for significance testing. (Moore, 2004)
provided additional evidence that although G2 may
not be useful for determining the significance of an
event, its near equivalence to mutual information
makes it an appropriate measure of word associa-
tion. (McInnes, 2004) applied G2 to the task of ex-
tracting three and four word collocations from raw
text.
G2, formally defined for trigrams in Equation 4,
compares the observed frequency counts with the
counts that would be expected if the words in the
trigram (3-gram; a sequence of three words) corre-
sponded to the hypothesized model.
</bodyText>
<equation confidence="0.801418">
nxyz * log( nxyz� (4)
mxyz
</equation>
<bodyText confidence="0.999976272727273">
The parameter nxyz is the observed frequency of
the trigram where x, y, and z respectively represent
the occurrence of the first, second and third words
in the trigram. The variable mxyz is the expected
frequency of the trigram which is calculated based
on the hypothesized model. This calculation varies
depending on the model used. Often the hypothe-
sized model used is the independence model which
assumes that the words in the trigram occur together
by chance. The calculation of the expected values
based on this model is as follows:
</bodyText>
<equation confidence="0.991162">
mxyz = nx++ * n+y+ * n++z/n+++ (5)
</equation>
<bodyText confidence="0.999952692307692">
The parameter, n+++, is the total number of tri-
grams that exist in the training data, and nx++,
n+y+, and n++z are the individual marginal counts
of seeing words x, y, and z in their respective posi-
tions in a trigram. A G2 score reflects the degree to
which the observed and expected values diverge. A
G2 score of zero implies that the observed values are
equal to the expected and the trigram is represented
perfectly by the hypothesized model. Hence, we
would say that the data ’fits’ the model. Therefore,
the higher the G2 score, the less likely the words
in the trigram are represented by the hypothesized
model.
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="method">
4 Methods
</sectionHeader>
<subsectionHeader confidence="0.9833515">
4.1 Applying Log Likelihood to Structural
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999980625">
The independence model is the only hypothesized
model used for bigrams (2-gram; a sequence of
two words). As the number of words in an N-
gram grows, the number of hypothesized models
also grows. The expected values for a trigram can
be based on four models. The first model is the
independence model discussed above. The second
is the model based on the probability that the first
word and the second word in the trigram are depen-
dent and independent of the third word. The third
model is based on the probability that the second
and third words are dependent and independent of
the first word. The last model is based on the prob-
ability that the first and third words are dependent
and independent of the second word. Table 1 shows
the different models for the trigram XYZ.
</bodyText>
<tableCaption confidence="0.953057">
Table 1: Models for the trigram XYZ
</tableCaption>
<bodyText confidence="0.9997001">
Model 1 P(XYZ) / P(X) P(Y) P(Z)
Model 2 P(XYZ) / P(XY) P(Z)
Model 3 P(XYZ) / P(X) / P(YZ)
Model 4 P(XYZ) / P(XZ) P(Y)
Slightly different formulas are used to calculate
the expected values for the different hypothesized
models. The expected values for Model 1 (the in-
dependence model) are given above in Equation 5.
The calculation of expected values for Model 2, 3, 4
are seen in Equations 6, 7, 8 respectively.
</bodyText>
<equation confidence="0.999270666666667">
mxyz = nxy+ * n++z/n+++
mxyz = nx++ * n+yz/n+++
mxyz = nx+z * n+y+/n+++
</equation>
<bodyText confidence="0.9656">
The parameter nxy+ is the number of times words
x and y occur in their respective positions, n+yz is
</bodyText>
<equation confidence="0.925649">
�G2 = 2 *
x,y,z
</equation>
<page confidence="0.980484">
11
</page>
<bodyText confidence="0.943747931034483">
the number of times words y and z occur in their
respective positions and n„+z is the number of times
that words x and z occur in their respective positions
in the trigram.
The hypothesized models result in different ex-
pected values which results in a different G2 score.
A G2 score of zero implies that the data are perfectly
represented by the hypothesized model and the ob-
served values are equal to the expected. Therefore,
the model that returns the lowest score for a given
trigram is the model that best represents the struc-
ture of that trigram, and hence, best ’fits’ the trigram.
For example, Table 2 shows the scores returned for
each of the four hypothesized models for the trigram
“small bowel obstruction”.
Table 2: Example for the term “small bowel obstruc-
tion”
Model G2 score Model G2 score
Model 1 11,635.45 Model 2 5,169.81
Model 3 8,532.90 Model 4 7,249.90
The smallest G2 score is returned by Model 2
which is based on the first and second words be-
ing dependent and independent of the third. Based
on the data, Model 2 best represents or ’fits’ the tri-
gram, “small bowel obstruction”. In this particular
case that happens to be the correct analysis.
The frequency counts and G2 scores for each
model were obtained using the N-gram Statistics
Package 1 (Banerjee and Pedersen, 2003).
</bodyText>
<subsectionHeader confidence="0.949387">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999125333333333">
The data for this study was collected from two
sources: the Mayo Clinic clinical notes and
SNOMED-CT terminology (Stearns et al., 2001).
</bodyText>
<subsubsectionHeader confidence="0.605839">
4.2.1 Clinical Notes
</subsubsectionHeader>
<bodyText confidence="0.999879875">
The corpus used in this study consists of over
100,000 clinical notes covering a variety of ma-
jor medical specialties at the Mayo Clinic. These
notes document each patient-physician contact and
are typically dictated over the telephone. They range
in length from a few lines to several pages of text
and represent a quasi-spontaneous discourse where
the dictations are made partly from notes and partly
</bodyText>
<footnote confidence="0.985346">
1http://www.d.umn.edu/ tpederse/nsp.html
</footnote>
<bodyText confidence="0.999039666666667">
from memory. At the Mayo Clinic, the dictations
are transcribed by trained personnel and are stored
in the patient’s chart electronically.
</bodyText>
<subsectionHeader confidence="0.461422">
4.2.2 SNOMED-CT
</subsectionHeader>
<bodyText confidence="0.968050055555555">
SNOMED-CT (Systematized Nomenclature of
Medicine, Clinical Terminology) is an ontologi-
cal resource produced by the College of American
Pathologists and distributed as part of the Unified
Medical Language System2 (UMLS) Metathesaurus
maintained by the National Library of Medicine.
SNOMED-CT is the single largest source of clini-
cal terms in the UMLS and as such lends itself well
to the analysis of terms found in clinical reports.
SNOMED-CT is used for many applications in-
cluding indexing electronic medical records, ICU
monitoring, clinical decision support, clinical trials,
computerized physician order entry, disease surveil-
lance, image indexing and consumer health informa-
tion services. The version of SNOMED-CT used in
this study consists of more than 361,800 unique con-
cepts with over 975,000 descriptions (entry terms)
(SNOMED-CT Fact Sheet, 2004).
</bodyText>
<subsectionHeader confidence="0.999333">
4.3 Testset of Three Word Terms
</subsectionHeader>
<bodyText confidence="0.9998626">
We used SNOMED-CT to compile a list of terms
in order to develop a test set to validate the G2
method. The test set was created by extracting all
trigrams from the corpus of clinical notes and all
three word terms found in SNOMED-CT. The inter-
section of the SNOMED-CT terms and the trigrams
found in the clinical notes was further restricted to
include only simple noun phrases that consist of a
head noun modified with a set of other nominal or
adjectival elements including adjectives and present
and past participles. Adverbial modification of ad-
jectives was also permitted (e.g. “partially edentu-
lous maxilla”). Noun phrases with nested prepo-
sitional phrases such as “fear of flying” as well as
three word terms that are not noun phrases such as
“does not eat” or “unable to walk” were excluded
from the test set. The resulting test set contains 710
items.
The intended interpretation of each three word
term (trigram) was determined by arriving at a
</bodyText>
<footnote confidence="0.996210666666667">
2Unified Medical Language System is a compendium of
over 130 controlled medical vocabularies encompassing over
one million concepts.
</footnote>
<page confidence="0.997256">
12
</page>
<bodyText confidence="0.9987926">
denotes a specific disorder where one leg is of a dif-
ferent length from the other. Various combinations
of subunits within this term result in nonsensical ex-
pressions.
consensus between two medical index experts
(kappa=0.704). These experts have over ten years of
experience with classifying medical diagnoses and
are highly qualified to carry out the task of deter-
mining the intended syntactic structure of a clinical
term.
</bodyText>
<tableCaption confidence="0.974792">
Table 4: Distribution of term types in the test set
Table 3: Four Types of Syntactic Structures of Tri-
gram Terms
</tableCaption>
<table confidence="0.994227083333333">
left-branching ((XY)Z):
[[urinary tract] infection]
[[right sided] weakness]
right-branching (X(YZ)):
[chronic [back pain]]
[low [blood pressure]]
non-branching ((X)(Y)(Z)):
[[follicular][thyroid][carcinoma]]
[[serum][dioxin][level]]
monolithic (XYZ):
[difficulty finding words]
[serous otitis media]
</table>
<bodyText confidence="0.9999112">
In the process of annotating the test set of tri-
grams, four types of terms emerged (Table 3). The
first two types are left and right-branching where the
left-branching phrases contain a left-adjoining group
that modifies the head of the noun phrase. The right-
branching phrases contain a right-adjoining group
that forms the kernel or the head of the noun phrase
and is modified by the remaining word on the left.
The non-branching type is where the phrase contains
a head noun that is independently modified by the
other two words. For example, in “follicular thyroid
carcinoma”, the experts felt that “carcinoma” was
modified by both “follicular” and “thyroid” indepen-
dently, where the former denotes the type of cancer
and the latter denotes its location. This intuition is
reflected in some formal medical classification sys-
tems such as the Hospital International Classifica-
tion of Disease Adaptation (HICDA) where cancers
are typically classified with at least two categories -
one for location and one for the type of malignancy.
This type of pattern is rare. We were able to iden-
tify only six examples out of the 710 terms. The
monolithic type captures the intuition that the terms
function as a collocation and are not decomposable
into subunits. For example, “leg length discrepancy”
</bodyText>
<table confidence="0.9986645">
Type Count %total
Left-branching 251 35.5
Right-branching 378 53.4
Non-branching 6 0.8
Monolithic 73 10.3
Total 708 100
</table>
<bodyText confidence="0.999868833333333">
Finally, there were two terms for which no con-
sensus could be reached: “heart irregularly irregu-
lar” and “subacute combined degeneration”. These
cases were excluded from the final set. Table 4
shows the distribution of the four types of terms in
the test set.
</bodyText>
<sectionHeader confidence="0.9978" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999833">
We hypothesize that general English typically has
a specific syntactic structure in the medical domain,
which provides a single semantic interpretation. The
patterns observed in the set of 710 medical terms
described in the previous section suggest that the
G2 method offers an intuitive way to determine the
structure of a term that underlies its syntactic struc-
ture.
</bodyText>
<tableCaption confidence="0.990497">
Table 5: G2 Model Descriptions
</tableCaption>
<table confidence="0.956175">
left-branching Model 2 [ [XY] Z ]
right-branching Model 3 [ X [YZ] ]
</table>
<bodyText confidence="0.9998435">
The left and right-branching patterns roughly cor-
respond to Models 2 and 3 in Table 5. Models 1
and 4 do not really correspond to any of the pat-
terns we were able to identify in the set of terms.
Model 1 would represent a term where words are
completely independent of each other, which is an
unlikely scenario given that we are working with
terms whose composition is dependent by definition.
This is not to say that in other applications (e.g.,
syntactic parsing) this model would not be relevant.
Model 4 suggests dependence between the outer
edges of a term and their independence from the
</bodyText>
<page confidence="0.99921">
13
</page>
<figureCaption confidence="0.999703">
Figure 1: Comparison of the results with two base-
lines: L-branching and R-branching assumptions
</figureCaption>
<bodyText confidence="0.999909666666667">
middle word, which is not motivated from the stand-
point of a traditional context free grammar which
prohibits branch crossing. However, this model may
be welcome in a dependency grammar paradigm.
One of the goals of this study is to test an ap-
plication of the G2 method trained on a corpus of
medical data to distinguish between left and right-
branching patterns. The method ought to suggest
the most likely analysis for an NP-based medical
term based on the empirical distribution of the term
and its components. As part of the evaluation, we
compute the G2 scores for each of the terms in the
test set, and picked the model with the lowest score
to represent the structural pattern of the term. We
compared these results with manually identified pat-
terns. At this preliminary stage, we cast the problem
of identifying the structure of a three word medical
term as a binary classification task where a term is
considered to be either left or right-branching, ef-
fectively forcing all terms to either be represented
by either Model 2 or Model 3.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="method">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999991818181818">
In order to validate the G2 method for determin-
ing the structure of medical terms, we calculated
the agreement between human experts’ interpreta-
tion of the syntactic structure of the terms and the
interpretation suggested by the G2 method. The
agreement was computed as the ratio of match-
ing interpretations to the total number of terms be-
ing interpreted. We used two baselines, one estab-
lished by assuming that each term is left-branching
and the other by assuming that each term is right-
branching. As is clear from Table 4, the left-
branching baseline is 35.5% and the right-branching
baseline is 53.4% meaning that if we simply as-
sign left-branching pattern to each three word term,
we would agree with human experts 35.5% of the
time. The G2 method correctly identifies 185 tri-
grams as being left-branching (Model 2) and 345 tri-
grams as being right-branching (Model 3). There are
116 right-branching trigrams incorrectly identified
as left-branching, and 62 left-branching trigrams in-
correctly identified as right- branching. Thus the
method and the human experts agreed on 530 (75%)
terms out of 708 (kappa=0.473), which is better than
both baselines (Figure 1). We did not find any over-
lap between the terms that human experts annotated
as non-branching and the terms whose corpus dis-
tribution can be represented by Model 4 ([[XZ]Y]).
This is not surprising as this pattern is very rare.
Most of the terms are represented by either Model 2
(left-branching) or Model 3 (right-branching). The
monolithic terms that the human experts felt were
not decomposable constitute 10% of all terms and
may be handled through some other mechanism
such as collocation extraction or dictionary lookup.
Excluding monolithic terms from testing results in
83.5% overall agreement (kappa=0.664).
We observed that 53% of the terms in our test
set are right-branching while only 35% are left-
branching. (Resnik, 1993) found between 64% and
67% of nominal compounds to be left-branching and
used that finding to establish a baseline for his exper-
iments with structural ambiguity resolution. (Nakov
and Hearst, 2005) also report a similar percentage
(66.8%) of left-branching noun compounds. Our
test set is not limited to nominal compounds, which
may account for the fact that a slight majority of the
terms are found to be right-branching as adjectival
modification in English is typically located to the
left of the head noun. This may also help explain
the fact that the method tends to have higher agree-
ment within the set of right-branching terms (85%)
vs. left-branching (62%).
We also observed that many of the terms marked
as monolithic by the experts are of Latin origin such
as the term in Example 9 or describe the functional
</bodyText>
<page confidence="0.997262">
14
</page>
<bodyText confidence="0.960377">
status of a patient such as the term in Example 10.
</bodyText>
<equation confidence="0.929164">
erythemas ab2 igne3 (9)
difficulty, swallowing2 solids3 (10)
</equation>
<bodyText confidence="0.999796956521739">
Example 10 merits further discussion as it illus-
trates another potential application of the method
in the domain of functional status terminology. As
was mentioned in the introduction, functional status
terms may be be represented as a predication with
a set of arguments. Such view of functional status
terminology lends itself well to a frame-based repre-
sentation of functional status terms in the context of
a database such as FrameNet 3 or PropBank4. One of
the challenging issues in representing functional sta-
tus terminology in terms of frames is the distinction
between the core predicate and the frame elements
(Ruggieri et al., 2004). It is not always clear what
lexical material should be part of the core predicate
and what lexical material should be part of one or
more arguments. Consider the term in Example 10
which represents a nominalized form of a predica-
tion. Conceivably, we could analyze this term as a
frame shown in Example 11 where the predication
consists of a predicate [DIFFICULTY] and two ar-
guments. Alternatively, Example 12 presents a dif-
ferent analysis where the predicate is a specific kind
of difficulty with a single argument.
</bodyText>
<equation confidence="0.7647954">
[P:DIFFICULTY]
[ARG1:SWALLOWING&lt;ACTIVITY&gt;] (11)
[ARG2:SOLIDS&lt;PATIENT&gt;]
[P:SWALLOWING DIFFICULTY] (12)
[ARG1: SOLIDS&lt;PATIENT&gt;]
</equation>
<bodyText confidence="0.998555363636364">
The analysis dictates the shape of the frames
and how the frames would fit into a network of
frames. The G2 method identifies Example 10 as
left-branching (Model 2), which suggests that it
would be possible to have a parent DIFFICULTY
frame and a child CLIMBING DIFFICULTY that
would inherit form its parent. An example where
this is not possible is the term “difficulty staying
asleep” where it would probably be nonsensical or at
least impractical to have a predicate such as [STAY-
ING DIFFICULTY]. It would be more intuitive to
</bodyText>
<footnote confidence="0.99995">
3http://www.icsi.berkeley.edu/framenet/
4http://www.cis.upenn.edu/ ace/
</footnote>
<bodyText confidence="0.999898222222222">
assign this term to the DIFFICULTY frame with
a frame element whose lexical content is “staying
asleep”. The method appropriately identifies the
term “difficulty staying asleep” as right-branching
(Model 3) where the words “staying asleep” are
grouped together. This is an example based on in-
formal observations; however, it does suggest a util-
ity in constructing frame-based representation of at
least some clinical terms.
</bodyText>
<sectionHeader confidence="0.996656" genericHeader="method">
7 Limitations
</sectionHeader>
<bodyText confidence="0.999758">
The main limitation of the G2 method is the expo-
nential growth in the number of models to be evalu-
ated with the growth in the length of the term. This
limitation can be partly alleviated by either only con-
sidering adjacent models and limiting the length to
5-6 words, or using a forward or backward sequen-
tial search proposed by (Pedersen et al., 1997) for
the problem of selecting models for the Word Sense
Disambiguation task.
</bodyText>
<sectionHeader confidence="0.995506" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999923913043478">
This paper presented a simple but effective method
based on G2 to determine the internal structure of
three-word noun phrase medical terms. The abil-
ity to determine the syntactic structure that gives
rise to a particular semantic interpretation of a med-
ical term may enable accurate mapping of unstruc-
tured medical text to standardized terminologies and
nomenclatures. Future directions to improve the ac-
curacy of our method include determining how other
measures of association, such as dice coefficient and
X2, perform on this task. We feel that there is a pos-
sibility that no single measure performs best over all
types of terms. In that case, we plan to investigate in-
corporating the different measures into an ensemble-
based algorithm.
We believe the model-fitting method is not lim-
ited to structural ambiguity resolution. This method
could be applied to automatic term extraction and
automatic text indexing of terms from a standard-
ized vocabulary. More broadly, the principles of us-
ing distributional characteristics of word sequences
derived from large corpora may be applied to unsu-
pervised syntactic parsing.
</bodyText>
<page confidence="0.996989">
15
</page>
<sectionHeader confidence="0.998054" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.986024">
We thank Barbara Abbott, Debra Albrecht and
Pauline Funk for their contribution to annotating the
test set and discussing aspects of medical terms.
This research was supported in part by the
NLM Training Grant in Medical Informatics (T15
LM07041-19). Ted Pedersen’s participation in this
project was supported by the NSF Faculty Early Ca-
reer Development Award (#0092784).
</bodyText>
<sectionHeader confidence="0.998536" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948804597701">
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistic Package. In
Proc. of the Fourth International Conference on Intel-
ligent Text Processing and Computational Linguistics,
Mexico City, February.
A.M. Buckeridge and R.F.E. Sutcliffe. 2002. Disam-
biguating noun compounds with latent semantic index-
ing. International Conference On Computational Lin-
guistics, pages 1–7.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61–74.
D.A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. Proc. of the
34th conference ofACL, pages 17–24.
D. Hindle and M. Rooth. 1993. Structural Ambigu-
ity and Lexical Relations. Computational Linguistics,
19(1):103–120.
M. Lapata and F. Keller. 2004. The Web as a Base-
line: Evaluaing the Performance of Unsupervised
Web-based Models for a Range of NLP Tasks. Proc.
ofHLT-NAACL, pages 121–128.
M. Lapata, S. McDonald, and F. Keller. 1999. Determi-
nants of Adjective-Noun Plausibility. Proc. of the 9th
Conference of the European Chapter ofACL, 30:36.
M. Lauer and M. Dras. 1994. A Probabilistic Model of
Compound Nouns. Proc. of the 7th Australian Joint
Conference on AI.
M. Lauer. 1995. Corpus Statistics Meet the Noun Com-
pound: Some Empirical Results. Proc. of the 33rd An-
nual Meeting ofACL, pages 47–55.
M. Liberman and R. Sproat. 1992. The stress and struc-
ture of modified noun phrases in English. Lexical Mat-
ters, CSLI Lecture Notes, 24:131–181.
M.P. Marcus. 1980. Theory of Syntactic Recognition
for Natural Languages. MIT Press Cambridge, MA,
USA.
B.T. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master’s thesis,
University of Minnesota.
R. Moore. 2004. On log-likelihood-ratios and the sig-
nificance of rare events. In Dekang Lin and Dekai
Wu, editors, Proc. of EMNLP 2004, pages 333–340,
Barcelona, Spain, July. Association for Computational
Linguistics.
P. Nakov and M. Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound
bracketing. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 17–24, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Howard Shrobe and Ted
Senator, editors, Proc. of the Thirteenth National Con-
ference on Artificial Intelligence and the Eighth Inno-
vative Applications of Artificial Intelligence Confer-
ence, Vol. 2, pages 455–460, Menlo Park, California.
AAAI Press.
T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen-
tial model selection for word sense disambiguation. In
Proc. of the Fifth Conference on Applied Natural Lan-
guage Processing, pages 388–395, Washington, DC,
April.
J. Pustejovsky, P. Anick, and S. Bergler. 1993. Lexi-
cal semantic techniques for corpus analysis. Compu-
tational Linguistics, 19(2):331–358.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Lnaguage Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
P. Resnik and M. Hearst. 1993. Structural Ambiguity
and Conceptual Relations. Proc. of the Workshop on
Very Large Corpora: Academic and Industrial Per-
spectives, June, 22(1993):58–64.
P.S. Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
A.P. Ruggieri, S. Pakhomov, and C.G. Chute. 2004. A
Corpus Driven Approach Applying the ”Frame Se-
mantic” Method for Modeling Functional Status Ter-
minology. Proc. ofMedInfo, 11(Pt 1):434–438.
M.Q. Stearns, C. Price, KA Spackman, and AY Wang.
2001. SNOMED clinical terms: overview of the de-
velopment process and project status. Proc AMIA
Symp, pages 662–6.
S. S. Wilks. 1938. The large-sample distribution of the
likelihood ratio for testing composite hypotheses. The
Annals ofMathematical Statistics, 9(1):60–62, March.
</reference>
<page confidence="0.9987">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.815476">
<title confidence="0.999791">Determining the Syntactic Structure of Medical Terms in Clinical Notes</title>
<author confidence="0.999014">Bridget T Ted Serguei V</author>
<affiliation confidence="0.9728978">Dept. of Computer Dept. of Computer Dept. of Pharmaceutical and University of Minnesota and Health Systems University of Duluth, MN, for Health Minneapolis, MN, tpederse@d.umn.edu University of bthomson@cs.umn.edu Minneapolis, MN,</affiliation>
<email confidence="0.986643">pakh0002@umn.edu</email>
<abstract confidence="0.995409777777778">This paper demonstrates a method for determining the syntactic structure of medical terms. We use a model-fitting method based on the Log Likelihood Ratio to classify three-word medical terms as right or left-branching. We validate this method by computing the agreement between the classification produced by the method and manually annotated classifications. The results show an agreement of 75% - 83%. This method may be used effectively to enable a wide range of applications that depend on the semantic interpretation of medical terms including automatic mapping of terms to standardized vocabularies and induction of terminologies from unstructured medical text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>The design, implementation, and use of the Ngram Statistic Package.</title>
<date>2003</date>
<booktitle>In Proc. of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City,</location>
<contexts>
<context position="13079" citStr="Banerjee and Pedersen, 2003" startWordPosition="2146" endWordPosition="2149">dels for the trigram “small bowel obstruction”. Table 2: Example for the term “small bowel obstruction” Model G2 score Model G2 score Model 1 11,635.45 Model 2 5,169.81 Model 3 8,532.90 Model 4 7,249.90 The smallest G2 score is returned by Model 2 which is based on the first and second words being dependent and independent of the third. Based on the data, Model 2 best represents or ’fits’ the trigram, “small bowel obstruction”. In this particular case that happens to be the correct analysis. The frequency counts and G2 scores for each model were obtained using the N-gram Statistics Package 1 (Banerjee and Pedersen, 2003). 4.2 Data The data for this study was collected from two sources: the Mayo Clinic clinical notes and SNOMED-CT terminology (Stearns et al., 2001). 4.2.1 Clinical Notes The corpus used in this study consists of over 100,000 clinical notes covering a variety of major medical specialties at the Mayo Clinic. These notes document each patient-physician contact and are typically dictated over the telephone. They range in length from a few lines to several pages of text and represent a quasi-spontaneous discourse where the dictations are made partly from notes and partly 1http://www.d.umn.edu/ tpede</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>S. Banerjee and T. Pedersen. 2003. The design, implementation, and use of the Ngram Statistic Package. In Proc. of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Buckeridge</author>
<author>R F E Sutcliffe</author>
</authors>
<title>Disambiguating noun compounds with latent semantic</title>
<date>2002</date>
<booktitle>indexing. International Conference On Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="6830" citStr="Buckeridge and Sutcliffe, 2002" startWordPosition="1066" endWordPosition="1069">mbiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the x2 measure based on statistics obtained from WWW search engines to compute valu</context>
</contexts>
<marker>Buckeridge, Sutcliffe, 2002</marker>
<rawString>A.M. Buckeridge and R.F.E. Sutcliffe. 2002. Disambiguating noun compounds with latent semantic indexing. International Conference On Computational Linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8238" citStr="Dunning, 1993" startWordPosition="1298" endWordPosition="1299"> nominal compounds extracted from MEDLINE abstracts. The novel contribution of our study is in demonstrating and validating a corpus-based method for determining the syntactic structure of medical terms that relies on using the statistical measure of association, the Log Likelihood Ratio, described in the following section. 10 3 Log Likelihood Ratio The Log Likelihood Ratio (G2) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivalence to mutual information makes it an appropriate measure of word association. (McInnes, 2004) applied G2 to the task of extracting three and four word collocations fro</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Evans</author>
<author>C Zhai</author>
</authors>
<title>Noun-phrase analysis in unrestricted text for information retrieval.</title>
<date>1996</date>
<booktitle>Proc. of the 34th conference ofACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="6739" citStr="Evans and Zhai, 1996" startWordPosition="1053" endWordPosition="1056">erred. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 20</context>
</contexts>
<marker>Evans, Zhai, 1996</marker>
<rawString>D.A. Evans and C. Zhai. 1996. Noun-phrase analysis in unrestricted text for information retrieval. Proc. of the 34th conference ofACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural Ambiguity and Lexical Relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="5180" citStr="Hindle and Rooth, 1993" startWordPosition="801" endWordPosition="804">rmining the structure of medical terms. Third, we describe the training corpus and discuss the compilation of a test set of medical terms and human expert annotation of those terms. Last, we present the results of a preliminary validation of the method and discuss several possible future directions. 2 Previous Work The problem of resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992),</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle and M. Rooth. 1993. Structural Ambiguity and Lexical Relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>The Web as a Baseline: Evaluaing the Performance of Unsupervised Web-based Models for a Range of NLP Tasks.</title>
<date>2004</date>
<booktitle>Proc. ofHLT-NAACL,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="6498" citStr="Lapata and Keller, 2004" startWordPosition="1017" endWordPosition="1021">, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>M. Lapata and F. Keller. 2004. The Web as a Baseline: Evaluaing the Performance of Unsupervised Web-based Models for a Range of NLP Tasks. Proc. ofHLT-NAACL, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>S McDonald</author>
<author>F Keller</author>
</authors>
<date>1999</date>
<booktitle>Determinants of Adjective-Noun Plausibility. Proc. of the 9th Conference of the European Chapter ofACL,</booktitle>
<pages>30--36</pages>
<contexts>
<context position="6894" citStr="Lapata et al., 1999" startWordPosition="1075" endWordPosition="1078">ceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the x2 measure based on statistics obtained from WWW search engines to compute values to determine acceptability of a syntactic analysis for nomina</context>
</contexts>
<marker>Lapata, McDonald, Keller, 1999</marker>
<rawString>M. Lapata, S. McDonald, and F. Keller. 1999. Determinants of Adjective-Noun Plausibility. Proc. of the 9th Conference of the European Chapter ofACL, 30:36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
<author>M Dras</author>
</authors>
<title>A Probabilistic Model of Compound Nouns.</title>
<date>1994</date>
<booktitle>Proc. of the 7th Australian Joint Conference on AI.</booktitle>
<contexts>
<context position="6147" citStr="Lauer and Dras, 1994" startWordPosition="959" endWordPosition="962"> et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (La</context>
</contexts>
<marker>Lauer, Dras, 1994</marker>
<rawString>M. Lauer and M. Dras. 1994. A Probabilistic Model of Compound Nouns. Proc. of the 7th Australian Joint Conference on AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus Statistics Meet the Noun Compound: Some Empirical Results.</title>
<date>1995</date>
<booktitle>Proc. of the 33rd Annual Meeting ofACL,</booktitle>
<pages>47--55</pages>
<contexts>
<context position="5559" citStr="Lauer, 1995" startWordPosition="865" endWordPosition="866">ed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras, 1994) and (Lauer,</context>
<context position="7604" citStr="Lauer, 1995" startWordPosition="1190" endWordPosition="1191">l methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the x2 measure based on statistics obtained from WWW search engines to compute values to determine acceptability of a syntactic analysis for nominal compounds. This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. The novel contribution of our study is in demonstrating and validating a corpus-based method for determining the syntactic structure of medical terms that relies on using the statistical measure of association, the Log Likelihood Ratio, described in the following section. 10 3 Log Likelihood Ratio The Log Likelihood Ratio (G2) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It wa</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>M. Lauer. 1995. Corpus Statistics Meet the Noun Compound: Some Empirical Results. Proc. of the 33rd Annual Meeting ofACL, pages 47–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liberman</author>
<author>R Sproat</author>
</authors>
<title>The stress and structure of modified noun phrases in English. Lexical Matters,</title>
<date>1992</date>
<booktitle>CSLI Lecture Notes,</booktitle>
<pages>24--131</pages>
<contexts>
<context position="5779" citStr="Liberman and Sproat, 1992" startWordPosition="898" endWordPosition="901">rb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependen</context>
</contexts>
<marker>Liberman, Sproat, 1992</marker>
<rawString>M. Liberman and R. Sproat. 1992. The stress and structure of modified noun phrases in English. Lexical Matters, CSLI Lecture Notes, 24:131–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>Theory of Syntactic Recognition for Natural Languages.</title>
<date>1980</date>
<publisher>MIT Press</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="5260" citStr="Marcus, 1980" startWordPosition="815" endWordPosition="816"> the compilation of a test set of medical terms and human expert annotation of those terms. Last, we present the results of a preliminary validation of the method and discuss several possible future directions. 2 Previous Work The problem of resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word n</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>M.P. Marcus. 1980. Theory of Syntactic Recognition for Natural Languages. MIT Press Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T McInnes</author>
</authors>
<title>Extending the log-likelihood ratio to improve collocation identification. Master’s thesis,</title>
<date>2004</date>
<institution>University of Minnesota.</institution>
<contexts>
<context position="8764" citStr="McInnes, 2004" startWordPosition="1382" endWordPosition="1383"> distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivalence to mutual information makes it an appropriate measure of word association. (McInnes, 2004) applied G2 to the task of extracting three and four word collocations from raw text. G2, formally defined for trigrams in Equation 4, compares the observed frequency counts with the counts that would be expected if the words in the trigram (3-gram; a sequence of three words) corresponded to the hypothesized model. nxyz * log( nxyz� (4) mxyz The parameter nxyz is the observed frequency of the trigram where x, y, and z respectively represent the occurrence of the first, second and third words in the trigram. The variable mxyz is the expected frequency of the trigram which is calculated based on</context>
</contexts>
<marker>McInnes, 2004</marker>
<rawString>B.T. McInnes. 2004. Extending the log-likelihood ratio to improve collocation identification. Master’s thesis, University of Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>On log-likelihood-ratios and the significance of rare events.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP 2004,</booktitle>
<pages>333--340</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="8542" citStr="Moore, 2004" startWordPosition="1348" endWordPosition="1349"> following section. 10 3 Log Likelihood Ratio The Log Likelihood Ratio (G2) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivalence to mutual information makes it an appropriate measure of word association. (McInnes, 2004) applied G2 to the task of extracting three and four word collocations from raw text. G2, formally defined for trigrams in Equation 4, compares the observed frequency counts with the counts that would be expected if the words in the trigram (3-gram; a sequence of three words) corresponded to the hypothesized model. nxyz * log( nxyz� (4) mxyz The parameter nxyz is the observed</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>R. Moore. 2004. On log-likelihood-ratios and the significance of rare events. In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP 2004, pages 333–340, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>M Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005),</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="6923" citStr="Nakov and Hearst, 2005" startWordPosition="1080" endWordPosition="1083">ould compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the x2 measure based on statistics obtained from WWW search engines to compute values to determine acceptability of a syntactic analysis for nominal compounds. This method is t</context>
<context position="22681" citStr="Nakov and Hearst, 2005" startWordPosition="3694" endWordPosition="3697">right-branching). The monolithic terms that the human experts felt were not decomposable constitute 10% of all terms and may be handled through some other mechanism such as collocation extraction or dictionary lookup. Excluding monolithic terms from testing results in 83.5% overall agreement (kappa=0.664). We observed that 53% of the terms in our test set are right-branching while only 35% are leftbranching. (Resnik, 1993) found between 64% and 67% of nominal compounds to be left-branching and used that finding to establish a baseline for his experiments with structural ambiguity resolution. (Nakov and Hearst, 2005) also report a similar percentage (66.8%) of left-branching noun compounds. Our test set is not limited to nominal compounds, which may account for the fact that a slight majority of the terms are found to be right-branching as adjectival modification in English is typically located to the left of the head noun. This may also help explain the fact that the method tends to have higher agreement within the set of right-branching terms (85%) vs. left-branching (62%). We also observed that many of the terms marked as monolithic by the experts are of Latin origin such as the term in Example 9 or de</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>P. Nakov and M. Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005), pages 17–24, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>M Kayaalp</author>
<author>R Bruce</author>
</authors>
<title>Significant lexical relationships. In</title>
<date>1996</date>
<booktitle>Proc. of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference,</booktitle>
<volume>2</volume>
<pages>455--460</pages>
<editor>Howard Shrobe and Ted Senator, editors,</editor>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, California.</location>
<contexts>
<context position="8364" citStr="Pedersen et al., 1996" startWordPosition="1320" endWordPosition="1323">ating a corpus-based method for determining the syntactic structure of medical terms that relies on using the statistical measure of association, the Log Likelihood Ratio, described in the following section. 10 3 Log Likelihood Ratio The Log Likelihood Ratio (G2) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivalence to mutual information makes it an appropriate measure of word association. (McInnes, 2004) applied G2 to the task of extracting three and four word collocations from raw text. G2, formally defined for trigrams in Equation 4, compares the observed frequency counts with the counts that would</context>
</contexts>
<marker>Pedersen, Kayaalp, Bruce, 1996</marker>
<rawString>T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Significant lexical relationships. In Howard Shrobe and Ted Senator, editors, Proc. of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference, Vol. 2, pages 455–460, Menlo Park, California. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Sequential model selection for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proc. of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>388--395</pages>
<location>Washington, DC,</location>
<contexts>
<context position="26098" citStr="Pedersen et al., 1997" startWordPosition="4246" endWordPosition="4249">g asleep” as right-branching (Model 3) where the words “staying asleep” are grouped together. This is an example based on informal observations; however, it does suggest a utility in constructing frame-based representation of at least some clinical terms. 7 Limitations The main limitation of the G2 method is the exponential growth in the number of models to be evaluated with the growth in the length of the term. This limitation can be partly alleviated by either only considering adjacent models and limiting the length to 5-6 words, or using a forward or backward sequential search proposed by (Pedersen et al., 1997) for the problem of selecting models for the Word Sense Disambiguation task. 8 Conclusions and Future Work This paper presented a simple but effective method based on G2 to determine the internal structure of three-word noun phrase medical terms. The ability to determine the syntactic structure that gives rise to a particular semantic interpretation of a medical term may enable accurate mapping of unstructured medical text to standardized terminologies and nomenclatures. Future directions to improve the accuracy of our method include determining how other measures of association, such as dice </context>
</contexts>
<marker>Pedersen, Bruce, Wiebe, 1997</marker>
<rawString>T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequential model selection for word sense disambiguation. In Proc. of the Fifth Conference on Applied Natural Language Processing, pages 388–395, Washington, DC, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Anick</author>
<author>S Bergler</author>
</authors>
<title>Lexical semantic techniques for corpus analysis.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5540" citStr="Pustejovsky et al., 1993" startWordPosition="860" endWordPosition="863">uity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras</context>
</contexts>
<marker>Pustejovsky, Anick, Bergler, 1993</marker>
<rawString>J. Pustejovsky, P. Anick, and S. Bergler. 1993. Lexical semantic techniques for corpus analysis. Computational Linguistics, 19(2):331–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Lnaguage Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5076" citStr="Ratnaparkhi, 1998" startWordPosition="785" endWordPosition="786">ambiguity resolution. Second, we describe the Log Likelihood Ratio and then its application to determining the structure of medical terms. Third, we describe the training corpus and discuss the compilation of a test set of medical terms and human expert annotation of those terms. Last, we present the results of a preliminary validation of the method and discuss several possible future directions. 2 Previous Work The problem of resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A. Ratnaparkhi. 1998. Maximum Entropy Models for Natural Lnaguage Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>M Hearst</author>
</authors>
<title>Structural Ambiguity and Conceptual Relations.</title>
<date>1993</date>
<booktitle>Proc. of the Workshop on Very Large Corpora: Academic and Industrial Perspectives,</booktitle>
<contexts>
<context position="5512" citStr="Resnik and Hearst, 1993" startWordPosition="856" endWordPosition="859"> resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] i</context>
<context position="7099" citStr="Resnik and Hearst, 1993" startWordPosition="1107" endWordPosition="1110">apata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the x2 measure based on statistics obtained from WWW search engines to compute values to determine acceptability of a syntactic analysis for nominal compounds. This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. The novel contribution</context>
</contexts>
<marker>Resnik, Hearst, 1993</marker>
<rawString>P. Resnik and M. Hearst. 1993. Structural Ambiguity and Conceptual Relations. Proc. of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, June, 22(1993):58–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Resnik</author>
</authors>
<title>Selection and Information: A ClassBased Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5485" citStr="Resnik, 1993" startWordPosition="854" endWordPosition="855">k The problem of resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-b</context>
<context position="22484" citStr="Resnik, 1993" startWordPosition="3665" endWordPosition="3666">istribution can be represented by Model 4 ([[XZ]Y]). This is not surprising as this pattern is very rare. Most of the terms are represented by either Model 2 (left-branching) or Model 3 (right-branching). The monolithic terms that the human experts felt were not decomposable constitute 10% of all terms and may be handled through some other mechanism such as collocation extraction or dictionary lookup. Excluding monolithic terms from testing results in 83.5% overall agreement (kappa=0.664). We observed that 53% of the terms in our test set are right-branching while only 35% are leftbranching. (Resnik, 1993) found between 64% and 67% of nominal compounds to be left-branching and used that finding to establish a baseline for his experiments with structural ambiguity resolution. (Nakov and Hearst, 2005) also report a similar percentage (66.8%) of left-branching noun compounds. Our test set is not limited to nominal compounds, which may account for the fact that a slight majority of the terms are found to be right-branching as adjectival modification in English is typically located to the left of the head noun. This may also help explain the fact that the method tends to have higher agreement within</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P.S. Resnik. 1993. Selection and Information: A ClassBased Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Ruggieri</author>
<author>S Pakhomov</author>
<author>C G Chute</author>
</authors>
<title>A Corpus Driven Approach Applying the ”Frame Semantic” Method for Modeling Functional Status Terminology.</title>
<date>2004</date>
<booktitle>Proc. ofMedInfo, 11(Pt</booktitle>
<pages>1--434</pages>
<contexts>
<context position="2176" citStr="Ruggieri et al., 2004" startWordPosition="331" endWordPosition="334">ety of linguistic forms, the most typical and widely accepted is the noun phrase (NP). In some even further specialized subdomains within medicine, such as nursing and surgery, an argument can be made that some concepts are represented by an entire predication rather than encapsulated within a single nominalized expression. For example, in order to describe someone’s ability to lift objects 5 pounds or heavier above their head, it may be necessary to use a term consisting of a predicate such as [LIFT] and a set of arguments corresponding to various thematic roles such as &lt;PATIENT&gt; and &lt;PATH&gt; (Ruggieri et al., 2004). In this paper, we address typical medical terms encoded as noun phrases (NPs) that are often structurally ambiguous, as in Example 1, and discuss a case for extending the proposed method to non-nominalized terms as well. small1 bowel2 obstruction3 (1) The NP in Example 1 can have at least two interpretations depending on the syntactic analysis: [[small1 bowel2] obstruction3] (2) [small1 [bowel2 obstruction3]] (3) The term in Example 2 denotes an obstruction in the small bowel, which is a diagnosable disorder; whereas, the term in Example 3 refers to a small unspecified obstruction in the bow</context>
<context position="24061" citStr="Ruggieri et al., 2004" startWordPosition="3924" endWordPosition="3927">rther discussion as it illustrates another potential application of the method in the domain of functional status terminology. As was mentioned in the introduction, functional status terms may be be represented as a predication with a set of arguments. Such view of functional status terminology lends itself well to a frame-based representation of functional status terms in the context of a database such as FrameNet 3 or PropBank4. One of the challenging issues in representing functional status terminology in terms of frames is the distinction between the core predicate and the frame elements (Ruggieri et al., 2004). It is not always clear what lexical material should be part of the core predicate and what lexical material should be part of one or more arguments. Consider the term in Example 10 which represents a nominalized form of a predication. Conceivably, we could analyze this term as a frame shown in Example 11 where the predication consists of a predicate [DIFFICULTY] and two arguments. Alternatively, Example 12 presents a different analysis where the predicate is a specific kind of difficulty with a single argument. [P:DIFFICULTY] [ARG1:SWALLOWING&lt;ACTIVITY&gt;] (11) [ARG2:SOLIDS&lt;PATIENT&gt;] [P:SWALLOW</context>
</contexts>
<marker>Ruggieri, Pakhomov, Chute, 2004</marker>
<rawString>A.P. Ruggieri, S. Pakhomov, and C.G. Chute. 2004. A Corpus Driven Approach Applying the ”Frame Semantic” Method for Modeling Functional Status Terminology. Proc. ofMedInfo, 11(Pt 1):434–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Q Stearns</author>
<author>C Price</author>
<author>KA Spackman</author>
<author>AY Wang</author>
</authors>
<title>SNOMED clinical terms: overview of the development process and project status.</title>
<date>2001</date>
<booktitle>Proc AMIA Symp,</booktitle>
<pages>662--6</pages>
<contexts>
<context position="13225" citStr="Stearns et al., 2001" startWordPosition="2170" endWordPosition="2173">odel 2 5,169.81 Model 3 8,532.90 Model 4 7,249.90 The smallest G2 score is returned by Model 2 which is based on the first and second words being dependent and independent of the third. Based on the data, Model 2 best represents or ’fits’ the trigram, “small bowel obstruction”. In this particular case that happens to be the correct analysis. The frequency counts and G2 scores for each model were obtained using the N-gram Statistics Package 1 (Banerjee and Pedersen, 2003). 4.2 Data The data for this study was collected from two sources: the Mayo Clinic clinical notes and SNOMED-CT terminology (Stearns et al., 2001). 4.2.1 Clinical Notes The corpus used in this study consists of over 100,000 clinical notes covering a variety of major medical specialties at the Mayo Clinic. These notes document each patient-physician contact and are typically dictated over the telephone. They range in length from a few lines to several pages of text and represent a quasi-spontaneous discourse where the dictations are made partly from notes and partly 1http://www.d.umn.edu/ tpederse/nsp.html from memory. At the Mayo Clinic, the dictations are transcribed by trained personnel and are stored in the patient’s chart electronic</context>
</contexts>
<marker>Stearns, Price, Spackman, Wang, 2001</marker>
<rawString>M.Q. Stearns, C. Price, KA Spackman, and AY Wang. 2001. SNOMED clinical terms: overview of the development process and project status. Proc AMIA Symp, pages 662–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Wilks</author>
</authors>
<title>The large-sample distribution of the likelihood ratio for testing composite hypotheses. The Annals ofMathematical Statistics,</title>
<date>1938</date>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="8070" citStr="Wilks, 1938" startWordPosition="1265" endWordPosition="1266">y of a syntactic analysis for nominal compounds. This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. The novel contribution of our study is in demonstrating and validating a corpus-based method for determining the syntactic structure of medical terms that relies on using the statistical measure of association, the Log Likelihood Ratio, described in the following section. 10 3 Log Likelihood Ratio The Log Likelihood Ratio (G2) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivale</context>
</contexts>
<marker>Wilks, 1938</marker>
<rawString>S. S. Wilks. 1938. The large-sample distribution of the likelihood ratio for testing composite hypotheses. The Annals ofMathematical Statistics, 9(1):60–62, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>