<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000234">
<title confidence="0.999209">
Paraphrase assessment in structured vector space:
Exploring parameters and datasets
</title>
<author confidence="0.997174">
Katrin Erk
</author>
<affiliation confidence="0.998637">
Department of Linguistics
University of Texas at Austin
</affiliation>
<email confidence="0.99842">
katrin.erk@mail.utexas.edu
</email>
<sectionHeader confidence="0.993913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9963707">
The appropriateness of paraphrases for words de-
pends often on context: “grab” can replace “catch”
in “catch a ball”, but not in “catch a cold”. Struc-
tured Vector Space (SVS) (Erk and Padó, 2008) is
a model that computes word meaning in context
in order to assess the appropriateness of such para-
phrases. This paper investigates “best-practice” pa-
rameter settings for SVS, and it presents a method to
obtain large datasets for paraphrase assessment from
corpora with WSD annotation.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802470588235">
The meaning of individual occurrences or tokens of
a word can change vastly according to its context. A
central challenge for computational lexical semantics
is describe these token meanings and how they can be
computed for new occurrences.
One prominent approach to this question is the
dictionary-based model of token meaning: The differ-
ent meanings of a word are a set of distinct, disjoint
senses enumerated in a lexicon or ontology, such as
WordNet. For each new occurrence, determining token
meaning means choosing one of the senses, a classifica-
tion task known as Word Sense Disambiguation (WSD).
Unfortunately, this task has turned out to be very hard
both for human annotators and for machines (Kilgarriff
and Rosenzweig, 2000), not at least due to granularity
problems with available resources (Palmer et al., 2007;
McCarthy, 2006). Some researchers have gone so far
as to suggest fundamental problems with the concept of
categorical word senses (Kilgarriff, 1997; Hanks, 2000).
An interesting alternative is offered by vector space
models of word meaning (Lund and Burgess, 1996; Mc-
Donald and Brew, 2004) which characterize the mean-
ing of a word entirely without reference to word senses.
Word meaning is described in terms of a vector in a high-
dimensional vector space that is constructed with dis-
tributional methods. Semantic similarity is then simply
distance to vectors of other words. Vector space models
have been most successful in modeling the meaning of
word types (i.e. in constructing type vectors). The char-
acterization of token meaning by corresponding token
vectors would represent a very interesting alternative to
dictionary-based methods by providing a direct, graded,
unsupervised measure of (dis-)similarity between words
in context that completely avoids reference to dictionary
</bodyText>
<author confidence="0.768356">
Sebastian Padó
</author>
<affiliation confidence="0.9474875">
Department of Linguistics
Stanford University
</affiliation>
<email confidence="0.966202">
pado@stanford.edu
</email>
<bodyText confidence="0.998917675675676">
senses. However, there are still considerable theoretical
and practical problems, even though there is a substan-
tial body of work (Landauer and Dumais, 1997; Schütze,
1998; Kintsch, 2001; Mitchell and Lapata, 2008).
In a recent paper (Erk and Padó, 2008), we have intro-
duced the structured vector space (SVS) model which
addresses this challenge. It yields one token vector per
input word. Token vectors are not computed by com-
bining the lexical meaning of the surrounding words –
which risks resulting in a “topicality” vector – but by
modifying the type meaning of a word with the semantic
expectations of syntactically related words, which can
be thought of as selectional preferences. For example,
in catch a ball, the token vector for ball is computed by
combining the type vector of ball with a vector for the
selectional preferences of catch for its object. The to-
ken vector for catch, conversely, is constructed from the
type vector of catch and the inverse object preference
vector of ball. The resulting token vectors describe the
meaning of a word in a particular sentence not through a
sense label, but through the distance of the token vector
to other vectors.
A natural question that arises is how vector-based
models of token meaning can be evaluated. It is of
course possible to apply them to a traditional WSD
task. However, this strategy remains vulnerable to all
criticism concerning the annotation of categorical word
senses, and also does not take advantage of the vec-
tor models’ central asset, namely gradedness. Thus,
paraphrase-based assessment for models of token mean-
ing was proposed as a representation-neutral disam-
biguation task that can replace WSD (McCarthy and
Navigli, 2007; Mitchell and Lapata, 2008). Given a
word token in context and a set of potential paraphrases,
the task consists of identifying the subset of valid para-
phrases. For example, in the following example, the
first paraphrase is appropriate, but the second is not:
</bodyText>
<listItem confidence="0.99352525">
(1) Google acquired YouTube �
Google bought YouTube
(2) How children acquire skills ��
How children buy skills
</listItem>
<bodyText confidence="0.998286">
This task is graded in the sense that there is no dis-
joint set of labels from which exactly one is picked for
each token; rather, the paraphrases form a set of labels
of which a subset is appropriate for each word token,
</bodyText>
<note confidence="0.949533">
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 57–65,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998788">
57
</page>
<bodyText confidence="0.996509433628319">
and the appropriate sets for two tokens may overlap to
varying degrees. In an ideal vector-based model, valid
paraphrases such as (1) should possess similar vectors,
and invalid ones such as (2) dissimilar ones.
In Erk and Padó (2008), we evaluated SVS on two
variants of the paraphrase assessment test: first, the pre-
diction of human judgments on a seven-point scale for
paraphrases for verb-subject pairs (Mitchell and Lap-
ata, 2008); and second, the original Lexical Substitution
task by McCarthy and Navigli (2007). To avoid overfit-
ting, we optimized our parameters on the first dataset
and evaluated only the best model on the second dataset.
However, given evidence for substantial inter-task differ-
ences, it is unclear to what extent these parameters are
optimal beyond the Mitchell and Lapata dataset. This
paper addresses this question with two experiments:
Impact of parameters. We re-examine three central
parameters of SVS. The first one is the choice of vector
combination function. Following Mitchell and Lap-
ata (2008), we previously used componentwise multi-
plication, whose interpretation in vector space is not
straightforward. The second one is reweighting. We
obtained the best performance when the context expec-
tations were reweighted by taking each component to
a (high) n-th power, which is counterintuitive. Finally,
we found subjects to be more informative in judging
the appropriateness of paraphrases than objects. This
appears to contradict work in theoretical syntax (Levin
and Rappaport Hovav, 2005).
To reassess the role of these parameters, we construct
a controlled dataset of transitive instances from the Lex-
ical Substitution corpus to reexamine and investigate
these issues, with the aim of providing “best practice”
settings for SVS. This turns out to be more difficult than
expected, leading us to suspect that a globally optimal
parameter setting across tasks may simply not exist. We
also test a simple extension of SVS that uses a richer
context (both subject and object) to construct the token
vector, with first positive results.
Dataset creation. The Lexical Substitution dataset
used in Erk and Padó (2008) was very small, which lim-
its the conclusions that can be drawn from it. This points
towards a more general problem of paraphrase-based
assessment for models of token meaning: Until now, all
datasets for this task were specifically created by hand.
It would provide a strong boost for paraphrase assess-
ment if the large annotated corpora that are available for
WSD could be reused.
We present an experiment on converting the WordNet-
annotated SemCor corpus into a set of “pseudo-
paraphrases” for paraphrase-based assessment. We use
the synonyms and direct hypernyms of an annotated
synset as these “pseudo-paraphrases”. While the syn-
onyms and hypernyms are not guaranteed to work as
direct replacements of the target word in the given con-
text, they are semantically similar to the target word.
The result is a dataset ten times larger than the Lex-
Sub dataset. As we describe in this paper, we find that
this method is nevertheless problematic: The resulting
dataset is considerably more difficult to model than the
existing hand-built paraphrase corpora, and its proper-
ties differ considerably from the manually constructed
Lexical Substitution dataset.
2 The structured vector space model
The main intuition behind the SVS model is to treat the
interpretation of a word in context as guided by expecta-
tions about typical events. This move to include typical
arguments and predicates into a model of word meaning
is motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expectations
about typical events on almost all aspects of human
language processing is well-established (McRae et al.,
1998; Narayanan and Jurafsky, 2002). In linguistics, ex-
pectations have long been used in semantic theories in
the form of selectional restrictions and selectional pref-
erences (Wilks, 1975), and more recently induced from
corpora (Resnik, 1996). Attention has mostly been lim-
ited to selectional preferences of verbs, which have been
used for for a variety of tasks (Hindle and Rooth, 1993;
Gildea and Jurafsky, 2002). A recent result that the SVS
model builds on is that selectional preferences can be
represented as prototype vectors constructed from seen
arguments (Erk, 2007; Padó et al., 2007).
Representing lemma meaning. To accommodate in-
formation about semantic expectations, the SVS model
extends the traditional representation of word meaning
as a single vector by a set of vectors, each of which
represents the word’s selectional preferences for each
relation that the word can assume in its linguistic con-
text. While we ultimately think of these relations as
“properly semantic” in the sense of semantic roles, the
instantiation of SVS we consider in this paper makes
use of dependency relations as a level of representation
that generalizes over a substantial amount of surface
variation but that can be obtained automatically with
high accuracy using current NLP tools.
The idea is illustrated in Figure 1. In the representa-
tion of the verb catch, the central square stands for the
lexical vector of catch itself. The three arrows link it to
catch’s preferences for dependency relations it can par-
ticipate in, such as for its subjects, its objects, and for
verbs for which it appears as a complement (comp−1).
The figure shows the head words that enter into the com-
putation of the selectional preference vector. Likewise,
ball is represented by one vector for ball itself, one for
ball’s preferences for its modifiers (mod), and two for
the verbs of which it can occur as a subject (subj−1)
and an object (obj−1), respectively.
This representation includes selectional preferences
(like subj, obj, mod) exactly parallel to inverse selec-
tional preferences (subj−1, obj−1, comp−1). The SVS
model is then formalized as follows. Let D be a vector
space, and let R be some set of relation labels. We then
</bodyText>
<page confidence="0.999287">
58
</page>
<figureCaption confidence="0.968159333333333">
Figure 1: Structured Vector Space representations for
noun ball and verb catch: Each box represents one
vector (lexical information or expectations)
</figureCaption>
<bodyText confidence="0.640779">
represent the meaning of a lemma w as a triple
</bodyText>
<equation confidence="0.984764">
m(w) = (vw, R, R−1)
</equation>
<bodyText confidence="0.784988444444444">
where vw E D is the type vector of the word w itself,
R : R → D maps each relation label onto a vector
that describes w’s selectional preferences, and R−1 :
R → D maps from role labels to vectors describing
inverse selectional preferences of w. Both R and R−1
are partial functions. For example, the direct object
preference is undefined for intransitive verbs.1
Computing meaning in context. SVS computes the
meaning of a word a in the context of another word
</bodyText>
<figure confidence="0.58561525">
b via their selectional preferences as follows: Let
m(a) = (va, Ra, R−1
a ) and m(b) = (vb, Rb, R−1
b ) be
</figure>
<listItem confidence="0.801472666666667">
the representations of the two words, and let r E R
be the relation linking a to b. Then, the meaning of a
and b in this context is defined as a pair of structured
vector triples: m(a r→ b) is the meaning of a with b as
its r-argument, and m(b r−1→ a) the meaning of b as the
r-argument of a:
</listItem>
<equation confidence="0.9359484">
r
m(a → b) = (va O Rb1(r), Ra − {r}, Ra1)
m(b r−1→ a) = (vb O Ra(r), Rb, R−1
b − {r})
(3)
</equation>
<bodyText confidence="0.93327545">
where v1 O v2 is a direct vector combination function
as in traditional models, e.g. addition or component-
wise multiplication. If either Ra(r) or R−1
b (r) are not
defined, the combination fails. Afterward, the filled
argument position r is deleted from Ra and R−1
b .
Figure 2 illustrates the procedure on the representa-
tions from Figure 1. The dotted lines indicate that the
lexical vector for catch is combined with the inverse
object preference of ball. Likewise, the lexical vector
for ball combines with the object preference vector of
catch.
Recursive application. In Erk and Padó (2008), we
considered only one combination step; however, the
1We use separate functions R, R−1 rather than a joint
syntactic context preference function because (a) this sepa-
ration models the conceptual difference between predicates
and arguments, and (b) it allows for a simpler, more elegant
formulation of the computation of meaning in context in Eq. 3.
</bodyText>
<figureCaption confidence="0.9797935">
Figure 2: Combining predicate and argument via
relation-specific semantic expectations
</figureCaption>
<bodyText confidence="0.999507571428571">
syntactic context of a word in a dependency tree often
consists of more than one word. It seems intuitively
plausible that disambiguation should profit from more
context information. Thus, we extend SVS with recur-
sive application. Let a stand in relation r to b. As
defined above, the result of combining m(a) and m(b)
by relation r are two structured vector triples m(a r→ b)
</bodyText>
<equation confidence="0.399015">
−1
</equation>
<bodyText confidence="0.60089225">
and m(b r→ a). If a also stands in relation s =� r to
a word c with m(c) = (va, Ra, R−1
a ), we define the
meaning of a in the context of b and c canonically as
</bodyText>
<equation confidence="0.790177857142857">
m(m(a r→ b) s→ c) = ((va O R−1
b (r)) O R−1
c (s),
Ra − {r, s}, Ra 1)
(4)
If O is associative and commutative, then m(m(a r →
b) s→ c) = m(m(a s→ c) r→ b). This will be the case
</equation>
<bodyText confidence="0.998383777777778">
for all the combination functions we use in this paper.
Note that this is a simplistic model of the influence
of multiple context words: it computes only lexical
meaning recursively, but does not model the influence
of context on the selectional preferences. For example,
the subject selectional preferences of catch are identical
to those of catch the ball, even though one would ex-
pect that the outfielder corresponds much better to the
expectations of catch the ball than of just catch.
</bodyText>
<sectionHeader confidence="0.996741" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999475">
The task that we are considering is paraphrase assess-
ment in context. Given a predicate-argument pair and
a paraphrase candidate, the models have to decide how
appropriate the paraphrase is for the predicate-argument
combination. This is the main task against which token
vector models have been evaluated in the past (Mitchell
and Lapata, 2008; Erk and Padó, 2008). In Experi-
ment 1, we use manually created paraphrases. In Exper-
iment 2, we replaces human-generated paraphrases with
“pseudo-paraphrases”, contextually similar words that
may not be completely appropriate as paraphrases in the
given context, but can be collected automatically. Our
parameter choices for SVS are as similar as possible to
the second experiment of our earlier paper.
Vector space. We use a dependency-based vector
space that counts a target word and a context word
</bodyText>
<figure confidence="0.999536627906977">
...
comp-1
catch
throw
catch
organise
...
subj
obj
obj-1 subj-1
...
cold
baseball
drift
ball
mod
...
obj
mod
accuse
say
claim
comp-1
catch
subj
ball
whirl
fly
provide
subj-1
throw
catch
organise
obj-1
he
fielder
dog
cold
baseball
drift
red
golf
elegant
</figure>
<page confidence="0.99251">
59
</page>
<bodyText confidence="0.999950642857143">
as co-occurring in a sentence if they are connected by
an “informative” path in the dependency graph for the
sentence.2 We build the space from a Minipar-parsed
version of the British National Corpus with dependency
parses obtained from Minipar (Lin, 1993). It uses raw
co-occurrence counts and 2000 dimensions.
Selectional preferences and reweighting. We use
a prototype-based selectional preference model (Erk,
2007). It models the selectional preferences of a predi-
cate for an argument position as the weighted centroid
of the vectors for all head words seen for this position
in a large corpus. Let f(a, r, b) denote the frequency of
a occurring in relation r to b in the parsed BNC. Then,
we compute the selectional preferences as:
</bodyText>
<equation confidence="0.993024">
1 � R� b(r) = N f(a, r, b) · va (5)
a:f(a,r,b)&gt;0
</equation>
<bodyText confidence="0.992315666666667">
where N is the number of fillers a with f(a, r, b) &gt; 0.
In Erk and Padó (2008), we found that applying a
reweighting step to the selectional preference vector by
taking each component of the centroid vector Rb(r) to
the n-th power lead to substantial improvements. The
motivation for this technique is to alleviate noise aris-
ing from the use of unfiltered head words for the con-
struction. The reweighted selectional preference vector
Rb(r) is defined as:
</bodyText>
<equation confidence="0.991486">
Rb(r) = (vi , ... , v��) for Rb(r) = (v1, ... , v-,,,,) (6)
</equation>
<bodyText confidence="0.9988164">
where we write (v1, ... , v,.) for the sequence of values
that make up a vector R�b(r). Inverse selectional pref-
erences R�1
b (r) of nouns are defined analogously, by
computing the centroid of the verbs seen as governors
of the noun in relation r.
In this paper, we test reweighting parameters of n be-
tween 0.5 and 30. Generally, small ns will decrease the
influence of the selectional preference vector. The result
can be thought of as a “word type vector modified by
context expectations”, while large ns increase the role
of context, until we arrive at a “contextual expectation
vector modified by the word type vector”. 3
Vector combination. We test three vector combina-
tion functions O, which have different interpretations
in vector space. The simplest one is componentwise
addition, abbreviated as add, i.e., simple vector addi-
tion.4 With addition, context dimensions receive a high
count whenever either of the two vectors has a high
co-occurrence count for the context.
</bodyText>
<footnote confidence="0.995404">
2We used the minimal context specification and plain
weight of the DependencyVectors software package.
3For the component-wise minimum combination (see be-
low), where we normalize the vectors before the combination,
the reweighting has a different effect. It shifts most of the mass
onto the largest-value dimensions and sets smaller dimensions
to values close to zero.
4Since we subsequently focus on cosine similarity, which
is length-invariant, vector addition can also be interpreted as
centroid computation.
</footnote>
<bodyText confidence="0.999956864864864">
Next, we test component-wise multiplication (mult).
This operation is more difficult to interpret in terms of
vector space, since it does not correspond to the standard
inner or outer vector products. The most straightforward
interpretation is to reinterpret the second vector as a di-
agonal matrix, i.e., as a linear transformation of the first
vector. Large entries in the second vector increase the
weight of the corresponding contexts; small entries de-
crease it. Mitchell and Lapata (2008) found this method
to yield the best results.
The third vector combination function we consider
is component-wise minimum (min). This combination
function results in a vector with high counts only for
contexts which co-occur frequently with both input vec-
tors and can thus be understood as an intersection be-
tween the two context sets. Since the entries of two
vectors need to be on the same order to magnitude for
this method to yield meaningful results, we normalize
vectors before the combination for min.
Assessing models of token meaning. Given a transi-
tive verb v with subject a and direct object b, we test
three variants of computing a token vector for v. The
first two involve only one combination step. In the subj
condition, v’s type vector is combined with the inverse
subject preference vector of a. In the obj condition, v’s
type vector is combined with the inverse object pref-
erence vector of b. The third variant is the recursive
application of the SVS combination procedure described
in Section 2 (condition both). Specifically, we combine
v’s type vector with both a’s inverse subject preference
and with b’s inverse object preference to obtain a “richer”
token vector.
In all three cases, the resulting token vector is com-
pared to the type vector of the paraphrase (in Experi-
ment 1) or the semantically related word (in Experiment
2). We use Cosine Similarity, a standard choice as vector
space similarity measure.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996817">
4.1 Experiment 1: The impact of parameters
</subsectionHeader>
<bodyText confidence="0.999934882352941">
In our 2008 paper, we tested the LexSub data only with
the parameters that showed best results on the Mitchell
and Lapata data: vector combination using component-
wise multiplication (mult), and the computation of (in-
verse) selectional preference vectors with high powers
of n = 20 or n = 30. However, there were indications
that the two datasets showed fundamental differences.
In particular, the Mitchell and Lapata data could only be
modeled using a PMI-transformed vector space, while
the LexSub data could only be modeled using raw co-
occurrence count vectors.
Another one of our findings that warrants further in-
quiry stems from our comparison of different context
choices (verb plus subject, verb plus object, noun plus
embedding verb). We found that subjects are better dis-
ambiguators than objects. This seems counterintuitive
both on theoretical and empirical grounds. Theoretically,
</bodyText>
<page confidence="0.991723">
60
</page>
<figureCaption confidence="0.99449">
Figure 3: Lexical substitution example items for “work”
</figureCaption>
<bodyText confidence="0.9996646875">
the notion of verb phrase has been motivated, among
other things, with the claim that direct objects contribute
more to a verb’s disambiguation than subjects (Levin
and Rappaport Hovav, 2005). Empirically, subjects
are known to be realized more often as pronouns than
objects, which makes their vector representations less
semantically specific. However, we used two different
datasets – the subject results on a set of intransitive
verbs, and the object results on a set of transitive verbs,
so the results are not comparable.
In this experiment, we construct a new, more con-
trolled dataset from the Lexical Substitution corpus to
systematically assess the importance of the three main
parameters: the relation used for disambiguation, the
combination function, and the reweighting parameter.
Construction of the LEXSUB-PARA dataset. The
original Lexical Substitution corpus, constructed for the
SemEval-1 lexical substitution task (McCarthy and Nav-
igli, 2007), consists of 10 instances each of 200 target
words in sentential contexts, drawn from a large inter-
net corpus (Sharoff, 2006). Contextually appropriate
paraphrases for each instance of each target word were
elicited from up to 6 participants. Figure 3 shows two in-
stances for the verb to work. The frequency distribution
over paraphrases can be understood as a characterization
of the target word’s meaning in each context.
For the current paper, we constructed a new subset of
LexSub we call LEXSUB-PARA by parsing LexSub with
Minipar (Lin, 1993) and extracting all 177 sentences
with transitive verbs that had overtly realized subjects
and objects, regardless of voice. We did not manually
verify the correctness of the parses, but discarded 17
sentences where we were not able to compute inverse
selectional preferences for the subject or object head
word (these were mostly rare proper names). This left
160 transitive instances of 42 verbs.
Evaluation For evaluation, we use a variant of the Se-
mEval “out of ten” (OOT) evaluation metrics defined by
McCarthy and Navigli (2007). They developed two met-
rics, OOT Precision and Recall, which compare where a
predicted set of appropriate paraphrases must be evalu-
ated against a gold standard set. Their metrics are called
“out of ten” because they are measure the accuracy of the
first ten paraphrases predicted by the system. Since they
allow systems to abstain from predictions for any num-
ber of tokens, their two variants average this accuracy
(a), over the tokens with a prediction (OOT Precision),
and (b), over all tokens (OOT Recall). Since our system
</bodyText>
<table confidence="0.9986634">
0.5 1 2 5 10 20
add obj 61.5 59.7 58.9 56.1 56.0 55.7
add subj 61.7 61.7 59.5 58.4 57.3 57.0
add both 61.3 60.0 60.2 57.7 57.1 56.7
mult obj 59.8 59.7 57.8 55.7 55.7 55.4
mult subj 60.3 59.7 59.3 57.3 57.7 56.7
mult both 59.9 58.8 57.1 55.8 55.3 &lt;1Pr
min obj 60.2 60.0 59.5 57.3 55.7 55.8
min subj 62.2 60.5 59.1 58.5 57.8 57.0
min both 62.3 60.2 59.8 57.3 55.8 55.1
</table>
<tableCaption confidence="0.822333">
Table 1: OOT accuracy on the LEXSUB-PARA dataset
across models and reweighting values (best results for
each model boldfaced). Random baseline: 53.7. Target
type vector baseline: 57.1. Pr: Numerical problem.
produces predictions for all tokens, OOT Precision and
Recall become identical.
</tableCaption>
<bodyText confidence="0.9992186">
Formally, let Gi be the gold paraphrases for occur-
rence i, and let f(s, i) be the frequency with which s
has been named as paraphrase for i. Let Mi be the ten
paraphrase candidates top-ranked by the SVS model for
i. We write out-of-ten accuracy (OOT) as:
</bodyText>
<equation confidence="0.989918666666667">
K,CMinGi/f PS,i) (7)
K,CGi PS,&apos;)
i)
</equation>
<bodyText confidence="0.999961666666667">
We compute two baselines. The first one is random
baseline that guesses whether paraphrases are appropri-
ate. The second baseline uses the original type vector
of the target verb without any combination, i.e., its “out
of context meaning”, as representation for the token.
Results. Table 1 shows the results on the LEXSUB-
PARA dataset. Recall that the task is to decide the ap-
propriateness of paraphrases for verb instances, disam-
biguated by the inverse selectional preferences of their
subjects (subj), their objects (obj), and both. The ran-
dom baseline attains an OOT accuracy of 53.7, and the
type vector of the target vector performs at 57.1.
SVS is able to outperform both baselines for all val-
ues of the reweighting parameter n &lt;2, and we find the
best results for the lowest value, n = 0.5. As for the
influence of the vector combination function, the best
result is yielded by min (OOT=62.3), followed by add
(OOT=61.7), while mult shows generally worse results
(OOT=60.3). For both add and mult, using only the sub-
ject as context only is optimal. The overall best result,
using min, is seen for both; however, the improvement
over subj is very small.
In the model mult-both-20, where target vectors were
multiplied with two very large expectation vectors, al-
most all instances failed due to overflow errors.
Discussion. Our results indicate that our parameter
optimization strategy in Erk and Padó (2008) was in fact
flawed. The parameters that were best for the Mitchell
and Lapata (2008) data (mult, n = 20) are suboptimal
for LEXSUB-PARA data.5 The good results for low val-
</bodyText>
<footnote confidence="0.7721745">
5We assume that our results hold for the Padó &amp; Erk (2008)
lexical substitution dataset as well, due to its similar nature.
</footnote>
<figure confidence="0.909673214285714">
Sentence
Substitutes
By asking people who work
there, I have since determined
that he didn’t. (# 2002)
be employed 4;
labour 1
Remember how hard your ances-
tors worked. (# 2005)
toil 4; labour 3;
task 1
�
OOT =1/III
i
</figure>
<page confidence="0.99499">
61
</page>
<bodyText confidence="0.999974130434783">
ues of n indicate that good discrimination between valid
and invalid paraphrases can be obtained by relatively
small modifications of the target vector in the direction
indicated by the context. Surprisingly, we still find that
the results in the subj condition are almost always better
than those in the obj condition, even though the dataset
consists only of transitive verbs, where we would have
expected the inverse result. We have two partial ex-
planations. First, we find that pronouns, which occur
frequently in subject position (I, he), are still informa-
tive enough to distinguish “animate” from “inanimate”
paraphrases of verbs such as touch. Second, we see
a higher number of Minipar errors in for object posi-
tions than for subject positions, and consequently more
data both for object fillers and for object selectional
preferences.
The overall best result was yielded by a condition that
used both (subject plus object) for disambiguation, using
the recursive modification from Eq. (4). While we see
this as a promising result, the difference to the second-
best result is very small, in almost all other conditions
the performance of both is close to the average of obj
and subj and thus a suboptimal choice.
</bodyText>
<subsectionHeader confidence="0.871176">
4.2 Experiment 2: Creating larger datasets with
pseudo-paraphrases
</subsectionHeader>
<bodyText confidence="0.99932015625">
With a size of 2,000 sentences, even the complete
LexSub dataset is tiny in comparison to many other
resources in NLP. Limiting attention to successfully
parsed transitive instances results in an even smaller
dataset on which it is difficult to distinguish noise from
genuine differences between models. This is a large
problem for the use of paraphrase appropriateness as
evaluation task for models of word meaning in context.
In consequence, the automatic creation of larger
datasets is an important task. While unsupervised meth-
ods for paraphrase induction are becoming available
(e.g., Callison-Burch (2008)), they are still so noisy
that the created datasets cannot serve as gold standards.
However, there is an alternative strategy: there is a
considerable amount of data in different languages an-
notated with categorical word sense, created (e.g.) for
Word Sense Disambiguation exercises such as Senseval.
We suggest to convert these data for use in a task similar
to paraphrase assessment, interpreting available infor-
mation about the word sense as pseudo-paraphrases.
Of course, the caveat is that these pseudo-paraphrases
may behave differently than genuine paraphrases. To
investigate this issue, we repeat Experiment 1 on this
dataset.
Construction of the SEMCOR-PARA dataset The
SemCor corpus is a subset of the Brown corpus that
contains 23,346 lemmas annotated with senses accord-
ing to WordNet 1.6. Fortunately, WordNet provides a
rich characterization of word senses. This allows us
to use the WordNet synonyms of a given word sense
as pseudo-paraphrases. Since it can be the case that
the target word is the only word in a synset, we also
</bodyText>
<table confidence="0.9992742">
0.5 1 2 5 10 20
add obj 21.7 20.7 23.2 24.3 24.2 21.8
add subj 20.6 20.1 22.9 24.4 23.3 19.7
add both 21.1 20.3 23.2 24.4 23.3 18.9
mult obj 22.6 24.8 25.0 24.4 24.2 21.4
mult subj 21.1 23.9 24.4 24.4 23.5 19.8
mult both 24.5 24.5 25.6 24.3 20.0 17.4
min obj 20.9 19.5 23.6 24.4 24.3 21.9
min subj 20.1 19.6 22.5 24.2 23.9 19.6
min both 20.1 19.8 25.2 24.5 24.3 19.0
</table>
<tableCaption confidence="0.7135595">
Table 2: OOT accuracy on the SEMCOR-PARA dataset
across models and reweighting values (best results for
each line boldfaced). Random baseline: 19.6. Target
type vector baseline: 20.8
</tableCaption>
<bodyText confidence="0.998069906976744">
need to add direct hypernyms. Direct hypernyms have
been used in annotation tasks to characterize WordNet
senses (Mihalcea and Chklovski, 2003), an indicator
that they are usually close enough in meaning to func-
tion as pseudo-paraphrases.
Again, we parsed the corpus with Minipar and iden-
tified all sense-tagged instances of the verbs from
LEXSUB-PARA, to keep the two corpora as compa-
rable as possible. For each instance wi of word w, we
collected all synonyms and direct hypernyms of the
synset as the set of appropriate paraphrases. The list
of synonyms and direct hypernyms of all other senses
of w, whether they occur in SemCor or not, were con-
sidered inappropriate paraphrases for the instance wi.
This method does not provide us with frequencies for
the pseudo-paraphrases; we thus assumed a uniform fre-
quency of 1. This does not do away with the gradedness
of the meaning representation, though, since each token
is still associated with a set of appropriate paraphrases.
Out of 2242 transitive verb instances, we further re-
moved 153 since we could not compute selectional pref-
erences for at least one of the fillers. 484 instances were
removed because WordNet did not list any verbal para-
phrases for the annotated synset or its direct hypernym.
This resulted in 1605 instances for 40 verbs, a dataset
an order of magnitude larger than LEXSUB-PARA. (See
Section 4.3 for an example verb with paraphrases.)
Results and Discussion. We again use the OOT ac-
curacy measure. The results for paraphrase assessment
on SEMCOR-PARA are shown in Table 2. The numbers
are substantially lower than for LEXSUB-PARA. This
is first and foremost a consequence of the higher “poly-
semy” of the pseudo-paraphrases. In LEXSUB-PARA,
the average numbers of possible paraphrases per tar-
get word is 20; in SEMCOR-PARA, 54. This is to be
expected and also reflected in the much lower random
baseline (19.6% OOT). However, we also observe that
the reduction in error rate over the baseline is consider-
ably lower for SEMCOR-PARA than for LEXSUB-PARA
(10% vs. 20% reduction).
Among the parameters of the model, we find the
largest impact for the reweighting parameter. The best
results occur in the middle range(n = 2 and n = 5),
</bodyText>
<page confidence="0.998189">
62
</page>
<bodyText confidence="0.999839142857143">
with both lower and higher weights yielding consid-
erably lower scores. Apparently, it is more difficult
to strike the right balance between the target and the
expectations on this dataset. This is also mirrored in
the smaller improvement of the target type vector base-
line over the random baseline. As for vector combi-
nation functions, we find the best results for the more
“intersection”-like mult and min combinations, with
somewhat lower results for add; however, the differ-
ences are rather small. Finally, combination with obj
works better than combination with subj. At least among
the best results, both is able to improve over the use of ei-
ther individual relation. The best result uses mult-both,
with an OOT accuracy of 25.6.
</bodyText>
<subsectionHeader confidence="0.999933">
4.3 Further analysis
</subsectionHeader>
<bodyText confidence="0.998085698630138">
In our two experiments, we have found systematic rela-
tionships between the SVS model parameters and their
performance within the LEXSUB-PARA and SEMCOR-
PARA datasets. Unfortunately, few of the parameter set-
tings we found to work well appear to generalize across
the two datasets; neither do they correspond to the op-
timal parameter values we established for the Mitchell
and Lapata dataset in our 2008 paper. Variables that
vary particularly strikingly are the reweighting parame-
ter and the performance of different relations. To better
understand these differences, we perform a further vali-
dation analysis that attempts to link model performance
to a variable that (a) behaves consistently across the two
datasets used in this paper and (b) sheds light onto the
patterns we have observed for the parameters.
The quantity we will use for this purpose is the aver-
age discriminativity of the model. We define discrimina-
tivity as the degree to which the token vector computed
by the model is on average more similar to the valid than
to the invalid paraphrases. For a paraphrase ordering
task such as the one we are considering, we want this
quantity to be as large as possible; very small quantities
indicate that the model is basically “guessing” an order.
Figure 4 plots disciminativity against model perfor-
mance. As can be expected, it is indeed a very strong
correlation between discriminativity and OOT accu-
racy across all models. A Pearson’s correlation test
confirms that the correlation is highly significant for
both datasets (LEXSUB-PARA: r=0.65, p &lt; 0.0001;
SEMCOR-PARA: r=0.76, p &lt; 0.0001).
Next, we considered the relationship between the
mean discriminativity for different combinations and
reweighting values n. Figure 5 shows the resulting plots,
which reveal two main differences between the datasets.
The first one is the influence of the reweighting parame-
ter. For LEXSUB-PARA, the highest discriminativity is
found for small values of n, with decreasing values for
higher parameter values. In contrast, SEMCOR-PARA
shows the highest discriminativity for middle values of
n (on the order of 5–10), with lowest values on either
side. The second difference is the relative discrimina-
tivity of obj and subj. On LEXSUB-PARA, the subj
predictions are more discriminative than obj predictions
for all values of n. On SEMCOR-PARA, this picture is
reversed, with more discriminative obj predictions for
the best (and thus relevant) values of n.
We interpret these patterns, which fit the observed
OOT accuracy numbers well, as additional evidence that
the variations we see between the datasets are not noise
or artifacts of the setup, but arise due to the different
makeup of the two datasets. This ties in with our intu-
itions about the differences between human-generated
paraphrases and WordNet “pseudo-paraphrases”. Com-
pare the following paraphrase lists:
dismiss (LexSub): banish, deride, discard, discharge, dis-
patch, excuse, fire, ignore, reject, release, remove, sack
dismiss (SemCor/WordNet): alter, axe, brush, can, change,
discount, displace, disregard, dissolve, drop, farewell,
fire, force, ignore, modify, notice, packing, push, reject,
remove, sack, send, terminate, throw, usher
The SEMCOR-PARA list contains a larger number of
unspecific pseudo-paraphrases such as change, push,
send, which stem from direct WordNet hypernyms of
the more specific dismiss senses. Presumably, these
terms are assigned rather general vectors which the SVS
finds difficult to rule out as paraphrases. This lowers
the discriminativity of the models, in particular for subj,
and results in the smaller relative improvement over
the baseline we observe for SEMCOR-PARA. This sug-
gests that the usability of word sense-derived datasets
in evaluations could be improved by taking depth in the
WordNet hierarchy into account when including direct
hypernyms among the pseudo-paraphrases.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999900818181818">
In this paper, we have explored the parameter space
for the computation of vector-based representations of
token meaning with the SVS model.
Our evaluation scenario was paraphrase assessment.
To systematically assess the impact of parameter choice,
we created two new controlled datasets. The first one,
the LEXSUB-PARA dataset, is a small subset of the Lex-
ical Substitution corpus (McCarthy and Navigli, 2007)
that was specifically created for this task. The second
dataset, SEMCOR-PARA, which is considerably larger,
consists in instances from the SemCor corpus whose
WordNet annotation was automatically converted into
“pseudo-paraphrase” annotation.6
We found a small number of regularities that hold for
both datasets: namely, that the reweighting parameter
is the most important choice for a SVS model, followed
by the relation used as context, while the influence of
the vector combination function is comparatively small.
Unfortunately, the actual settings of these parameters
appeared not to generalize well from one dataset to
the other. We have collected evidence that these diver-
gences are not due to noise, but to genuine differences
</bodyText>
<footnote confidence="0.957058">
6Both datasets can be obtained from the authors.
</footnote>
<page confidence="0.997441">
63
</page>
<figure confidence="0.997311169014085">
0.56 0.57 0.58 0.59 0.60 0.61 0.62
out of ten precision
0.18 0.20 0.22 0.24
ut of ten precision
0.018 0.020 0.022 0.024 0.026 0.028 0.030
mean sim(val)−sim(inval)
0.005 0.010 0.015 0.020 0.025 0.030
mean sim(val)−sim(inval)
●●
●
●●●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●
●
</figure>
<figureCaption confidence="0.9919005">
Figure 4: Scatterplot of &amp;quot;out of ten&amp;quot; accuracy against model discriminativity between valid and invalid paraphrases.
Left: LEXSUB-PARA, right: SEMCOR-PARA.
</figureCaption>
<figure confidence="0.998614166666667">
exponent
exponent
0 5 10 15 20 25 30
mean sim(val)−sim(inval)
0.020 0.022 0.024 0.026 0.028
target + object selpref
target + subject selpref
0 5 10 15 20 25 30
mean sim(val)−sim(inval)
0.005 0.010 0.015 0.020 0.025
target + object selpref
target + subject selpref
</figure>
<figureCaption confidence="0.999335">
Figure 5: Average amount to which predictions are more similar to valid than to invalid paraphrases, for different
reweighting values. Left: LEXSUB-PARA, right: SEMCOR-PARA.
</figureCaption>
<bodyText confidence="0.999974558823529">
in the datasets. We describe an auxiliary quantity, dis-
criminativity, that measures the ability of the model’s
predictions to distinguish between valid and invalid para-
phrases.
The consequence we draw from this study is that it
is surprisingly difficult to establish generalizable “best
practice” parameter setting for SVS. Good parameter
values appear to be sensitive to the properties of datasets.
For example, we have attributed the observation that
subjects are more informative on LEXSUB-PARA, while
objects work better on SEMCOR-PARA, to differences
in the set of paraphrase competitors. In this regard,
the conversion of the WSD corpus can be considered a
partial success. We have constructed the largest existing
paraphrase assessment corpus. However, the use of
WordNet information to create paraphrases results in a
very difficult corpus. We will investigate methods that
exclude overly general hypernyms of the target words as
paraphrases to alleviate the problems we see currently.
Discriminativity further suggests that paraphrase as-
sessment can be improved by selectional preference
representations that are trained to maximize the dis-
tance between valid and invalid paraphrases. Such a
representation could be provided by discriminative for-
mulations (Bergsma et al., 2008), or by exemplar-based
models that are able to deal better with the ambiguity
present in the preferences of very general words.
Another important topic for further research is the
computation of token vectors that incorporate more than
one context word. The current results we obtain for
“both” are promising but limited; it appears that the suc-
cessful integration of multiple context words requires
strategies that go beyond simplistic addition or intersec-
tion of observed contexts.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.960650743902439">
S. Bergsma, D. Lin, and R. Goebel. 2008. Discrimina-
tive learning of selectional preference from unlabeled
text. In Proceedings of EMNLP, pages 59–68.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of EMNLP, pages 196–205.
K. Erk and S. Padó. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP.
S. Narayanan and D. Jurafsky. 2002. A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Proceedings of NIPS, pages
59–65.
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of ACL, pages
216–223.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205–215.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103–120.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the
Humanities, 34(1-2).
A. Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities, 31(2):91–113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173–202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104(2):211–240.
B. Levin and M. Rappaport Hovav. 2005. Argument
Realization. Research Surveys in Linguistics Series.
CUP.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proceedings of ACL, pages 112–120.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28.
D. McCarthy and R. Navigli. 2007. SemEval-2007
Task 10: English Lexical Substitution Task. In Pro-
ceedings of SemEval, pages 48–53.
D. McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the ACL
Workshop on Making Sense of Sense, pages 17–24.
S. McDonald and C. Brew. 2004. A distributional
model of semantic context effects in lexical process-
ing. In Proceedings of ACL, pages 17–24.
K. McRae, M. Spivey-Knowlton, and M. Tanenhaus.
1998. Modeling the influence of thematic fit (and
other constraints) in on-line sentence comprehension.
Journal of Memory and Language, 38:283–312.
R. Mihalcea and T. Chklovski. 2003. Open Mind
Word Expert: Creating large annotated data collec-
tions with web users’ help. In Proceedings of the
EACL 2003 Workshop on Linguistically Annotated
Corpora (LINC 2003), Budapest, Hungary.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
pages 236–244.
S. Padó, U. Padó, and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
In Proceedings of EMNLP/CoNLL, pages 400–409.
M. Palmer, H. Dang, and C. Fellbaum. 2007. Making
fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natural
Language Engineering. To appear.
P. Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127–159.
H. Schütze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97–124.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435–462.
Y. Wilks. 1975. Preference semantics. In Formal
Semantics of Natural Language. CUP.
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895877">
<title confidence="0.998763">Paraphrase assessment in structured vector Exploring parameters and datasets</title>
<author confidence="0.969629">Katrin</author>
<affiliation confidence="0.997755">Department of University of Texas at</affiliation>
<email confidence="0.998107">katrin.erk@mail.utexas.edu</email>
<abstract confidence="0.993523090909091">The appropriateness of paraphrases for words depends often on context: “grab” can replace “catch” in “catch a ball”, but not in “catch a cold”. Struc- Vector Space (Erk and Padó, 2008) is model that computes meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates “best-practice” pasettings for and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>D Lin</author>
<author>R Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>59--68</pages>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>S. Bergsma, D. Lin, and R. Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of EMNLP, pages 59–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>196--205</pages>
<contexts>
<context position="28653" citStr="Callison-Burch (2008)" startWordPosition="4712" endWordPosition="4713">raphrases With a size of 2,000 sentences, even the complete LexSub dataset is tiny in comparison to many other resources in NLP. Limiting attention to successfully parsed transitive instances results in an even smaller dataset on which it is difficult to distinguish noise from genuine differences between models. This is a large problem for the use of paraphrase appropriateness as evaluation task for models of word meaning in context. In consequence, the automatic creation of larger datasets is an important task. While unsupervised methods for paraphrase induction are becoming available (e.g., Callison-Burch (2008)), they are still so noisy that the created datasets cannot serve as gold standards. However, there is an alternative strategy: there is a considerable amount of data in different languages annotated with categorical word sense, created (e.g.) for Word Sense Disambiguation exercises such as Senseval. We suggest to convert these data for use in a task similar to paraphrase assessment, interpreting available information about the word sense as pseudo-paraphrases. Of course, the caveat is that these pseudo-paraphrases may behave differently than genuine paraphrases. To investigate this issue, we </context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>C. Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of EMNLP, pages 196–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2827" citStr="Erk and Padó, 2008" startWordPosition="428" endWordPosition="431">e characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measure of (dis-)similarity between words in context that completely avoids reference to dictionary Sebastian Padó Department of Linguistics Stanford University pado@stanford.edu senses. However, there are still considerable theoretical and practical problems, even though there is a substantial body of work (Landauer and Dumais, 1997; Schütze, 1998; Kintsch, 2001; Mitchell and Lapata, 2008). In a recent paper (Erk and Padó, 2008), we have introduced the structured vector space (SVS) model which addresses this challenge. It yields one token vector per input word. Token vectors are not computed by combining the lexical meaning of the surrounding words – which risks resulting in a “topicality” vector – but by modifying the type meaning of a word with the semantic expectations of syntactically related words, which can be thought of as selectional preferences. For example, in catch a ball, the token vector for ball is computed by combining the type vector of ball with a vector for the selectional preferences of catch for i</context>
<context position="5298" citStr="Erk and Padó (2008)" startWordPosition="837" endWordPosition="840">ense that there is no disjoint set of labels from which exactly one is picked for each token; rather, the paraphrases form a set of labels of which a subset is appropriate for each word token, Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 57–65, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 57 and the appropriate sets for two tokens may overlap to varying degrees. In an ideal vector-based model, valid paraphrases such as (1) should possess similar vectors, and invalid ones such as (2) dissimilar ones. In Erk and Padó (2008), we evaluated SVS on two variants of the paraphrase assessment test: first, the prediction of human judgments on a seven-point scale for paraphrases for verb-subject pairs (Mitchell and Lapata, 2008); and second, the original Lexical Substitution task by McCarthy and Navigli (2007). To avoid overfitting, we optimized our parameters on the first dataset and evaluated only the best model on the second dataset. However, given evidence for substantial inter-task differences, it is unclear to what extent these parameters are optimal beyond the Mitchell and Lapata dataset. This paper addresses this</context>
<context position="7212" citStr="Erk and Padó (2008)" startWordPosition="1133" endWordPosition="1136">o reassess the role of these parameters, we construct a controlled dataset of transitive instances from the Lexical Substitution corpus to reexamine and investigate these issues, with the aim of providing “best practice” settings for SVS. This turns out to be more difficult than expected, leading us to suspect that a globally optimal parameter setting across tasks may simply not exist. We also test a simple extension of SVS that uses a richer context (both subject and object) to construct the token vector, with first positive results. Dataset creation. The Lexical Substitution dataset used in Erk and Padó (2008) was very small, which limits the conclusions that can be drawn from it. This points towards a more general problem of paraphrase-based assessment for models of token meaning: Until now, all datasets for this task were specifically created by hand. It would provide a strong boost for paraphrase assessment if the large annotated corpora that are available for WSD could be reused. We present an experiment on converting the WordNetannotated SemCor corpus into a set of “pseudoparaphrases” for paraphrase-based assessment. We use the synonyms and direct hypernyms of an annotated synset as these “pse</context>
<context position="12816" citStr="Erk and Padó (2008)" startWordPosition="2085" endWordPosition="2088">r), Rb, R−1 b − {r}) (3) where v1 O v2 is a direct vector combination function as in traditional models, e.g. addition or componentwise multiplication. If either Ra(r) or R−1 b (r) are not defined, the combination fails. Afterward, the filled argument position r is deleted from Ra and R−1 b . Figure 2 illustrates the procedure on the representations from Figure 1. The dotted lines indicate that the lexical vector for catch is combined with the inverse object preference of ball. Likewise, the lexical vector for ball combines with the object preference vector of catch. Recursive application. In Erk and Padó (2008), we considered only one combination step; however, the 1We use separate functions R, R−1 rather than a joint syntactic context preference function because (a) this separation models the conceptual difference between predicates and arguments, and (b) it allows for a simpler, more elegant formulation of the computation of meaning in context in Eq. 3. Figure 2: Combining predicate and argument via relation-specific semantic expectations syntactic context of a word in a dependency tree often consists of more than one word. It seems intuitively plausible that disambiguation should profit from more</context>
<context position="14850" citStr="Erk and Padó, 2008" startWordPosition="2444" endWordPosition="2447">. For example, the subject selectional preferences of catch are identical to those of catch the ball, even though one would expect that the outfielder corresponds much better to the expectations of catch the ball than of just catch. 3 Experimental Setup The task that we are considering is paraphrase assessment in context. Given a predicate-argument pair and a paraphrase candidate, the models have to decide how appropriate the paraphrase is for the predicate-argument combination. This is the main task against which token vector models have been evaluated in the past (Mitchell and Lapata, 2008; Erk and Padó, 2008). In Experiment 1, we use manually created paraphrases. In Experiment 2, we replaces human-generated paraphrases with “pseudo-paraphrases”, contextually similar words that may not be completely appropriate as paraphrases in the given context, but can be collected automatically. Our parameter choices for SVS are as similar as possible to the second experiment of our earlier paper. Vector space. We use a dependency-based vector space that counts a target word and a context word ... comp-1 catch throw catch organise ... subj obj obj-1 subj-1 ... cold baseball drift ball mod ... obj mod accuse say</context>
<context position="16451" citStr="Erk and Padó (2008)" startWordPosition="2715" endWordPosition="2718">(Lin, 1993). It uses raw co-occurrence counts and 2000 dimensions. Selectional preferences and reweighting. We use a prototype-based selectional preference model (Erk, 2007). It models the selectional preferences of a predicate for an argument position as the weighted centroid of the vectors for all head words seen for this position in a large corpus. Let f(a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC. Then, we compute the selectional preferences as: 1 � R� b(r) = N f(a, r, b) · va (5) a:f(a,r,b)&gt;0 where N is the number of fillers a with f(a, r, b) &gt; 0. In Erk and Padó (2008), we found that applying a reweighting step to the selectional preference vector by taking each component of the centroid vector Rb(r) to the n-th power lead to substantial improvements. The motivation for this technique is to alleviate noise arising from the use of unfiltered head words for the construction. The reweighted selectional preference vector Rb(r) is defined as: Rb(r) = (vi , ... , v��) for Rb(r) = (v1, ... , v-,,,,) (6) where we write (v1, ... , v,.) for the sequence of values that make up a vector R�b(r). Inverse selectional preferences R�1 b (r) of nouns are defined analogously,</context>
<context position="26240" citStr="Erk and Padó (2008)" startWordPosition="4315" endWordPosition="4318">.5. As for the influence of the vector combination function, the best result is yielded by min (OOT=62.3), followed by add (OOT=61.7), while mult shows generally worse results (OOT=60.3). For both add and mult, using only the subject as context only is optimal. The overall best result, using min, is seen for both; however, the improvement over subj is very small. In the model mult-both-20, where target vectors were multiplied with two very large expectation vectors, almost all instances failed due to overflow errors. Discussion. Our results indicate that our parameter optimization strategy in Erk and Padó (2008) was in fact flawed. The parameters that were best for the Mitchell and Lapata (2008) data (mult, n = 20) are suboptimal for LEXSUB-PARA data.5 The good results for low val5We assume that our results hold for the Padó &amp; Erk (2008) lexical substitution dataset as well, due to its similar nature. Sentence Substitutes By asking people who work there, I have since determined that he didn’t. (# 2002) be employed 4; labour 1 Remember how hard your ancestors worked. (# 2005) toil 4; labour 3; task 1 � OOT =1/III i 61 ues of n indicate that good discrimination between valid and invalid paraphrases can</context>
</contexts>
<marker>Erk, Padó, 2008</marker>
<rawString>K. Erk and S. Padó. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>59--65</pages>
<contexts>
<context position="8875" citStr="Narayanan and Jurafsky, 2002" startWordPosition="1399" endWordPosition="1402">araphrase corpora, and its properties differ considerably from the manually constructed Lexical Substitution dataset. 2 The structured vector space model The main intuition behind the SVS model is to treat the interpretation of a word in context as guided by expectations about typical events. This move to include typical arguments and predicates into a model of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accomm</context>
</contexts>
<marker>Narayanan, Jurafsky, 2002</marker>
<rawString>S. Narayanan and D. Jurafsky. 2002. A Bayesian model predicts human parse preference and reading time in sentence processing. In Proceedings of NIPS, pages 59–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="9416" citStr="Erk, 2007" startWordPosition="1486" endWordPosition="1487"> well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accommodate information about semantic expectations, the SVS model extends the traditional representation of word meaning as a single vector by a set of vectors, each of which represents the word’s selectional preferences for each relation that the word can assume in its linguistic context. While we ultimately think of these relations as “properly semantic” in the sense of semantic roles, the instantiation of SVS we consider in this paper makes use of dependency relations as a level of representation that generalizes over a substantial amoun</context>
<context position="16005" citStr="Erk, 2007" startWordPosition="2627" endWordPosition="2628">... cold baseball drift ball mod ... obj mod accuse say claim comp-1 catch subj ball whirl fly provide subj-1 throw catch organise obj-1 he fielder dog cold baseball drift red golf elegant 59 as co-occurring in a sentence if they are connected by an “informative” path in the dependency graph for the sentence.2 We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993). It uses raw co-occurrence counts and 2000 dimensions. Selectional preferences and reweighting. We use a prototype-based selectional preference model (Erk, 2007). It models the selectional preferences of a predicate for an argument position as the weighted centroid of the vectors for all head words seen for this position in a large corpus. Let f(a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC. Then, we compute the selectional preferences as: 1 � R� b(r) = N f(a, r, b) · va (5) a:f(a,r,b)&gt;0 where N is the number of fillers a with f(a, r, b) &gt; 0. In Erk and Padó (2008), we found that applying a reweighting step to the selectional preference vector by taking each component of the centroid vector Rb(r) to the n-th power l</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>K. Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of ACL, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="9255" citStr="Gildea and Jurafsky, 2002" startWordPosition="1459" endWordPosition="1462">ivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accommodate information about semantic expectations, the SVS model extends the traditional representation of word meaning as a single vector by a set of vectors, each of which represents the word’s selectional preferences for each relation that the word can assume in its linguistic context. While we ultimately think of these relations as “properly semantic” in the sense of semantic r</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<date>2000</date>
<booktitle>Do word meanings exist? Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1671" citStr="Hanks, 2000" startWordPosition="257" endWordPosition="258">distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vect</context>
</contexts>
<marker>Hanks, 2000</marker>
<rawString>P. Hanks. 2000. Do word meanings exist? Computers and the Humanities, 34(1-2):205–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="9227" citStr="Hindle and Rooth, 1993" startWordPosition="1455" endWordPosition="1458">l of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accommodate information about semantic expectations, the SVS model extends the traditional representation of word meaning as a single vector by a set of vectors, each of which represents the word’s selectional preferences for each relation that the word can assume in its linguistic context. While we ultimately think of these relations as “properly semantic</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle and M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<title>Framework and results for English Senseval. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="1420" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="217" endWordPosition="220">central challenge for computational lexical semantics is describe these token meanings and how they can be computed for new occurrences. One prominent approach to this question is the dictionary-based model of token meaning: The different meanings of a word are a set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semant</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff and J. Rosenzweig. 2000. Framework and results for English Senseval. Computers and the Humanities, 34(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>I don’t believe in word senses.</title>
<date>1997</date>
<journal>Computers and the Humanities,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="1657" citStr="Kilgarriff, 1997" startWordPosition="255" endWordPosition="256">word are a set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by correspond</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>A. Kilgarriff. 1997. I don’t believe in word senses. Computers and the Humanities, 31(2):91–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<contexts>
<context position="2759" citStr="Kintsch, 2001" startWordPosition="418" endWordPosition="419"> meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measure of (dis-)similarity between words in context that completely avoids reference to dictionary Sebastian Padó Department of Linguistics Stanford University pado@stanford.edu senses. However, there are still considerable theoretical and practical problems, even though there is a substantial body of work (Landauer and Dumais, 1997; Schütze, 1998; Kintsch, 2001; Mitchell and Lapata, 2008). In a recent paper (Erk and Padó, 2008), we have introduced the structured vector space (SVS) model which addresses this challenge. It yields one token vector per input word. Token vectors are not computed by combining the lexical meaning of the surrounding words – which risks resulting in a “topicality” vector – but by modifying the type meaning of a word with the semantic expectations of syntactically related words, which can be thought of as selectional preferences. For example, in catch a ball, the token vector for ball is computed by combining the type vector </context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>W. Kintsch. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="2729" citStr="Landauer and Dumais, 1997" startWordPosition="412" endWordPosition="415"> have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measure of (dis-)similarity between words in context that completely avoids reference to dictionary Sebastian Padó Department of Linguistics Stanford University pado@stanford.edu senses. However, there are still considerable theoretical and practical problems, even though there is a substantial body of work (Landauer and Dumais, 1997; Schütze, 1998; Kintsch, 2001; Mitchell and Lapata, 2008). In a recent paper (Erk and Padó, 2008), we have introduced the structured vector space (SVS) model which addresses this challenge. It yields one token vector per input word. Token vectors are not computed by combining the lexical meaning of the surrounding words – which risks resulting in a “topicality” vector – but by modifying the type meaning of a word with the semantic expectations of syntactically related words, which can be thought of as selectional preferences. For example, in catch a ball, the token vector for ball is computed</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
<author>M Rappaport Hovav</author>
</authors>
<title>Argument Realization. Research Surveys in Linguistics Series.</title>
<date>2005</date>
<publisher>CUP.</publisher>
<marker>Levin, Hovav, 2005</marker>
<rawString>B. Levin and M. Rappaport Hovav. 2005. Argument Realization. Research Surveys in Linguistics Series. CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>112--120</pages>
<contexts>
<context position="15843" citStr="Lin, 1993" startWordPosition="2606" endWordPosition="2607">Vector space. We use a dependency-based vector space that counts a target word and a context word ... comp-1 catch throw catch organise ... subj obj obj-1 subj-1 ... cold baseball drift ball mod ... obj mod accuse say claim comp-1 catch subj ball whirl fly provide subj-1 throw catch organise obj-1 he fielder dog cold baseball drift red golf elegant 59 as co-occurring in a sentence if they are connected by an “informative” path in the dependency graph for the sentence.2 We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993). It uses raw co-occurrence counts and 2000 dimensions. Selectional preferences and reweighting. We use a prototype-based selectional preference model (Erk, 2007). It models the selectional preferences of a predicate for an argument position as the weighted centroid of the vectors for all head words seen for this position in a large corpus. Let f(a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC. Then, we compute the selectional preferences as: 1 � R� b(r) = N f(a, r, b) · va (5) a:f(a,r,b)&gt;0 where N is the number of fillers a with f(a, r, b) &gt; 0. In Erk and Pad</context>
<context position="22821" citStr="Lin, 1993" startWordPosition="3735" endWordPosition="3736">mEval-1 lexical substitution task (McCarthy and Navigli, 2007), consists of 10 instances each of 200 target words in sentential contexts, drawn from a large internet corpus (Sharoff, 2006). Contextually appropriate paraphrases for each instance of each target word were elicited from up to 6 participants. Figure 3 shows two instances for the verb to work. The frequency distribution over paraphrases can be understood as a characterization of the target word’s meaning in each context. For the current paper, we constructed a new subset of LexSub we call LEXSUB-PARA by parsing LexSub with Minipar (Lin, 1993) and extracting all 177 sentences with transitive verbs that had overtly realized subjects and objects, regardless of voice. We did not manually verify the correctness of the parses, but discarded 17 sentences where we were not able to compute inverse selectional preferences for the subject or object head word (these were mostly rare proper names). This left 160 transitive instances of 42 verbs. Evaluation For evaluation, we use a variant of the SemEval “out of ten” (OOT) evaluation metrics defined by McCarthy and Navigli (2007). They developed two metrics, OOT Precision and Recall, which comp</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>D. Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of ACL, pages 112–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<volume>28</volume>
<contexts>
<context position="1773" citStr="Lund and Burgess, 1996" startWordPosition="271" endWordPosition="274"> occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>SemEval-2007 Task 10: English Lexical Substitution Task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>48--53</pages>
<contexts>
<context position="4279" citStr="McCarthy and Navigli, 2007" startWordPosition="667" endWordPosition="670">nce not through a sense label, but through the distance of the token vector to other vectors. A natural question that arises is how vector-based models of token meaning can be evaluated. It is of course possible to apply them to a traditional WSD task. However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models’ central asset, namely gradedness. Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD (McCarthy and Navigli, 2007; Mitchell and Lapata, 2008). Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases. For example, in the following example, the first paraphrase is appropriate, but the second is not: (1) Google acquired YouTube � Google bought YouTube (2) How children acquire skills �� How children buy skills This task is graded in the sense that there is no disjoint set of labels from which exactly one is picked for each token; rather, the paraphrases form a set of labels of which a subset is appropriate for each word token, Proceed</context>
<context position="5581" citStr="McCarthy and Navigli (2007)" startWordPosition="881" endWordPosition="884">ge Semantics, pages 57–65, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 57 and the appropriate sets for two tokens may overlap to varying degrees. In an ideal vector-based model, valid paraphrases such as (1) should possess similar vectors, and invalid ones such as (2) dissimilar ones. In Erk and Padó (2008), we evaluated SVS on two variants of the paraphrase assessment test: first, the prediction of human judgments on a seven-point scale for paraphrases for verb-subject pairs (Mitchell and Lapata, 2008); and second, the original Lexical Substitution task by McCarthy and Navigli (2007). To avoid overfitting, we optimized our parameters on the first dataset and evaluated only the best model on the second dataset. However, given evidence for substantial inter-task differences, it is unclear to what extent these parameters are optimal beyond the Mitchell and Lapata dataset. This paper addresses this question with two experiments: Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector spac</context>
<context position="22273" citStr="McCarthy and Navigli, 2007" startWordPosition="3643" endWordPosition="3647">y specific. However, we used two different datasets – the subject results on a set of intransitive verbs, and the object results on a set of transitive verbs, so the results are not comparable. In this experiment, we construct a new, more controlled dataset from the Lexical Substitution corpus to systematically assess the importance of the three main parameters: the relation used for disambiguation, the combination function, and the reweighting parameter. Construction of the LEXSUB-PARA dataset. The original Lexical Substitution corpus, constructed for the SemEval-1 lexical substitution task (McCarthy and Navigli, 2007), consists of 10 instances each of 200 target words in sentential contexts, drawn from a large internet corpus (Sharoff, 2006). Contextually appropriate paraphrases for each instance of each target word were elicited from up to 6 participants. Figure 3 shows two instances for the verb to work. The frequency distribution over paraphrases can be understood as a characterization of the target word’s meaning in each context. For the current paper, we constructed a new subset of LexSub we call LEXSUB-PARA by parsing LexSub with Minipar (Lin, 1993) and extracting all 177 sentences with transitive ve</context>
<context position="37525" citStr="McCarthy and Navigli, 2007" startWordPosition="6135" endWordPosition="6138">sts that the usability of word sense-derived datasets in evaluations could be improved by taking depth in the WordNet hierarchy into account when including direct hypernyms among the pseudo-paraphrases. 5 Conclusions In this paper, we have explored the parameter space for the computation of vector-based representations of token meaning with the SVS model. Our evaluation scenario was paraphrase assessment. To systematically assess the impact of parameter choice, we created two new controlled datasets. The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. The second dataset, SEMCOR-PARA, which is considerably larger, consists in instances from the SemCor corpus whose WordNet annotation was automatically converted into “pseudo-paraphrase” annotation.6 We found a small number of regularities that hold for both datasets: namely, that the reweighting parameter is the most important choice for a SVS model, followed by the relation used as context, while the influence of the vector combination function is comparatively small. Unfortunately, the actual settings of these parameters appeared not to generaliz</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10: English Lexical Substitution Task. In Proceedings of SemEval, pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Relating WordNet senses for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Making Sense of Sense,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1525" citStr="McCarthy, 2006" startWordPosition="235" endWordPosition="236">w occurrences. One prominent approach to this question is the dictionary-based model of token meaning: The different meanings of a word are a set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most succe</context>
</contexts>
<marker>McCarthy, 2006</marker>
<rawString>D. McCarthy. 2006. Relating WordNet senses for word sense disambiguation. In Proceedings of the ACL Workshop on Making Sense of Sense, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
<author>C Brew</author>
</authors>
<title>A distributional model of semantic context effects in lexical processing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1799" citStr="McDonald and Brew, 2004" startWordPosition="275" endWordPosition="279"> token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measu</context>
</contexts>
<marker>McDonald, Brew, 2004</marker>
<rawString>S. McDonald and C. Brew. 2004. A distributional model of semantic context effects in lexical processing. In Proceedings of ACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>M Spivey-Knowlton</author>
<author>M Tanenhaus</author>
</authors>
<title>Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<pages>38--283</pages>
<contexts>
<context position="8844" citStr="McRae et al., 1998" startWordPosition="1395" endWordPosition="1398">xisting hand-built paraphrase corpora, and its properties differ considerably from the manually constructed Lexical Substitution dataset. 2 The structured vector space model The main intuition behind the SVS model is to treat the interpretation of a word in context as guided by expectations about typical events. This move to include typical arguments and predicates into a model of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Repres</context>
</contexts>
<marker>McRae, Spivey-Knowlton, Tanenhaus, 1998</marker>
<rawString>K. McRae, M. Spivey-Knowlton, and M. Tanenhaus. 1998. Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38:283–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
</authors>
<title>Open Mind Word Expert: Creating large annotated data collections with web users’ help.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL 2003 Workshop on Linguistically Annotated Corpora (LINC</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="30391" citStr="Mihalcea and Chklovski, 2003" startWordPosition="5002" endWordPosition="5005">0.6 20.1 22.9 24.4 23.3 19.7 add both 21.1 20.3 23.2 24.4 23.3 18.9 mult obj 22.6 24.8 25.0 24.4 24.2 21.4 mult subj 21.1 23.9 24.4 24.4 23.5 19.8 mult both 24.5 24.5 25.6 24.3 20.0 17.4 min obj 20.9 19.5 23.6 24.4 24.3 21.9 min subj 20.1 19.6 22.5 24.2 23.9 19.6 min both 20.1 19.8 25.2 24.5 24.3 19.0 Table 2: OOT accuracy on the SEMCOR-PARA dataset across models and reweighting values (best results for each line boldfaced). Random baseline: 19.6. Target type vector baseline: 20.8 need to add direct hypernyms. Direct hypernyms have been used in annotation tasks to characterize WordNet senses (Mihalcea and Chklovski, 2003), an indicator that they are usually close enough in meaning to function as pseudo-paraphrases. Again, we parsed the corpus with Minipar and identified all sense-tagged instances of the verbs from LEXSUB-PARA, to keep the two corpora as comparable as possible. For each instance wi of word w, we collected all synonyms and direct hypernyms of the synset as the set of appropriate paraphrases. The list of synonyms and direct hypernyms of all other senses of w, whether they occur in SemCor or not, were considered inappropriate paraphrases for the instance wi. This method does not provide us with fr</context>
</contexts>
<marker>Mihalcea, Chklovski, 2003</marker>
<rawString>R. Mihalcea and T. Chklovski. 2003. Open Mind Word Expert: Creating large annotated data collections with web users’ help. In Proceedings of the EACL 2003 Workshop on Linguistically Annotated Corpora (LINC 2003), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="2787" citStr="Mitchell and Lapata, 2008" startWordPosition="420" endWordPosition="423">d types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measure of (dis-)similarity between words in context that completely avoids reference to dictionary Sebastian Padó Department of Linguistics Stanford University pado@stanford.edu senses. However, there are still considerable theoretical and practical problems, even though there is a substantial body of work (Landauer and Dumais, 1997; Schütze, 1998; Kintsch, 2001; Mitchell and Lapata, 2008). In a recent paper (Erk and Padó, 2008), we have introduced the structured vector space (SVS) model which addresses this challenge. It yields one token vector per input word. Token vectors are not computed by combining the lexical meaning of the surrounding words – which risks resulting in a “topicality” vector – but by modifying the type meaning of a word with the semantic expectations of syntactically related words, which can be thought of as selectional preferences. For example, in catch a ball, the token vector for ball is computed by combining the type vector of ball with a vector for th</context>
<context position="4307" citStr="Mitchell and Lapata, 2008" startWordPosition="671" endWordPosition="674">l, but through the distance of the token vector to other vectors. A natural question that arises is how vector-based models of token meaning can be evaluated. It is of course possible to apply them to a traditional WSD task. However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models’ central asset, namely gradedness. Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD (McCarthy and Navigli, 2007; Mitchell and Lapata, 2008). Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases. For example, in the following example, the first paraphrase is appropriate, but the second is not: (1) Google acquired YouTube � Google bought YouTube (2) How children acquire skills �� How children buy skills This task is graded in the sense that there is no disjoint set of labels from which exactly one is picked for each token; rather, the paraphrases form a set of labels of which a subset is appropriate for each word token, Proceedings of the EACL 2009 Worksh</context>
<context position="6095" citStr="Mitchell and Lapata (2008)" startWordPosition="960" endWordPosition="964">s (Mitchell and Lapata, 2008); and second, the original Lexical Substitution task by McCarthy and Navigli (2007). To avoid overfitting, we optimized our parameters on the first dataset and evaluated only the best model on the second dataset. However, given evidence for substantial inter-task differences, it is unclear to what extent these parameters are optimal beyond the Mitchell and Lapata dataset. This paper addresses this question with two experiments: Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a (high) n-th power, which is counterintuitive. Finally, we found subjects to be more informative in judging the appropriateness of paraphrases than objects. This appears to contradict work in theoretical syntax (Levin and Rappaport Hovav, 2005). To reassess the role of these parameters, we construct a controlled dataset of transitive instances fro</context>
<context position="14829" citStr="Mitchell and Lapata, 2008" startWordPosition="2440" endWordPosition="2443">the selectional preferences. For example, the subject selectional preferences of catch are identical to those of catch the ball, even though one would expect that the outfielder corresponds much better to the expectations of catch the ball than of just catch. 3 Experimental Setup The task that we are considering is paraphrase assessment in context. Given a predicate-argument pair and a paraphrase candidate, the models have to decide how appropriate the paraphrase is for the predicate-argument combination. This is the main task against which token vector models have been evaluated in the past (Mitchell and Lapata, 2008; Erk and Padó, 2008). In Experiment 1, we use manually created paraphrases. In Experiment 2, we replaces human-generated paraphrases with “pseudo-paraphrases”, contextually similar words that may not be completely appropriate as paraphrases in the given context, but can be collected automatically. Our parameter choices for SVS are as similar as possible to the second experiment of our earlier paper. Vector space. We use a dependency-based vector space that counts a target word and a context word ... comp-1 catch throw catch organise ... subj obj obj-1 subj-1 ... cold baseball drift ball mod .</context>
<context position="18871" citStr="Mitchell and Lapata (2008)" startWordPosition="3104" endWordPosition="3107"> subsequently focus on cosine similarity, which is length-invariant, vector addition can also be interpreted as centroid computation. Next, we test component-wise multiplication (mult). This operation is more difficult to interpret in terms of vector space, since it does not correspond to the standard inner or outer vector products. The most straightforward interpretation is to reinterpret the second vector as a diagonal matrix, i.e., as a linear transformation of the first vector. Large entries in the second vector increase the weight of the corresponding contexts; small entries decrease it. Mitchell and Lapata (2008) found this method to yield the best results. The third vector combination function we consider is component-wise minimum (min). This combination function results in a vector with high counts only for contexts which co-occur frequently with both input vectors and can thus be understood as an intersection between the two context sets. Since the entries of two vectors need to be on the same order to magnitude for this method to yield meaningful results, we normalize vectors before the combination for min. Assessing models of token meaning. Given a transitive verb v with subject a and direct obje</context>
<context position="26325" citStr="Mitchell and Lapata (2008)" startWordPosition="4330" endWordPosition="4333">s yielded by min (OOT=62.3), followed by add (OOT=61.7), while mult shows generally worse results (OOT=60.3). For both add and mult, using only the subject as context only is optimal. The overall best result, using min, is seen for both; however, the improvement over subj is very small. In the model mult-both-20, where target vectors were multiplied with two very large expectation vectors, almost all instances failed due to overflow errors. Discussion. Our results indicate that our parameter optimization strategy in Erk and Padó (2008) was in fact flawed. The parameters that were best for the Mitchell and Lapata (2008) data (mult, n = 20) are suboptimal for LEXSUB-PARA data.5 The good results for low val5We assume that our results hold for the Padó &amp; Erk (2008) lexical substitution dataset as well, due to its similar nature. Sentence Substitutes By asking people who work there, I have since determined that he didn’t. (# 2002) be employed 4; labour 1 Remember how hard your ancestors worked. (# 2005) toil 4; labour 3; task 1 � OOT =1/III i 61 ues of n indicate that good discrimination between valid and invalid paraphrases can be obtained by relatively small modifications of the target vector in the direction </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Padó</author>
<author>U Padó</author>
<author>K Erk</author>
</authors>
<title>Flexible, corpusbased modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL,</booktitle>
<pages>400--409</pages>
<contexts>
<context position="9436" citStr="Padó et al., 2007" startWordPosition="1488" endWordPosition="1491">lished (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accommodate information about semantic expectations, the SVS model extends the traditional representation of word meaning as a single vector by a set of vectors, each of which represents the word’s selectional preferences for each relation that the word can assume in its linguistic context. While we ultimately think of these relations as “properly semantic” in the sense of semantic roles, the instantiation of SVS we consider in this paper makes use of dependency relations as a level of representation that generalizes over a substantial amount of surface variati</context>
</contexts>
<marker>Padó, Padó, Erk, 2007</marker>
<rawString>S. Padó, U. Padó, and K. Erk. 2007. Flexible, corpusbased modelling of human plausibility judgements. In Proceedings of EMNLP/CoNLL, pages 400–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>H Dang</author>
<author>C Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Journal of Natural Language Engineering.</journal>
<note>To appear.</note>
<contexts>
<context position="1508" citStr="Palmer et al., 2007" startWordPosition="231" endWordPosition="234">an be computed for new occurrences. One prominent approach to this question is the dictionary-based model of token meaning: The different meanings of a word are a set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models hav</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>M. Palmer, H. Dang, and C. Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Journal of Natural Language Engineering. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="9083" citStr="Resnik, 1996" startWordPosition="1432" endWordPosition="1433"> of a word in context as guided by expectations about typical events. This move to include typical arguments and predicates into a model of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accommodate information about semantic expectations, the SVS model extends the traditional representation of word meaning as a single vector by a set of vectors, each of which represents the word’s selectional pref</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>P. Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="2744" citStr="Schütze, 1998" startWordPosition="416" endWordPosition="417">in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measure of (dis-)similarity between words in context that completely avoids reference to dictionary Sebastian Padó Department of Linguistics Stanford University pado@stanford.edu senses. However, there are still considerable theoretical and practical problems, even though there is a substantial body of work (Landauer and Dumais, 1997; Schütze, 1998; Kintsch, 2001; Mitchell and Lapata, 2008). In a recent paper (Erk and Padó, 2008), we have introduced the structured vector space (SVS) model which addresses this challenge. It yields one token vector per input word. Token vectors are not computed by combining the lexical meaning of the surrounding words – which risks resulting in a “topicality” vector – but by modifying the type meaning of a word with the semantic expectations of syntactically related words, which can be thought of as selectional preferences. For example, in catch a ball, the token vector for ball is computed by combining t</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>H. Schütze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Open-source corpora: Using the net to fish for linguistic data.</title>
<date>2006</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="22399" citStr="Sharoff, 2006" startWordPosition="3667" endWordPosition="3668">f transitive verbs, so the results are not comparable. In this experiment, we construct a new, more controlled dataset from the Lexical Substitution corpus to systematically assess the importance of the three main parameters: the relation used for disambiguation, the combination function, and the reweighting parameter. Construction of the LEXSUB-PARA dataset. The original Lexical Substitution corpus, constructed for the SemEval-1 lexical substitution task (McCarthy and Navigli, 2007), consists of 10 instances each of 200 target words in sentential contexts, drawn from a large internet corpus (Sharoff, 2006). Contextually appropriate paraphrases for each instance of each target word were elicited from up to 6 participants. Figure 3 shows two instances for the verb to work. The frequency distribution over paraphrases can be understood as a characterization of the target word’s meaning in each context. For the current paper, we constructed a new subset of LexSub we call LEXSUB-PARA by parsing LexSub with Minipar (Lin, 1993) and extracting all 177 sentences with transitive verbs that had overtly realized subjects and objects, regardless of voice. We did not manually verify the correctness of the par</context>
</contexts>
<marker>Sharoff, 2006</marker>
<rawString>Serge Sharoff. 2006. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, 11(4):435–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Preference semantics. In Formal Semantics of Natural Language.</title>
<date>1975</date>
<publisher>CUP.</publisher>
<contexts>
<context position="9028" citStr="Wilks, 1975" startWordPosition="1424" endWordPosition="1425">on behind the SVS model is to treat the interpretation of a word in context as guided by expectations about typical events. This move to include typical arguments and predicates into a model of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Representing lemma meaning. To accommodate information about semantic expectations, the SVS model extends the traditional representation of word meaning as a single vector by a set of vector</context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Y. Wilks. 1975. Preference semantics. In Formal Semantics of Natural Language. CUP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>