<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.849039">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 176-183, Lisbon, Portugal, 2000.
</note>
<title confidence="0.655906">
Learning from a Substructural Perspective
</title>
<note confidence="0.76942325">
Pieter Adriaans and Erik de Haas
Syllogic,
P.O. Box 2729, 3800GG Amersfoort, The Netherlands,
and
</note>
<affiliation confidence="0.948342">
University of Amsterdam, Fac. of Mathematics, Computer Science, Physics and Astronomy,
</affiliation>
<address confidence="0.742178">
Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands
</address>
<email confidence="0.990314">
pieter.adriaans@ps.net, erik@propersolution.n1
</email>
<sectionHeader confidence="0.982675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999619928571429">
In this paper we study learning from a logical
perspective. We show that there is a strong re-
lationship between a learning strategy, its for-
mal learning framework and its logical represen-
tational theory. This relationship enables one
to translate learnability results from one theory
to another. Moreover if we go from a classi-
cal logic theory to a substructural logic theory,
we can transform learnability results of logical
concepts to results for string languages. In this
paper we will demonstrate such a translation by
transforming the Valiant learnability result for
boolean concepts to a learnability result for a
class of string pattern languages.
</bodyText>
<sectionHeader confidence="0.996303" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982950819672">
There is a strong relation between a learn-
ing strategy, its formal learning framework and
its representational theory. Such a representa-
tional theory typically is (equivalent to) a logic.
As an example for this strong relationship as-
sume that the implication A —&gt; B is a given
fact, and you observe A; then you can deduce
B, which means that you can learn B from A
based on the underlying representational the-
ory. The learning strategy is very tightly con-
nected to its underlying logic. Continuing the
above example, suppose you observe –.B. In a
representational theory based on classical logic
you may deduce given the fact A —&gt; B.
In intuitionistic logic however, this deduction
is not valid. This example shows that the char-
acter of the representational theory is essential
for your learning strategy, in terms of what can
be learned from the facts and examples.
In the science of the representational theo-
ries, i.e. logic, it is a common approach to
connect different representational theories, and
transform results of one representational theory
to results in an other representational theory.
Interesting is now whether we can transform
learnability results of learning strategies within
one representational theory to others. Observe
that to get from a first order calculus to a string
calculus one needs to eliminate structural rules
from the calculus. Imagine now that we do the
same transformation to the learning strategies,
we would come up with a learning strategy for
the substructural string calculus starting from a
learning strategy for the full first order calculus.
The observation that learning categorial
grammars translates to the task of learning
derivations in a substructural logic theory moti-
vates a research program that investigates learn-
ing strategies from a logical point of view (Adri-
aans and de Haas, 1999). Many domains for
learning tasks can be embedded in a formal
learning framework based on a logical repre-
sentational theory. In Adriaans and de Haas
(1999) we presented two examples of substruc-
tural logics, that were suitable representational
theories for different learning tasks; The first
example was the Lambek calculus for learning
categorial grammars, the second example dealt
with a substructural logic that was designed to
study modern Object Oriented modeling lan-
guages like UML (OMG, 1997), (Fowler, 1997).
In the first case the representation theory is first
order logic without structural rules, the formal
learning theory from a logical point of view is
inductive substructural logic programming and
an example of a learning strategy in this frame-
work is EMILE, a learning algorithm that learns
categorial grammars (Adriaans, 1992).
In this paper we concentrate on the trans-
formation of classical logic to substructural
logic and show that Valiant&apos;s proof of PAC-
</bodyText>
<page confidence="0.997889">
176
</page>
<bodyText confidence="0.999857868421052">
learnability of boolean concepts can be trans-
formed to a PAC learnability proof for learning
a class of finite languages. We discuss the ex-
tension of this learnability approach to the full
range of substructural logics. Our strategy in
exploring the concept of learning is to look at
the logical structure of a learning algorithm, and
by this reveal the inner working of the learning
strategy.
In Valiant (1984) the principle of Probably
Approximately Correct learning (PAC learning)
was introduced. There it has been shown that
k-CNF (k-length Conjunctive Normal Form)
boolean concepts can be learned efficiently in
the model of PAC learning. For the proof
that shows that these boolean concepts can be
learned efficiently Valiant presents a learning al-
gorithm and shows by probabilistic arguments
that boolean concept can be PAC learned in
polynomial time. In this paper we investigate
the logical mechanism behind the learning al-
gorithm. By revealing the logical mechanism
behind this learning algorithm we are able to
study PAC learnability of various other logics in
the substructural landscape of first order propo-
sitional logic.
In this paper we will first briefly introduce
substructural logic in section 2. Consequently
we will reconstruct in section 3 Valiant&apos;s result
on learnability of boolean concepts in terms of
logic. Then in section 4 we will show that the
learnability result of Valiant for k-CNF boolean
concepts can be transformed to a learnability re-
sult for a grammar of string patterns denoted by
a substructural variant of the k-CNF formulas.
We will conclude this paper with a discussion
an indicate how this result could be extended
to learnability results for categorial grammars.
</bodyText>
<sectionHeader confidence="0.931043" genericHeader="method">
2 Substructural logic
</sectionHeader>
<bodyText confidence="0.99956552631579">
In Gentzen style sequential formalisms a sub-
structural logic shows itself by the absence of
(some of) the so-called structural rules. Exam-
ples of such logics are relevance logic (Dunn,
1986), linear logic (Girard, 1987) and BCK logic
(Grishin, 1974). Notable is the substructural
behavior of categorial logic, which in its proto-
type form is the Lambek calculus. Categorial
logics are motivated by its use as grammar for
natural languages. The absence of the struc-
tural rules degrades the abstraction of sets in
the semantic domain to strings, where elements
in a string have position and arity, while they
do not have that in a set. As we will see further
on in this paper the elimination of the struc-
tural rules in the learning context of the boolean
concepts will transform the learning framework
from sets of valuated variables to strings of val-
uated variables.
</bodyText>
<construct confidence="0.726966809523809">
Example 2.1 In a domain of sets the following
&apos;expressions&apos; are equivalent, while they are not
in the domain of strings:
a,a,b,a a,b,b
In a calculus with all the structural rules the fea-
tures &apos;position&apos; and &apos;arity&apos; are irrelevant in the
semantic domain, because aggregates that differ
in these features can be proved equivalent with
the structural rules. To see this observe that
the left side of the above equation can be trans-
formed to the right side by performing the fol-
lowing operation:
a, a, b, a
contract a, a in first two positions
to a
a, b, a
exchange b, a in last to positions to
a,b
a, a, b
contract again a, a in first two
positions to a
</construct>
<equation confidence="0.42560775">
a, b
weaken expression b in last position
to b,b
a,b,b
</equation>
<bodyText confidence="0.9998556">
In figure 2 we list the axiomatics of the first
order propositional sequent calculus&apos;, with the
axioms , the cut rule, rules for the connectives
and the structural rules for exchange, weakening
and contraction.
</bodyText>
<sectionHeader confidence="0.7054855" genericHeader="method">
3 PAC Boolean concept learning
revisited
</sectionHeader>
<bodyText confidence="0.9767172">
In this section we describe the principle of Prob-
ably Approximately Correct Learning (PAC
learning) of Boolean concepts. We will reveal
&apos;Note that in the variant we use here we have a special
case of the RA rule.
</bodyText>
<page confidence="0.966015">
177
</page>
<figure confidence="0.962642409090909">
representational
theory
formal learning
framework
learning strategy
First order
propositional
logic
Boolean
concepts
PAC learning
k-CNF
formulas
Substructural
propositional
logic
String
pattern
languages
PAC learning
k-CNF
patterns
</figure>
<figureCaption confidence="0.965862">
Figure 1: Relation between learning strategy, learning framework and representational theory
</figureCaption>
<figure confidence="0.998431666666667">
A, F&apos;, A,=/
(cut) r„ A&apos;, A
A, A ri B,
A AB,A
FA,A F
FAVB,A 1-1AVB,A
(Ax) A A
(LA) f,AAB A
(LV) r,A F, B A
(RA)
(Rv)
r,A A B,Fi =A
(Ex)
1P,B A A,1&apos; A
(Weak) r A
F,AA
A
(Contr)
</figure>
<figureCaption confidence="0.999689">
Figure 2: First order propositional sequent calculus
</figureCaption>
<bodyText confidence="0.971256157894737">
the logical deduction process behind the learn-
ing algorithm.
Consider the sample space for boolean con-
cepts. An example is a vector denoting the
truth (presence,l) or falsehood (absence,0) of
propositional variables. Such an example vec-
tor can be described by a formula consisting of
the conjunction of all propositional variables or
negations of propositional variables, depending
on the fact whether there is a 1 or a 0 in the
position of the propositional variable name in
the vector. A collection of vectors, i.e. a con-
cept, in its turn can be denoted by a formula
too, being the disjunction of all the formula&apos;s of
the vectors.
Example 3.1 Let universe U = {a, b} and let
concept f = {(0,1)}, then the following formula
exactly describes f:
Ab
</bodyText>
<page confidence="0.992083">
178
</page>
<bodyText confidence="0.9894024">
A little more extensive: Let uni-
verse U&apos; = {a, b, c} and let concept
= {(0, 0,0), (0,0, 1), (0, 1,1), (1,1, 1)1
Then the following formula exactly describes 1&apos;
(with a clear translation):
</bodyText>
<equation confidence="0.338552">
(rzAbAZ)V(dAbAc)V (TiAbAc)V AbAc)
</equation>
<bodyText confidence="0.8901021">
Note that these formulas are in Disjunctive nor-
mal form (DNF).
An interesting observation now is that the
learning algorithm of Valiant that learns k-CNF
formulas actually is trying to prove the equiv-
alence between a DNF formula and a k-CNF
formula.
Example 3.2 Let universe U = {a, b} and let
concept f {(0, 1)}, then the following sequent
should be &apos;learned&apos; by a 2-CNF learning algo-
</bodyText>
<equation confidence="0.867785">
rithm2 :
rtAb4# (aVb) A (r1V b) A(V6)
</equation>
<bodyText confidence="0.943595634146342">
A little more extensive: Let U&apos; =
{a, b, c} and let concept f
{(0,0, 0), (0,0, 1), (0, 1, 1), (1, 1,1)} Then
the following sequent should be &apos;learned&apos; by a
2-CNF learning algorithm:
(U,AbAZ)V(TiAbAc)V(dAbAc)V(aAbAc)
b) A (r/Vb)A (a V b)
The above observation says in logical terms
that the learning algorithm needs to implement
an inductive procedure to find this desired proof
and the concluding concept description (2-CNF
formula) from examples. In the search space for
this proof the learning algorithm can use the ax-
ioms and rules from the representational theory.
In the framework of boolean concept learning
this means that the learning algorithm may use
all the rules and axioms from the representa-
tional theory of classical propositional logic.
Example 3.3 Let U =-- {a, b} and let concept
f {(0,1)} and assume f can be represented
by a 2-CNF formula. to learn the 2-CNF de-
scription of concept f the learning algorithm
needs to find the proof for a sequent starting
2i.e. an algorithm that can learn 2-CNF boolean con-
cepts.
from the DNF formula TEA b to a 2-CNF for-
mula and vice versa (&lt;=&gt;) and to do so it may
use all the rules and axioms from the first or-
der propositional calculus including the struc-
tural rules. The proof for one side of such a
sequent is spelled out in figure 3.
In general an inductive logic programming al-
gorithm for the underlying representational the-
ory can do the job of learning the concept; i.e.
from the examples (DNF formulas) one can in-
duce possible sequents, targeting on a 2-CNF
sequent on the righthand side. The learning al-
gorithm we present here is more specific and
simply shows that an efficient algorithm for the
proof search exists.
The steps:
</bodyText>
<listItem confidence="0.980285153846154">
1. Form the collection G of all 2-CNF
clauses (p V q)
2. do / times
(a) pick an example al A •-• Aam
(b) form the collection of all
2-CNF clauses deducible from
al A ••• A am and intersect this
collection with G resulting in
a new G
Correctness proof (outline): By (Ax),
(RV), (Weak), (LA) and (Ex) we can proof
that for any conjunction (i.e. example vector)
al A • • A am we have for all 1 &lt; i &lt; m and
</listItem>
<bodyText confidence="0.5623936">
any b a clause of a 2-CNF in which ai occurs
with b , hence having all clauses deducible from
the vector proven individually enabling one to
form the collection of all clauses deducible from
a vector; i.e.
</bodyText>
<equation confidence="0.545643">
al A ••• A am ai V b
A••• A am b V ai
</equation>
<bodyText confidence="0.998142428571429">
By (RA) and (Contr) we can proof the conjunc-
tion of an arbitrary subset of all the clauses de-
ducible from the vector, in particular all those
clauses that happen to be common to all the
vectors for each individual vector we have seen
so far, hence proving the 2-CNF for every indi-
vidual vector; i.e.
</bodyText>
<page confidence="0.671139">
al A ••• A am = clausei A••• A clausep
179
</page>
<figure confidence="0.996307967741935">
(Ax)
_ (Rv)
Ti V b_ (Weak)
rz,biTtVb
_ (LA)
dAbdVb
b b (Ax)
(Rv)
(Weak)
b,rt-tiVb
(LA)
bAd-eriVb
(Ex)
(rt A b), (rt A b)
b b (Ax)
(Rv)
baVb
(Weak)
b,riaVb
(LA)
bAriaVb
(Ex)
iT,AbaVb
(RA)
V b) A (a V b)
(RA)
(a A b), (d A b),(aAb)(av -6) A (Ta V b) A (a V b) (c
°n
(Tx A b), (at A b) = (Ti V b) A (at v b) A (a V b)
(conto
(a A b) =(&amp;quot;Ci vb) A (at V b) A (a V b)
</figure>
<figureCaption confidence="0.999886">
Figure 3: Proof to be found for boolean concept learning
</figureCaption>
<equation confidence="0.739203666666667">
Now by (LV) we can prove the complete DNF
to 2-CNF sequent; i.e.
vectori V • • V vector/ = clausei A • • • A clausep
</equation>
<bodyText confidence="0.9999284">
It is easy to see that for the above algorithm
the same complexity analysis holds as for the
Valiant algorithm, because we have the same
progression in 1 steps, an the individual steps
have constant overhead.
</bodyText>
<sectionHeader confidence="0.974876" genericHeader="method">
4 PAC learning substructural logic
</sectionHeader>
<bodyText confidence="0.999846333333333">
When we transform the representational theory
of the boolean concept learning framework to a
substructural logic, we do the following:
</bodyText>
<listItem confidence="0.925378">
• eliminate the structural rules from the cal-
culus of first order propositional logic
</listItem>
<bodyText confidence="0.9973852">
When we want to translate the learnability re-
sult of k-CNF expressible boolean concepts we
need to do the same with the formal learning
framework and the strategy (algorithm). In
other words:
</bodyText>
<listItem confidence="0.886657714285714">
• the learning framework will contain con-
cepts that are sensitive to the features
which were before abstracted by the struc-
tural rules (&apos;position&apos; and &apos;arity&apos; )
• the learning algorithm from above is no
longer allowed to use the structural rules
in its inductive steps.
</listItem>
<bodyText confidence="0.9999424">
Below we present a learning algorithm for
the substructural logic representational theory.
Suppose again the universe U = {al, .. •
and the concept f is a CNF expressible concept
for vectors of length m.
</bodyText>
<listItem confidence="0.974939333333333">
1. start with m empty clauses (i.e. disjunction
of zero literals) clause&apos;, , clausem
2. do 1 times
(a) pick an example al A • • • A am
(b) for all 1 &lt; i &lt; m add a, to clause, if
a, does not occur in clause,.
</listItem>
<bodyText confidence="0.98362325">
Correctness proof (outline): By (Ax) and
(RV) we can proof for any a, that the sequent
a, = clause, for any clause, containing a, as one
of its disjuncts, especially for a clause, contain-
ing next to a, all the a&amp;quot;, from the former exam-
ples. Then by (RA) and (LA) we can position
all the vectors and clauses in the right-hand po-
sition; i.e.
</bodyText>
<equation confidence="0.37321">
al A • • • A am = clausei A • • A clausem
</equation>
<bodyText confidence="0.885395333333333">
Hence justifying the adding of the literal a, of
a vector in clause,. Now (LV) completes the
sequent for all the example vectors; i.e.
</bodyText>
<equation confidence="0.490436">
(al A • • • A am) V (di A • • • Aa) V...
clausei A • • A clausem
</equation>
<bodyText confidence="0.99911775">
For the algorithmic complexity in terms of
PAC learning, suppose we want present exam-
ples of concept f and that the algorithm learned
concept f in 1 steps. Concept f then de-
scribes a subset of concept f because on every
position in the CNF formula contains a sub-
set of the allowed variables; i.e. those vari-
ables that have encountered in the examples3.
</bodyText>
<footnote confidence="0.99797325">
3note that the CNF formula&apos;s can only describe par-
ticular sets of n-strings; namely those sets that are com-
plete for varying symbols locally on the different posi-
tions in the string.
</footnote>
<page confidence="0.993389">
180
</page>
<figure confidence="0.999415090909091">
b b (Ax) b b (Ax)
(Rv) (Rv)
brtVb baVb (RA)
(Ax) b,b (ZEV) A (a V b)
_ (Rv) (LA)
b Ab (TaV b) A (a V b)
(RA)
b Ab = caw -6) A (U, V b) A (a V b) (LA)
AbAb = (ci V-6) A (TO/ b) A (a V b)
(r/ArtAa)V(riAdAb)V(rtAbAa)V(rtAbAb)V(bAdAa)
V(bArtAb)V(bAbAa)\/(bAbAb)(rtVb)A(UVOA(aVb)
</figure>
<figureCaption confidence="0.999554">
Figure 4: Proof to be found for string pattern learning
</figureCaption>
<equation confidence="0.730660666666667">
(Lv)
Now let E = P(f I) be the error then again
6 = (1 - e)m is the confidence parameter as we
</equation>
<bodyText confidence="0.999341">
have m positions in the string. By the same
argument as for the Valiant algorithm we may
conclude that € and S decrease exponentially in
the number of examples 1, meaning that we have
an efficient polynomial time learning algorithm
for arbitrary c and 6.
</bodyText>
<sectionHeader confidence="0.998039" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999967111111111">
We showed that the learnability result of
Valiant for learning boolean concepts can be
transformed to a learnability result for pat-
tern languages by looking at the transforma-
tion of the underlying representational theories;
i.e. looking at the transformation from clas-
sical first order propositional logic (underlying
the boolean concepts) to substructural first or-
der propositional logic (underlying the pattern
languages). An interesting extension would be
to look at the substructural concept language
that includes implication (instead of the CNF
formula&apos;s only). A language that allows impli-
cation coincides with the full Lambek calculus,
and a learning algorithm and learnability result
for this framework amounts to results for all lan-
guages that can be described by context free
grammars. This is subject to future research.
</bodyText>
<sectionHeader confidence="0.975839" genericHeader="method">
References -
</sectionHeader>
<reference confidence="0.9660752">
P. Adriaans and E. de Haas. 1999. Grammar in-
duction as substructural inductive logic program-
ming. In Proceedings of the workshop on Learning
Language in Logic (LLL99), pages 117-126, Bled,
Slovenia, jun.
P. Adriaans. 1992. Language Learning from a Cate-
gorial Perspective. Ph.D. thesis, Universiteit van
Amsterdam. Academisch proefschrift.
J. Dunn. 1986. Relevance logic and entailment. In
F. Guenthner D. Gabbay, editor, Handbook of
Philosophical Logic III, pages 117-224. D. Reidel
Publishing Company.
M. Fowler. 1997. UML Distilled: Applying the Stan-
dard Object Modeling Language. Addison Wesley
Longman.
J.-Y. Girard. 1987. Linear logic. Theoretical Com-
puter Science, 50:1-102.
V.N. Grishin. 1974. A non-standard logic, and its
applications to set theory. In Studies in formal-
ized languages and nonclassical logics, pages 135-
171. Naulca.
Object Management Group OMG. 1997. Uml 1.1
specification. OMG documents ad970802-ad0809.
L.G. Valiant. 1984. Theory of the learnable. Comm.
of the ACM, 27:1134-1142.
</reference>
<page confidence="0.998714">
181
</page>
<sectionHeader confidence="0.837712" genericHeader="method">
Addendum: PAC learning
</sectionHeader>
<bodyText confidence="0.803316727272727">
The model of PAC learning arises from the work
of Valiant (Valiant, 1984). In this model of
learning it is assumed that we have a sample
space U* of vectors over an alphabet U, where
each position in a vector denotes the presence
(1) or absence (0) of a symbol a E U in the
sample vector. A concept f is a subset of vec-
tors from the sample space U.
Example 5.1 Let U = {a, b} be an alphabet,
then the following table describes the sample
space U* over U:
</bodyText>
<figure confidence="0.9907428">
a b
00
0 I
1 0
1 1
</figure>
<bodyText confidence="0.9963661875">
an example of a concept is f := {(0, 1)} and an
other example is g := {(0, 0), (0,1), (1, 1)1.
A concept can be learned by an algorithm by
giving this algorithm positive and/or negative
examples of the target concept to be learned.
An algorithm efficiently learns a concept if this
algorithm produces a description of this con-
cept in polynomial time. Informally a concept is
PAC (Probably Approximately Correct) learned
if the algorithm produces a description of a con-
cept that is by approximation the same as the
target concept from which examples are feeded
into the algorithm. A collection of concepts con-
stitutes to a concept class. A concept class can
be (PAC) learned if all the concepts in the con-
cept class can be (PAC) learned.
</bodyText>
<construct confidence="0.988760666666667">
Definition 5.2 (PAC Learnable) Let F be a
concept class, 8 (0 &lt; 8&lt; 1) a confidence param-
eter, E (0 &lt; E &lt; 1) an error parameter. A con-
cept class F is PAC learnable if for all targets
f E F and all probability distributions P on the
sample space U* the learning algorithm A out-
puts a concept g E F such that with probability
(1-8) it holds that we have a chance on an error
with P(f Ag) € (where fig = (f —g)U(g— f))
</construct>
<bodyText confidence="0.997489038461538">
We are especially interested in concept classes
that are defined by some formalism (language).
In other words a language can describe come
collection of concepts. An example of such
a language is the language of boolean formu-
las. A boolean formula describes a concept
that consists of all the vectors over the alpha-
bet of propositional variable names that satisfy
the formula. These concepts are called boolean
concepts.
Example 5.3 Let U := {a, b} be an alphabet of
propositional variable names. Then the formula
d A b describes the concept f := {(0,1)} of the
sample space U*; and the formula TtV b describes
the concept g := {(0,0), (0, 1), (1, 1)1.
In Valiant (1984) Valiant proves that the lan-
guage of k-CNF boolean formula&apos;s can be ef-
ficiently PAC learned. This means that for an
arbitrary k the concept class defined by the lan-
guage of k-CNF formula&apos;s can be PAC learned
by an algorithm in a polynomial number of
steps. Below we briefly recapitulate this result.
Definition 5.4 (Boolean concept languages)
Let U be a set of propositional variable names,
then the language L of boolean formulas is de-
fined by:
</bodyText>
<equation confidence="0.982035">
L := UIL V LIL A LIT,
</equation>
<bodyText confidence="0.7143455">
A literal is a propositional variable or a negation
of a propositional variable; i.e.
</bodyText>
<sectionHeader confidence="0.315689" genericHeader="method">
LIT := UIU
</sectionHeader>
<bodyText confidence="0.997886666666667">
A conjunction of a collection of formulas C is
a finite sequence of formulas from C connected
by the binary connective A; i.e.
</bodyText>
<equation confidence="0.916923">
CON(C) := CICON(C) A C
</equation>
<bodyText confidence="0.997775333333333">
A disjunction of a collection of formulas C is a
finite sequence of formulas from C connected by
the binary connective V; i.e.
</bodyText>
<equation confidence="0.859837">
DIS(C) := CIDIS(C) V C
</equation>
<bodyText confidence="0.5041992">
A formula is a CNF formula (Conjunctive Nor-
mal Form) if the formula is a conjunction of
disjunctions of literals. A formula is a k-CNF
formula if all the disjuctions in the formula are
of length k. A formula is a DNF formula (Dis-
junctive Normal Form) if the formula is a dis-
junction of conjunctions of literals.
Theorem 5.5 (Valiant (1984)) The classes of
k-CNF boolean concept languages are PAC
learnable in polynomial time.
</bodyText>
<page confidence="0.977245">
182
</page>
<figure confidence="0.991335333333333">
f A f&apos;
sample space
(set of all vectors)
</figure>
<figureCaption confidence="0.999867">
Figure 5: Valiant&apos;s proof for boolean concept learning
</figureCaption>
<bodyText confidence="0.995398111111111">
Proof (outline): Let U := {al, • ,an}(n E
Ai) be a alphabet and let concept f be a set
of vectors V := {vi, ,v,,}(m &lt; n) over U*,
which is equivalent to the k-CNF formula A.
Let P be an arbitrary probability distribution
over concept f such that E,4EfP(vi) = 1; i.e.
P(f) =1. Examples picked using the distribu-
tion based on P will be feeded into the following
learning algorithm:
</bodyText>
<listItem confidence="0.9818322">
• Form the collection G := ,cnkl
of all the clauses (disjunctions of
literals) of length k.
• do / times
- v := pick-an-example
</listItem>
<bodyText confidence="0.910128307692308">
- for each ci in G
* delete ci from G if v ci
Now suppose that the algorithm learned con-
cept f&apos; from 1 examples (1 taken from the algo-
rithm). The concept f&apos; now is a concept that
is a subset of f, because it may not have seen
enough examples to eliminate all the clauses
that are in conflict with f; i.e. there are still
clauses in f&apos; restricting this concept in the con-
junction of clauses, while it is disqualified by a
vector in f. What is the size of the number of
examples 1 we need to let fi approximate f with
a confidence 6 and error E. We have that
</bodyText>
<equation confidence="0.573822">
17(f)== 1
€=-P(fAr)
(the error is the chance of rejecting an
</equation>
<bodyText confidence="0.694375">
example in f because it is not in f&apos;)
</bodyText>
<equation confidence="0.745768">
6 = (1- Om
</equation>
<bodyText confidence="0.9620068">
(confidence is the chance of not making an
error after learning from 1 examples)
thus
1116 &lt;/1n(1 - E)
resulting in the following expression for 1:
lno
/ &lt;
ln(1 - E)
This means that the confidence parameter 6 and
the error parameter E are exponentially small
w.r.t. the number of examples 1 feeded into the
learning algorithm. This means that for an arbi-
trary 6 and E we can keep 1 polynomial because
the 6 and € decrease exponentially with respect
to 1.
</bodyText>
<page confidence="0.998761">
183
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.601039">
<note confidence="0.9683">of CoNLL-2000 and LLL-2000, 176-183, Lisbon, Portugal, 2000.</note>
<title confidence="0.958513">Learning from a Substructural Perspective Adriaans de</title>
<author confidence="0.955288">GG Amersfoort</author>
<author confidence="0.955288">The</author>
<affiliation confidence="0.987288">University of Amsterdam, Fac. of Mathematics, Computer Science, Physics and</affiliation>
<note confidence="0.7264255">Plantage Muidergracht 24, 1018TV Amsterdam, The pieter.adriaans@ps.net, erik@propersolution.n1</note>
<abstract confidence="0.999809466666666">In this paper we study learning from a logical perspective. We show that there is a strong relationship between a learning strategy, its formal learning framework and its logical representational theory. This relationship enables one to translate learnability results from one theory to another. Moreover if we go from a classical logic theory to a substructural logic theory, we can transform learnability results of logical concepts to results for string languages. In this paper we will demonstrate such a translation by transforming the Valiant learnability result for boolean concepts to a learnability result for a class of string pattern languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Adriaans</author>
<author>E de Haas</author>
</authors>
<title>Grammar induction as substructural inductive logic programming.</title>
<date>1999</date>
<booktitle>In Proceedings of the workshop on Learning Language in Logic (LLL99),</booktitle>
<pages>117--126</pages>
<location>Bled, Slovenia,</location>
<marker>Adriaans, de Haas, 1999</marker>
<rawString>P. Adriaans and E. de Haas. 1999. Grammar induction as substructural inductive logic programming. In Proceedings of the workshop on Learning Language in Logic (LLL99), pages 117-126, Bled, Slovenia, jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Adriaans</author>
</authors>
<title>Language Learning from a Categorial Perspective.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit van Amsterdam.</institution>
<note>Academisch proefschrift.</note>
<contexts>
<context position="3781" citStr="Adriaans, 1992" startWordPosition="584" endWordPosition="585">resentational theories for different learning tasks; The first example was the Lambek calculus for learning categorial grammars, the second example dealt with a substructural logic that was designed to study modern Object Oriented modeling languages like UML (OMG, 1997), (Fowler, 1997). In the first case the representation theory is first order logic without structural rules, the formal learning theory from a logical point of view is inductive substructural logic programming and an example of a learning strategy in this framework is EMILE, a learning algorithm that learns categorial grammars (Adriaans, 1992). In this paper we concentrate on the transformation of classical logic to substructural logic and show that Valiant&apos;s proof of PAC176 learnability of boolean concepts can be transformed to a PAC learnability proof for learning a class of finite languages. We discuss the extension of this learnability approach to the full range of substructural logics. Our strategy in exploring the concept of learning is to look at the logical structure of a learning algorithm, and by this reveal the inner working of the learning strategy. In Valiant (1984) the principle of Probably Approximately Correct learn</context>
</contexts>
<marker>Adriaans, 1992</marker>
<rawString>P. Adriaans. 1992. Language Learning from a Categorial Perspective. Ph.D. thesis, Universiteit van Amsterdam. Academisch proefschrift.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dunn</author>
</authors>
<title>Relevance logic and entailment.</title>
<date>1986</date>
<booktitle>Handbook of Philosophical Logic III,</booktitle>
<pages>117--224</pages>
<editor>In F. Guenthner D. Gabbay, editor,</editor>
<publisher>Publishing Company.</publisher>
<contexts>
<context position="5836" citStr="Dunn, 1986" startWordPosition="914" endWordPosition="915">cepts in terms of logic. Then in section 4 we will show that the learnability result of Valiant for k-CNF boolean concepts can be transformed to a learnability result for a grammar of string patterns denoted by a substructural variant of the k-CNF formulas. We will conclude this paper with a discussion an indicate how this result could be extended to learnability results for categorial grammars. 2 Substructural logic In Gentzen style sequential formalisms a substructural logic shows itself by the absence of (some of) the so-called structural rules. Examples of such logics are relevance logic (Dunn, 1986), linear logic (Girard, 1987) and BCK logic (Grishin, 1974). Notable is the substructural behavior of categorial logic, which in its prototype form is the Lambek calculus. Categorial logics are motivated by its use as grammar for natural languages. The absence of the structural rules degrades the abstraction of sets in the semantic domain to strings, where elements in a string have position and arity, while they do not have that in a set. As we will see further on in this paper the elimination of the structural rules in the learning context of the boolean concepts will transform the learning f</context>
</contexts>
<marker>Dunn, 1986</marker>
<rawString>J. Dunn. 1986. Relevance logic and entailment. In F. Guenthner D. Gabbay, editor, Handbook of Philosophical Logic III, pages 117-224. D. Reidel Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fowler</author>
</authors>
<title>UML Distilled: Applying the Standard Object Modeling Language.</title>
<date>1997</date>
<publisher>Addison Wesley Longman.</publisher>
<contexts>
<context position="3452" citStr="Fowler, 1997" startWordPosition="533" endWordPosition="534">am that investigates learning strategies from a logical point of view (Adriaans and de Haas, 1999). Many domains for learning tasks can be embedded in a formal learning framework based on a logical representational theory. In Adriaans and de Haas (1999) we presented two examples of substructural logics, that were suitable representational theories for different learning tasks; The first example was the Lambek calculus for learning categorial grammars, the second example dealt with a substructural logic that was designed to study modern Object Oriented modeling languages like UML (OMG, 1997), (Fowler, 1997). In the first case the representation theory is first order logic without structural rules, the formal learning theory from a logical point of view is inductive substructural logic programming and an example of a learning strategy in this framework is EMILE, a learning algorithm that learns categorial grammars (Adriaans, 1992). In this paper we concentrate on the transformation of classical logic to substructural logic and show that Valiant&apos;s proof of PAC176 learnability of boolean concepts can be transformed to a PAC learnability proof for learning a class of finite languages. We discuss the</context>
</contexts>
<marker>Fowler, 1997</marker>
<rawString>M. Fowler. 1997. UML Distilled: Applying the Standard Object Modeling Language. Addison Wesley Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-Y Girard</author>
</authors>
<title>Linear logic.</title>
<date>1987</date>
<journal>Theoretical Computer Science,</journal>
<pages>50--1</pages>
<contexts>
<context position="5865" citStr="Girard, 1987" startWordPosition="918" endWordPosition="919">en in section 4 we will show that the learnability result of Valiant for k-CNF boolean concepts can be transformed to a learnability result for a grammar of string patterns denoted by a substructural variant of the k-CNF formulas. We will conclude this paper with a discussion an indicate how this result could be extended to learnability results for categorial grammars. 2 Substructural logic In Gentzen style sequential formalisms a substructural logic shows itself by the absence of (some of) the so-called structural rules. Examples of such logics are relevance logic (Dunn, 1986), linear logic (Girard, 1987) and BCK logic (Grishin, 1974). Notable is the substructural behavior of categorial logic, which in its prototype form is the Lambek calculus. Categorial logics are motivated by its use as grammar for natural languages. The absence of the structural rules degrades the abstraction of sets in the semantic domain to strings, where elements in a string have position and arity, while they do not have that in a set. As we will see further on in this paper the elimination of the structural rules in the learning context of the boolean concepts will transform the learning framework from sets of valuate</context>
</contexts>
<marker>Girard, 1987</marker>
<rawString>J.-Y. Girard. 1987. Linear logic. Theoretical Computer Science, 50:1-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Grishin</author>
</authors>
<title>A non-standard logic, and its applications to set theory.</title>
<date>1974</date>
<booktitle>In Studies in formalized languages and nonclassical logics,</booktitle>
<pages>135--171</pages>
<publisher>Naulca.</publisher>
<contexts>
<context position="5895" citStr="Grishin, 1974" startWordPosition="923" endWordPosition="924">that the learnability result of Valiant for k-CNF boolean concepts can be transformed to a learnability result for a grammar of string patterns denoted by a substructural variant of the k-CNF formulas. We will conclude this paper with a discussion an indicate how this result could be extended to learnability results for categorial grammars. 2 Substructural logic In Gentzen style sequential formalisms a substructural logic shows itself by the absence of (some of) the so-called structural rules. Examples of such logics are relevance logic (Dunn, 1986), linear logic (Girard, 1987) and BCK logic (Grishin, 1974). Notable is the substructural behavior of categorial logic, which in its prototype form is the Lambek calculus. Categorial logics are motivated by its use as grammar for natural languages. The absence of the structural rules degrades the abstraction of sets in the semantic domain to strings, where elements in a string have position and arity, while they do not have that in a set. As we will see further on in this paper the elimination of the structural rules in the learning context of the boolean concepts will transform the learning framework from sets of valuated variables to strings of valu</context>
</contexts>
<marker>Grishin, 1974</marker>
<rawString>V.N. Grishin. 1974. A non-standard logic, and its applications to set theory. In Studies in formalized languages and nonclassical logics, pages 135-171. Naulca.</rawString>
</citation>
<citation valid="true">
<title>Group OMG.</title>
<date>1997</date>
<journal>Comm. of the ACM,</journal>
<booktitle>Uml 1.1 specification. OMG documents ad970802-ad0809. L.G. Valiant.</booktitle>
<pages>27--1134</pages>
<institution>Object Management</institution>
<marker>1997</marker>
<rawString>Object Management Group OMG. 1997. Uml 1.1 specification. OMG documents ad970802-ad0809. L.G. Valiant. 1984. Theory of the learnable. Comm. of the ACM, 27:1134-1142.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>