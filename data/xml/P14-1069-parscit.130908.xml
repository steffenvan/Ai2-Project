<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000296">
<title confidence="0.9664405">
Joint POS Tagging and Transition-based Constituent Parsing in Chinese
with Non-local Features
</title>
<author confidence="0.992554">
Zhiguo Wang
</author>
<affiliation confidence="0.97924">
Brandeis University
</affiliation>
<address confidence="0.722919">
Waltham, MA, USA
</address>
<email confidence="0.999222">
zgwang@brandeis.edu
</email>
<sectionHeader confidence="0.994808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906818181818">
We propose three improvements to ad-
dress the drawbacks of state-of-the-art
transition-based constituent parsers. First,
to resolve the error propagation problem
of the traditional pipeline approach, we
incorporate POS tagging into the syntac-
tic parsing process. Second, to allevi-
ate the negative influence of size differ-
ences among competing action sequences,
we align parser states during beam-search
decoding. Third, to enhance the pow-
er of parsing models, we enlarge the fea-
ture set with non-local features and semi-
supervised word cluster features. Exper-
imental results show that these modifica-
tions improve parsing performance signif-
icantly. Evaluated on the Chinese Tree-
Bank (CTB), our final performance reach-
es 86.3% (F1) when trained on CTB 5.1,
and 87.1% when trained on CTB 6.0, and
these results outperform all state-of-the-art
parsers.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985192">
Constituent parsing is one of the most fundamen-
tal tasks in Natural Language Processing (NLP). It
seeks to uncover the underlying recursive phrase
structure of sentences. Most of the state-of-the-
art parsers are based on the PCFG paradigm and
chart-based decoding algorithms (Collins, 1999;
Charniak, 2000; Petrov et al., 2006). Chart-based
parsers perform exhaustive search with dynam-
ic programming, which contributes to their high
accuracy, but they also suffer from higher run-
time complexity and can only exploit simple local
structural information.
Transition-based constituent parsing (Sagae and
Lavie, 2005; Wang et al., 2006; Zhang and Clark,
2009) is an attractive alternative. It utilizes a se-
</bodyText>
<note confidence="0.914474666666667">
Nianwen Xue
Brandeis University
Waltham, MA, USA
</note>
<email confidence="0.980018">
xuen@brandeis.edu
</email>
<bodyText confidence="0.999967731707317">
ries of deterministic shift-reduce decisions to con-
struct syntactic trees. Therefore, it runs in linear
time and can take advantage of arbitrarily complex
structural features from already constructed sub-
trees. The downside is that they only search a tiny
fraction of the whole space and are therefore com-
monly considered to be less accurate than chart-
based parsers. Recent studies (Zhu et al., 2013;
Zhang et al., 2013) show, however, that this ap-
proach can also achieve the state-of-the-art perfor-
mance with improved training procedures and the
use of additional source of information as features.
However, there is still room for improvemen-
t for these state-of-the-art transition-based con-
stituent parsers. First, POS tagging is typically
performed separately as a preliminary step, and
POS tagging errors will propagate to the parsing
process. This problem is especially severe for lan-
guages where the POS tagging accuracy is rela-
tively low, and this is the case for Chinese where
there are fewer contextual clues that can be used
to inform the tagging process and some of the
tagging decisions are actually influenced by the
syntactic structure of the sentence. This creates
a chicken and egg problem that needs to be ad-
dressed when designing a parsing model. Second,
due to the existence of unary rules in constituen-
t trees, competing candidate parses often have d-
ifferent number of actions, and this increases the
disambiguation difficulty for the parsing model.
Third, transition-based parsers have the freedom
to define arbitrarily complex structural features,
but this freedom has not fully been taken advan-
tage of and most of the present approaches only
use simple structural features.
In this paper, we address these drawbacks to
improve the transition-based constituent parsing
for Chinese. First, we integrate POS tagging in-
to the parsing process and jointly optimize these
two processes simultaneously. Because non-local
syntactic information is now available to POS tag
</bodyText>
<page confidence="0.982586">
733
</page>
<note confidence="0.831008">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999813433333334">
determination, the accuracy of POS tagging im-
proves, and this will in turn improve parsing ac-
curacy. Second, we propose a novel state align-
ment strategy to align candidate parses with dif-
ferent action sizes during beam-search decoding.
With this strategy, parser states and their unary
extensions are put into the same beam, therefore
the parsing model could decide whether or not
to use unary actions within local decision beam-
s. Third, we take into account two groups of
complex structural features that have not been
previously used in transition-based parsing: non-
local features (Charniak and Johnson, 2005) and
semi-supervised word cluster features (Koo et al.,
2008). With the help of the non-local features,
our transition-based parsing system outperform-
s all previous single systems in Chinese. After
integrating semi-supervised word cluster features,
the parsing accuracy is further improved to 86.3%
when trained on CTB 5.1 and 87.1% when trained
on CTB 6.0, and this is the best reported perfor-
mance for Chinese.
The remainder of this paper is organized as fol-
lows: Section 2 introduces the standard transition-
based constituent parsing approach. Section 3
describes our three improvements to standard
transition-based constituent parsing. We discuss
and analyze the experimental results in Section 4.
Section 5 discusses related work. Finally, we con-
clude this paper in Section 6.
</bodyText>
<sectionHeader confidence="0.951439" genericHeader="method">
2 Transition-based Constituent Parsing
</sectionHeader>
<bodyText confidence="0.999987">
This section describes the transition-based con-
stituent parsing model, which is the basis of Sec-
tion 3 and the baseline model in Section 4.
</bodyText>
<subsectionHeader confidence="0.989631">
2.1 Transition-based Constituent Parsing
Model
</subsectionHeader>
<bodyText confidence="0.998882230769231">
A transition-based constituent parsing model is a
quadruple C = (5, T, so, 5t), where 5 is a set of
parser states (sometimes called configurations), T
is a finite set of actions, so is an initialization func-
tion to map each input sentence into a unique ini-
tial state, and 5t ∈ 5 is a set of terminal states.
Each action t ∈ T is a transition function to tran-
sit a state into a new state. A parser state s ∈ 5 is
defined as a tuple s = (σ, β), where σ is a stack
which is maintained to hold partial subtrees that
are already constructed, and β is a queue which is
used for storing word-POS pairs that remain un-
processed. In particular, the initial state has an
</bodyText>
<figure confidence="0.952657315789474">
A0,2
c2,3
w0
w
F2,3
E0,2
c2,3
w
A0,2
D
b
sh,sh,rr-A,sh,rl-B
(a)
80,3
80,3
b
a0,1
w
Type Feature Templates
</figure>
<equation confidence="0.827424090909091">
p0tc, p0wc, p1tc, p1wc, p2tc
p2wc, p3tc, p3wc, q0wt, q1wt
q2wt, q3wt, p0lwc, p0rwc
p0uwc, p1lwc, p1rwc, p1uwc
bigrams p0wp1w, p0wp1c, p0cp1w, p0cp1c
p0wq0w, p0wq0t, p0cq0w, p0cq0t
q0wq1w, q0wq1t, q0tq1w, q0tq1t
p1wq0w, p1wq0t, p1cq0w, p1cq0t
p0cp1cp2c, p0wp1cp2c, p0cp1wq0t
trigrams p0cp1cp2w, p0cp1cq0t, p0wp1cq0t
p0cp1wq0t, p0cp1cq0w
</equation>
<bodyText confidence="0.949245333333333">
Table 1: Baseline features, where pi represents the
ith subtree in the stack σ and qi denotes the ith
item in the queue β. w refers to the head lexicon,
t refers to the head POS, and c refers to the con-
stituent label. pil and pir refer to the left and right
child for a binary subtree pi, and piu refers to the
child of a unary subtree pi.
process such trees, we employ binarization and
debinarization processes described in Zhang and
Clark (2009) to transform multi-branch trees into
binary-branch trees and restore the generated bi-
nary trees back to their original forms.
</bodyText>
<subsectionHeader confidence="0.999922">
2.2 Modeling, Training and Decoding
</subsectionHeader>
<bodyText confidence="0.997852">
To determine which action t E T should the parser
perform at a state s E 5, we use a linear model to
score each possible (s, t) combination:
</bodyText>
<equation confidence="0.9913535">
�score(s, t) = w� · 0(s, t) = wifi(s, t) (1)
i
</equation>
<bodyText confidence="0.999897818181818">
where 0(s, t) is the feature function used for map-
ping a state-action pair into a feature vector, and
w� is the weight vector. The score of a parser state
s is the sum of the scores for all state-action pairs
in the transition path from the initial state to the
current state. Table 1 lists the feature templates
used in our baseline parser, which is adopted from
Zhang and Clark (2009). To train the weight vec-
tor w, we employ the averaged perceptron algo-
rithm with early update (Collins and Roark, 2004).
We employ the beam search decoding algorith-
</bodyText>
<listItem confidence="0.881833217391304">
m (Zhang and Clark, 2009) to balance the trade-
off between accuracy and efficiency. Algorithm
1 gives details of the process. In the algorithm,
we maintain a beam (sometimes called agenda)
to keep k best states at each step. The first beam0
Algorithm 1 Beam-search Constituent Parsing
Input: A POS-tagged sentence, beam size k.
Output: A constituent parse tree.
1: beam0 +— {s0} &gt; initialization
2: i +— 0 &gt; step index
3: loop
4: P +— {} &gt; a priority queue
5: while beami is not empty do
6: s +— POP(beami)
7: for all possible t E T do
8: snew +— apply t to s
9: score snew with E.q (1)
10: insert snew into P
11: beami+1 +— k best states of P
12: sbest +— best state in beami+1
13: if sbest E 5t then
14: return sbest
15: i +— i + 1
</listItem>
<bodyText confidence="0.999478285714286">
is initialized with the initial state s0 (line 1). At
step i, each of the k states in beami is extended
by applying all possible actions (line 5-10). For
all newly generated states, only the k best states
are preserved for beami+1 (line 11). The decod-
ing process repeats until the highest scored state in
beami+1 reaches a terminal state (line 12-14).
</bodyText>
<sectionHeader confidence="0.948162" genericHeader="method">
3 Joint POS Tagging and Parsing with
Non-local Features
</sectionHeader>
<bodyText confidence="0.9998436">
To address the drawbacks of the standard
transition-based constituent parsing model (de-
scribed in Section 1), we propose a model to joint-
ly solve POS tagging and constituent parsing with
non-local features.
</bodyText>
<subsectionHeader confidence="0.997997">
3.1 Joint POS Tagging and Parsing
</subsectionHeader>
<bodyText confidence="0.999842384615385">
POS tagging is often taken as a preliminary step
for transition-based constituent parsing, therefore
the accuracy of POS tagging would greatly affec-
t parsing performance. In our experiment (de-
scribed in Section 4.2), parsing accuracy would
decrease by 8.5% in F1 in Chinese parsing when
using automatically generated POS tags instead of
gold-standard ones. To tackle this issue, we inte-
grate POS tagging into the transition-based con-
stituent parsing process and jointly optimize these
two processes simultaneously. Inspired from Ha-
tori et al. (2011), we modify the sh action by as-
signing a POS tag for the word when it is shifted:
</bodyText>
<listItem confidence="0.993843">
• SHIFT-X (sh-x): remove the first word from
</listItem>
<equation confidence="0.279264">
unigrams
</equation>
<page confidence="0.974055">
735
</page>
<bodyText confidence="0.999962884615384">
β, assign POS tag X to the word and push it
onto the top of σ.
With such an action, POS tagging becomes a nat-
ural part of transition-based parsing. However,
some feature templates in Table 1 become unavail-
able, because POS tags for the look-ahead words
are not specified yet under the joint framework.
For example, for the template q0wt , the POS tag
of the first word q0 in the queue β is required, but
it is not specified yet at the present state.
To overcome the lack of look-ahead POS tags,
we borrow the concept of delayed features origi-
nally developed for dependency parsing (Hatori et
al., 2011). Features that require look-ahead POS
tags are defined as delayed features. In these fea-
tures, look-ahead POS tags are taken as variables.
During parsing, delayed features are extracted and
passed from one state to the next state. When a
sh-x action is performed, the look-ahead POS
tag of some delayed features is specified, there-
fore these delayed features can be transformed in-
to normal features (by replacing variable with the
newly specified POS tag). The remaining delayed
features will be transformed similarly when their
look-ahead POS tags are specified during the fol-
lowing parsing steps.
</bodyText>
<subsectionHeader confidence="0.999539">
3.2 State Alignment
</subsectionHeader>
<bodyText confidence="0.984369807692308">
Assuming an input sentence contains n words, in
order to reach a terminal state, the initial state re-
quires n sh-x actions to consume all words in β,
and n − 1 rl/rr-x actions to construct a com-
plete parse tree by consuming all the subtrees in
σ. However, ru-x is a very special action. It on-
ly constructs a new unary node for the subtree on
top of σ, but does not consume any items in σ or
β. As a result, the number of ru-x actions varies
among terminal states for the same sentence. For
example, the parse tree in Figure 1a contains no
ru-x action, while the parse tree for the same in-
put sentence in Figure 1b contains four ru-x ac-
tions. This makes the lengths of complete action
sequences very different, and the parsing model
has to disambiguate among terminal states with
varying action sizes. Zhu et al. (2013) proposed a
padding method to align terminal states containing
different number of actions. The idea is to append
some IDLE actions to terminal states with shorter
action sequence, and make sure all terminal states
contain the same number of actions (including I-
DLE actions).
Algorithm 2 Beam-search with State Alignment
Input: A word-segmented sentence, beam size k.
Output: A constituent parse tree.
</bodyText>
<listItem confidence="0.981098470588235">
1: beam0 ← {s0} &gt; initialization
2: for i ← 0 to 2n − 1 do &gt; n is sentence length
3: P0 ← {}, P1 ← {} &gt; two priority queues
4: while beami is not empty do
5: s ← POP(beami)
6: for t ∈ {sh-x, rl-x, rr-x} do
7: snew ← apply t to s
8: score snew with E.q (1)
9: insert snew into P0
10: for all state s in P0 do
11: for all possible t ∈ {ru-x} do
12: snew ← apply t to s
13: score snew with E.q (1)
14: insert snew into P1
15: insert all states of P1 into P0
16: beami+1 ← k best states of P0
17: return the best state in beam2n−1
</listItem>
<bodyText confidence="0.999913689655172">
We propose a novel method to align states dur-
ing the parsing process instead of just aligning ter-
minal states like Zhu et al. (2013). We classify all
the actions into two groups according to whether
they consume items in σ or β. sh-x, rl-x, and
rr-x belong to consuming actions, and ru-x be-
longs to non-consuming action. Algorithm 2 gives
the details of our method. It is based on the beam
search decoding algorithm described in Algorith-
m 1. Different from Algorithm 1, Algorithm 2 is
guaranteed to perform 2n − 1 parsing steps for an
input sentence containing n words (line 2), and
divides each parsing step into two parsing phas-
es. In the first phase (line 4-9), each of the k s-
tates in beami is extended by consuming action-
s. In the second phase (line 10-14), each of the
newly generated states is further extended by non-
consuming actions. Then, all these states extend-
ed by both consuming and non-consuming action-
s are considered together (line 15), and only the
k highest-scored states are preserved for beami+1
(line 16). After these 2n − 1 parsing steps, the
highest scored state in beam2n−1 is returned as
the final result (line 17). Figure 2 shows the states
aligning process for the two trees in Figure 1. We
find that our new method aligns states with their
ru-x extensions in the same beam, therefore the
parsing model could make decisions on whether
using ru-x actions or not within local decision
</bodyText>
<page confidence="0.986255">
736
</page>
<figure confidence="0.972829">
beam0 beam1 beam2 beam3 beam4 beam5
</figure>
<figureCaption confidence="0.976564">
Figure 2: State alignment for the two trees in Fig-
ure 1, where so is the initial state, To and Ti are
terminal states corresponding to the two trees in
Figure 1. For clarity, we represent each state as a
rectangle with the label of top subtree in the stack
σ. We also denote sh-x with →, ru-x with T or
↓, rl-x with /, and rr-x with \.
</figureCaption>
<bodyText confidence="0.874863">
beams.
</bodyText>
<subsectionHeader confidence="0.998548">
3.3 Feature Extension
</subsectionHeader>
<bodyText confidence="0.999175081967213">
One advantage of transition-based constituen-
t parsing is that it is capable of incorporating ar-
bitrarily complex structural features from the al-
ready constructed subtrees in σ and unprocessed
words in β. However, all the feature templates
given in Table 1 are just some simple structural
features. To further improve the performance of
our transition-based constituent parser, we con-
sider two group of complex structural features:
non-local features (Charniak and Johnson, 2005;
Collins and Koo, 2005) and semi-supervised word
cluster features (Koo et al., 2008).
Table 2 lists all the non-local features we want
to use. These features have been proved very help-
ful for constituent parsing (Charniak and Johnson,
2005; Collins and Koo, 2005). But almost all pre-
vious work considered non-local features only in
parse reranking frameworks. Instead, we attempt
to extract non-local features from newly construct-
ed subtrees during the decoding process as they
become incrementally available and score newly
generated parser states with them. One difficul-
ty is that the subtrees built by our baseline pars-
er are binary trees (only the complete parse tree
is debinarized into its original multi-branch form),
but most of the non-local features need to be ex-
tracted from their original multi-branch forms. To
resolve this conflict, we integrate the debinariza-
tion process into the parsing process, i.e., when a
Table 2: Non-local features for constituent pars-
ing.
new subtree is constructed during parsing, we de-
binarize it immediately if it is not rooted with an
intermediate node 1. The other subtrees for sub-
sequent parsing steps will be built based on these
debinarized subtrees. After the modification, our
parser can extract non-local features incrementally
during the parsing process.
Semi-supervised word cluster features have
been successfully applied to many NLP tasks
(Miller et al., 2004; Koo et al., 2008; Zhu et
al., 2013). Here, we adopt such features for our
transition-based constituent parser. Given a large-
scale unlabeled corpus (word segmentation should
be performed), we employ the Brown cluster al-
gorithm (Liang, 2005) to cluster all words into a
binary tree. Within this binary tree, words ap-
pear as leaves, left branches are labeled with 0 and
right branches are labeled with 1. Each word can
be uniquely identified by its path from the root,
and represented as a bit-string. By using various
length of prefixes of the bit-string, we can produce
word clusters of different granularities (Miller et
al., 2004). Inspired from Koo et al. (2008), we
employ two types of word clusters: (1) taking 4
bit-string prefixes of word clusters as replacements
of POS tags, and (2) taking 8 bit-string prefixes as
replacements of words. Using these two types of
clusters, we construct semi-supervised word clus-
ter features by mimicking the template structure of
the original baseline features in Table 1.
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998247">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9992614">
We conducted experiments on the Penn Chinese
Treebank (CTB) version 5.1 (Xue et al., 2005):
Articles 001-270 and 400-1151 were used as the
training set, Articles 301-325 were used as the
development set, and Articles 271-300 were used
</bodyText>
<footnote confidence="0.434301">
1Intermediate nodes are produced by binarization process.
</footnote>
<figure confidence="0.999311108108108">
s
0
C0,1
a0,1
D1,2
b1,2
b1,2
A0,2
A0,2
E0,2
F2,3
c2,3
c2,3
80,3
80,3
T1
T0
CoLenPar
(Collins and Koo, 2005)
(Charniak and Johnson, 2005)
CoPar HeadTree
Rules
Bigrams
Grandparent Rules
Grandparent Bigrams
Lexical Bigrams
Two-level Rules
Two-level Bigrams
Trigrams
Head-Modifiers
RightBranch
Heavy
Neighbours
NGramTree
Heads
Wproj
Word
</figure>
<page confidence="0.987008">
737
</page>
<bodyText confidence="0.999896375">
as the test set. Standard corpus preparation step-
s were performed before our experiments: emp-
ty nodes and functional tags were removed, and
the unary chains were collapsed to single unary
rules as Harper and Huang (2011). To build word
clusters, we used the unlabeled Chinese Gigaword
(LDC2003T09) and conducted Chinese word seg-
mentation using a CRF-based segmenter.
We used EVALB 2 tool to evaluate parsing per-
formance. The metrics include labeled precision
(LP), labeled recall (LR), bracketing F1 and POS
tagging accuracy. We set the beam size k to 16,
which brings a good balance between efficiency
and accuracy. We tuned the optimal number of
iterations of perceptron training algorithm on the
development set.
</bodyText>
<subsectionHeader confidence="0.9780005">
4.2 Pipeline Approach vs Joint POS Tagging
and Parsing
</subsectionHeader>
<bodyText confidence="0.999964740740741">
In this subsection, we conducted some experi-
ments to illustrate the drawbacks of the pipeline
approach and the advantages of our joint approach.
We built three parsing systems: Pipeline-Gold
system is our baseline parser (described in Sec-
tion 2) taking gold-standard POS tags as input;
Pipeline system is our baseline parser taking as
input POS tags automatically assigned by Stan-
ford POS Tagger 3; and JointParsing system is
our joint POS tagging and transition-based pars-
ing system described in subsection 3.1. We trained
these three systems on the training set and evalu-
ated them on the development set. The second,
third and forth rows in Table 3 show the parsing
performances. We can see that the parsing F1 de-
creased by about 8.5 percentage points in F1 score
when using automatically assigned POS tags in-
stead of gold-standard ones, and this shows that
the pipeline approach is greatly affected by the
quality of its preliminary POS tagging step. Af-
ter integrating the POS tagging step into the pars-
ing process, our JointParsing system improved the
POS tagging accuracy to 94.8% and parsing F1
to 85.8%, which are significantly better than the
Pipeline system. Therefore, the joint parsing ap-
proach is much more effective for transition-based
constituent parsing.
</bodyText>
<subsectionHeader confidence="0.997747">
4.3 State Alignment Evaluation
</subsectionHeader>
<bodyText confidence="0.999746">
We built two new systems to verify the effective-
ness of our state alignment strategy proposed in
</bodyText>
<footnote confidence="0.9980545">
2http://nlp.cs.nyu.edu/evalb/
3http://nlp.stanford.edu/downloads/tagger.shtml
</footnote>
<table confidence="0.999872777777778">
System LP LR F1 POS
Pipeline-Gold 92.2 92.5 92.4 100
Pipeline 83.9 83.8 83.8 93.0
JointParsing 85.1 86.6 85.8 94.8
Padding 85.4 86.4 85.9 94.8
StateAlign 86.9 85.9 86.4 95.2
Nonlocal 88.0 86.5 87.2 95.3
Cluster 89.0 88.3 88.7 96.3
Nonlocal&amp;Cluster 89.4 88.7 89.1 96.2
</table>
<tableCaption confidence="0.9755475">
Table 3: Parsing performance on Chinese devel-
opment set.
</tableCaption>
<bodyText confidence="0.8951103125">
Subsection 3.2. The first system Padding extend-
s our JointParsing system by aligning terminal s-
tates with the padding strategy proposed in Zhu et
al. (2013), and the second system StateAlign ex-
tends the JointParsing system with our state align-
ment strategy. The fifth and sixth rows of Table 3
give the performances of these two systems. Com-
pared with the JointParsing system which does not
employ any alignment strategy, the Padding sys-
tem only achieved a slight improvement on pars-
ing F1 score, but no improvement on POS tag-
ging accuracy. In contrast, our StateAlign system
achieved an improvement of 0.6% on parsing F1 s-
core and 0.4% on POS tagging accuracy. All these
results show us that our state alignment strategy is
more helpful for beam-search decoding.
</bodyText>
<subsectionHeader confidence="0.998049">
4.4 Feature Extension Evaluation
</subsectionHeader>
<bodyText confidence="0.999995263157895">
In this subsection, we examined the usefulness
of the new non-local features and the semi-
supervised word cluster features described in Sub-
section 3.3. We built three new parsing system-
s based on the StateAlign system: Nonlocal sys-
tem extends the feature set of StateAlign system
with non-local features, Cluster system extends
the feature set with semi-supervised word cluster
features, and Nonlocal&amp;Cluster system extend the
feature set with both groups of features. Parsing
performances of the three systems are shown in
the last three rows of Table 3. Compared with the
StateAlign system which takes only the baseline
features, the non-local features improved parsing
F1 by 0.8%, while the semi-supervised word clus-
ter features result in an improvement of 2.3% in
parsing F1 and an 1.1% improvement on POS tag-
ging accuracy. When integrating both groups of
features, the final parsing F1 reaches 89.1%. Al-
</bodyText>
<page confidence="0.993215">
738
</page>
<table confidence="0.999655">
Type System LP LR F1 POS
Pipeline 80.0 80.3 80.1 94.0
JointParsing 82.4 83.0 82.7 95.1
Padding 82.7 83.6 83.2 95.1
Our Systems StateAlign 84.2 82.9 83.6 95.5
Nonlocal 85.6 84.2 84.9 95.9
Cluster 85.2 84.5 84.9 95.8
Nonlocal&amp;Cluster 86.6 85.9 86.3 96.0
Petrov and Klein (2007) 81.9 84.8 83.3 -
Single Systems Zhu et al. (2013) 82.1 84.3 83.2 -
Charniak and Johnson (2005)* 80.8 83.8 82.3 -
Reranking Systems Wang and Zong (2011) - - 85.7 -
Semi-supervised Systems Zhu et al. (2013) 84.4 86.8 85.6 -
</table>
<tableCaption confidence="0.999955">
Table 4: Parsing performance on Chinese test set. *Huang (2009) adapted the parse reranker to CTB5.
</tableCaption>
<bodyText confidence="0.990439">
l these results show that both the non-local fea-
tures and the semi-supervised features are helpful
for our transition-based constituent parser.
</bodyText>
<subsectionHeader confidence="0.998515">
4.5 Final Results on Test Set
</subsectionHeader>
<bodyText confidence="0.999965678571429">
In this subsection, we present the performances of
our systems on the CTB test set. The correspond-
ing results are listed in the top rows of Table 4.
We can see that all these systems maintain a simi-
lar relative relationship as they do on the develop-
ment set, which shows the stability of our systems.
To further illustrate the effectiveness of our
systems, we compare them with some state-of-
the-art systems. We group parsing systems into
three categories: single systems, reranking sys-
tems and semi-supervised systems. Our Pipeline,
JointParsing, Padding, StateAlign and Nonlocal
systems belong to the category of single system-
s, because they don’t utilize any extra process-
ing steps or resources. Our Cluster and Nonlo-
cal&amp;Cluster systems belong to semi-supervised
systems, because both of them have employed
semi-supervised word cluster features. The pars-
ing performances of state-of-the-art systems are
shown in the bottom rows of Table 4. We can see
that the final F1 of our Nonlocal system reached
84.9%, and it outperforms state-of-the-art single
systems by more than 1.6%. As far as we know,
this is the best result on the CTB test set acquired
by single systems. Our Nonlocal&amp;Cluster sys-
tem further improved the parsing F1 to 86.3%,
and it outperforms all reranking systems and semi-
supervised systems. To our knowledge, this is the
</bodyText>
<table confidence="0.970251666666667">
System F1
Huang and Harper (2009) 85.2
Nonlocal&amp;Cluster 87.1
</table>
<tableCaption confidence="0.999837">
Table 5: Parsing performance based on CTB 6.
</tableCaption>
<bodyText confidence="0.99527152631579">
best reported performance in Chinese parsing.
All previous experiments were conducted on
CTB 5. To check whether more labeled data can
further improve our parsing system, we evaluat-
ed our Nonlocal&amp;Cluster system on the Chinese
TreeBank version 6.0 (CTB6), which is a super
set of CTB5 and contains more annotated data.
We used the same development set and test set
as CTB5, and took all the remaining data as the
new training set. Table 5 shows the parsing per-
formances on CTB6. Our Nonlocal&amp;Cluster sys-
tem improved the final F1 to 87.1%, which is 1.9%
better than the state-of-the-art performance on CT-
B6 (Huang and Harper, 2009). Compared with it-
s performance on CTB5 (in Table 4), our Nonlo-
cal&amp;Cluster system also got 0.8% improvemen-
t. All these results show that our approach can
become more powerful when given more labeled
training data.
</bodyText>
<subsectionHeader confidence="0.978571">
4.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9962865">
To better understand the linguistic behavior of
our systems, we employed the berkeley-parser-
analyser tool 4 (Kummerfeld et al., 2013) to cat-
egorize the errors. Table 6 presents the average
</bodyText>
<footnote confidence="0.979686">
4http://code.google.com/p/berkeley-parser-analyser/
</footnote>
<page confidence="0.997242">
739
</page>
<tableCaption confidence="0.887473">
Table 7: POS tagging error patterns on Chinese test set. For each error pattern, the left hand side tag is
the gold-standard tag, and the right hand side is the wrongly assigned tag.
</tableCaption>
<table confidence="0.99809475">
NP 1-Word Mod. Verb Diff Clause Noun
System Unary Coord
Int. Span Attach Args Label Attach Edge
Worst 1.75 0.74 0.44 0.49 0.39 0.37 0.29 0.15 0.14
Pipeline
JointParsing
Padding
StateAlign
Nonlocal
Cluster
Nonlocal&amp;Cluster
Best 1.33 0.42 0.28 0.29 0.19 0.21 0.17 0.07 0.09
</table>
<tableCaption confidence="0.815627">
Table 6: Parse errors on Chinese test set. The shaded area of each bar indicates average number of that
error type per sentence, and the completely full bar indicates the number in the Worst row.
</tableCaption>
<table confidence="0.8218047">
System VV→NN NN→VV DEC→DEG JJ→NN NR→NN DEG→DEC NN→NR NN→JJ
Worst 0.26 0.18 0.15 0.09 0.08 0.07 0.06 0.05
Pipeline
JointParsing
Padding
StateAlign
Nonlocal
Cluster
Nonlocal&amp;Cluster
Best 0.14 0.10 0.03 0.07 0.05 0.03 0.03 0.02
</table>
<bodyText confidence="0.999765708333333">
number of errors for each error type by our pars-
ing systems. We can see that almost all the Worst
numbers are produced by the Pipeline system. The
JointParsing system reduced errors of all types
produced by the Pipeline system except for the
coordination error type (Coord). The StateAlign
system corrected a lot of the NP-internal errors
(NP Int.). The Nonlocal system and the Cluster
system produced similar numbers of errors for al-
l error types. The Nonlocal&amp;Cluster system pro-
duced the Best numbers for all the error types. NP-
internal errors are still the most frequent error type
in our parsing systems.
Table 7 presents the statistics of frequent POS
tagging error patterns. We can see that JointPars-
ing system disambiguates {VV, NN} and {DEC,
DEG} better than Pipeline system, but cannot deal
with the NN→JJ pattern very well. StateAlign
system got better results in most of the patterns,
but cannot disambiguate {NR, NN} well. Non-
local&amp;Cluster system got the best results in dis-
ambiguating the most ambiguous POS tag pairs of
{VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, N-
R}.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999989541666667">
Joint POS tagging with parsing is not a new idea.
In PCFG-based parsing (Collins, 1999; Charniak,
2000; Petrov et al., 2006), POS tagging is consid-
ered as a natural step of parsing by employing lex-
ical rules. For transition-based parsing, Hatori et
al. (2011) proposed to integrate POS tagging with
dependency parsing. Our joint approach can be
seen as an adaption of Hatori et al. (2011)’s ap-
proach for constituent parsing. Zhang et al. (2013)
proposed a transition-based constituent parser to
process an input sentence from the character level.
However, manual annotation of the word-internal
structures need to be added to the original Tree-
bank in order to train such a parser.
Non-local features have been successfully used
for constituent parsing (Charniak and Johnson,
2005; Collins and Koo, 2005; Huang, 2008).
However, almost all of the previous work use non-
local features at the parse reranking stage. The
reason is that the single-stage chart-based parser
cannot use non-local structural features. In con-
trast, the transition-based parser can use arbitrari-
ly complex structural features. Therefore, we can
concisely utilize non-local features in a single-
</bodyText>
<page confidence="0.982914">
740
</page>
<bodyText confidence="0.955871">
stage parsing system.
</bodyText>
<sectionHeader confidence="0.997213" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999819142857143">
In this paper, we proposed three improvements to
transition-based constituent parsing for Chinese.
First, we incorporated POS tagging into transition-
based constituent parsing to resolve the error prop-
agation problem of the pipeline approach. Second,
we proposed a state alignment strategy to align
competing decision sequences that have different
number of actions. Finally, we enhanced our pars-
ing model by enlarging the feature set with non-
local features and semi-supervised word cluster
features. Experimental results show that all these
methods improved the parsing performance sub-
stantially, and the final performance of our parsing
system outperformed all state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999483714285714">
We thank three anonymous reviewers for their
cogent comments. This work is funded by the
DAPRA via contract HR0011-11-C-0145 entitled
&amp;quot;Linguistic Resources for Multilingual Process-
ing&amp;quot;. All opinions expressed here are those of the
authors and do not necessarily reflect the views of
DARPA.
</bodyText>
<sectionHeader confidence="0.997155" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998358538461538">
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173–180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132–139. Asso-
ciation for Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25–70.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.
Michael Collins. 1999. HEAD-DRIVEN STATISTI-
CAL MODELS FOR NATURAL LANGUAGE PARS-
ING. Ph.D. thesis, University of Pennsylvania.
Mary Harper and Zhongqiang Huang. 2011. Chinese
statistical parsing. Handbook of Natural Language
Processing and Machine Translation.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 1216–1224, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations
across languages. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 832–841.
Association for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586–
594.
Ling-Ya Huang. 2009. Improve chinese parsing with
max-ent reranking parser. Master Project Report,
Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595–603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 98–103, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337–342. Citeseer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages
404–411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computation-
al Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 433–
440. Association for Computational Linguistics.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125–132. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.970095">
741
</page>
<reference confidence="0.999790909090909">
Zhiguo Wang and Chengqing Zong. 2011. Parse r-
eranking based on higher-order lexical dependen-
cies. In IJCNLP, pages 1251–1259.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for chi-
nese. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computation-
al Linguistics, pages 425–432. Association for Com-
putational Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207–238.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162–171. Association for Computational Linguistic-
s.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 125–134, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434–443, Sofia, Bulgaria, August. Association for
Computational Linguistics.
</reference>
<page confidence="0.997282">
742
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.161279">
<title confidence="0.997522">Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features</title>
<author confidence="0.528873">Zhiguo</author>
<affiliation confidence="0.416112">Brandeis Waltham, MA,</affiliation>
<email confidence="0.999113">zgwang@brandeis.edu</email>
<abstract confidence="0.994403260869565">We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers. First, to resolve the error propagation problem of the traditional pipeline approach, we incorporate POS tagging into the syntactic parsing process. Second, to alleviate the negative influence of size differences among competing action sequences, we align parser states during beam-search decoding. Third, to enhance the power of parsing models, we enlarge the feature set with non-local features and semisupervised word cluster features. Experimental results show that these modifications improve parsing performance significantly. Evaluated on the Chinese Tree- Bank (CTB), our final performance reaches 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4598" citStr="Charniak and Johnson, 2005" startWordPosition="696" endWordPosition="699">nal Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into the same beam, therefore the parsing model could decide whether or not to use unary actions within local decision beams. Third, we take into account two groups of complex structural features that have not been previously used in transition-based parsing: nonlocal features (Charniak and Johnson, 2005) and semi-supervised word cluster features (Koo et al., 2008). With the help of the non-local features, our transition-based parsing system outperforms all previous single systems in Chinese. After integrating semi-supervised word cluster features, the parsing accuracy is further improved to 86.3% when trained on CTB 5.1 and 87.1% when trained on CTB 6.0, and this is the best reported performance for Chinese. The remainder of this paper is organized as follows: Section 2 introduces the standard transitionbased constituent parsing approach. Section 3 describes our three improvements to standard</context>
<context position="15388" citStr="Charniak and Johnson, 2005" startWordPosition="2618" endWordPosition="2621">th the label of top subtree in the stack σ. We also denote sh-x with →, ru-x with T or ↓, rl-x with /, and rr-x with \. beams. 3.3 Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees bui</context>
<context position="18286" citStr="Charniak and Johnson, 2005" startWordPosition="3084" endWordPosition="3087">ese two types of clusters, we construct semi-supervised word cluster features by mimicking the template structure of the original baseline features in Table 1. 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005): Articles 001-270 and 400-1151 were used as the training set, Articles 301-325 were used as the development set, and Articles 271-300 were used 1Intermediate nodes are produced by binarization process. s 0 C0,1 a0,1 D1,2 b1,2 b1,2 A0,2 A0,2 E0,2 F2,3 c2,3 c2,3 80,3 80,3 T1 T0 CoLenPar (Collins and Koo, 2005) (Charniak and Johnson, 2005) CoPar HeadTree Rules Bigrams Grandparent Rules Grandparent Bigrams Lexical Bigrams Two-level Rules Two-level Bigrams Trigrams Head-Modifiers RightBranch Heavy Neighbours NGramTree Heads Wproj Word 737 as the test set. Standard corpus preparation steps were performed before our experiments: empty nodes and functional tags were removed, and the unary chains were collapsed to single unary rules as Harper and Huang (2011). To build word clusters, we used the unlabeled Chinese Gigaword (LDC2003T09) and conducted Chinese word segmentation using a CRF-based segmenter. We used EVALB 2 tool to evaluat</context>
<context position="23139" citStr="Charniak and Johnson (2005)" startWordPosition="3860" endWordPosition="3863">improved parsing F1 by 0.8%, while the semi-supervised word cluster features result in an improvement of 2.3% in parsing F1 and an 1.1% improvement on POS tagging accuracy. When integrating both groups of features, the final parsing F1 reaches 89.1%. Al738 Type System LP LR F1 POS Pipeline 80.0 80.3 80.1 94.0 JointParsing 82.4 83.0 82.7 95.1 Padding 82.7 83.6 83.2 95.1 Our Systems StateAlign 84.2 82.9 83.6 95.5 Nonlocal 85.6 84.2 84.9 95.9 Cluster 85.2 84.5 84.9 95.8 Nonlocal&amp;Cluster 86.6 85.9 86.3 96.0 Petrov and Klein (2007) 81.9 84.8 83.3 - Single Systems Zhu et al. (2013) 82.1 84.3 83.2 - Charniak and Johnson (2005)* 80.8 83.8 82.3 - Reranking Systems Wang and Zong (2011) - - 85.7 - Semi-supervised Systems Zhu et al. (2013) 84.4 86.8 85.6 - Table 4: Parsing performance on Chinese test set. *Huang (2009) adapted the parse reranker to CTB5. l these results show that both the non-local features and the semi-supervised features are helpful for our transition-based constituent parser. 4.5 Final Results on Test Set In this subsection, we present the performances of our systems on the CTB test set. The corresponding results are listed in the top rows of Table 4. We can see that all these systems maintain a simi</context>
<context position="28843" citStr="Charniak and Johnson, 2005" startWordPosition="4798" endWordPosition="4801">s a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based parser can use arbitrarily complex structural features. Therefore, we can concisely utilize non-local features in a single740 stage parsing system. 6 Conclusion In this paper, we proposed three improvements to transition-based constituent parsing for Chinese. First, we incorporated POS tagging into transitionbased constituent parsing t</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1338" citStr="Charniak, 2000" startWordPosition="195" endWordPosition="196">Experimental results show that these modifications improve parsing performance significantly. Evaluated on the Chinese TreeBank (CTB), our final performance reaches 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers. 1 Introduction Constituent parsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of a</context>
<context position="28166" citStr="Charniak, 2000" startWordPosition="4692" endWordPosition="4693">pe in our parsing systems. Table 7 presents the statistics of frequent POS tagging error patterns. We can see that JointParsing system disambiguates {VV, NN} and {DEC, DEG} better than Pipeline system, but cannot deal with the NN→JJ pattern very well. StateAlign system got better results in most of the patterns, but cannot disambiguate {NR, NN} well. Nonlocal&amp;Cluster system got the best results in disambiguating the most ambiguous POS tag pairs of {VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, NR}. 5 Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features ha</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="15412" citStr="Collins and Koo, 2005" startWordPosition="2622" endWordPosition="2625">in the stack σ. We also denote sh-x with →, ru-x with T or ↓, rl-x with /, and rr-x with \. beams. 3.3 Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees built by our baseline parse</context>
<context position="18257" citStr="Collins and Koo, 2005" startWordPosition="3080" endWordPosition="3083">ments of words. Using these two types of clusters, we construct semi-supervised word cluster features by mimicking the template structure of the original baseline features in Table 1. 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005): Articles 001-270 and 400-1151 were used as the training set, Articles 301-325 were used as the development set, and Articles 271-300 were used 1Intermediate nodes are produced by binarization process. s 0 C0,1 a0,1 D1,2 b1,2 b1,2 A0,2 A0,2 E0,2 F2,3 c2,3 c2,3 80,3 80,3 T1 T0 CoLenPar (Collins and Koo, 2005) (Charniak and Johnson, 2005) CoPar HeadTree Rules Bigrams Grandparent Rules Grandparent Bigrams Lexical Bigrams Two-level Rules Two-level Bigrams Trigrams Head-Modifiers RightBranch Heavy Neighbours NGramTree Heads Wproj Word 737 as the test set. Standard corpus preparation steps were performed before our experiments: empty nodes and functional tags were removed, and the unary chains were collapsed to single unary rules as Harper and Huang (2011). To build word clusters, we used the unlabeled Chinese Gigaword (LDC2003T09) and conducted Chinese word segmentation using a CRF-based segmenter. We</context>
<context position="28866" citStr="Collins and Koo, 2005" startWordPosition="4802" endWordPosition="4805">by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based parser can use arbitrarily complex structural features. Therefore, we can concisely utilize non-local features in a single740 stage parsing system. 6 Conclusion In this paper, we proposed three improvements to transition-based constituent parsing for Chinese. First, we incorporated POS tagging into transitionbased constituent parsing to resolve the error pro</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="8010" citStr="Collins and Roark, 2004" startWordPosition="1287" endWordPosition="1290">s E 5, we use a linear model to score each possible (s, t) combination: �score(s, t) = w� · 0(s, t) = wifi(s, t) (1) i where 0(s, t) is the feature function used for mapping a state-action pair into a feature vector, and w� is the weight vector. The score of a parser state s is the sum of the scores for all state-action pairs in the transition path from the initial state to the current state. Table 1 lists the feature templates used in our baseline parser, which is adopted from Zhang and Clark (2009). To train the weight vector w, we employ the averaged perceptron algorithm with early update (Collins and Roark, 2004). We employ the beam search decoding algorithm (Zhang and Clark, 2009) to balance the tradeoff between accuracy and efficiency. Algorithm 1 gives details of the process. In the algorithm, we maintain a beam (sometimes called agenda) to keep k best states at each step. The first beam0 Algorithm 1 Beam-search Constituent Parsing Input: A POS-tagged sentence, beam size k. Output: A constituent parse tree. 1: beam0 +— {s0} &gt; initialization 2: i +— 0 &gt; step index 3: loop 4: P +— {} &gt; a priority queue 5: while beami is not empty do 6: s +— POP(beami) 7: for all possible t E T do 8: snew +— apply t t</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<date>1999</date>
<tech>HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING. Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1322" citStr="Collins, 1999" startWordPosition="193" endWordPosition="194">ster features. Experimental results show that these modifications improve parsing performance significantly. Evaluated on the Chinese TreeBank (CTB), our final performance reaches 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers. 1 Introduction Constituent parsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can tak</context>
<context position="28150" citStr="Collins, 1999" startWordPosition="4690" endWordPosition="4691">equent error type in our parsing systems. Table 7 presents the statistics of frequent POS tagging error patterns. We can see that JointParsing system disambiguates {VV, NN} and {DEC, DEG} better than Pipeline system, but cannot deal with the NN→JJ pattern very well. StateAlign system got better results in most of the patterns, but cannot disambiguate {NR, NN} well. Nonlocal&amp;Cluster system got the best results in disambiguating the most ambiguous POS tag pairs of {VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, NR}. 5 Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-l</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Harper</author>
<author>Zhongqiang Huang</author>
</authors>
<title>Chinese statistical parsing.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation.</booktitle>
<contexts>
<context position="18708" citStr="Harper and Huang (2011)" startWordPosition="3145" endWordPosition="3148">e used 1Intermediate nodes are produced by binarization process. s 0 C0,1 a0,1 D1,2 b1,2 b1,2 A0,2 A0,2 E0,2 F2,3 c2,3 c2,3 80,3 80,3 T1 T0 CoLenPar (Collins and Koo, 2005) (Charniak and Johnson, 2005) CoPar HeadTree Rules Bigrams Grandparent Rules Grandparent Bigrams Lexical Bigrams Two-level Rules Two-level Bigrams Trigrams Head-Modifiers RightBranch Heavy Neighbours NGramTree Heads Wproj Word 737 as the test set. Standard corpus preparation steps were performed before our experiments: empty nodes and functional tags were removed, and the unary chains were collapsed to single unary rules as Harper and Huang (2011). To build word clusters, we used the unlabeled Chinese Gigaword (LDC2003T09) and conducted Chinese word segmentation using a CRF-based segmenter. We used EVALB 2 tool to evaluate parsing performance. The metrics include labeled precision (LP), labeled recall (LR), bracketing F1 and POS tagging accuracy. We set the beam size k to 16, which brings a good balance between efficiency and accuracy. We tuned the optimal number of iterations of perceptron training algorithm on the development set. 4.2 Pipeline Approach vs Joint POS Tagging and Parsing In this subsection, we conducted some experiments</context>
</contexts>
<marker>Harper, Huang, 2011</marker>
<rawString>Mary Harper and Zhongqiang Huang. 2011. Chinese statistical parsing. Handbook of Natural Language Processing and Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint pos tagging and dependency parsing in chinese.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1216--1224</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="9986" citStr="Hatori et al. (2011)" startWordPosition="1635" endWordPosition="1639">ent parsing with non-local features. 3.1 Joint POS Tagging and Parsing POS tagging is often taken as a preliminary step for transition-based constituent parsing, therefore the accuracy of POS tagging would greatly affect parsing performance. In our experiment (described in Section 4.2), parsing accuracy would decrease by 8.5% in F1 in Chinese parsing when using automatically generated POS tags instead of gold-standard ones. To tackle this issue, we integrate POS tagging into the transition-based constituent parsing process and jointly optimize these two processes simultaneously. Inspired from Hatori et al. (2011), we modify the sh action by assigning a POS tag for the word when it is shifted: • SHIFT-X (sh-x): remove the first word from unigrams 735 β, assign POS tag X to the word and push it onto the top of σ. With such an action, POS tagging becomes a natural part of transition-based parsing. However, some feature templates in Table 1 become unavailable, because POS tags for the look-ahead words are not specified yet under the joint framework. For example, for the template q0wt , the POS tag of the first word q0 in the queue β is required, but it is not specified yet at the present state. To overcom</context>
<context position="28323" citStr="Hatori et al. (2011)" startWordPosition="4717" endWordPosition="4720">V, NN} and {DEC, DEG} better than Pipeline system, but cannot deal with the NN→JJ pattern very well. StateAlign system got better results in most of the patterns, but cannot disambiguate {NR, NN} well. Nonlocal&amp;Cluster system got the best results in disambiguating the most ambiguous POS tag pairs of {VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, NR}. 5 Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work </context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2011</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental joint pos tagging and dependency parsing in chinese. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1216–1224, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining pcfg grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>832--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24913" citStr="Huang and Harper (2009)" startWordPosition="4152" endWordPosition="4155">semi-supervised systems, because both of them have employed semi-supervised word cluster features. The parsing performances of state-of-the-art systems are shown in the bottom rows of Table 4. We can see that the final F1 of our Nonlocal system reached 84.9%, and it outperforms state-of-the-art single systems by more than 1.6%. As far as we know, this is the best result on the CTB test set acquired by single systems. Our Nonlocal&amp;Cluster system further improved the parsing F1 to 86.3%, and it outperforms all reranking systems and semisupervised systems. To our knowledge, this is the System F1 Huang and Harper (2009) 85.2 Nonlocal&amp;Cluster 87.1 Table 5: Parsing performance based on CTB 6. best reported performance in Chinese parsing. All previous experiments were conducted on CTB 5. To check whether more labeled data can further improve our parsing system, we evaluated our Nonlocal&amp;Cluster system on the Chinese TreeBank version 6.0 (CTB6), which is a super set of CTB5 and contains more annotated data. We used the same development set and test set as CTB5, and took all the remaining data as the new training set. Table 5 shows the parsing performances on CTB6. Our Nonlocal&amp;Cluster system improved the final F</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining pcfg grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 832–841. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="28880" citStr="Huang, 2008" startWordPosition="4806" endWordPosition="4807">les. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based parser can use arbitrarily complex structural features. Therefore, we can concisely utilize non-local features in a single740 stage parsing system. 6 Conclusion In this paper, we proposed three improvements to transition-based constituent parsing for Chinese. First, we incorporated POS tagging into transitionbased constituent parsing to resolve the error propagation probl</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL, pages 586– 594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling-Ya Huang</author>
</authors>
<title>Improve chinese parsing with max-ent reranking parser.</title>
<date>2009</date>
<tech>Master Project Report,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="23330" citStr="Huang (2009)" startWordPosition="3896" endWordPosition="3897">eatures, the final parsing F1 reaches 89.1%. Al738 Type System LP LR F1 POS Pipeline 80.0 80.3 80.1 94.0 JointParsing 82.4 83.0 82.7 95.1 Padding 82.7 83.6 83.2 95.1 Our Systems StateAlign 84.2 82.9 83.6 95.5 Nonlocal 85.6 84.2 84.9 95.9 Cluster 85.2 84.5 84.9 95.8 Nonlocal&amp;Cluster 86.6 85.9 86.3 96.0 Petrov and Klein (2007) 81.9 84.8 83.3 - Single Systems Zhu et al. (2013) 82.1 84.3 83.2 - Charniak and Johnson (2005)* 80.8 83.8 82.3 - Reranking Systems Wang and Zong (2011) - - 85.7 - Semi-supervised Systems Zhu et al. (2013) 84.4 86.8 85.6 - Table 4: Parsing performance on Chinese test set. *Huang (2009) adapted the parse reranker to CTB5. l these results show that both the non-local features and the semi-supervised features are helpful for our transition-based constituent parser. 4.5 Final Results on Test Set In this subsection, we present the performances of our systems on the CTB test set. The corresponding results are listed in the top rows of Table 4. We can see that all these systems maintain a similar relative relationship as they do on the development set, which shows the stability of our systems. To further illustrate the effectiveness of our systems, we compare them with some state-</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Ling-Ya Huang. 2009. Improve chinese parsing with max-ent reranking parser. Master Project Report, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4659" citStr="Koo et al., 2008" startWordPosition="705" endWordPosition="708">d this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into the same beam, therefore the parsing model could decide whether or not to use unary actions within local decision beams. Third, we take into account two groups of complex structural features that have not been previously used in transition-based parsing: nonlocal features (Charniak and Johnson, 2005) and semi-supervised word cluster features (Koo et al., 2008). With the help of the non-local features, our transition-based parsing system outperforms all previous single systems in Chinese. After integrating semi-supervised word cluster features, the parsing accuracy is further improved to 86.3% when trained on CTB 5.1 and 87.1% when trained on CTB 6.0, and this is the best reported performance for Chinese. The remainder of this paper is organized as follows: Section 2 introduces the standard transitionbased constituent parsing approach. Section 3 describes our three improvements to standard transition-based constituent parsing. We discuss and analyze</context>
<context position="15473" citStr="Koo et al., 2008" startWordPosition="2631" endWordPosition="2634"> with /, and rr-x with \. beams. 3.3 Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees built by our baseline parser are binary trees (only the complete parse tree is debinariz</context>
<context position="16821" citStr="Koo et al., 2008" startWordPosition="2844" endWordPosition="2847">rms. To resolve this conflict, we integrate the debinarization process into the parsing process, i.e., when a Table 2: Non-local features for constituent parsing. new subtree is constructed during parsing, we debinarize it immediately if it is not rooted with an intermediate node 1. The other subtrees for subsequent parsing steps will be built based on these debinarized subtrees. After the modification, our parser can extract non-local features incrementally during the parsing process. Semi-supervised word cluster features have been successfully applied to many NLP tasks (Miller et al., 2004; Koo et al., 2008; Zhu et al., 2013). Here, we adopt such features for our transition-based constituent parser. Given a largescale unlabeled corpus (word segmentation should be performed), we employ the Brown cluster algorithm (Liang, 2005) to cluster all words into a binary tree. Within this binary tree, words appear as leaves, left branches are labeled with 0 and right branches are labeled with 1. Each word can be uniquely identified by its path from the root, and represented as a bit-string. By using various length of prefixes of the bit-string, we can produce word clusters of different granularities (Mille</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT, pages 595–603, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Daniel Tse</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>An empirical examination of challenges in chinese parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>98--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="25982" citStr="Kummerfeld et al., 2013" startWordPosition="4329" endWordPosition="4332">, and took all the remaining data as the new training set. Table 5 shows the parsing performances on CTB6. Our Nonlocal&amp;Cluster system improved the final F1 to 87.1%, which is 1.9% better than the state-of-the-art performance on CTB6 (Huang and Harper, 2009). Compared with its performance on CTB5 (in Table 4), our Nonlocal&amp;Cluster system also got 0.8% improvement. All these results show that our approach can become more powerful when given more labeled training data. 4.6 Error Analysis To better understand the linguistic behavior of our systems, we employed the berkeley-parseranalyser tool 4 (Kummerfeld et al., 2013) to categorize the errors. Table 6 presents the average 4http://code.google.com/p/berkeley-parser-analyser/ 739 Table 7: POS tagging error patterns on Chinese test set. For each error pattern, the left hand side tag is the gold-standard tag, and the right hand side is the wrongly assigned tag. NP 1-Word Mod. Verb Diff Clause Noun System Unary Coord Int. Span Attach Args Label Attach Edge Worst 1.75 0.74 0.44 0.49 0.39 0.37 0.29 0.15 0.14 Pipeline JointParsing Padding StateAlign Nonlocal Cluster Nonlocal&amp;Cluster Best 1.33 0.42 0.28 0.29 0.19 0.21 0.17 0.07 0.09 Table 6: Parse errors on Chinese </context>
</contexts>
<marker>Kummerfeld, Tse, Curran, Klein, 2013</marker>
<rawString>Jonathan K. Kummerfeld, Daniel Tse, James R. Curran, and Dan Klein. 2013. An empirical examination of challenges in chinese parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 98–103, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="17044" citStr="Liang, 2005" startWordPosition="2880" endWordPosition="2881">diately if it is not rooted with an intermediate node 1. The other subtrees for subsequent parsing steps will be built based on these debinarized subtrees. After the modification, our parser can extract non-local features incrementally during the parsing process. Semi-supervised word cluster features have been successfully applied to many NLP tasks (Miller et al., 2004; Koo et al., 2008; Zhu et al., 2013). Here, we adopt such features for our transition-based constituent parser. Given a largescale unlabeled corpus (word segmentation should be performed), we employ the Brown cluster algorithm (Liang, 2005) to cluster all words into a binary tree. Within this binary tree, words appear as leaves, left branches are labeled with 0 and right branches are labeled with 1. Each word can be uniquely identified by its path from the root, and represented as a bit-string. By using various length of prefixes of the bit-string, we can produce word clusters of different granularities (Miller et al., 2004). Inspired from Koo et al. (2008), we employ two types of word clusters: (1) taking 4 bit-string prefixes of word clusters as replacements of POS tags, and (2) taking 8 bit-string prefixes as replacements of </context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<volume>4</volume>
<pages>337--342</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="16803" citStr="Miller et al., 2004" startWordPosition="2840" endWordPosition="2843">ginal multi-branch forms. To resolve this conflict, we integrate the debinarization process into the parsing process, i.e., when a Table 2: Non-local features for constituent parsing. new subtree is constructed during parsing, we debinarize it immediately if it is not rooted with an intermediate node 1. The other subtrees for subsequent parsing steps will be built based on these debinarized subtrees. After the modification, our parser can extract non-local features incrementally during the parsing process. Semi-supervised word cluster features have been successfully applied to many NLP tasks (Miller et al., 2004; Koo et al., 2008; Zhu et al., 2013). Here, we adopt such features for our transition-based constituent parser. Given a largescale unlabeled corpus (word segmentation should be performed), we employ the Brown cluster algorithm (Liang, 2005) to cluster all words into a binary tree. Within this binary tree, words appear as leaves, left branches are labeled with 0 and right branches are labeled with 1. Each word can be uniquely identified by its path from the root, and represented as a bit-string. By using various length of prefixes of the bit-string, we can produce word clusters of different gr</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In HLT-NAACL, volume 4, pages 337–342. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="23044" citStr="Petrov and Klein (2007)" startWordPosition="3842" endWordPosition="3845"> with the StateAlign system which takes only the baseline features, the non-local features improved parsing F1 by 0.8%, while the semi-supervised word cluster features result in an improvement of 2.3% in parsing F1 and an 1.1% improvement on POS tagging accuracy. When integrating both groups of features, the final parsing F1 reaches 89.1%. Al738 Type System LP LR F1 POS Pipeline 80.0 80.3 80.1 94.0 JointParsing 82.4 83.0 82.7 95.1 Padding 82.7 83.6 83.2 95.1 Our Systems StateAlign 84.2 82.9 83.6 95.5 Nonlocal 85.6 84.2 84.9 95.9 Cluster 85.2 84.5 84.9 95.8 Nonlocal&amp;Cluster 86.6 85.9 86.3 96.0 Petrov and Klein (2007) 81.9 84.8 83.3 - Single Systems Zhu et al. (2013) 82.1 84.3 83.2 - Charniak and Johnson (2005)* 80.8 83.8 82.3 - Reranking Systems Wang and Zong (2011) - - 85.7 - Semi-supervised Systems Zhu et al. (2013) 84.4 86.8 85.6 - Table 4: Parsing performance on Chinese test set. *Huang (2009) adapted the parse reranker to CTB5. l these results show that both the non-local features and the semi-supervised features are helpful for our transition-based constituent parser. 4.5 Final Results on Test Set In this subsection, we present the performances of our systems on the CTB test set. The corresponding r</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1360" citStr="Petrov et al., 2006" startWordPosition="197" endWordPosition="200">ults show that these modifications improve parsing performance significantly. Evaluated on the Chinese TreeBank (CTB), our final performance reaches 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers. 1 Introduction Constituent parsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of arbitrarily complex str</context>
<context position="28188" citStr="Petrov et al., 2006" startWordPosition="4694" endWordPosition="4697">g systems. Table 7 presents the statistics of frequent POS tagging error patterns. We can see that JointParsing system disambiguates {VV, NN} and {DEC, DEG} better than Pipeline system, but cannot deal with the NN→JJ pattern very well. StateAlign system got better results in most of the patterns, but cannot disambiguate {NR, NN} well. Nonlocal&amp;Cluster system got the best results in disambiguating the most ambiguous POS tag pairs of {VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, NR}. 5 Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully u</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433– 440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>125--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1645" citStr="Sagae and Lavie, 2005" startWordPosition="236" endWordPosition="239">roduction Constituent parsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of arbitrarily complex structural features from already constructed subtrees. The downside is that they only search a tiny fraction of the whole space and are therefore commonly considered to be less accurate than chartbased parsers. Recent studies (Zhu et al., 2013; Zhang et al., 2013) show, however, that thi</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 125–132. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Parse reranking based on higher-order lexical dependencies.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1251--1259</pages>
<contexts>
<context position="23196" citStr="Wang and Zong (2011)" startWordPosition="3870" endWordPosition="3873">ter features result in an improvement of 2.3% in parsing F1 and an 1.1% improvement on POS tagging accuracy. When integrating both groups of features, the final parsing F1 reaches 89.1%. Al738 Type System LP LR F1 POS Pipeline 80.0 80.3 80.1 94.0 JointParsing 82.4 83.0 82.7 95.1 Padding 82.7 83.6 83.2 95.1 Our Systems StateAlign 84.2 82.9 83.6 95.5 Nonlocal 85.6 84.2 84.9 95.9 Cluster 85.2 84.5 84.9 95.8 Nonlocal&amp;Cluster 86.6 85.9 86.3 96.0 Petrov and Klein (2007) 81.9 84.8 83.3 - Single Systems Zhu et al. (2013) 82.1 84.3 83.2 - Charniak and Johnson (2005)* 80.8 83.8 82.3 - Reranking Systems Wang and Zong (2011) - - 85.7 - Semi-supervised Systems Zhu et al. (2013) 84.4 86.8 85.6 - Table 4: Parsing performance on Chinese test set. *Huang (2009) adapted the parse reranker to CTB5. l these results show that both the non-local features and the semi-supervised features are helpful for our transition-based constituent parser. 4.5 Final Results on Test Set In this subsection, we present the performances of our systems on the CTB test set. The corresponding results are listed in the top rows of Table 4. We can see that all these systems maintain a similar relative relationship as they do on the development s</context>
</contexts>
<marker>Wang, Zong, 2011</marker>
<rawString>Zhiguo Wang and Chengqing Zong. 2011. Parse reranking based on higher-order lexical dependencies. In IJCNLP, pages 1251–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Kenji Sagae</author>
<author>Teruko Mitamura</author>
</authors>
<title>A fast, accurate deterministic parser for chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>425--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1664" citStr="Wang et al., 2006" startWordPosition="240" endWordPosition="243">arsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of arbitrarily complex structural features from already constructed subtrees. The downside is that they only search a tiny fraction of the whole space and are therefore commonly considered to be less accurate than chartbased parsers. Recent studies (Zhu et al., 2013; Zhang et al., 2013) show, however, that this approach can also</context>
</contexts>
<marker>Wang, Sagae, Mitamura, 2006</marker>
<rawString>Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006. A fast, accurate deterministic parser for chinese. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 425–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus. Natural language engineering,</title>
<date>2005</date>
<pages>11--2</pages>
<contexts>
<context position="17947" citStr="Xue et al., 2005" startWordPosition="3028" endWordPosition="3031"> prefixes of the bit-string, we can produce word clusters of different granularities (Miller et al., 2004). Inspired from Koo et al. (2008), we employ two types of word clusters: (1) taking 4 bit-string prefixes of word clusters as replacements of POS tags, and (2) taking 8 bit-string prefixes as replacements of words. Using these two types of clusters, we construct semi-supervised word cluster features by mimicking the template structure of the original baseline features in Table 1. 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005): Articles 001-270 and 400-1151 were used as the training set, Articles 301-325 were used as the development set, and Articles 271-300 were used 1Intermediate nodes are produced by binarization process. s 0 C0,1 a0,1 D1,2 b1,2 b1,2 A0,2 A0,2 E0,2 F2,3 c2,3 c2,3 80,3 80,3 T1 T0 CoLenPar (Collins and Koo, 2005) (Charniak and Johnson, 2005) CoPar HeadTree Rules Bigrams Grandparent Rules Grandparent Bigrams Lexical Bigrams Two-level Rules Two-level Bigrams Trigrams Head-Modifiers RightBranch Heavy Neighbours NGramTree Heads Wproj Word 737 as the test set. Standard corpus preparation steps were per</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural language engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>162--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1688" citStr="Zhang and Clark, 2009" startWordPosition="244" endWordPosition="247">e most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of arbitrarily complex structural features from already constructed subtrees. The downside is that they only search a tiny fraction of the whole space and are therefore commonly considered to be less accurate than chartbased parsers. Recent studies (Zhu et al., 2013; Zhang et al., 2013) show, however, that this approach can also achieve the state-of-th</context>
<context position="7154" citStr="Zhang and Clark (2009)" startWordPosition="1129" endWordPosition="1132">0t, p0cq0w, p0cq0t q0wq1w, q0wq1t, q0tq1w, q0tq1t p1wq0w, p1wq0t, p1cq0w, p1cq0t p0cp1cp2c, p0wp1cp2c, p0cp1wq0t trigrams p0cp1cp2w, p0cp1cq0t, p0wp1cq0t p0cp1wq0t, p0cp1cq0w Table 1: Baseline features, where pi represents the ith subtree in the stack σ and qi denotes the ith item in the queue β. w refers to the head lexicon, t refers to the head POS, and c refers to the constituent label. pil and pir refer to the left and right child for a binary subtree pi, and piu refers to the child of a unary subtree pi. process such trees, we employ binarization and debinarization processes described in Zhang and Clark (2009) to transform multi-branch trees into binary-branch trees and restore the generated binary trees back to their original forms. 2.2 Modeling, Training and Decoding To determine which action t E T should the parser perform at a state s E 5, we use a linear model to score each possible (s, t) combination: �score(s, t) = w� · 0(s, t) = wifi(s, t) (1) i where 0(s, t) is the feature function used for mapping a state-action pair into a feature vector, and w� is the weight vector. The score of a parser state s is the sum of the scores for all state-action pairs in the transition path from the initial </context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies, pages 162–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Chinese parsing exploiting characters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>125--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2221" citStr="Zhang et al., 2013" startWordPosition="328" endWordPosition="331"> constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of arbitrarily complex structural features from already constructed subtrees. The downside is that they only search a tiny fraction of the whole space and are therefore commonly considered to be less accurate than chartbased parsers. Recent studies (Zhu et al., 2013; Zhang et al., 2013) show, however, that this approach can also achieve the state-of-the-art performance with improved training procedures and the use of additional source of information as features. However, there is still room for improvement for these state-of-the-art transition-based constituent parsers. First, POS tagging is typically performed separately as a preliminary step, and POS tagging errors will propagate to the parsing process. This problem is especially severe for languages where the POS tagging accuracy is relatively low, and this is the case for Chinese where there are fewer contextual clues th</context>
<context position="28508" citStr="Zhang et al. (2013)" startWordPosition="4748" endWordPosition="4751"> {NR, NN} well. Nonlocal&amp;Cluster system got the best results in disambiguating the most ambiguous POS tag pairs of {VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, NR}. 5 Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based pa</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 125–134, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>434--443</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2200" citStr="Zhu et al., 2013" startWordPosition="324" endWordPosition="327">. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a seNianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu ries of deterministic shift-reduce decisions to construct syntactic trees. Therefore, it runs in linear time and can take advantage of arbitrarily complex structural features from already constructed subtrees. The downside is that they only search a tiny fraction of the whole space and are therefore commonly considered to be less accurate than chartbased parsers. Recent studies (Zhu et al., 2013; Zhang et al., 2013) show, however, that this approach can also achieve the state-of-the-art performance with improved training procedures and the use of additional source of information as features. However, there is still room for improvement for these state-of-the-art transition-based constituent parsers. First, POS tagging is typically performed separately as a preliminary step, and POS tagging errors will propagate to the parsing process. This problem is especially severe for languages where the POS tagging accuracy is relatively low, and this is the case for Chinese where there are fewe</context>
<context position="12166" citStr="Zhu et al. (2013)" startWordPosition="2027" endWordPosition="2030">arse tree by consuming all the subtrees in σ. However, ru-x is a very special action. It only constructs a new unary node for the subtree on top of σ, but does not consume any items in σ or β. As a result, the number of ru-x actions varies among terminal states for the same sentence. For example, the parse tree in Figure 1a contains no ru-x action, while the parse tree for the same input sentence in Figure 1b contains four ru-x actions. This makes the lengths of complete action sequences very different, and the parsing model has to disambiguate among terminal states with varying action sizes. Zhu et al. (2013) proposed a padding method to align terminal states containing different number of actions. The idea is to append some IDLE actions to terminal states with shorter action sequence, and make sure all terminal states contain the same number of actions (including IDLE actions). Algorithm 2 Beam-search with State Alignment Input: A word-segmented sentence, beam size k. Output: A constituent parse tree. 1: beam0 ← {s0} &gt; initialization 2: for i ← 0 to 2n − 1 do &gt; n is sentence length 3: P0 ← {}, P1 ← {} &gt; two priority queues 4: while beami is not empty do 5: s ← POP(beami) 6: for t ∈ {sh-x, rl-x, r</context>
<context position="16840" citStr="Zhu et al., 2013" startWordPosition="2848" endWordPosition="2851">is conflict, we integrate the debinarization process into the parsing process, i.e., when a Table 2: Non-local features for constituent parsing. new subtree is constructed during parsing, we debinarize it immediately if it is not rooted with an intermediate node 1. The other subtrees for subsequent parsing steps will be built based on these debinarized subtrees. After the modification, our parser can extract non-local features incrementally during the parsing process. Semi-supervised word cluster features have been successfully applied to many NLP tasks (Miller et al., 2004; Koo et al., 2008; Zhu et al., 2013). Here, we adopt such features for our transition-based constituent parser. Given a largescale unlabeled corpus (word segmentation should be performed), we employ the Brown cluster algorithm (Liang, 2005) to cluster all words into a binary tree. Within this binary tree, words appear as leaves, left branches are labeled with 0 and right branches are labeled with 1. Each word can be uniquely identified by its path from the root, and represented as a bit-string. By using various length of prefixes of the bit-string, we can produce word clusters of different granularities (Miller et al., 2004). In</context>
<context position="21217" citStr="Zhu et al. (2013)" startWordPosition="3542" endWordPosition="3545">effectiveness of our state alignment strategy proposed in 2http://nlp.cs.nyu.edu/evalb/ 3http://nlp.stanford.edu/downloads/tagger.shtml System LP LR F1 POS Pipeline-Gold 92.2 92.5 92.4 100 Pipeline 83.9 83.8 83.8 93.0 JointParsing 85.1 86.6 85.8 94.8 Padding 85.4 86.4 85.9 94.8 StateAlign 86.9 85.9 86.4 95.2 Nonlocal 88.0 86.5 87.2 95.3 Cluster 89.0 88.3 88.7 96.3 Nonlocal&amp;Cluster 89.4 88.7 89.1 96.2 Table 3: Parsing performance on Chinese development set. Subsection 3.2. The first system Padding extends our JointParsing system by aligning terminal states with the padding strategy proposed in Zhu et al. (2013), and the second system StateAlign extends the JointParsing system with our state alignment strategy. The fifth and sixth rows of Table 3 give the performances of these two systems. Compared with the JointParsing system which does not employ any alignment strategy, the Padding system only achieved a slight improvement on parsing F1 score, but no improvement on POS tagging accuracy. In contrast, our StateAlign system achieved an improvement of 0.6% on parsing F1 score and 0.4% on POS tagging accuracy. All these results show us that our state alignment strategy is more helpful for beam-search de</context>
<context position="23094" citStr="Zhu et al. (2013)" startWordPosition="3852" endWordPosition="3855">e features, the non-local features improved parsing F1 by 0.8%, while the semi-supervised word cluster features result in an improvement of 2.3% in parsing F1 and an 1.1% improvement on POS tagging accuracy. When integrating both groups of features, the final parsing F1 reaches 89.1%. Al738 Type System LP LR F1 POS Pipeline 80.0 80.3 80.1 94.0 JointParsing 82.4 83.0 82.7 95.1 Padding 82.7 83.6 83.2 95.1 Our Systems StateAlign 84.2 82.9 83.6 95.5 Nonlocal 85.6 84.2 84.9 95.9 Cluster 85.2 84.5 84.9 95.8 Nonlocal&amp;Cluster 86.6 85.9 86.3 96.0 Petrov and Klein (2007) 81.9 84.8 83.3 - Single Systems Zhu et al. (2013) 82.1 84.3 83.2 - Charniak and Johnson (2005)* 80.8 83.8 82.3 - Reranking Systems Wang and Zong (2011) - - 85.7 - Semi-supervised Systems Zhu et al. (2013) 84.4 86.8 85.6 - Table 4: Parsing performance on Chinese test set. *Huang (2009) adapted the parse reranker to CTB5. l these results show that both the non-local features and the semi-supervised features are helpful for our transition-based constituent parser. 4.5 Final Results on Test Set In this subsection, we present the performances of our systems on the CTB test set. The corresponding results are listed in the top rows of Table 4. We c</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 434–443, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>