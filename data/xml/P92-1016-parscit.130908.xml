<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.839673666666667">
UNDERSTANDING
NATURAL LANGUAGE INSTRUCTIONS:
THE CASE OF PURPOSE CLAUSES
</title>
<author confidence="0.995539">
Barbara Di Eugenio *
</author>
<affiliation confidence="0.9842215">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.648472">
Philadelphia, PA
</address>
<email confidence="0.998653">
dieugeni@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.993093" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999910428571429">
This paper presents an analysis of purpose clauses in
the context of instruction understanding. Such analysis
shows that goals affect the interpretation and / or exe-
cution of actions, lends support to the proposal of using
generation and enablement to model relations between
actions, and sheds light on some inference processes
necessary to interpret purpose clauses.
</bodyText>
<sectionHeader confidence="0.997859" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.902528242424242">
A speaker (S) gives instructions to a hearer (H) in
order to affect H&apos;s behavior. Researchers including
(Winograd, 1972), (Chapman, 1991), (Vere and Bick-
more, 1990), (Cohen and Levesque, 1990), (Alterman et
al., 1991) have been and are addressing many complex
facets of the problem of mapping Natural Language in-
structions onto an agent&apos;s behavior. However, an aspect
that no one has really considered is computing the ob-
jects of the intentions H&apos;s adopts, namely, the actions to
be performed. In general, researchers have equated such
objects with logical forms extracted from the NI. input.
This is perhaps sufficient for simple positive impera-
tives, but more complex imperatives require that action
descriptions be computed, not simply extracted, from the
input instruction. To clarify my point, consider:
Ex. 1 a) Place a plank between two ladders.
b) Place a plank between two ladders
to create a simple scaffold.
In both a) and b), the action to be executed is place
a plank between two ladders. However, Ex. 1.a would
be correctly interpreted by placing the plank anywhere
between the two ladders: this shows that in b) H must
be inferring the proper position for the plank from the
expressed goal to create a simple scaffold. Therefore,
the goal an action is meant to achieve constrains the
interpretation and / or the execution of the action itself.
The infinitival sentence in Ex. 1.b is a purpose clause,
&apos;Mailing address: IRCS - 3401, Walnut St - Suite 400C -
Philadelphia, PA, 19104 - USA.
which, as its name says, expresses the agent&apos;s purpose
in performing a certain action. The analysis of purpose
clauses is relevant to the problem of understanding Nat-
ural Language instructions, because:
</bodyText>
<listItem confidence="0.5421888">
1. Purpose clauses explicitly encode goals and their
interpretation shows that the goals that H adopts
guide his/her computation of the action(s) to per-
form.
2. Purpose clauses appear to express generation or en-
</listItem>
<bodyText confidence="0.986276615384616">
ablement, supporting the proposal, made by (Allen,
1984), (Pollack, 1986), (Grosz and Sidner, 1990),
(Ballcansld, 1990), that these two relations are nec-
essary to model actions.
After a general description of purpose clauses, I will
concentrate on the relations between actions that they
express, and on the inference processes that their in-
terpretation requires. I see these inferences as instan-
tiations of general accommodation processes necessary
to interpret instructions, where the term accommodation
is borrowed from (Lewis, 1979). I will conclude by
describing the algorithm that implements the proposed
inference processes.
</bodyText>
<sectionHeader confidence="0.907726" genericHeader="method">
PURPOSE CLAUSES
</sectionHeader>
<bodyText confidence="0.986715666666667">
I am not the first one to analyze purpose clauses: how-
ever, they have received attention almost exclusively
from a syntactic point of view — see for example (Jones,
1985), (Hegarty, 1990). Notice that I am not using the
term purpose clause in the technical way it has been
used in syntax, where it refers to infinitival to clauses
adjoined to NPs. In contrast, the infinitival clauses I
have concentrated on are adjoined to a matrix clause,
and are termed rational clauses in syntax; in fact all the
data I will discuss in this paper belong to a particular
subclass of such clauses, subject-gap rational clauses.
As far as I know, very little attention has been paid
to purpose clauses in the semantics literature: in (1990),
Jackendoff briefly analyzes expressions of purpose, goal,
or rationale, normally encoded as an infinitival, in order
</bodyText>
<page confidence="0.99216">
120
</page>
<bodyText confidence="0.999574692307692">
to-phrase, or for-phrase. He represents them by means
of a subordinating function FOR, which has the adjunct
clause as an argument; in turn, FOR plus its argument
is a restrictive modifier of the main clause. However,
Jackendoff&apos;s semantic decomposition doesn&apos;t go beyond
the construction of the logical form of a sentence, and
he doesn&apos;t pursue the issue of what the relation between
the actions described in the matrix and adjunct really is.
The only other work that mentions purpose clauses in
a computational setting is (Balkanski, 1991). However,
she doesn&apos;t present any linguistic analysis of the data; as
I will show, such analysis raises many interesting issues,
such as 1:
</bodyText>
<listItem confidence="0.995724">
• It is fairly clear that S uses purpose clauses to explain
to H the goal 3 to whose achievement the execution of a
contributes. However, an important point that had been
overlooked so far is that the goal # also constrains the
interpretation of a, as I observed with respect to Ex. 1.b.
Another example in point is:
</listItem>
<bodyText confidence="0.935555857142857">
Ex. 2 Cut the square in half to create two triangles.
The action to be performed is cutting the square in half.
However, such action description is underspecified, in
that there is an infinite number of ways of cutting a
square in half: the goal create two triangles restricts
the choice to cutting the square along one of the two
diagonals.
</bodyText>
<listItem confidence="0.9138997">
• Purpose clauses relate action descriptions at different
levels of abstraction, such as a physical action and an
abstract process, or two physical actions, but at different
levels of granularity:
Ex. 3 Heat on stove to simmer.
• As far as what is described in purpose clauses, I have
been implying that both matrix and purpose clauses de-
scribe an action, a and fl respectively. There are rare
cases — in fact, I found only one — in which one of the
two clauses describes a state a:
</listItem>
<bodyText confidence="0.658473333333333">
Ex. 4 To be successfully covered, a wood wall must be
fiat and smooth.
I haven&apos;t found any instances in which both matrix and
purpose clauses describe a state. Intuitively, this makes
sense because S uses a purpose clause to inform H of
the purpose of a given action 2.
</bodyText>
<listItem confidence="0.873184">
• In most cases, the goal describes a change in the
world. However, in some cases
1. The change is not in the world, but in H&apos;s knowl-
edge. By executing a, H can change the state of
his knowledge with respect to a certain proposition
or to the value of a certain entity.
</listItem>
<bodyText confidence="0.920106481481482">
II collected one hundred and one consecutive instances of
purpose clauses from a how-to-do book on installing wall cov-
erings, and from two craft magazines.
2There are clearly other ways of describing that a state is
the goal of a certain action, for example by means of so/such
that, but I won&apos;t deal with such data here.
Ex. 5 You may want to hang a coordinating border
around the room at the top of the walls. To deter-
mine the amount of border, measure the width (in
feet) of all walls to be covered and divide by three.
Since borders are sold by the yard, this will give you
the number of yards needed.
Many of such examples involve verbs such as
check, make sure etc. followed by a that-
complement describing a state IS. The use of such
verbs has the pragmatic effect that not only does H
check whether 0 holds, but, if 0 doesn&apos;t hold, s/he
will also do something so that 0 comes to hold.
Ex. 6 To attach the wires to the new switch, use the
paper clip to move the spring type clip aside and
slip the wire into place. Tug gently on each wire to
make sure it&apos;s secure.
2. The purpose clause may inform H that the world
should not change, namely, that a given event
should be prevented from happening:
Ex. 7 Tape raw edges of fabric to prevent threads
from raveling as you work.
</bodyText>
<listItem confidence="0.910097166666667">
• From a discourse processing point of view, interpret-
ing a purpose clause may affect the discourse model, in
particular by introducing new referents. This happens
when the effect of a is to create a new object, and [3
identifies it. Verbs frequently used in this context are
create, make, form etc.
</listItem>
<bodyText confidence="0.92660975">
Ex. 8 Join the short ends of the hat band to form a circle.
Similarly, in Ex. 2 the discourse referents for the tri-
angles created by cutting the square in half, and in Ex. 5
the referent for amount of border are introduced.
</bodyText>
<sectionHeader confidence="0.997949" genericHeader="method">
RELATIONS BETWEEN ACTIONS
</sectionHeader>
<bodyText confidence="0.999531642857143">
So far, I have mentioned that a contributes to achiev-
ing the goal [3. The notion of contribution can be made
more specific by examining naturally occurring purpose
clauses. In the majority of cases, they express genera-
tion, and in the rest enablement. Also (Grosz and Sid-
ner, 1990) use contribute as a relation between actions,
and they define it as a place holder for any relation ...
that can hold between actions when one can be said to
contribute (for example, by generating or enabling) to
the performance of the other. However, they don&apos;t jus-
tify this in terms of naturally occurring data. Balkanski
(1991) does mention that purpose clauses express gen-
eration or enablement, but she doesn&apos;t provide evidence
to support this claim.
</bodyText>
<sectionHeader confidence="0.944655" genericHeader="method">
GENERATION
</sectionHeader>
<bodyText confidence="0.999927142857143">
Generation is a relation between actions that has been
extensively studied, first in philosophy (Goldman, 1970)
and then in discourse analysis (Allen, 1984), (Pollack,
1986), (Grosz and Sidner, 1990), (Balkanski, 1990).
According to Goldman, intuitively generation is the re-
lation between actions conveyed by the preposition by
in English — turning on the light by flipping the switch.
</bodyText>
<page confidence="0.990355">
121
</page>
<bodyText confidence="0.9719895">
More formally, we can say that an action a conditionally
generates another action 13 iff 3:
</bodyText>
<listItem confidence="0.906989142857143">
1. a and 13 are simultaneous;
2. a is not part of doing (as in the case of playing
a C note as part of playing a C triad on a piano);
3. when a occurs, a set of conditions C hold, such that
the joint occurrence of a and C imply the occur-
rence of 13. In the case of the generation relation
between flipping the switch and turning on the light,
</listItem>
<bodyText confidence="0.979225411764706">
C will include that the wire, the switch and the bulb
are working.
Although generation doesn&apos;t hold between a and fl if a
is part of a sequence of actions A to do fl, generation
may hold between the whole sequence A and 19.
Generation is a pervasive relation between action de-
scriptions in naturally occurring data. However, it ap-
pears from my corpus that by clauses are used less fre-
quently than purpose clauses to express generation 4:
about 95% of my 101 purpose clauses express gener-
ation, while in the same corpus there are only 27 by
clauses. It does look like generation in instructional text
is mainly expressed by means of purpose clauses. They
may express either a direct generation relation between
a and /3, or an indirect generation relation between a
and )3, where by indirect generation I mean that a be-
longs to a sequence of actions A which generates 3.
</bodyText>
<sectionHeader confidence="0.742524" genericHeader="method">
ENABLEMENT
</sectionHeader>
<bodyText confidence="0.995981222222222">
Following first Pollack (1986) and then Balkanski
(1990), enablement holds between two actions a and
fl if and only if an occurrence of a brings about a set of
conditions that are necessary (but not necessarily suffi-
cient) for the subsequent performance of 13.
Only about 5% of my examples express enablement:
Ex. 9 Unscrew the protective plate to expose the box.
Unscrew the protective plate enables taking the plate off
which generates exposing the box.
</bodyText>
<sectionHeader confidence="0.998217" genericHeader="method">
GENERATION AND ENABLEMENT IN
MODELING ACTIONS
</sectionHeader>
<bodyText confidence="0.988002142857143">
That purpose clauses do express generation and enable-
ment is a welcome finding: these two relations have
been proposed as necessary to model actions (Allen,
1984), (Pollack, 1986), (Grosz and Sidner, 1990),
(Ballcanski, 1990), but this proposal has not been jus-
tified by offering an extensive analysis of whether and
how these relations are expressed in NL.
</bodyText>
<footnote confidence="0.849311">
3Goldman distinguishes among four kinds of generation re-
lations: subsequent work has been mainly influenced by con-
ditional generation.
4Generation can also be expressed with a simple free ad-
junct; however, this use of free adjuncts is not very common
</footnote>
<subsectionHeader confidence="0.394417">
— see (Webber and Di Eugenio, 1990).
</subsectionHeader>
<bodyText confidence="0.976421884615385">
A further motivation for using generation and enable-
ment in modeling actions is that they allow us to draw
conclusions about action execution as well — a particu-
larly useful consequence given that my work is taking
place in the framework of the Animation from Natural
Language — AnimNL, project (Sadler et al., 1990; Web-
ber et al., 1991) in which the input instructions do have
to be executed, namely, animated.
As has already been observed by other researchers, if
a generates 13, two actions are described, but only a,
the generator, needs to be performed. In Ex. 2, there is
no creating action per se that has to be executed: the
physical action to be performed is cutting, constrained
by the goal as explained above.
In contrast to generation, if a enables )9, after execut-
ing a, 13 still needs to be executed: a has to temporally
precede fl, in the sense that a has to begin, but not nec-
essarily end, before 3. In Ex. 10, hold has to continue
for the whole duration of fill:
Ex. 10 Hold the cup under the spigot to fill it with coffee.
Notice that, in the same way that the generatee affects
the execution of the generator, so the enabled action
affects the execution of the enabling action. Consider
the difference in the interpretation of to in go to the
mirror, depending upon whether the action to be enabled
is seeing oneself or carrying the mirror somewhere else.
</bodyText>
<sectionHeader confidence="0.98038" genericHeader="method">
INFERENCE PROCESSES
</sectionHeader>
<bodyText confidence="0.997409333333333">
So far, I have been talking about the purpose clause
constraining the interpretation of the matrix clause. I
will now provide some details on how such constraints
are computed. The inferences that I have identified so
far as necessary to interpret purpose clauses can be de-
scribed as
</bodyText>
<listItem confidence="0.830413333333333">
1. Computing a more specific action description.
2. Computing assumptions that have to hold for a cer-
tain relation between actions to hold.
</listItem>
<bodyText confidence="0.803057764705882">
Computing more specific action descriptions.
In Ex. 2 — Cut the square in half to create two triangles
— it is necessary to find a more specific action al which
will achieve the goal specified by the purpose clause, as
shown in Fig. 1.
For Ex. 2 we have ,3 = create two triangles, a =
cut the square in half, al = cut the square in half along
the diagonal. The reader will notice that the inputs to
accommodation are linguistic expressions, while its out-
puts are predicate — argument structures: I have used
the latter in Fig. 1 to indicate that accommodation infers
relations between action types. However, as I will show
later, the representation I adopt is not based on predi-
cate — argument structures. Also notice that I am using
Greek symbols for both linguistic expressions and action
types: the context should be sufficient to disambiguate
which one is meant.
</bodyText>
<note confidence="0.625672">
Computing assumptions. Let&apos;s consider:
</note>
<page confidence="0.956498">
122
</page>
<figure confidence="0.997805166666667">
13 (create two triangles)
?I
13 (create (agent, two-triangles))
accommodation
,e a (cut (agent, square, in-half))
a (cut the square in half) a (cut (agent, square, in-half, along (diagonal)))
</figure>
<figureCaption confidence="0.998293">
Figure 1: Schematic depiction of the first kind of accommodation
</figureCaption>
<figure confidence="0.9984515">
accommodation
a
</figure>
<figureCaption confidence="0.997623">
Figure 2: Schematic depiction of the second kind of accommodation
</figureCaption>
<bodyText confidence="0.946480844444444">
Ex. 11 Go into the other room to get the urn of coffee.
Presumably, H doesn&apos;t have a particular plan that deals
with getting an urn of coffee. S/he will have a generic
plan about get x, which s/he will adapt to the instructions
S gives him 5. In particular, H has to find the connection
between go into the other room and get the urn of coffee.
This connection requires reasoning about the effects of
go with respect to the plan get x; notice that the (most
direct) connection between these two actions requires
the assumption that the referent of the urn of coffee is
in the other room. Schematically, one could represent
this kind of inference as in Fig. 2 — f3 is the goal, a the
instruction to accommodate, Ak the actions belonging
to the plan to achieve )3, C the necessary assumptions.
It could happen that these two kinds of inference need
to be combined: however, no example I have found so
far requires it.
INTERPRETING Do a to do )3
In this section, I will describe the algorithm that im-
5Actually H may have more than one single plan for get x,
in which case go into the other room may in fact help to select
the plan the instructor has in mind.
plements the two kinds of accommodation described in
the previous section. Before doing that, I will make
some remarks on the action representation I adopt and
on the structure of the intentions — the plan graph — that
my algorithm contributes to building.
Action representation. To represent action types, I use
an hybrid system (Brachman et a/., 1983), whose primi-
tives are taken from Jackendoff&apos;s Conceptual Structures
(1990); relations between action types are represented in
another module of the system, the action library.
I&apos;d like to spend a few words justifying the choice
of an hybrid system: this choice is neither casual, nor
determined by the characteristics of the AnimNL project.
Generally, in systems that deal with NL instructions,
action types are represented as predicate — argument
structures; the crucial assumption is then made that the
logical form of an input instruction will exactly match
one of these definitions. However, there is an infinite
number of NL descriptions that correspond to a basic
predicate — argument structure: just think of all the pos-
sible modifiers that can be added to a basic sentence
containing only a verb and its arguments. Therefore it
is necessary to have a flexible knowledge representation
</bodyText>
<page confidence="0.9978">
123
</page>
<bodyText confidence="0.996116905660377">
system that can help us understand the relation between
the input description and the stored one. I claim that
hybrid KR systems provide such flexibility, given their
virtual lattice structure and the classification algorithm
operating on the lattice: in the last section of this paper
I will provide an example supporting my claim.
Space doesn&apos;t allow me to deal with the reason why
Conceptual Structures are relevant, namely, that they are
useful to compute assumptions. For further details, the
interested reader is referred to (Di Eugenio, 1992; Di
Eugenio and White, 1992).
Just a reminder to the reader that hybrid systems have
two components: the terminological box, or T-Box,
where concepts are defined, and on which the classi-
fication algorithm works by computing subsumption re-
lations between different concepts. The algorithm is cru-
cial for adding new concepts to the KB: it computes the
subsumption relations between the new concept and all
the other concepts in the lattice, so that it can &amp;quot;position&amp;quot;
the new concept in the right place in the lattice. The
other component of an hybrid system is the assertional
box, or A-box, where assertions are stored, and which
is equipped with a theorem-prover.
In my case, the T-Box contains knowledge about ac-
tion types, while assertions about individual actions —
instances of the types — are contained in the A-Box:
such individuals correspond to the action descriptions
contained in the input instructions 6.
The action library contains simple plans relating ac-
tions; simple plans are either generation or enablement
relations between pairs: the first member of the pair is
either a single action or a sequence of action, and the
second member is an action. In case the first member of
the pair is an individual action, I will talk about direct
generation or enablement. For the moment, generation
and enablement are represented in a way very similar to
(Ballcanski, 1990).
The plan graph represents the structure of the inten-
tions derived from the input instructions. It is composed
of nodes that contain descriptions of actions, and arcs
that denote relations between them. A node contains
the Conceptual Structures representation of an action,
augmented with the consequent state achieved after the
execution of that action. The arcs represent, among oth-
ers: temporal relations; generation; enablement.
The plan graph is built by an interpretation algorithm
that works by keeping track of active nodes, which for
the moment include the goal currently in focus and the
nodes just added to the graph; it is manipulated by var-
ious inference processes, such as plan expansion, and
plan recognition.
My algorithm is described in Fig. 3 7. Clearly the
inferences I describe are possible only because I rely
</bodyText>
<footnote confidence="0.9979985">
6Notice that these individuals are simply instances of
generic concepts, and not necessarily action tokens, namely,
nothing is asserted with regard to their happening in the world.
7As I mentioned earlier in the paper, the Greek symbols
</footnote>
<bodyText confidence="0.9990544">
on the other AnitnNL modules for 1) parsing the in-
put and providing a logical form expressed in terms of
Conceptual Structures primitives; 2) managing the dis-
course model, solving anaphora, performing temporal
inferences etc (Webber et al., 1991).
</bodyText>
<sectionHeader confidence="0.434466" genericHeader="method">
AN EXAMPLE OF THE ALGORITHM
</sectionHeader>
<bodyText confidence="0.919015891304347">
I will conclude by showing how step 4a in Fig. 3 takes
advantage of the classification algorithm with which hy-
brid systems are equipped.
Consider the T-Box, or better said, the portion of T-
Box shown in Fig. 4 8.
Given Ex. 2 — Cut the square in half to create two
triangles — as input, the individual action description
cut (the) square in half will be asserted in the A-Box
and recognized as an instance of a — the shaded concept
cut (a) square in half — which is a descendant of cut
and an abstraction of w — cut (a) square in half along
the diagonal, as shown in Fig. 5 9. Notice that this
does not imply that the concept cut (a) square in half
is known beforehand: the classification process is able
to recognize it as a virtual concept and to find the right
place for it in the lattice 1°. Given that a is ancestor
of w, and that w generates # — create two triangles, the
fact that the action to be performed is actually w and not
a can be inferred. This implements step 4(a)ii.
The classification process can also help to deal with
cases in which a is in conflict with w — step 4(a)iv. If a
were cut (a) square along a perpendicular axis, a con-
flict with w — cut (a) square in half along the diagonal
— would be recognized. Given the T-Box in fig. 4, the
classification process would result in a being a sister to
C4): my algorithm would try to unify them, but this would
not be possible, because the role fillers of location on
a and 4/ cannot be unified, being along(perpendicular-
axis) and along(diagonal) respectively. I haven&apos;t ad-
dressed the issue yet of which strategies to adopt in case
such a conflict is detected.
Another point left for future work is what to do when
step 2 yields more than one simple plan.
The knowledge representation system I am using is
BACK (Peltason et al., 1989); the algorithm is being
implemented in QUINTUS PROLOG.
refer both to input descriptions and to action types.
8The reader may find that the representation in Fig. 4 is
not very perspicuous, as it mixes linguistic expressions, such
as along(diagonal), with conceptual knowledge about entities.
Actually, roles and concepts are expressed in terms of Con-
ceptual Structures primitives, which provide a uniform way
of representing knowledge apparently belonging to different
types. However, a T-Box expressed in terms of Conceptual
Structures becomes very complex, so in Fig. 4 I adopted a
more readable representation.
</bodyText>
<footnote confidence="0.998207">
6The agent role does not appear on cut square in half in
the A-Box for the sake of readability.
16In fact, such concept is not really added to the lattice.
</footnote>
<page confidence="0.994545">
124
</page>
<figure confidence="0.924147538461538">
Input: the Conceptual Structures logical forms for a and 13, the current plan graph, and the list of active nodes.
1. Add to A-Box individuals corresponding to the two logical forms. Set flag ACCOM if they don&apos;t exactly match
known concepts.
2. Retrieve from the action library the simple plan(s) associated with - generation relations in which fl is the
generatee, enablement relations in which # is the enablee.
3. If ACCOM is not set
(a) If there is a direct generation or enablement relation between a and 3, augment plan graph with the structure
derived from it, after calling compute-assumptions.
(b) If there is no such direct relation, recursively look for possible connections between a and the components yi
of sequences that either generate or enable /3.
Augment plan graph, after calling c omput e -a s sumpt ions.
4. If ACCOM is set,
(a) If there is w such that co directly generates or enables /3, check whether
</figure>
<bodyText confidence="0.8522003">
i. w is an ancestor of a: take a as the intended action.
co is a descendant of a: take co as the intended action.
If co and a are not ancestors of each other, but they can be unified - all the information they provide
is compatible, as in the case of cut square in half along diagonal and cut square carefully - then their
unification cv LI a is the action to be executed.
iv. If co and a are not ancestors of each other, and provide conflicting information - such as cut square along
diagonal and cut square along perpendicular axis - then signal failure.
(b) If there is no such co, look for possible connections between a and the components -yi of sequences that either
generate or enable )3, as in step 3b. Given that a is not known to the system, apply the inferences described
in 4a to a and -yi.
</bodyText>
<figureCaption confidence="0.997164">
Figure 3: The algorithm for Do a to do )3
</figureCaption>
<page confidence="0.838226">
125
</page>
<figure confidence="0.894227">
individual
instantiates
</figure>
<figureCaption confidence="0.938436">
Figure 4: A portion of the action hierarchy
A-BOX
Figure 5: Dealing with less specific action descriptions
</figureCaption>
<page confidence="0.996458">
126
</page>
<sectionHeader confidence="0.994797" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999942818181818">
I have shown that the analysis of purpose clauses
lends support to the proposal of using generation and
enablement to model actions, and that the interpretation
of purpose clauses originates specific inferences: I have
illustrated two of them, that can be seen as examples of
accommodation processes (Lewis, 1979), and that show
how the hearer&apos;s inference processes are directed by the
goal(s) s/he is adopting.
Future work includes fully developing the action rep-
resentation formalism, and the algorithm, especially the
part regarding computing assumptions.
</bodyText>
<sectionHeader confidence="0.990213" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.994745545454546">
For financial support I acknowledge DARPA grant no.
N0014-904-1863 and ARO grant no. DAAL03-89-
C0031PR1. Thanks to Bonnie Webber for support, in-
sights and countless discussions, and to all the members
of the AnimNL group, in particular to Mike White. Fi-
nally, thanks to the Dipartimento di Informatica - Uni-
versita&apos; di Torino - Italy for making their computing
environment available to me, and in particular thanks to
Felice Cardone, Luca Console, Leonardo Lesmo, and
Vincenzo Lombardo, who helped me through a last
minute computer crash.
</bodyText>
<sectionHeader confidence="0.994065" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98413295">
(Allen, 1984) James Allen. Towards a general theory
of action and time. Artificial Intelligence, 23:123-
154, 1984.
(Alterman et a/., 1991) Richard Alterman, Roland Zito-
Wolf, and Tamitha Carpenter. Interaction, Com-
prehension, and Instruction Usage. Technical Re-
port CS-91-161, Dept. of Computer Science, Cen-
ter for Complex Systems, Brandeis University,
1991.
(Badler et al., 1990) Norman Badler, Bonnie Webber,
Jeff Esakov, and Jugal ICalita. Animation from in-
structions. In Badler, Barsky, and Zeltzer, editors,
Making them Move, MIT Press, 1990.
(Ballcanslci, 1990) Cecile Ballcanski. Modelling act-type
relations in collaborative activity. Technical Re-
port TR-23-90, Center for Research in Computing
Technology, Harvard University, 1990.
(Balkanski, 1991) Cecile Ballcanski. Logical form of
complex sentences in task-oriented dialogues. In
Proceedings of the 29th Annual Meeting of the ACL,
Student Session, 1991.
(Brachman et a/., 1983) R. Bractunan, R.Fikes, and H.
Levesque. KRYPTON: A Functional Approach
to Knowledge Representation. Technical Re-
port FLAIR 16, Fairchild Laboratories for Artificial
Intelligence, Palo Alto, California, 1983.
(Chapman, 1991) David Chapman. Vision, Instruction
and Action. Cambridge: MIT Press, 1991.
(Cohen and Levesque, 1990) Philip Cohen and Hector
Levesque. Rational Interaction as the Basis for
Communication. In J. Morgan, P. Cohen, and
M. Pollack, editors, Intentions in Communication,
MIT Press, 1990.
(Di Eugenio, 1992) Barbara Di Eugenio. Goals and Ac-
tions in Natural Language Instructions. Technical
Report MS-CIS-92-07, University of Pennsylvania,
1992.
(Di Eugenio and White, 1992) Barbara Di Eugenio and
Michael White. On the Interpretation of Natural
Language Instructions. 1992. COLING 92.
(Goldman, 1970) Alvin Goldman. A Theory of Human
Action. Princeton University Press, 1970.
(Grosz and Sidner, 1990) Barbara Grosz and Candace
Sidner. Plans for Discourse. In J. Morgan, P. Co-
hen, and M. Pollack, editors, Intentions in Commu-
nication, MIT Press, 1990.
(Hegarty, 1990) Michael Hegarty. Secondary Predi-
cation and Null Operators in English. 1990.
Manuscript.
(Jackendoff, 1990) Ray Jackendoff. Semantic Struc-
tures. Current Studies in Linguistics Series, The
MIT Press, 1990.
(Jones, 1985) Charles Jones. Agent, patient, and con-
trol into purpose clauses. In Chicago Linguistic
Society, 21, 1985.
(Lewis, 1979) David Lewis. Scorekeeping in a lan-
guage game. Journal of Philosophical Language,
8:339-359, 1979.
(Peltason et al., 1989) C. Peltason, A. Schmiedel, C.
Kindennann, and J. Quanta. The BACK System
Revisited. Technical Report KIT 75, Technische
Universitaet Berlin, 1989.
(Pollack, 1986) Martha Pollack. Inferring domain plans
in question-answering. PhD thesis, University of
Pennsylvania, 1986.
(Vere and Biclunore, 1990) Steven Vere and Timothy
Bickmore. A basic agent. Computational Intel-
ligence, 6:41-60, 1990.
(Webber and Di Eugenio, 1990) Bonnie Webber and
Barbara Di Eugenio. Free Adjuncts in Natural Lan-
guage Instructions. In Proceedings Thirteenth In-
ternational Conference on Computational Linguis-
tics, COLING 90, pages 395-400, 1990.
(Webber et al., 1991) Bonnie Webber, Norman Badler,
Barbara Di Eugenio, Libby Levison, and Michael
White. Instructing Animated Agents. In Proc. US-
Japan Workshop on Integrated Systems in Multi-
Media Environments. Las Cruces, NM, 1991.
(Winograd, 1972) Terry Winograd. Understanding Nat-
ural Language. Academic Press, 1972.
</reference>
<page confidence="0.997331">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955938">
<title confidence="0.995695666666667">UNDERSTANDING NATURAL LANGUAGE INSTRUCTIONS: THE CASE OF PURPOSE CLAUSES</title>
<author confidence="0.999792">Barbara Di_Eugenio</author>
<affiliation confidence="0.9999005">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.98016">Philadelphia, PA</address>
<email confidence="0.999403">dieugeni@linc.cis.upenn.edu</email>
<abstract confidence="0.998388">This paper presents an analysis of purpose clauses in the context of instruction understanding. Such analysis shows that goals affect the interpretation and / or execution of actions, lends support to the proposal of using generation and enablement to model relations between actions, and sheds light on some inference processes necessary to interpret purpose clauses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Towards a general theory of action and time.</title>
<date>1984</date>
<journal>Artificial Intelligence,</journal>
<pages>23--123</pages>
<marker>(Allen, 1984)</marker>
<rawString>James Allen. Towards a general theory of action and time. Artificial Intelligence, 23:123-154, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Alterman</author>
<author>Roland ZitoWolf</author>
<author>Tamitha Carpenter</author>
</authors>
<title>Interaction, Comprehension, and Instruction Usage.</title>
<date>1991</date>
<tech>Technical Report CS-91-161,</tech>
<institution>Dept. of Computer Science, Center for Complex Systems, Brandeis University,</institution>
<marker>(Alterman et a/., 1991)</marker>
<rawString>Richard Alterman, Roland ZitoWolf, and Tamitha Carpenter. Interaction, Comprehension, and Instruction Usage. Technical Report CS-91-161, Dept. of Computer Science, Center for Complex Systems, Brandeis University, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman Badler</author>
<author>Bonnie Webber</author>
</authors>
<title>Jeff Esakov, and Jugal ICalita. Animation from instructions.</title>
<date>1990</date>
<booktitle>Making them Move,</booktitle>
<editor>In Badler, Barsky, and Zeltzer, editors,</editor>
<publisher>MIT Press,</publisher>
<marker>(Badler et al., 1990)</marker>
<rawString>Norman Badler, Bonnie Webber, Jeff Esakov, and Jugal ICalita. Animation from instructions. In Badler, Barsky, and Zeltzer, editors, Making them Move, MIT Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile Ballcanski</author>
</authors>
<title>Modelling act-type relations in collaborative activity.</title>
<date>1990</date>
<tech>Technical Report TR-23-90,</tech>
<institution>Center for Research in Computing Technology, Harvard University,</institution>
<marker>(Ballcanslci, 1990)</marker>
<rawString>Cecile Ballcanski. Modelling act-type relations in collaborative activity. Technical Report TR-23-90, Center for Research in Computing Technology, Harvard University, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile Ballcanski</author>
</authors>
<title>Logical form of complex sentences in task-oriented dialogues.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL, Student Session,</booktitle>
<marker>(Balkanski, 1991)</marker>
<rawString>Cecile Ballcanski. Logical form of complex sentences in task-oriented dialogues. In Proceedings of the 29th Annual Meeting of the ACL, Student Session, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bractunan</author>
<author>R Fikes</author>
<author>H Levesque</author>
</authors>
<title>KRYPTON: A Functional Approach to Knowledge Representation.</title>
<date>1983</date>
<tech>Technical Report FLAIR 16,</tech>
<institution>Fairchild Laboratories for Artificial Intelligence,</institution>
<location>Palo Alto, California,</location>
<marker>(Brachman et a/., 1983)</marker>
<rawString>R. Bractunan, R.Fikes, and H. Levesque. KRYPTON: A Functional Approach to Knowledge Representation. Technical Report FLAIR 16, Fairchild Laboratories for Artificial Intelligence, Palo Alto, California, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chapman</author>
</authors>
<title>Vision, Instruction and Action.</title>
<date>1991</date>
<publisher>MIT Press,</publisher>
<location>Cambridge:</location>
<marker>(Chapman, 1991)</marker>
<rawString>David Chapman. Vision, Instruction and Action. Cambridge: MIT Press, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Cohen</author>
<author>Hector Levesque</author>
</authors>
<title>Rational Interaction as the Basis for Communication. In</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<editor>J. Morgan, P. Cohen, and M. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<marker>(Cohen and Levesque, 1990)</marker>
<rawString>Philip Cohen and Hector Levesque. Rational Interaction as the Basis for Communication. In J. Morgan, P. Cohen, and M. Pollack, editors, Intentions in Communication, MIT Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
</authors>
<title>Goals and Actions in Natural Language Instructions.</title>
<date>1992</date>
<tech>Technical Report MS-CIS-92-07,</tech>
<institution>University of Pennsylvania,</institution>
<marker>(Di Eugenio, 1992)</marker>
<rawString>Barbara Di Eugenio. Goals and Actions in Natural Language Instructions. Technical Report MS-CIS-92-07, University of Pennsylvania, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael White</author>
</authors>
<title>On the Interpretation of Natural Language Instructions.</title>
<date>1992</date>
<journal>COLING</journal>
<volume>92</volume>
<marker>(Di Eugenio and White, 1992)</marker>
<rawString>Barbara Di Eugenio and Michael White. On the Interpretation of Natural Language Instructions. 1992. COLING 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvin Goldman</author>
</authors>
<title>A Theory of Human Action.</title>
<date>1970</date>
<publisher>Princeton University Press,</publisher>
<marker>(Goldman, 1970)</marker>
<rawString>Alvin Goldman. A Theory of Human Action. Princeton University Press, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Plans for Discourse. In</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<editor>J. Morgan, P. Cohen, and M. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<marker>(Grosz and Sidner, 1990)</marker>
<rawString>Barbara Grosz and Candace Sidner. Plans for Discourse. In J. Morgan, P. Cohen, and M. Pollack, editors, Intentions in Communication, MIT Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hegarty</author>
</authors>
<title>Secondary Predication and Null Operators in English.</title>
<date>1990</date>
<publisher>Manuscript.</publisher>
<marker>(Hegarty, 1990)</marker>
<rawString>Michael Hegarty. Secondary Predication and Null Operators in English. 1990. Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures. Current Studies in Linguistics Series,</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<marker>(Jackendoff, 1990)</marker>
<rawString>Ray Jackendoff. Semantic Structures. Current Studies in Linguistics Series, The MIT Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>patient Agent</author>
</authors>
<title>and control into purpose clauses.</title>
<date>1985</date>
<booktitle>In Chicago Linguistic Society,</booktitle>
<volume>21</volume>
<marker>(Jones, 1985)</marker>
<rawString>Charles Jones. Agent, patient, and control into purpose clauses. In Chicago Linguistic Society, 21, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Scorekeeping in a language game.</title>
<date>1979</date>
<journal>Journal of Philosophical Language,</journal>
<pages>8--339</pages>
<marker>(Lewis, 1979)</marker>
<rawString>David Lewis. Scorekeeping in a language game. Journal of Philosophical Language, 8:339-359, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Peltason</author>
<author>A Schmiedel</author>
<author>C Kindennann</author>
<author>J Quanta</author>
</authors>
<title>The BACK System Revisited.</title>
<date>1989</date>
<tech>Technical Report KIT 75,</tech>
<institution>Technische Universitaet</institution>
<location>Berlin,</location>
<marker>(Peltason et al., 1989)</marker>
<rawString>C. Peltason, A. Schmiedel, C. Kindennann, and J. Quanta. The BACK System Revisited. Technical Report KIT 75, Technische Universitaet Berlin, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Pollack</author>
</authors>
<title>Inferring domain plans in question-answering.</title>
<date>1986</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<marker>(Pollack, 1986)</marker>
<rawString>Martha Pollack. Inferring domain plans in question-answering. PhD thesis, University of Pennsylvania, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Vere</author>
<author>Timothy Bickmore</author>
</authors>
<title>A basic agent.</title>
<date>1990</date>
<journal>Computational Intelligence,</journal>
<pages>6--41</pages>
<marker>(Vere and Biclunore, 1990)</marker>
<rawString>Steven Vere and Timothy Bickmore. A basic agent. Computational Intelligence, 6:41-60, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Free Adjuncts in Natural Language Instructions.</title>
<date>1990</date>
<booktitle>In Proceedings Thirteenth International Conference on Computational Linguistics, COLING 90,</booktitle>
<pages>395--400</pages>
<marker>(Webber and Di Eugenio, 1990)</marker>
<rawString>Bonnie Webber and Barbara Di Eugenio. Free Adjuncts in Natural Language Instructions. In Proceedings Thirteenth International Conference on Computational Linguistics, COLING 90, pages 395-400, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Norman Badler</author>
<author>Barbara Di Eugenio</author>
<author>Libby Levison</author>
<author>Michael White</author>
</authors>
<title>Instructing Animated Agents.</title>
<date>1991</date>
<booktitle>In Proc. USJapan Workshop on Integrated Systems in MultiMedia Environments. Las</booktitle>
<location>Cruces, NM,</location>
<marker>(Webber et al., 1991)</marker>
<rawString>Bonnie Webber, Norman Badler, Barbara Di Eugenio, Libby Levison, and Michael White. Instructing Animated Agents. In Proc. USJapan Workshop on Integrated Systems in MultiMedia Environments. Las Cruces, NM, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<marker>(Winograd, 1972)</marker>
<rawString>Terry Winograd. Understanding Natural Language. Academic Press, 1972.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>