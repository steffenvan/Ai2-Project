<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985745">
The Development of a Question-Answering Services System for
the Farmer through SMS: Query Analysis
</title>
<author confidence="0.9511985">
Mukda Suktarachan,
Patthrawan Rattanamanee
</author>
<affiliation confidence="0.9217455">
Department of Computer Engineer-
ing, Kasetsart University, Bangkok,
</affiliation>
<address confidence="0.952907">
Thailand, 10900
</address>
<email confidence="0.980076">
naist_da_da@yahoo.com,
tiptop317@hotmail.com
</email>
<author confidence="0.968799">
Asanee Kawtrakul
</author>
<affiliation confidence="0.900325">
Department of Computer Engineering, Kasetsart
University, Bangkok, Thailand, 10900
National Electronics and Computer Technology
Center, Thailand
</affiliation>
<email confidence="0.97778">
asanee_naist@yahoo.com
asanee.kawtrakul@nectec.or.th
</email>
<sectionHeader confidence="0.998398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995489066666667">
In this paper, we propose the development of
the Question-Answering Services System for
the Farmer, through SMS, by focusing on
query analysis and annotation based on a simi-
lar technique previously applied to language
generation, thematic roles, and primitive sys-
tems of the Lexical Conceptual Structure
(LCS). The annotation places emphasis on the
semantics model of “What” and “How” que-
ries, lexical inference identification, and se-
mantic role, for the answer. Finally, we show
how these annotations and inference rules con-
tribute to the generalization of the matching
system over semantic categories in order to
have a large scale question-answering system.
</bodyText>
<sectionHeader confidence="0.96042" genericHeader="categories and subject descriptors">
1 Challenges and Goals
</sectionHeader>
<bodyText confidence="0.999952645833334">
In the era of Information and Communications
Technology (ICT), mobile is a fast and conven-
ient way to communicate over a network.
Knowledge service via a mobile as “a right in-
formation for a right man” is a challenging task.
However, this means of interchange between
persons has the limitation of personal timing.
Therefore, Short Message Service (SMS) is a
better way for giving knowledge service, espe-
cially automatic interchange of short text mes-
sages, by providing the information from an
automatic Question &amp; Answering System.
From the results of the statistical ICT data
survey concerning the number and percent of the
population 6 years of age and over who use in-
formation and communication technology: 2003
- 2007 by the National Statistical Office1, Thai-
land, it was found that 47.2% of people in the
entire kingdom have owned their mobile(s).
Consequently, communicating via SMS facili-
tates an effective knowledge service for support-
ing the farmers in problem-solving, decision
making, and early warning, and also supports the
government, or a related organization, in order to
e-communicate to the farmer by changing the
model of “Training and Visit” to e-service and
changing the collective to support cooperative
problem solving. This kind of communication
will provide the necessary long-term cost reduc-
tions to the agricultural economy in the areas of
travel, visiting, productivity, etc.
Nowadays, providing a knowledge service
through SMS is not limited to only a Question-
Answering Services System, but also for such
one-way services as early warning systems, for
example, a Tsunami Alert System2, a FloodSMS
– Early Detection and Warning of Catastrophic
Flooding via SMS3, etc.
The development of a Question-Answering
Services System through SMS is not the design
of a new technology. There have been several
theories developed earlier, in the context of NLP
or cognitive sciences, such as Natural Language
Information Retrieval (NLIR), rule based Q&amp;A,
etc. Nevertheless, some former theories of Q&amp;A
relied on complex semantic information. For in-
stance, a Wireless Natural Language Search En-
gine [6] was implemented using a system resid-
</bodyText>
<footnote confidence="0.9571214">
1 http://web.nso.go.th/en/survey/keystat/keystat08.pdf
2 http://www.wap.ait.ac.th/tsunami.html
3 http://www.netsquared.org/projects/floodsms-
%E2%80%93-early-detection-and-warning-
catastrophic-flooding-sms
</footnote>
<page confidence="0.951166">
3
</page>
<note confidence="0.9997665">
Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 3–10,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999911946428571">
ing on a server, which can translate questions or
phrases into search engine queries or queries to
SOAP Web services, where a gateway mediates
between the mobile network and the Internet.
Also, [15] developed the SMS for Question-
Answering in the m-Learning Scenario System
by using the Simple Matching Algorithm to
match the learners’ answer messages with the
original answer string, thus facilitating the learn-
ers to get the necessary feedback and assessment.
In this paper, we propose the development of
the Question-Answering Services System for the
Farmer through SMS by focusing on query
analysis and annotation, as well as on selected
text matching utilizing lexical inference and se-
mantic roles. The annotation emphasizes the se-
mantics model of “What” and “How” queries.
Finally, we show how these annotations and in-
ference rules contribute to the generalization of
the matching system over semantic categories in
order to have a large scale question-answering
system.
In the current stage, we have designed Q&amp;A
schema with thematic roles and have borrowed
some primitive systems of the Lexical Concep-
tual Structure (LCS). Also, we are annotating
1000 questions and text related to the query (but
we randomly choose 100 pairs of Q&amp;A for the
experiment). In the same time, we are generaliz-
ing inference rules in order to match a question
to its answer. This is particularly crucial when
there is no straightforward response, e.g. when
they require some form of lexical inference,
elaboration, and reasoning or when the response
is not a simple item, but a well-formed fragment
of text, e.g. a chain of events leading to a conse-
quence, a procedure, etc.
The project we present here emerged from a
need of the real end-users, the Agricultural Land
Reform Office, Ministry of Agriculture and Co-
operative, Thailand, in the project of ALRO Cy-
berBrain [3], which is a social network frame-
work that combines approaches based on knowl-
edge science and engineering with language en-
gineering, consisting of an ontology-based search
engine, information extraction for Q&amp;A system,
knowledge aggregation through a knowledge
portal and visualized in a browser with semantic
links between problems, methods of problems
solving and man who is the problem solver
(PMM map Model) [1]. The main goal is to de-
velop tools for e-Farming, in particular rice farm-
ing, so that farmers can easily get information on
farming rice and rice diseases. Now, it has been
extended to provide question-answering services
for the farmers through SMS [2].
</bodyText>
<sectionHeader confidence="0.894808" genericHeader="method">
2 Problem Statements
</sectionHeader>
<bodyText confidence="0.999966666666667">
There are two main problems in Q&amp;A analy-
sis: semantic interpretation for a question word
and answer identification.
</bodyText>
<subsectionHeader confidence="0.974134">
2.1 Question’s Semantic Roles
2.1.1 Question Word Interpretation.
</subsectionHeader>
<bodyText confidence="0.9999668">
In general, when we query for the answer by a
traditional search engine system, we might get
many answers at different levels, depending on
the role of the question: Definition vs Fact or set
of Facts. For example, with the question
</bodyText>
<equation confidence="0.996102">
Q1: “TTf1|1&amp;quot;a,A|f1�fl|fl_-1T”
Rice |Blast |is |what
</equation>
<bodyText confidence="0.78854165">
“What is a Rice Blast?”
The answer can be returned as the definitions,
fact or a set of facts, which are:
A1.1: Blast, also called rotten neck, is one of the
most destructive diseases of Missouri rice.
Blast does not develop every year but is
very destructive when it occurs.4
A1.2: Disease of Leave Burnt caused by Pyricu-
laria Oryzae can destroy all rice growing
period from start until harvest period.5
The answer can be returned as the characteris-
tics detail or set of facts, such as the following:
A1.3 : Blast symptoms can occur on leaves,
leaf collars, nodes and panicles. Leaf spots are
typically diamond shaped, with gray- white cen-
ters and brown to red-brown margins. Fully de-
veloped leaf lesions are approximately 0.4 to 0.7
inch long and 0.1 to 0.2 inch wide. Both the
shape and color vary depending on the environ-
ment, age of the lesion and rice variety.6
</bodyText>
<subsectionHeader confidence="0.991909">
2.1.2 Variety of Question Forms.
</subsectionHeader>
<bodyText confidence="0.9999835">
In natural language, the question can be asked
with different words and styles, for example:
</bodyText>
<equation confidence="0.739463333333333">
Q2.1:�fl�tlld|fl&amp;quot;155d9J&amp;quot;1G1|11fl4|�5�1|�YT7�|l7J�ll|flU�&amp;quot;14�5
situation |outbreak |of |disease |Rice Blast|
is |how
“What is the situation of Rice Blast?”
Q2.2:fl&amp;quot;155d9J&amp;quot;1G1|11fl4|�5�1|�YT7�|7�|�fl�tlld|flU�&amp;quot;14�5
outbreak |of |disease |Rice Blast |is |characteristic |how
</equation>
<footnote confidence="0.8971135">
“How does the rice blast outbreak look like?”
4 http://aes.missouri.edu/delta/muguide/mp645.stm
5 http://www.sotus.co.th/article_4.html
6 http://aes.missouri.edu/delta/muguide/mp645.stm
</footnote>
<page confidence="0.962835">
4
</page>
<equation confidence="0.6747925">
Q2.3:โรค|ไหม|ระบาด|ได|อยางไร
Rice Blast |disperse |able |how
</equation>
<bodyText confidence="0.984985">
“How can the rice blast disperse?”
The reply can be returned the same answers
with a descriptive set of events, as the following:
A2.1: To prevent the Rice Blast: for the places
that we often found the disease, use the dis-
ease-resistant rice variety. Don&apos;t sow the
rice seed too densely. Don&apos;t use to much Ni-
trogen. If it is severe outbreak and it is the
state of young plant, plow and sow again. If
it was the epidemic state, use Fungus-
Removal chemical as Carbendasim.
A2.2: Brown spot may be reduced by balanced
fertilization, crop rotation, and the use of
high quality planting seed. Seed treatment
fungicides reduce the incidence and severity
of seedling blight caused by this fungus.
The examples above show that using different
verbs or noun phrases can be represent the same
meaning. Moreover, there is non-correspondent
focus word between Q and A.
</bodyText>
<subsectionHeader confidence="0.998379">
2.2 Answer Type Identification
</subsectionHeader>
<bodyText confidence="0.660004615384615">
2.2.1 Ambiguity between subtopic and an-
swer form
To identify the answer, sometimes there is an
ambiguity that verb phrases occurring after the
focus word of the question can be both subtopics
and the answer, like a procedural answer, for
example,:
Q3: วิธีการ|ปองกัน|โรค|ไหม|ทํา|ได|อยางไร
method |control |Rice Blast |to do |how
“What method can be used to control Rice
Blast?”
้
A3: วิธีการ|ปองกัน|โรค|ไหม |มี|ดังนี
</bodyText>
<listItem confidence="0.821787666666667">
method |control |Rice Blast |have |such as
“Methods for preventing the Rice Blast are:”
• ใช|สารเคมี|ที่|เหมาะสม
use |Chemical Substance  |that |appropriate
“Use appropriate Chemical Substance.”
• ใช|พันธุ|ที่|เหมาะสม
</listItem>
<bodyText confidence="0.509901">
use |type of rice  |that |appropriate
“Use appropriate type of rice.”
</bodyText>
<listItem confidence="0.95625525">
• ใช|กลไก|ใน|การ|ปองกัน
use |mechanism |in |prevent
“Use mechanism to prevent.”
• ใช|วิธีการ|ผสมผสาน
</listItem>
<bodyText confidence="0.812162666666667">
use |methods |hybrid
“Use hybrid methods.”
The examples above convey the 4 types of
method for Rice Blast control or names of meth-
ods, but it is not the process or the answers that
represent how to control the disease.
</bodyText>
<subsubsectionHeader confidence="0.474823">
2.2.2 Non-correspondence between Q &amp; A:
</subsubsectionHeader>
<bodyText confidence="0.9999855">
Sometimes, the question and answer were not
matched because the clue words or focus words
in the question have never appeared in the an-
swers. This makes the question not correspond to
the answer and also causes difficulty in finding
the expected answer. For example,
</bodyText>
<equation confidence="0.983789555555556">
้
Q4:
can |control |pests  |rice  |these  |How
“How can these rice pests be controlled?”
้
A4:
ประการ|รวม|กัน ||คือ ||ปลูก|ขาว|พันธุ|ตานทาน|โรค|, ||ปลูก|
ขาว|ตาม|ฤดูกาล|, |ควบคุม|ระดับ|น้ํา|ใน|นา |และ|ไม|ควร|ใช|
สาร|ฆา|แมลง|ที่|ทํา|ให|เกิด|การ|เพิ่ม|ระบาด||ของ|โรค|
</equation>
<bodyText confidence="0.999348">
“These pests can be managed through inte-
grated approach including sowing insect re-
sistant rice varieties, sowing rice crop at rec-
ommended time, proper water management
conservation and augmentation of bio-control
predators.”
From the example, the focus word of the
question is “control,” but there is no word “con-
trol” in the answer. For this kind of Q&amp;A match-
ing solution, WordNet and ontology are neces-
sary.
</bodyText>
<sectionHeader confidence="0.953467" genericHeader="method">
3 Outline of the Project and Methodol-
ogy
</sectionHeader>
<bodyText confidence="0.999952388888889">
The needs of the Thai Ministry of Agriculture
have been specified in a simple way via a corpus
composed of (1) questions raised in real life by
farmers (about 1000 questions), (2) the responses
which have been provided by experts, based on
existing documents (possibly several responses
per question) and, quite often, (3) the texts they
originate from. In general, the response is found
in a unique text: there are no multiple answers,
since most texts are not redundant, although
some responses, in particular complex (e.g.
evaluative questions) or indirect ones, may in-
volve the taking into account of several inde-
pendent texts. We will not address here the prob-
lem of message length reduction so that it fits
into an SMS format (although this is also an im-
portant semantic problem).
The system overview is shown in Figure 2.
</bodyText>
<page confidence="0.980536">
5
</page>
<figureCaption confidence="0.999721">
Figure 2. System Architecture
</figureCaption>
<bodyText confidence="0.924571117647059">
To develop a Thai QA system, the preprocess-
ing of Thai morphology and syntax is necessary.
The NAIST lab at the University of Kasetsart has
basic tools to manage morphological analysis,
parts-of-speech recognition, simple syntactic
analysis, as well as Thai parsing, and an Element
Discourse Unit System (EDU). These tools were
designed as basic tools in natural language proc-
essing applications. (accessible on
http://vivaldi.cpe.ku.ac.th:9292/ with a recom-
mendation to use the Mozilla Firefox browser)
A few examples of question-answer pairs are:
Q5: วิธีการ|ปองกัน|กําจัด|ขาว|วัชพืช|ทํา|ได|อยางไร
“How to prevent the Weedy rice”
A5.1: ควร|เวน|ปลูก|ขาว|บาง|ฤดู
“Skip some seasons when growing rice,”
A5.2: ปลูก|พืช|อายุ|สั้น|อื่น
</bodyText>
<equation confidence="0.864830666666667">
“Grow hydrotonics plants.”
Q6: โรค|ใบ|ขีด|โปรง|แสง|มี|วิธีการ|ควบคุม|อยางไร
“How to control the Bacterial Leaf
Streak Disease”
A6: ไม|ควร|ใส|ปุย|ไนโตรเจน|มาก|เกินไป|
“Do not put too much Nitrogen.”
้
Q7: ถา |พบ|เพลีย |ไฟ|ระบาด ||สามารถ|กําจัด|ดวย|สาร|
ฆา|แมลง|ชนิด|ใด
“How to eradicate the rice thrips”
A7: ฉีด|พน|ดวย|มาลาไทออน|หรือ|คารบาริล|ทุก|อาทิตย ||
ใส|ปุย|และ|ให|น้ํา|ทุก ||ๆ ||สอง|วัน
</equation>
<bodyText confidence="0.99805775">
“Spray with Malathion or Carbaryl
every week, add fertilizer and water
every two days.”
Questions are essentially factoid questions
(e.g. best periods for rice planting, rice varieties
suggestion, symptoms of a disease), why ques-
tions, where responses are chains of events (rea-
sons for something to happen) and a large num-
ber of procedural questions [4], in particular for
treating diseases. There are relatively few com-
parative or evaluative questions besides general
questions, such as: What are the major rice
pests?
In most cases, questions do not have responses
which can be immediately found in the texts by
standard term matching techniques. For example:
“How does the Sheath Blight affect the rice
growth?” has the following response in a text:
Plants heavily infected at these stages produce
poorly filled grain, particularly in the lower por-
tion of the panicle. Additional losses result from
... Therefore, some lexical semantics devices
(e.g. a semantic link between affect and infect) or
more elaborated reasoning schemas, based on
domain knowledge, are needed to allow appro-
priate question-text matching [11, 7]. The kind of
domain knowledge at stake may be quite unex-
pected (i.e., not the main topics that everyone
knows, but more subtle pieces of information, as
will be seen in 4.3). This is the major challenge
of this work, which we try to resolve via a full
annotation of the matching process, from ques-
tion parsing to response production, identifying
matching and reasoning aspects.
Complex questions may, e.g., require the
elaboration of a diagnosis from premises given in
the question before finding the response, either
factoid or procedural (My rice has weedy leaves
and some yellow spots, what should I do?). This
question requires one to select all texts where
such a symptom is identified, and then, e.g., to
enter into a dialogue with the user if there are
several possible diagnoses, leading to different
treatments.
The second aspect of this problem is to be able
to extract the complete text portion that responds
to the question. For that purpose we are develop-
ing an annotation methodology whose goal is to
identify the different processes at stake and the
needed resources. This method allows us to iden-
tify relevant text portions and then to delimit
them appropriately.
</bodyText>
<sectionHeader confidence="0.9980185" genericHeader="method">
4 The Question-Answering Process An-
notation
</sectionHeader>
<bodyText confidence="0.999112">
Since the task is quite large (a large group of stu-
dents are annotating a set of 600 questions and
related texts), we need to establish norms and
annotation guidelines. Using the research con-
ducted at IRIT on annotating procedural ques-
tions and instructions based on semantic roles
</bodyText>
<figure confidence="0.998657375">
Preprocessing Process
Name Entities Recognition
Word Segmentation
EDU Segmentation
POS Tagging
WordNet
Ontology
Question Analysis
and Annotation
Q&amp;A Matching and
Answer Generation
Question
Text Analysis and Annotation
Titles Recognition
Document Indexing
Text Annotation
</figure>
<page confidence="0.969855">
6
</page>
<bodyText confidence="0.999977">
(TextCoop project) and a few rhetorical relations
(e.g. elaboration, example, explanation), we first
annotated the questions and their corresponding
responses in texts provided by the Thai Ministry
of Agriculture. One of the challenges was to
identify relevant linguistic marks or patterns [9,
10, 14].
There are many attempts to annotate argu-
ments by means of primitives; our approach,
here, is oriented towards the precise task at stake
and the specific actions. Therefore roles are not
as standard as they are in general. An earlier at-
tempt with a similar technique applied to lan-
guage generation was carried out in, e.g., [10, 7].
Semantic tags are either close to thematic roles
(instrument, location, etc.) [8], or borrowed from
the primitive systems of the Lexical Conceptual
Structure (LCS) [13], in particular, to establish
useful links between arguments or between a
large variety of constituents, which thematic
roles cannot do. For example, in the first Thai
university we have a link between &apos;first&apos; and &apos;Thai
university&apos; which is either loctemp or loc+char+ident,
depending on the interpretation of first (oldest or
the best). However, in a majority of cases, se-
mantic roles based on thematic roles have a suf-
ficient granularity, and these are the ones which
are used in the examples in 4.1.
The main roles we consider are: agents (for
humans and animals like insects, and metaphori-
cally for diseases and natural forces), themes
(undergoing actions, basically plants and soils,
and artificial products), location (spatial), time
(covering dates and also periods), instruments
(from tools to chemical products), manners,
means, conditions (under which to realize an ac-
tion, or related to observation e.g. of a disease),
cause, goals, and results.
Besides, the tags &lt;action&gt;...&lt;/action&gt; or
&lt;fact&gt;...&lt;/fact&gt; were considered to tag the verb
with it’s arguments or adjuncts.
In the remainder of this section we briefly re-
port the different steps of the process as they
stand at the moment, i.e. almost at the end of the
experimental stage, before automating knowl-
edge acquisition, and implementing the applica-
tion.
</bodyText>
<subsectionHeader confidence="0.999145">
4.1 Dealing with Questions
</subsectionHeader>
<bodyText confidence="0.998940181818182">
As in most systems dealing with complex types
of questions, questions are represented by a tri-
ple: the question type (which can be in our case
polymorphic), the question focus (usually an NP
or a VP in case events or procedures are induced)
and the question body, annotated by means of
semantic roles, as indicated above.
The main types of questions we have identi-
fied from our corpus are the following; they are
quite different from standard classifications, but
they correspond to more operational views:
F: fact, with subtypes: temp (temporal, time,
date), loc (location) or product,
E: an event (with a subtype event: cause)
SF: set of facts
SE: set of events (not related, and without any
form of sequence: different from SqE be-
low)
PROC: procedure, more or less complex, it
may be just a single instruction; it can also
describe the use of an instrument.
SqE: sequence of events, which follow each
other.
EVAL: evaluation, making value decisions
about issues or resolving controversies or
differences of opinion.
DEF: definition, the description of object.
Some questions may bear several non-
conflicting types, in particular when the nature of
the response is not straightforward to determine
from the question. For example, “What is the
symptom of Bakanae?” would get the types SF
and SE.
An annotated question is, for example:
&lt;question type=“ SF or SE” focus=“ symptom of
Bakanae”&gt; What &lt;fact&gt; is &lt;theme&gt; the symp-
tom of Bakanae &lt;/theme&gt; &lt;/fact&gt; ? &lt;/question&gt;
As can be noted, the response is the set of
those facts that contribute, together or independ-
ently, to the spreading of the disease.
By the observation from 100 random inter-
rogative sentences corpus analysis, we found that
the semantic types of questions correspondent to
the question words are the following
</bodyText>
<table confidence="0.998515666666667">
Q-Types What When Where Why Who Which How
F 11 6 1 1 9 2
E 1 6
SF 3 15 3
SE 7 2 5
PROC 15
SqE 7
EVAL 2 1
DEF 3
</table>
<tableCaption confidence="0.977963">
Table 1 the correspondence between questions
</tableCaption>
<bodyText confidence="0.996455">
and semantic types of questions
From Table 1, it is clear that “what” and
“how” questions vary in types of question, be-
cause they have many forms to use, for example,
“how + verb to be + noun”, “how + do(es) +
noun + verb”, “how to”, “how can”, etc. or
“what + verb to be + noun”, “what + noun +
auxiliary verb”, etc. This is why we point out the
“What” and “How” questions.
</bodyText>
<page confidence="0.997899">
7
</page>
<subsectionHeader confidence="0.758701">
4.2 Dealing with texts: document indexing
and associated annotations
</subsectionHeader>
<bodyText confidence="0.999928685714286">
Texts are initially indexed based on the main
terms they contain which are relevant w.r.t. the
questions given in the corpus. Our representation
resembles a frame approach, but it is more
flexible since there is no predefined structure to
represent indexes. This is more in accordance
with the variety of texts in terms of contents. In-
dexes basically are formed from:
− Top-level terms that structure the domain:
for example, concepts like symptom,
spreading, treatment, time, place, effect,
etc. where predicative (action terms) terms
as well as entities are found,
− relatively generic terms, found in the
questions and structured in the domain on-
tology: water, clean, control, eradicate,
etc., which are organized w.r.t. the top
concepts above,
− named entities, typed as: disease names,
location names, chemical product names,
bacteria names, etc.
In our representation, those generic terms (and
near synonyms) are represented as predicates,
while arguments are represented as attribute-
value pairs (or attributes alone), include typed
name entities and any kind of terms besides the
generic terms.
Indexes are associated with texts in the text
database. Indexes must remain general so that
indexing is fast and as reliable as possible. The
idea is that when a question is uttered, a small
number of texts are first selected on the basis of
the indexes for further analysis. An example
below can be indexed and annotated [2] as the
following:
</bodyText>
<construct confidence="0.9789589">
Index: disease-name (Bakanae), symptoms (disease: Bakanae),
origin (disease: Bakanae, place: California, date: 1999), spread-
ing(disease: Bakanae, period: winter, medium: [soil, water]), treat-
ment(disease: Bakanae, product).
&lt;title type=“goal” level=“1” &gt; Rice Bakanae &lt;/title&gt;
&lt;title type=“goal” level=“2”&gt;SYMPTOMS &lt;/title&gt;
&lt;task type = “SF”&gt;
&lt;theme&gt;Symptoms of Bakanae&lt;/theme&gt; first appear about a
month after planting. Infected seedlings appear to be taller, more
slender, and slightly chlorotic ... The rapid elongation of infected
plants is caused by the pathogen’s production of the plant hormone,
gibberellin &lt;/task&gt;
&lt;title type=“goal” level=“2”&gt;COMMENTS ON THE DISEASE
&lt;/title&gt;
Bakanae is one of the oldest known diseases of rice in Asia but has
only been observed in California rice since 1999 and now occurs in
all California rice-growing regions. While very damaging in Asia,
the extent to which Bakanae may effect California rice production
is unknown. As diseased plants .....
&lt;title type=“goal” level=“2”&gt;MANAGEMENT&lt;/title&gt;
&lt;task type = “PROC”&gt;
The most effective means to&lt;action&gt; treat &lt;theme&gt; this disease
&lt;/theme&gt; &lt;/action&gt; is the &lt;instruction compound&gt;&lt;instruction
type=&amp;quot;imperative&amp;quot;&gt;use of noninfested seed&lt;/instruction&gt;.
Also,&lt;connector type=&amp;quot;advice&amp;quot;&gt; when possible&lt;/connector&gt;, &lt;ad-
vice&gt;burning plant residues&lt;/advice&gt; with known infection in fall
may help limit the disease. Field trials indicate that a seed treat-
ment with sodium hypochlorite (Ultra Clorox Germicidal Bleach) is
effective at reducing the incidence of this disease.... &lt;/instruction
compound&gt;&lt;/task&gt;
</construct>
<subsectionHeader confidence="0.874882">
4.3 Matching selected texts with questions:
the deep indexing level
</subsectionHeader>
<bodyText confidence="0.985616386363637">
The main words of the question focus and body
are used to select a subset of indexed texts as
potential candidates containing the response.
Then, in each of these texts, the few sentences
where the terms of the question or derived terms
(closely related terms) are effectively found are
annotated by means of semantic roles as for the
question, for further analysis and investigations.
For that purpose, we have developed guide-
lines for annotating those text fragments where
the response is and the associated knowledge,
based on the same semantic roles as those used
in the questions. These annotations remain so far
exploratory, in terms of feasibility and automa-
tion. Our major concern is to develop a method
for annotators so that a large number of texts can
be tagged homogeneously and also so that the
technique can be reproduced for other technical
areas. Finally, in terms of response identification,
the goal is to define a metric that defines the best
match and selects the text fragment(s) that best
respond(s) to the question among several poten-
tial candidates.
Let us first consider a simple example. Given
the question:
Q8: “How to eradicate Bakanae ?”
with the following representation:
&lt;question type=“PROC or SqE” focus “eradi-
cate Bakanae” &gt; How to &lt;action&gt; eradicate
&lt;theme&gt; Bakanae &lt;/theme&gt; &lt;/action&gt; ?
&lt;/question&gt;
The main terms of the question are ‘eradicate’
and ‘Bakanae’. The text above is therefore se-
lected on the basis of its indexes, because ‘treat-
ment’ is a closely related term (in terms of se-
mantic relation: ‘way to realize an event’) of
‘eradicate’ in the domain ontology.
Then, the question terms are searched in the
selected text and the sentences that contain them
are annotated using semantic roles. For example,
the following sentence is a candidate:
The most effective means to treat this disease
is the use of noninfested seeds.
It is tagged as:
</bodyText>
<page confidence="0.995298">
8
</page>
<bodyText confidence="0.965653290322581">
...&lt;action&gt; treat &lt;theme&gt; this disease &lt;/theme&gt;
is the use of &lt;instrument&gt; noninfested seeds
&lt;/instrument&gt; &lt;/action&gt; .
The answer is the above sentence and the text
fragment that follows (introduced by the connec-
tor also) since the response is of type procedure:
The most effective means to treat this disease
is the use of noninfested seed. Also, when possi-
ble, burning plant residues with known infection
in fall may help limit the disease.
Following [5], this structure is annotated as a
single instructional compound, which is the fun-
damental unit in a procedural text. This is the
structure which is typically returned to users.
Let us present here another illustrative exam-
ple of a text fragment where the response is an-
notated together with the required related reason-
ing elements:
Q9: “How can thrips destroy the rice ?”
annotation:
&lt;question type=“SqE” focus = ”destroy”&gt;How
can &lt;agent&gt; thrips &lt;/agent&gt; &lt;action&gt;destroy
&lt;theme&gt;the rice&lt;/theme&gt; &lt;/action&gt;?&lt;/question&gt;
The text fragment that corresponds to the an-
swer is annotated as follows:
&lt;response&gt; &lt;agent&gt; The rice thrips&lt;/agent&gt;
&lt;action&gt; sucks the sap &lt;source&gt; from the young
plant. &lt;/source&gt; &lt;/action&gt; &lt;/response&gt;
To match the action ‘destroy’ in the question
with the text portion from which the response is
extracted, it is then necessary to identify the in-
ference:
&lt;lex_inference&gt; &lt;action&gt; Suck sap of X
&lt;/action&gt; &lt;entail&gt; &lt;modality&gt; probably
&lt;/modality&gt; &lt;action&gt; destroy X &lt;/action&gt;
&lt;/entail&gt; , &lt;type&gt; X : plant &lt;/type&gt;
&lt;part-of&gt; sap : X &lt;/part-of &gt; &lt;/lex inference&gt;
This example shows that (1) in the question
and in the answer, annotations are used to iden-
tify the different components, arguments, ad-
juncts, but also some other components (e.g.
temporal adverbs), and (2) the annotation is de-
veloped to characterize the matching steps and
inferential components (either lexical or domain
knowledge) between the question and the an-
swer. This latter form of annotation, which is
quite time-consuming to develop, is the means
we use to induce and develop domain dependent
forms of lexical inference (or other phenomena
like synonymy, lexical equivalence, etc.) and
relevant domain knowledge. The types and lexi-
cal functions which are introduced are then used
in the process of induction of generalizations
over some semantic categories (plants, products,
etc.), and verb classes. This way of annotating
knowledge and inferences is obviously a simple
bottom-up process, with well known limitations,
but we feel it may have some advantages for in-
ducing an upper organization of knowledge, in
conjunction, and as a complement to, the domain
ontology. It is also simple and accessible to an-
notators. Obviously this remains to be evaluated.
</bodyText>
<subsectionHeader confidence="0.961721">
4.4 Generalizing inferences for question-
answer matching
</subsectionHeader>
<bodyText confidence="0.997434821428571">
At this level, the inferences which may be drawn
are directly attached to the terms which are
tagged. This is obviously too limited. We are
now experimenting with different generalization
strategies in order to tune the lexical inference
rules. This process involves:
(1) developing various generic principles over
different types and categories (via the domain
ontology), We will annotation the title for match-
ing the “theme” of the answer to the “theme” and
“Focus” of the question by using word net and
ontology as shown below.
Surface Form Concept
destroy, destruct, eliminate, kill,... destroy
treat, prevent, eradicate, protect,... manage
suck, eat, bite, drink,... consume
spread out, diffuse, disperse,... spread
(2) a set of principles that limit these generali-
zations via, for example, the taking into account
of the semantics restrictions imposed by lexical
items, in particular verbs. The main words of the
question focus and text body that already anno-
tated will be considered for extracting the poten-
tial candidates containing the response. The sen-
tences, where the terms of the question or de-
rived terms (closely related terms) are effectively
found, will be the corresponding answer by using
matching function as shown below.
</bodyText>
<figure confidence="0.941290882352941">
Function Matching (Question Q, Answer A){
Match = false;
// Relevant document
If (Q.focus = A.index) then
// Relevant answer
If (Q.type = A.task type) then
//Detect Answer for the Question
If (Q.focus = A.title) then
Match = true;
Else if (Q.action = A.action and
Q.theme = A.theme or
Q.agent = A.agent) then
Match = true;
End If
End If
End If
Return Match;}
</figure>
<bodyText confidence="0.9997305">
The tuning of the level of these generalizations
is obviously one main parameter of our project.
It has several conceptual dimensions that we ex-
plore and may also be domain dependent.
</bodyText>
<page confidence="0.988584">
9
</page>
<subsectionHeader confidence="0.517574">
Perspectives
</subsectionHeader>
<bodyText confidence="0.999972857142857">
The matching problem between questions and
documents to retrieve answers in question-
answering systems in concrete applicative con-
texts is often a difficult problem. This matching
procedure often requires very accurate domain
knowledge, besides ontological descriptions. It is
not always easy to access this knowledge in a
structured way or to extract it from texts. The
present contribution, still experimental and in an
early stage of development, is an attempt, via
annotations, at resolving this problem, following
a simple and clear methodology.
This task needs to be developed and evaluated
gradually. So far, it is too early to evaluate the
quality of the generalizations and the inferential
patterns we get.
This approach, and the principles we have
briefly outlined, allow us to introduce a working
method for the development of question-
answering systems for concrete applications, es-
pecially for non-factoid questions, an area which
is still not very much developed in spite of its
obvious usefulness. One of the reasons is that
non-factoid questions require a language proc-
essing technology, analysis methods, reasoning
aspects, and a conceptual approach, which are
substantially different from what is used for fac-
toid questions.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999734454545455">
The work described in this paper has been sup-
ported by the NECTEC No. NT-B-22-KE-12-50-
19, within the project, “I-KnowII: CAT, EAT,
RATs,” and “Agricultural Question &amp; Answer-
ing Service System,” granted by the KURDI,
Kasetsart University. We would like to espe-
cially thank Prof. Patrick Saint Dizier for origi-
nating, advising and collaborating in the devel-
opment of Q&amp;A system. We also thank Prof.
William I. Grosky for helping to revise our Eng-
lish.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999972393939395">
1. Asanee Kawtrakul, et al. Chaveevan Pechsiri, Sa-
chit Rajbhandari, Frederic Andres, Problems-
Solving Map Extraction with Collective Intelli-
gence Analy-sis and Language Engineering , Book
Chapter 18, Medical Information Science Refer-
ence in Information Retrieval in Biomedicine
ISBN: 978-1-60566-274-9; pp 460
2. Asanee Kawtrakul, et al. 2009. From CyberBrain to
Q&amp;A Services: A Development of Question - An-
swering Services System for the Farmer through
the SMS, WCCA2009, Grand Sierra Resort, Reno,
Nevada, USA.
3. Asanee Kawtrakul, et al. 2008. “CyberBrain: To-
wards the Next Generation Social Intelligence”
IAALD AFITA WCCA 2008, Tokyo, Japan.
4. Dan Moldovan, Sanda Harabagiu, Marius Pasca,
Rada Mihalcea, Roxana Girju, Richard Goodrum,
Vasile Rus. 2000. The Structure and Performance
of an Open-Domain Question Answering System,
Proceedings of the 38th Meeting of the Association
for Computational Linguistics (ACL), Hong Kong.
5. Estelle Delpech, Patrick Saint-Dizier. 2008. Inves-
tigating the Structure of Procedural Texts for An-
swering How-to Questions, LREC2008, Marra-
kech.
6. Jochen L. Leidner, 2005. A wireless natural lan-
guage search engine. Proceedings of the 28th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval
table of contents: 677 – 677, ACM, New York,
USA
7. Judy Delin, Anthony Hartley, Cecile Paris, Donia
Scott, Keith Vander Linden. 1994. Expressing Pro-
cedural Relation-ships in Multilingual Instructions,
Proceedings of the 7th International Workshop on
Natural Language Generation: 61-70, Maine, USA.
8. Karen Sparck Jones, Branimir Boguraev.1987. A
note on a study of cases, research note in Computa-
tional Linguistics archive, Volume 13 , Issue 1-2
(January-June 1987) : 65 - 68.
9. Leonard Talmy. 1976. Semantic Causative Types,
In M. Shibatani (ed.), Syntax and Semantics 6: The
Grammar of Causative Constructions. New York:
Academic Press: 43-116.
10. Leonard Talmy. 1985. Lexicalization Patterns:
Seman-tic Structure in Lexical Forms, in Language
Typol-ogy and Syntactic Description 3: Grammati-
cal Categories and the Lexicon, T. Shopen(ed.),
57-149, Cambridge University Press.
11. Mark Thomas Maybury. 2004. New Directions in
Question Answering, The MIT Press, Menlo Park.
12. Mineki Takechi, Takenobu Tokunaga, Yuji Ma-
tsumoto, Hozumi Tanaka. 2003. Feature Selection
in Categorizing Procedural Expressions, The 6th
International Workshop on Information Retrieval
with Asian Languages (IRAL2003):49-56.
13. Ray Jackendoff. 1990. Semantic Structures, MIT
Press.
14. Robert E. Longacre. 1982. Discourse Typology in
Relation to Language Typology, Sture Allen ´ed.,
Text Processing, Proceeding of Nobel Symposium
51, Stockholm, Almquist and Wiksell, 457-486.
15. Sadhu Balasundaram Ramakishnan and
Balakrishnan Ramadoss. 2007. SMS for Question-
Answering in the m-Learning Scena-rio, Journal of
Computer Science 3(2):119-121.
</reference>
<page confidence="0.997801">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.982723333333333">The Development of a Question-Answering Services System for the Farmer through SMS: Query Analysis Mukda</title>
<author confidence="0.968056">Patthrawan</author>
<affiliation confidence="0.955178333333333">of Computer ing, Kasetsart University, Thailand,</affiliation>
<email confidence="0.997831">tiptop317@hotmail.com</email>
<author confidence="0.448897">Asanee</author>
<affiliation confidence="0.999925">Department of Computer Engineering,</affiliation>
<address confidence="0.615308">University, Bangkok, Thailand,</address>
<affiliation confidence="0.961479">National Electronics and Computer Center,</affiliation>
<email confidence="0.979267">asanee.kawtrakul@nectec.or.th</email>
<abstract confidence="0.998196194805195">In this paper, we propose the development of the Question-Answering Services System for the Farmer, through SMS, by focusing on query analysis and annotation based on a similar technique previously applied to language generation, thematic roles, and primitive systems of the Lexical Conceptual Structure (LCS). The annotation places emphasis on the semantics model of “What” and “How” queries, lexical inference identification, and semantic role, for the answer. Finally, we show how these annotations and inference rules contribute to the generalization of the matching system over semantic categories in order to have a large scale question-answering system. 1 Challenges and Goals In the era of Information and Communications Technology (ICT), mobile is a fast and convenient way to communicate over a network. Knowledge service via a mobile as “a right information for a right man” is a challenging task. However, this means of interchange between persons has the limitation of personal timing. Therefore, Short Message Service (SMS) is a better way for giving knowledge service, especially automatic interchange of short text messages, by providing the information from an automatic Question &amp; Answering System. From the results of the statistical ICT data survey concerning the number and percent of the population 6 years of age and over who use information and communication technology: 2003 2007 by the National Statistical Thailand, it was found that 47.2% of people in the entire kingdom have owned their mobile(s). Consequently, communicating via SMS facilitates an effective knowledge service for supporting the farmers in problem-solving, decision making, and early warning, and also supports the government, or a related organization, in order to e-communicate to the farmer by changing the model of “Training and Visit” to e-service and changing the collective to support cooperative problem solving. This kind of communication will provide the necessary long-term cost reductions to the agricultural economy in the areas of travel, visiting, productivity, etc. Nowadays, providing a knowledge service through SMS is not limited to only a Question- Answering Services System, but also for such one-way services as early warning systems, for a Tsunami Alert a FloodSMS – Early Detection and Warning of Catastrophic via etc. The development of a Question-Answering Services System through SMS is not the design of a new technology. There have been several theories developed earlier, in the context of NLP or cognitive sciences, such as Natural Language Information Retrieval (NLIR), rule based Q&amp;A, etc. Nevertheless, some former theories of Q&amp;A relied on complex semantic information. For instance, a Wireless Natural Language Search En- [6] was implemented using a system resid- 1http://web.nso.go.th/en/survey/keystat/keystat08.pdf 2http://www.wap.ait.ac.th/tsunami.html 3http://www.netsquared.org/projects/floodsms- %E2%80%93-early-detection-and-warningcatastrophic-flooding-sms 3 of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP pages 3–10, Singapore, 6 August 2009. ACL and AFNLP ing on a server, which can translate questions or phrases into search engine queries or queries to SOAP Web services, where a gateway mediates between the mobile network and the Internet.</abstract>
<note confidence="0.67172">Also, [15] developed the SMS for Question-</note>
<title confidence="0.643877">Answering in the m-Learning Scenario System</title>
<abstract confidence="0.993896907407407">by using the Simple Matching Algorithm to match the learners’ answer messages with the original answer string, thus facilitating the learners to get the necessary feedback and assessment. In this paper, we propose the development of the Question-Answering Services System for the Farmer through SMS by focusing on query analysis and annotation, as well as on selected text matching utilizing lexical inference and semantic roles. The annotation emphasizes the semantics model of “What” and “How” queries. Finally, we show how these annotations and inference rules contribute to the generalization of the matching system over semantic categories in order to have a large scale question-answering system. In the current stage, we have designed Q&amp;A schema with thematic roles and have borrowed some primitive systems of the Lexical Conceptual Structure (LCS). Also, we are annotating 1000 questions and text related to the query (but we randomly choose 100 pairs of Q&amp;A for the experiment). In the same time, we are generalizing inference rules in order to match a question to its answer. This is particularly crucial when there is no straightforward response, e.g. when they require some form of lexical inference, elaboration, and reasoning or when the response is not a simple item, but a well-formed fragment of text, e.g. a chain of events leading to a consequence, a procedure, etc. The project we present here emerged from a need of the real end-users, the Agricultural Land Reform Office, Ministry of Agriculture and Cooperative, Thailand, in the project of ALRO CyberBrain [3], which is a social network framework that combines approaches based on knowledge science and engineering with language engineering, consisting of an ontology-based search engine, information extraction for Q&amp;A system, knowledge aggregation through a knowledge portal and visualized in a browser with semantic links between problems, methods of problems solving and man who is the problem solver (PMM map Model) [1]. The main goal is to develop tools for e-Farming, in particular rice farming, so that farmers can easily get information on farming rice and rice diseases. Now, it has been extended to provide question-answering services for the farmers through SMS [2]. 2 Problem Statements There are two main problems in Q&amp;A analysis: semantic interpretation for a question word and answer identification. 2.1 Question’s Semantic Roles 2.1.1 Question Word Interpretation. In general, when we query for the answer by a traditional search engine system, we might get many answers at different levels, depending on the role of the question: Definition vs Fact or set of Facts. For example, with the question Rice |Blast |is |what “What is a Rice Blast?” The answer can be returned as the definitions, fact or a set of facts, which are: A1.1: Blast, also called rotten neck, is one of the most destructive diseases of Missouri rice. Blast does not develop every year but is destructive when it Disease of Leave Burnt caused by Pyricularia Oryzae can destroy all rice growing from start until harvest The answer can be returned as the characteristics detail or set of facts, such as the following: A1.3 : Blast symptoms can occur on leaves, leaf collars, nodes and panicles. Leaf spots are typically diamond shaped, with graywhite centers and brown to red-brown margins. Fully developed leaf lesions are approximately 0.4 to 0.7 inch long and 0.1 to 0.2 inch wide. Both the shape and color vary depending on the environage of the lesion and rice 2.1.2 Variety of Question Forms. In natural language, the question can be asked with different words and styles, for example: situation |outbreak |of |disease |Rice Blast| is |how “What is the situation of Rice Blast?” outbreak |of |disease |Rice Blast |is |characteristic |how “How does the rice blast outbreak look like?” 4 http://aes.missouri.edu/delta/muguide/mp645.stm 5 http://www.sotus.co.th/article_4.html 6 http://aes.missouri.edu/delta/muguide/mp645.stm 4 Rice Blast |disperse |able |how “How can the rice blast disperse?” The reply can be returned the same answers with a descriptive set of events, as the following: A2.1: To prevent the Rice Blast: for the places that we often found the disease, use the disease-resistant rice variety. Don&apos;t sow the rice seed too densely. Don&apos;t use to much Nitrogen. If it is severe outbreak and it is the state of young plant, plow and sow again. If it was the epidemic state, use Fungus- Removal chemical as Carbendasim. A2.2: Brown spot may be reduced by balanced fertilization, crop rotation, and the use of high quality planting seed. Seed treatment fungicides reduce the incidence and severity of seedling blight caused by this fungus. The examples above show that using different verbs or noun phrases can be represent the same meaning. Moreover, there is non-correspondent focus word between Q and A. 2.2 Answer Type Identification 2.2.1 Ambiguity between subtopic and answer form To identify the answer, sometimes there is an ambiguity that verb phrases occurring after the focus word of the question can be both subtopics and the answer, like a procedural answer, for example,: method |control |Rice Blast |to do |how “What method can be used to control Rice Blast?” ้ method |control |Rice Blast |have |such as “Methods for preventing the Rice Blast are:” • use |Chemical Substance  |that |appropriate “Use appropriate Chemical Substance.” • use |type of rice  |that |appropriate “Use appropriate type of rice.” • use |mechanism |in |prevent “Use mechanism to prevent.” • use |methods |hybrid “Use hybrid methods.” The examples above convey the 4 types of method for Rice Blast control or names of methods, but it is not the process or the answers that represent how to control the disease. 2.2.2 Non-correspondence between Q &amp; A: Sometimes, the question and answer were not matched because the clue words or focus words in the question have never appeared in the answers. This makes the question not correspond to the answer and also causes difficulty in finding the expected answer. For example, ้ Q4: can |control |pests  |rice  |these  |How “How can these rice pests be controlled?” ้ A4: pests can be managed through integrated approach including sowing insect resistant rice varieties, sowing rice crop at recommended time, proper water management conservation and augmentation of bio-control From the example, the focus word of the question is “control,” but there is no word “control” in the answer. For this kind of Q&amp;A matching solution, WordNet and ontology are necessary. 3 Outline of the Project and Methodology The needs of the Thai Ministry of Agriculture have been specified in a simple way via a corpus composed of (1) questions raised in real life by farmers (about 1000 questions), (2) the responses which have been provided by experts, based on existing documents (possibly several responses per question) and, quite often, (3) the texts they originate from. In general, the response is found in a unique text: there are no multiple answers, since most texts are not redundant, although some responses, in particular complex (e.g. evaluative questions) or indirect ones, may involve the taking into account of several independent texts. We will not address here the problem of message length reduction so that it fits into an SMS format (although this is also an important semantic problem). The system overview is shown in Figure 2. 5 Figure 2. System Architecture To develop a Thai QA system, the preprocessing of Thai morphology and syntax is necessary. The NAIST lab at the University of Kasetsart has basic tools to manage morphological analysis, parts-of-speech recognition, simple syntactic analysis, as well as Thai parsing, and an Element Discourse Unit System (EDU). These tools were as basic tools in natural language processing applications. (accessible http://vivaldi.cpe.ku.ac.th:9292/ with a recommendation to use the Mozilla Firefox browser) A few examples of question-answer pairs are: “How to prevent the Weedy rice” “Skip some seasons when growing rice,” “Grow hydrotonics plants.” “How to control the Bacterial Leaf Streak Disease” “Do not put too much Nitrogen.” ้ “How to eradicate the rice thrips” | “Spray with Malathion or Carbaryl every week, add fertilizer and water every two days.” Questions are essentially factoid questions (e.g. best periods for rice planting, rice varieties suggestion, symptoms of a disease), why questions, where responses are chains of events (reasons for something to happen) and a large number of procedural questions [4], in particular for treating diseases. There are relatively few comparative or evaluative questions besides general questions, such as: What are the major rice pests? In most cases, questions do not have responses which can be immediately found in the texts by standard term matching techniques. For example: does the Sheath Blight affect the rice the following response in a text: Plants heavily infected at these stages produce poorly filled grain, particularly in the lower portion of the panicle. Additional losses result from Therefore, some lexical semantics devices a semantic link between or more elaborated reasoning schemas, based on domain knowledge, are needed to allow appropriate question-text matching [11, 7]. The kind of domain knowledge at stake may be quite unexpected (i.e., not the main topics that everyone knows, but more subtle pieces of information, as will be seen in 4.3). This is the major challenge of this work, which we try to resolve via a full annotation of the matching process, from question parsing to response production, identifying matching and reasoning aspects. Complex questions may, e.g., require the elaboration of a diagnosis from premises given in the question before finding the response, either or procedural rice has weedy leaves some yellow spots, what should I This question requires one to select all texts where such a symptom is identified, and then, e.g., to enter into a dialogue with the user if there are several possible diagnoses, leading to different treatments. The second aspect of this problem is to be able to extract the complete text portion that responds to the question. For that purpose we are developing an annotation methodology whose goal is to identify the different processes at stake and the needed resources. This method allows us to identify relevant text portions and then to delimit them appropriately. 4 The Question-Answering Process Annotation Since the task is quite large (a large group of students are annotating a set of 600 questions and related texts), we need to establish norms and annotation guidelines. Using the research conducted at IRIT on annotating procedural ques-</abstract>
<title confidence="0.965892882352941">tions and instructions based on semantic roles Preprocessing Process Name Entities Recognition Word Segmentation EDU Segmentation POS Tagging WordNet Ontology Question and Annotation Q&amp;A Matching Answer Generation Question Text Analysis and Annotation Titles Recognition Document Indexing Text Annotation</title>
<abstract confidence="0.997119394984326">6 (TextCoop project) and a few rhetorical relations (e.g. elaboration, example, explanation), we first annotated the questions and their corresponding responses in texts provided by the Thai Ministry of Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constituents, which thematic cannot do. For example, in first Thai have a link between &apos;first&apos; and &apos;Thai which is either or on the interpretation of or the best). However, in a majority of cases, semantic roles based on thematic roles have a sufficient granularity, and these are the ones which are used in the examples in 4.1. The main roles we consider are: agents (for humans and animals like insects, and metaphorically for diseases and natural forces), themes (undergoing actions, basically plants and soils, and artificial products), location (spatial), time (covering dates and also periods), instruments (from tools to chemical products), manners, means, conditions (under which to realize an action, or related to observation e.g. of a disease), cause, goals, and results. Besides, the tags &lt;action&gt;...&lt;/action&gt; or &lt;fact&gt;...&lt;/fact&gt; were considered to tag the verb with it’s arguments or adjuncts. In the remainder of this section we briefly report the different steps of the process as they stand at the moment, i.e. almost at the end of the experimental stage, before automating knowledge acquisition, and implementing the application. 4.1 Dealing with Questions As in most systems dealing with complex types of questions, questions are represented by a triple: the question type (which can be in our case polymorphic), the question focus (usually an NP or a VP in case events or procedures are induced) and the question body, annotated by means of semantic roles, as indicated above. The main types of questions we have identified from our corpus are the following; they are quite different from standard classifications, but they correspond to more operational views: fact, subtypes: temp (temporal, time, date), loc (location) or product, an event a subtype event: cause) SF: set of facts set of events related, and without any form of sequence: different from SqE below) procedure, or less complex, it may be just a single instruction; it can also describe the use of an instrument. sequence of events, follow each other. evaluation, value decisions about issues or resolving controversies or differences of opinion. definition, description of object. Some questions may bear several nonconflicting types, in particular when the nature of the response is not straightforward to determine the question. For example, is the of Bakanae?” get the types SF and SE. An annotated question is, for example: type=“ or focus=“ symptom of What &lt;fact&gt; is &lt;theme&gt; the sympof &lt;/fact&gt; ? &lt;/question&gt; As can be noted, the response is the set of those facts that contribute, together or independently, to the spreading of the disease. By the observation from 100 random interrogative sentences corpus analysis, we found that the semantic types of questions correspondent to the question words are the following Q-Types What When Where Why Who Which How F 11 6 1 1 9 2 E 1 6 SF 3 15 3 SE 7 2 5 PROC 15 SqE 7 EVAL 2 1 DEF 3 1 the between questions and semantic types of questions From Table 1, it is clear that “what” and “how” questions vary in types of question, because they have many forms to use, for example, “how + verb to be + noun”, “how + do(es) + noun + verb”, “how to”, “how can”, etc. or “what + verb to be + noun”, “what + noun + verb”, This is why we point out the “What” and “How” questions. 7 4.2 Dealing with texts: document indexing and associated annotations Texts are initially indexed based on the main terms they contain which are relevant w.r.t. the questions given in the corpus. Our representation resembles a frame approach, but it is more flexible since there is no predefined structure to represent indexes. This is more in accordance with the variety of texts in terms of contents. Indexes basically are formed from: terms that structure the domain: for example, concepts like symptom, spreading, treatment, time, place, effect, etc. where predicative (action terms) terms as well as entities are found, generic terms, found in the questions and structured in the domain ontology: water, clean, control, eradicate, etc., which are organized w.r.t. the top concepts above, entities, typed as: disease names, location names, chemical product names, bacteria names, etc. In our representation, those generic terms (and near synonyms) are represented as predicates, while arguments are represented as attributevalue pairs (or attributes alone), include typed name entities and any kind of terms besides the generic terms. Indexes are associated with texts in the text database. Indexes must remain general so that indexing is fast and as reliable as possible. The idea is that when a question is uttered, a small number of texts are first selected on the basis of the indexes for further analysis. An example below can be indexed and annotated [2] as the following: Index: disease-name (Bakanae), symptoms (disease: Bakanae), origin (disease: Bakanae, place: California, date: 1999), spreading(disease: Bakanae, period: winter, medium: [soil, water]), treatment(disease: Bakanae, product). &lt;title type=“goal” level=“1” &gt; Rice Bakanae &lt;/title&gt; &lt;title type=“goal” level=“2”&gt;SYMPTOMS &lt;/title&gt; &lt;task type = “SF”&gt; &lt;theme&gt;Symptoms of Bakanae&lt;/theme&gt; first appear about a month after planting. Infected seedlings appear to be taller, more slender, and slightly chlorotic ... The rapid elongation of infected plants is caused by the pathogen’s production of the plant hormone, gibberellin &lt;/task&gt; &lt;title type=“goal” level=“2”&gt;COMMENTS ON THE DISEASE &lt;/title&gt; Bakanae is one of the oldest known diseases of rice in Asia but has only been observed in California rice since 1999 and now occurs in all California rice-growing regions. While very damaging in Asia, the extent to which Bakanae may effect California rice production unknown. As diseased plants &lt;title type=“goal” level=“2”&gt;MANAGEMENT&lt;/title&gt; &lt;task type = “PROC”&gt; The most effective means to&lt;action&gt; treat &lt;theme&gt; this disease &lt;/theme&gt; &lt;/action&gt; is the &lt;instruction compound&gt;&lt;instruction type=&amp;quot;imperative&amp;quot;&gt;use of noninfested seed&lt;/instruction&gt;. Also,&lt;connector type=&amp;quot;advice&amp;quot;&gt; when possible&lt;/connector&gt;, &lt;advice&gt;burning plant residues&lt;/advice&gt; with known infection in fall help limit the disease. Field trials indicate that a seed ment with sodium hypochlorite (Ultra Clorox Germicidal Bleach) is effective at reducing the incidence of this disease.... &lt;/instruction compound&gt;&lt;/task&gt; 4.3 Matching selected texts with questions: the deep indexing level The main words of the question focus and body are used to select a subset of indexed texts as potential candidates containing the response. Then, in each of these texts, the few sentences where the terms of the question or derived terms (closely related terms) are effectively found are annotated by means of semantic roles as for the question, for further analysis and investigations. For that purpose, we have developed guidelines for annotating those text fragments where the response is and the associated knowledge, based on the same semantic roles as those used in the questions. These annotations remain so far exploratory, in terms of feasibility and automation. Our major concern is to develop a method for annotators so that a large number of texts can be tagged homogeneously and also so that the technique can be reproduced for other technical areas. Finally, in terms of response identification, the goal is to define a metric that defines the best match and selects the text fragment(s) that best respond(s) to the question among several potential candidates. Let us first consider a simple example. Given the question: to eradicate Bakanae ?” with the following representation: type=“PROC or SqE” focus “eradicate Bakanae” &gt; How to &lt;action&gt; eradicate &lt;theme&gt; Bakanae &lt;/theme&gt; &lt;/action&gt; ? &lt;/question&gt; The main terms of the question are ‘eradicate’ and ‘Bakanae’. The text above is therefore selected on the basis of its indexes, because ‘treatment’ is a closely related term (in terms of semantic relation: ‘way to realize an event’) of ‘eradicate’ in the domain ontology. Then, the question terms are searched in the selected text and the sentences that contain them are annotated using semantic roles. For example, the following sentence is a candidate: The most effective means to treat this disease is the use of noninfested seeds. It is tagged as: 8 ...&lt;action&gt; treat &lt;theme&gt; this disease &lt;/theme&gt; is the use of &lt;instrument&gt; noninfested seeds &lt;/instrument&gt; &lt;/action&gt; . The answer is the above sentence and the text fragment that follows (introduced by the connecalso) since the response is of type The most effective means to treat this disease is the use of noninfested seed. Also, when possible, burning plant residues with known infection in fall may help limit the disease. Following [5], this structure is annotated as a single instructional compound, which is the fundamental unit in a procedural text. This is the structure which is typically returned to users. Let us present here another illustrative example of a text fragment where the response is annotated together with the required related reasoning elements: can thrips destroy the rice ?” annotation: &lt;question type=“SqE” focus = ”destroy”&gt;How can &lt;agent&gt; thrips &lt;/agent&gt; &lt;action&gt;destroy &lt;theme&gt;the rice&lt;/theme&gt; &lt;/action&gt;?&lt;/question&gt; The text fragment that corresponds to the answer is annotated as follows: &lt;response&gt; &lt;agent&gt; The rice thrips&lt;/agent&gt; &lt;action&gt; sucks the sap &lt;source&gt; from the young plant. &lt;/source&gt; &lt;/action&gt; &lt;/response&gt; To match the action ‘destroy’ in the question with the text portion from which the response is extracted, it is then necessary to identify the inference: &lt;lex_inference&gt; &lt;action&gt; Suck sap of X &lt;/action&gt; &lt;entail&gt; &lt;modality&gt; probably &lt;/modality&gt; &lt;action&gt; destroy X &lt;/action&gt; &lt;/entail&gt; , &lt;type&gt; X : plant &lt;/type&gt; &lt;part-of&gt; sap : X &lt;/part-of &gt; &lt;/lex inference&gt; This example shows that (1) in the question and in the answer, annotations are used to identify the different components, arguments, adjuncts, but also some other components (e.g. temporal adverbs), and (2) the annotation is developed to characterize the matching steps and inferential components (either lexical or domain knowledge) between the question and the answer. This latter form of annotation, which is quite time-consuming to develop, is the means we use to induce and develop domain dependent forms of lexical inference (or other phenomena like synonymy, lexical equivalence, etc.) and relevant domain knowledge. The types and lexical functions which are introduced are then used in the process of induction of generalizations over some semantic categories (plants, products, etc.), and verb classes. This way of annotating knowledge and inferences is obviously a simple bottom-up process, with well known limitations, but we feel it may have some advantages for inducing an upper organization of knowledge, in conjunction, and as a complement to, the domain ontology. It is also simple and accessible to annotators. Obviously this remains to be evaluated. 4.4 Generalizing inferences for questionanswer matching At this level, the inferences which may be drawn are directly attached to the terms which are tagged. This is obviously too limited. We are now experimenting with different generalization strategies in order to tune the lexical inference rules. This process involves: (1) developing various generic principles over different types and categories (via the domain ontology), We will annotation the title for matching the “theme” of the answer to the “theme” and “Focus” of the question by using word net and ontology as shown below. Surface Form Concept destroy, destruct, eliminate, kill,... destroy treat, prevent, eradicate, protect,... manage suck, eat, bite, drink,... consume spread out, diffuse, disperse,... spread (2) a set of principles that limit these generalizations via, for example, the taking into account of the semantics restrictions imposed by lexical items, in particular verbs. The main words of the question focus and text body that already annotated will be considered for extracting the potential candidates containing the response. The sentences, where the terms of the question or derived terms (closely related terms) are effectively found, will be the corresponding answer by using matching function as shown below.</abstract>
<title confidence="0.673809285714286">Function Matching (Question Q, Answer A){ Match = false; // Relevant document If (Q.focus = A.index) then // Relevant answer If (Q.type = A.task type) then //Detect Answer for the Question</title>
<author confidence="0.5040165">If then Match true</author>
<affiliation confidence="0.276114">Else if (Q.action = A.action and</affiliation>
<title confidence="0.752152285714286">Q.theme = A.theme or Q.agent = A.agent) then Match = true; End If End If End If Return Match;</title>
<abstract confidence="0.971824934782609">The tuning of the level of these generalizations is obviously one main parameter of our project. It has several conceptual dimensions that we explore and may also be domain dependent. 9 Perspectives The matching problem between questions and documents to retrieve answers in questionanswering systems in concrete applicative contexts is often a difficult problem. This matching procedure often requires very accurate domain knowledge, besides ontological descriptions. It is not always easy to access this knowledge in a structured way or to extract it from texts. The present contribution, still experimental and in an early stage of development, is an attempt, via annotations, at resolving this problem, following a simple and clear methodology. This task needs to be developed and evaluated gradually. So far, it is too early to evaluate the quality of the generalizations and the inferential patterns we get. This approach, and the principles we have briefly outlined, allow us to introduce a working method for the development of questionanswering systems for concrete applications, especially for non-factoid questions, an area which is still not very much developed in spite of its obvious usefulness. One of the reasons is that non-factoid questions require a language processing technology, analysis methods, reasoning aspects, and a conceptual approach, which are substantially different from what is used for factoid questions. Acknowledgments The work described in this paper has been supported by the NECTEC No. NT-B-22-KE-12-50- 19, within the project, “I-KnowII: CAT, EAT, RATs,” and “Agricultural Question &amp; Answering Service System,” granted by the KURDI, Kasetsart University. We would like to especially thank Prof. Patrick Saint Dizier for originating, advising and collaborating in the development of Q&amp;A system. We also thank Prof. William I. Grosky for helping to revise our English.</abstract>
<note confidence="0.833302583333333">References 1. Asanee Kawtrakul, et al. Chaveevan Pechsiri, Sachit Rajbhandari, Frederic Andres, Problems- Solving Map Extraction with Collective Intelligence Analy-sis and Language Engineering , Book Chapter 18, Medical Information Science Reference in Information Retrieval in Biomedicine ISBN: 978-1-60566-274-9; pp 460 2. Asanee Kawtrakul, et al. 2009. From CyberBrain to Q&amp;A Services: A Development of Question - Answering Services System for the Farmer through the SMS, WCCA2009, Grand Sierra Resort, Reno,</note>
<address confidence="0.926004">Nevada, USA.</address>
<note confidence="0.740331625">3. Asanee Kawtrakul, et al. 2008. “CyberBrain: Towards the Next Generation Social Intelligence” IAALD AFITA WCCA 2008, Tokyo, Japan. 4. Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum, Vasile Rus. 2000. The Structure and Performance of an Open-Domain Question Answering System, Proceedings of the 38th Meeting of the Association</note>
<abstract confidence="0.9291816">for Computational Linguistics (ACL), Hong Kong. 5. Estelle Delpech, Patrick Saint-Dizier. 2008. Investigating the Structure of Procedural Texts for Answering How-to Questions, LREC2008, Marrakech. 6. Jochen L. Leidner, 2005. A wireless natural language search engine. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval table of contents: 677 – 677, ACM, New York,</abstract>
<address confidence="0.792080333333333">USA 7. Judy Delin, Anthony Hartley, Cecile Paris, Donia Scott, Keith Vander Linden. 1994. Expressing Pro-</address>
<note confidence="0.965984166666667">cedural Relation-ships in Multilingual Instructions, Proceedings of the 7th International Workshop on Natural Language Generation: 61-70, Maine, USA. 8. Karen Sparck Jones, Branimir Boguraev.1987. A note on a study of cases, research note in Computational Linguistics archive, Volume 13 , Issue 1-2 (January-June 1987) : 65 - 68. 9. Leonard Talmy. 1976. Semantic Causative Types, In M. Shibatani (ed.), Syntax and Semantics 6: The Grammar of Causative Constructions. New York: Academic Press: 43-116. 10. Leonard Talmy. 1985. Lexicalization Patterns:</note>
<title confidence="0.807411">Seman-tic Structure in Lexical Forms, in Language Typol-ogy and Syntactic Description 3: Grammati-</title>
<author confidence="0.859267">cal Categories</author>
<author confidence="0.859267">the Lexicon</author>
<author confidence="0.859267">T Shopen</author>
<note confidence="0.975879777777778">57-149, Cambridge University Press. 11. Mark Thomas Maybury. 2004. New Directions in Question Answering, The MIT Press, Menlo Park. 12. Mineki Takechi, Takenobu Tokunaga, Yuji Matsumoto, Hozumi Tanaka. 2003. Feature Selection in Categorizing Procedural Expressions, The 6th International Workshop on Information Retrieval with Asian Languages (IRAL2003):49-56. 13. Ray Jackendoff. 1990. Semantic Structures, MIT Press. 14. Robert E. Longacre. 1982. Discourse Typology in Relation to Language Typology, Sture Allen ´ed., Text Processing, Proceeding of Nobel Symposium 51, Stockholm, Almquist and Wiksell, 457-486. 15. Sadhu Balasundaram Ramakishnan and Balakrishnan Ramadoss. 2007. SMS for Question- Answering in the m-Learning Scena-rio, Journal of Computer Science 3(2):119-121.</note>
<intro confidence="0.474165">10</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Asanee Kawtrakul</author>
</authors>
<title>Chaveevan Pechsiri, Sachit Rajbhandari, Frederic Andres, ProblemsSolving Map Extraction with Collective Intelligence Analy-sis and Language Engineering</title>
<booktitle>Book Chapter 18, Medical Information Science Reference in Information Retrieval in Biomedicine ISBN:</booktitle>
<pages>978--1</pages>
<contexts>
<context position="5977" citStr="[1]" startWordPosition="894" endWordPosition="894">ect we present here emerged from a need of the real end-users, the Agricultural Land Reform Office, Ministry of Agriculture and Cooperative, Thailand, in the project of ALRO CyberBrain [3], which is a social network framework that combines approaches based on knowledge science and engineering with language engineering, consisting of an ontology-based search engine, information extraction for Q&amp;A system, knowledge aggregation through a knowledge portal and visualized in a browser with semantic links between problems, methods of problems solving and man who is the problem solver (PMM map Model) [1]. The main goal is to develop tools for e-Farming, in particular rice farming, so that farmers can easily get information on farming rice and rice diseases. Now, it has been extended to provide question-answering services for the farmers through SMS [2]. 2 Problem Statements There are two main problems in Q&amp;A analysis: semantic interpretation for a question word and answer identification. 2.1 Question’s Semantic Roles 2.1.1 Question Word Interpretation. In general, when we query for the answer by a traditional search engine system, we might get many answers at different levels, depending on th</context>
</contexts>
<marker>1.</marker>
<rawString>Asanee Kawtrakul, et al. Chaveevan Pechsiri, Sachit Rajbhandari, Frederic Andres, ProblemsSolving Map Extraction with Collective Intelligence Analy-sis and Language Engineering , Book Chapter 18, Medical Information Science Reference in Information Retrieval in Biomedicine ISBN: 978-1-60566-274-9; pp 460</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asanee Kawtrakul</author>
</authors>
<title>From CyberBrain to Q&amp;A Services: A Development of Question - Answering Services System for the Farmer through the SMS, WCCA2009, Grand Sierra Resort,</title>
<date>2009</date>
<location>Reno, Nevada, USA.</location>
<contexts>
<context position="6230" citStr="[2]" startWordPosition="937" endWordPosition="937">sed on knowledge science and engineering with language engineering, consisting of an ontology-based search engine, information extraction for Q&amp;A system, knowledge aggregation through a knowledge portal and visualized in a browser with semantic links between problems, methods of problems solving and man who is the problem solver (PMM map Model) [1]. The main goal is to develop tools for e-Farming, in particular rice farming, so that farmers can easily get information on farming rice and rice diseases. Now, it has been extended to provide question-answering services for the farmers through SMS [2]. 2 Problem Statements There are two main problems in Q&amp;A analysis: semantic interpretation for a question word and answer identification. 2.1 Question’s Semantic Roles 2.1.1 Question Word Interpretation. In general, when we query for the answer by a traditional search engine system, we might get many answers at different levels, depending on the role of the question: Definition vs Fact or set of Facts. For example, with the question Q1: “TTf1|1&amp;quot;a,A|f1�fl|fl_-1T” Rice |Blast |is |what “What is a Rice Blast?” The answer can be returned as the definitions, fact or a set of facts, which are: A1.1</context>
<context position="22057" citStr="[2]" startWordPosition="3422" endWordPosition="3422">names, bacteria names, etc. In our representation, those generic terms (and near synonyms) are represented as predicates, while arguments are represented as attributevalue pairs (or attributes alone), include typed name entities and any kind of terms besides the generic terms. Indexes are associated with texts in the text database. Indexes must remain general so that indexing is fast and as reliable as possible. The idea is that when a question is uttered, a small number of texts are first selected on the basis of the indexes for further analysis. An example below can be indexed and annotated [2] as the following: Index: disease-name (Bakanae), symptoms (disease: Bakanae), origin (disease: Bakanae, place: California, date: 1999), spreading(disease: Bakanae, period: winter, medium: [soil, water]), treatment(disease: Bakanae, product). &lt;title type=“goal” level=“1” &gt; Rice Bakanae &lt;/title&gt; &lt;title type=“goal” level=“2”&gt;SYMPTOMS &lt;/title&gt; &lt;task type = “SF”&gt; &lt;theme&gt;Symptoms of Bakanae&lt;/theme&gt; first appear about a month after planting. Infected seedlings appear to be taller, more slender, and slightly chlorotic ... The rapid elongation of infected plants is caused by the pathogen’s production </context>
</contexts>
<marker>2.</marker>
<rawString>Asanee Kawtrakul, et al. 2009. From CyberBrain to Q&amp;A Services: A Development of Question - Answering Services System for the Farmer through the SMS, WCCA2009, Grand Sierra Resort, Reno, Nevada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asanee Kawtrakul</author>
</authors>
<date>2008</date>
<journal>CyberBrain: Towards the Next Generation Social Intelligence” IAALD AFITA WCCA</journal>
<location>Tokyo, Japan.</location>
<contexts>
<context position="5562" citStr="[3]" startWordPosition="831" endWordPosition="831">riment). In the same time, we are generalizing inference rules in order to match a question to its answer. This is particularly crucial when there is no straightforward response, e.g. when they require some form of lexical inference, elaboration, and reasoning or when the response is not a simple item, but a well-formed fragment of text, e.g. a chain of events leading to a consequence, a procedure, etc. The project we present here emerged from a need of the real end-users, the Agricultural Land Reform Office, Ministry of Agriculture and Cooperative, Thailand, in the project of ALRO CyberBrain [3], which is a social network framework that combines approaches based on knowledge science and engineering with language engineering, consisting of an ontology-based search engine, information extraction for Q&amp;A system, knowledge aggregation through a knowledge portal and visualized in a browser with semantic links between problems, methods of problems solving and man who is the problem solver (PMM map Model) [1]. The main goal is to develop tools for e-Farming, in particular rice farming, so that farmers can easily get information on farming rice and rice diseases. Now, it has been extended to</context>
</contexts>
<marker>3.</marker>
<rawString>Asanee Kawtrakul, et al. 2008. “CyberBrain: Towards the Next Generation Social Intelligence” IAALD AFITA WCCA 2008, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Roxana Girju</author>
<author>Richard Goodrum</author>
<author>Vasile Rus</author>
</authors>
<title>The Structure and Performance of an Open-Domain Question Answering System,</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Meeting of the Association for Computational Linguistics (ACL), Hong Kong.</booktitle>
<contexts>
<context position="13549" citStr="[4]" startWordPosition="2028" endWordPosition="2028">|มาก|เกินไป| “Do not put too much Nitrogen.” ้ Q7: ถา |พบ|เพลีย |ไฟ|ระบาด ||สามารถ|กําจัด|ดวย|สาร| ฆา|แมลง|ชนิด|ใด “How to eradicate the rice thrips” A7: ฉีด|พน|ดวย|มาลาไทออน|หรือ|คารบาริล|ทุก|อาทิตย || ใส|ปุย|และ|ให|น้ํา|ทุก ||ๆ ||สอง|วัน “Spray with Malathion or Carbaryl every week, add fertilizer and water every two days.” Questions are essentially factoid questions (e.g. best periods for rice planting, rice varieties suggestion, symptoms of a disease), why questions, where responses are chains of events (reasons for something to happen) and a large number of procedural questions [4], in particular for treating diseases. There are relatively few comparative or evaluative questions besides general questions, such as: What are the major rice pests? In most cases, questions do not have responses which can be immediately found in the texts by standard term matching techniques. For example: “How does the Sheath Blight affect the rice growth?” has the following response in a text: Plants heavily infected at these stages produce poorly filled grain, particularly in the lower portion of the panicle. Additional losses result from ... Therefore, some lexical semantics devices (e.g.</context>
</contexts>
<marker>4.</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum, Vasile Rus. 2000. The Structure and Performance of an Open-Domain Question Answering System, Proceedings of the 38th Meeting of the Association for Computational Linguistics (ACL), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Estelle Delpech</author>
<author>Patrick Saint-Dizier</author>
</authors>
<date>2008</date>
<booktitle>Investigating the Structure of Procedural Texts for Answering How-to Questions, LREC2008,</booktitle>
<location>Marrakech.</location>
<contexts>
<context position="26074" citStr="[5]" startWordPosition="4030" endWordPosition="4030">example, the following sentence is a candidate: The most effective means to treat this disease is the use of noninfested seeds. It is tagged as: 8 ...&lt;action&gt; treat &lt;theme&gt; this disease &lt;/theme&gt; is the use of &lt;instrument&gt; noninfested seeds &lt;/instrument&gt; &lt;/action&gt; . The answer is the above sentence and the text fragment that follows (introduced by the connector also) since the response is of type procedure: The most effective means to treat this disease is the use of noninfested seed. Also, when possible, burning plant residues with known infection in fall may help limit the disease. Following [5], this structure is annotated as a single instructional compound, which is the fundamental unit in a procedural text. This is the structure which is typically returned to users. Let us present here another illustrative example of a text fragment where the response is annotated together with the required related reasoning elements: Q9: “How can thrips destroy the rice ?” annotation: &lt;question type=“SqE” focus = ”destroy”&gt;How can &lt;agent&gt; thrips &lt;/agent&gt; &lt;action&gt;destroy &lt;theme&gt;the rice&lt;/theme&gt; &lt;/action&gt;?&lt;/question&gt; The text fragment that corresponds to the answer is annotated as follows: &lt;respons</context>
</contexts>
<marker>5.</marker>
<rawString>Estelle Delpech, Patrick Saint-Dizier. 2008. Investigating the Structure of Procedural Texts for Answering How-to Questions, LREC2008, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jochen L Leidner</author>
</authors>
<title>A wireless natural language search engine.</title>
<date>2005</date>
<booktitle>Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval table of contents: 677 – 677,</booktitle>
<publisher>ACM,</publisher>
<location>New York, USA</location>
<contexts>
<context position="3296" citStr="[6]" startWordPosition="488" endWordPosition="488">, but also for such one-way services as early warning systems, for example, a Tsunami Alert System2, a FloodSMS – Early Detection and Warning of Catastrophic Flooding via SMS3, etc. The development of a Question-Answering Services System through SMS is not the design of a new technology. There have been several theories developed earlier, in the context of NLP or cognitive sciences, such as Natural Language Information Retrieval (NLIR), rule based Q&amp;A, etc. Nevertheless, some former theories of Q&amp;A relied on complex semantic information. For instance, a Wireless Natural Language Search Engine [6] was implemented using a system resid1 http://web.nso.go.th/en/survey/keystat/keystat08.pdf 2 http://www.wap.ait.ac.th/tsunami.html 3 http://www.netsquared.org/projects/floodsms%E2%80%93-early-detection-and-warningcatastrophic-flooding-sms 3 Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 3–10, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP ing on a server, which can translate questions or phrases into search engine queries or queries to SOAP Web services, where a gateway mediates between the mobile network and the Internet. A</context>
</contexts>
<marker>6.</marker>
<rawString>Jochen L. Leidner, 2005. A wireless natural language search engine. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval table of contents: 677 – 677, ACM, New York, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judy Delin</author>
<author>Anthony Hartley</author>
<author>Cecile Paris</author>
<author>Donia Scott</author>
<author>Keith Vander Linden</author>
</authors>
<date>1994</date>
<booktitle>Expressing Procedural Relation-ships in Multilingual Instructions, Proceedings of the 7th International Workshop on Natural Language Generation:</booktitle>
<pages>61--70</pages>
<location>Maine, USA.</location>
<contexts>
<context position="14320" citStr="[11, 7]" startWordPosition="2147" endWordPosition="2148">s? In most cases, questions do not have responses which can be immediately found in the texts by standard term matching techniques. For example: “How does the Sheath Blight affect the rice growth?” has the following response in a text: Plants heavily infected at these stages produce poorly filled grain, particularly in the lower portion of the panicle. Additional losses result from ... Therefore, some lexical semantics devices (e.g. a semantic link between affect and infect) or more elaborated reasoning schemas, based on domain knowledge, are needed to allow appropriate question-text matching [11, 7]. The kind of domain knowledge at stake may be quite unexpected (i.e., not the main topics that everyone knows, but more subtle pieces of information, as will be seen in 4.3). This is the major challenge of this work, which we try to resolve via a full annotation of the matching process, from question parsing to response production, identifying matching and reasoning aspects. Complex questions may, e.g., require the elaboration of a diagnosis from premises given in the question before finding the response, either factoid or procedural (My rice has weedy leaves and some yellow spots, what shoul</context>
<context position="16733" citStr="[10, 7]" startWordPosition="2535" endWordPosition="2536">ew rhetorical relations (e.g. elaboration, example, explanation), we first annotated the questions and their corresponding responses in texts provided by the Thai Ministry of Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constituents, which thematic roles cannot do. For example, in the first Thai university we have a link between &apos;first&apos; and &apos;Thai university&apos; which is either loctemp or loc+char+ident, depending on the interpretation of first (oldest or the best). However, in a majority of cases, semantic roles based on thematic roles have a sufficient gr</context>
</contexts>
<marker>7.</marker>
<rawString>Judy Delin, Anthony Hartley, Cecile Paris, Donia Scott, Keith Vander Linden. 1994. Expressing Procedural Relation-ships in Multilingual Instructions, Proceedings of the 7th International Workshop on Natural Language Generation: 61-70, Maine, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>Branimir Boguraev.1987. A note on a study of cases, research note</title>
<booktitle>in Computational Linguistics archive, Volume 13 , Issue 1-2 (January-June 1987) :</booktitle>
<pages>65--68</pages>
<contexts>
<context position="16816" citStr="[8]" startWordPosition="2548" endWordPosition="2548">e questions and their corresponding responses in texts provided by the Thai Ministry of Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constituents, which thematic roles cannot do. For example, in the first Thai university we have a link between &apos;first&apos; and &apos;Thai university&apos; which is either loctemp or loc+char+ident, depending on the interpretation of first (oldest or the best). However, in a majority of cases, semantic roles based on thematic roles have a sufficient granularity, and these are the ones which are used in the examples in 4.1. The main r</context>
</contexts>
<marker>8.</marker>
<rawString>Karen Sparck Jones, Branimir Boguraev.1987. A note on a study of cases, research note in Computational Linguistics archive, Volume 13 , Issue 1-2 (January-June 1987) : 65 - 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>Semantic Causative Types,</title>
<date>1976</date>
<booktitle>Syntax and Semantics 6: The Grammar of Causative Constructions.</booktitle>
<pages>43--116</pages>
<editor>In M. Shibatani (ed.),</editor>
<publisher>Academic Press:</publisher>
<location>New York:</location>
<contexts>
<context position="16401" citStr="[9, 10, 14]" startWordPosition="2476" endWordPosition="2478">instructions based on semantic roles Preprocessing Process Name Entities Recognition Word Segmentation EDU Segmentation POS Tagging WordNet Ontology Question Analysis and Annotation Q&amp;A Matching and Answer Generation Question Text Analysis and Annotation Titles Recognition Document Indexing Text Annotation 6 (TextCoop project) and a few rhetorical relations (e.g. elaboration, example, explanation), we first annotated the questions and their corresponding responses in texts provided by the Thai Ministry of Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constit</context>
</contexts>
<marker>9.</marker>
<rawString>Leonard Talmy. 1976. Semantic Causative Types, In M. Shibatani (ed.), Syntax and Semantics 6: The Grammar of Causative Constructions. New York: Academic Press: 43-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>Lexicalization Patterns: Seman-tic Structure in Lexical Forms,</title>
<date>1985</date>
<booktitle>in Language Typol-ogy and Syntactic Description 3: Grammatical Categories and the Lexicon, T. Shopen(ed.),</booktitle>
<pages>57--149</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16401" citStr="[9, 10, 14]" startWordPosition="2476" endWordPosition="2478">instructions based on semantic roles Preprocessing Process Name Entities Recognition Word Segmentation EDU Segmentation POS Tagging WordNet Ontology Question Analysis and Annotation Q&amp;A Matching and Answer Generation Question Text Analysis and Annotation Titles Recognition Document Indexing Text Annotation 6 (TextCoop project) and a few rhetorical relations (e.g. elaboration, example, explanation), we first annotated the questions and their corresponding responses in texts provided by the Thai Ministry of Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constit</context>
</contexts>
<marker>10.</marker>
<rawString>Leonard Talmy. 1985. Lexicalization Patterns: Seman-tic Structure in Lexical Forms, in Language Typol-ogy and Syntactic Description 3: Grammatical Categories and the Lexicon, T. Shopen(ed.), 57-149, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Thomas Maybury</author>
</authors>
<title>New Directions in Question Answering,</title>
<date>2004</date>
<publisher>The MIT Press,</publisher>
<location>Menlo Park.</location>
<contexts>
<context position="14320" citStr="[11, 7]" startWordPosition="2147" endWordPosition="2148">s? In most cases, questions do not have responses which can be immediately found in the texts by standard term matching techniques. For example: “How does the Sheath Blight affect the rice growth?” has the following response in a text: Plants heavily infected at these stages produce poorly filled grain, particularly in the lower portion of the panicle. Additional losses result from ... Therefore, some lexical semantics devices (e.g. a semantic link between affect and infect) or more elaborated reasoning schemas, based on domain knowledge, are needed to allow appropriate question-text matching [11, 7]. The kind of domain knowledge at stake may be quite unexpected (i.e., not the main topics that everyone knows, but more subtle pieces of information, as will be seen in 4.3). This is the major challenge of this work, which we try to resolve via a full annotation of the matching process, from question parsing to response production, identifying matching and reasoning aspects. Complex questions may, e.g., require the elaboration of a diagnosis from premises given in the question before finding the response, either factoid or procedural (My rice has weedy leaves and some yellow spots, what shoul</context>
</contexts>
<marker>11.</marker>
<rawString>Mark Thomas Maybury. 2004. New Directions in Question Answering, The MIT Press, Menlo Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mineki Takechi</author>
<author>Takenobu Tokunaga</author>
<author>Yuji Matsumoto</author>
<author>Hozumi Tanaka</author>
</authors>
<date>2003</date>
<booktitle>Feature Selection in Categorizing Procedural Expressions, The 6th International Workshop on Information Retrieval with Asian Languages</booktitle>
<pages>2003--49</pages>
<marker>12.</marker>
<rawString>Mineki Takechi, Takenobu Tokunaga, Yuji Matsumoto, Hozumi Tanaka. 2003. Feature Selection in Categorizing Procedural Expressions, The 6th International Workshop on Information Retrieval with Asian Languages (IRAL2003):49-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures,</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16903" citStr="[13]" startWordPosition="2561" endWordPosition="2561">f Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constituents, which thematic roles cannot do. For example, in the first Thai university we have a link between &apos;first&apos; and &apos;Thai university&apos; which is either loctemp or loc+char+ident, depending on the interpretation of first (oldest or the best). However, in a majority of cases, semantic roles based on thematic roles have a sufficient granularity, and these are the ones which are used in the examples in 4.1. The main roles we consider are: agents (for humans and animals like insects, and metaphorically f</context>
</contexts>
<marker>13.</marker>
<rawString>Ray Jackendoff. 1990. Semantic Structures, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Longacre</author>
</authors>
<date>1982</date>
<booktitle>Discourse Typology in Relation to Language Typology, Sture Allen ´ed., Text Processing, Proceeding of Nobel Symposium 51, Stockholm, Almquist and Wiksell,</booktitle>
<pages>457--486</pages>
<contexts>
<context position="16401" citStr="[9, 10, 14]" startWordPosition="2476" endWordPosition="2478">instructions based on semantic roles Preprocessing Process Name Entities Recognition Word Segmentation EDU Segmentation POS Tagging WordNet Ontology Question Analysis and Annotation Q&amp;A Matching and Answer Generation Question Text Analysis and Annotation Titles Recognition Document Indexing Text Annotation 6 (TextCoop project) and a few rhetorical relations (e.g. elaboration, example, explanation), we first annotated the questions and their corresponding responses in texts provided by the Thai Ministry of Agriculture. One of the challenges was to identify relevant linguistic marks or patterns [9, 10, 14]. There are many attempts to annotate arguments by means of primitives; our approach, here, is oriented towards the precise task at stake and the specific actions. Therefore roles are not as standard as they are in general. An earlier attempt with a similar technique applied to language generation was carried out in, e.g., [10, 7]. Semantic tags are either close to thematic roles (instrument, location, etc.) [8], or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS) [13], in particular, to establish useful links between arguments or between a large variety of constit</context>
</contexts>
<marker>14.</marker>
<rawString>Robert E. Longacre. 1982. Discourse Typology in Relation to Language Typology, Sture Allen ´ed., Text Processing, Proceeding of Nobel Symposium 51, Stockholm, Almquist and Wiksell, 457-486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadhu Balasundaram Ramakishnan</author>
<author>Balakrishnan Ramadoss</author>
</authors>
<title>SMS for QuestionAnswering in the m-Learning Scena-rio,</title>
<date>2007</date>
<journal>Journal of Computer Science</journal>
<pages>3--2</pages>
<contexts>
<context position="3905" citStr="[15]" startWordPosition="561" endWordPosition="561">implemented using a system resid1 http://web.nso.go.th/en/survey/keystat/keystat08.pdf 2 http://www.wap.ait.ac.th/tsunami.html 3 http://www.netsquared.org/projects/floodsms%E2%80%93-early-detection-and-warningcatastrophic-flooding-sms 3 Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 3–10, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP ing on a server, which can translate questions or phrases into search engine queries or queries to SOAP Web services, where a gateway mediates between the mobile network and the Internet. Also, [15] developed the SMS for QuestionAnswering in the m-Learning Scenario System by using the Simple Matching Algorithm to match the learners’ answer messages with the original answer string, thus facilitating the learners to get the necessary feedback and assessment. In this paper, we propose the development of the Question-Answering Services System for the Farmer through SMS by focusing on query analysis and annotation, as well as on selected text matching utilizing lexical inference and semantic roles. The annotation emphasizes the semantics model of “What” and “How” queries. Finally, we show how</context>
</contexts>
<marker>15.</marker>
<rawString>Sadhu Balasundaram Ramakishnan and Balakrishnan Ramadoss. 2007. SMS for QuestionAnswering in the m-Learning Scena-rio, Journal of Computer Science 3(2):119-121.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>