<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.992103">
Towards Domain-Independent Deep Linguistic Processing:
Ensuring Portability and Re-Usability of Lexicalised Grammars
</title>
<author confidence="0.751741">
Kostadin Cholakov†, Valia Kordoni†‡, Yi Zhang†‡† Department of Computational Linguistics, Saarland University, Germany
</author>
<address confidence="0.434244">
‡ LT-Lab, DFKI GmbH, Germany
</address>
<email confidence="0.994039">
{kostadin,kordoni,yzhang}@coli.uni-sb.de
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999442714285714">
In this paper we illustrate and underline
the importance of making detailed linguis-
tic information a central part of the pro-
cess of automatic acquisition of large-scale
lexicons as a means for enhancing robust-
ness and at the same time ensuring main-
tainability and re-usability of deep lexi-
calised grammars. Using the error mining
techniques proposed in (van Noord, 2004)
we show very convincingly that the main
hindrance to portability of deep lexicalised
grammars to domains other than the ones
originally developed in, as well as to ro-
bustness of systems using such grammars
is low lexical coverage. To this effect,
we develop linguistically-driven methods
that use detailed morphosyntactic informa-
tion to automatically enhance the perfor-
mance of deep lexicalised grammars main-
taining at the same time their usually al-
ready achieved high linguistic quality.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954833333333">
We focus on enhancing robustness and ensur-
ing maintainability and re-usability for a large-
scale deep grammar of German (GG; (Crysmann,
2003)), developed in the framework of Head-
driven Phrase Structure Grammar (HPSG). Specif-
ically, we show that the incorporation of detailed
linguistic information into the process of auto-
matic extension of the lexicon of such a language
resource enhances its performance and provides
linguistically sound and more informative predic-
tions which bring a bigger benefit for the grammar
when employed in practical real-life applications.
</bodyText>
<note confidence="0.723261">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967191">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999948073170732">
In recent years, various techniques and re-
sources have been developed in order to improve
robustness of deep grammars for real-life applica-
tions in various domains. Nevertheless, low cover-
age of such grammars remains the main hindrance
to their employment in open domain natural lan-
guage processing. (Baldwin et al., 2004), as well
as (van Noord, 2004) and (Zhang and Kordoni,
2006) have clearly shown that the majority of pars-
ing failures with large-scale deep grammars are
caused by missing or wrong entries in the lexicons
accompanying grammars like the aforementioned
ones. Based on these findings, it has become clear
that it is crucial to explore and develop efficient
methods for automated (Deep) Lexical Acquisition
(henceforward (D)LA), the process of automati-
cally recovering missing entries in the lexicons of
deep grammars.
Recently, various high-quality DLA approaches
have been proposed. (Baldwin, 2005), as well
as (Zhang and Kordoni, 2006), (van de Cruys,
2006) and (Nicholson et al., 2008) describe effi-
cient methods towards the task of lexicon acqui-
sition for large-scale deep grammars for English,
Dutch and German. They treat DLA as a classi-
fication task and make use of various robust and
efficient machine learning techniques to perform
the acquisition process.
However, it is our claim that to achieve bet-
ter and more practically useful results, apart from
good learning algorithms, we also need to incorpo-
rate into the learning process fine-grained linguis-
tic information which deep grammars inherently
include and provide for. As we clearly show in
the following, it is not sufficient to only develop
and use good and complicated classification algo-
rithms. We must look at the detailed linguistic in-
formation that is already included and provided for
by the grammar itself and try to capture and make
as much use of it as possible, for this is the infor-
mation we aim at learning when performing DLA.
</bodyText>
<page confidence="0.983957">
57
</page>
<note confidence="0.8492835">
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 57–64
Manchester, August 2008
</note>
<bodyText confidence="0.999957673913044">
In this way, the learning process is facilitated and
at the same time it is as much as possible ensured
that its outcome be linguistically more informative
and, thus, practically more useful.
We use the GG deep grammar for the work we
present in this paper because German is a language
with rich morphology and free word order, which
exhibits a range of interesting linguistic phenom-
ena, a fair number of which are already analysed in
the GG. Thus, the grammar is a valuable linguistic
resource since it provides linguistically sound and
detailed analyses of these phenomena. Apart from
the interesting syntactic structures, though, the lex-
ical entries in the lexicon of the aforementioned
grammar also exhibit a rich and complicated struc-
ture and contain various important linguistic con-
straints. Based on our claim above, in this pa-
per we show how the information these constraints
provide can be captured and used in linguistically-
motivated DLA methods which we propose here.
We then apply our approach on real-life data and
observe the impact it has on the the grammar cov-
erage and its practical application. In this way we
try to prove our assumption that the linguistic in-
formation we incorporate into our DLA methods
is vital for the good performance of the acquisition
process and for the maintainability and re-usability
of the grammar, as well for its successful practical
application.
The remainder of the paper is organised as fol-
lows. In Section 2 we show that low (lexical) cov-
erage is a serious issue for the GG when employed
for open domain natural language processing. Sec-
tion 3 presents the types in the lexical architecture
of the GG that are considered to be relevant for the
purposes of our experiments. Section 4 describes
the extensive linguistic analysis we perform in or-
der to deal with the linguistic information these
types provide and presents the target type inven-
tory for our DLA methods. Section 5 reports on
statistical approaches towards automatic DLA and
shows the importance of a good and linguistically-
motivated feature selection. Section 6 illustrates
the practical usage of the proposed DLA methods
and their impact on grammar coverage. Section 7
concludes the paper.
</bodyText>
<sectionHeader confidence="0.951762" genericHeader="method">
2 Coverage Test with the GG
</sectionHeader>
<bodyText confidence="0.999782108695653">
We start off adopting the automated error mining
method described in (van Noord, 2004) for iden-
tification of the major type of errors in the GG.
As an HPSG grammar, the GG is based on typed
feature structures. The GG types are strictly de-
fined within a type hierarchy. The GG also con-
tains constructional and lexical rules and a lexicon
with its entries belonging to lexical types which
are themselves defined again within the type hier-
archy. The grammar originates from (M¨uller and
Kasper, 2000), but continued to improve after the
end of the Verbmobil project (Wahlster, 2000) and
it currently consists of 5K types, 115 rules and the
lexicon contains approximately 35K entries. These
entries belong to 386 distinct lexical types.
In the experiments we report here two corpora
of different kind and size have been used. The
first one has been extracted from the Frankfurter
Rundschau newspaper and contains about 614K
sentences that have between 5 and 20 tokens. The
second corpus is a subset of the German part of the
Wacky project (Kilgarriff and Grefenstette, 2003).
The Wacky project aims at the creation of large
corpora for different languages, including German,
from various web sources, such as online news-
papers and magazines, legal texts, internet fora,
university and science web sites, etc. The Ger-
man part, named deWaC (Web as Corpus), con-
tains about 93M sentences and 1.65 billion tokens.
The subset used in our experiments is extracted
by randomly selecting 2.57M sentences that have
between 4 and 30 tokens. These corpora have
been chosen because it is interesting to observe
the grammar performance on a relatively balanced
newspaper corpus that does not include so many
long sentences and sophisticated linguistic con-
structions and to compare it with the performance
of the grammar on a random open domain text cor-
pus.
The sentences are fed into the PET HPSG parser
(Callmeier, 2000) with the GG loaded. The parser
has been configured with a maximum edge num-
ber limit of 100K and it is running in the best-only
mode so that it does not exhaustively find all pos-
sible parses. The result of each sentence is marked
as one of the following four cases:
</bodyText>
<listItem confidence="0.999320857142857">
• P means at least one parse is found for the
sentence;
• L means the parser halted after the morpho-
logical analysis and was not able to construct
any lexical item for the input token;
• N means that the parser exhausted the search-
ing and was not able to parse the sentence;
</listItem>
<page confidence="0.925334">
58
</page>
<listItem confidence="0.985216333333333">
• E means the parser reached the maximum
edge number limit and was still not able to
find a parse.
</listItem>
<bodyText confidence="0.7951325">
Table 1 shows the results of the experiments
with the two corpora. From these results it can
</bodyText>
<table confidence="0.998888285714286">
FR deWaC
Result #Sentences % #Sentences %
P 62,768 10.22% 109,498 4.3%
L 464,112 75.55% 2,328,490 90.5%
N 87,415 14.23% 134,917 5.2%
E 3 – 14 –
Total: 614,298 100% 2,572,919 100%
</table>
<tableCaption confidence="0.9251905">
Table 1: Parsing results with the GG and the test
corpora
</tableCaption>
<bodyText confidence="0.998691">
be seen that the GG has full lexical span for only
a small portion of the sentences– about 25% and
10% for the Frankfurter Rundschau and the deWaC
corpora, respectively. The output of the error min-
ing confirms our assumption that missing lexical
entries are the main problem when it comes to
robust performance of the GG and illustrates the
need for efficient DLA methods.
</bodyText>
<sectionHeader confidence="0.971385" genericHeader="method">
3 Atomic Lexical Types
</sectionHeader>
<bodyText confidence="0.998717490196079">
Before describing the proposed DLA algorithm,
we should define what exactly is being learnt.
Most of the so called deep grammars are strongly
lexicalised. As mentioned in the previous section,
the GG employs a type inheritance system and its
lexicon has a flat structure with each lexical entry
mapped onto one type in the inheritance hierarchy.
Normally, the types assigned to the lexical entries
are maximal on the type hierarchy, i.e., they do not
have any subtypes. They provide the most specific
information available for this branch of the hierar-
chy. These maximal types which the lexical entries
are mapped onto are called atomic lexical types.
Thus, in our experiment setup, we can define the
lexicon of the grammar as being a one-to-one map-
ping from word stems to atomic lexical types. It is
this mapping which must be automatically learnt
(guessed) by the different DLA methods.
We are interested in learning open-class words,
i.e., nouns, adjectives, verbs and adverbs. We as-
sume that the close-class words are already in the
lexicon or the grammar can handle them through
various lexical rules and they are not crucial for
the grammar performance in real life applications.
Thus, for the purposes of our experiments, we con-
sider only the open-class lexical types. Moreover,
we propose an inventory of open-class lexical types
with sufficient type and token frequency. The type
frequency of a given lexical type is defined as
the number of lexical entries in the lexicon of the
grammar that belong to this type and the token fre-
quency is the number of words in some corpus that
belong to this type.
We use sentences from the Verbmobil corpus
which have been treebanked with the GG in order
to determine the token frequency and to map the
lexemes to their correct entries in the lexicon for
the purposes of the experiment. This set contains
11K sentences and about 73K tokens; this gives an
average of 6.8 words per sentence. The sentences
are taken from spoken dialogues. Hence, they are
not long and most of them do not exhibit interest-
ing linguistic properties which is a clear drawback
but currently there is no other annotated data com-
patible with the GG.
We used a type frequency threshold of 10 entries
in the lexicon and a token frequency threshold of
3 occurrences in the treebanked sentences to form
a list of relevant open-class lexical types. The re-
sulting list contains 38 atomic lexical types with a
total of 32,687 lexical entries.
</bodyText>
<sectionHeader confidence="0.902348" genericHeader="method">
4 Incorporation of Linguistic Features
</sectionHeader>
<bodyText confidence="0.999994708333333">
However, in the case of the GG this type inventory
is not a sufficient solution. As already mentioned,
in the lexicon of the grammar much of the relevant
linguistic information is encoded not in the type
definition itself but in the form of constraints in the
feature structures of the various types. Moreover,
given that German has a rich morphology, a given
attribute may have many different values among
lexical entries of the same type and it is crucial for
the DLA process to capture all the different com-
binations. That is why we expand the identified
38 atomic lexical type definitions by including the
values of various features into them.
By doing this, we are trying to facilitate the
DLA process because, in that way, it can ‘learn’
to differentiate not only the various lexical types
but also significant morphosyntactic differences
among entries that belong to the same lexical type.
That gives the DLA methods access to much more
linguistic information and they are able to apply
more linguistically fine-tuned classification crite-
ria when deciding which lexical type the unknown
word must be assigned to. Furthermore, we en-
sure that the learning process deliver linguistically
</bodyText>
<page confidence="0.997692">
59
</page>
<table confidence="0.965989777777778">
Feature Values Meaning
SUBJOPT (subject options) + in some cases the article for the noun can be omitted
- the noun always goes with an article
+ raising verb
- non-raising verb
KEYAGR (key agreement) – case-number-gender information for nouns
c-s-n underspecified-singular-neutral
c-p-g underspecified-plural-underspecified
... ...
(O)COMPAGR ((oblique) a-n-g, d-n-g, etc. case-number-gender information
complement – for (oblique) verb complements
agreement – case-number-gender of the modified noun (for adjectives)
(O)COMPTOPT ((oblique) – verbs can take a different number of complements
complement + the respective (oblique) complement is present
options - the respective (oblique) complement is absent
KEYFORM – the auxiliary verb used for the formation of perfect tense
haben the auxiliary verb is ‘haben’
sein the auxiliary verb is ‘sein’
</table>
<tableCaption confidence="0.99054">
Table 2: Relevant features used for type expansion
</tableCaption>
<bodyText confidence="0.999768965517241">
plausible, precise and more practically useful re-
sults. The more the captured and used linguistic
information is, the better and more useful the DLA
results will be.
However, we have to avoid creating data sparse
problems. We do so by making the assumption
that not every feature could really contribute to the
classification process and by filtering out these fea-
tures that we consider irrelevant for the enhance-
ment of the DLA task. Naturally, the question
which features are to be considered relevant arises.
After performing an extensive linguistic analysis,
we have decided to take the features shown in Ta-
ble 2 into account.
We have thoroughly analysed each of these fea-
tures and selected them on the basis of their lin-
guistic meaning and their significance and contri-
bution to the DLA process. The SUBJOPT fea-
ture can be used to differentiate among nouns that
have a similar morphosyntactic behaviour but dif-
fer only in the usage of articles; 4 out of the consid-
ered 9 noun atomic lexical types do not define this
feature. Furthermore, using this feature, we can
also refine our classification within a single atomic
lexical type. For example, the entry ‘adresse-n’
(address) of the type ‘count-noun-le’1 has ‘-’ for
the SUBJOPT value, whereas the value for the en-
try ‘anbindung-n’ (connection) of the same type is
‘+’:
</bodyText>
<table confidence="0.53708375">
(1) a. Das Hotel hat gute
det.NEUT.NOM hotel have.3PER.SG good
Anbindung an die ¨offentlichen
connection to det.PL.ACC public
</table>
<footnote confidence="0.689212">
1count noun lexeme; all lexical entries in the lexicon end
with le which stands for lexeme.
</footnote>
<figure confidence="0.957260833333333">
Verkehrsmittel.
transportation means
‘The hotel has a good connection to public
transportation.’
b. Die
det.FEM.NOM
</figure>
<figureCaption confidence="0.490076666666667">
dem Zug ist gut.
det.MASC.DAT train be.3PER.SG good
‘The train connection to Rome is good.’
</figureCaption>
<bodyText confidence="0.99958816">
The distinction between raising and non-raising
verbs that this feature expresses is also an impor-
tant contribution to the classification process.
The case-number-gender data the KEYAGR and
(O)COMPAGR features provide allows for a bet-
ter usage of morphosyntactic information for the
purposes of DLA. Based on this data, the classifi-
cation method is able to capture words with sim-
ilar morphosyntactic behaviour and give various
indications for their syntactic nature; for instance,
if the word is a subject, direct or indirect object.
This is especially relevant and useful for languages
with rich morphology and relatively free word or-
der such as German. The same is also valid for
the (O)COMPOPT and KEYFORM features– they
allow the DLA method to successfully learn and
classify verbs with similar syntactic properties.
The values of the features are just attached to the
old type name to form a new type definition. In this
way, we ‘promote’ them and these features are now
part of the type hierarchy of the grammar which
makes them accessible for the DLA process since
this operates on the type level. For example, the
original type of the entry for the noun ‘abenteuer’
(adventure):
</bodyText>
<figure confidence="0.8786831">
abenteuer-n := count-noun-le &amp;
[ [ --SUBJOPT -,
Anbindung
connection
an
to
Rom
Rome
mit
with
</figure>
<page confidence="0.396423">
60
</page>
<equation confidence="0.36124875">
KEYAGR c-n-n,
KEYREL &amp;quot;_abenteuer_n_rel&amp;quot;,
KEYSORT situation,
MCLASS nclass-2_-u_-e ] ].
</equation>
<bodyText confidence="0.9365446">
will become abenteuer-n := count-noun-le - c-n-
n when we incorporate the values of the features
SUBJOPT and KEYAGR into the original type
definition. The new expanded type inventory is
shown in Table 3.
</bodyText>
<table confidence="0.99720475">
Original Expanded
lexicon lexicon
Number of lexical types 386 485
Atomic lexical types 38 137
-nouns 9 72
-verbs 19 53
-adjectives 3 5
-adverbs 7 7
</table>
<tableCaption confidence="0.999934">
Table 3: Expanded atomic lexical types
</tableCaption>
<bodyText confidence="0.985155877551021">
The features we have ignored do not contribute
to the learning process and are likely to cre-
ate sparse data problems. The (O)COMPFORM
((oblique) complement form) features which de-
note dependent to verbs prepositions are not con-
sidered to be relevant. An example of OCOMP-
FORM is the lexical entry ‘begr¨unden mit-v’ (jus-
tify with) where the feature has the preposition
‘mit’ (with) as its value. Though for German
prepositions can be considered as case markers, the
DLA has already a reliable access to case informa-
tion through the (O)COMPAGR features. More-
over, a given dependent preposition is distributed
across many types and it does not indicate clearly
which type the respective verb belongs to.
The same is valid for the feature VCOPMFORM
(verb complement form) that denotes the separa-
ble particle (if present) of the verb in question.
An example of this feature is the lexical entry
‘abdecken-v’ (to cover) where VCOMPFORM has
the separable particle ‘ab’ as its value. However,
treating such discontinuous verb-particle combina-
tions as a lexical unit could help for the acquisi-
tion of subcategorizational frames. For example,
anh¨oren (to listen to someone/something) takes an
accusative NP as argument, zuh¨oren (to listen to)
takes a dative NP and aufh¨oren (to stop, to termi-
nate) takes an infinitival complement. Thus, ignor-
ing VCOMPFORM could be a hindrance for the
acquisition of some verb types2.
We have also tried to incorporate some sort of
semantic information into the expanded atomic
2We thank the anonymous reviewer who pointed this out
for us.
lexical type definitions by also attaching the
KEYSORT semantic feature to them. KEYSORT
defines a certain situation semantics category
(‘anything’, ‘action sit’, ‘mental sit’) which the
lexical entry belongs to. However, this has caused
again a sparse data problem because the semantic
classification is too specific and, thus, the number
of possible classes is too large. Moreover, seman-
tic classification is done based on completely dif-
ferent criteria and it cannot be directly linked to the
morphosyntactic features. That is why we have fi-
nally excluded this feature, as well.
Armed with this elaborate target type inventory,
we now proceed with the DLA experiments for the
GG.
</bodyText>
<sectionHeader confidence="0.995124" genericHeader="method">
5 DLA Experiments with the GG
</sectionHeader>
<bodyText confidence="0.999986571428571">
For our DLA experiments, we adopted the Max-
imum Entropy based model described in (Zhang
and Kordoni, 2006), which has been applied to the
ERG (Copestake and Flickinger, 2000), a wide-
coverage HPSG grammar for English. For the pro-
posed prediction model, the probability of a lexical
type t given an unknown word and its context c is:
</bodyText>
<equation confidence="0.949693">
(2) p(t|c) = r exp(K Θifi(t,c))
/�t′∈T exp(Ei Θifi(t′,c))
</equation>
<bodyText confidence="0.739974666666667">
where fi(t, c) may encode arbitrary characteristics
of the context and Oi is a weighting factor esti-
mated on a training corpus. Our experiments have
been performed with the feature set shown in Table
4.
Features
the prefix of the unknown word
(length is less or equal 4)
the suffix of the unknown word
(length is less or equal 4)
the 2 words before and after the unknown word
the 2 types before and after the unknown word
</bodyText>
<tableCaption confidence="0.784776">
Table 4: Features for the DLA experiment
</tableCaption>
<bodyText confidence="0.999923909090909">
We have also experimented with prefix and suf-
fix lengths up to 3. To evaluate the contribution
of various features and the overall precision of the
ME-based unknown word prediction model, we
have done a 10-fold cross validation on the Verb-
mobil treebanked data. For each fold, words that
do not occur in the training partition are assumed
to be unknown and are temporarily removed from
the lexicon.
For comparison, we have also built a baseline
model that always assigns a majority type to each
</bodyText>
<page confidence="0.998331">
61
</page>
<bodyText confidence="0.9995666">
unknown word according to its POS tag. Specifi-
cally, we tag the input sentence with a small POS
tagset. It is then mapped to a most popular lexi-
cal type for that POS. Table 5 shows the relevant
mappings.
</bodyText>
<table confidence="0.9588838">
POS Majority lexical type
noun count-noun-le - c-n-f
verb trans-nerg-str-verb-le haben-auxf
adj adj-non-prd-le
adv intersect-adv-le
</table>
<tableCaption confidence="0.999481">
Table 5: POS tags to lexical types mapping
</tableCaption>
<bodyText confidence="0.999851063492064">
Again for comparison, we have built another
simple baseline model using the TnT POS tagger
(Brants, 2000). TnT is a general-purpose HMM-
based trigram tagger. We have trained the tagging
models with all the lexical types as the tagset. The
tagger tags the whole sentence but only the output
tags for the unknown words are taken to generate
lexical entries and to be considered for the eval-
uation. The precisions of the different prediction
models are given in Table 6.
The baseline achieves a precision of about 38%
and the POS tagger outperforms it by nearly 10%.
These results can be explained by the nature of the
Verbmobil data. The vast majority of the adjec-
tives and the adverbs in the sentences belong to
the majority types shown in Table 5 and, thus, the
baseline model assigns the correct lexical types to
almost every adjective and adverb, which brings
up the overall precision. The short sentence length
facilitates the tagger extremely, for TnT, as an
HMM-based tagger, makes predictions based on
the whole sentence. The longer the sentences are,
the more challenging the tagging task for TnT is.
The results of these models clearly show that the
task of unknown word type prediction for deep
grammars is non-trivial.
Our ME-based models give the best results in
terms of precision. However, verbs and adverbs
remain extremely difficult for classification. The
simple morphological features we use in the ME
model are not good enough for making good pre-
dictions for verbs. Morphology cannot capture
such purely syntactic features as subcategoriza-
tional frames, for example.
While the errors for verbs are pretty random,
there is one major type of wrong predictions for
adverbs. Most of them are correctly predicted as
such but they receive the majority type for adverbs,
namely ‘intersect-adv-le’. Since most of the ad-
verbs in the Verbmobil data we are using belong
to the majority adverb type, the predictor is biased
towards assigning it to the unknown words which
have been identified as adverbs.
The results in the top half of the Table 6 show
that morphological features are already very good
for predicting adjectives. In contrast with ad-
verbs, adjectives occur in pretty limited number of
contexts. Moreover, when dealing with morpho-
logically rich languages such as German, adjec-
tives are typically marked by specific affixes cor-
responding to a specific case-number-gender com-
bination. Since we have incorporated this kind of
linguistic information into our target lexical type
definitions, this significantly helps the prediction
process based on morphological features.
Surprisingly, nouns seem to be hard to learn.
Apparently, the vast majority of the wrong pre-
dictions have been made for nouns that belong to
the expanded variants of the lexical type ‘count-
noun-le’ which is also the most common non-
expanded lexical type for nouns in the original lex-
icon. Many nouns have been assigned the right lex-
ical type except for the gender:
</bodyText>
<listItem confidence="0.799636666666667">
(3) Betrieb (business, company, enterprise)
prediction: count-noun-le - c-n-n
correct type: count-noun-le - c-n-m
</listItem>
<bodyText confidence="0.99974488">
According to the strict exact-match evaluate mea-
sure we use, such cases are considered to be errors
because the predicted lexical type does not match
the type of the lexical entry in the lexicon.
The low numbers for verbs and adverbs show
clearly that we also need to incorporate some sort
of syntactic information into the prediction model.
We adopt the method described in (Zhang and Ko-
rdoni, 2006) where the disambiguation model of
the parser is used for this purpose. We also believe
that the kind of detailed morphosyntactic informa-
tion which the learning process now has access
to would facilitate the disambiguation model be-
cause the input to the model is linguistically more
fine-grained. In another DLA experiment we let
PET use the top 3 predictions provided by the lex-
ical type predictor in order to generate sentence
analyses. Then we use the disambiguation model,
trained on the Verbmobil data, to choose the best
one of these analyses and the corresponding lexical
entry is taken to be the final result of the prediction
process.
As shown in the last line of Table 6, we achieve
an increase of 19% which means that in many
cases the correct lexical type has been ranked sec-
</bodyText>
<page confidence="0.997751">
62
</page>
<table confidence="0.999714333333333">
Model Precision Nouns Adjectives Verbs Adverbs
Baseline 37.89% 27.03% 62.69% 33.57% 67.14%
TnT 47.53% 53.76% 74.52% 26.94% 32.68%
ME(affix length=3) 51.2% 48.25% 75.41% 44.06% 44.13%
ME(affix length=4) 54.63% 53.55% 76.79% 47.10% 43.55%
ME + disamb. 73.54% 75% 88.24% 65.98% 65.90%
</table>
<tableCaption confidence="0.999448">
Table 6: Precision of unknown word type predictors
</tableCaption>
<bodyText confidence="0.99982175">
ond or third by the predictor. This proves that
the expanded lexical types improve also the perfor-
mance of the disambiguation model and allow for
its successful application for the purposes of DLA.
It also shows, once again, the importance of the
morphology in the case of the GG and proves the
rightness of our decision to expand the type defini-
tions with detailed linguistic information.3
</bodyText>
<sectionHeader confidence="0.99828" genericHeader="method">
6 Practical Application
</sectionHeader>
<bodyText confidence="0.999931375">
Since our main claim in this paper is that for
good and practically useful DLA, which at the
same time may facilitate robustness and ensure
maintainability and re-usability of deep lexicalised
grammars, we do not only need good machine
learning algorithms but also classification and fea-
ture selection that are based on an extensive lin-
guistic analysis, we apply our DLA methods to real
test data. We believe that due to our expanded lex-
ical type definitions, we provide much more lin-
guistically accurate predictions. With this type of
predictions, we anticipate a bigger improvement of
the grammar coverage and accuracy for the pre-
diction process delivers much more linguistically
relevant information which facilitates parsing with
the GG.
We have conducted experiments with PET and
the two corpora we have used for the error mining
to determine whether we can improve coverage by
using our DLA method to predict the types of un-
known words online. We have trained the predic-
tor on the whole set of treebanked sentences and
extracted a subset of 50K sentences from each cor-
pus. Since lexical types are not available for these
sentences, we have used POS tags instead as fea-
tures for our prediction model. Coverage is mea-
sured as the number of sentences that received at
least one parse and accuracy is measured as the
number of sentences that received a correct analy-
sis. The results are shown in Table 7.
The coverage for FR improves with more than
12% and the accuracy number remains almost the
</bodyText>
<footnote confidence="0.611566">
3Another reason for this high result is the short average
length of the treebanked sentences which facilitates the dis-
ambiguation model of the parser.
Parsed Corpus Coverage Accuracy
FR with the vanilla version GG 8.89% 85%
FR with the GG + DLA 21.08% 83%
deWaC with the vanilla version GG 7.46%
deWaC with the GG + DLA 16.95%
</footnote>
<tableCaption confidence="0.977449">
Table 7: Coverage results
</tableCaption>
<bodyText confidence="0.999933125">
same. Thus, with our linguistically-oriented DLA
method, we have managed to increase parsing cov-
erage and at the same time to preserve the high
accuracy of the grammar. It is also interesting to
note the increase in coverage for the deWaC cor-
pus. It is about 10%, and given the fact that deWaC
is an open and unbalanced corpus, this is a clear
improvement. However, we do not measure ac-
curacy on the deWaC corpus because many sen-
tences are not well formed and the corpus itself
contains much ‘noise’. Still, these results show
that the incorporation of detailed linguistic infor-
mation in the prediction process contributed to the
parser performance and the robustness of the gram-
mar without harming the quality of the delivered
analyses.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997165">
In this paper, we have tackled from a more
linguistically-oriented point of view the lexicon
acquisition problem for a large-scale deep gram-
mar for German, developed in HPSG. We have
shown clearly that missing lexical entries are the
main cause for parsing failures and, thus, illus-
trated the importance of increasing the lexical cov-
erage of the grammar. The target type inventory
for the learning process has been developed in a
linguistically motivated way in an attempt to cap-
ture significant morphosyntactic information and,
thus, achieve a better performance and more prac-
tically useful results.
With the proposed DLA approach and our elab-
orate target type inventory we have achieved nearly
75% precision and this way we have illustrated the
importance of fine-grained linguistic information
for the lexical prediction process. In the end, we
have shown that with our linguistically motivated
DLA methods, the parsing coverage of the afore-
</bodyText>
<page confidence="0.997856">
63
</page>
<bodyText confidence="0.99998671875">
mentioned deep grammar improves significantly
while its linguistic quality remains intact.
The conclusion, therefore, is that it is vital to
be able to capture linguistic information and suc-
cessfully incorporate it in DLA processes, for it
facilitates deep grammars and makes processing
with them much more robust for applications. At
the same time, the almost self-evident portability
to new domains and the re-usability of the gram-
mar for open domain natural language processing
is significantly enhanced.
The DLA method we propose can be used as
an external module that can help the grammar be
ported and operate on different domains. Thus,
specifically in the case of HPSG, DLA can also
be seen as a way for achieving more modular-
ity in the grammar. Moreover, in a future re-
search, the proposed kind of DLA might also be
used in order to facilitate the division and transi-
tion from a core deep grammar with a core lex-
icon towards subgrammars with domain specific
lexicons/lexical constraints in a linguistically mo-
tivated way. The use of both these divisions nat-
urally leads to a highly modular structure of the
grammar and the system using the grammar, which
at the same time helps in controlling its complex-
ity.
Our linguistically motivated approach provides
fine-grained results that can be used in a number
of different ways. It is a valuable linguistic tool
and it is up to the grammar developer to choose
how to use the many opportunities it provides.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999431245283019">
Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the English
Resource Grammar over the British National Corpus. In
Proceedings of the Fourth Internation Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lisbon,
Portugal.
Baldwin, Timothy. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisi-
tion, pages 67–76, Ann Arbor, USA.
Brants, Thorsten. 2000. TnT- a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Applied
Natural Language Processing ANLP-2000, Seattle, WA,
USA.
Callmeier, Ulrich. 2000. PET- a platform for experimenta-
tion with efficient HPSG processing techniques. In Jour-
nal ofNatural Language Engineering, volume 6(1), pages
99–108.
Copestake, Ann and Dan Flickinger. 2000. An open-sourse
grammar development environment and broad-coverage
English grammar using HPSG. In Proceedings of the Sec-
ond conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Crysmann, Berthold. 2003. On the efficient implementation
of German verb placement in HPSG. In Proceedings of
RANLP 2003, pages 112–116, Borovets, Bulgaria.
Kilgarriff, Adam and G Grefenstette. 2003. Introduction to
the special issue on the web as corpus. Computational Lin-
guistics, 29:333–347.
M¨uller, Stephan and Walter Kasper. 2000. HPSG analysis of
German. In Wahlster, Wolfgang, editor, Verbmobil: Foun-
dations of Speech-to-Speech Translation, pages 238–253.
Springer-Verlag.
Nicholson, Jeremy, Valia Kordoni, Yi Zhang, Timothy Bald-
win, and Rebecca Dridan. 2008. Evaluating and extend-
ing the coverage of HPSG grammars. In In proceedings of
LREC, Marrakesh, Marocco.
van de Cruys, Tim. 2006. Automatically extending the lexi-
con for parsing. In Huitink, Janneke and Sophia Katrenko,
editors, Proceedings of the Student Session of the Euro-
pean Summer School in Logic, Language and Information
(ESSLLI), pages 180–191, Malaga, Spain.
van Noord, Gertjan. 2004. Error mining for wide coverage
grammar engineering. In Proceedings of the 42nd Meeting
of the Assiciation for Computational Linguistics (ACL’04),
Main Volume, pages 446–453, Barcelona, Spain.
Wahlster, Wolfgang, editor. 2000. Verbmobil: Foundations
of Speech-to-Speech Translation. Artificial Intelligence.
Springer.
Zhang, Yi and Valia Kordoni. 2006. Automated deep lexical
acquisition for robust open text processing. In Proceed-
ings of the Fifth International Conference on Language
Resourses and Evaluation (LREC 2006), Genoa, Italy.
</reference>
<page confidence="0.9994">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.663083">
<title confidence="0.9891835">Towards Domain-Independent Deep Linguistic Ensuring Portability and Re-Usability of Lexicalised Grammars</title>
<affiliation confidence="0.854917">Valia Yi of Computational Linguistics, Saarland University, DFKI GmbH,</affiliation>
<abstract confidence="0.998047545454545">In this paper we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and at the same time ensuring mainand re-usability of lexicalised grammars. Using the error mining techniques proposed in (van Noord, 2004) we show very convincingly that the main to portability of grammars to domains other than the ones originally developed in, as well as to robustness of systems using such grammars is low lexical coverage. To this effect, we develop linguistically-driven methods that use detailed morphosyntactic information to automatically enhance the perforof grammars maintaining at the same time their usually already achieved high linguistic quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Ara Kim</author>
<author>Stephan Oepen</author>
</authors>
<title>Road-testing the English Resource Grammar over the British National Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth Internation Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2258" citStr="Baldwin et al., 2004" startWordPosition="318" endWordPosition="321"> informative predictions which bring a bigger benefit for the grammar when employed in practical real-life applications. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. (Baldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as </context>
</contexts>
<marker>Baldwin, Bender, Flickinger, Kim, Oepen, 2004</marker>
<rawString>Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara Kim, and Stephan Oepen. 2004. Road-testing the English Resource Grammar over the British National Corpus. In Proceedings of the Fourth Internation Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>Bootstrapping deep lexical resources: Resources for courses.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition,</booktitle>
<pages>67--76</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2853" citStr="Baldwin, 2005" startWordPosition="410" endWordPosition="411">ldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and (Nicholson et al., 2008) describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English, Dutch and German. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. However, it is our claim that to achieve better and more practically useful results, apart from good learning algorithms, we also need to incorporate into the learning process fine-grained linguistic information which deep grammars i</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Baldwin, Timothy. 2005. Bootstrapping deep lexical resources: Resources for courses. In Proceedings of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition, pages 67–76, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT- a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000,</booktitle>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="21703" citStr="Brants, 2000" startWordPosition="3545" endWordPosition="3546">ily removed from the lexicon. For comparison, we have also built a baseline model that always assigns a majority type to each 61 unknown word according to its POS tag. Specifically, we tag the input sentence with a small POS tagset. It is then mapped to a most popular lexical type for that POS. Table 5 shows the relevant mappings. POS Majority lexical type noun count-noun-le - c-n-f verb trans-nerg-str-verb-le haben-auxf adj adj-non-prd-le adv intersect-adv-le Table 5: POS tags to lexical types mapping Again for comparison, we have built another simple baseline model using the TnT POS tagger (Brants, 2000). TnT is a general-purpose HMMbased trigram tagger. We have trained the tagging models with all the lexical types as the tagset. The tagger tags the whole sentence but only the output tags for the unknown words are taken to generate lexical entries and to be considered for the evaluation. The precisions of the different prediction models are given in Table 6. The baseline achieves a precision of about 38% and the POS tagger outperforms it by nearly 10%. These results can be explained by the nature of the Verbmobil data. The vast majority of the adjectives and the adverbs in the sentences belon</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, Thorsten. 2000. TnT- a statistical part-of-speech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
</authors>
<title>PET- a platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>In Journal ofNatural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>99--108</pages>
<contexts>
<context position="8120" citStr="Callmeier, 2000" startWordPosition="1285" endWordPosition="1286">e web sites, etc. The German part, named deWaC (Web as Corpus), contains about 93M sentences and 1.65 billion tokens. The subset used in our experiments is extracted by randomly selecting 2.57M sentences that have between 4 and 30 tokens. These corpora have been chosen because it is interesting to observe the grammar performance on a relatively balanced newspaper corpus that does not include so many long sentences and sophisticated linguistic constructions and to compare it with the performance of the grammar on a random open domain text corpus. The sentences are fed into the PET HPSG parser (Callmeier, 2000) with the GG loaded. The parser has been configured with a maximum edge number limit of 100K and it is running in the best-only mode so that it does not exhaustively find all possible parses. The result of each sentence is marked as one of the following four cases: • P means at least one parse is found for the sentence; • L means the parser halted after the morphological analysis and was not able to construct any lexical item for the input token; • N means that the parser exhausted the searching and was not able to parse the sentence; 58 • E means the parser reached the maximum edge number lim</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>Callmeier, Ulrich. 2000. PET- a platform for experimentation with efficient HPSG processing techniques. In Journal ofNatural Language Engineering, volume 6(1), pages 99–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>An open-sourse grammar development environment and broad-coverage English grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second conference on Language Resources and Evaluation (LREC</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="20042" citStr="Copestake and Flickinger, 2000" startWordPosition="3255" endWordPosition="3258">ta problem because the semantic classification is too specific and, thus, the number of possible classes is too large. Moreover, semantic classification is done based on completely different criteria and it cannot be directly linked to the morphosyntactic features. That is why we have finally excluded this feature, as well. Armed with this elaborate target type inventory, we now proceed with the DLA experiments for the GG. 5 DLA Experiments with the GG For our DLA experiments, we adopted the Maximum Entropy based model described in (Zhang and Kordoni, 2006), which has been applied to the ERG (Copestake and Flickinger, 2000), a widecoverage HPSG grammar for English. For the proposed prediction model, the probability of a lexical type t given an unknown word and its context c is: (2) p(t|c) = r exp(K Θifi(t,c)) /�t′∈T exp(Ei Θifi(t′,c)) where fi(t, c) may encode arbitrary characteristics of the context and Oi is a weighting factor estimated on a training corpus. Our experiments have been performed with the feature set shown in Table 4. Features the prefix of the unknown word (length is less or equal 4) the suffix of the unknown word (length is less or equal 4) the 2 words before and after the unknown word the 2 ty</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>Copestake, Ann and Dan Flickinger. 2000. An open-sourse grammar development environment and broad-coverage English grammar using HPSG. In Proceedings of the Second conference on Language Resources and Evaluation (LREC 2000), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berthold Crysmann</author>
</authors>
<title>On the efficient implementation of German verb placement in HPSG.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP 2003,</booktitle>
<pages>112--116</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="1330" citStr="Crysmann, 2003" startWordPosition="189" endWordPosition="190">t the main hindrance to portability of deep lexicalised grammars to domains other than the ones originally developed in, as well as to robustness of systems using such grammars is low lexical coverage. To this effect, we develop linguistically-driven methods that use detailed morphosyntactic information to automatically enhance the performance of deep lexicalised grammars maintaining at the same time their usually already achieved high linguistic quality. 1 Introduction We focus on enhancing robustness and ensuring maintainability and re-usability for a largescale deep grammar of German (GG; (Crysmann, 2003)), developed in the framework of Headdriven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed linguistic information into the process of automatic extension of the lexicon of such a language resource enhances its performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammar when employed in practical real-life applications. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights rese</context>
</contexts>
<marker>Crysmann, 2003</marker>
<rawString>Crysmann, Berthold. 2003. On the efficient implementation of German verb placement in HPSG. In Proceedings of RANLP 2003, pages 112–116, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>G Grefenstette</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--333</pages>
<contexts>
<context position="7287" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="1147" endWordPosition="1150">archy. The grammar originates from (M¨uller and Kasper, 2000), but continued to improve after the end of the Verbmobil project (Wahlster, 2000) and it currently consists of 5K types, 115 rules and the lexicon contains approximately 35K entries. These entries belong to 386 distinct lexical types. In the experiments we report here two corpora of different kind and size have been used. The first one has been extracted from the Frankfurter Rundschau newspaper and contains about 614K sentences that have between 5 and 20 tokens. The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). The Wacky project aims at the creation of large corpora for different languages, including German, from various web sources, such as online newspapers and magazines, legal texts, internet fora, university and science web sites, etc. The German part, named deWaC (Web as Corpus), contains about 93M sentences and 1.65 billion tokens. The subset used in our experiments is extracted by randomly selecting 2.57M sentences that have between 4 and 30 tokens. These corpora have been chosen because it is interesting to observe the grammar performance on a relatively balanced newspaper corpus that does </context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Kilgarriff, Adam and G Grefenstette. 2003. Introduction to the special issue on the web as corpus. Computational Linguistics, 29:333–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan M¨uller</author>
<author>Walter Kasper</author>
</authors>
<title>HPSG analysis of German.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speech-to-Speech Translation,</booktitle>
<pages>238--253</pages>
<editor>In Wahlster, Wolfgang, editor,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>M¨uller, Kasper, 2000</marker>
<rawString>M¨uller, Stephan and Walter Kasper. 2000. HPSG analysis of German. In Wahlster, Wolfgang, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 238–253. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Nicholson</author>
<author>Valia Kordoni</author>
<author>Yi Zhang</author>
<author>Timothy Baldwin</author>
<author>Rebecca Dridan</author>
</authors>
<title>Evaluating and extending the coverage of HPSG grammars.</title>
<date>2008</date>
<booktitle>In In proceedings of LREC,</booktitle>
<location>Marrakesh, Marocco.</location>
<contexts>
<context position="2942" citStr="Nicholson et al., 2008" startWordPosition="424" endWordPosition="427">have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and (Nicholson et al., 2008) describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English, Dutch and German. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. However, it is our claim that to achieve better and more practically useful results, apart from good learning algorithms, we also need to incorporate into the learning process fine-grained linguistic information which deep grammars inherently include and provide for. As we clearly show in the following, it is not suffici</context>
</contexts>
<marker>Nicholson, Kordoni, Zhang, Baldwin, Dridan, 2008</marker>
<rawString>Nicholson, Jeremy, Valia Kordoni, Yi Zhang, Timothy Baldwin, and Rebecca Dridan. 2008. Evaluating and extending the coverage of HPSG grammars. In In proceedings of LREC, Marrakesh, Marocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim van de Cruys</author>
</authors>
<title>Automatically extending the lexicon for parsing.</title>
<date>2006</date>
<booktitle>Proceedings of the Student Session of the European Summer School in Logic, Language and Information (ESSLLI),</booktitle>
<pages>180--191</pages>
<editor>In Huitink, Janneke and Sophia Katrenko, editors,</editor>
<location>Malaga, Spain.</location>
<marker>van de Cruys, 2006</marker>
<rawString>van de Cruys, Tim. 2006. Automatically extending the lexicon for parsing. In Huitink, Janneke and Sophia Katrenko, editors, Proceedings of the Student Session of the European Summer School in Logic, Language and Information (ESSLLI), pages 180–191, Malaga, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Error mining for wide coverage grammar engineering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Assiciation for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>446--453</pages>
<location>Barcelona,</location>
<marker>van Noord, 2004</marker>
<rawString>van Noord, Gertjan. 2004. Error mining for wide coverage grammar engineering. In Proceedings of the 42nd Meeting of the Assiciation for Computational Linguistics (ACL’04), Main Volume, pages 446–453, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of Speech-to-Speech Translation. Artificial Intelligence.</title>
<date>2000</date>
<editor>Wahlster, Wolfgang, editor.</editor>
<publisher>Springer.</publisher>
<marker>2000</marker>
<rawString>Wahlster, Wolfgang, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Artificial Intelligence. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
</authors>
<title>Automated deep lexical acquisition for robust open text processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resourses and Evaluation (LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="2318" citStr="Zhang and Kordoni, 2006" startWordPosition="329" endWordPosition="332"> the grammar when employed in practical real-life applications. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. (Baldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and </context>
<context position="19974" citStr="Zhang and Kordoni, 2006" startWordPosition="3244" endWordPosition="3247"> entry belongs to. However, this has caused again a sparse data problem because the semantic classification is too specific and, thus, the number of possible classes is too large. Moreover, semantic classification is done based on completely different criteria and it cannot be directly linked to the morphosyntactic features. That is why we have finally excluded this feature, as well. Armed with this elaborate target type inventory, we now proceed with the DLA experiments for the GG. 5 DLA Experiments with the GG For our DLA experiments, we adopted the Maximum Entropy based model described in (Zhang and Kordoni, 2006), which has been applied to the ERG (Copestake and Flickinger, 2000), a widecoverage HPSG grammar for English. For the proposed prediction model, the probability of a lexical type t given an unknown word and its context c is: (2) p(t|c) = r exp(K Θifi(t,c)) /�t′∈T exp(Ei Θifi(t′,c)) where fi(t, c) may encode arbitrary characteristics of the context and Oi is a weighting factor estimated on a training corpus. Our experiments have been performed with the feature set shown in Table 4. Features the prefix of the unknown word (length is less or equal 4) the suffix of the unknown word (length is les</context>
<context position="25065" citStr="Zhang and Kordoni, 2006" startWordPosition="4093" endWordPosition="4097">or nouns in the original lexicon. Many nouns have been assigned the right lexical type except for the gender: (3) Betrieb (business, company, enterprise) prediction: count-noun-le - c-n-n correct type: count-noun-le - c-n-m According to the strict exact-match evaluate measure we use, such cases are considered to be errors because the predicted lexical type does not match the type of the lexical entry in the lexicon. The low numbers for verbs and adverbs show clearly that we also need to incorporate some sort of syntactic information into the prediction model. We adopt the method described in (Zhang and Kordoni, 2006) where the disambiguation model of the parser is used for this purpose. We also believe that the kind of detailed morphosyntactic information which the learning process now has access to would facilitate the disambiguation model because the input to the model is linguistically more fine-grained. In another DLA experiment we let PET use the top 3 predictions provided by the lexical type predictor in order to generate sentence analyses. Then we use the disambiguation model, trained on the Verbmobil data, to choose the best one of these analyses and the corresponding lexical entry is taken to be </context>
</contexts>
<marker>Zhang, Kordoni, 2006</marker>
<rawString>Zhang, Yi and Valia Kordoni. 2006. Automated deep lexical acquisition for robust open text processing. In Proceedings of the Fifth International Conference on Language Resourses and Evaluation (LREC 2006), Genoa, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>