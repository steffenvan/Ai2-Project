<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001138">
<title confidence="0.9982195">
EL92: Entity Linking Combining Open Source Annotators
via Weighted Voting
</title>
<author confidence="0.965689">
Pablo Ruiz and Thierry Poibeau
</author>
<affiliation confidence="0.728572">
Laboratoire LATTICE
CNRS, École Normale Supérieure, U. Paris 3 Sorbonne Nouvelle
</affiliation>
<address confidence="0.9728545">
1, rue Maurice Arnoux
92120 Montrouge, France
</address>
<email confidence="0.997188">
{pablo.ruiz.fabo,thierry.poibeau}@ens.fr
</email>
<sectionHeader confidence="0.996953" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.7796666">
Our participation at SemEval’s Multilingual
All-Words Sense Disambiguation and Entity
Linking task is described. An English entity
linking (EL) system is presented, which com-
bines the annotations of four public open
source EL services. The annotations are com-
bined through a weighted voting scheme in-
spired on the ROVER method, which had not
been previously tested on EL outputs. Results
on the task’s EL items were competitive.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999479105263158">
The paper describes our participation at SemEval
2015, Task 13 (Moro and Navigli, 2015): Multilin-
gual all-words Sense Disambiguation (WSD) and
Entity Linking (EL). Systems performing both
tasks, or either one, can participate. The preferred
word-sense and entity inventory is Babelnet (Navi-
gli and Ponzetto, 2012); other inventories are al-
lowed. Our system performs English EL to
Wikipedia, combining the output of open-source,
publicly available EL systems via weighted voting.
The system is relevant to the task’s interest in
comparing the results of EL systems that apply
encyclopedic knowledge only, like ours, and sys-
tems that jointly exploit encyclopedic and lexico-
graphic resources for EL.
The paper’s structure is the following: Section 2
discusses related work, and Section 3 describes the
system. Sections 4 and 5 present the results and a
conclusion.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999908592592593">
General surveys on EL can be found in (Cornolti et
al., 2013) and (Rao et al., 2013). Work on combin-
ing NLP annotators and on evaluating EL systems
is particularly relevant for our submission.
The goal of combining different NLP systems is
obtaining combined results that are better than the
results of each individual system. Fiscus (1997)
created the ROVER method, with weighted voting
to improve speech recognition outputs. A ROVER
was found to improve parsing results by De la
Clergerie et al. (2008). Rizzo et al. (2014) im-
proved Named Entity Recognition results, combin-
ing systems via different machine learning
algorithms. Our approach is inspired on the
ROVER method, which had not been previously
attempted for EL to our knowledge. Systems that
combine entity linkers exist (NERD, Rizzo and
Troncy, 2012). However, a difference in our sys-
tem is that the set of linkers we combine is public
and open-source. A second difference is the set of
methods we employed to combine annotations.
EL evaluation work (Cornolti et al., 2013),
(Usbeck et al., 2015) has highlighted to what an
extent EL systems’ performance can differ depend-
ing on characteristics of the corpus. This motivates
testing whether different EL systems, properly
combined, can complement each other.
</bodyText>
<page confidence="0.990616">
355
</page>
<note confidence="0.609178">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.988678" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999959555555556">
The system performs English EL to Wikipedia,
combining the outputs of the following EL sys-
tems: Tagme 21 (Ferragina and Scaiella, 2010),
DBpedia Spotlight2 (Mendes et al. 2011), Wikipe-
dia Miner3 (Milne and Witten, 2008) and Babelfy4
(Moro et al. 2014). Babelfy outputs were only con-
sidered if they started with a WIKI prefix or their
first character was uppercase.5 Details about each
of our workflow’s steps follow.
</bodyText>
<subsectionHeader confidence="0.996171">
3.1 Individual Systems’ Thresholds
</subsectionHeader>
<bodyText confidence="0.9998863">
First of all, a client requests the annotations for a
text from each linker’s web-service, using the ser-
vices’ default settings except for the confidence
threshold, which is configured in our system. An-
notations whose confidence is below a threshold
are eliminated.
All of the linkers used, except Babelfy, output
confidence scores for their annotations. Cornolti et
al., (2013) reported optimal confidence-score
thresholds for all our linkers (except Babelfy). Us-
ing Cornolti’s BAT Framework, we verified that
the thresholds are still valid.6 We adopted the
weak-annotation match thresholds for the IITB
dataset, since we consider the IITB corpus close to
the task’s data, in text-length and topical variety.
Our thresholds were 0.102 for Tagme, 0.023 for
Spotlight, and 0.219 for Wikipedia Miner. Since
Babelfy does not output confidence scores, all of
its annotations were accepted to the next step in the
workflow.
</bodyText>
<subsectionHeader confidence="0.999925">
3.2 Ranking the Systems to Combine
</subsectionHeader>
<bodyText confidence="0.9989865">
Our method for combining annotators’ outputs re-
quires the annotators to be previously ranked for
precision on an annotated reference set. It is not
viable to annotate a reference set for each new cor-
pus. To help overcome this issue, we adopt the fol-
lowing heuristic: We have ranked the annotators
</bodyText>
<footnote confidence="0.865541777777778">
1 http://tagme.di.unipi.it/tagme_help.html
2 https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki
3 http://wikipedia-miner.cms.waikato.ac.nz/
4 http://babelfy.org/download.jsp
5 Babelfy was a late addition to our pipeline; the reader will
note that we made some ad-hoc decisions to benefit from its
outputs while complying with previously defined features in
our workflow.
6 https://github.com/marcocor/bat-framework
</footnote>
<bodyText confidence="0.999566025641026">
on a series of very different reference corpora. To
perform EL on a new corpus, our heuristic consid-
ers the following criteria: First, the types of EL
annotations needed by the user. Second, how simi-
lar the new corpus is (along dimensions described
below) to the reference corpora on which we have
pre-ranked the annotators. To apply the workflow
to a new corpus, the heuristic chooses the annota-
tor-ranking obtained with the reference corpus that
is most similar to that new corpus, while still re-
specting the annotation-types needed by the user.
The reference corpora on which we pre-ranked
the annotators are AIDA/CoNLL Test B (Hoffart
et al., 2011), and IITB (Kulkarni et al., 2009).
These corpora are very different to each other, in
terms of character length, topical variety, and re-
garding whether they annotate common-noun men-
tions or not. Moreover, some EL systems obtain
opposite results when evaluated on AIDA/CoNLL
B vs. IITB, as tests by Cornolti et al. (2013) and on
the GERBIL platform7 have shown.
The heuristic’s first criterion is the types of an-
notations needed: If the user needs annotations for
common-noun mentions, the IITB ranking is used,
since IITB is the only one in our reference-datasets
that was annotated for such mentions. If the user
does not need common noun annotations, our heu-
ristic compares the user’s corpus with our two ref-
erence corpora in terms of character length and of
a measure of lexical cohesion. Both factors have
been argued to influence linkers’ uneven results
across corpora (Cornolti et al., 2013).
We accepted common-noun annotations for the
task, as they were relevant for the task’s domains
(e.g. disease names for the biomedical texts). Ac-
cordingly, the heuristic ranked annotators as per
their IITB results: 1st Wikipedia Miner (0.568 pre-
cision), 2nd Babelfy (0.493), 3rd Spotlight (0.462),
4th Tagme (0.452).8
</bodyText>
<subsectionHeader confidence="0.999726">
3.3 Weighting and Selecting Annotations
</subsectionHeader>
<bodyText confidence="0.897069444444445">
Using the linker ranking from the previous step,
the annotations are voted, and selected for final
output or rejected based on the vote. We used two
7 See http://gerbil.aksw.org/gerbil/overview at the site for the
GERBIL platform (Usbeck et al., 2015):
8 The precision is from tests in Cornolti et al., 2013, using
weak-annotation-match. Babefly was not tested. In order to be
able to rank it, instead of its precision we assigned it the aver-
age of all other annotators’ precisions.
</bodyText>
<page confidence="0.995524">
356
</page>
<bodyText confidence="0.992876709677419">
voting schemes. The first one relies on each anno-
tation’s confidence score, weighted by the annota-
tor’s rank and precision on the ranking datasets
from 3.2. The rationale is that a high-confidence
annotation for a low-ranked annotator can be better
than a low-confidence annotation for a higher-
ranked annotator. The definition is in Figure 1: For
each annotation (m, e) in the results, m is its men-
tion,9 e is the entity paired with m, and Ωm is the
set of annotations in the results whose mentions
overlap10 with m. If the size of Ωm is 1, the scaled
confidence11 oscf of Ωm’s unique annotation ο must
reach threshold tuniq in order for ο to be accepted.
Threshold tuniq is the average of the scaled confi-
dence scores for all annotations in the corpus. If
Ωm has more than one annotation, the voting is
thus: For each annotation ο in Ωm, ο’s vote is a
product determined by several factors: oscf is o’s
scaled confidence.12 N is the total number of anno-
tators we combine (i.e. 4). Operand roant is the rank
of annotator oant, which produced annotation o.
Poant is that annotator’s precision on the ranking
reference corpus (3.2 above). For roant, 0 is the best
rank and N – 1 the worst. Parameter α influences
the distance between the annotations’ votes based
on their annotators’ rank, and was set at 0. The
annotation with the highest vote in Ωm is accepted;
the rest are rejected.
for each set Ωm of overlapping annotations:
if |Ωm |= 1
for o ∊ Ωm: if oscf ≥ tuniq accept o
</bodyText>
<figure confidence="0.720897">
else reject o
else
select max [(oSCf • (N − ( roant − α )) • Poant]
o ∊ Ωm
</figure>
<figureCaption confidence="0.999364">
Figure 1: Annotation voting scheme used in Run 1.
</figureCaption>
<bodyText confidence="0.4165216">
9 The string of characters in the text that the annotation is
based on (the term mention is often used in EL for this notion).
10 Assume two mentions (p1, e1) and (p2, e2), where p1 and
p2 are the mentions’ first character indices, and e1 and e2 are
the mentions’ last character indices. The mentions overlap iff
</bodyText>
<equation confidence="0.987009">
((p1 = p2) A (e1 = e2)) ˅ ((p1 = p2) A (e1 &lt; e2)) ˅ ((p1 = p2) A
(e2 &lt; e1)) ˅ ((e1 = e2) A (p1 &lt; p2)) ˅ ((e1 = e2) A (p2 &lt; p1)) ˅
((p1 &lt; p2) A (p2 &lt; e1)) ˅ ((p2 &lt; p1) A (p1 &lt; e2)).
</equation>
<bodyText confidence="0.904849692307692">
11 Since the range of confidence-scores output by each annota-
tor was different, we minmax-scaled all original (orig) confi-
dence scores to a 0-1 range: scaled_confidence =
(orig_confidence – corpus_min_orig_confidence) /
(corpus_max_orig_confidence – corpus_min_orig_confidence)
12 As Babelfy does not provide confidence scores, its annota-
tions were assigned the average over the whole result-set of
the scaled confidence-scores output by the other annotators.
The second voting scheme is similar to the
ROVER method in (De la Clergerie et al., 2008).
The method assesses annotations based on how
many linkers have produced them, using the link-
ers’ rank, and their precision on the ranking-sets,
as weights. If enough lower-ranked annotators
have linked to an entity, this entity can win over an
entity proposed by a higher-ranked annotator.
The voting is defined in Figure 2. For each an-
notation (mention m, entity e), Ωm is the set of an-
notations whose mentions overlap10 with m. Based
on the different entities in Ωm’s annotations, Ωm is
divided into disjoint subsets, each of which con-
tains annotations linking to a different entity. Each
of these subsets L is voted by vote(L). In vote(L),
for each annotation o in L, terms N, roant, α, Poant
have the same meaning as the terms bearing the
same names in Figure 1, and are described above.
</bodyText>
<equation confidence="0.945765428571429">
for each set Ωm of overlapping annotations:
for L ∊ Ωm:
∑ o ∈ L (N − ( roant − α )) • Poant
vote(L) =
N
if max (vote(L) ) &gt; Pmax : select argmax(vote(L))
L ∊ ΩmL ∊ Ωm
</equation>
<figureCaption confidence="0.99557">
Figure 2: Entity voting scheme used in Runs 2 and 3.
</figureCaption>
<bodyText confidence="0.999989166666667">
The entity for the subset L which obtains the
highest vote among Ωm’s subsets is selected if its
vote is higher than Pmax, i.e. the maximum preci-
sion in the ranking dataset (0.568, see Section 3.2).
After selecting the winning entity, we still need to
select a mention for it. The mention is selected at
random among the mentions of the annotations in
the winning subset L. This implementation of men-
tion selection is meant as a baseline that can be
refined in the future. Two initial factors to consider
in mention selection would be mention length and
the annotators having chosen each mention.
</bodyText>
<subsectionHeader confidence="0.920175">
3.4 Entity Classification
</subsectionHeader>
<bodyText confidence="0.999980714285714">
After the vote, entities in the selected annotations
are classified before final output. The classification
is rule-based. It exploits the category or type labels
output by the EL services we combined—except
Babelfy, which does not output such information.
The classification-rules are based on type labels
in the NERD ontology (Rizzo and Troncy, 2012)13
</bodyText>
<footnote confidence="0.731267">
13 http://nerd.eurecom.fr/ontology/nerd-v0.5.n3
</footnote>
<page confidence="0.997514">
357
</page>
<bodyText confidence="0.999949">
and on a subset of the DBpedia ontology classes
(Mendes et al. 2011)14 relevant for the task’s do-
mains. For types Person, Location, Organization,
Wikipedia category labels were also exploited.
Some rules involve an exact match against the
annotations’ categories or types, e.g. “Assign type
Location if the annotation has type DBpe-
dia:Place”. Some rules involve a partial match,
e.g. “Assign type Person if one of the Wikipedia
category labels for the entity contains births”.
For Babelfy outputs, Wikipedia category labels
and DBpedia types were obtained through Wikipe-
dia Miner’s3 and DBpedia’s15 APIs.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.995236916666666">
Since the task was open to systems doing either
WSD or EL, or both, the corpus targeted both
WSD and EL. Participant systems were evaluated
on a different set of items depending on their na-
ture (EL only, WSD only, both). The corpus con-
tained 4 generic and domain-specific documents
with 1094 single-word instances, 82 multi-words
and 86 named entities (NE).
Our system was conceived and evaluated as an
EL system. Table 1 shows our precision, recall and
F1 for all three runs. Column TopF1 is the maxi-
mum F1 attained by a participant on the EL items.
</bodyText>
<table confidence="0.99971875">
EL P R F1 TopF1
Run1 100 75.6 86.1
Run2 98.3 66.3 79.2 88.9
Run3 100 66.3 79.7
</table>
<tableCaption confidence="0.999971">
Table 1: English EL results for all domains.
</tableCaption>
<bodyText confidence="0.999812384615385">
Run 1 results were competitive, ranking 3rd of 10,
if we compare all participants’ best runs. Runs 2
and 3 lag behind, due to lower recall. Run 1 em-
ployed the voting scheme in Figure 1. Runs 2 and
3 correspond to the scheme defined in Figure 2,
with parameter α set to 0 in Run 2 and to 1 in
Run 3. In spite of its results, the voting scheme
from Figure 2 has advantages over the first one: It
does not require confidence scores, so it accom-
modates linkers that don’t score their annotations.
Also, it does not need a separate threshold to de-
cide on annotations produced by one annotator on-
ly. More work is needed to determine the reason
</bodyText>
<footnote confidence="0.985747">
14 http://mappings.dbpedia.org/server/ontology/classes/
15 http://dbpedia.org/sparql
</footnote>
<bodyText confidence="0.998759181818182">
for this difference in results, i.e. whether the sec-
ond approach itself is not useful to combine EL
annotations, or whether its worse results were re-
lated to our implementation.
One of the task’s purposes was to compare sys-
tems’ performance across domains. Table 2 shows
our best run’s results per domain. Column N re-
flects the number of EL items in the corpus for
each domain. All other columns have the same
meaning as in Table 1, but considering the per-
domain results.
</bodyText>
<table confidence="0.9995275">
N P R F1 TopF1
Biomedical 48 100 83.3 90.9 100
Math &amp; Computer 22 100 54.4 70.6 74.3
General 16 100 81.3 89.7 90.3
</table>
<tableCaption confidence="0.999764">
Table 2: English EL Run 1 results by domain.
</tableCaption>
<bodyText confidence="0.999947416666667">
Note that the small number of EL items availa-
ble for each domain limits in our opinion the relia-
bility of interpretations for these results.
Since our workflow combines several EL sys-
tems, it would be interesting to compare results for
each individual system by itself vs. the results for
the combined system. In later work (Ruiz and
Poibeau, 2015), using an improved version of the
system described here, and larger EL golden-sets,
we performed such comparisons, finding signifi-
cant improvements in the combined system vs. the
individual ones.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999952230769231">
The entity linking (EL) system presented was
ranked 3rd (out of 10) on the task’s EL items. The
system combines the outputs of four public open
source EL services. Two weighted voting methods
were described to combine the outputs. The first
method relies on annotations’ confidence scores;
the second one is a weighted majority vote. The
first method obtained better results, but the second
one has the advantage of being easily applicable to
non-scored annotations. More work is needed to
assess the reasons for the methods’ differential per-
formance. Future work also includes adding other
public open source systems to the workflow.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.6212375">
Pablo Ruiz was supported by a PhD scholarship
from Région Île-de-France.
</reference>
<page confidence="0.998325">
358
</page>
<sectionHeader confidence="0.996345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913513157895">
Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. (2013). A framework for benchmarking
entity-annotation systems. In Proc. of WWW, 249–
260.
Éric Vilemonte De La Clergerie, Olivier Hamon, Dja-
mel Mostefa, Christelle Ayache, Patrick Paroubek,
and Anne Vilnat. (2008). Passage: from French par-
ser evaluation to large sized treebank. In Proc. of
LREC 2008, 3570–3576.
Paolo Ferragina and Ugo Scaiella. (2010). Tagme: on-
the-fly annotation of short text fragments (by wikipe-
dia entities). In Proc. of CIKM’10, 1625–1628.
Jonathan G Fiscus. (1997). A post-processing system to
yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. of the
IEEE Workshop on Automatic Speech Recognition
and Understanding, 1997, 347–354.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordi-
no, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. (2011). Robust disambiguation of named
entities in text. In Proc. of EMNLP, 782–792.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. (2009). Collective annota-
tion of Wikipedia entities in web text. In Proc. ACM
SIGKDD, 457–466.
Pablo N. Mendes, Max Jakob, Andrés García-Silva, and
Christian Bizer. (2011). DBpedia spotlight: shedding
light on the web of documents. In Proc. of the 7th
Int. Conf. on Semantic Systems, I-SEMANTICS’11,
1–8.
David Milne and Ian H. Witten. (2008). An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proc. of AAAI Workshop on
Wikipedia and Artificial Intelligence: an Evolving
Synergy, 25–30
Andrea Moro and Roberto Navigli (2015) SemEval-
2015 Task 13: Multilingual All-Words Sense Disam-
biguation and Entity Linking. In Proc. of SemEval-
2015.
Andrea Moro, Alessandro Raganato, and Roberto Navi-
gli. (2014). Entity Linking meets Word Sense Dis-
ambiguation: A Unified Approach. Transactions of
the ACL, 2, 231–244.
Roberto Navigli and Simone Ponzetto. (2012).
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193, 217–
250.
Delip Rao, Paul McNamee, and Mark Dredze. (2013).
Entity linking: Finding extracted entities in a
knowledge base. In Multi-source, Multilingual In-
formation Extraction and Summarization, 93–115.
Springer.
Giuseppe Rizzo and Raphaël Troncy. (2012). NERD: a
framework for unifying named entity recognition and
disambiguation extraction tools. In Proc. of the
Demonstrations at EACL’12, 73–76.
Giuseppe Rizzo, Marieke van Erp, and Raphaël Troncy.
(2014). Benchmarking the Extraction and Disambig-
uation of Named Entities on the Semantic Web. In
Proc. of LREC 2014, 4593–4600.
Pablo Ruiz and Thierry Poibeau. (2015). Combining
Open Source Annotators for Entity Linking through
Weighted Voting. In Proceedings of *SEM 2015.
Fourth Joint Conference on Lexical and Computa-
tional Semantics. Denver, U.S.
Ricardo Usbeck, Michael Röder, Axel-Cyrille Ngonga,
Ciro Baron, Andrea Both, Martin Brümmer, Diego
Ceccarelli, Marco Cornolti, Didier Cherix, Bernd
Eickmann, Paolo Ferragina, Christiane Lemke, An-
drea Moro, Roberto Navigli, Francesco Piccino,
Giuseppe Rizzo, Harald Sack, René Speck, Raphaël
Troncy, Jörg Waitelonis, and Lars Wesemann.
(2015). GERBIL–General Entity Annotator Bench-
marking Framework. In Proc. of WWW.
</reference>
<page confidence="0.999077">
359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.268869">
<title confidence="0.8285005">EL92: Entity Linking Combining Open Source via Weighted Voting</title>
<author confidence="0.590533">Ruiz</author>
<affiliation confidence="0.6391965">Laboratoire CNRS, École Normale Supérieure, U. Paris 3 Sorbonne</affiliation>
<address confidence="0.9506935">1, rue Maurice 92120 Montrouge,</address>
<email confidence="0.998157">pablo.ruiz.fabo@ens.fr</email>
<email confidence="0.998157">thierry.poibeau@ens.fr</email>
<abstract confidence="0.999060545454546">Our participation at SemEval’s Multilingual All-Words Sense Disambiguation and Entity Linking task is described. An English entity linking (EL) system is presented, which combines the annotations of four public open source EL services. The annotations are combined through a weighted voting scheme inspired on the ROVER method, which had not been previously tested on EL outputs. Results on the task’s EL items were competitive.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Pablo Ruiz was supported by a PhD scholarship from Région Île-de-France.</title>
<marker></marker>
<rawString>Pablo Ruiz was supported by a PhD scholarship from Région Île-de-France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Cornolti</author>
<author>Paolo Ferragina</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>A framework for benchmarking entity-annotation systems.</title>
<date>2013</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>249--260</pages>
<contexts>
<context position="1664" citStr="Cornolti et al., 2013" startWordPosition="247" endWordPosition="250">ther inventories are allowed. Our system performs English EL to Wikipedia, combining the output of open-source, publicly available EL systems via weighted voting. The system is relevant to the task’s interest in comparing the results of EL systems that apply encyclopedic knowledge only, like ours, and systems that jointly exploit encyclopedic and lexicographic resources for EL. The paper’s structure is the following: Section 2 discusses related work, and Section 3 describes the system. Sections 4 and 5 present the results and a conclusion. 2 Related Work General surveys on EL can be found in (Cornolti et al., 2013) and (Rao et al., 2013). Work on combining NLP annotators and on evaluating EL systems is particularly relevant for our submission. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the</context>
<context position="3913" citStr="Cornolti et al., (2013)" startWordPosition="599" endWordPosition="602">itten, 2008) and Babelfy4 (Moro et al. 2014). Babelfy outputs were only considered if they started with a WIKI prefix or their first character was uppercase.5 Details about each of our workflow’s steps follow. 3.1 Individual Systems’ Thresholds First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold, which is configured in our system. Annotations whose confidence is below a threshold are eliminated. All of the linkers used, except Babelfy, output confidence scores for their annotations. Cornolti et al., (2013) reported optimal confidence-score thresholds for all our linkers (except Babelfy). Using Cornolti’s BAT Framework, we verified that the thresholds are still valid.6 We adopted the weak-annotation match thresholds for the IITB dataset, since we consider the IITB corpus close to the task’s data, in text-length and topical variety. Our thresholds were 0.102 for Tagme, 0.023 for Spotlight, and 0.219 for Wikipedia Miner. Since Babelfy does not output confidence scores, all of its annotations were accepted to the next step in the workflow. 3.2 Ranking the Systems to Combine Our method for combining</context>
<context position="6183" citStr="Cornolti et al. (2013)" startWordPosition="944" endWordPosition="947"> new corpus, the heuristic chooses the annotator-ranking obtained with the reference corpus that is most similar to that new corpus, while still respecting the annotation-types needed by the user. The reference corpora on which we pre-ranked the annotators are AIDA/CoNLL Test B (Hoffart et al., 2011), and IITB (Kulkarni et al., 2009). These corpora are very different to each other, in terms of character length, topical variety, and regarding whether they annotate common-noun mentions or not. Moreover, some EL systems obtain opposite results when evaluated on AIDA/CoNLL B vs. IITB, as tests by Cornolti et al. (2013) and on the GERBIL platform7 have shown. The heuristic’s first criterion is the types of annotations needed: If the user needs annotations for common-noun mentions, the IITB ranking is used, since IITB is the only one in our reference-datasets that was annotated for such mentions. If the user does not need common noun annotations, our heuristic compares the user’s corpus with our two reference corpora in terms of character length and of a measure of lexical cohesion. Both factors have been argued to influence linkers’ uneven results across corpora (Cornolti et al., 2013). We accepted common-no</context>
<context position="7426" citStr="Cornolti et al., 2013" startWordPosition="1144" endWordPosition="1147"> task, as they were relevant for the task’s domains (e.g. disease names for the biomedical texts). Accordingly, the heuristic ranked annotators as per their IITB results: 1st Wikipedia Miner (0.568 precision), 2nd Babelfy (0.493), 3rd Spotlight (0.462), 4th Tagme (0.452).8 3.3 Weighting and Selecting Annotations Using the linker ranking from the previous step, the annotations are voted, and selected for final output or rejected based on the vote. We used two 7 See http://gerbil.aksw.org/gerbil/overview at the site for the GERBIL platform (Usbeck et al., 2015): 8 The precision is from tests in Cornolti et al., 2013, using weak-annotation-match. Babefly was not tested. In order to be able to rank it, instead of its precision we assigned it the average of all other annotators’ precisions. 356 voting schemes. The first one relies on each annotation’s confidence score, weighted by the annotator’s rank and precision on the ranking datasets from 3.2. The rationale is that a high-confidence annotation for a low-ranked annotator can be better than a low-confidence annotation for a higherranked annotator. The definition is in Figure 1: For each annotation (m, e) in the results, m is its mention,9 e is the entity</context>
</contexts>
<marker>Cornolti, Ferragina, Ciaramita, 2013</marker>
<rawString>Marco Cornolti, Paolo Ferragina, and Massimiliano Ciaramita. (2013). A framework for benchmarking entity-annotation systems. In Proc. of WWW, 249– 260.</rawString>
</citation>
<citation valid="true">
<title>Éric Vilemonte De La Clergerie, Olivier Hamon, Djamel Mostefa, Christelle Ayache, Patrick Paroubek, and Anne Vilnat.</title>
<date>2008</date>
<booktitle>In Proc. of LREC</booktitle>
<pages>3570--3576</pages>
<contexts>
<context position="2107" citStr="(2008)" startWordPosition="322" endWordPosition="322">Section 3 describes the system. Sections 4 and 5 present the results and a conclusion. 2 Related Work General surveys on EL can be found in (Cornolti et al., 2013) and (Rao et al., 2013). Work on combining NLP annotators and on evaluating EL systems is particularly relevant for our submission. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which had not been previously attempted for EL to our knowledge. Systems that combine entity linkers exist (NERD, Rizzo and Troncy, 2012). However, a difference in our system is that the set of linkers we combine is public and open-source. A second difference is the set of methods we employed to combine annotations. EL evaluation work (Cornolti et al., 2013), (Usbeck et al., 2015) has highlighted to what an extent EL systems</context>
</contexts>
<marker>2008</marker>
<rawString>Éric Vilemonte De La Clergerie, Olivier Hamon, Djamel Mostefa, Christelle Ayache, Patrick Paroubek, and Anne Vilnat. (2008). Passage: from French parser evaluation to large sized treebank. In Proc. of LREC 2008, 3570–3576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>Tagme: onthe-fly annotation of short text fragments (by wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proc. of CIKM’10,</booktitle>
<pages>1625--1628</pages>
<contexts>
<context position="3218" citStr="Ferragina and Scaiella, 2010" startWordPosition="490" endWordPosition="493">ations. EL evaluation work (Cornolti et al., 2013), (Usbeck et al., 2015) has highlighted to what an extent EL systems’ performance can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 System Description The system performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008) and Babelfy4 (Moro et al. 2014). Babelfy outputs were only considered if they started with a WIKI prefix or their first character was uppercase.5 Details about each of our workflow’s steps follow. 3.1 Individual Systems’ Thresholds First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold, which is configured in our system. Annotations whose confidence is below a threshold are eliminated. All of the linker</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>Paolo Ferragina and Ugo Scaiella. (2010). Tagme: onthe-fly annotation of short text fragments (by wikipedia entities). In Proc. of CIKM’10, 1625–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="1943" citStr="Fiscus (1997)" startWordPosition="294" endWordPosition="295">like ours, and systems that jointly exploit encyclopedic and lexicographic resources for EL. The paper’s structure is the following: Section 2 discusses related work, and Section 3 describes the system. Sections 4 and 5 present the results and a conclusion. 2 Related Work General surveys on EL can be found in (Cornolti et al., 2013) and (Rao et al., 2013). Work on combining NLP annotators and on evaluating EL systems is particularly relevant for our submission. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which had not been previously attempted for EL to our knowledge. Systems that combine entity linkers exist (NERD, Rizzo and Troncy, 2012). However, a difference in our system is that the set of linkers we combine is public and open-source. A second difference is t</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G Fiscus. (1997). A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding, 1997, 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen Fürstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>782--792</pages>
<contexts>
<context position="5862" citStr="Hoffart et al., 2011" startWordPosition="891" endWordPosition="894">eference corpora. To perform EL on a new corpus, our heuristic considers the following criteria: First, the types of EL annotations needed by the user. Second, how similar the new corpus is (along dimensions described below) to the reference corpora on which we have pre-ranked the annotators. To apply the workflow to a new corpus, the heuristic chooses the annotator-ranking obtained with the reference corpus that is most similar to that new corpus, while still respecting the annotation-types needed by the user. The reference corpora on which we pre-ranked the annotators are AIDA/CoNLL Test B (Hoffart et al., 2011), and IITB (Kulkarni et al., 2009). These corpora are very different to each other, in terms of character length, topical variety, and regarding whether they annotate common-noun mentions or not. Moreover, some EL systems obtain opposite results when evaluated on AIDA/CoNLL B vs. IITB, as tests by Cornolti et al. (2013) and on the GERBIL platform7 have shown. The heuristic’s first criterion is the types of annotations needed: If the user needs annotations for common-noun mentions, the IITB ranking is used, since IITB is the only one in our reference-datasets that was annotated for such mention</context>
</contexts>
<marker>Hoffart, Yosef, Bordino, Fürstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. (2011). Robust disambiguation of named entities in text. In Proc. of EMNLP, 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proc. ACM SIGKDD,</booktitle>
<pages>457--466</pages>
<contexts>
<context position="5896" citStr="Kulkarni et al., 2009" startWordPosition="897" endWordPosition="900">n a new corpus, our heuristic considers the following criteria: First, the types of EL annotations needed by the user. Second, how similar the new corpus is (along dimensions described below) to the reference corpora on which we have pre-ranked the annotators. To apply the workflow to a new corpus, the heuristic chooses the annotator-ranking obtained with the reference corpus that is most similar to that new corpus, while still respecting the annotation-types needed by the user. The reference corpora on which we pre-ranked the annotators are AIDA/CoNLL Test B (Hoffart et al., 2011), and IITB (Kulkarni et al., 2009). These corpora are very different to each other, in terms of character length, topical variety, and regarding whether they annotate common-noun mentions or not. Moreover, some EL systems obtain opposite results when evaluated on AIDA/CoNLL B vs. IITB, as tests by Cornolti et al. (2013) and on the GERBIL platform7 have shown. The heuristic’s first criterion is the types of annotations needed: If the user needs annotations for common-noun mentions, the IITB ranking is used, since IITB is the only one in our reference-datasets that was annotated for such mentions. If the user does not need commo</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. (2009). Collective annotation of Wikipedia entities in web text. In Proc. ACM SIGKDD, 457–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo N Mendes</author>
<author>Max Jakob</author>
<author>Andrés García-Silva</author>
<author>Christian Bizer</author>
</authors>
<title>DBpedia spotlight: shedding light on the web of documents.</title>
<date>2011</date>
<booktitle>In Proc. of the 7th Int. Conf. on Semantic Systems, I-SEMANTICS’11,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3259" citStr="Mendes et al. 2011" startWordPosition="496" endWordPosition="499"> (Usbeck et al., 2015) has highlighted to what an extent EL systems’ performance can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 System Description The system performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008) and Babelfy4 (Moro et al. 2014). Babelfy outputs were only considered if they started with a WIKI prefix or their first character was uppercase.5 Details about each of our workflow’s steps follow. 3.1 Individual Systems’ Thresholds First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold, which is configured in our system. Annotations whose confidence is below a threshold are eliminated. All of the linkers used, except Babelfy, output confidence</context>
<context position="12364" citStr="Mendes et al. 2011" startWordPosition="2029" endWordPosition="2032">itial factors to consider in mention selection would be mention length and the annotators having chosen each mention. 3.4 Entity Classification After the vote, entities in the selected annotations are classified before final output. The classification is rule-based. It exploits the category or type labels output by the EL services we combined—except Babelfy, which does not output such information. The classification-rules are based on type labels in the NERD ontology (Rizzo and Troncy, 2012)13 13 http://nerd.eurecom.fr/ontology/nerd-v0.5.n3 357 and on a subset of the DBpedia ontology classes (Mendes et al. 2011)14 relevant for the task’s domains. For types Person, Location, Organization, Wikipedia category labels were also exploited. Some rules involve an exact match against the annotations’ categories or types, e.g. “Assign type Location if the annotation has type DBpedia:Place”. Some rules involve a partial match, e.g. “Assign type Person if one of the Wikipedia category labels for the entity contains births”. For Babelfy outputs, Wikipedia category labels and DBpedia types were obtained through Wikipedia Miner’s3 and DBpedia’s15 APIs. 4 Results and Discussion Since the task was open to systems doi</context>
</contexts>
<marker>Mendes, Jakob, García-Silva, Bizer, 2011</marker>
<rawString>Pablo N. Mendes, Max Jakob, Andrés García-Silva, and Christian Bizer. (2011). DBpedia spotlight: shedding light on the web of documents. In Proc. of the 7th Int. Conf. on Semantic Systems, I-SEMANTICS’11, 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective, low-cost measure of semantic relatedness obtained from Wikipedia links.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="3302" citStr="Milne and Witten, 2008" startWordPosition="503" endWordPosition="506">to what an extent EL systems’ performance can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 System Description The system performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008) and Babelfy4 (Moro et al. 2014). Babelfy outputs were only considered if they started with a WIKI prefix or their first character was uppercase.5 Details about each of our workflow’s steps follow. 3.1 Individual Systems’ Thresholds First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold, which is configured in our system. Annotations whose confidence is below a threshold are eliminated. All of the linkers used, except Babelfy, output confidence scores for their annotations. Cornolti et </context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. (2008). An effective, low-cost measure of semantic relatedness obtained from Wikipedia links. In Proc. of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy, 25–30</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<date>2015</date>
<booktitle>SemEval2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking. In Proc. of SemEval2015.</booktitle>
<contexts>
<context position="813" citStr="Moro and Navigli, 2015" startWordPosition="114" endWordPosition="117">ue Maurice Arnoux 92120 Montrouge, France {pablo.ruiz.fabo,thierry.poibeau}@ens.fr Abstract Our participation at SemEval’s Multilingual All-Words Sense Disambiguation and Entity Linking task is described. An English entity linking (EL) system is presented, which combines the annotations of four public open source EL services. The annotations are combined through a weighted voting scheme inspired on the ROVER method, which had not been previously tested on EL outputs. Results on the task’s EL items were competitive. 1 Introduction The paper describes our participation at SemEval 2015, Task 13 (Moro and Navigli, 2015): Multilingual all-words Sense Disambiguation (WSD) and Entity Linking (EL). Systems performing both tasks, or either one, can participate. The preferred word-sense and entity inventory is Babelnet (Navigli and Ponzetto, 2012); other inventories are allowed. Our system performs English EL to Wikipedia, combining the output of open-source, publicly available EL systems via weighted voting. The system is relevant to the task’s interest in comparing the results of EL systems that apply encyclopedic knowledge only, like ours, and systems that jointly exploit encyclopedic and lexicographic resource</context>
</contexts>
<marker>Moro, Navigli, 2015</marker>
<rawString>Andrea Moro and Roberto Navigli (2015) SemEval2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking. In Proc. of SemEval2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity Linking meets Word Sense Disambiguation: A Unified Approach.</title>
<date>2014</date>
<journal>Transactions of the ACL,</journal>
<volume>2</volume>
<pages>231--244</pages>
<contexts>
<context position="3334" citStr="Moro et al. 2014" startWordPosition="509" endWordPosition="512">nce can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 System Description The system performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008) and Babelfy4 (Moro et al. 2014). Babelfy outputs were only considered if they started with a WIKI prefix or their first character was uppercase.5 Details about each of our workflow’s steps follow. 3.1 Individual Systems’ Thresholds First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold, which is configured in our system. Annotations whose confidence is below a threshold are eliminated. All of the linkers used, except Babelfy, output confidence scores for their annotations. Cornolti et al., (2013) reported optimal con</context>
</contexts>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Andrea Moro, Alessandro Raganato, and Roberto Navigli. (2014). Entity Linking meets Word Sense Disambiguation: A Unified Approach. Transactions of the ACL, 2, 231–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>217--250</pages>
<contexts>
<context position="1039" citStr="Navigli and Ponzetto, 2012" startWordPosition="145" endWordPosition="149"> linking (EL) system is presented, which combines the annotations of four public open source EL services. The annotations are combined through a weighted voting scheme inspired on the ROVER method, which had not been previously tested on EL outputs. Results on the task’s EL items were competitive. 1 Introduction The paper describes our participation at SemEval 2015, Task 13 (Moro and Navigli, 2015): Multilingual all-words Sense Disambiguation (WSD) and Entity Linking (EL). Systems performing both tasks, or either one, can participate. The preferred word-sense and entity inventory is Babelnet (Navigli and Ponzetto, 2012); other inventories are allowed. Our system performs English EL to Wikipedia, combining the output of open-source, publicly available EL systems via weighted voting. The system is relevant to the task’s interest in comparing the results of EL systems that apply encyclopedic knowledge only, like ours, and systems that jointly exploit encyclopedic and lexicographic resources for EL. The paper’s structure is the following: Section 2 discusses related work, and Section 3 describes the system. Sections 4 and 5 present the results and a conclusion. 2 Related Work General surveys on EL can be found i</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Ponzetto. (2012). BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193, 217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Paul McNamee</author>
<author>Mark Dredze</author>
</authors>
<title>Entity linking: Finding extracted entities in a knowledge base.</title>
<date>2013</date>
<booktitle>In Multi-source, Multilingual Information Extraction and Summarization,</booktitle>
<pages>93--115</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1687" citStr="Rao et al., 2013" startWordPosition="252" endWordPosition="255">. Our system performs English EL to Wikipedia, combining the output of open-source, publicly available EL systems via weighted voting. The system is relevant to the task’s interest in comparing the results of EL systems that apply encyclopedic knowledge only, like ours, and systems that jointly exploit encyclopedic and lexicographic resources for EL. The paper’s structure is the following: Section 2 discusses related work, and Section 3 describes the system. Sections 4 and 5 present the results and a conclusion. 2 Related Work General surveys on EL can be found in (Cornolti et al., 2013) and (Rao et al., 2013). Work on combining NLP annotators and on evaluating EL systems is particularly relevant for our submission. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which ha</context>
</contexts>
<marker>Rao, McNamee, Dredze, 2013</marker>
<rawString>Delip Rao, Paul McNamee, and Mark Dredze. (2013). Entity linking: Finding extracted entities in a knowledge base. In Multi-source, Multilingual Information Extraction and Summarization, 93–115. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Rizzo</author>
<author>Raphaël Troncy</author>
</authors>
<title>NERD: a framework for unifying named entity recognition and disambiguation extraction tools.</title>
<date>2012</date>
<booktitle>In Proc. of the Demonstrations at EACL’12,</booktitle>
<pages>73--76</pages>
<contexts>
<context position="2416" citStr="Rizzo and Troncy, 2012" startWordPosition="367" endWordPosition="370">n. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which had not been previously attempted for EL to our knowledge. Systems that combine entity linkers exist (NERD, Rizzo and Troncy, 2012). However, a difference in our system is that the set of linkers we combine is public and open-source. A second difference is the set of methods we employed to combine annotations. EL evaluation work (Cornolti et al., 2013), (Usbeck et al., 2015) has highlighted to what an extent EL systems’ performance can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, Denver, Colorado, June 4-5, 2015. c�2</context>
<context position="12241" citStr="Rizzo and Troncy, 2012" startWordPosition="2013" endWordPosition="2016">the winning subset L. This implementation of mention selection is meant as a baseline that can be refined in the future. Two initial factors to consider in mention selection would be mention length and the annotators having chosen each mention. 3.4 Entity Classification After the vote, entities in the selected annotations are classified before final output. The classification is rule-based. It exploits the category or type labels output by the EL services we combined—except Babelfy, which does not output such information. The classification-rules are based on type labels in the NERD ontology (Rizzo and Troncy, 2012)13 13 http://nerd.eurecom.fr/ontology/nerd-v0.5.n3 357 and on a subset of the DBpedia ontology classes (Mendes et al. 2011)14 relevant for the task’s domains. For types Person, Location, Organization, Wikipedia category labels were also exploited. Some rules involve an exact match against the annotations’ categories or types, e.g. “Assign type Location if the annotation has type DBpedia:Place”. Some rules involve a partial match, e.g. “Assign type Person if one of the Wikipedia category labels for the entity contains births”. For Babelfy outputs, Wikipedia category labels and DBpedia types wer</context>
</contexts>
<marker>Rizzo, Troncy, 2012</marker>
<rawString>Giuseppe Rizzo and Raphaël Troncy. (2012). NERD: a framework for unifying named entity recognition and disambiguation extraction tools. In Proc. of the Demonstrations at EACL’12, 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Rizzo</author>
<author>Marieke van Erp</author>
<author>Raphaël Troncy</author>
</authors>
<title>Benchmarking the Extraction and Disambiguation of Named Entities on the Semantic Web. In</title>
<date>2014</date>
<booktitle>Proc. of LREC 2014,</booktitle>
<pages>4593--4600</pages>
<marker>Rizzo, van Erp, Troncy, 2014</marker>
<rawString>Giuseppe Rizzo, Marieke van Erp, and Raphaël Troncy. (2014). Benchmarking the Extraction and Disambiguation of Named Entities on the Semantic Web. In Proc. of LREC 2014, 4593–4600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Ruiz</author>
<author>Thierry Poibeau</author>
</authors>
<title>Combining Open Source Annotators for Entity Linking through Weighted Voting.</title>
<date>2015</date>
<booktitle>In Proceedings of *SEM 2015. Fourth Joint Conference on Lexical and Computational Semantics.</booktitle>
<location>Denver, U.S.</location>
<contexts>
<context position="15304" citStr="Ruiz and Poibeau, 2015" startWordPosition="2542" endWordPosition="2545">pus for each domain. All other columns have the same meaning as in Table 1, but considering the perdomain results. N P R F1 TopF1 Biomedical 48 100 83.3 90.9 100 Math &amp; Computer 22 100 54.4 70.6 74.3 General 16 100 81.3 89.7 90.3 Table 2: English EL Run 1 results by domain. Note that the small number of EL items available for each domain limits in our opinion the reliability of interpretations for these results. Since our workflow combines several EL systems, it would be interesting to compare results for each individual system by itself vs. the results for the combined system. In later work (Ruiz and Poibeau, 2015), using an improved version of the system described here, and larger EL golden-sets, we performed such comparisons, finding significant improvements in the combined system vs. the individual ones. 5 Conclusion The entity linking (EL) system presented was ranked 3rd (out of 10) on the task’s EL items. The system combines the outputs of four public open source EL services. Two weighted voting methods were described to combine the outputs. The first method relies on annotations’ confidence scores; the second one is a weighted majority vote. The first method obtained better results, but the second</context>
</contexts>
<marker>Ruiz, Poibeau, 2015</marker>
<rawString>Pablo Ruiz and Thierry Poibeau. (2015). Combining Open Source Annotators for Entity Linking through Weighted Voting. In Proceedings of *SEM 2015. Fourth Joint Conference on Lexical and Computational Semantics. Denver, U.S.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ricardo Usbeck</author>
<author>Michael Röder</author>
<author>Axel-Cyrille Ngonga</author>
<author>Ciro Baron</author>
<author>Andrea Both</author>
<author>Martin Brümmer</author>
<author>Diego Ceccarelli</author>
<author>Marco Cornolti</author>
<author>Didier Cherix</author>
<author>Bernd Eickmann</author>
<author>Paolo Ferragina</author>
</authors>
<title>GERBIL–General Entity Annotator Benchmarking Framework.</title>
<date>2015</date>
<journal>Sack, René Speck, Raphaël Troncy, Jörg Waitelonis, and</journal>
<booktitle>In Proc. of WWW.</booktitle>
<location>Christiane Lemke, Andrea Moro, Roberto Navigli, Francesco Piccino, Giuseppe Rizzo, Harald</location>
<contexts>
<context position="2662" citStr="Usbeck et al., 2015" startWordPosition="410" endWordPosition="413">found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which had not been previously attempted for EL to our knowledge. Systems that combine entity linkers exist (NERD, Rizzo and Troncy, 2012). However, a difference in our system is that the set of linkers we combine is public and open-source. A second difference is the set of methods we employed to combine annotations. EL evaluation work (Cornolti et al., 2013), (Usbeck et al., 2015) has highlighted to what an extent EL systems’ performance can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 System Description The system performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), W</context>
<context position="7370" citStr="Usbeck et al., 2015" startWordPosition="1133" endWordPosition="1136">al., 2013). We accepted common-noun annotations for the task, as they were relevant for the task’s domains (e.g. disease names for the biomedical texts). Accordingly, the heuristic ranked annotators as per their IITB results: 1st Wikipedia Miner (0.568 precision), 2nd Babelfy (0.493), 3rd Spotlight (0.462), 4th Tagme (0.452).8 3.3 Weighting and Selecting Annotations Using the linker ranking from the previous step, the annotations are voted, and selected for final output or rejected based on the vote. We used two 7 See http://gerbil.aksw.org/gerbil/overview at the site for the GERBIL platform (Usbeck et al., 2015): 8 The precision is from tests in Cornolti et al., 2013, using weak-annotation-match. Babefly was not tested. In order to be able to rank it, instead of its precision we assigned it the average of all other annotators’ precisions. 356 voting schemes. The first one relies on each annotation’s confidence score, weighted by the annotator’s rank and precision on the ranking datasets from 3.2. The rationale is that a high-confidence annotation for a low-ranked annotator can be better than a low-confidence annotation for a higherranked annotator. The definition is in Figure 1: For each annotation (</context>
</contexts>
<marker>Usbeck, Röder, Ngonga, Baron, Both, Brümmer, Ceccarelli, Cornolti, Cherix, Eickmann, Ferragina, 2015</marker>
<rawString>Ricardo Usbeck, Michael Röder, Axel-Cyrille Ngonga, Ciro Baron, Andrea Both, Martin Brümmer, Diego Ceccarelli, Marco Cornolti, Didier Cherix, Bernd Eickmann, Paolo Ferragina, Christiane Lemke, Andrea Moro, Roberto Navigli, Francesco Piccino, Giuseppe Rizzo, Harald Sack, René Speck, Raphaël Troncy, Jörg Waitelonis, and Lars Wesemann. (2015). GERBIL–General Entity Annotator Benchmarking Framework. In Proc. of WWW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>