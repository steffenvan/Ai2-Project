<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.46827">
Mistake-Driven Mixture of Hierarchical Tag Context Trees
</title>
<author confidence="0.337907">
Masahiko Haruno
</author>
<affiliation confidence="0.23275">
NTT Communication Science Laboratories
</affiliation>
<address confidence="0.5182585">
1-1 Hikari-No-Oka Yokosuka-Shi
Kanagawa 239, Japan
</address>
<email confidence="0.718509">
harunoOcslab.kecl.ntt.co.jp
</email>
<note confidence="0.7685205">
Yuji Matsumoto
NAIST
</note>
<address confidence="0.6886735">
8916-3 Takayama-cho Ikoma-Shi
Nara 630-01, Japan
</address>
<email confidence="0.941891">
matsuOis.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.993331" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999228">
This paper proposes a mistake-driven mix-
ture method for learning a tag model. The
method iteratively performs two proce-
dures: 1. constructing a tag model based
on the current data distribution and 2.
updating the distribution by focusing on
data that are not well predicted by the
constructed model. The final tag model
is constructed by mixing all the models
according to their performance. To well
reflect the data distribution, we repre-
sent each tag model as a hierarchical tag
(i.e.,NTT1 &lt; proper noun &lt; noun) con-
text tree. By using the hierarchical tag
context tree, the constituents of sequential
tag models gradually change from broad
coverage tags (e.g.,noun) to specific excep-
tional words that cannot be captured by
gener&apos;al tags. In other words, the method
incorporates not only frequent connec-
tions but also infrequent ones that are of-
ten considered to be collocational. We
evaluate several tag models by implement-
ing Japanese part-of-speech taggers that
share all other conditions (i.e.,dictionary
and word model) other than their tag
models. The experimental results show
the proposed method significantly outper-
forms both hand-crafted and conventional
statistical methods.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985366074074074">
The last few years have seen the great success of
stochastic part-of-speech (POS) taggers (Church,
1988; Kupiec, 1992; Charniak et al., 1993; Brill,
1992; Nagata, 1994). The stochastic approach gen-
erally attains 94 to 96% accuracy and replaces the
labor-intensive compilation of linguistics rules by
using an automated learning algorithm. However,
&apos;NTT is an abbreviation of Nippon Telegraph and
Telephone Corporation.
practical systems require more accuracy because
POS tagging is an inevitable pre-processing step for
all practical systems.
To derive a new stochastic tagger, we have two
options since stochastic taggers generally comprise
two components: word model and tag model. The
word model is a set of probabilities that a word oc-
curs with a tag (part-of-speech) when given the pre-
ceding words and their tags in a sentence. On the
contrary, the tag model is a set of probabilities that
a tag appears after the preceding words and their
tags.
The first option is to construct more sophisticated
word models. (Charniak et al., 1993) reports that
their model considers the roots and suffixes of words
to greatly improve tagging accuracy for English cor-
pora. However, the word model approach has the
following shortcomings:
</bodyText>
<listItem confidence="0.7720581">
• For agglutinative languages such as Japanese
and Chinese, the simple Bayes transfer rule is
inapplicable because the word length of a sen-
tence is not fixed in all possible segmentations&apos;.
We can only use simpler word models in these
languages.
• Sophisticated word models largely depend on
the target language. It is time-consuming to
compile fine-grained word models for each lan-
guage.
</listItem>
<bodyText confidence="0.999850583333333">
The second option is to devise a new tag model.
(Schiitze and Singer. 1994) have introduced a
variable-memory-length tag model. Unlike conven-
tional bi-gram and tri-gram models, the method
selects the optimal length by using the context
tree (Rissanen, 1983) which was originally intro-
duced for use in data compression (Cover and
Thomas, 1991). Although the variable-memory
length approach remarkably reduces the number of
parameters, tagging accuracy is only as good as con-
ventional methods. Why didn&apos;t the method have
higher accuracy ? The crucial problem for current
</bodyText>
<equation confidence="0.462482">
2I P(w,lt,) _ , P(w,) cannot be consid-
ered to be identical for all segmentations.
</equation>
<page confidence="0.981569">
230
</page>
<bodyText confidence="0.999199307692307">
tag models is the set of collocational sequences of
words that cannot be captured by just their tags.
Because the maximal likelihood estimator (MLE)
emphasizes the most frequent connections, an ex-
ceptional connection is placed in the same class as a
frequent connection.
To tackle this problem, we introduce a new tag
model based on the mistake-driven mixture of hi-
erarchical tag context trees. Compared to Schiitze
and Singer&apos;s context tree (Schlitze and Singer, 1994),
the hierarchical tag context tree is extended in that
the context is represented by a hierarchical tag set
(i.e. ,NTT &lt; proper noun &lt; noun). This is extremely
useful in capturing exceptional connections that can
be detected only at the word level.
To make the best use of the hierarchical con-
text tree, the mistake-driven mixture method imi-
tates the process in which linguists incorporate ex-
ceptional connections into hand-crafted rules: They
first construct coarse rules which seems to cover
broad range of data. They then try to analyze data
by using the rules and extract exceptions that the
rules cannot handle. Next they generalize the ex-
ceptions and refine the previous rules. The following
two steps abstract the human algorithm for incorpo-
rating exceptional connections.
</bodyText>
<listItem confidence="0.957072333333333">
1. construct temporary rules which seem to well
generalize given data.
2. try to analyze data by using the constructed
rules and extract the exceptions that cannot
be correctly handled, then return to the first
step and focus on the exceptions.
</listItem>
<bodyText confidence="0.9974654">
To put the above idea into our learning algo-
rithm, The mistake-driven mixture method attaches
a weight vector to each example and iteratively per-
forms the following two procedures in the training
phase:
</bodyText>
<listItem confidence="0.94267">
1. constructing a context tree based on the current
data distribution (weight vector)
2. updating the distribution (weight vector) by fo-
cusing on data not well predicted by the con-
</listItem>
<bodyText confidence="0.968486185185185">
structed tree. More precisely, the algorithm re-
duces the weight of examples that are correctly
handled.
For the prediction phase, it then outputs a final
tag model by mixing all the constructed models ac-
cording to their performance. By using the hierar-
chical tag context tree, the constituents of a series
of tag models gradually change from broad coverage
tags (e.g.,noun) to specific exceptional words that
cannot be captured by general tags. In other words,
the method incorporates not only frequent connec-
tions but also infrequent ones that are often consid-
ered to be exceptional.
The construction of the paper is as follows. Sec-
tion 2 describes the stochastic POS tagging scheme
and hierarchical tag setting. Section 3 presents a
new probability estimator that uses a hierarchical
tag context tree and Section 4 explains the mistake-
driven mixture method. Section 5 reports a prelim-
inary evaluation using Japanese newspaper articles.
We tested several tag models by keeping all other
conditions (i.e., dictionary and word model) iden-
tical. The experimental results show that the pro-
posed method significantly outperforms both hand-
crafted and conventional statistical methods. Sec-
tion 6 concerns related works and Sections 7 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.945994" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.98906">
2.1 Basic Equation
</subsectionHeader>
<bodyText confidence="0.999907333333333">
In this section, we will briefly review the basic
equations for part-of-speech tagging and introduce
hierarchical-tag setting.
The tagging problem is formally defined as finding
a sequence of tags ti,„ that maximize the probability
of input string L.
</bodyText>
<equation confidence="0.952017333333333">
argmaxt,,P(wi,n,ti,nIL)= argmaxi,„
P(L)
•=. argrnaxii,„,wiEL P(tl,n • tV1,n
</equation>
<bodyText confidence="0.997911">
We break out P(ii,n, wi,n) as a sequence of the prod-
ucts of tag probability and word probability.
</bodyText>
<equation confidence="0.991992">
P(il,n, Wl,n)
i=1
</equation>
<bodyText confidence="0.99997625">
By approximating word probability as con-
strained only by its tag, we obtain equation (1).
Equation (1) yields various types of stochastic tag-
gers. For example, hi-gram and tri-gram models
approximate their tag probability as P(tilti_j) and
P(ti 14_1, ti_,), respectively. In the rest of the pa-
per, we assume all tagging methods share the word
model P(w114) and differ only in the tag model
</bodyText>
<equation confidence="0.985069">
P(iiiti,,-1, wi,i).
71
argrnaxti .1„ EL
.„, H P(tilti,i_1,w1.i)P(wilti) (1)
. i=1•
</equation>
<subsectionHeader confidence="0.992057">
2.2 Hierarchical Tag Set
</subsectionHeader>
<bodyText confidence="0.9999894">
To construct a tag model that captures excep-
tional connections, we have to consider word-level
context as well as tag-level. In a more general
form, we introduce a tag set that has a hierarchi-
cal structure. Our tag set has a three-level struc-
ture as shown in Figure 1. The topmost and the
second level of the hierarchy are part-of-speech level
and part-of-speech subdivision level respectively. Al-
though stochastic taggers usually make use of subdi-
vision level, part-of-speech level is remarkably robust
</bodyText>
<equation confidence="0.793007">
P(1.01,n,t1,L)
</equation>
<page confidence="0.867892">
231
</page>
<figure confidence="0.9968414">
(adverb)
part-of-speech level
(proper) (numeral) (declarative) (degree) subdivision level
word level
NTT AT&amp;T
</figure>
<figureCaption confidence="0.999961">
Figure 1: Hierarchical Tag Set
</figureCaption>
<bodyText confidence="0.999560625">
against data sparseness. The bottom level is word
level and is indispensable in coping with exceptional
and collocational sequences of words. Our objective
is to construct a tag model that precisely evaluates
P(t1lti,i_i,w1,i) (in equation (1)) by using the three-
level tag set.
To construct this model, we have to answer the
following questions.
</bodyText>
<listItem confidence="0.99916725">
1. Which level is appropriate for ti ?
2. Which length is to be considered for t1,1_1 and
wi,i ?
3. Which level is appropriate for and ?
</listItem>
<bodyText confidence="0.999960214285714">
To resolve the first question, we fix ti at subdivision
level as is done in other tag models. The second and
third questions are resolved by introducing hierar-
chical tag context trees and mistake-driven mixture
method that are respectively described in Section 3
and 4.
Before moving to the next section, let us define
the basic tag set. If all words are considered con-
text candidates, the search space will be enormous.
Thus, it is reasonable for the tagger to constrain the
candidates to frequent open class words and closed
class words. The basic tag set is a set of the most
detailed context elements that comprises the words
selected above and part-of-speech subdivision level.
</bodyText>
<sectionHeader confidence="0.993164" genericHeader="method">
3 Hierarchical Tag Context Tree
</sectionHeader>
<bodyText confidence="0.9999833">
A hierarchical tag context tree is constructed by a
two-step methodology. The first step produces a
context tree by using the basic tag set. The sec-
ond step then produces the hierarchical tag context
tree. It generalizes the basic tag context tree and
avoids over-fitting the data by replacing excessively
specific context in the tree with more general tags.
Finally, the generated tree is transformed into a fi-
nite automaton to improve tagging efficiency (Ron
et al., 1997).
</bodyText>
<subsectionHeader confidence="0.998709">
3.1 Constructing a Basic Tag Context Tree
</subsectionHeader>
<bodyText confidence="0.999850933333333">
In this section, we construct a basic tag context tree.
Before going into detail of the algorithm, we briefly
explain the context tree by using a simple binary
case. The context tree was originally introduced
in the field of data compression (Rissanen, 1983;
Willems et al., 1995; Cover and Thomas, 1991) to
represent how many times and in what context each
symbol appeared in a sequence of symbols. Figure
2 exemplifies two context trees comprising binary
symbols &apos;a&apos; and &apos;6&apos;. T(4) is constructed from the se-
quence &apos;baab&apos; and T(6) from &apos;baobab&apos;. The root node
of T(4) explains that both &apos;a &apos; and &apos;b&apos; appeared twice
in &apos;boob&apos; when no consideration is taken of previous
symbols. The nodes of depth 1 represent an order 1
(bi-gram) model. The left node of T(4) represents
that both &apos;a &apos; and &apos;b&apos; appeared only once after sym-
bol &apos;a&apos;, while the right node of T(4) represents only
&apos;a &apos;occurred once after &apos;b &apos;. In the same way, the node
of depth 2 in T(6) represents an order 2 (tri-gram)
context model.
It is straightforward to extend this binary tree to a
basic tag context tree. In this case, context symbols
a&apos; and &apos;b are replaced by an element of the basic
tag set and the frequency table of each node then
consists of the part-of-speech subdivision set.
The procedure construct-Wee which constructs a
basic tag context tree is given below. Let a set of
subdivision tags to be si.- • Let weight[t] be
a weight vector attached to the tth example x(t).
Initial values of weight[t] are set to 1.
</bodyText>
<listItem confidence="0.935999666666667">
1. the only node, the root, is marked with the
count table (c(si,A),• • c(s„,A) = (0,.
2. Apply the following recursively. Let T(t-1) be
</listItem>
<page confidence="0.876626">
232
</page>
<figure confidence="0.999634375">
a
— (3,3)—
(2,0)
(0,0)
a b
— (2,2)
(1,1) (1,0)
T(4)
</figure>
<figureCaption confidence="0.999844">
Figure 2: Context Trees for &apos;baab • and &apos;baabab&apos;
</figureCaption>
<bodyText confidence="0.9589485">
the last constructed tree with counts of nodes
z, (c(si,z),- • •, c(s„,z)). After the next symbol
whose subdivision is x(t) is observed, generate
the next tree T(t) as follows: follow the T(t-1),
starting at the root and taking the branch in-
dicated by each successive symbol in the past
sequence by using basic tag level. For each
node z visited, increment the component count
c(x(t),z) by weight[t]. Continue until node w
is a leaf node.
3. If w is a leaf, extend the tree by creat-
ing new leaves: c(x(t),wsi)=-
= weight [L}, c(x(t),wsi)=- • .= c(x(t),wsn)=0.
Define the resulting tree to be T(t).
</bodyText>
<subsectionHeader confidence="0.9935635">
3.2 Constructing a Hierarchical Tag
Context Tree
</subsectionHeader>
<bodyText confidence="0.998981777777778">
This section delineates how a hierarchical tag con-
text tree is constructed from a basic tag context tree.
Before describing the algorithm, we prepare some
definitions and notations.
Let A be a part-of-speech subdivision set. As de-
scribed in the previous section, frequency tables of
each node consist of the set A. At any node s of a
context tree, let n(als) and 15(als) be the count of
element a and its probability, respectively.
</bodyText>
<equation confidence="0.979751">
(als) = n(als)
</equation>
<bodyText confidence="0.999895833333333">
probability distribution P(ds) at node s and P(•186)
at node sb. Thus, the larger A(sb) is, the more
meaningful it is to expand a node by sb.
Now, we go back to the hierarchical tag context
tree construction. As illustrated in Figure 3, the gen-
eration process amounts to the iterative selection of b
out of word level, subdivision, part-of-speech and null
(no expansion). Let us look at the procedure from
the information-theoretical viewpoint. Breaking out
equation (2) as (3), .A(sb) is represented as the prod-
uct of the frequencies of all subdivision symbols at
node sb and Kullback-Leibler (KL) divergence.
</bodyText>
<equation confidence="0.9988865">
n(alsb) lo_P(alsb)
A(s6) = n(sb) E
n(sb) P(as)
aCA
(13) aisb)
= n(sb) E P(alsb)log
aCA P(alS)
= TOODKL(P(.ISb), P(Is)) (3)
</equation>
<bodyText confidence="0.99865225">
Because the KL divergence defines a distance
measure between probability distributions, P(•isb)
and P(ds), there is the following trade-off between
the two terms of equation (3).
</bodyText>
<listItem confidence="0.956380666666667">
• The more general b is, the more subdivision
symbols appear at node sb.
• The more specific b is, the more P(I) and
i&apos;(•Isb) differ.
By using the trade-off, the optimal level of b is se-
• lected.
</listItem>
<bodyText confidence="0.993929">
Table 1 summarizes the algorithm construct-hi ree
that constructs the hierarchical tag context tree.
First, construct-htree generates a basic tag context
tree by calling construct-btree. Assume that the
</bodyText>
<equation confidence="0.51063">
EbcA n(hls)
</equation>
<bodyText confidence="0.918081">
We introduce an information-theoretical criteria
./.1(sb) (Weinberger et al., 1995) to evaluate the gain
of expanding a node s by its daughter sb.
</bodyText>
<equation confidence="0.595796666666667">
.A(sb) = E n(alsb)logP,(alsb)
(2)
aCA P(als)
</equation>
<bodyText confidence="0.546196">
.:1(sb) is the difference in optimal code lengths
when symbols at node sb are compressed by using
</bodyText>
<page confidence="0.890322">
233
</page>
<figure confidence="0.870192333333333">
adjective
(root)
proper noun adjective
Which is appropriate for b
word, subdivision, part-of-speech
Sb or null ?
</figure>
<figureCaption confidence="0.996444777777778">
Figure 3: Constructing Hierarchical Tag Context Tree
training examples consist of a sequence of triples,
&lt; p, s, w &gt;, in which pt, st and tut represent
part-of-speech, subdivision and word, respectively.
Eachtime the algorithm reads an example, it first
reaches current leaf node s by following the past se-
quence, computes A(sb), and then selects the opti-
mal b. The initially constructed basic tag context
tree is used to compute A(sb)s.
</figureCaption>
<sectionHeader confidence="0.539186" genericHeader="method">
4 Mistake-Driven Mixture of
Hierarchical Tag Context Trees
</sectionHeader>
<bodyText confidence="0.99861790625">
Up to this section, we introduced a new tag model
that uses a single hierarchical tag context tree to
cope with the exceptional connections that cannot
be captured by just part-of-speech level. However,
this approach has a clear limitation; the exceptional
connections that do not occur so often cannot be
detected by the single tree model. In such a case,
the first term n(sb) in equation (3) is enormous for
general b and the tree is expanded by using more
general symbols.
To overcome this limitation, we devised the
mistake-driven mixture algorithm summarized in Ta-
ble 4 which constructs T context trees and outputs
the final tag model.
mistake-driven mixture sets the weights to 1 for
all examples and repeats the following procedures
T times. The algorithm first construct a hierarchi-
cal context tree by using the current weight vector.
Example data are then tagged by the tree and the
weights of correctly handled examples are reduced
by equation (4). Finally; the final tag model is con-
structed by mixing T trees according to equation
(5)
By using the mistake-driven mixture method, the
constituents of a series of hierarchical tag context
trees gradually change from broad coverage tags
(e.g.,noun) to specific exceptional words that can-
not be captured by part-of-speech and subdivisions.
The method, by mixing different levels of trees, in-
corporates not only frequent connections but also
infrequent ones that are often considered to be col-
locational without over-fitting the data.
</bodyText>
<sectionHeader confidence="0.91624" genericHeader="method">
5 Preliminary Evaluation
</sectionHeader>
<bodyText confidence="0.999750206896552">
We performed an preliminary evaluation using the
first 8939 Japanese sentences in a year&apos;s volume of
newspaper articles(Mainichi, 1993). We first auto-
matically segmented and tagged these sentences and
then revised them by hand. The total number of
words in the hand-revised corpus was 226162. We
trained our tag models on the corpora with every
tenth sentence removed (starting with the first sen-
tence) and then tested the removed sentences. There
were 22937 words in the test corpus.
As the first milestone of performance, we tested
a hand-crafted tag model of JUMAN (Kurohashi et
al., 1994), the most widely used Japanese part-of-
speech tagger. The tagging accuracy of JUNIAN for
the test corpus was only 92.0 %. This shows that our
corpus is difficult to tag because the corpus contains
various genres of texts: from obituaries to poetry.
Next, we compared the mixture of bi-grams and
the mixture of hierarchical tag context trees. In this
experiment, only post-positional particles and aux-
iliaries were word-level elements of basic tags and all
other elements were subdivision level. In contrast,
bi-gram was constructed by using subdivision level.
We set the iteration number T to 5. The results of
our experiments are summarized in Figure 4.
As a single tree estimator (Number of Mixture =
1), the hierarchical tag context tree attained 94.1 %
accuracy, while bi-gram yielded 93.1 %. A hierarchi-
cal tag context tree offers a slight improvement, but
</bodyText>
<page confidence="0.986584">
234
</page>
<bodyText confidence="0.736957">
Initialize weight[j] = 1 for all examples j
</bodyText>
<equation confidence="0.755182666666667">
t = 1
call construct-btree
do
</equation>
<bodyText confidence="0.928312">
Read tth example xt(&lt; Pt,dt,wt &gt;)
in which pt, dt and wt represent part-of-speech, subdivision and word, respectively.
Follow .rt-i ,xt--2, • • .xt-(i_i) and Reach leaf nodes
</bodyText>
<equation confidence="0.877732666666667">
low = swt_i, high = sdt_i
while(max(A(low), A(high))&gt; Threshold) {
if(A(low)&gt; A(high))
</equation>
<bodyText confidence="0.620560333333333">
Expand the tree by the node low
else if(high==spt_z)
Expand the tree by the node high
</bodyText>
<equation confidence="0.6494765">
else low = sdt_i, high = spt_i
II
= t 1
while(xt is not empty)
</equation>
<tableCaption confidence="0.991873">
Table 1: Algorithm construct-htree
</tableCaption>
<bodyText confidence="0.906356272727273">
Input: sequence of N examples &lt; pi, di, wi &gt;, • •, &lt; PN,dN, WN &gt;
in which pi, di and wi represent part-of-speech, subdivision and word, respectively.
Initialize the weight vector weight[i] =1 for i =1,...,N
Do for t = 1,2, ..., T
Call construct-htree providing it with the weight vector weight 0 and
Construct a part-of-speech tagger ht
Let Error be a set of examples that are not identified by ht
Compute the error rate of ht: et = EiCError weight[i]lE.N._1weight[i]
For examples correctly predicted by ht, update the weights vector to be
weight[i] = weight[i}/3t (4)
Output a final tag model
</bodyText>
<equation confidence="0.698155">
=ET-i(log*)htl ET(log*) (5)
</equation>
<tableCaption confidence="0.967684">
Table 2: Algorithm mistake-driven mixture
</tableCaption>
<bodyText confidence="0.998300894736842">
not a gret deal. This conclusion agrees with SchiAze
and Singer&apos;s experiments that used a context tree of
usual part-of-speech.
When we turn to the mixture estimator, a great
difference is seen between hierarchical tag context
trees and bi-grams. The hierarchical tag con-
text trees produced by the mistake-driven mixture
method, greatly improved the accuracy and over-
fitting data was not serious. The best and worst
performances were 96.1 % (Number of Mixture = 3)
and 94.1 % (Number of Mixture = 1), respectively.
On the other hand, the performance of the bi-gram
mixture was not satisfactory. The best and worst
performances were 93.8 % (Number of Mixture = 2)
and 90.8 % (Number of Mixture = 5), respectively.
From the result, we may say exceptional connec-
tions are well captured by hierarchical context trees
but not by bi-grams. Bi-grams of subdivision are too
general to selectively detect exceptions.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999981928571429">
Although statistical natural language processing has
mainly focused on Maximum Likelihood Estimators,
(Pereira et al., 1995) proposed a mixture approach
to predict next words by using the Context Tree
Weighting (CTW) method ‘(Willems et al., 1995).
The CTW method computes probability by mixing
subtrees in a single context tree in Bayesian fashion.
Although the method is very efficient, it cannot be
used to construct hierarchical tag context trees.
Various kinds of re-sampling techniques have been
studied in statistics (Efron, 1979; Efron and Tibshi-
rani, 1993) and machine learning (Breiman, 1996;
Hull et al., 1996; Freund and Schapire, 1996a).
In particular, the mistake-driven mixture algorithm
</bodyText>
<page confidence="0.98882">
235
</page>
<figure confidence="0.998190636363636">
Tagging Accuracy (%)
96
93
90
95
94
92
91
97
2 3 4 5
Number of Mixture
</figure>
<figureCaption confidence="0.99997">
Figure 4: Context Tree Mixture v.s. Bi-gram Mixture
</figureCaption>
<bodyText confidence="0.9999661">
was directly motivated by Adaboost (Freund and
Schapire, 1996a). The Adaboost method was de-
signed to construct a high-performance predictor by
iteratively calling a weak learning algorithm (that
is slightly better than random guess). An em-
pirical work reports that the method greatly im-
proved the performance of decision-tree, k-nearest-
neighbor, and other learning methods given rela-
tively simple and sparse data (Freund and Schapire,
1996b). We borrowed the idea of re-sampling to de-
tect exceptional connections and first proved that
such a re-sampling method is also effective for a
practical application using a large amount of data.
The next step is to fill the gap between theory and
practition. Most theoretical work on re-sampling as-
sumes i.i.d (identically, independently distributed)
samples. This is not a realistic assumption in part-
of-speech tagging and other NL applications. An
interesting future research direction is to construct
a theory that handles Markov processes.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999936875">
We have described a new tag model that uses
mistake-driven mixture to produce hierarchical tag
context trees that can deal with exceptional con-
nections whose detection is not possible at part-of-
speech level. Our experimental results show that
combining hierarchical tag context trees with the
mistake-driven mixture method is extremely effec-
tive for 1. incorporating exceptional connections
and 2. avoiding data over-fitting. Although we have
focused on part-of-speech tagging in this paper, the
mistake-driven mixture method should be useful for
other applications because detecting and incorporat-
ing exceptions is a central problem in corpus-based
NLP. We are now costructing a Japanese depen-
dency parser that employes mistake-driven mixture
of decision trees.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999161076923077">
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123-140, August.
Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proc. Third Conference on Applied
Natural Language Processing, pages 152-155.
Eugene Charniak, Curtis Hendrickson, Neil Jacob-
son, and Mike Perkowits. 1993. Equations for
Part-of-Speech Tagging. In Proc. 11th AAAI,
pages 784-789.
K. W. Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In
Proc. ACL 2nd Conference 071 Applied Natural
Language Processing, pages 126-143.
</reference>
<page confidence="0.977115">
236
</page>
<reference confidence="0.994415961538461">
T.M. Cover and J.A. Thomas, 1991. Elements of
Information Theory. John Wiley &amp; Sons.
B. Efron and R. Tibshirani, 1993. An Introduction
to the Bootstrap. Chapman and Hall.
B. Efron. 1979. Bootstrap: another look at the
jackknife. The .4nnals of Statistics, 7(1):1-26.
Yoav Freund and Robert Schapire. 1996a. A
decision-theoretic generalization of on-line learn-
ing and an application to boosting.
Yoav Freund and Robert Schapire. 1996b. Experi-
ments with a New Boosting algorithm. In Proc.
13rd International Conference on Machine Learn-
ing, pages 148-156.
David A. Hull, Jan 0. Pedersen, and Hinrich
Schiitze. 1996. Method combination for docu-
ment filtering. In Proc. ACM SIGIR 96, pages
279-287.
J. Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech
and Language, 6:225-242.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer juman. In
Proc. International Workshop on Sharable Nat-
ural Language Resources, pages 22-28.
Mainichi, 1993. CD Mainichi Shinbun. Nichigai As-
sociates Co.
Masaaki Nagata. 1994. A Stochastic Japanese Mor-
phological Analyzer Using Forward-DP
Backward-As N-Best Search Algorithm. In Proc.
15th COLING, pages 201-207.
Fernando C. Pereira, Yoram Singer, and Naftali
Tishby. 1995. Beyond Word N-Grams. In Proc.
Third Workshop on Very Large Corpora, pages
95-106.
Jorma Rissanen. 1983. A universal data compres-
sion system. IEEE Transaction on Information
Theory, 29(5):656-664, September.
Dana Ron, Yoram Singer, and Naftali Tishby. 1997.
The power of amnesia: Learning probabilistic au-
tomata with variable memory length. (to appear)
Machine Learning Special Issue on COLT94.
H. Schiitze and Y. Singer. 1994. Part-of-speech tag-
ging using a variable markov model. In the 32th
Annual Meeting of ACL, pages 181-187.
M J. Weinberger, J J. Rissanen, and M. Feder. 1995.
A universal finite memory source. IEEE Transac-
tion on Information Theory, 41(3):643-652, May.
F M J. Willems, Y M. Shtarkov, and T J. Tjalkens.
1995. The context-tree weigting method: Ba-
sic properties. IEEE Transaction on Information
Theory, 41(3):653-664, May.
</reference>
<page confidence="0.997321">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.334874">
<title confidence="0.999799">Mistake-Driven Mixture of Hierarchical Tag Context Trees</title>
<author confidence="0.995544">Masahiko Haruno</author>
<affiliation confidence="0.999687">NTT Communication Science Laboratories</affiliation>
<address confidence="0.9700085">1-1 Hikari-No-Oka Yokosuka-Shi Kanagawa 239, Japan</address>
<email confidence="0.968104">harunoOcslab.kecl.ntt.co.jp</email>
<author confidence="0.838709">Yuji Matsumoto</author>
<affiliation confidence="0.610732">NAIST</affiliation>
<address confidence="0.4836465">8916-3 Takayama-cho Ikoma-Shi Nara 630-01, Japan</address>
<email confidence="0.808585">matsuOis.aist-nara.ac.jp</email>
<abstract confidence="0.999226225806452">paper proposes a mixfor learning a tag model. The method iteratively performs two procea tag model based the current data distribution and updating the distribution by focusing on data that are not well predicted by the constructed model. The final tag model is constructed by mixing all the models according to their performance. To well reflect the data distribution, we repreeach tag model as a tag &lt; proper noun &lt; noun) contree. using the hierarchical tag context tree, the constituents of sequential tag models gradually change from broad coverage tags (e.g.,noun) to specific exceptional words that cannot be captured by gener&apos;al tags. In other words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>24--2</pages>
<contexts>
<context position="20992" citStr="Breiman, 1996" startWordPosition="3413" endWordPosition="3414">elated Work Although statistical natural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy (%) 96 93 90 95 94 92 91 97 2 3 4 5 Number of Mixture Figure 4: Context Tree Mixture v.s. Bi-gram Mixture was directly motivated by Adaboost (Freund and Schapire, 1996a). The Adaboost method was designed to construct a high-performance predictor by iteratively calling a weak learning algorithm (that is slightly better than random guess). An empirical work reports that the method greatly improved the performance of decision-tree, k-nearestneighbor, and other learning method</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123-140, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proc. Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="1650" citStr="Brill, 1992" startWordPosition="240" endWordPosition="241">ther words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods. 1 Introduction The last few years have seen the great success of stochastic part-of-speech (POS) taggers (Church, 1988; Kupiec, 1992; Charniak et al., 1993; Brill, 1992; Nagata, 1994). The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm. However, &apos;NTT is an abbreviation of Nippon Telegraph and Telephone Corporation. practical systems require more accuracy because POS tagging is an inevitable pre-processing step for all practical systems. To derive a new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model. The word model is a set of probabilities that a word occurs with a ta</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proc. Third Conference on Applied Natural Language Processing, pages 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Curtis Hendrickson</author>
<author>Neil Jacobson</author>
<author>Mike Perkowits</author>
</authors>
<title>Equations for Part-of-Speech Tagging.</title>
<date>1993</date>
<booktitle>In Proc. 11th AAAI,</booktitle>
<pages>784--789</pages>
<contexts>
<context position="1637" citStr="Charniak et al., 1993" startWordPosition="236" endWordPosition="239"> by gener&apos;al tags. In other words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods. 1 Introduction The last few years have seen the great success of stochastic part-of-speech (POS) taggers (Church, 1988; Kupiec, 1992; Charniak et al., 1993; Brill, 1992; Nagata, 1994). The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm. However, &apos;NTT is an abbreviation of Nippon Telegraph and Telephone Corporation. practical systems require more accuracy because POS tagging is an inevitable pre-processing step for all practical systems. To derive a new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model. The word model is a set of probabilities that a word occ</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowits, 1993</marker>
<rawString>Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowits. 1993. Equations for Part-of-Speech Tagging. In Proc. 11th AAAI, pages 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proc. ACL 2nd Conference 071 Applied Natural Language Processing,</booktitle>
<pages>126--143</pages>
<contexts>
<context position="1600" citStr="Church, 1988" startWordPosition="232" endWordPosition="233">ords that cannot be captured by gener&apos;al tags. In other words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods. 1 Introduction The last few years have seen the great success of stochastic part-of-speech (POS) taggers (Church, 1988; Kupiec, 1992; Charniak et al., 1993; Brill, 1992; Nagata, 1994). The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm. However, &apos;NTT is an abbreviation of Nippon Telegraph and Telephone Corporation. practical systems require more accuracy because POS tagging is an inevitable pre-processing step for all practical systems. To derive a new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model. The word model is a</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. ACL 2nd Conference 071 Applied Natural Language Processing, pages 126-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="3457" citStr="Cover and Thomas, 1991" startWordPosition="525" endWordPosition="528">ecause the word length of a sentence is not fixed in all possible segmentations&apos;. We can only use simpler word models in these languages. • Sophisticated word models largely depend on the target language. It is time-consuming to compile fine-grained word models for each language. The second option is to devise a new tag model. (Schiitze and Singer. 1994) have introduced a variable-memory-length tag model. Unlike conventional bi-gram and tri-gram models, the method selects the optimal length by using the context tree (Rissanen, 1983) which was originally introduced for use in data compression (Cover and Thomas, 1991). Although the variable-memory length approach remarkably reduces the number of parameters, tagging accuracy is only as good as conventional methods. Why didn&apos;t the method have higher accuracy ? The crucial problem for current 2I P(w,lt,) _ , P(w,) cannot be considered to be identical for all segmentations. 230 tag models is the set of collocational sequences of words that cannot be captured by just their tags. Because the maximal likelihood estimator (MLE) emphasizes the most frequent connections, an exceptional connection is placed in the same class as a frequent connection. To tackle this p</context>
<context position="10557" citStr="Cover and Thomas, 1991" startWordPosition="1671" endWordPosition="1674"> tree. It generalizes the basic tag context tree and avoids over-fitting the data by replacing excessively specific context in the tree with more general tags. Finally, the generated tree is transformed into a finite automaton to improve tagging efficiency (Ron et al., 1997). 3.1 Constructing a Basic Tag Context Tree In this section, we construct a basic tag context tree. Before going into detail of the algorithm, we briefly explain the context tree by using a simple binary case. The context tree was originally introduced in the field of data compression (Rissanen, 1983; Willems et al., 1995; Cover and Thomas, 1991) to represent how many times and in what context each symbol appeared in a sequence of symbols. Figure 2 exemplifies two context trees comprising binary symbols &apos;a&apos; and &apos;6&apos;. T(4) is constructed from the sequence &apos;baab&apos; and T(6) from &apos;baobab&apos;. The root node of T(4) explains that both &apos;a &apos; and &apos;b&apos; appeared twice in &apos;boob&apos; when no consideration is taken of previous symbols. The nodes of depth 1 represent an order 1 (bi-gram) model. The left node of T(4) represents that both &apos;a &apos; and &apos;b&apos; appeared only once after symbol &apos;a&apos;, while the right node of T(4) represents only &apos;a &apos;occurred once after &apos;b &apos;.</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T.M. Cover and J.A. Thomas, 1991. Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman and Hall.</publisher>
<contexts>
<context position="20956" citStr="Efron and Tibshirani, 1993" startWordPosition="3405" endWordPosition="3409"> too general to selectively detect exceptions. 6 Related Work Although statistical natural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy (%) 96 93 90 95 94 92 91 97 2 3 4 5 Number of Mixture Figure 4: Context Tree Mixture v.s. Bi-gram Mixture was directly motivated by Adaboost (Freund and Schapire, 1996a). The Adaboost method was designed to construct a high-performance predictor by iteratively calling a weak learning algorithm (that is slightly better than random guess). An empirical work reports that the method greatly improved the performance of decision-tree, k-neares</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. Tibshirani, 1993. An Introduction to the Bootstrap. Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
</authors>
<title>Bootstrap: another look at the jackknife. The .4nnals of Statistics,</title>
<date>1979</date>
<pages>7--1</pages>
<contexts>
<context position="20927" citStr="Efron, 1979" startWordPosition="3403" endWordPosition="3404">bdivision are too general to selectively detect exceptions. 6 Related Work Although statistical natural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy (%) 96 93 90 95 94 92 91 97 2 3 4 5 Number of Mixture Figure 4: Context Tree Mixture v.s. Bi-gram Mixture was directly motivated by Adaboost (Freund and Schapire, 1996a). The Adaboost method was designed to construct a high-performance predictor by iteratively calling a weak learning algorithm (that is slightly better than random guess). An empirical work reports that the method greatly improved the performan</context>
</contexts>
<marker>Efron, 1979</marker>
<rawString>B. Efron. 1979. Bootstrap: another look at the jackknife. The .4nnals of Statistics, 7(1):1-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1996</date>
<contexts>
<context position="21038" citStr="Freund and Schapire, 1996" startWordPosition="3419" endWordPosition="3422">atural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy (%) 96 93 90 95 94 92 91 97 2 3 4 5 Number of Mixture Figure 4: Context Tree Mixture v.s. Bi-gram Mixture was directly motivated by Adaboost (Freund and Schapire, 1996a). The Adaboost method was designed to construct a high-performance predictor by iteratively calling a weak learning algorithm (that is slightly better than random guess). An empirical work reports that the method greatly improved the performance of decision-tree, k-nearestneighbor, and other learning methods given relatively simple and sparse data (Fre</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert Schapire. 1996a. A decision-theoretic generalization of on-line learning and an application to boosting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Experiments with a New Boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proc. 13rd International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="21038" citStr="Freund and Schapire, 1996" startWordPosition="3419" endWordPosition="3422">atural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy (%) 96 93 90 95 94 92 91 97 2 3 4 5 Number of Mixture Figure 4: Context Tree Mixture v.s. Bi-gram Mixture was directly motivated by Adaboost (Freund and Schapire, 1996a). The Adaboost method was designed to construct a high-performance predictor by iteratively calling a weak learning algorithm (that is slightly better than random guess). An empirical work reports that the method greatly improved the performance of decision-tree, k-nearestneighbor, and other learning methods given relatively simple and sparse data (Fre</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert Schapire. 1996b. Experiments with a New Boosting algorithm. In Proc. 13rd International Conference on Machine Learning, pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
<author>Jan</author>
</authors>
<title>Method combination for document filtering.</title>
<date>1996</date>
<booktitle>In Proc. ACM SIGIR 96,</booktitle>
<pages>279--287</pages>
<marker>Hull, Jan, 1996</marker>
<rawString>David A. Hull, Jan 0. Pedersen, and Hinrich Schiitze. 1996. Method combination for document filtering. In Proc. ACM SIGIR 96, pages 279-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="1614" citStr="Kupiec, 1992" startWordPosition="234" endWordPosition="235">ot be captured by gener&apos;al tags. In other words, the method incorporates not only frequent connections but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods. 1 Introduction The last few years have seen the great success of stochastic part-of-speech (POS) taggers (Church, 1988; Kupiec, 1992; Charniak et al., 1993; Brill, 1992; Nagata, 1994). The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm. However, &apos;NTT is an abbreviation of Nippon Telegraph and Telephone Corporation. practical systems require more accuracy because POS tagging is an inevitable pre-processing step for all practical systems. To derive a new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model. The word model is a set of probab</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Toshihisa Nakamura</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer juman.</title>
<date>1994</date>
<booktitle>In Proc. International Workshop on Sharable Natural Language Resources,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="17418" citStr="Kurohashi et al., 1994" startWordPosition="2826" endWordPosition="2829">data. 5 Preliminary Evaluation We performed an preliminary evaluation using the first 8939 Japanese sentences in a year&apos;s volume of newspaper articles(Mainichi, 1993). We first automatically segmented and tagged these sentences and then revised them by hand. The total number of words in the hand-revised corpus was 226162. We trained our tag models on the corpora with every tenth sentence removed (starting with the first sentence) and then tested the removed sentences. There were 22937 words in the test corpus. As the first milestone of performance, we tested a hand-crafted tag model of JUMAN (Kurohashi et al., 1994), the most widely used Japanese part-ofspeech tagger. The tagging accuracy of JUNIAN for the test corpus was only 92.0 %. This shows that our corpus is difficult to tag because the corpus contains various genres of texts: from obituaries to poetry. Next, we compared the mixture of bi-grams and the mixture of hierarchical tag context trees. In this experiment, only post-positional particles and auxiliaries were word-level elements of basic tags and all other elements were subdivision level. In contrast, bi-gram was constructed by using subdivision level. We set the iteration number T to 5. The </context>
</contexts>
<marker>Kurohashi, Nakamura, Matsumoto, Nagao, 1994</marker>
<rawString>Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer juman. In Proc. International Workshop on Sharable Natural Language Resources, pages 22-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mainichi</author>
</authors>
<date>1993</date>
<journal>CD Mainichi Shinbun. Nichigai Associates Co.</journal>
<contexts>
<context position="16961" citStr="Mainichi, 1993" startWordPosition="2751" endWordPosition="2752">ation (5) By using the mistake-driven mixture method, the constituents of a series of hierarchical tag context trees gradually change from broad coverage tags (e.g.,noun) to specific exceptional words that cannot be captured by part-of-speech and subdivisions. The method, by mixing different levels of trees, incorporates not only frequent connections but also infrequent ones that are often considered to be collocational without over-fitting the data. 5 Preliminary Evaluation We performed an preliminary evaluation using the first 8939 Japanese sentences in a year&apos;s volume of newspaper articles(Mainichi, 1993). We first automatically segmented and tagged these sentences and then revised them by hand. The total number of words in the hand-revised corpus was 226162. We trained our tag models on the corpora with every tenth sentence removed (starting with the first sentence) and then tested the removed sentences. There were 22937 words in the test corpus. As the first milestone of performance, we tested a hand-crafted tag model of JUMAN (Kurohashi et al., 1994), the most widely used Japanese part-ofspeech tagger. The tagging accuracy of JUNIAN for the test corpus was only 92.0 %. This shows that our c</context>
</contexts>
<marker>Mainichi, 1993</marker>
<rawString>Mainichi, 1993. CD Mainichi Shinbun. Nichigai Associates Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A Stochastic Japanese Morphological Analyzer Using Forward-DP Backward-As N-Best Search Algorithm.</title>
<date>1994</date>
<booktitle>In Proc. 15th COLING,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="1665" citStr="Nagata, 1994" startWordPosition="242" endWordPosition="243">he method incorporates not only frequent connections but also infrequent ones that are often considered to be collocational. We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models. The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods. 1 Introduction The last few years have seen the great success of stochastic part-of-speech (POS) taggers (Church, 1988; Kupiec, 1992; Charniak et al., 1993; Brill, 1992; Nagata, 1994). The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm. However, &apos;NTT is an abbreviation of Nippon Telegraph and Telephone Corporation. practical systems require more accuracy because POS tagging is an inevitable pre-processing step for all practical systems. To derive a new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model. The word model is a set of probabilities that a word occurs with a tag (part-of-spee</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A Stochastic Japanese Morphological Analyzer Using Forward-DP Backward-As N-Best Search Algorithm. In Proc. 15th COLING, pages 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C Pereira</author>
<author>Yoram Singer</author>
<author>Naftali Tishby</author>
</authors>
<title>Beyond Word N-Grams.</title>
<date>1995</date>
<booktitle>In Proc. Third Workshop on Very Large Corpora,</booktitle>
<pages>95--106</pages>
<contexts>
<context position="20515" citStr="Pereira et al., 1995" startWordPosition="3337" endWordPosition="3340">es were 96.1 % (Number of Mixture = 3) and 94.1 % (Number of Mixture = 1), respectively. On the other hand, the performance of the bi-gram mixture was not satisfactory. The best and worst performances were 93.8 % (Number of Mixture = 2) and 90.8 % (Number of Mixture = 5), respectively. From the result, we may say exceptional connections are well captured by hierarchical context trees but not by bi-grams. Bi-grams of subdivision are too general to selectively detect exceptions. 6 Related Work Although statistical natural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy </context>
</contexts>
<marker>Pereira, Singer, Tishby, 1995</marker>
<rawString>Fernando C. Pereira, Yoram Singer, and Naftali Tishby. 1995. Beyond Word N-Grams. In Proc. Third Workshop on Very Large Corpora, pages 95-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>A universal data compression system.</title>
<date>1983</date>
<journal>IEEE Transaction on Information Theory,</journal>
<pages>29--5</pages>
<contexts>
<context position="3372" citStr="Rissanen, 1983" startWordPosition="513" endWordPosition="514">uch as Japanese and Chinese, the simple Bayes transfer rule is inapplicable because the word length of a sentence is not fixed in all possible segmentations&apos;. We can only use simpler word models in these languages. • Sophisticated word models largely depend on the target language. It is time-consuming to compile fine-grained word models for each language. The second option is to devise a new tag model. (Schiitze and Singer. 1994) have introduced a variable-memory-length tag model. Unlike conventional bi-gram and tri-gram models, the method selects the optimal length by using the context tree (Rissanen, 1983) which was originally introduced for use in data compression (Cover and Thomas, 1991). Although the variable-memory length approach remarkably reduces the number of parameters, tagging accuracy is only as good as conventional methods. Why didn&apos;t the method have higher accuracy ? The crucial problem for current 2I P(w,lt,) _ , P(w,) cannot be considered to be identical for all segmentations. 230 tag models is the set of collocational sequences of words that cannot be captured by just their tags. Because the maximal likelihood estimator (MLE) emphasizes the most frequent connections, an exceptio</context>
<context position="10510" citStr="Rissanen, 1983" startWordPosition="1665" endWordPosition="1666"> produces the hierarchical tag context tree. It generalizes the basic tag context tree and avoids over-fitting the data by replacing excessively specific context in the tree with more general tags. Finally, the generated tree is transformed into a finite automaton to improve tagging efficiency (Ron et al., 1997). 3.1 Constructing a Basic Tag Context Tree In this section, we construct a basic tag context tree. Before going into detail of the algorithm, we briefly explain the context tree by using a simple binary case. The context tree was originally introduced in the field of data compression (Rissanen, 1983; Willems et al., 1995; Cover and Thomas, 1991) to represent how many times and in what context each symbol appeared in a sequence of symbols. Figure 2 exemplifies two context trees comprising binary symbols &apos;a&apos; and &apos;6&apos;. T(4) is constructed from the sequence &apos;baab&apos; and T(6) from &apos;baobab&apos;. The root node of T(4) explains that both &apos;a &apos; and &apos;b&apos; appeared twice in &apos;boob&apos; when no consideration is taken of previous symbols. The nodes of depth 1 represent an order 1 (bi-gram) model. The left node of T(4) represents that both &apos;a &apos; and &apos;b&apos; appeared only once after symbol &apos;a&apos;, while the right node of T(4</context>
</contexts>
<marker>Rissanen, 1983</marker>
<rawString>Jorma Rissanen. 1983. A universal data compression system. IEEE Transaction on Information Theory, 29(5):656-664, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Ron</author>
<author>Yoram Singer</author>
<author>Naftali Tishby</author>
</authors>
<title>The power of amnesia: Learning probabilistic automata with variable memory length. (to appear) Machine Learning Special Issue on COLT94.</title>
<date>1997</date>
<contexts>
<context position="10209" citStr="Ron et al., 1997" startWordPosition="1613" endWordPosition="1616">e most detailed context elements that comprises the words selected above and part-of-speech subdivision level. 3 Hierarchical Tag Context Tree A hierarchical tag context tree is constructed by a two-step methodology. The first step produces a context tree by using the basic tag set. The second step then produces the hierarchical tag context tree. It generalizes the basic tag context tree and avoids over-fitting the data by replacing excessively specific context in the tree with more general tags. Finally, the generated tree is transformed into a finite automaton to improve tagging efficiency (Ron et al., 1997). 3.1 Constructing a Basic Tag Context Tree In this section, we construct a basic tag context tree. Before going into detail of the algorithm, we briefly explain the context tree by using a simple binary case. The context tree was originally introduced in the field of data compression (Rissanen, 1983; Willems et al., 1995; Cover and Thomas, 1991) to represent how many times and in what context each symbol appeared in a sequence of symbols. Figure 2 exemplifies two context trees comprising binary symbols &apos;a&apos; and &apos;6&apos;. T(4) is constructed from the sequence &apos;baab&apos; and T(6) from &apos;baobab&apos;. The root </context>
</contexts>
<marker>Ron, Singer, Tishby, 1997</marker>
<rawString>Dana Ron, Yoram Singer, and Naftali Tishby. 1997. The power of amnesia: Learning probabilistic automata with variable memory length. (to appear) Machine Learning Special Issue on COLT94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
<author>Y Singer</author>
</authors>
<title>Part-of-speech tagging using a variable markov model.</title>
<date>1994</date>
<booktitle>In the 32th Annual Meeting of ACL,</booktitle>
<pages>181--187</pages>
<marker>Schiitze, Singer, 1994</marker>
<rawString>H. Schiitze and Y. Singer. 1994. Part-of-speech tagging using a variable markov model. In the 32th Annual Meeting of ACL, pages 181-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Weinberger</author>
<author>J J Rissanen</author>
<author>M Feder</author>
</authors>
<title>A universal finite memory source.</title>
<date>1995</date>
<journal>IEEE Transaction on Information Theory,</journal>
<pages>41--3</pages>
<contexts>
<context position="14488" citStr="Weinberger et al., 1995" startWordPosition="2350" endWordPosition="2353">ure between probability distributions, P(•isb) and P(ds), there is the following trade-off between the two terms of equation (3). • The more general b is, the more subdivision symbols appear at node sb. • The more specific b is, the more P(I) and i&apos;(•Isb) differ. By using the trade-off, the optimal level of b is se• lected. Table 1 summarizes the algorithm construct-hi ree that constructs the hierarchical tag context tree. First, construct-htree generates a basic tag context tree by calling construct-btree. Assume that the EbcA n(hls) We introduce an information-theoretical criteria ./.1(sb) (Weinberger et al., 1995) to evaluate the gain of expanding a node s by its daughter sb. .A(sb) = E n(alsb)logP,(alsb) (2) aCA P(als) .:1(sb) is the difference in optimal code lengths when symbols at node sb are compressed by using 233 adjective (root) proper noun adjective Which is appropriate for b word, subdivision, part-of-speech Sb or null ? Figure 3: Constructing Hierarchical Tag Context Tree training examples consist of a sequence of triples, &lt; p, s, w &gt;, in which pt, st and tut represent part-of-speech, subdivision and word, respectively. Eachtime the algorithm reads an example, it first reaches current leaf n</context>
</contexts>
<marker>Weinberger, Rissanen, Feder, 1995</marker>
<rawString>M J. Weinberger, J J. Rissanen, and M. Feder. 1995. A universal finite memory source. IEEE Transaction on Information Theory, 41(3):643-652, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M J Willems</author>
<author>Y M Shtarkov</author>
<author>T J Tjalkens</author>
</authors>
<title>The context-tree weigting method: Basic properties.</title>
<date>1995</date>
<journal>IEEE Transaction on Information Theory,</journal>
<pages>41--3</pages>
<contexts>
<context position="10532" citStr="Willems et al., 1995" startWordPosition="1667" endWordPosition="1670">erarchical tag context tree. It generalizes the basic tag context tree and avoids over-fitting the data by replacing excessively specific context in the tree with more general tags. Finally, the generated tree is transformed into a finite automaton to improve tagging efficiency (Ron et al., 1997). 3.1 Constructing a Basic Tag Context Tree In this section, we construct a basic tag context tree. Before going into detail of the algorithm, we briefly explain the context tree by using a simple binary case. The context tree was originally introduced in the field of data compression (Rissanen, 1983; Willems et al., 1995; Cover and Thomas, 1991) to represent how many times and in what context each symbol appeared in a sequence of symbols. Figure 2 exemplifies two context trees comprising binary symbols &apos;a&apos; and &apos;6&apos;. T(4) is constructed from the sequence &apos;baab&apos; and T(6) from &apos;baobab&apos;. The root node of T(4) explains that both &apos;a &apos; and &apos;b&apos; appeared twice in &apos;boob&apos; when no consideration is taken of previous symbols. The nodes of depth 1 represent an order 1 (bi-gram) model. The left node of T(4) represents that both &apos;a &apos; and &apos;b&apos; appeared only once after symbol &apos;a&apos;, while the right node of T(4) represents only &apos;a &apos;</context>
<context position="20638" citStr="Willems et al., 1995" startWordPosition="3357" endWordPosition="3360"> of the bi-gram mixture was not satisfactory. The best and worst performances were 93.8 % (Number of Mixture = 2) and 90.8 % (Number of Mixture = 5), respectively. From the result, we may say exceptional connections are well captured by hierarchical context trees but not by bi-grams. Bi-grams of subdivision are too general to selectively detect exceptions. 6 Related Work Although statistical natural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method ‘(Willems et al., 1995). The CTW method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 Tagging Accuracy (%) 96 93 90 95 94 92 91 97 2 3 4 5 Number of Mixture Figure 4: Context Tree Mixture v.s. Bi-gram Mixture was directly moti</context>
</contexts>
<marker>Willems, Shtarkov, Tjalkens, 1995</marker>
<rawString>F M J. Willems, Y M. Shtarkov, and T J. Tjalkens. 1995. The context-tree weigting method: Basic properties. IEEE Transaction on Information Theory, 41(3):653-664, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>