<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<sectionHeader confidence="0.905551" genericHeader="abstract">
STRUCTURAL DISAMBIGUATION WITH
CONSTRAINT PROPAGATION
</sectionHeader>
<author confidence="0.495281">
Hiroshi Maruyarna
</author>
<affiliation confidence="0.51989">
IBM Research, Tokyo Research Laboratory
</affiliation>
<address confidence="0.890894">
5-19 Sanbancho, Chiyoda-ku,
Tokyo 102 Japan
</address>
<email confidence="0.875374">
maruyamaqjpntscvni.bitnet
</email>
<sectionHeader confidence="0.990587" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9999466">
We present a new grammatical formalism called Con-
straint Dependency Grammar (CDG) in which every
grammatical rule is given as a constraint on word-
to-word modifications. CDG parsing is formalized
as a constraint satisfaction problem over a finite do-
main so that efficient constraint-propagation algo-
rithms can be employed to reduce structural am-
biguity without generating individual parse trees.
The weak generative capacity and the computational
complexity of CDG parsing are also discussed_
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999405939393939">
We are interested in an efficient treatment of struc-
tural ambiguity in natural language analysis. It is
known that &amp;quot;every-way&amp;quot; ambiguous constructs, such
as prepositional attachment in English, have a Cata-
lan number of ambiguous parses (Church and Patil
1982), which grows at a faster than exponential rate
(Knuth 1975). A parser should be provided with
a disambiguation mechanism that does not involve
generating such a combinatorial number of parse
trees explicitly.
We have developed a. parsing method in which an
intermediate parsing result is represented as a data
structure called a constraint network. Every solution
that satisfies all the constraints simultaneously corre-
sponds to an individual parse tree. No explicit parse
trees are generated until ultimately necessary. Pars-
ing and successive disambiguation are performed by
adding new constraints to the constraint network.
Newly added constraints are efficiently propagated
over the network by Constraint Propagation (Waltz
1975, Montanan i 1976) to remove inconsistent values.
In this paper, we present the basic ideas of a
formal grammatical theory called Constraint Depen-
dency Grammar (CDG for short) that makes this
parsing technique possible_ CDG has a reasonable
time bound in its parsing, while its weak generative
capacity is strictly greater than that of Context Free
Grammar (CFG).
We give the definition of CDG in the next section.
Then, in Section 3, we describe the parsing method
based on constraint propagation, using a step-by-
step example. Formal properties of CDG are dis-
cussed in Section 4.
</bodyText>
<sectionHeader confidence="0.996734" genericHeader="method">
2 CDG: DEFINITION
</sectionHeader>
<bodyText confidence="0.998592777777778">
Let a sentence s w1w2 wn be a finite string on
a finite alphabet E. Let R =-- {r1,r2, ,Tk} be a
finite set of rale-ids. Suppose that each word i in a
sentences has k-different roles ri(i), T2 (i) • • • r (i) •
Roles are like variables, and each role can have a pair
&lt;a, d&gt; as its value, where the label a is a member of
a finite set L = fat, a2, • • • , ail and the modifies d
is either 1 &lt; d &lt; n or a special symbol nil. An
analysis of the sentence s is obtained by assigning
appropriate values to the n x k roles (we can regard
this situation as one in which each word has a. frame
with k Slots, as shown in Figure 1).
An assignment A of a sentence s is a function that
assigns values to the roles. Given an assignment A,
the label and the modifiee of a role x are determined.
We define the following four functions to represent
the various aspect of the role x, assuming that x is
an rl-role of the word i:
</bodyText>
<page confidence="0.999844">
31
</page>
<figure confidence="0.966108">
r, -role
r2 -role
rk -role
W w2 wn
</figure>
<listItem confidence="0.998936">
• Predicate symbols: =, &lt;,&gt;, and E
• Logical connectors: Sz, m and
</listItem>
<bodyText confidence="0.99892775">
Specifically, we call a, subformula P, a &apos;unary con-
straint when P, contains only one variable, and a
binary constraint when P, contains exactly two vari-
ables.
</bodyText>
<figureCaption confidence="0.998414">
Figure 1: Words and their roles.
</figureCaption>
<bodyText confidence="0.801805">
,def ,
</bodyText>
<listItem confidence="0.9761756">
• posx )= the position i
• rid(x)d-4 the role id ri
• lab(x)cl=f the label of x
del
• mod(x) = the modifiee of x
</listItem>
<bodyText confidence="0.9900848">
We also define word(i) as the terminal symbol
occurring at the position i.1
An individual grammar G =&lt; E, R, L, C &gt; in the
CDG theory determines a set of possible assignments
of a given sentence, where
</bodyText>
<listItem confidence="0.9964218">
• E is a finite set of terminal symbols.
• R is a finite set of role-ids.
• L is a finite set of labels.
• C is a constraint that an assignment A should
satisfy.
</listItem>
<equation confidence="0.8635325">
A constraint C is a logical formula in a form
Vx1x2...xp : role; PiSEP28z...&amp;P,
</equation>
<bodyText confidence="0.999922">
where the variables xi, x2, ..., xp range over the set
of roles in an assignment A and each subformula P
consists only of the following vocabulary:
</bodyText>
<listItem confidence="0.9948784">
• Variables: x1, x2, xp
• Constants: elements and subsets of
EuLURU {nil, 3., 2, ...}
• Function symbols: word(),pos(), rid(), lab(),
and mod()
</listItem>
<footnote confidence="0.9526298">
11n this paper, when referring to a word, we purposely use
the position (1,2,...,n) of the word rather than the word itself
(wt.w2$ • - ,vh,), because the same word can occur in many
different positions in a sentence. For readability, however, we
sometimes use the notation wor4.sit.3n-
</footnote>
<bodyText confidence="0.9963898">
The semantics of the functions have been defined
above. The semantics of the predicates and the logi-
cal connectors are defined as usual, except that com-
paring an expression containing nil with another
value by the inequality predicates always yields the
truth value false.
These conditions guarantee that, given an assign-
ment A, it is possible to compute whether the values
of xl, x2, ,x satisfy C in a constant time, regard-
less of the sentence length n.
</bodyText>
<sectionHeader confidence="0.47905" genericHeader="method">
Definition
</sectionHeader>
<listItem confidence="0.943899833333333">
• The degree of a grammar G is the size k of the
rote-id set R.
• The arity of a grammar G is the number of vari-
ables p in the constraint C.
Unless otherwise stated, we deal with only ar-
ity=2 cases.
• A nonnull string s over the alphabet E is gener-
ated if there exits an assignment A that satisfies
the constraint C.
• L(G) is a language generated by the grammar G
if L(G) is the set of all sentences generated by
a grammar G.
Example
Let us consider GI =&lt; El, RI, Ll Cl where
• El =
• R1 = {governor}
• Ll = {DET,SUBJ,ROOT}
• Cl = Vsy : role; Pl.
</listItem>
<bodyText confidence="0.997368333333333">
The formula Fl of the constraint Cl is the con-
junction of the following four subformulas (an infor-
mal description is attached to each constraint):
</bodyText>
<equation confidence="0.93742896">
(Gl-l) word(pos(x))=D ( Iab(z)=DET,
worcl(mod(x))=N, pos(x) &lt; mod(x) )
&amp;quot;A determiner (D) modifies a noun (N) on the
32 right with the label DET.&amp;quot;
Value
&lt;DET 2&gt;
&lt;SUBJ , 3&gt;
&lt;ROOT,nil&gt;
Role
govern.or(&amp;quot;ai&amp;quot;)
governor( dog2&amp;quot;)
governor(&amp;quot;runs3&amp;quot;)
Figure 2: Assignment Satisfying (G1-1) to (G1-4)
SUBJ
doC7g2)
(IDE;
(G1-2) word(pos(x))=N iab(x)=SUBJ,
word(mod(x))=V , pos(x) mod(x) )
&amp;quot;A noun modifies a verb (V) on the right with
the label SUBJ.&amp;quot;
(G1-3) inord(pos(x))=V iab(x)=ROOT,
mod(x)=nil)
&amp;quot;A verb modifies nothing and its label should
be ROOT.&amp;quot;
(G1-4) ( mod(x)=mod(y), lab(x)=lab(y) ) x=y
</equation>
<bodyText confidence="0.999384090909091">
&amp;quot;No two words can modify the same word with
the same label.&amp;quot;
Analyzing a sentence with G1 means assigning
a label-modifiee pair to the only role &amp;quot;governor&amp;quot; of
each word so that the assignment satisfies (G1-1) to
(G1-4) simultaneously. For example, sentence (1)
is analyzed as shown in Figure 2 provided that the
words &amp;quot;a,&amp;quot; &amp;quot;dog,&amp;quot; and &amp;quot;runs&amp;quot; are given parts-of-
speech D, N, and V, respectively (the subscript at-
tached to the words indicates the position of the word
in the sentence).
</bodyText>
<equation confidence="0.967734">
(1) A1 dog2 runs3.
</equation>
<bodyText confidence="0.9981105">
Thus, sentence (1) is generated by the grammar
GI. On the other hand, sentences (2) and (3) are
not generated since there are no proper assignments
for such sentences.
</bodyText>
<listItem confidence="0.998677">
(2) A runs.
(3) Dog dog runs.
</listItem>
<bodyText confidence="0.992899">
We can graphically represent the parsing result of
sentence (1) as shown in Figure 3 if we interpret the
governor rote of a word as a pointer to the syntactic
governor of the word. Thus, the syntactic structure
produced by a CDG is usually a dependency structure
(flays 1964) rather than a phtyrse structure.
</bodyText>
<figureCaption confidence="0.997241">
Figure 3: Dependency tree
</figureCaption>
<sectionHeader confidence="0.999618" genericHeader="method">
3 PARSING WITH
CONSTRAINT PROPAGATION
</sectionHeader>
<bodyText confidence="0.9965496875">
CDG parsing is done by assigning values to n x k
roles, whose values are selected from a finite set
L x {1,2, ... , nil}_ Therefore, CDG parsing can
be viewed as a constraint satisfaction problem over
a finite domain. Many interesting artificial intelli-
gence problems, including graph coloring and scene
labeling, are classified in this group of problems, and
much effort has been spent on the development of
efficient techniques to solve these problems. Con-
straint propagation (Waltz 1975, Montanan i 1976),
sometimes called filtering, is one such technique. One
advantage of the filtering algorithm is that it allows
new constraints to be added easily so that a better
solution can be obtained when many candidates re-
main. Usually, CDG parsing is done in the following
three steps:
</bodyText>
<listItem confidence="0.995557">
1. Form an initial constraint network using a
&amp;quot;core&amp;quot; grammar.
2. Remove local inconsistencies by filtering.
3. If any ambiguity remains, add new constraints
and go to Step 2.
</listItem>
<bodyText confidence="0.898066909090909">
In this section, we will show, through a step-by-step
example, that the filtering algorithms can be effec-
tively used to narrow down the structural ambigui-
ties of CDG parsing.
The Example
We use a PP-attachment example. Consider sen-
tence (4). Because of the three consecutive preposi-
tional phrases (PPs), this sentence has many struc-
tural ambiguities.
(4) Put the block on the floor on the table in
the room.
</bodyText>
<page confidence="0.679534">
33
</page>
<equation confidence="0.672020642857143">
{L1,P2.P3,P4}
Li P2 P3P4
LI 1 0 0 1
P2 1 1 0 1
P3 1 1 1 1
(Roil}
Roil
1L1 P2 P3P4
011 1 1 1 1
{01}
1L1
&apos;RnilI 1 1
{Li,P2.P3}
Li P2P3
1 0 1
I 1 1
Li
P2
1 Li P2P
1 1 1
I L1 P2 P3 1L1 P2 P3P4
Fin1I1 1 1 1 HMO 1 1 1 1
Li P2
011 1 1
{LI.P.2}
Li P2P3P4
1_11 1 0 1 1
P2 1 1 1 i
</equation>
<figureCaption confidence="0.980741666666667">
Figure 5: Initial constraint network (the values Rnii,
Li, P2, ... should be read as &lt;ROCIT,nil&gt;, &lt;LOC,1&gt;,
&lt;POSTMOD,2&gt;, ..., and so on.)
</figureCaption>
<figure confidence="0.63607">
Put the block on the goo( on the table in the room
V, M PFS PP4 PP5
oe4 FCSTMOO POSTP+100
LOC
</figure>
<figureCaption confidence="0.999535">
Figure 4: Possible dependency structure
</figureCaption>
<bodyText confidence="0.985782785714286">
One of the possible syntactic structures is shown
in Figure 42.
To simplify the following discussion, we treat the
grammatical symbols V. NP, and PP as terminal sym-
bols (words), since the analysis of the internal struc-
tures of such phrases is irrelevant to the point be-
ing made. The correspondence between such simpli-
fied dependency structures and the equivalent phrase
structures should be clear. Formally, the input sen-
tence that we will parse with CDG is (5).
(5) vl NP2 PP3 PP4 PPs
First, we consider a &amp;quot;core&amp;quot; grammar that con-
tains purely syntactic rules only. We define a CDG
G2a =&lt; E2, R2, L2, C2&gt; as follows:
</bodyText>
<listItem confidence="0.99989825">
• E2 = {V ,NP,PP}
• R2 = {governor}
• L2= {ROOT, OBJ ,LCIC,POSTMOD}
• C2 = Vxy : rote; P2,
</listItem>
<bodyText confidence="0.999935">
where the formula P2 is the conjunction of the
following unary and binary constraints :
</bodyText>
<equation confidence="0.9756666">
(G2a-1) word(pos(x))=PP word(mod(x)) E
{PP,NP,V}, mod(x) pos(x) )
&amp;quot;A PP modifies a PP, an NP, or a V on the left.&amp;quot;
(G2a-2) word(pos(s))=PP, word(mod(x)) E {PP ,NP}
lab(x)=POSTVIOD
</equation>
<bodyText confidence="0.6579112">
&amp;quot;If a PP modifies a PP or an NP, its label should
be POSTNOD.&amp;quot;
(G2a-3) word(pos(x))=PP, word(ntod(x))=V
iab(x)=LOC
&amp;quot;If a PP modifies a V, its label should be LOC.&amp;quot;
2In linguistics, arrows are usually drawn in the opposite
direction in a dependency diagram: from a governor (modifiee)
to its dependent (modifier). In this paper, however, we draw
an arrow from a modifier to its modifiee in order to emphasize
that this information is contained in a modifier&apos;s role.
</bodyText>
<equation confidence="0.991080444444445">
(G2a-4) word(pos(x))=NP = ( word(mod(x))=V,
dab(x)=-OBJ , mod(x) pos(x) )
&amp;quot;An NP modifies a V on the left with the label
CBI&amp;quot;
(G2a-5) word(pos(x))=V mod(s)=nil,
?ab(x)ROOT)
&amp;quot;A V modifies nothing with the label ROOT.&amp;quot;
(G2a-6) mod(s) pos(y) pos(x)
mod(x) mod(y) pos(x)
</equation>
<bodyText confidence="0.988578090909091">
&amp;quot;Modification links do not cross each other.&amp;quot;
According to the grammar G2a, sentence (5) has
14 (= Catalan(4)) different syntactic structures. We
do not generate these syntactic structures one by
one, since the number of the structures may grow
more rapidly than exponentially when the sentence
becomes long. Instead, we build a packed data struc-
ture, called a constraint network, that contains all
the syntactic structures implicitly. Explicit parse
trees can be generated whenever necessary, but it
may take a more than exponential computation time.
</bodyText>
<subsectionHeader confidence="0.652903">
Formation of initial network
</subsectionHeader>
<bodyText confidence="0.9804878">
Figure 5 shows the initial constraint network for sen-
tence (5). A node in a constraint network corre-
sponds to a role. Since each word has only one role
governor in the grammar G2, the constraint network
has five nodes corresponding to the five words in the
</bodyText>
<page confidence="0.995497">
34
</page>
<bodyText confidence="0.999252238095239">
sentence. In the figure, the node labeled V1 repre-
sents the governor role of the word Vi , and so on. A
node is associated with a set of possible values that
the role can take as its value, called a domain. The
domains of the initial constraint network are com-
puted by examining unary constraints ((G2a-1) to
(G2a-5) in our example). For example, the modifiee
of the role of the word V1 must be ROOT and its label
must be nil according to the unary constraint (G2a-
5), and therefore the domain of the corresponding
node is a singleton set f &lt;ROOT,nil&gt;1. In the figure,
values are abbreviated by concatenating the initial
letter of the label and the modifiee, such as Rnil for
&lt;P.00T , nil&gt;, 01 for &lt;OBJ 1&gt;, and so on.
An arc in a constraint network represents a bi-
nary constraint imposed on two roles. Each arc
is associated with a two-dimensional matrix called
a constraint matrix, whose xy-elements are either
1 or 0. The rows and the columns correspond to
the possible values of each of the two roles. The
value 0 indicates that this particular combination
of role values violates the binary constraints. A
constraint matrix is calculated by generating every
possible pair of values and by checking its validity
according to the binary constraints. For example,
the case in which governor(PP3) = &lt;1,0C,1&gt; and
governor(PP 4) = &lt;P0STM0D,2&gt; violates the binary
constraint (G2a-6), so the 1,1-P2 element of the con-
straint matrix between PP3 and PP4 is set to zero.
The reader should not confuse the undirected arcs
in a constraint network with the directed modifica-
tion links in a dependency diagram. An arc in a
constraint network represents the existence of a bi-
nary constraint between two nodes, and has nothing
to do with the modifier-modifiee relationships. The
possible modification relationships are represented as
the modifiee part of the domain values in a constraint
network.
A constraint network contains all the information
needed to produce the parsing results. No grammati-
cal knowledge is necessary to recover parse trees from
a constraint network. A simple backtrack search
can generate the 14 parse trees of sentence (5) from
the constraint network shown in Figure 5 at any
time. Therefore, we regard a constraint network as
a packed representation of parsing results.
Filtering
A constraint network is said to be arc consistent if,
for any constraint matrix, there are no rows and no
columns that contain only zeros. A node value cor-
responding to such a row or a column cannot partici-
pate in any solution, so it can be abandoned without
further checking. The filtering algorithm identifies
such inconsistent values and removes them from the
domains. Removing a value from one domain may
make another value in another domain inconsistent,
so the process is propagated over the network until
the network becomes arc consistent.
Filtering does not generate solutions, but may sig-
nificantly reduce the search space. In our example,
the constraint network shown in Figure 5 is already
arc consistent, so nothing can be done by filtering at
this point.
</bodyText>
<sectionHeader confidence="0.530251" genericHeader="method">
Adding New Constraints
</sectionHeader>
<bodyText confidence="0.999986333333333">
To illustrate how we can add new constraints to nar-
row down the ambiguity, let us introduce additional
constraints (G2b-1) and (G2b-2), assuming that ap-
propriate syntactic and/or semantic features are at-
tached to each word and that the function NO is
provided to access these features.
Note that these constraints are not purely syntac-
tic. Any kind of knowledge, syntactic, semantic, or
even pragmatic, can be applied in CDG parsing as
long as it is expressed as a unary or binary constraint
on word-to-word modifications.
Each value or pair of values is tested against the
newly added constraints. In the network in Figure 5,
the value P3 (i.e. &lt;POSTMOD,3&gt;) of the node PP4 (i.e..
&amp;quot;on the table (PP4)&amp;quot; modifies &amp;quot;on the floor (PP3)&amp;quot;) vi-
olates the constraint (G2b-1), so we remove P3 from
the domain of PRI. Accordingly, corresponding rows
and columns in the four constraint matrices adjacent
to the node PP4 are removed. The binary constraint
(G2b-2) affects the elements of the constraint ma-
trices. For the matrix between the nodes PP3 and
</bodyText>
<figure confidence="0.931755666666667">
(G2b-1) word(pos(x))=PP , on_table C I e(pos(x))
-i(floor C f e(mod(x)) )
&amp;quot;A floor is not on a table.&amp;quot;
(G2b-2) iab(x)=1,0C, lab(y)=1,0C, moci(x)=rnod(y),
word(mod(x))=V x=y
&amp;quot;No verb can take two locatives.&amp;quot;
</figure>
<page confidence="0.86426">
35
</page>
<figureCaption confidence="0.999397">
Figure 6: Modified network
</figureCaption>
<figure confidence="0.975381272727273">
Ll F2
[
Rni!1i
{Anil}
[01
Final 1
I L1 P2P3P4
C11 1 1 I 1
(011
I L1 P2P3P4
Fr/111 1 1 1 1
(1.1,P2,P3,P4)
Ll P2P3P4
L11 00 0 1
P2 I 1 1 0 1
{L1,P2}
I LIP
/ 1
ILi P2
Ci I 1 1
(L1R2)
Ll
P2
L1 P2
00
1 1
Ll P2 Ll P2P3P4
011 1 1 L1 00 1 1
P2 1111
I L1 I P4
Anil[ 1 Rnil r 1
I P2 _114
011 1 P-2I 1
</figure>
<figureCaption confidence="0.998827">
Figure 8: Unambiguous parsing result
</figureCaption>
<equation confidence="0.9936606">
ILI P2 I L1 P2P4
Rnil [ 1 1 Flnil I 1 1
(VIWP5
1RWVPIre1
{Ll.P21
ILi P2
P2 1 1
(P2)
[ L1 P2P4
PElli
</equation>
<figureCaption confidence="0.998834">
Figure 7: Filtered network
</figureCaption>
<bodyText confidence="0.999351130434783">
PP4, the element in row 1,1 (&lt;1..0C ,1&gt;) and column
Li (&lt;1,0C , 1&gt;) is set to zero, since both are modifica-
tions to VI with the label LOC. Similarly, the Li-Li
elements of the matrices PP3-PP5 and PP4-PP5 are
set to zero. The modified network is shown in Fig-
ure 6, where the updated elements are indicated by
asterisks.
Note that the network in Figure 6 is not arc
consistent. For example, the Li row of the matrix
PP3-PP4 consists of all zero elements. The filtering
algorithm identifies such locally inconsistent values
and eliminates them until there are no more incon-
sistent values left. The resultant network is shown
in Figure 7. This network implicitly represents the
remaining four parses of sentence (5).
Since the sentence is still ambiguous, let us con-
sider another constraint.
This sets the P2-P2 element of the matrix PP3-PP4
to zero. Filtering on this network again results in the
network shown in Figure 8, which is unambiguous,
since every node has a singleton domain. Recovering
the dependency structure (the one in Figure 4) from
this network is straightforward.
</bodyText>
<subsectionHeader confidence="0.940009">
Related Work
</subsectionHeader>
<bodyText confidence="0.992718375">
Several researchers have proposed variant data struc-
tures for representing a set of syntactic structures.
Chart (Kaplan 1973) and shared, packed for-
est (Tomita 1987) are packed data structures for
context-free parsing. In these data structures, a
substring that is recognized as a certain phrase is
represented as a single edge or node regardless of
how many different readings are possible for this
phrase. Since the production rules are context free,
it is unnecessary to check the internal structure of an
edge when combining it with another edge to form
a higher edge. However, this property is true only
when the grammar is purely context-free. If one in-
troduces context sensitivity by attaching augmenta-
tions and controlling the applicability of the produc-
tion rules, different readings of the same string with
</bodyText>
<figure confidence="0.933674625">
[Li P2P4
01 1 1 1
(01}
{RIO
101
Ti
,=-Rnil I I
Ll P2
oil i 1
oit 1
(Ll.P2,P41 (G2c-1) inh(x)=POSTMOD, lab(y)=POSTMOD,
.L-1P2P4
L1I0 0 1
P2 I 1 1 1 E f e(pos(y)) x=y
&amp;quot;No object can be on two distinct objects.&amp;quot;
raod(x)=Inod(y), on E fe(pos(x)), on
</figure>
<page confidence="0.993437">
36
</page>
<bodyText confidence="0.999945371428572">
the same nonterminal symbol have to be represented
by separate edges, and this may cause a combinato-
rial explosion.
Seo and Simmons (1988) propose a data structure
called a syntactic graph as a. packed representation of
context-free parsing. A syntactic graph is similar to a
constraint network in the sense that it is dependency-
oriented (nodes are words) and that an exclusion ma-
trix is used to represent the co-occurrence conditions
between modification links. A syntactic graph is,
however, built after context-free parsing and is there-
fore used to represent only context-free parse trees.
The formal descriptive power of syntactic graphs is
not known. As will be discussed in Section 4, the
formal descriptive power of CDG is strictly greater
than that of CFG and hence, a constraint network
can represent non-context-free parse trees as well.
Sugimura et al. (1988) propose the use of a con-
straint logic program for analyzing modifier-modifiee
relationships of Japanese. An arbitrary logical for-
mula can be a constraint, and a constraint solver
called CIL (Mukai 1985) is responsible for solving the
constraints. The generative capacity and the compu-
tational complexity of this formalism are not clear.
The above-mentioned works seem to have concen-
trated on the efficient representation of the output of
a parsing process, and lacked the formalization of a
structural disambiguation process, that is, they did
not specify what kind of knowledge can be used in
what way for structural disambiguation. In CDG
parsing, any knowledge is applicable to a constraint
network as long as it can be expressed as a constraint
between two modifications, and an efficient filtering
algorithm effectively uses it to reduce structural am-
biguities.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="method">
4 FORMAL PROPERTIES
</sectionHeader>
<subsectionHeader confidence="0.549656">
Weak Generative Capacity of CDG
</subsectionHeader>
<bodyText confidence="0.999919333333333">
Consider the language Lww = {wwlw E (o.+ b)*},
the language of strings that are obtained by con-
catenating the same arbitrary string over an alpha-
bet { a,b}. Lww is known to be non-context-free
(Hoperoft and Ullman 1979), and is frequently men-
tioned when discussing the non-context-freeness of
the &amp;quot;respectively&amp;quot; construct (e.g. &amp;quot;A, B, and C do
D, E, and F, respectively&amp;quot;) of various natural lan-
guages (e.g., Savitch et al. 1987). Although there
</bodyText>
<equation confidence="0.92542875">
E = fa, b}
L={1}
R = {partner}
C = conjunction of the following subformulas:
</equation>
<figure confidence="0.68678425">
• (word(pos(x))=a word(mod(x))=a)
&amp; (word(pos(x))=b word(mod(x))=b)
• mod(x) = pos(y) = mod(y) = pos(x)
• mod(x) pos(x) &amp; mod(x) 0 nil
• pos(x) &lt; pos(y) &lt; mod(y)
pos(x) mod(x) mod(y)
• niod(ll) &lt; Pos(V) &lt; Pos(x)
mod(y) mod(x) pos(x)
</figure>
<figureCaption confidence="0.998762">
Figure 9: Definition of Gww
Figure 10: Assignment for a sentence of Lam)
</figureCaption>
<bodyText confidence="0.999259777777778">
is no context-free grammar that generates Lww, the
grammar Gww =&lt; E, L, R, C &gt; shown in Figure 9
generates it (Maruyama 1990). An assignment given
to a sentence &amp;quot;a.aba.ab&amp;quot; is shown in Figure 10.
On the other hand, any context-free language
can be generated by a degree=2 CDG. This can
be proved by constructing a constraint dependency
grammar GC DG from an arbitrary context-free gram-
mar GeFG in Greibach Normal Form, and by show-
ing that the two grammars generate exactly the same
language. Since GeFa is in Greibach Normal Form,
it is easy to make one-to-one correspondence between
a word in a sentence and a rule application in a
phrase-structure tree. The details of the proof are
given in Maruyama (1990). This, combined with the
fact that Gww generates Lww, means that the weak
generative capacity of CDG with degree=2 is strictly
greater than that of CFG.
</bodyText>
<subsectionHeader confidence="0.686362">
Computational complexity of COG parsing
</subsectionHeader>
<bodyText confidence="0.99954575">
Let us consider a constraint dependency grammar
G =&lt; E, R, L,C &gt; with arity=2 and degree=k. Let
be the length of the input sentence. Consider the
space complexity of the constraint network first. In
</bodyText>
<page confidence="0.997917">
37
</page>
<bodyText confidence="0.999697714285714">
CDG parsing, every word has k roles, so there are nx
k nodes in total. A role can have nx possible values,
where 1 is the size of L, so the maximum domain
size is ax 1. Binary constraints may be imposed on
arbitrary pairs of roles, and therefore the number of
constraint matrices is at most proportional to (nk)2.
Since the size of a constraint matrix is (n1)2, the
total space complexity of the constraint network is
0(1210n4). Since k and 1 are grammatical constants,
it is 0(n4) for the sentence length n.
As the initial formation of a constraint network
takes a computation time proportional to the size of
the constraint network, the time complexity of the
initial formation of a constraint network is OM.
The complexity of adding new constraints to a con-
straint network never exceeds the complexity of the
initial formation of a constraint network, so it is also
bounded by 0(0).
The most efficient filtering algorithm developed
so far runs in 0(ea2) time, where e is the number
of arcs and a is the size of the domains in a con-
straint network (Mohr and Henderson 1986). Since
the number of arcs is at most 0((nk)2), filtering can
be performed in 0Unk)2(n1)2), which is 0(n4) with-
out grammatical constants.
Thus, in CDG parsing with arity 2, both the ini-
tial formation of a constraint network and filtering
are bounded in 0(n4) time.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="conclusions">
5 CONCLUSION
</sectionHeader>
<bodyText confidence="0.999967058823529">
We have proposed a formal grammar that allows effi-
cient structural disambiguation. Grammar rules are
constraints on word-to-word modifications, and pars-
ing is done by adding the constraints to a data struc-
ture called a constraint network. The initial forma-
tion of a constraint network and the filtering have a
polynomial time bound whereas the weak generative
capacity of CDG is strictly greater than that of CFG.
CDG is actually being used for an interac-
tive Japanese parser of a Japanese-to-English ma-
chine translation system for a newspaper domain
(Maruyama et. al. 1990). A parser for such a wide
domain should make use of any kind of information
available to the system, including user-supplied in-
formation. The parser treats this information as an-
other set of unary constraints and applies it to the
constraint network.
</bodyText>
<sectionHeader confidence="0.994334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999672434782609">
1. Church, K. and Patti. R. 1982, &amp;quot;Coping with
syntactic ambiguity, or how to put the block
in the box on the table,&amp;quot; American Journal of
Computational Linguistics, Vol. 8, No. 3-4.
2. Hays, D.E. 1964, &amp;quot;Dependency theory: a for-
malism and some observations,&amp;quot; Language, Vol.
40.
3. Hoperoft, J.E. and Ullman, J.D., 1979, Intro-
duction to Automata Theory, Languages, and
Computation, Addison-Wesley.
4. Kaplan, R.M. 1973, &amp;quot;A general syntactic pro-
cessor,&amp;quot; in: Rustin, R. (ed.) Natural Language
Processing, Algorithmics Press.
5. Maruya,ma, H. 1990, &amp;quot;Constraint Dependency
Grammar,&amp;quot; TRL Research Report RT0044, IBM
Research, Tokyo Research Laboratory.
6. Maruyama, H., Watanabe, H., and Ogino, S,
1990, &amp;quot;An interactive Japanese parser for ma-
chine translation,&amp;quot; COLING &apos;90, to appear.
7. Mohr, R. and Henderson, T. 1986,- &amp;quot;Arc and
path consistency revisited,&amp;quot; Artificial Intelli-
gence, Vol. 28.
8. Montanan, U. 1976, &amp;quot;Networks of constraints:
Fundamental properties and applications to pic-
ture processing,&amp;quot; Information Science, Vol. 7.
9. Muhl, K. 1985, &amp;quot;Unification over complex inde-
terminates in Prolog,&amp;quot; ICOT Technical Report
TR-113.
10. Savitch, W.J. et at. (eds.) 1987, The Formal
Complexity of Natural Language, Reidel.
11. Seo, J. and Simmons, R. 1988, &amp;quot;Syntactic
graphs: a representation for the union of all
ambiguous parse trees,&amp;quot; Computational Linguis-
tics, Vol. 15, No. 7.
12. Sugirnura, R., Miyoshi, H., and Mukai, K. 1988,
&amp;quot;Constraint analysis on Japanese modification,&amp;quot;
in: Dahl, V. and Saint-Dizier, P. (eds.) Natu-
ral Language Understanding and Logic Program-
ming, IL Elsevier.
13. Tomita, M. 1987, &amp;quot;An efficient augmented-
context-free parsing algorithm,&amp;quot; Computational
Linguistics, Vol. 13.
14. Waltz, D.L. 1975, &amp;quot;Understanding line draw-
ings of scenes with shadows,&amp;quot; in: Winston,
P.H. (ed.): The Psychology of Computer Vision,
McGraw-Hill.
</reference>
<page confidence="0.999348">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.587559">
<title confidence="0.997165">STRUCTURAL DISAMBIGUATION WITH CONSTRAINT PROPAGATION</title>
<author confidence="0.968871">Hiroshi Maruyarna</author>
<affiliation confidence="0.999965">IBM Research, Tokyo Research Laboratory</affiliation>
<address confidence="0.841825">5-19 Sanbancho, Chiyoda-ku, Tokyo 102 Japan</address>
<email confidence="0.977868">maruyamaqjpntscvni.bitnet</email>
<abstract confidence="0.986761">present a new grammatical formalism called Con- Dependency Grammar in which every rule is given constraint on wordto-word modifications. CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees. The weak generative capacity and the computational complexity of CDG parsing are also discussed_</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R</author>
</authors>
<title>Coping with syntactic ambiguity, or how to put the block in the box on the table,&amp;quot;</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<pages>3--4</pages>
<contexts>
<context position="6642" citStr="(1)" startWordPosition="1158" endWordPosition="1158">re 2: Assignment Satisfying (G1-1) to (G1-4) SUBJ doC7g2) (IDE; (G1-2) word(pos(x))=N iab(x)=SUBJ, word(mod(x))=V , pos(x) mod(x) ) &amp;quot;A noun modifies a verb (V) on the right with the label SUBJ.&amp;quot; (G1-3) inord(pos(x))=V iab(x)=ROOT, mod(x)=nil) &amp;quot;A verb modifies nothing and its label should be ROOT.&amp;quot; (G1-4) ( mod(x)=mod(y), lab(x)=lab(y) ) x=y &amp;quot;No two words can modify the same word with the same label.&amp;quot; Analyzing a sentence with G1 means assigning a label-modifiee pair to the only role &amp;quot;governor&amp;quot; of each word so that the assignment satisfies (G1-1) to (G1-4) simultaneously. For example, sentence (1) is analyzed as shown in Figure 2 provided that the words &amp;quot;a,&amp;quot; &amp;quot;dog,&amp;quot; and &amp;quot;runs&amp;quot; are given parts-ofspeech D, N, and V, respectively (the subscript attached to the words indicates the position of the word in the sentence). (1) A1 dog2 runs3. Thus, sentence (1) is generated by the grammar GI. On the other hand, sentences (2) and (3) are not generated since there are no proper assignments for such sentences. (2) A runs. (3) Dog dog runs. We can graphically represent the parsing result of sentence (1) as shown in Figure 3 if we interpret the governor rote of a word as a pointer to the syntactic go</context>
</contexts>
<marker>1.</marker>
<rawString>Church, K. and Patti. R. 1982, &amp;quot;Coping with syntactic ambiguity, or how to put the block in the box on the table,&amp;quot; American Journal of Computational Linguistics, Vol. 8, No. 3-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Hays</author>
</authors>
<title>Dependency theory: a formalism and some observations,&amp;quot;</title>
<date>1964</date>
<journal>Language,</journal>
<volume>Vol.</volume>
<contexts>
<context position="6966" citStr="(2)" startWordPosition="1217" endWordPosition="1217">ab(x)=lab(y) ) x=y &amp;quot;No two words can modify the same word with the same label.&amp;quot; Analyzing a sentence with G1 means assigning a label-modifiee pair to the only role &amp;quot;governor&amp;quot; of each word so that the assignment satisfies (G1-1) to (G1-4) simultaneously. For example, sentence (1) is analyzed as shown in Figure 2 provided that the words &amp;quot;a,&amp;quot; &amp;quot;dog,&amp;quot; and &amp;quot;runs&amp;quot; are given parts-ofspeech D, N, and V, respectively (the subscript attached to the words indicates the position of the word in the sentence). (1) A1 dog2 runs3. Thus, sentence (1) is generated by the grammar GI. On the other hand, sentences (2) and (3) are not generated since there are no proper assignments for such sentences. (2) A runs. (3) Dog dog runs. We can graphically represent the parsing result of sentence (1) as shown in Figure 3 if we interpret the governor rote of a word as a pointer to the syntactic governor of the word. Thus, the syntactic structure produced by a CDG is usually a dependency structure (flays 1964) rather than a phtyrse structure. Figure 3: Dependency tree 3 PARSING WITH CONSTRAINT PROPAGATION CDG parsing is done by assigning values to n x k roles, whose values are selected from a finite set L x {1,2, ..</context>
</contexts>
<marker>2.</marker>
<rawString>Hays, D.E. 1964, &amp;quot;Dependency theory: a formalism and some observations,&amp;quot; Language, Vol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation,</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="6974" citStr="(3)" startWordPosition="1219" endWordPosition="1219">b(y) ) x=y &amp;quot;No two words can modify the same word with the same label.&amp;quot; Analyzing a sentence with G1 means assigning a label-modifiee pair to the only role &amp;quot;governor&amp;quot; of each word so that the assignment satisfies (G1-1) to (G1-4) simultaneously. For example, sentence (1) is analyzed as shown in Figure 2 provided that the words &amp;quot;a,&amp;quot; &amp;quot;dog,&amp;quot; and &amp;quot;runs&amp;quot; are given parts-ofspeech D, N, and V, respectively (the subscript attached to the words indicates the position of the word in the sentence). (1) A1 dog2 runs3. Thus, sentence (1) is generated by the grammar GI. On the other hand, sentences (2) and (3) are not generated since there are no proper assignments for such sentences. (2) A runs. (3) Dog dog runs. We can graphically represent the parsing result of sentence (1) as shown in Figure 3 if we interpret the governor rote of a word as a pointer to the syntactic governor of the word. Thus, the syntactic structure produced by a CDG is usually a dependency structure (flays 1964) rather than a phtyrse structure. Figure 3: Dependency tree 3 PARSING WITH CONSTRAINT PROPAGATION CDG parsing is done by assigning values to n x k roles, whose values are selected from a finite set L x {1,2, ... , nil}</context>
</contexts>
<marker>3.</marker>
<rawString>Hoperoft, J.E. and Ullman, J.D., 1979, Introduction to Automata Theory, Languages, and Computation, Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>A general syntactic processor,&amp;quot;</title>
<date>1973</date>
<booktitle>Natural Language Processing,</booktitle>
<editor>in: Rustin, R. (ed.)</editor>
<publisher>Algorithmics Press.</publisher>
<contexts>
<context position="8653" citStr="(4)" startWordPosition="1500" endWordPosition="1500">ithm is that it allows new constraints to be added easily so that a better solution can be obtained when many candidates remain. Usually, CDG parsing is done in the following three steps: 1. Form an initial constraint network using a &amp;quot;core&amp;quot; grammar. 2. Remove local inconsistencies by filtering. 3. If any ambiguity remains, add new constraints and go to Step 2. In this section, we will show, through a step-by-step example, that the filtering algorithms can be effectively used to narrow down the structural ambiguities of CDG parsing. The Example We use a PP-attachment example. Consider sentence (4). Because of the three consecutive prepositional phrases (PPs), this sentence has many structural ambiguities. (4) Put the block on the floor on the table in the room. 33 {L1,P2.P3,P4} Li P2 P3P4 LI 1 0 0 1 P2 1 1 0 1 P3 1 1 1 1 (Roil} Roil 1L1 P2 P3P4 011 1 1 1 1 {01} 1L1 &apos;RnilI 1 1 {Li,P2.P3} Li P2P3 1 0 1 I 1 1 Li P2 1 Li P2P 1 1 1 I L1 P2 P3 1L1 P2 P3P4 Fin1I1 1 1 1 HMO 1 1 1 1 Li P2 011 1 1 {LI.P.2} Li P2P3P4 1_11 1 0 1 1 P2 1 1 1 i Figure 5: Initial constraint network (the values Rnii, Li, P2, ... should be read as &lt;ROCIT,nil&gt;, &lt;LOC,1&gt;, &lt;POSTMOD,2&gt;, ..., and so on.) Put the block on the </context>
<context position="11182" citStr="(4)" startWordPosition="1973" endWordPosition="1973">ependency diagram: from a governor (modifiee) to its dependent (modifier). In this paper, however, we draw an arrow from a modifier to its modifiee in order to emphasize that this information is contained in a modifier&apos;s role. (G2a-4) word(pos(x))=NP = ( word(mod(x))=V, dab(x)=-OBJ , mod(x) pos(x) ) &amp;quot;An NP modifies a V on the left with the label CBI&amp;quot; (G2a-5) word(pos(x))=V mod(s)=nil, ?ab(x)ROOT) &amp;quot;A V modifies nothing with the label ROOT.&amp;quot; (G2a-6) mod(s) pos(y) pos(x) mod(x) mod(y) pos(x) &amp;quot;Modification links do not cross each other.&amp;quot; According to the grammar G2a, sentence (5) has 14 (= Catalan(4)) different syntactic structures. We do not generate these syntactic structures one by one, since the number of the structures may grow more rapidly than exponentially when the sentence becomes long. Instead, we build a packed data structure, called a constraint network, that contains all the syntactic structures implicitly. Explicit parse trees can be generated whenever necessary, but it may take a more than exponential computation time. Formation of initial network Figure 5 shows the initial constraint network for sentence (5). A node in a constraint network corresponds to a role. Since each</context>
</contexts>
<marker>4.</marker>
<rawString>Kaplan, R.M. 1973, &amp;quot;A general syntactic processor,&amp;quot; in: Rustin, R. (ed.) Natural Language Processing, Algorithmics Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ma Maruya</author>
<author>H</author>
</authors>
<title>Constraint Dependency Grammar,&amp;quot;</title>
<date>1990</date>
<tech>TRL Research Report RT0044,</tech>
<institution>IBM Research, Tokyo Research Laboratory.</institution>
<contexts>
<context position="9827" citStr="(5)" startWordPosition="1741" endWordPosition="1741">so on.) Put the block on the goo( on the table in the room V, M PFS PP4 PP5 oe4 FCSTMOO POSTP+100 LOC Figure 4: Possible dependency structure One of the possible syntactic structures is shown in Figure 42. To simplify the following discussion, we treat the grammatical symbols V. NP, and PP as terminal symbols (words), since the analysis of the internal structures of such phrases is irrelevant to the point being made. The correspondence between such simplified dependency structures and the equivalent phrase structures should be clear. Formally, the input sentence that we will parse with CDG is (5). (5) vl NP2 PP3 PP4 PPs First, we consider a &amp;quot;core&amp;quot; grammar that contains purely syntactic rules only. We define a CDG G2a =&lt; E2, R2, L2, C2&gt; as follows: • E2 = {V ,NP,PP} • R2 = {governor} • L2= {ROOT, OBJ ,LCIC,POSTMOD} • C2 = Vxy : rote; P2, where the formula P2 is the conjunction of the following unary and binary constraints : (G2a-1) word(pos(x))=PP word(mod(x)) E {PP,NP,V}, mod(x) pos(x) ) &amp;quot;A PP modifies a PP, an NP, or a V on the left.&amp;quot; (G2a-2) word(pos(s))=PP, word(mod(x)) E {PP ,NP} lab(x)=POSTVIOD &amp;quot;If a PP modifies a PP or an NP, its label should be POSTNOD.&amp;quot; (G2a-3) word(pos(x))=PP</context>
<context position="11161" citStr="(5)" startWordPosition="1969" endWordPosition="1969">site direction in a dependency diagram: from a governor (modifiee) to its dependent (modifier). In this paper, however, we draw an arrow from a modifier to its modifiee in order to emphasize that this information is contained in a modifier&apos;s role. (G2a-4) word(pos(x))=NP = ( word(mod(x))=V, dab(x)=-OBJ , mod(x) pos(x) ) &amp;quot;An NP modifies a V on the left with the label CBI&amp;quot; (G2a-5) word(pos(x))=V mod(s)=nil, ?ab(x)ROOT) &amp;quot;A V modifies nothing with the label ROOT.&amp;quot; (G2a-6) mod(s) pos(y) pos(x) mod(x) mod(y) pos(x) &amp;quot;Modification links do not cross each other.&amp;quot; According to the grammar G2a, sentence (5) has 14 (= Catalan(4)) different syntactic structures. We do not generate these syntactic structures one by one, since the number of the structures may grow more rapidly than exponentially when the sentence becomes long. Instead, we build a packed data structure, called a constraint network, that contains all the syntactic structures implicitly. Explicit parse trees can be generated whenever necessary, but it may take a more than exponential computation time. Formation of initial network Figure 5 shows the initial constraint network for sentence (5). A node in a constraint network corresponds </context>
<context position="14048" citStr="(5)" startWordPosition="2455" endWordPosition="2455">twork with the directed modification links in a dependency diagram. An arc in a constraint network represents the existence of a binary constraint between two nodes, and has nothing to do with the modifier-modifiee relationships. The possible modification relationships are represented as the modifiee part of the domain values in a constraint network. A constraint network contains all the information needed to produce the parsing results. No grammatical knowledge is necessary to recover parse trees from a constraint network. A simple backtrack search can generate the 14 parse trees of sentence (5) from the constraint network shown in Figure 5 at any time. Therefore, we regard a constraint network as a packed representation of parsing results. Filtering A constraint network is said to be arc consistent if, for any constraint matrix, there are no rows and no columns that contain only zeros. A node value corresponding to such a row or a column cannot participate in any solution, so it can be abandoned without further checking. The filtering algorithm identifies such inconsistent values and removes them from the domains. Removing a value from one domain may make another value in another do</context>
<context position="17444" citStr="(5)" startWordPosition="3068" endWordPosition="3068">ons to VI with the label LOC. Similarly, the Li-Li elements of the matrices PP3-PP5 and PP4-PP5 are set to zero. The modified network is shown in Figure 6, where the updated elements are indicated by asterisks. Note that the network in Figure 6 is not arc consistent. For example, the Li row of the matrix PP3-PP4 consists of all zero elements. The filtering algorithm identifies such locally inconsistent values and eliminates them until there are no more inconsistent values left. The resultant network is shown in Figure 7. This network implicitly represents the remaining four parses of sentence (5). Since the sentence is still ambiguous, let us consider another constraint. This sets the P2-P2 element of the matrix PP3-PP4 to zero. Filtering on this network again results in the network shown in Figure 8, which is unambiguous, since every node has a singleton domain. Recovering the dependency structure (the one in Figure 4) from this network is straightforward. Related Work Several researchers have proposed variant data structures for representing a set of syntactic structures. Chart (Kaplan 1973) and shared, packed forest (Tomita 1987) are packed data structures for context-free parsing.</context>
</contexts>
<marker>5.</marker>
<rawString>Maruya,ma, H. 1990, &amp;quot;Constraint Dependency Grammar,&amp;quot; TRL Research Report RT0044, IBM Research, Tokyo Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
<author>H Watanabe</author>
<author>S Ogino</author>
</authors>
<title>An interactive Japanese parser for machine translation,&amp;quot;</title>
<date>1990</date>
<journal>COLING</journal>
<note>90, to appear.</note>
<marker>6.</marker>
<rawString>Maruyama, H., Watanabe, H., and Ogino, S, 1990, &amp;quot;An interactive Japanese parser for machine translation,&amp;quot; COLING &apos;90, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mohr</author>
<author>T Henderson</author>
</authors>
<title>Arc and path consistency revisited,&amp;quot;</title>
<date>1986</date>
<journal>Artificial Intelligence,</journal>
<volume>28</volume>
<marker>7.</marker>
<rawString>Mohr, R. and Henderson, T. 1986,- &amp;quot;Arc and path consistency revisited,&amp;quot; Artificial Intelligence, Vol. 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Montanan</author>
</authors>
<title>Networks of constraints: Fundamental properties and applications to picture processing,&amp;quot;</title>
<date>1976</date>
<journal>Information Science,</journal>
<volume>7</volume>
<marker>8.</marker>
<rawString>Montanan, U. 1976, &amp;quot;Networks of constraints: Fundamental properties and applications to picture processing,&amp;quot; Information Science, Vol. 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Muhl</author>
</authors>
<title>Unification over complex indeterminates in Prolog,&amp;quot;</title>
<date>1985</date>
<tech>ICOT Technical Report TR-113.</tech>
<marker>9.</marker>
<rawString>Muhl, K. 1985, &amp;quot;Unification over complex indeterminates in Prolog,&amp;quot; ICOT Technical Report TR-113.</rawString>
</citation>
<citation valid="false">
<booktitle>1987, The Formal Complexity of Natural Language,</booktitle>
<editor>Savitch, W.J. et at. (eds.)</editor>
<location>Reidel.</location>
<marker>10.</marker>
<rawString>Savitch, W.J. et at. (eds.) 1987, The Formal Complexity of Natural Language, Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Seo</author>
<author>R Simmons</author>
</authors>
<title>Syntactic graphs: a representation for the union of all ambiguous parse trees,&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>15</volume>
<marker>11.</marker>
<rawString>Seo, J. and Simmons, R. 1988, &amp;quot;Syntactic graphs: a representation for the union of all ambiguous parse trees,&amp;quot; Computational Linguistics, Vol. 15, No. 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sugirnura</author>
<author>H Miyoshi</author>
<author>K Mukai</author>
</authors>
<title>Constraint analysis on Japanese modification,&amp;quot;</title>
<date>1988</date>
<booktitle>Natural Language Understanding and Logic Programming, IL</booktitle>
<editor>in: Dahl, V. and Saint-Dizier, P. (eds.)</editor>
<publisher>Elsevier.</publisher>
<marker>12.</marker>
<rawString>Sugirnura, R., Miyoshi, H., and Mukai, K. 1988, &amp;quot;Constraint analysis on Japanese modification,&amp;quot; in: Dahl, V. and Saint-Dizier, P. (eds.) Natural Language Understanding and Logic Programming, IL Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An efficient augmentedcontext-free parsing algorithm,&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<marker>13.</marker>
<rawString>Tomita, M. 1987, &amp;quot;An efficient augmentedcontext-free parsing algorithm,&amp;quot; Computational Linguistics, Vol. 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
</authors>
<title>Understanding line drawings of scenes with shadows,&amp;quot; in:</title>
<date>1975</date>
<booktitle>The Psychology of Computer Vision,</booktitle>
<editor>Winston, P.H. (ed.):</editor>
<publisher>McGraw-Hill.</publisher>
<marker>14.</marker>
<rawString>Waltz, D.L. 1975, &amp;quot;Understanding line drawings of scenes with shadows,&amp;quot; in: Winston, P.H. (ed.): The Psychology of Computer Vision, McGraw-Hill.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>