<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000526">
<title confidence="0.998024">
Automatically Identifying Implicit Arguments to
Improve Argument Linking and Coherence Modeling
</title>
<author confidence="0.982669">
Michael Roth and Anette Frank
</author>
<affiliation confidence="0.9138555">
Department of Computational Linguistics
Heidelberg University, Germany
</affiliation>
<email confidence="0.998179">
{mroth,frank}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998524">
Implicit arguments are a discourse-level phe-
nomenon that has not been extensively stud-
ied in semantic processing. One reason for
this lies in the scarce amount of annotated data
sets available. We argue that more data of
this kind would be helpful to improve exist-
ing approaches to linking implicit arguments
in discourse and to enable more in-depth stud-
ies of the phenomenon itself. In this paper, we
present a range of studies that empirically val-
idate this claim. Our contributions are three-
fold: we present a heuristic approach to auto-
matically identify implicit arguments and their
antecedents by exploiting comparable texts;
we show how the induced data can be used as
training data for improving existing argument
linking models; finally, we present a novel ap-
proach to modeling local coherence that ex-
tends previous approaches by taking into ac-
count non-explicit entity references.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999847125">
Semantic role labeling systems traditionally process
text in a sentence-by-sentence fashion, construct-
ing local structures of semantic meaning (Palmer et
al., 2010). Information relevant to these structures,
however, can be non-local in natural language texts
(Palmer et al., 1986; Fillmore, 1986, inter alia). In
this paper, we view instances of this phenomenon,
also referred to as implicit arguments, as elements
of discourse. In a coherent discourse, each utter-
ance focuses on a salient set of entities, also called
“foci” (Sidner, 1979) or “centers” (Joshi and Kuhn,
1979). According to the theory of Centering (Grosz
et al., 1995), the salience of an entity in a discourse
is reflected by linguistic factors such as choice of
referring expression and syntactic form. Both ex-
tremes of salience, i.e., contexts of referential conti-
nuity (Brown, 1983) and irrelevance, can also be re-
flected by the non-realization of an entity. Altough
specific instances of non-realization, so-called zero
anaphora, have been well-studied in discourse anal-
ysis (Sag and Hankamer, 1984; Tanenhaus and Carl-
son, 1990, inter alia), this phenomenon has widely
been ignored in computational approaches to entity-
based coherence modeling. It could, however, pro-
vide an explanation for local coherence in cases that
are not covered by current models of Centering (cf.
Louis and Nenkova (2010)). In this work, we pro-
pose a new model to predict whether realizing an
argument contributes to local coherence in a given
position in discourse. Example (1) shows a text frag-
ment, in which argument realization is necessary in
the first sentence but redundant in the second.
</bodyText>
<listItem confidence="0.648877">
(1) El Salvador is now the only Latin Ameri-
</listItem>
<bodyText confidence="0.999660333333333">
can country which still has troops in [Iraq].
Nicaragua, Honduras and the Dominican
Republic have withdrawn their troops [0].
From a semantic processing perspective, a human
reader can easily infer that “Iraq”, the marked en-
tity in the first sentence of Example (1), is also an
implicit argument of the predicate “withdraw” in the
second sentence. This inference step is, however,
difficult to model computationally as it involves an
interplay of two challenging sub-tasks: first, a se-
mantic processor has to determine that an argument
is not realized (but inferrable); and second, a suit-
</bodyText>
<page confidence="0.984123">
306
</page>
<note confidence="0.9007425">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 306–316, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999909157894737">
able antecedent has to be found within the discourse
context. For the remainder of this paper, we refer to
these steps as identifying and linking implicit argu-
ments to discourse antecedents.
As indicated by Example (1), implicit arguments
are an important aspect in semantic processing, yet
they are not captured in traditional semantic role la-
beling systems. The main reasons for this are the
scarcity of annotated data, and the inherent difficulty
of inferring discourse antecedents automatically.
In this paper, we propose to induce implicit ar-
guments and discourse antecedents by exploiting
complementary (explicit) information obtained from
monolingual comparable texts (Section 3). We ap-
ply the empirically acquired data in argument link-
ing (Section 4) and coherence modeling (Section 5).
We conclude with a discussion on the advantages of
our data set and outline directions for future work
(Section 6).
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999983636363637">
The most prominent approach to entity-based coher-
ence modeling nowadays is the entity grid model by
Barzilay and Lapata (2005). It has originally been
proposed for automatic sentence ordering but has
also been applied in coherence evaluation and read-
ability assessment (Barzilay and Lapata, 2008; Pitler
and Nenkova, 2008), and story generation (McIntyre
and Lapata, 2009). Based on the original model,
a few extensions have been proposed: for exam-
ple, Filippova and Strube (2007) and Elsner and
Charniak (2011b) suggested additional features to
characterize semantic relatedness between entities
and features specific to single entities, respectively.
Other entity-based approaches to coherence model-
ing include the pronoun model by Charniak and El-
sner (2009) and the discourse-new model by Elsner
and Charniak (2008). All of these approaches are,
however, based on explicitly realized entity men-
tions only, ignoring references that are inferrable.
The role of implicit arguments has been studied
early on in the context of semantic processing (Fill-
more, 1986; Palmer et al., 1986). Yet, the phe-
nomenon has mostly been ignored in semantic role
labeling. First data sets, focusing on implicit argu-
ments, have only recently become available: Rup-
penhofer et al. (2010) organized a SemEval shared
task on “linking events and participants in dis-
course”, Gerber and Chai (2012) made available im-
plicit argument annotations for the NomBank corpus
(Meyers et al., 2008) and Moor et al. (2013) pro-
vide annotations for parts of the OntoNotes corpus
(Weischedel et al., 2011). However, these resources
are very limited: The annotations by Moor et al. and
Gerber and Chai are restricted to 5 and 10 predi-
cate types, respectively. The training set of the Se-
mEval task contains only 245 resolved implicit argu-
ments in total. As pointed out by Silberer and Frank
(2012), additional training data can be heuristically
created by treating anaphoric mentions as implicit
arguments. Their experimental results showed that
artificial training data can indeed improve results,
but only when obtained from corpora with manual
semantic role annotations (on the sentence level) and
gold coreference chains.
</bodyText>
<sectionHeader confidence="0.9919695" genericHeader="method">
3 Identifying and linking implicit
arguments
</sectionHeader>
<bodyText confidence="0.999898571428572">
The aim of this work is to automatically construct
a data set of implicit arguments and their discourse
antecedents. We propose an induction approach that
exploits complementary information obtained from
pairs of comparable texts. As a basis for this ap-
proach, we rely on several preparatory steps pro-
posed in the literature that first identify informa-
tion two documents have in common (cf. Figure 1).
In particular, we align corresponding predicate-
argument structures (PAS) using graph-based clus-
tering (Roth and Frank, 2012b). We then determine
co-referring entities across the texts using corefer-
ence resolution techniques on concatenated docu-
ment pairs (Lee et al., 2012). These preprocessing
steps are described in more detail in Section 3.1.
Given the preprocessed comparable texts and
aligned PAS, we propose to heuristically iden-
tify implicit arguments and link them to their
antecedents via the cross-document coreference
chains. We describe the details of this approach in
Section 3.2.
</bodyText>
<subsectionHeader confidence="0.999335">
3.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.990703666666667">
The starting point for our approach is the data set of
automatically aligned predicate pairs that has been
released by Roth and Frank (2012a).1 This data
</bodyText>
<footnote confidence="0.972459">
1cf. http://www.cl.uni-heidelberg.de/%7Emroth/
</footnote>
<page confidence="0.99582">
307
</page>
<bodyText confidence="0.928487857142857">
Sentence that comprises a PAS with an (correctly predicted) implicit argument induced antecedent
The [OA0] [operatingA3] loss, as measured by ... widened to 189 million euros ...
It was handed over to Mozambican control ... 33 years after [OA0] independence.
... [local officials A0] failed to immediately report [the accident A1] [OA2] . . .
T-Online[’s]
Mozambique[’s]
[to] the government
</bodyText>
<tableCaption confidence="0.99547">
Table 1: Three positive examples of automatically induced implicit argument and antecedent pairs.
</tableCaption>
<figureCaption confidence="0.698814666666667">
Figure 1: Illustration of the induction approach: texts
consist of PAS (represented by overlapping circles);
we exploit alignments between corresponding predicates
across texts (marked by solid lines) and co-referring enti-
ties (marked by dotted lines) to infer implicit arguments
(marked by ‘i’) and link antecedents (curly dashed line)
</figureCaption>
<bodyText confidence="0.981334578947368">
set, henceforth just R&amp;F data, is a collection of
283,588 predicate pairs that have been aligned “with
high precision”2 across comparable newswire arti-
cles from the Gigaword corpus (Parker et al., 2011).
To use these documents for our argument induc-
tion technique, we apply a couple of pre-processing
tools on each single document and perform cross-
document entity coreference on pairs of documents.
Single document pre-processing. We apply sev-
eral preprocessing steps to all documents in
the R&amp;F data: we use the Stanford CoreNLP
package3 for tokenization and sentence split-
ting. We then apply MATE tools (Bohnet, 2010;
Bj¨orkelund et al., 2010), including the integrated
PropBank/NomBank-style semantic parser, to re-
construct local predicate-argument structures for
aligned predicates. Finally, we resolve pronouns that
occur in a PAS using the coreference resolution sys-
tem by Martschat et al. (2012).
</bodyText>
<footnote confidence="0.999237">
2The used method achieved a precision of 86.2% at a recall
of 29.1% on the Roth and Frank (2012a) test set.
3http://nlp.stanford.edu/software/
</footnote>
<bodyText confidence="0.979045545454545">
Cross-document coreference. We apply cross-
document coreference resolution to induce an-
tecedents for implicit arguments. In practice, we
use the Stanford Coreference System (Lee et al.,
2013) and run it on pairs of texts by simply pro-
viding a single document as input, comprising of a
concatenation of the two texts. To perform this step
with high precision, we only use the most precise
resolution sieves: “String Match”, “Relaxed String
Match”, “Precise Constructs”, “Strict Head Match
[A-C]”, and “Proper Head Noun Match”.
</bodyText>
<subsectionHeader confidence="0.998279">
3.2 Identification and linking approach
</subsectionHeader>
<bodyText confidence="0.999096777777778">
Given a pair of aligned predicates from two compa-
rable texts, we examine the parser output to identify
the arguments in each predicate-argument structure
(PAS). We compare the set of realized argument po-
sitions in both structures to determine whether one
PAS contains an argument position (explicit) that
has not been realized in the other PAS (implicit).
For each implicit argument, we identify appropri-
ate antecedents by considering the cross-document
coreference chain of its explicit counterpart. As our
goal is to link arguments within discourse, we re-
strict candidate antecedents to mentions that occur
in the same document as the implicit argument.
We apply a number of restrictions to the resulting
pairs of implicit arguments and antecedents to mini-
mize the impact of errors from preprocessing:
- The aligned PAS should consist of a different
number of arguments (to minimize the impact
of argument labeling errors)
- The antecedent should not be a resolved pro-
noun (to avoid errors resulting from incorrect
pronoun resolution)
- The antecedent should not be in the same sen-
tence as the implicit argument (to circumvent
cases, in which an implicit argument is actu-
ally explicit but has not been recognized by the
parser)
</bodyText>
<page confidence="0.997639">
308
</page>
<subsectionHeader confidence="0.998793">
3.3 Resulting data set
</subsectionHeader>
<bodyText confidence="0.99997228">
We apply the identification and linking approach to
the full R&amp;F data set of aligned predicates. As a re-
sult, we induce a total of 701 implicit argument and
antecedent pairs, each in a separate document, in-
volving 535 different predicates. Examples are dis-
played in Table 1. Note that 701 implicit arguments
from 283,588 pairs of predicate-argument structures
seem to represent a fairly low recall. Most predicate
pairs in the high precision data set of Roth and Frank
(2012a) do, however, consist of identical argument
positions (84.5%). In the remaining cases, in which
an implicit argument can be identified (15.5%), an
antecedent in discourse cannot always be found us-
ing the high precision coreference sieves. This does
not mean that implicit arguments are a rare phe-
nomenon in general. In fact, 38.9% of all manually
aligned predicate pairs in Roth and Frank (2012a)
involved a different number of arguments.
We manually evaluated a subset of 90 induced im-
plicit arguments and found 80 discourse antecedents
to be correct (89%). Some incorrectly linked in-
stances still result from preprocessing errors. In Ta-
ble 2, we present a range of different error types that
occurred when extracting implicit arguments with-
out any restrictions.
</bodyText>
<sectionHeader confidence="0.9911775" genericHeader="method">
4 Experiment 1: Linking implicit
arguments
</sectionHeader>
<bodyText confidence="0.999953818181818">
Our first experiment assesses the utility of automat-
ically induced implicit arguments and antecedent
pairs for the task of implicit argument linking. For
evaluation, we use the data sets from the SemEval
2010 task on Linking Events and their Participants
in Discourse (Ruppenhofer et al., 2010, henceforth
just SemEval). For direct comparison with previous
results and heuristic acquisition techniques (cf. Sec-
tion 2), we apply the implicit argument identifica-
tion and linking model by Silberer and Frank (2012,
henceforth S&amp;F) for training and testing.
</bodyText>
<subsectionHeader confidence="0.998594">
4.1 Task summary
</subsectionHeader>
<bodyText confidence="0.99996196">
Both the training and test sets of the SemEval task
are text corpora extracted from Sherlock Holmes
novels, with manual frame semantic annotations in-
cluding implicit arguments. In the actual linking
task (“NI-only”), labels are provided for local argu-
ments and participating systems have to perform the
following three sub-tasks: (1) identify implicit argu-
ments (IA), (2) predict whether each IA is resolvable
and, if so, (3) find an appropriate antecedent.
The task organizers provide two versions of their
data sets: one based on FrameNet annotations and
one based on PropBank/NomBank annotations. We
found that the latter, however, only contains a sub-
set of the implicit argument annotations from the
FrameNet-based version. As all previous results in
this task have been reported on the FrameNet data
set, we adopt the same setting. Note that our addi-
tional training data is automatically labeled with a
PropBank/NomBank-style parser. That is, we need
to map our annotations to FrameNet. The organizers
of the SemEval shared task provide a manual map-
ping dictionary for predicates in the annotated data
set. We make use of this manual mapping and ad-
ditionally use SemLink 1.14 for mapping predicates
and arguments not in the dictionary.
</bodyText>
<subsectionHeader confidence="0.998368">
4.2 Model details
</subsectionHeader>
<bodyText confidence="0.99999728">
We make use of the system by S&amp;F to train a new
model for the NI-only task. As mentioned in the pre-
vious sub-section, this task consists of three steps:
In step (1), implicit arguments are identified as un-
filled FrameNet core roles that are not competing
with roles that are already filled; in step (2), a SVM
classifier is used to predict whether implicit argu-
ments are resolvable based on a small amount of
features – semantic type of the affected Frame Ele-
ment, the relative frequency of its realization type in
the SemEval training corpus, and a boolean feature
that indicates whether the affected sentence is in pas-
sive voice and does not contain a (deep) subject. In
step (3), we apply the same features and classifier as
S&amp;F, i.e., the BayesNet implementation from Weka
(Witten and Frank, 2005), to find appropriate an-
tecedents for (predicted) resolvable arguments. S&amp;F
report that their best results were obtained when
considering all entities as candidate antecedents that
are syntactic constituents from the present and the
past two sentences, or entities that occurred at least
five times in the previous discourse (“Chains+Win”
setting). In their evaluation, the latter of these two
restrictions crucially depended on gold coreference
chains. As the automatic coreference chains in our
</bodyText>
<footnote confidence="0.970018">
4http://verbs.colorado.edu/semlink/
</footnote>
<page confidence="0.995237">
309
</page>
<bodyText confidence="0.891142">
Sentence that comprises a PAS with an (incorrectly predicted) implicit argument induced antecedent
</bodyText>
<listItem confidence="0.995669333333333">
(1) .. [Statistics*] released [Tuesday TMP] [0A0] showed the death toll dropped ...
(2) A [French LOC*] [OA0] draft resolution ... demands full ... compliance ...
(3) An earthquake ... is capable of causing.. [heavy EXT] damage [OA2*]
</listItem>
<bodyText confidence="0.727242">
official statistics
France
major
</bodyText>
<tableCaption confidence="0.594404">
Table 2: Examples of erroneous pairs of implicit arguments and antecedents. In (1), the parser did not recognize
“Statistics” as an argument of showed; in (2), the parser mislabeled “French” as a locative modifier; both errors lead
to incorrectly identified implicit arguments. In (3), the implicit argument is correct but the wrong antecedent was
identified because “major” had been mislabeled in the aligned predicate-argument structure
</tableCaption>
<bodyText confidence="0.999973947368421">
data are rather sparse (and noisy), we only consider
syntactic constituents from the present and the past
two sentences as antecedents (“SentWin” setting).
Before training and testing a new model with
our own data, we perform feature selection us-
ing 10-fold cross validation. We run the feature
selection on a combination of the SemEval train-
ing data and our additional data set in order to
find a set of features that generalizes best across
the two different corpora. We found these to be
features regarding “prominence”, selectional pref-
erences (“sp supersense”), the POS tags of entity
mentions, and semantic types of argument positions
(“semType dni.entity”). Note that the S&amp;F system
does not make use of any lexicalized information.
Instead, semantic features are computed based on
the highest abstraction level in WordNet (Fellbaum,
1998). For detailed description of all features, see
Silberer and Frank (2012).
</bodyText>
<subsectionHeader confidence="0.885366">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999946">
For direct comparison in the full task, both with
S&amp;F’s model and other previously published results,
we adopt the precision, recall and F1 measures as
defined in Ruppenhofer et al. (2010). We compare
our results with those previously reported on the Se-
mEval task (see Table 3 for a summary): Chen et
al. (2010) adapted SEMAFOR, the best performing
system that participated in the actual task in 2010.
Tonelli and Delmonte (2011) presented a revised
version of their SemEval system (Tonelli and Del-
monte, 2010), which outperformed SEMAFOR in
terms of recall (6%) and F1 score (8%). The best
results in terms of recall and F1 score up to date
have been reported by Laparra and Rigau (2012),
with 25% and 19%, respectively. Our model outper-
forms their state-of-the-art system in terms of preci-
sion (21%) but at a higher cost of recall (8%). Two
</bodyText>
<table confidence="0.999799111111111">
P R F
Chen et al. (2010)5 0.25 0.01 0.02
Tonelli and Delmonte (2011) 0.13 0.06 0.08
Laparra and Rigau (2012) 0.15 0.25 0.19
Laparra and Rigau (2013) 0.14 0.18 0.16
Gorinski et al. (2013)6 0.14 0.12 0.13
S&amp;F (no additional data) 0.06 0.09 0.07
S&amp;F (best additional data) 0.09 0.11 0.10
This paper 0.21 0.08 0.12
</table>
<tableCaption confidence="0.702085666666667">
Table 3: Results in terms of precision (P), recall (R) and
Fl score (F) for identifying and linking implicit argu-
ments in the SemEval test set.
</tableCaption>
<bodyText confidence="0.990691304347826">
influencing factors for their high recall are probably
(1) their improved method for identifying (resolv-
able) implicit arguments, and (2) their addition of
lexicalized and ontological features.
Comparison to the original results reported by
S&amp;F, whose system we use, shows that our addi-
tional data improves precision (from 6% to 21%)
and F1 score (from 7% to 12%). The loss in recall
is marginal (-1%) given the size of the test set (259
resolvable cases in total). The result in precision is
the second highest score reported on this task. Inter-
estingly, the improvements are higher than those of
the best training set used in the original study by Sil-
berer and Frank (2012), even though their additional
data set is three times bigger than ours and is based
on manual semantic annotations. We conjecture that
their low gain in precision could be a side effect trig-
gered by two factors: on the one hand, their model
crucially relies on coreference chains, which are au-
tomatically generated for the test set and hence are
rather noisy. On the other hand, their heuristically
created training data might not represent implicit ar-
gument instances adequately.
</bodyText>
<page confidence="0.996191">
310
</page>
<sectionHeader confidence="0.90198" genericHeader="method">
5 Experiment 2: Implicit arguments in
coherence modeling
</sectionHeader>
<bodyText confidence="0.999948642857143">
In our second experiment, we examine the effect of
implicit arguments on local coherence, i.e., the ques-
tion of how well a local argument (non-)realization
fits into a given context. We approach this question
as follows: first, we assemble a data set of document
pairs that differ only with respect to a single realiza-
tion decision (Section 5.1). Given each pair in this
data set, we ask human annotators to indicate their
preference for the implicit or explicit argument re-
alization in the pre-specified context (Section 5.2).
Second, we attempt to emulate the decision pro-
cess computationally using a discriminative model
based on discourse and entity-specific features (Sec-
tion 5.3).
</bodyText>
<subsectionHeader confidence="0.969015">
5.1 Data compilation
</subsectionHeader>
<bodyText confidence="0.923828583333333">
We use the induced data set (henceforth source
data), as described in Section 3, as a starting point
for composing a set of document pairs that involve
implicit and explicit arguments. To make sure that
each document pair in this data set only differs with
respect to a single realization decision, we first cre-
ate two copies of each document from the source
data: one copy remains in its original form, and the
other copy will be modified with respect to a sin-
gle argument realization. Example (2) illustrates an
example of an original and modified (marked by an
asterik) sentence:
</bodyText>
<listItem confidence="0.813074666666667">
(2) [The Dalai Lama’sA0] visit [to FranceA1] ends
on Tuesday.
* [The Dalai Lama’sA0] visit ends on Tuesday.
</listItem>
<bodyText confidence="0.999805">
Note that adding and removing arguments at ran-
dom can lead to structures that are semantically
implausible. Hence, we restrict this procedure to
predicate-argument structures (PAS) that actually
occur and are aligned across two texts, and create
modifications by replacing a single argument posi-
tion in one text with the corresponding argument po-
sition in the comparable text. Examples (2) and (3)
</bodyText>
<footnote confidence="0.82544575">
5Results as reported in Tonelli and Delmonte (2011)
6Results computed as an average over the scores given for
both test files; rounded towards the number given for the test
file that contained more instances.
</footnote>
<bodyText confidence="0.999977333333333">
show two such comparable texts. The original PAS
in Example (2) contains an explicit argument that is
implicit in the aligned PAS and hence removed in
the modified version. Vice versa, the original text
in (3) involves an implicit argument, which is made
explicit in the modified version.
</bodyText>
<listItem confidence="0.85268">
(3) [The Dalai Lama’sA0] visit coincides with the
Beijing Olympics.
* [The Dalai Lama’sA0] visit [to FranceA1] co-
</listItem>
<bodyText confidence="0.989027615384615">
incides with the Beijing Olympics.
We ensure that the modified structure fits into
the given context grammatically by only consid-
ering PAS with identical predicate form and con-
stituent order. We found that this restriction con-
strains affected arguments to be modifiers, prepo-
sitional phrases and direct objects. We argue that
this is actually a desirable property because more
complicated alternations could affect coherence by
themselves; resulting interplays would make it diffi-
cult to distinguish between the isolated effect of ar-
gument realization itself and other effects, triggered
for example by sentence order (Gordon et al., 1993).
</bodyText>
<subsectionHeader confidence="0.995442">
5.2 Annotation
</subsectionHeader>
<bodyText confidence="0.99999185">
We set up a web experiment using the NLTK pack-
age (Belz and Kow, 2011) to collect (local) coher-
ence ratings for implicit and explicit arguments. For
this experiment, we compiled a data set of 150 doc-
ument pairs. As described in Section 5.1, each text
pair consists of mostly the same text, with the only
difference being one argument realization.
We presented all 150 pairs to two annotators7 and
asked them to indicate their preference for one al-
ternative over the other using a continuous slider
scale. The annotators got to see the full texts, with
the alternatives presented next to each other. To
make texts easier to read and differences easier to
spot, we collapsed all identical sentences into one
column and highlighted the aligned predicate (in
both texts) and the affected argument (in the explicit
case). An example is shown in Figure 2. To avoid
any bias in the annotation process, we shuffled the
sequence of text pairs and randomly assigned the
side of display (left/right) of each realization type
</bodyText>
<footnote confidence="0.956805">
7Both annotators are undergraduate students in Computa-
tional Linguistics.
</footnote>
<page confidence="0.997665">
311
</page>
<figureCaption confidence="0.99981">
Figure 2: Texts as displayed to the annotators.
</figureCaption>
<bodyText confidence="0.999840863636364">
(explicit/implicit). Note that instead of providing a
definition of local coherence ourselves, we simply
asked the annotators to rate how “natural” a realiza-
tion sounds given the discourse context.
We found that annotators made use of the full rat-
ing scale, which spans from -50 to +50, with the ex-
tremes indicating either a strong preference for the
text on the left hand side or the right hand side, re-
spectively. Most ratings are, however, concentrated
more towards the center of the scale (i.e., around
zero). This seems to imply that the use of im-
plicit or explicit arguments did not make a consid-
erable difference most of the time. The first author
confirmed this assumption and resolved disagree-
ments between annotators in several group discus-
sions. The annotators also affirmed that some cases
do not read naturally when a specific argument is or
is not realized at a given position in discourse. Ex-
amples (4) and (5) illustrate two cases, in which a
redundant argument is realized (A4, or destination)
or a coherence establishing argument has been omit-
ted (A2, or co-signer).8
</bodyText>
<listItem confidence="0.999026571428572">
(4) ? The remaining contraband was picked up at
Le Havre. The containers had arrived [in
Le Havre] from China.
(5) ? Lt.-Gen. Mohamed Lamari (... ) denied
his country wanted South African weapons
to fight Muslim rebels fighting the govern-
ment. “We are not going to fight a flea with
</listItem>
<bodyText confidence="0.949047153846154">
8Note that both examples are only excerpts from the affected
texts. The annotators got to see the full context.
a hammer,” Lamari told reporters after sign-
ing the agreement of intent [0].
Following discussions with the annotators, we
discarded all items from the final data set, for which
no clear preference could be established (72%) or
the annotators had different preferences (9%). We
mapped all remaining items into two classes accord-
ing to whether the affected argument had to be im-
plicit (9 texts) or explicit (20 texts). All 29 uniquely
classified texts are used as a small gold standard test
set for evaluation.
</bodyText>
<subsectionHeader confidence="0.990676">
5.3 Coherence model
</subsectionHeader>
<bodyText confidence="0.995625636363636">
We model the decision process that underlies the
(non-)realization of arguments using a SVM classi-
fier and a range of discourse features. The features
can be classified into three groups: features specific
to the affected predicate-argument structure (Parg),
the (automatic) coreference chain of the affected ar-
gument (Coref), and the discourse context (Disc).
Parg includes the absolute and relative number of
realized arguments; the number of modifiers in the
PAS; and the total length (in words) of the PAS and
the complete sentence.
Coref includes the number of previous/follow-up
mentions in a fixed sentence window; the distance
(in number of words/sentences) to the previous/next
mention; the distribution of occurrences over the
previous/succeeding two sentences;9 and the POS of
previous/follow-up mentions.
Disc includes the total number of coreference
chains in the text; the occurrence of pronouns
in the current sentence; lexical repetitions in the
previous/follow-up sentence; the current position in
discourse (begin, middle, end); and a feature indi-
cating whether the affected argument occured in the
first sentence.
Note that most of these features overlap with
those successfully applied in previous work. For
example, Pitler and Nenkova (2008) also use text
9This type of feature is very similar to the transition pat-
terns in the original entity grid. The only difference is that our
features are not typed with respect to the grammatical function
of explicit realizations. The reason for skipping this informa-
tion lies in the insignificant amount of relevant samples in our
(noisy) training data.
</bodyText>
<page confidence="0.997318">
312
</page>
<bodyText confidence="0.999895333333333">
length, sentence-to-sentence transitions, word over-
lap and pronoun occurrences as features for predict-
ing readability. Our own contribution lies in the defi-
nition of PAS-specific features and the adaptation of
all features to the task of predicting (non-)realization
of arguments in a predicate-argument structure.
</bodyText>
<subsectionHeader confidence="0.996931">
5.4 Training data
</subsectionHeader>
<bodyText confidence="0.999975333333333">
We do not make use of any manually annotated data
for training. Instead, our model relies solely on the
automatically induced source data, described in Sec-
tion 3, for learning. We prepare this data set as fol-
lows: first, we remove all data points that also occur
in the test set. Second, we split all pairs of texts into
two groups – texts that contain a predicate-argument
structure in which an implicit argument has been
identified (IA), and their comparable counterparts
that contain the aligned PAS with an explicit argu-
ment (EA). All texts are labelled according to their
group. For all texts in group EA, we remove the ex-
plicit argument from the aligned PAS. This way, the
feature extractor always gets to see the text and au-
tomatic annotations as if the realization decision had
not been performed and can thus extract unbiased
feature values for the affected entity and argument
position.
</bodyText>
<subsectionHeader confidence="0.979859">
5.5 Evaluation setting
</subsectionHeader>
<bodyText confidence="0.999976543478261">
The goal of this task is to correctly predict the re-
alization type (implicit or explicit) of an argument
that maximizes the coherence of the document. As
a proxy for coherence, we use the naturalness rat-
ings given by our annotators. We evaluate classifica-
tion performance on the part of our test set for which
clear preferences have been established. We report
results in terms of precision, recall and Fl score. We
compute precision as the fraction of correct classifier
decisions divided by the total number of classifica-
tions; and recall as the fraction of correct classifier
decisions divided by the total number of test items.
Note that precision and recall are identical when the
model provides a class label for every test item. We
compute Fl as the harmonic mean between precision
and recall.
For comparison with previous work, we further
apply a couple of previously proposed local co-
herence models: the original entity grid model by
Barzilay and Lapata (2005), a modified version that
uses topic models (Elsner and Charniak, 2011a) and
an extended version that includes entity-specific fea-
tures (Elsner and Charniak, 2011b). We further ap-
ply the discourse-new model by Elsner and Charniak
(2008) and the pronoun-based model by Charniak
and Elsner (2009). For all of the aforementioned
models, we use their respective implementation pro-
vided with the Brown Coherence Toolkit10. Note
that the toolkit only returns one coherence score for
each document. To use the toolkit for argument clas-
sification, we use two documents per data point –
one that contains the affected argument explicitly
and one that does not (implicit argument) – and treat
the higher scoring variant as classification output. If
both documents achieve the same score, we neither
count the test item as correctly nor as incorrectly
classified. In contrast, we apply our own model only
on the document that contains the implicit argument,
and use the classifier to predict whether this realiza-
tion type fits into the given context or not. Note that
our model has an advantage here because it is specif-
ically designed for this task. Yet, all models com-
pute local coherence ratings based on entity occur-
rences and should thus be able to predict which re-
alization type coheres best with the given discourse
context.11
</bodyText>
<subsectionHeader confidence="0.544398">
5.6 Results
</subsectionHeader>
<bodyText confidence="0.999850066666667">
The results are summarized in Table 4. As all mod-
els provided class labels for almost all test instances,
we focus our discussion on Fl scores. The majority
class in our test set is the explicit realization type,
making up 20 of the 29 test items (69%).
The original entity grid model produced differing
scores for the two realization types only in 26 cases.
The model exhibits a strong preference for the im-
plicit realization type: it predicts this class in 22
cases, resulting in an Fl score of only 15%. Tak-
ing a closer look at the features of the model reveals
that this an expected outcome: in its original set-
ting, the entity grid learns realization patterns in the
form of sentence-to-sentence transitions. Most enti-
ties are, however, only mentioned a few times in a
</bodyText>
<footnote confidence="0.8460472">
10cf. http://www.ling.ohio-state.edu/%7Emelsner/
11Recall that input document pairs are identical except for the
affected argument position. Consequently, the resulting coher-
ence scores only differ with respect to affected entity realiza-
tions.
</footnote>
<page confidence="0.995112">
313
</page>
<table confidence="0.999962">
P R F
Entity grid models – – –
Baseline entity grid 0.15** 0.14** 0.15**
Extended entity grid 0.19** 0.17** 0.18**
Topical entity grid 0.34** 0.34** 0.34**
Other models – – –
Pronouns 0.43** 0.34** 0.38**
Discourse-newness 0.48** 0.48** 0.48**
This paper – – –
Our (full) model 0.90 0.90 0.90
Simplified model 0.83 0.83 0.83
Majority class 0.69* 0.69* 0.69*
</table>
<tableCaption confidence="0.96332125">
Table 4: Results in terms of precision (P), recall (R) and
Fl score for correctly predicting argument realization; re-
sults that significantly differ from our (full) model are
marked with asterisks (* p&lt;0.1; ** p&lt;0.01)
</tableCaption>
<bodyText confidence="0.999955676470588">
text, which means that non-realizations make up the
‘most probable’ class – independently of whether
they are relevant in a given context or not. The mod-
els by Charniak and Elsner (2009) and Elsner and
Charniak (2011a), which are not based on an entity
grid, do not suffer from this effect and achieve bet-
ter results, with Fl scores of 38% and 48%, respec-
tively. The topical and entity-specific refinements to
the entity grid model also alleviate the bias towards
non-realizations, resulting in improved Fl scores of
18% and 34%, respectively.
To counter-balance this issue altogether, we train
a simplified version of our own model that only
uses features that involve occurrence patterns. The
main difference between this simplified model and
the original entity grid model lies in the different
use of training data: while entity grid models treat
all non-realized items equally, our model gets to
“see” actual examples of entities that are implicit.
In other words, our simplified model takes into ac-
count implicit mentions of entities, not only explicit
ones. The results show that this extra information
has a significant (p&lt;0.01, using a randomization test
(Yeh, 2000)) impact on test set performance, basi-
cally raising Fl from 15% to 83%. Using all features
of our model further increases Fl score to 90%, the
highest score achieved overall.
The highest weighted features in our model in-
clude all three feature groups: for example, the
number of coreferent mentions within the preceed-
ing/following two sentences (Coref), the number
of words already realized in the affected predicate-
argument structure (Parg), and the total number of
coreference chains in the document (Disc).
</bodyText>
<sectionHeader confidence="0.996307" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99996875">
In this paper, we presented a novel approach to ac-
curately induce implicit arguments and discourse an-
tecedents from comparable texts (cf. Section 3). We
demonstrated the benefit of this kind of data for link-
ing implicit arguments and modeling local coher-
ence. Our experiments revealed three particularly
interesting results.
Firstly, a small data set of (automatically induced)
implicit arguments can have a greater impact on ar-
gument linking models than a bigger data set of ar-
tificially created instances (cf. Section 4). Secondly,
the use of implicit vs. explicit arguments, while be-
ing a subtle difference in most contexts, can have a
clear impact on text ratings. Thirdly, our automat-
ically created training data enables models to learn
features that considerably improve prediction of lo-
cally coherent argument realizations (cf. Section 5).
For the task of implicit argument linking, more
training data will be needed to further advance
the state-of-the-art. Our method for inducing
this kind of data, by exploiting aligned predicate-
argument structures from comparable texts, has
shown promising results. Future work will have
to explore this direction more fully, for example,
by identifying ways to induce data with higher re-
call. Integrating argument (non-)realization into a
full model of local coherence also remains part of
future work. In this paper, we presented a suitable
basis for such work: a training set that contains em-
pirical data on implicit arguments in discourse; and
a feature set that models argument realization with
high accuracy.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998648">
We are grateful to the Landesgraduiertenf¨orderung
Baden-W¨urttemberg for funding within the research
initiative “Coherence in language processing” at
Heidelberg University. We thank our annotators and
four anonymous reviewers.
</bodyText>
<page confidence="0.998448">
314
</page>
<sectionHeader confidence="0.995778" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999499716981132">
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor, Michi-
gan, USA, 25–30 June 2005, pages 141–148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Anja Belz and Eric Kow. 2011. Discrete vs. continuous
rating scales for language evaluation in nlp. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230–235, Portland, Oregon, USA,
June.
Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33–36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89–97, Beijing, China,
August.
Cheryl Brown. 1983. Topic continuity in written english
narrative. In Talmy Givon, editor, Topic Continuity
in Discourse: A Quantitative Cross-Language Study.
John Benjamins, Amsterdam, The Netherlands.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148–156, Athens, Greece,
March.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 264–267, Uppsala, Sweden, July.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings ofACL-
08: HLT, Short Papers, pages 41–44, Columbus, Ohio,
June.
Micha Elsner and Eugene Charniak. 2011a. Disentan-
gling chat with local coherence models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1179–1189, Portland, Oregon, USA,
June.
Micha Elsner and Eugene Charniak. 2011b. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 125–129, Portland, Oregon, USA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Katja Filippova and Michael Strube. 2007. Extending
the entity-grid coherence model to semantically re-
lated entities. In Proceedings of the 11th European
Workshop on Natural Language Generation, Schloss
Dagstuhl, Germany, 17–20 June 2007, pages 139–142.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the twelfth annual meet-
ing of the Berkeley Linguistics Society, pages 95–107.
Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Predi-
cates. Computational Linguistics, 38(4):755–798.
Peter C. Gordon, Barbara J. Grosz, and Laura A. Gilliom.
1993. Pronouns, names, and the centering of attention
in discourse. Cognitive Science, 17:311–347.
Philip Gorinski, Josef Ruppenhofer, and Caroline
Sporleder. 2013. Towards weakly supervised resolu-
tion of null instantiations. In Proceedings of the 10th
International Conference on Computational Semantics
(IWCS 2013) – Long Papers, pages 119–130, Potsdam,
Germany, March.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Aravind K. Joshi and Steve Kuhn. 1979. Centered logic:
The role of entity centered sentence representation in
natural language inferencing. In Proceedings of the
6th International Joint Conference on Artificial Intel-
ligence, Tokyo, Japan, August, pages 435–439.
Egoitz Laparra and German Rigau. 2012. Exploiting ex-
plicit annotations and semantic types for implicit argu-
ment resolution. In Proceedings of the Sixth IEEE In-
ternational Conference on Semantic Computing (ICSC
2010), pages 75–78, Palermo, Italy, September. IEEE
Computer Society.
Egoitz Laparra and German Rigau. 2013. Sources of ev-
idence for implicit argument resolution. In Proceed-
ings of the 10th International Conference on Compu-
tational Semantics (IWCS 2013) – Long Papers, pages
155–166, Potsdam, Germany, March.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500, Jeju Island, Korea, July.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
</reference>
<page confidence="0.989586">
315
</page>
<reference confidence="0.999261084905661">
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4). Accepted for publication.
Annie Louis and Ani Nenkova. 2010. Creating local
coherence: An empirical assessment. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 313–316, Los An-
geles, California, June.
Sebastian Martschat, Jie Cai, Samuel Broscheit, ´Eva
M´ujdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 100–106, Jeju Island, Korea, July.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing, Singapore, 2–7 Au-
gust 2009, pages 217–225.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Tatjana Moor, Michael Roth, and Anette Frank. 2013.
Predicate-specific annotations for implicit role bind-
ing: Corpus annotation, data analysis and evaluation
experiments. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS 2013)
– Short Papers, pages 369–375, Potsdam, Germany,
March.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and John
Dowding. 1986. Recovering implicit information. In
Proceedings of the 24th Annual Meeting of the Associ-
ation for Computational Linguistics, New York, N.Y.,
10–13 June 1986, pages 10–19.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Synthesis Lectures on Human Language Technolo-
gies. Morgan &amp; Claypool.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186–195, Honolulu, Hawaii, October.
Michael Roth and Anette Frank. 2012a. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, pages 218–227, Montreal, Canada,
June.
Michael Roth and Anette Frank. 2012b. Aligning
predicates across monolingual comparable texts us-
ing graph-based clustering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 171–182, Jeju Island, Ko-
rea, July.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45–50, Up-
psala, Sweden, July.
Ivan A. Sag and Jorge Hankamer. 1984. Towards a The-
ory of Anaphoric Processing. Linguistics and Philos-
ophy, 7:325–345.
Candace L. Sidner. 1979. Towards a computational the-
ory of definite anaphora comprehension in English.
Technical Report AI-Memo 537, Massachusetts Insti-
tute of Technology, AI Lab, Cambridge, Mass.
Carina Silberer and Anette Frank. 2012. Casting implicit
role linking as an anaphora resolution task. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012), pages 1–10,
Montr´eal, Canada, 7-8 June.
Michael K. Tanenhaus and Greg N. Carlson. 1990. Com-
prehension of Deep and Surface Verbphrase Anaphors.
Language and Cognitive Processes, 5(4):257–280.
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 296–299, Uppsala, Sweden, July.
Sara Tonelli and Rodolfo Delmonte. 2011. Desperately
seeking implicit arguments in text. In Proceedings of
the ACL 2011 Workshop on Relational Models of Se-
mantics, pages 54–62, Portland, Oregon, USA, June.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, California, USA, 2nd
edition.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics, pages 947–953, Saarbr¨ucken, Ger-
many, August.
</reference>
<page confidence="0.999271">
316
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958454">
<title confidence="0.997379">Automatically Identifying Implicit Arguments Improve Argument Linking and Coherence Modeling</title>
<author confidence="0.980481">Roth</author>
<affiliation confidence="0.999919">Department of Computational Heidelberg University,</affiliation>
<abstract confidence="0.999084904761905">Implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing. One reason for this lies in the scarce amount of annotated data sets available. We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and to enable more in-depth studies of the phenomenon itself. In this paper, we present a range of studies that empirically validate this claim. Our contributions are threefold: we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="4715" citStr="Barzilay and Lapata (2005)" startWordPosition="724" endWordPosition="727">the inherent difficulty of inferring discourse antecedents automatically. In this paper, we propose to induce implicit arguments and discourse antecedents by exploiting complementary (explicit) information obtained from monolingual comparable texts (Section 3). We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun m</context>
<context position="30498" citStr="Barzilay and Lapata (2005)" startWordPosition="4848" endWordPosition="4851">lished. We report results in terms of precision, recall and Fl score. We compute precision as the fraction of correct classifier decisions divided by the total number of classifications; and recall as the fraction of correct classifier decisions divided by the total number of test items. Note that precision and recall are identical when the model provides a class label for every test item. We compute Fl as the harmonic mean between precision and recall. For comparison with previous work, we further apply a couple of previously proposed local coherence models: the original entity grid model by Barzilay and Lapata (2005), a modified version that uses topic models (Elsner and Charniak, 2011a) and an extended version that includes entity-specific features (Elsner and Charniak, 2011b). We further apply the discourse-new model by Elsner and Charniak (2008) and the pronoun-based model by Charniak and Elsner (2009). For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit10. Note that the toolkit only returns one coherence score for each document. To use the toolkit for argument classification, we use two documents per data point – one that contains the </context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan, USA, 25–30 June 2005, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4884" citStr="Barzilay and Lapata, 2008" startWordPosition="749" endWordPosition="752">omplementary (explicit) information obtained from monolingual comparable texts (Section 3). We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity m</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>Discrete vs. continuous rating scales for language evaluation in nlp.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>230--235</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="23575" citStr="Belz and Kow, 2011" startWordPosition="3718" endWordPosition="3721">tically by only considering PAS with identical predicate form and constituent order. We found that this restriction constrains affected arguments to be modifiers, prepositional phrases and direct objects. We argue that this is actually a desirable property because more complicated alternations could affect coherence by themselves; resulting interplays would make it difficult to distinguish between the isolated effect of argument realization itself and other effects, triggered for example by sentence order (Gordon et al., 1993). 5.2 Annotation We set up a web experiment using the NLTK package (Belz and Kow, 2011) to collect (local) coherence ratings for implicit and explicit arguments. For this experiment, we compiled a data set of 150 document pairs. As described in Section 5.1, each text pair consists of mostly the same text, with the only difference being one argument realization. We presented all 150 pairs to two annotators7 and asked them to indicate their preference for one alternative over the other using a continuous slider scale. The annotators got to see the full texts, with the alternatives presented next to each other. To make texts easier to read and differences easier to spot, we collaps</context>
</contexts>
<marker>Belz, Kow, 2011</marker>
<rawString>Anja Belz and Eric Kow. 2011. Discrete vs. continuous rating scales for language evaluation in nlp. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 230–235, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Bernd Bohnet</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>A high-performance syntactic and semantic dependency parser.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Demonstration Volume,</booktitle>
<pages>33--36</pages>
<location>Beijing, China,</location>
<marker>Bj¨orkelund, Bohnet, Hafdell, Nugues, 2010</marker>
<rawString>Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and semantic dependency parser. In Coling 2010: Demonstration Volume, pages 33–36, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="9484" citStr="Bohnet, 2010" startWordPosition="1453" endWordPosition="1454">enceforth just R&amp;F data, is a collection of 283,588 predicate pairs that have been aligned “with high precision”2 across comparable newswire articles from the Gigaword corpus (Parker et al., 2011). To use these documents for our argument induction technique, we apply a couple of pre-processing tools on each single document and perform crossdocument entity coreference on pairs of documents. Single document pre-processing. We apply several preprocessing steps to all documents in the R&amp;F data: we use the Stanford CoreNLP package3 for tokenization and sentence splitting. We then apply MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), including the integrated PropBank/NomBank-style semantic parser, to reconstruct local predicate-argument structures for aligned predicates. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012). 2The used method achieved a precision of 86.2% at a recall of 29.1% on the Roth and Frank (2012a) test set. 3http://nlp.stanford.edu/software/ Cross-document coreference. We apply crossdocument coreference resolution to induce antecedents for implicit arguments. In practice, we use the Stanford Coreference System</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheryl Brown</author>
</authors>
<title>Topic continuity in written english narrative.</title>
<date>1983</date>
<booktitle>In Talmy Givon, editor, Topic Continuity in Discourse: A Quantitative Cross-Language Study. John Benjamins,</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="2000" citStr="Brown, 1983" startWordPosition="301" endWordPosition="302">al language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument con</context>
</contexts>
<marker>Brown, 1983</marker>
<rawString>Cheryl Brown. 1983. Topic continuity in written english narrative. In Talmy Givon, editor, Topic Continuity in Discourse: A Quantitative Cross-Language Study. John Benjamins, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>148--156</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="5349" citStr="Charniak and Elsner (2009)" startWordPosition="815" endWordPosition="819"> originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and</context>
<context position="30792" citStr="Charniak and Elsner (2009)" startWordPosition="4893" endWordPosition="4896">that precision and recall are identical when the model provides a class label for every test item. We compute Fl as the harmonic mean between precision and recall. For comparison with previous work, we further apply a couple of previously proposed local coherence models: the original entity grid model by Barzilay and Lapata (2005), a modified version that uses topic models (Elsner and Charniak, 2011a) and an extended version that includes entity-specific features (Elsner and Charniak, 2011b). We further apply the discourse-new model by Elsner and Charniak (2008) and the pronoun-based model by Charniak and Elsner (2009). For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit10. Note that the toolkit only returns one coherence score for each document. To use the toolkit for argument classification, we use two documents per data point – one that contains the affected argument explicitly and one that does not (implicit argument) – and treat the higher scoring variant as classification output. If both documents achieve the same score, we neither count the test item as correctly nor as incorrectly classified. In contrast, we apply our own model only </context>
<context position="33617" citStr="Charniak and Elsner (2009)" startWordPosition="5362" endWordPosition="5365">.34** 0.34** 0.34** Other models – – – Pronouns 0.43** 0.34** 0.38** Discourse-newness 0.48** 0.48** 0.48** This paper – – – Our (full) model 0.90 0.90 0.90 Simplified model 0.83 0.83 0.83 Majority class 0.69* 0.69* 0.69* Table 4: Results in terms of precision (P), recall (R) and Fl score for correctly predicting argument realization; results that significantly differ from our (full) model are marked with asterisks (* p&lt;0.1; ** p&lt;0.01) text, which means that non-realizations make up the ‘most probable’ class – independently of whether they are relevant in a given context or not. The models by Charniak and Elsner (2009) and Elsner and Charniak (2011a), which are not based on an entity grid, do not suffer from this effect and achieve better results, with Fl scores of 38% and 48%, respectively. The topical and entity-specific refinements to the entity grid model also alleviate the bias towards non-realizations, resulting in improved Fl scores of 18% and 34%, respectively. To counter-balance this issue altogether, we train a simplified version of our own model that only uses features that involve occurrence patterns. The main difference between this simplified model and the original entity grid model lies in th</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 148–156, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desai Chen</author>
<author>Nathan Schneider</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>SEMAFOR: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>264--267</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="18236" citStr="Chen et al. (2010)" startWordPosition="2834" endWordPosition="2837">ment positions (“semType dni.entity”). Note that the S&amp;F system does not make use of any lexicalized information. Instead, semantic features are computed based on the highest abstraction level in WordNet (Fellbaum, 1998). For detailed description of all features, see Silberer and Frank (2012). 4.3 Results For direct comparison in the full task, both with S&amp;F’s model and other previously published results, we adopt the precision, recall and F1 measures as defined in Ruppenhofer et al. (2010). We compare our results with those previously reported on the SemEval task (see Table 3 for a summary): Chen et al. (2010) adapted SEMAFOR, the best performing system that participated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of precision (21%) but at a higher cost of recall (8%). Two P R F Chen et al. (2010)5 0.25 0.01 0.02 Tonelli and Delmonte (2011)</context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith. 2010. SEMAFOR: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264–267, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Coreferenceinspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL08: HLT, Short Papers,</booktitle>
<pages>41--44</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5407" citStr="Elsner and Charniak (2008)" startWordPosition="825" endWordPosition="828">but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations </context>
<context position="30734" citStr="Elsner and Charniak (2008)" startWordPosition="4884" endWordPosition="4887">decisions divided by the total number of test items. Note that precision and recall are identical when the model provides a class label for every test item. We compute Fl as the harmonic mean between precision and recall. For comparison with previous work, we further apply a couple of previously proposed local coherence models: the original entity grid model by Barzilay and Lapata (2005), a modified version that uses topic models (Elsner and Charniak, 2011a) and an extended version that includes entity-specific features (Elsner and Charniak, 2011b). We further apply the discourse-new model by Elsner and Charniak (2008) and the pronoun-based model by Charniak and Elsner (2009). For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit10. Note that the toolkit only returns one coherence score for each document. To use the toolkit for argument classification, we use two documents per data point – one that contains the affected argument explicitly and one that does not (implicit argument) – and treat the higher scoring variant as classification output. If both documents achieve the same score, we neither count the test item as correctly nor as incorre</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. Coreferenceinspired coherence modeling. In Proceedings ofACL08: HLT, Short Papers, pages 41–44, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Disentangling chat with local coherence models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1179--1189</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5099" citStr="Elsner and Charniak (2011" startWordPosition="783" endWordPosition="786"> a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mo</context>
<context position="30568" citStr="Elsner and Charniak, 2011" startWordPosition="4859" endWordPosition="4862">e compute precision as the fraction of correct classifier decisions divided by the total number of classifications; and recall as the fraction of correct classifier decisions divided by the total number of test items. Note that precision and recall are identical when the model provides a class label for every test item. We compute Fl as the harmonic mean between precision and recall. For comparison with previous work, we further apply a couple of previously proposed local coherence models: the original entity grid model by Barzilay and Lapata (2005), a modified version that uses topic models (Elsner and Charniak, 2011a) and an extended version that includes entity-specific features (Elsner and Charniak, 2011b). We further apply the discourse-new model by Elsner and Charniak (2008) and the pronoun-based model by Charniak and Elsner (2009). For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit10. Note that the toolkit only returns one coherence score for each document. To use the toolkit for argument classification, we use two documents per data point – one that contains the affected argument explicitly and one that does not (implicit argument)</context>
<context position="33647" citStr="Elsner and Charniak (2011" startWordPosition="5367" endWordPosition="5370">s – – – Pronouns 0.43** 0.34** 0.38** Discourse-newness 0.48** 0.48** 0.48** This paper – – – Our (full) model 0.90 0.90 0.90 Simplified model 0.83 0.83 0.83 Majority class 0.69* 0.69* 0.69* Table 4: Results in terms of precision (P), recall (R) and Fl score for correctly predicting argument realization; results that significantly differ from our (full) model are marked with asterisks (* p&lt;0.1; ** p&lt;0.01) text, which means that non-realizations make up the ‘most probable’ class – independently of whether they are relevant in a given context or not. The models by Charniak and Elsner (2009) and Elsner and Charniak (2011a), which are not based on an entity grid, do not suffer from this effect and achieve better results, with Fl scores of 38% and 48%, respectively. The topical and entity-specific refinements to the entity grid model also alleviate the bias towards non-realizations, resulting in improved Fl scores of 18% and 34%, respectively. To counter-balance this issue altogether, we train a simplified version of our own model that only uses features that involve occurrence patterns. The main difference between this simplified model and the original entity grid model lies in the different use of training da</context>
</contexts>
<marker>Elsner, Charniak, 2011</marker>
<rawString>Micha Elsner and Eugene Charniak. 2011a. Disentangling chat with local coherence models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1179–1189, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Extending the entity grid with entity-specific features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>125--129</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5099" citStr="Elsner and Charniak (2011" startWordPosition="783" endWordPosition="786"> a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mo</context>
<context position="30568" citStr="Elsner and Charniak, 2011" startWordPosition="4859" endWordPosition="4862">e compute precision as the fraction of correct classifier decisions divided by the total number of classifications; and recall as the fraction of correct classifier decisions divided by the total number of test items. Note that precision and recall are identical when the model provides a class label for every test item. We compute Fl as the harmonic mean between precision and recall. For comparison with previous work, we further apply a couple of previously proposed local coherence models: the original entity grid model by Barzilay and Lapata (2005), a modified version that uses topic models (Elsner and Charniak, 2011a) and an extended version that includes entity-specific features (Elsner and Charniak, 2011b). We further apply the discourse-new model by Elsner and Charniak (2008) and the pronoun-based model by Charniak and Elsner (2009). For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit10. Note that the toolkit only returns one coherence score for each document. To use the toolkit for argument classification, we use two documents per data point – one that contains the affected argument explicitly and one that does not (implicit argument)</context>
<context position="33647" citStr="Elsner and Charniak (2011" startWordPosition="5367" endWordPosition="5370">s – – – Pronouns 0.43** 0.34** 0.38** Discourse-newness 0.48** 0.48** 0.48** This paper – – – Our (full) model 0.90 0.90 0.90 Simplified model 0.83 0.83 0.83 Majority class 0.69* 0.69* 0.69* Table 4: Results in terms of precision (P), recall (R) and Fl score for correctly predicting argument realization; results that significantly differ from our (full) model are marked with asterisks (* p&lt;0.1; ** p&lt;0.01) text, which means that non-realizations make up the ‘most probable’ class – independently of whether they are relevant in a given context or not. The models by Charniak and Elsner (2009) and Elsner and Charniak (2011a), which are not based on an entity grid, do not suffer from this effect and achieve better results, with Fl scores of 38% and 48%, respectively. The topical and entity-specific refinements to the entity grid model also alleviate the bias towards non-realizations, resulting in improved Fl scores of 18% and 34%, respectively. To counter-balance this issue altogether, we train a simplified version of our own model that only uses features that involve occurrence patterns. The main difference between this simplified model and the original entity grid model lies in the different use of training da</context>
</contexts>
<marker>Elsner, Charniak, 2011</marker>
<rawString>Micha Elsner and Eugene Charniak. 2011b. Extending the entity grid with entity-specific features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 125–129, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Extending the entity-grid coherence model to semantically related entities.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Workshop on Natural Language Generation, Schloss Dagstuhl,</booktitle>
<pages>139--142</pages>
<contexts>
<context position="5069" citStr="Filippova and Strube (2007)" startWordPosition="778" endWordPosition="781">ng (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 198</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related entities. In Proceedings of the 11th European Workshop on Natural Language Generation, Schloss Dagstuhl, Germany, 17–20 June 2007, pages 139–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Pragmatically controlled zero anaphora.</title>
<date>1986</date>
<booktitle>In Proceedings of the twelfth annual meeting of the Berkeley Linguistics Society,</booktitle>
<pages>95--107</pages>
<contexts>
<context position="1442" citStr="Fillmore, 1986" startWordPosition="210" endWordPosition="211">antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references. 1 Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by</context>
<context position="5649" citStr="Fillmore, 1986" startWordPosition="864" endWordPosition="866">ample, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai</context>
</contexts>
<marker>Fillmore, 1986</marker>
<rawString>C. J. Fillmore. 1986. Pragmatically controlled zero anaphora. In Proceedings of the twelfth annual meeting of the Berkeley Linguistics Society, pages 95–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Chai</author>
</authors>
<title>Semantic Role Labeling of Implicit Arguments for Nominal Predicates.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="5961" citStr="Gerber and Chai (2012)" startWordPosition="914" endWordPosition="917">ner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their e</context>
</contexts>
<marker>Gerber, Chai, 2012</marker>
<rawString>Matthew Gerber and Joyce Chai. 2012. Semantic Role Labeling of Implicit Arguments for Nominal Predicates. Computational Linguistics, 38(4):755–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter C Gordon</author>
<author>Barbara J Grosz</author>
<author>Laura A Gilliom</author>
</authors>
<title>Pronouns, names, and the centering of attention in discourse.</title>
<date>1993</date>
<journal>Cognitive Science,</journal>
<pages>17--311</pages>
<contexts>
<context position="23488" citStr="Gordon et al., 1993" startWordPosition="3701" endWordPosition="3704">ijing Olympics. We ensure that the modified structure fits into the given context grammatically by only considering PAS with identical predicate form and constituent order. We found that this restriction constrains affected arguments to be modifiers, prepositional phrases and direct objects. We argue that this is actually a desirable property because more complicated alternations could affect coherence by themselves; resulting interplays would make it difficult to distinguish between the isolated effect of argument realization itself and other effects, triggered for example by sentence order (Gordon et al., 1993). 5.2 Annotation We set up a web experiment using the NLTK package (Belz and Kow, 2011) to collect (local) coherence ratings for implicit and explicit arguments. For this experiment, we compiled a data set of 150 document pairs. As described in Section 5.1, each text pair consists of mostly the same text, with the only difference being one argument realization. We presented all 150 pairs to two annotators7 and asked them to indicate their preference for one alternative over the other using a continuous slider scale. The annotators got to see the full texts, with the alternatives presented next</context>
</contexts>
<marker>Gordon, Grosz, Gilliom, 1993</marker>
<rawString>Peter C. Gordon, Barbara J. Grosz, and Laura A. Gilliom. 1993. Pronouns, names, and the centering of attention in discourse. Cognitive Science, 17:311–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Gorinski</author>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
</authors>
<title>Towards weakly supervised resolution of null instantiations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>119--130</pages>
<location>Potsdam, Germany,</location>
<contexts>
<context position="18954" citStr="Gorinski et al. (2013)" startWordPosition="2960" endWordPosition="2963">li and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of precision (21%) but at a higher cost of recall (8%). Two P R F Chen et al. (2010)5 0.25 0.01 0.02 Tonelli and Delmonte (2011) 0.13 0.06 0.08 Laparra and Rigau (2012) 0.15 0.25 0.19 Laparra and Rigau (2013) 0.14 0.18 0.16 Gorinski et al. (2013)6 0.14 0.12 0.13 S&amp;F (no additional data) 0.06 0.09 0.07 S&amp;F (best additional data) 0.09 0.11 0.10 This paper 0.21 0.08 0.12 Table 3: Results in terms of precision (P), recall (R) and Fl score (F) for identifying and linking implicit arguments in the SemEval test set. influencing factors for their high recall are probably (1) their improved method for identifying (resolvable) implicit arguments, and (2) their addition of lexicalized and ontological features. Comparison to the original results reported by S&amp;F, whose system we use, shows that our additional data improves precision (from 6% to 21</context>
</contexts>
<marker>Gorinski, Ruppenhofer, Sporleder, 2013</marker>
<rawString>Philip Gorinski, Josef Ruppenhofer, and Caroline Sporleder. 2013. Towards weakly supervised resolution of null instantiations. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 119–130, Potsdam, Germany, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1782" citStr="Grosz et al., 1995" startWordPosition="264" endWordPosition="267">eling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however,</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Steve Kuhn</author>
</authors>
<title>Centered logic: The role of entity centered sentence representation in natural language inferencing.</title>
<date>1979</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>435--439</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="1723" citStr="Joshi and Kuhn, 1979" startWordPosition="254" endWordPosition="257">-explicit entity references. 1 Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational appro</context>
</contexts>
<marker>Joshi, Kuhn, 1979</marker>
<rawString>Aravind K. Joshi and Steve Kuhn. 1979. Centered logic: The role of entity centered sentence representation in natural language inferencing. In Proceedings of the 6th International Joint Conference on Artificial Intelligence, Tokyo, Japan, August, pages 435–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Exploiting explicit annotations and semantic types for implicit argument resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth IEEE International Conference on Semantic Computing (ICSC 2010),</booktitle>
<pages>75--78</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Palermo, Italy,</location>
<contexts>
<context position="18613" citStr="Laparra and Rigau (2012)" startWordPosition="2898" endWordPosition="2901">d other previously published results, we adopt the precision, recall and F1 measures as defined in Ruppenhofer et al. (2010). We compare our results with those previously reported on the SemEval task (see Table 3 for a summary): Chen et al. (2010) adapted SEMAFOR, the best performing system that participated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of precision (21%) but at a higher cost of recall (8%). Two P R F Chen et al. (2010)5 0.25 0.01 0.02 Tonelli and Delmonte (2011) 0.13 0.06 0.08 Laparra and Rigau (2012) 0.15 0.25 0.19 Laparra and Rigau (2013) 0.14 0.18 0.16 Gorinski et al. (2013)6 0.14 0.12 0.13 S&amp;F (no additional data) 0.06 0.09 0.07 S&amp;F (best additional data) 0.09 0.11 0.10 This paper 0.21 0.08 0.12 Table 3: Results in terms of precision (P), recall (R) and Fl score (F) for identifying and linking implicit arguments in the SemEval </context>
</contexts>
<marker>Laparra, Rigau, 2012</marker>
<rawString>Egoitz Laparra and German Rigau. 2012. Exploiting explicit annotations and semantic types for implicit argument resolution. In Proceedings of the Sixth IEEE International Conference on Semantic Computing (ICSC 2010), pages 75–78, Palermo, Italy, September. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Sources of evidence for implicit argument resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>155--166</pages>
<location>Potsdam, Germany,</location>
<contexts>
<context position="18916" citStr="Laparra and Rigau (2013)" startWordPosition="2953" endWordPosition="2956">ipated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of precision (21%) but at a higher cost of recall (8%). Two P R F Chen et al. (2010)5 0.25 0.01 0.02 Tonelli and Delmonte (2011) 0.13 0.06 0.08 Laparra and Rigau (2012) 0.15 0.25 0.19 Laparra and Rigau (2013) 0.14 0.18 0.16 Gorinski et al. (2013)6 0.14 0.12 0.13 S&amp;F (no additional data) 0.06 0.09 0.07 S&amp;F (best additional data) 0.09 0.11 0.10 This paper 0.21 0.08 0.12 Table 3: Results in terms of precision (P), recall (R) and Fl score (F) for identifying and linking implicit arguments in the SemEval test set. influencing factors for their high recall are probably (1) their improved method for identifying (resolvable) implicit arguments, and (2) their addition of lexicalized and ontological features. Comparison to the original results reported by S&amp;F, whose system we use, shows that our additional </context>
</contexts>
<marker>Laparra, Rigau, 2013</marker>
<rawString>Egoitz Laparra and German Rigau. 2013. Sources of evidence for implicit argument resolution. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 155–166, Potsdam, Germany, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>489--500</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="7493" citStr="Lee et al., 2012" startWordPosition="1152" endWordPosition="1155">set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been released by Roth and Frank (2012a).1 This data 1cf. http://www.cl.uni-heidelberg.de/%7Emroth/ 307 Sentence that comprises a PAS with an (correctly pred</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 489–500, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<date>2013</date>
<contexts>
<context position="10103" citStr="Lee et al., 2013" startWordPosition="1539" endWordPosition="1542">Bj¨orkelund et al., 2010), including the integrated PropBank/NomBank-style semantic parser, to reconstruct local predicate-argument structures for aligned predicates. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012). 2The used method achieved a precision of 86.2% at a recall of 29.1% on the Roth and Frank (2012a) test set. 3http://nlp.stanford.edu/software/ Cross-document coreference. We apply crossdocument coreference resolution to induce antecedents for implicit arguments. In practice, we use the Stanford Coreference System (Lee et al., 2013) and run it on pairs of texts by simply providing a single document as input, comprising of a concatenation of the two texts. To perform this step with high precision, we only use the most precise resolution sieves: “String Match”, “Relaxed String Match”, “Precise Constructs”, “Strict Head Match [A-C]”, and “Proper Head Noun Match”. 3.2 Identification and linking approach Given a pair of aligned predicates from two comparable texts, we examine the parser output to identify the arguments in each predicate-argument structure (PAS). We compare the set of realized argument positions in both struct</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.</rawString>
</citation>
<citation valid="false">
<title>Deterministic coreference resolution based on entitycentric, precision-ranked rules.</title>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<note>Accepted for publication.</note>
<marker></marker>
<rawString>Deterministic coreference resolution based on entitycentric, precision-ranked rules. Computational Linguistics, 39(4). Accepted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Creating local coherence: An empirical assessment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>313--316</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="2516" citStr="Louis and Nenkova (2010)" startWordPosition="378" endWordPosition="381">ession and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example (1) shows a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. (1) El Salvador is now the only Latin American country which still has troops in [Iraq]. Nicaragua, Honduras and the Dominican Republic have withdrawn their troops [0]. From a semantic processing perspective, a human reader can easily infer that “Iraq”, the marked entity in the first sentence of Example (1), is also an impl</context>
</contexts>
<marker>Louis, Nenkova, 2010</marker>
<rawString>Annie Louis and Ani Nenkova. 2010. Creating local coherence: An empirical assessment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 313–316, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
<author>Jie Cai</author>
<author>Samuel Broscheit</author>
<author>´Eva M´ujdricza-Maydt</author>
<author>Michael Strube</author>
</authors>
<title>A multigraph model for coreference resolution.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>100--106</pages>
<location>Jeju Island, Korea,</location>
<marker>Martschat, Cai, Broscheit, M´ujdricza-Maydt, Strube, 2012</marker>
<rawString>Sebastian Martschat, Jie Cai, Samuel Broscheit, ´Eva M´ujdricza-Maydt, and Michael Strube. 2012. A multigraph model for coreference resolution. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 100–106, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil McIntyre</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to tell tales: A data-driven approach to story generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing,</booktitle>
<pages>217--225</pages>
<contexts>
<context position="4961" citStr="McIntyre and Lapata, 2009" startWordPosition="760" endWordPosition="763">ts (Section 3). We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit a</context>
</contexts>
<marker>McIntyre, Lapata, 2009</marker>
<rawString>Neil McIntyre and Mirella Lapata. 2009. Learning to tell tales: A data-driven approach to story generation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing, Singapore, 2–7 August 2009, pages 217–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
</authors>
<date>2008</date>
<booktitle>NomBank v1.0. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="6051" citStr="Meyers et al., 2008" startWordPosition="928" endWordPosition="931">s are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but o</context>
</contexts>
<marker>Meyers, Reeves, Macleod, 2008</marker>
<rawString>Adam Meyers, Ruth Reeves, and Catherine Macleod. 2008. NomBank v1.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatjana Moor</author>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Predicate-specific annotations for implicit role binding: Corpus annotation, data analysis and evaluation experiments.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Short Papers,</booktitle>
<pages>369--375</pages>
<location>Potsdam, Germany,</location>
<contexts>
<context position="6074" citStr="Moor et al. (2013)" startWordPosition="933" endWordPosition="936">explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from </context>
</contexts>
<marker>Moor, Roth, Frank, 2013</marker>
<rawString>Tatjana Moor, Michael Roth, and Anette Frank. 2013. Predicate-specific annotations for implicit role binding: Corpus annotation, data analysis and evaluation experiments. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Short Papers, pages 369–375, Potsdam, Germany, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha S Palmer</author>
<author>Deborah A Dahl</author>
<author>Rebecca J Schiffman</author>
<author>Lynette Hirschman</author>
<author>Marcia Linebarger</author>
<author>John Dowding</author>
</authors>
<title>Recovering implicit information.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>10--19</pages>
<location>New York, N.Y., 10–13</location>
<contexts>
<context position="1426" citStr="Palmer et al., 1986" startWordPosition="206" endWordPosition="209"> arguments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references. 1 Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also</context>
<context position="5671" citStr="Palmer et al., 1986" startWordPosition="867" endWordPosition="870"> and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 a</context>
</contexts>
<marker>Palmer, Dahl, Schiffman, Hirschman, Linebarger, Dowding, 1986</marker>
<rawString>Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiffman, Lynette Hirschman, Marcia Linebarger, and John Dowding. 1986. Recovering implicit information. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, New York, N.Y., 10–13 June 1986, pages 10–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Nianwen Xue</author>
</authors>
<title>Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="1310" citStr="Palmer et al., 2010" startWordPosition="189" endWordPosition="192">idate this claim. Our contributions are threefold: we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references. 1 Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntact</context>
</contexts>
<marker>Palmer, Gildea, Xue, 2010</marker>
<rawString>Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Jumbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Fifth Edition. Linguistic Data Consortium,</title>
<date>2011</date>
<location>Philadelphia.</location>
<contexts>
<context position="9068" citStr="Parker et al., 2011" startWordPosition="1385" endWordPosition="1388">sitive examples of automatically induced implicit argument and antecedent pairs. Figure 1: Illustration of the induction approach: texts consist of PAS (represented by overlapping circles); we exploit alignments between corresponding predicates across texts (marked by solid lines) and co-referring entities (marked by dotted lines) to infer implicit arguments (marked by ‘i’) and link antecedents (curly dashed line) set, henceforth just R&amp;F data, is a collection of 283,588 predicate pairs that have been aligned “with high precision”2 across comparable newswire articles from the Gigaword corpus (Parker et al., 2011). To use these documents for our argument induction technique, we apply a couple of pre-processing tools on each single document and perform crossdocument entity coreference on pairs of documents. Single document pre-processing. We apply several preprocessing steps to all documents in the R&amp;F data: we use the Stanford CoreNLP package3 for tokenization and sentence splitting. We then apply MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), including the integrated PropBank/NomBank-style semantic parser, to reconstruct local predicate-argument structures for aligned predicates. Finally, we res</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Jumbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth Edition. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4911" citStr="Pitler and Nenkova, 2008" startWordPosition="753" endWordPosition="756">ormation obtained from monolingual comparable texts (Section 3). We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring refe</context>
<context position="27917" citStr="Pitler and Nenkova (2008)" startWordPosition="4422" endWordPosition="4425">n number of words/sentences) to the previous/next mention; the distribution of occurrences over the previous/succeeding two sentences;9 and the POS of previous/follow-up mentions. Disc includes the total number of coreference chains in the text; the occurrence of pronouns in the current sentence; lexical repetitions in the previous/follow-up sentence; the current position in discourse (begin, middle, end); and a feature indicating whether the affected argument occured in the first sentence. Note that most of these features overlap with those successfully applied in previous work. For example, Pitler and Nenkova (2008) also use text 9This type of feature is very similar to the transition patterns in the original entity grid. The only difference is that our features are not typed with respect to the grammatical function of explicit realizations. The reason for skipping this information lies in the insignificant amount of relevant samples in our (noisy) training data. 312 length, sentence-to-sentence transitions, word overlap and pronoun occurrences as features for predicting readability. Our own contribution lies in the definition of PAS-specific features and the adaptation of all features to the task of pre</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186–195, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>218--227</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="7343" citStr="Roth and Frank, 2012" startWordPosition="1130" endWordPosition="1133"> the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been r</context>
<context position="9865" citStr="Roth and Frank (2012" startWordPosition="1509" endWordPosition="1512">pairs of documents. Single document pre-processing. We apply several preprocessing steps to all documents in the R&amp;F data: we use the Stanford CoreNLP package3 for tokenization and sentence splitting. We then apply MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), including the integrated PropBank/NomBank-style semantic parser, to reconstruct local predicate-argument structures for aligned predicates. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012). 2The used method achieved a precision of 86.2% at a recall of 29.1% on the Roth and Frank (2012a) test set. 3http://nlp.stanford.edu/software/ Cross-document coreference. We apply crossdocument coreference resolution to induce antecedents for implicit arguments. In practice, we use the Stanford Coreference System (Lee et al., 2013) and run it on pairs of texts by simply providing a single document as input, comprising of a concatenation of the two texts. To perform this step with high precision, we only use the most precise resolution sieves: “String Match”, “Relaxed String Match”, “Precise Constructs”, “Strict Head Match [A-C]”, and “Proper Head Noun Match”. 3.2 Identification and link</context>
<context position="12209" citStr="Roth and Frank (2012" startWordPosition="1883" endWordPosition="1886">nt (to circumvent cases, in which an implicit argument is actually explicit but has not been recognized by the parser) 308 3.3 Resulting data set We apply the identification and linking approach to the full R&amp;F data set of aligned predicates. As a result, we induce a total of 701 implicit argument and antecedent pairs, each in a separate document, involving 535 different predicates. Examples are displayed in Table 1. Note that 701 implicit arguments from 283,588 pairs of predicate-argument structures seem to represent a fairly low recall. Most predicate pairs in the high precision data set of Roth and Frank (2012a) do, however, consist of identical argument positions (84.5%). In the remaining cases, in which an implicit argument can be identified (15.5%), an antecedent in discourse cannot always be found using the high precision coreference sieves. This does not mean that implicit arguments are a rare phenomenon in general. In fact, 38.9% of all manually aligned predicate pairs in Roth and Frank (2012a) involved a different number of arguments. We manually evaluated a subset of 90 induced implicit arguments and found 80 discourse antecedents to be correct (89%). Some incorrectly linked instances still</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012a. Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 218–227, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning predicates across monolingual comparable texts using graph-based clustering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>171--182</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="7343" citStr="Roth and Frank, 2012" startWordPosition="1130" endWordPosition="1133"> the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been r</context>
<context position="9865" citStr="Roth and Frank (2012" startWordPosition="1509" endWordPosition="1512">pairs of documents. Single document pre-processing. We apply several preprocessing steps to all documents in the R&amp;F data: we use the Stanford CoreNLP package3 for tokenization and sentence splitting. We then apply MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), including the integrated PropBank/NomBank-style semantic parser, to reconstruct local predicate-argument structures for aligned predicates. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012). 2The used method achieved a precision of 86.2% at a recall of 29.1% on the Roth and Frank (2012a) test set. 3http://nlp.stanford.edu/software/ Cross-document coreference. We apply crossdocument coreference resolution to induce antecedents for implicit arguments. In practice, we use the Stanford Coreference System (Lee et al., 2013) and run it on pairs of texts by simply providing a single document as input, comprising of a concatenation of the two texts. To perform this step with high precision, we only use the most precise resolution sieves: “String Match”, “Relaxed String Match”, “Precise Constructs”, “Strict Head Match [A-C]”, and “Proper Head Noun Match”. 3.2 Identification and link</context>
<context position="12209" citStr="Roth and Frank (2012" startWordPosition="1883" endWordPosition="1886">nt (to circumvent cases, in which an implicit argument is actually explicit but has not been recognized by the parser) 308 3.3 Resulting data set We apply the identification and linking approach to the full R&amp;F data set of aligned predicates. As a result, we induce a total of 701 implicit argument and antecedent pairs, each in a separate document, involving 535 different predicates. Examples are displayed in Table 1. Note that 701 implicit arguments from 283,588 pairs of predicate-argument structures seem to represent a fairly low recall. Most predicate pairs in the high precision data set of Roth and Frank (2012a) do, however, consist of identical argument positions (84.5%). In the remaining cases, in which an implicit argument can be identified (15.5%), an antecedent in discourse cannot always be found using the high precision coreference sieves. This does not mean that implicit arguments are a rare phenomenon in general. In fact, 38.9% of all manually aligned predicate pairs in Roth and Frank (2012a) involved a different number of arguments. We manually evaluated a subset of 90 induced implicit arguments and found 80 discourse antecedents to be correct (89%). Some incorrectly linked instances still</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012b. Aligning predicates across monolingual comparable texts using graph-based clustering. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 171–182, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval2010 Task 10: Linking Events and Their Participants in Discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations,</booktitle>
<pages>45--50</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5855" citStr="Ruppenhofer et al. (2010)" startWordPosition="896" endWordPosition="900">spectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additiona</context>
<context position="13311" citStr="Ruppenhofer et al., 2010" startWordPosition="2057" endWordPosition="2060">induced implicit arguments and found 80 discourse antecedents to be correct (89%). Some incorrectly linked instances still result from preprocessing errors. In Table 2, we present a range of different error types that occurred when extracting implicit arguments without any restrictions. 4 Experiment 1: Linking implicit arguments Our first experiment assesses the utility of automatically induced implicit arguments and antecedent pairs for the task of implicit argument linking. For evaluation, we use the data sets from the SemEval 2010 task on Linking Events and their Participants in Discourse (Ruppenhofer et al., 2010, henceforth just SemEval). For direct comparison with previous results and heuristic acquisition techniques (cf. Section 2), we apply the implicit argument identification and linking model by Silberer and Frank (2012, henceforth S&amp;F) for training and testing. 4.1 Task summary Both the training and test sets of the SemEval task are text corpora extracted from Sherlock Holmes novels, with manual frame semantic annotations including implicit arguments. In the actual linking task (“NI-only”), labels are provided for local arguments and participating systems have to perform the following three sub</context>
<context position="18113" citStr="Ruppenhofer et al. (2010)" startWordPosition="2811" endWordPosition="2814">res regarding “prominence”, selectional preferences (“sp supersense”), the POS tags of entity mentions, and semantic types of argument positions (“semType dni.entity”). Note that the S&amp;F system does not make use of any lexicalized information. Instead, semantic features are computed based on the highest abstraction level in WordNet (Fellbaum, 1998). For detailed description of all features, see Silberer and Frank (2012). 4.3 Results For direct comparison in the full task, both with S&amp;F’s model and other previously published results, we adopt the precision, recall and F1 measures as defined in Ruppenhofer et al. (2010). We compare our results with those previously reported on the SemEval task (see Table 3 for a summary): Chen et al. (2010) adapted SEMAFOR, the best performing system that participated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of pr</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. SemEval2010 Task 10: Linking Events and Their Participants in Discourse. In Proceedings of the 5th International Workshop on Semantic Evaluations, pages 45–50, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Jorge Hankamer</author>
</authors>
<date>1984</date>
<booktitle>Towards a Theory of Anaphoric Processing. Linguistics and Philosophy,</booktitle>
<pages>7--325</pages>
<contexts>
<context position="2217" citStr="Sag and Hankamer, 1984" startWordPosition="331" endWordPosition="334">urse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example (1) shows a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. (1) El Salvador is now the</context>
</contexts>
<marker>Sag, Hankamer, 1984</marker>
<rawString>Ivan A. Sag and Jorge Hankamer. 1984. Towards a Theory of Anaphoric Processing. Linguistics and Philosophy, 7:325–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Towards a computational theory of definite anaphora comprehension in English.</title>
<date>1979</date>
<tech>Technical Report AI-Memo 537,</tech>
<institution>Massachusetts Institute of Technology, AI Lab,</institution>
<location>Cambridge, Mass.</location>
<contexts>
<context position="1687" citStr="Sidner, 1979" startWordPosition="250" endWordPosition="251">s by taking into account non-explicit entity references. 1 Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely</context>
</contexts>
<marker>Sidner, 1979</marker>
<rawString>Candace L. Sidner. 1979. Towards a computational theory of definite anaphora comprehension in English. Technical Report AI-Memo 537, Massachusetts Institute of Technology, AI Lab, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Anette Frank</author>
</authors>
<title>Casting implicit role linking as an anaphora resolution task.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM 2012),</booktitle>
<pages>1--10</pages>
<location>Montr´eal,</location>
<contexts>
<context position="6444" citStr="Silberer and Frank (2012)" startWordPosition="997" endWordPosition="1000">available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable te</context>
<context position="13528" citStr="Silberer and Frank (2012" startWordPosition="2089" endWordPosition="2092"> occurred when extracting implicit arguments without any restrictions. 4 Experiment 1: Linking implicit arguments Our first experiment assesses the utility of automatically induced implicit arguments and antecedent pairs for the task of implicit argument linking. For evaluation, we use the data sets from the SemEval 2010 task on Linking Events and their Participants in Discourse (Ruppenhofer et al., 2010, henceforth just SemEval). For direct comparison with previous results and heuristic acquisition techniques (cf. Section 2), we apply the implicit argument identification and linking model by Silberer and Frank (2012, henceforth S&amp;F) for training and testing. 4.1 Task summary Both the training and test sets of the SemEval task are text corpora extracted from Sherlock Holmes novels, with manual frame semantic annotations including implicit arguments. In the actual linking task (“NI-only”), labels are provided for local arguments and participating systems have to perform the following three sub-tasks: (1) identify implicit arguments (IA), (2) predict whether each IA is resolvable and, if so, (3) find an appropriate antecedent. The task organizers provide two versions of their data sets: one based on FrameNe</context>
<context position="17911" citStr="Silberer and Frank (2012)" startWordPosition="2778" endWordPosition="2781"> selection on a combination of the SemEval training data and our additional data set in order to find a set of features that generalizes best across the two different corpora. We found these to be features regarding “prominence”, selectional preferences (“sp supersense”), the POS tags of entity mentions, and semantic types of argument positions (“semType dni.entity”). Note that the S&amp;F system does not make use of any lexicalized information. Instead, semantic features are computed based on the highest abstraction level in WordNet (Fellbaum, 1998). For detailed description of all features, see Silberer and Frank (2012). 4.3 Results For direct comparison in the full task, both with S&amp;F’s model and other previously published results, we adopt the precision, recall and F1 measures as defined in Ruppenhofer et al. (2010). We compare our results with those previously reported on the SemEval task (see Table 3 for a summary): Chen et al. (2010) adapted SEMAFOR, the best performing system that participated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The </context>
<context position="19898" citStr="Silberer and Frank (2012)" startWordPosition="3121" endWordPosition="3125">ably (1) their improved method for identifying (resolvable) implicit arguments, and (2) their addition of lexicalized and ontological features. Comparison to the original results reported by S&amp;F, whose system we use, shows that our additional data improves precision (from 6% to 21%) and F1 score (from 7% to 12%). The loss in recall is marginal (-1%) given the size of the test set (259 resolvable cases in total). The result in precision is the second highest score reported on this task. Interestingly, the improvements are higher than those of the best training set used in the original study by Silberer and Frank (2012), even though their additional data set is three times bigger than ours and is based on manual semantic annotations. We conjecture that their low gain in precision could be a side effect triggered by two factors: on the one hand, their model crucially relies on coreference chains, which are automatically generated for the test set and hence are rather noisy. On the other hand, their heuristically created training data might not represent implicit argument instances adequately. 310 5 Experiment 2: Implicit arguments in coherence modeling In our second experiment, we examine the effect of implic</context>
</contexts>
<marker>Silberer, Frank, 2012</marker>
<rawString>Carina Silberer and Anette Frank. 2012. Casting implicit role linking as an anaphora resolution task. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM 2012), pages 1–10, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Greg N Carlson</author>
</authors>
<date>1990</date>
<booktitle>Comprehension of Deep and Surface Verbphrase Anaphors. Language and Cognitive Processes,</booktitle>
<pages>5--4</pages>
<contexts>
<context position="2246" citStr="Tanenhaus and Carlson, 1990" startWordPosition="335" endWordPosition="339">uses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz et al., 1995), the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example (1) shows a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. (1) El Salvador is now the only Latin American country </context>
</contexts>
<marker>Tanenhaus, Carlson, 1990</marker>
<rawString>Michael K. Tanenhaus and Greg N. Carlson. 1990. Comprehension of Deep and Surface Verbphrase Anaphors. Language and Cognitive Processes, 5(4):257–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Tonelli</author>
<author>Rodolfo Delmonte</author>
</authors>
<title>VENSES++: Adapting a deep semantic processing system to the identification of null instantiations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>296--299</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="18435" citStr="Tonelli and Delmonte, 2010" startWordPosition="2864" endWordPosition="2868">el in WordNet (Fellbaum, 1998). For detailed description of all features, see Silberer and Frank (2012). 4.3 Results For direct comparison in the full task, both with S&amp;F’s model and other previously published results, we adopt the precision, recall and F1 measures as defined in Ruppenhofer et al. (2010). We compare our results with those previously reported on the SemEval task (see Table 3 for a summary): Chen et al. (2010) adapted SEMAFOR, the best performing system that participated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of precision (21%) but at a higher cost of recall (8%). Two P R F Chen et al. (2010)5 0.25 0.01 0.02 Tonelli and Delmonte (2011) 0.13 0.06 0.08 Laparra and Rigau (2012) 0.15 0.25 0.19 Laparra and Rigau (2013) 0.14 0.18 0.16 Gorinski et al. (2013)6 0.14 0.12 0.13 S&amp;F (no additional data) 0.06 0.09 0.07 S&amp;F (best additional dat</context>
</contexts>
<marker>Tonelli, Delmonte, 2010</marker>
<rawString>Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++: Adapting a deep semantic processing system to the identification of null instantiations. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 296–299, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Tonelli</author>
<author>Rodolfo Delmonte</author>
</authors>
<title>Desperately seeking implicit arguments in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics,</booktitle>
<pages>54--62</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="18354" citStr="Tonelli and Delmonte (2011)" startWordPosition="2852" endWordPosition="2855">ion. Instead, semantic features are computed based on the highest abstraction level in WordNet (Fellbaum, 1998). For detailed description of all features, see Silberer and Frank (2012). 4.3 Results For direct comparison in the full task, both with S&amp;F’s model and other previously published results, we adopt the precision, recall and F1 measures as defined in Ruppenhofer et al. (2010). We compare our results with those previously reported on the SemEval task (see Table 3 for a summary): Chen et al. (2010) adapted SEMAFOR, the best performing system that participated in the actual task in 2010. Tonelli and Delmonte (2011) presented a revised version of their SemEval system (Tonelli and Delmonte, 2010), which outperformed SEMAFOR in terms of recall (6%) and F1 score (8%). The best results in terms of recall and F1 score up to date have been reported by Laparra and Rigau (2012), with 25% and 19%, respectively. Our model outperforms their state-of-the-art system in terms of precision (21%) but at a higher cost of recall (8%). Two P R F Chen et al. (2010)5 0.25 0.01 0.02 Tonelli and Delmonte (2011) 0.13 0.06 0.08 Laparra and Rigau (2012) 0.15 0.25 0.19 Laparra and Rigau (2013) 0.14 0.18 0.16 Gorinski et al. (2013)</context>
<context position="22289" citStr="Tonelli and Delmonte (2011)" startWordPosition="3512" endWordPosition="3515"> example of an original and modified (marked by an asterik) sentence: (2) [The Dalai Lama’sA0] visit [to FranceA1] ends on Tuesday. * [The Dalai Lama’sA0] visit ends on Tuesday. Note that adding and removing arguments at random can lead to structures that are semantically implausible. Hence, we restrict this procedure to predicate-argument structures (PAS) that actually occur and are aligned across two texts, and create modifications by replacing a single argument position in one text with the corresponding argument position in the comparable text. Examples (2) and (3) 5Results as reported in Tonelli and Delmonte (2011) 6Results computed as an average over the scores given for both test files; rounded towards the number given for the test file that contained more instances. show two such comparable texts. The original PAS in Example (2) contains an explicit argument that is implicit in the aligned PAS and hence removed in the modified version. Vice versa, the original text in (3) involves an implicit argument, which is made explicit in the modified version. (3) [The Dalai Lama’sA0] visit coincides with the Beijing Olympics. * [The Dalai Lama’sA0] visit [to FranceA1] coincides with the Beijing Olympics. We en</context>
</contexts>
<marker>Tonelli, Delmonte, 2011</marker>
<rawString>Sara Tonelli and Rodolfo Delmonte. 2011. Desperately seeking implicit arguments in text. In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics, pages 54–62, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Martha Palmer</author>
<author>Mitchell Marcus</author>
<author>Eduard Hovy</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
</authors>
<title>OntoNotes Release 4.0. Linguistic Data Consortium,</title>
<date>2011</date>
<location>Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert</location>
<contexts>
<context position="6154" citStr="Weischedel et al., 2011" startWordPosition="946" endWordPosition="949">errable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold c</context>
</contexts>
<marker>Weischedel, Palmer, Marcus, Hovy, Pradhan, Ramshaw, 2011</marker>
<rawString>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. 2011. OntoNotes Release 4.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, California, USA,</location>
<note>2nd edition.</note>
<contexts>
<context position="15649" citStr="Witten and Frank, 2005" startWordPosition="2440" endWordPosition="2443">uments are identified as unfilled FrameNet core roles that are not competing with roles that are already filled; in step (2), a SVM classifier is used to predict whether implicit arguments are resolvable based on a small amount of features – semantic type of the affected Frame Element, the relative frequency of its realization type in the SemEval training corpus, and a boolean feature that indicates whether the affected sentence is in passive voice and does not contain a (deep) subject. In step (3), we apply the same features and classifier as S&amp;F, i.e., the BayesNet implementation from Weka (Witten and Frank, 2005), to find appropriate antecedents for (predicted) resolvable arguments. S&amp;F report that their best results were obtained when considering all entities as candidate antecedents that are syntactic constituents from the present and the past two sentences, or entities that occurred at least five times in the previous discourse (“Chains+Win” setting). In their evaluation, the latter of these two restrictions crucially depended on gold coreference chains. As the automatic coreference chains in our 4http://verbs.colorado.edu/semlink/ 309 Sentence that comprises a PAS with an (incorrectly predicted) i</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, California, USA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="34606" citStr="Yeh, 2000" startWordPosition="5522" endWordPosition="5523">is issue altogether, we train a simplified version of our own model that only uses features that involve occurrence patterns. The main difference between this simplified model and the original entity grid model lies in the different use of training data: while entity grid models treat all non-realized items equally, our model gets to “see” actual examples of entities that are implicit. In other words, our simplified model takes into account implicit mentions of entities, not only explicit ones. The results show that this extra information has a significant (p&lt;0.01, using a randomization test (Yeh, 2000)) impact on test set performance, basically raising Fl from 15% to 83%. Using all features of our model further increases Fl score to 90%, the highest score achieved overall. The highest weighted features in our model include all three feature groups: for example, the number of coreferent mentions within the preceeding/following two sentences (Coref), the number of words already realized in the affected predicateargument structure (Parg), and the total number of coreference chains in the document (Disc). 6 Conclusions In this paper, we presented a novel approach to accurately induce implicit a</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International Conference on Computational Linguistics, pages 947–953, Saarbr¨ucken, Germany, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>