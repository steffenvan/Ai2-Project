<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000456">
<title confidence="0.641765">
Semi-supervised learning of concatenative morphology
</title>
<author confidence="0.915926">
Oskar Kohonen and Sami Virpioja and Krista Lagus
</author>
<affiliation confidence="0.9674365">
Aalto University School of Science and Technology
Adaptive Informatics Research Centre
</affiliation>
<address confidence="0.923946">
P.O. Box 15400, FI-00076 AALTO, Finland
</address>
<email confidence="0.998735">
{oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi
</email>
<sectionHeader confidence="0.994793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999757315789474">
We consider morphology learning in a
semi-supervised setting, where a small
set of linguistic gold standard analyses is
available. We extend Morfessor Base-
line, which is a method for unsupervised
morphological segmentation, to this task.
We show that known linguistic segmenta-
tions can be exploited by adding them into
the data likelihood function and optimiz-
ing separate weights for unlabeled and la-
beled data. Experiments on English and
Finnish are presented with varying amount
of labeled data. Results of the linguis-
tic evaluation of Morpho Challenge im-
prove rapidly already with small amounts
of labeled data, surpassing the state-of-
the-art unsupervised methods at 1000 la-
beled words for English and at 100 labeled
words for Finnish.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997845">
Morphological analysis is required in many natu-
ral language processing problems. Especially, in
agglutinative and compounding languages, where
each word form consists of a combination of stems
and affixes, the number of unique word forms in
a corpus is very large. This leads to problems in
word-based statistical language modeling: Even
with a large training corpus, many of the words en-
countered when applying the model did not occur
in the training corpus, and thus there is no infor-
mation available on how to process them. Using
morphological units, such as stems and affixes, in-
stead of complete word forms alleviates this prob-
lem. Unfortunately, for many languages morpho-
logical analysis tools either do not exist or they
are not freely available. In many cases, the prob-
lems of availability also apply to morphologically
annotated corpora, making supervised learning in-
feasible.
In consequence, there has been a need for ap-
proaches for morphological processing that would
require little language-dependent resources. Due
to this need, as well as the general interest in
language acquisition and unsupervised language
learning, the research on unsupervised learning
of morphology has been active during the past
ten years. Especially, methods that perform mor-
phological segmentation have been studied exten-
sively (Goldsmith, 2001; Creutz and Lagus, 2002;
Monson et al., 2004; Bernhard, 2006; Dasgupta
and Ng, 2007; Snyder and Barzilay, 2008b; Poon
et al., 2009). These methods have shown to pro-
duce results that improve performance in several
applications, such as speech recognition and in-
formation retrieval (Creutz et al., 2007; Kurimo et
al., 2008).
While unsupervised methods often work quite
well across different languages, it is difficult to
avoid biases toward certain kinds of languages and
analyses. For example, in isolating languages, the
average amount of morphemes per word is low,
whereas in synthetic languages the amount may be
very high. Also, different applications may need
a particular bias, for example, not analyzing fre-
quent compound words as consisting of smaller
parts could be beneficial in information retrieval.
In many cases, even a small amount of labeled data
can be used to adapt a method to a particular lan-
guage and task. Methodologically, this is referred
to as semi-supervised learning.
In semi-supervised learning, the learning sys-
tem has access to both labeled and unlabeled data.
Typically, the labeled data set is too small for su-
pervised methods to be effective, but there is a
large amount of unlabeled data available. There
are many different approaches to this class of
problems, as presented by Zhu (2005). One ap-
proach is to use generative models, which spec-
ify a join distribution over all variables in the
model. They can be utilized both in unsupervised
</bodyText>
<page confidence="0.979029">
78
</page>
<note confidence="0.957422">
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 78–86,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999920565217391">
and supervised learning. In contrast, discrimina-
tive models only specify the conditional distribu-
tion between input data and labels, and therefore
require labeled data. Both, however, can be ex-
tended to the semi-supervised case. For generative
models, it is, in principle, very easy to use both la-
beled and unlabeled data. For unsupervised learn-
ing one can consider the labels as missing data and
estimate their values using the Expectation Maxi-
mization (EM) algorithm (Dempster et al., 1977).
In the semi-supervised case, some labels are avail-
able, and the rest are considered missing and esti-
mated with EM.
In this paper, we extend the Morfessor Base-
line method for the semi-supervised case. Morfes-
sor (Creutz and Lagus, 2002; Creutz and Lagus,
2005; Creutz and Lagus, 2007, etc.) is one of the
well-established methods for morphological seg-
mentation. It applies a simple generative model.
The basic idea, inspired by the Minimum Descrip-
tion Length principle (Rissanen, 1989), is to en-
code the words in the training data with a lexicon
of morphs, that are segments of the words. The
number of bits needed to encode both the morph
lexicon and the data using the lexicon should be
minimized. Morfessor does not limit the num-
ber of morphemes per word form, making it suit-
able for modeling a large variety of agglutinative
languages irrespective of them being more isolat-
ing or synthetic. We show that the model can be
trained in a similar fashion in the semi-supervised
case as in the unsupervised case. However, with
a large set of unlabeled data, the effect of the su-
pervision on the results tends to be small. Thus,
we add a discriminative weighting scheme, where
a small set of word forms with gold standard ana-
lyzes are used for tuning the respective weights of
the labeled and unlabeled data.
The paper is organized as follows: First, we
discuss related work on semi-supervised learning.
Then we describe the Morfessor Baseline model
and the unsupervised algorithm, followed by our
semi-supervised extension. Finally, we present ex-
perimental results for English and Finnish using
the Morpho Challenge data sets (Kurimo et al.,
2009).
</bodyText>
<sectionHeader confidence="0.697947" genericHeader="introduction">
1.1 Related work
</sectionHeader>
<bodyText confidence="0.999851574468085">
There is surprisingly little work that consider im-
proving the unsupervised models of morphology
with small amounts of annotated data. In the
related tasks that deal with sequential labeling
(word segmentation, POS tagging, shallow pars-
ing, named-entity recognition), semi-supervised
learning is more common.
Snyder and Barzilay (2008a; 2008b) consider
learning morphological segmentation with non-
parametric Bayesian model from multilingual
data. For multilingual settings, they extract 6139
parallel short phrases from the Hebrew, Arabic,
Aramaic and English bible. Using the aligned
phrase pairs, the model can learn the segmen-
tations for two languages at the same time. In
one of the papers (2008a), they consider also
semi-supervised scenarios, where annotated data
is available either in only one language or both of
the languages. However, the amount of annotated
data is fixed to the half of the full data. This differs
from our experimental setting, where the amount
of unlabeled data is very large and the amount of
labeled data relatively small.
Poon et al. (2009) apply a log-linear, undi-
rected generative model for learning the morphol-
ogy of Arabic and Hebrew. They report results
for the same small data set as Snyder and Barzilay
(2008a) in both unsupervised and semi-supervised
settings. For the latter, they use somewhat smaller
proportions of annotated data, varying from 25%
to 100% of the total data, but the amount of unla-
beled data is still very small. Results are reported
also for a larger 120 000 word Arabic data set, but
only for unsupervised learning.
A problem similar to morphological segmen-
tation is word segmentation for the languages
where orthography does not specify word bound-
aries. However, the amount of labeled data is
usually large, and unlabeled data is just an addi-
tional source of information. Li and McCallum
(2005) apply a semi-supervised approach to Chi-
nese word segmentation where unlabeled data is
utilized for forming word clusters, which are then
used as features for a supervised classifier. Xu
et al. (2008) adapt a Chinese word segmentation
specifically to a machine translation task, by using
the indirect supervision from a parallel corpus.
</bodyText>
<sectionHeader confidence="0.954149" genericHeader="method">
2 Method
</sectionHeader>
<bodyText confidence="0.9997446">
We present an extension of the Morfessor Baseline
method to the semi-supervised setting. Morfes-
sor Baseline is based on a generative probabilis-
tic model. It is a method for modeling concatena-
tive morphology, where the morphs—i.e., the sur-
</bodyText>
<page confidence="0.997356">
79
</page>
<bodyText confidence="0.997047333333333">
face forms of morphemes—of a word are its non-
overlapping segments. The model parameters 9
encode a morph lexicon, which includes the prop-
erties of the morphs, such as their string represen-
tations. Each morph m in the lexicon has a proba-
bility of occurring in a word, P(M = m |9).1 The
probabilities are assumed to be independent. The
model uses a prior P(9), derived using the Min-
imum Description Length (MDL) principle, that
controls the complexity of the model. Intuitively,
the prior assigns higher probability to models that
store fewer morphs, where a morph is considered
stored if P(M = m |9) &gt; 0. During model learn-
ing, 9 is optimized to maximize the posterior prob-
ability:
</bodyText>
<equation confidence="0.99690825">
9MAP = arg max P(9|DW)
e
= arg max {P(9)P(DW |9)}, (1)
e
</equation>
<bodyText confidence="0.999990375">
where DW includes the words in the training
data. In this section, we first consider sepa-
rately the likelihood P(DW |9) and the prior P(9)
used in Morfessor Baseline. Then we describe
the algorithms, first unsupervised and then semi-
supervised, for finding optimal model parameters.
Last, we shortly discuss the algorithm for seg-
menting new words after the model training.
</bodyText>
<subsectionHeader confidence="0.972143">
2.1 Likelihood
</subsectionHeader>
<bodyText confidence="0.996521636363636">
The latent variable of the model, Z =
(Z1, ... , Z|DW |), contains the analyses of the
words in the training data DW. An instance of
a single analysis for the j:th word is a sequence of
morphs, zj = (mj1, ... , mj|zj|). During training,
each word wj is assumed to have only one possible
analysis. Thus, instead of using the joint distribu-
tion P(DW, Z |9), we need to use the likelihood
function only conditioned on the analyses of the
observed words, P(DW  |Z, 9). The conditional
likelihood is
</bodyText>
<equation confidence="0.999375">
P(DW  |Z = z, 9)
P(W = wj |Z = z, 9)
H |zj |P(M = mji |9), (2)
i=1
</equation>
<bodyText confidence="0.957142">
where mij is the i:th morph in word wj.
</bodyText>
<footnote confidence="0.7625385">
1We denote variables with uppercase letters and their in-
stances with lowercase letters.
</footnote>
<subsectionHeader confidence="0.995435">
2.2 Priors
</subsectionHeader>
<bodyText confidence="0.98719">
Morfessor applies Maximum A Posteriori (MAP)
estimation, so priors for the model parameters
need to be defined. The parameters 9 of the model
are:
</bodyText>
<listItem confidence="0.91325225">
• Morph type count, or the size of the morph
lexicon, µ E Z+
• Morph token count, or the number of morphs
tokens in the observed data, v E Z+
• Morph strings (a1, ... , aµ), ai E E∗
• Morph counts (T1, ... , Tµ), Ti E J1,. . . , v},
Ei Ti = v. Normalized with v, these give
the probabilities of the morphs.
</listItem>
<bodyText confidence="0.996979285714286">
MDL-inspired and non-informative priors have
been preferred. When using such priors, morph
type count and morph token counts can be ne-
glected when optimizing the model. The morph
string prior is based on length distribution P(L)
and distribution P(C) of characters over the char-
acter set E, both assumed to be known:
</bodyText>
<equation confidence="0.995869">
P(ai) = P(L = |ai|) H|σi |P(C = aij) (3)
j=1
</equation>
<bodyText confidence="0.999957">
We use the implicit length prior (Creutz and La-
gus, 2005), which is obtained by removing P(L)
and using end-of-word mark as an additional char-
acter in P(C). For morph counts, the non-
informative prior
</bodyText>
<equation confidence="0.97793025">
1
P(T1, ... , Tµ) v-
- 1/ (4)
µ − 1
</equation>
<bodyText confidence="0.9938948">
gives equal probability to each possible combina-
tion of the counts when µ and v are known, as
there are (ν−1 ) possible ways to choose µ positive
µ−1
integers that sum up to v.
</bodyText>
<subsectionHeader confidence="0.998375">
2.3 Unsupervised learning
</subsectionHeader>
<bodyText confidence="0.999982111111111">
In principle, unsupervised learning can be per-
formed by looking for the MAP estimate with the
EM-algorithm. In the case of Morfessor Baseline,
this is problematic, because the prior only assigns
higher probability to lexicons where fewer morphs
have nonzero probabilities. The EM-algorithm has
the property that it will not assign a zero probabil-
ity to any morph, that has a nonzero likelihood in
the previous step, and this will hold for all morphs
</bodyText>
<equation confidence="0.988412833333333">
|DW |
= H
j=1
|DW |
= H
j=1
</equation>
<page confidence="0.900534">
80
</page>
<bodyText confidence="0.989218705882353">
that initially have a nonzero probability. In con-
sequence, Morfessor Baseline instead uses a local
search algorithm, which will assign zero probabil-
ity to a large part of the potential morphs. This
is memory-efficient, since only the morphs with
nonzero probabilities need to be stored in mem-
ory. The training algorithm of Morfessor Base-
line, described by Creutz and Lagus (2005), tries
to minimize the cost function
L(0, z, DW) = − ln P(0) − ln P(DW  |z, 0)
(5)
by testing local changes to z, modifying the pa-
rameters according to each change, and selecting
the best one. More specifically, one word is pro-
cessed at a time, and the segmentation that min-
imizes the cost function with the optimal model
parameters is selected:
</bodyText>
<equation confidence="0.997089714285714">
zj = arg min
(t+1)
zj nmin L(0, z(t), DW) o. (6)
0
Next, the parameters are updated:
0(t+1) = arg min nL(0, z(t+1), DW)}. (7)
0 J
</equation>
<bodyText confidence="0.999994142857143">
As neither of the steps can increase the cost func-
tion, this will converge to a local optimum. The
initial parameters are obtained by adding all the
words into the morph lexicon. Due to the context
independence of the morphs within a word, the op-
timal analysis for a segment does not depend on
in which context the segment appears. Thus, it is
possible to encode z as a binary tree-like graph,
where the words are the top nodes and morphs the
leaf nodes. For each word, every possible split into
two morphs is tested in addition to no split. If the
word is split, the same test is applied recursively
to its parts. See, e.g., Creutz and Lagus (2005) for
more details and pseudo-code.
</bodyText>
<subsectionHeader confidence="0.972876">
2.4 Semi-supervised learning
</subsectionHeader>
<bodyText confidence="0.999976413793104">
A straightforward way to do semi-supervised
learning is to fix the analyses z for the labeled ex-
amples. Early experiments indicated that this has
little effect on the results. The Morfessor Baseline
model only contains local parameters for morphs,
and relies on the bias given by its prior to guide
the amount of segmentation. Therefore, it may not
be well suited for semi-supervised learning. The
labeled data affects only the morphs that are found
in the labeled data, and even their analyses can be
overwhelmed by a large amount of unsupervised
data and the bias of the prior.
We suggest a fairly simple solution to this by
introducing extra parameters that guide the more
general behavior of the model. The amount of
segmentation is mostly affected by the balance
between the prior and the model. The Morfes-
sor Baseline model has been developed to ensure
this balance is sensible. However, the labeled
data gives a strong source of information regarding
the amount of segmentation preferred by the gold
standard. We can utilize this information by intro-
ducing the weight α on the likelihood. To address
the problem of labeled data being overwhelmed by
the large amount of unlabeled data we introduce a
second weight Q on the likelihood for the labeled
data. These weights are optimized on a separate
held-out set. Thus, instead of optimizing the MAP
estimate, we minimize the following function:
</bodyText>
<equation confidence="0.99911625">
L(0, z, DW, DW7→A) =
− ln P(0)
− α x lnP(DW  |z, 0)
− Q x lnP(DW7→A  |z, 0) (8)
</equation>
<bodyText confidence="0.93069">
The labeled training set DW7→A may include al-
ternative analyses for some of the words. Let
A(wj) = {aj1, ... , ajk} be the set of known anal-
yses for word wj. Assuming the training samples
are independent, and giving equal weight for each
analysis, the likelihood of the labeled data would
be
</bodyText>
<equation confidence="0.99956575">
P(DW7→A  |0)
|DW�→A|
Y= Y
j=1 ajk∈A(wj)
</equation>
<bodyText confidence="0.999976">
However, when the analyses of the words are
fixed, the product over alternative analyses in A
is problematic, because the model cannot select
several of them at the same time. A sum over
A(wj):s would avoid this problem, but then the
logarithm of the likelihood function becomes non-
trivial (i.e., logarithm of sum of products) and too
slow to calculate during the training. Instead, we
use the hidden variable Z to select only one anal-
ysis also for the labeled samples, but now with the
restriction that Zj E A(wj). The likelihood func-
tion for DW7→A is then equivalent to Equation 2.
Because the recursive algorithm search assumes
that a string is segmented in the same way irre-
spective of its context, the labeled data can still
</bodyText>
<equation confidence="0.9992405">
P(M = mjki |0). (9)
|ajk|
Y
i=1
</equation>
<page confidence="0.965295">
81
</page>
<bodyText confidence="0.996693666666667">
get zero probabilities. In practice, zero probabil-
ities in the labeled data likelihood are treated as
very large, but not infinite, costs.
</bodyText>
<subsectionHeader confidence="0.994749">
2.5 Segmenting new words
</subsectionHeader>
<bodyText confidence="0.999955454545454">
After training the model, a Viterbi-like algorithm
can be applied to find the optimal segmentation
of each word. As proposed by Virpioja and Ko-
honen (2009), also new morph types can be al-
lowed by utilizing an approximate cost of adding
them to the lexicon. As this enables reasonable re-
sults also when the training data is small, we use a
similar technique. The cost is calculated from the
decrease in the probabilities given in Equations 3
and 4 when a new morph is assumed to be in the
lexicon.
</bodyText>
<sectionHeader confidence="0.999749" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998535">
In the experiments, we compare six different vari-
ants of the Morfessor Baseline algorithm:
</bodyText>
<listItem confidence="0.914281714285714">
• Unsupervised: The classic, unsupervised
Morfessor baseline.
• Unsupervised + weighting: A held-out set
is used for adjusting the weight of the likeli-
hood α. When α = 1 the method is equiva-
lent to the unsupervised baseline. The main
effect of adjusting α is to control how many
</listItem>
<bodyText confidence="0.751208666666667">
segments per word the algorithm prefers.
Higher α leads to fewer and lower α to more
segments per word.
</bodyText>
<listItem confidence="0.987476214285714">
• Supervised: The semi-supervised method
trained with only the labeled data.
• Supervised + weighting: As above, but the
weight of the likelihood Q is optimized on
the held-out set. The weight can only af-
fect which segmentations are selected from
the possible alternative segmentations in the
labeled data.
• Semi-supervised: The semi-supervised
method trained with both labeled and
unlabeled data.
• Semi-supervised + weighting: As above,
but the parameters α and Q are optimized us-
ing the the held-out set.
</listItem>
<bodyText confidence="0.9936695">
All variations are evaluated using the linguistic
gold standard evaluation of Morpho Challenge
2009. For supervised and semi-supervised meth-
ods, the amount of labeled data is varied be-
tween 100 and 10 000 words, whereas the held-
out set has 500 gold standard analyzes. To obtain
precision-recall curves, we calculated weighted
F0.5 and F2 scores in addition to the normal F1
score. The parameters α and Q were optimized
also for those.
</bodyText>
<subsectionHeader confidence="0.998317">
3.1 Data and evaluation
</subsectionHeader>
<bodyText confidence="0.999979666666667">
We used the English and Finnish data sets from
Competition 1 of Morpho Challenge 2009 (Ku-
rimo et al., 2009). Both are extracted from a
three million sentence corpora. For English, there
were 62185 728 word tokens and 384 903 word
types. For Finnish, there were 36 207 308 word
tokens and 2 206 719 word types. The complexity
of Finnish morphology is indicated by the almost
ten times larger number of word types than in En-
glish, while the number of word tokens is smaller.
We applied also the evaluation method of the
Morpho Challenge 2009: The results of the mor-
phological segmentation were compared to a lin-
guistic gold standard analysis. Precision measures
whether the words that share morphemes in the
proposed analysis have common morphemes also
in the gold standard, and recall measures the op-
posite. The final score to optimize was F-measure,
i.e, the harmonic mean of the precision and re-
call.2 In addition to the unweighted F1 score, we
have applied F2 and F0.5 scores, which give more
weight to recall and precision, respectively.
Finnish gold standards are based on FINT-
WOL morphological analyzer from Lingsoft, Inc.,
that applies the two-level model by Koskenniemi
(1983). English gold standards are from the
CELEX English database. The final test sets are
the same as in Morpho Challenge, based on 10 000
English word forms and 200 000 Finnish word
forms. The test sets are divided into ten parts for
calculating deviations and statistical significances.
For parameter tuning, we applied a small held-out
set containing 500 word forms that were not in-
cluded in the test set.
For supervised and semi-supervised training,
we created sets of five different sizes: 100, 300,
1000, 3 000, and 10 000. They did not contain any
of the word forms in the final test set, but were
otherwise randomly selected from the words for
</bodyText>
<footnote confidence="0.999247">
2Both the data sets and evaluation scripts are available
from the Morpho Challenge 2009 web page: http://www.
cis.hut.fi/morphochallenge2009/
</footnote>
<page confidence="0.998513">
82
</page>
<figureCaption confidence="0.9906155">
Figure 1: The F-measure for English as a function
of the number of labeled training samples.
</figureCaption>
<bodyText confidence="0.9971038">
which the gold standard analyses were available.
In order to use them for training Morfessor, the
morpheme analyses were converted to segmenta-
tions using the Hutmegs package by Creutz and
Lind´en (2004).
</bodyText>
<subsectionHeader confidence="0.809047">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.94879232">
Figure 1 shows a comparison of the unsupervised,
supervised and semi-supervised Morfessor Base-
line for English. It can be seen that optimiz-
ing the likelihood weight α alone does not im-
prove much over the unsupervised case, imply-
ing that the Morfessor Baseline is well suited for
English morphology. Without weighting of the
likelihood function, semi-supervised training im-
proves the results somewhat, but it outperforms
weighted unsupervised model only barely. With
weighting, however, semi-supervised training im-
proves the results significantly already for only
100 labeled training samples. For comparison,
in Morpho Challenges (Kurimo et al., 2009), the
unsupervised Morfessor Baseline and Morfessor
Categories-MAP by Creutz and Lagus (2007) have
achieved F-measures of 59.84% and 50.50%, re-
spectively, and the all time best unsupervised re-
sult by a method that does not provide alternative
analyses for words is 66.24%, obtained by Bern-
hard (2008).3 This best unsupervised result is sur-
passed by the semi-supervised algorithm at 1000
labeled samples.
As shown in Figure 1, the supervised method
obtains inconsistent scores for English with the
</bodyText>
<footnote confidence="0.4837475">
3Better results (68.71%) have been achieved by Monson
et al. (2008), but as they were obtained by combining of
two systems as alternative analyses, the comparison is not as
meaningful.
</footnote>
<figureCaption confidence="0.998902333333333">
Figure 2: The F-measure for Finnish as a function
of the number of labeled training samples. The
semi-supervised and unsupervised lines overlap.
</figureCaption>
<bodyText confidence="0.998636029411765">
smallest training data sizes. The supervised al-
gorithm only knows the morphs in the training
set, and therefore is crucially dependent on the
Viterbi segmentation algorithm for analyzing new
data. Thus, overfitting to some small data sets is
not surprising. At 10 000 labeled training samples
it clearly outperforms the unsupervised algorithm.
The improvement obtained from tuning the weight
β in the supervised case is small.
Figure 2 shows the corresponding results for
Finnish. The optimization of the likelihood weight
gives a large improvement to the F-measure al-
ready in the unsupervised case. This is mainly be-
cause the standard unsupervised Morfessor Base-
line method does not, on average, segment words
into as many segments as would be appropriate for
Finnish. Without weighting, the semi-supervised
method does not improve over the unsupervised
one: The unlabeled training data is so much larger
that the labeled data has no real effect.
For Finnish, the unsupervised Morfessor Base-
line and Categories-MAP obtain F-measures of
26.75% and 44.61%, respectively (Kurimo et al.,
2009). The all time best for an unsupervised
method is 52.45% by Bernhard (2008). With op-
timized likelihood weights, the semi-supervised
Morfessor Baseline achieves higher F-measures
with only 100 labeled training samples. Fur-
thermore, the largest improvement for the semi-
supervised method is achieved already from 1000
labeled training samples. Unlike English, the su-
pervised method is quite a lot worse than the un-
supervised one for small training data. This is
natural because of the more complex morphology
</bodyText>
<page confidence="0.998854">
83
</page>
<figureCaption confidence="0.957573">
Figure 3: Precision-recall graph for English with
</figureCaption>
<bodyText confidence="0.982317666666667">
varying amount of labeled training data. Parame-
ters α and Q have been optimized for three differ-
ent measures: F0.5, F1 and F2 on the held-out set.
Precision and recall values are from the final test
set, error bars indicate one standard deviation.
in Finnish; good results are not achieved just by
knowing the few most common suffixes.
Figures 3 and 4 show precision-recall graphs
of the performance of the semi-supervised method
for English and Finnish. The parameters α and Q
have been optimized for three differently weighted
F-measures (F0.5, F1, and F2) on the held-out set.
The weight tells how much recall is emphasized;
F1 is the symmetric F-measure that emphasizes
precision and recall alike. The graphs show that
the more there are labeled training data, the more
constrained the model parameters are: With many
labeled examples, the model cannot be forced to
achieve high precision or recall only. The phe-
nomenon is more evident in the Finnish data (Fig-
ure 3), where the same amount of words contains
more information (morphemes) than in the En-
glish data. Table 1 shows the F0.5, F1 and F2
measures numerically.
Table 2 shows the values for the F1-optimal
weights α and Q that were chosen for different
amounts of labeled data using the held-out set. As
even the largest labeled sets are much smaller than
the unlabeled training set, it is natural that Q ≫ α.
The small optimal α for Finnish explains why the
difference between unsupervised unweighted and
weighted versions in Figure 2 was so large. Gener-
ally, the more there is labeled data, the smaller Q is
needed. A possible increase in overall likelihood
cost is compensated by a smaller α. Finnish with
100 labeled words is an exception; probably a very
Figure 4: Precision-recall graph for Finnish with
varying amount of labeled training data. Param-
eters α and Q have been optimized for three dif-
ferent measures: F0.5, F1 and F2 on the held-out
set. Precision and recall values are from the final
test set, error bars indicate one standard deviation,
which here is very small.
high Q would end in overlearning of the small set
words at the cost of overall performance.
</bodyText>
<sectionHeader confidence="0.998285" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999991923076923">
The method developed in this paper is a straight-
forward extension of Morfessor Baseline. In the
semi-supervised setting, it should be possible to
develop a generative model that would not require
any discriminative reweighting, but could learn,
e.g., the amount of segmentation from the labeled
data. Moreover, it would be possible to learn the
morpheme labels instead of just the segmentation
into morphs, either within the current model or as
a separate step after the segmentation. We made
initial experiment with a trivial context-free label-
ing: A mapping between the segments and mor-
pheme labels was extracted from the labeled train-
ing data. If some label did not have a correspond-
ing segment, it was appended to the previous la-
bel. E.g., if the labels for “found” are “find V
+PAST”, “found” was mapped to both labels. Af-
ter segmentation, each segment in the test data was
replaced by the most common label or label se-
quence whenever such was available. The results
using training data with 1000 and 10 000 labeled
samples are shown in Table 3. Although preci-
sions decrease somewhat, recalls improve consid-
erably, and significant gains in F-measure are ob-
tained. A more advanced, context-sensitive label-
ing should perform much better.
</bodyText>
<page confidence="0.997328">
84
</page>
<table confidence="0.9993716875">
English Segmented Labeled
labeled data F0.5 F1 F2
0 69.16 61.05 62.70
100 73.23 65.18 68.30
300 72.98 65.63 68.81
1000 71.86 68.29 69.68
3000 74.34 69.13 72.01
10000 76.04 72.85 73.89
Finnish
labeled data F0.5 F1 F2
0 56.81 49.07 53.95
100 58.96 52.66 57.01
300 59.33 54.92 57.16
1000 61.75 56.38 58.24
3000 63.72 58.21 58.90
10000 66.58 60.26 57.24
</table>
<tableCaption confidence="0.8808705">
Table 1: The F0.5, F1 and F2 measures for the
semi-supervised + weighting method.
</tableCaption>
<table confidence="0.999224625">
English Finnish
labeled data α Q α Q
0 0.75 - 0.01 -
100 0.75 750 0.01 500
300 1 500 0.005 5000
1000 1 500 0.05 2500
3000 1.75 350 0.1 1000
10000 1.75 175 0.1 500
</table>
<tableCaption confidence="0.67079325">
Table 2: The values for the weights α and Q
that the semisupervised algorithm chose for differ-
ent amounts of labeled data when optimizing F1-
measure.
</tableCaption>
<bodyText confidence="0.999846388888889">
The semi-supervised extension could easily be
applied to the other versions and extensions of
Morfessor, such as Morfessor Categories-MAP
(Creutz and Lagus, 2007) and Allomorfessor (Vir-
pioja and Kohonen, 2009). Especially the model-
ing of allomorphy might benefit from even small
amounts of labeled data, because those allomorphs
that are hardest to find (affixes, stems with irregu-
lar orthographic changes) are often more common
than the easy cases, and thus likely to be found
even from a small labeled data set.
Even without labeling, it will be interesting
to see how well the semi-supervised morphology
learning works in applications such as information
retrieval. Compared to unsupervised learning, we
obtained much higher recall for reasonably good
levels of precision, which should be beneficial to
most applications.
</bodyText>
<table confidence="0.9683803125">
English, D = 1000
Precision 69.72% 69.30%
Recall 66.92% 72.21%
F-measure 68.29% 70.72%
English, D = 10 000
Precision 77.35% 77.07%
Recall 68.85% 77.78%
F-measure 72.86% 77.42%
Finnish, D = 1000
Precision 61.03% 58.96%
Recall 52.38% 66.55%
F-measure 56.38% 62.53%
Finnish, D = 10 000
Precision
Recall
F-measure
</table>
<tableCaption confidence="0.9854735">
Table 3: Results of a simple morph labeling after
segmentation with semi-supervised Morfessor.
</tableCaption>
<sectionHeader confidence="0.998615" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999983">
We have evaluated an extension of the Morfessor
Baseline method to semi-supervised morphologi-
cal segmentation. Even with our simple method,
the scores improve far beyond the best unsuper-
vised results. Moreover, already one hundred
known segmentations give significant gain over
the unsupervised method even with the optimized
data likelihood weight.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999954">
This work was funded by Academy of Finland and
Graduate School of Language Technology in Fin-
land. We thank Mikko Kurimo and Tiina Lindh-
Knuutila for comments on the manuscript, and
Nokia foundation for financial support.
</bodyText>
<sectionHeader confidence="0.987806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.765534916666667">
Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proceedings of the
PASCAL Challenge Workshop on Unsupervised seg-
mentation of words into morphemes, Venice, Italy.
PASCAL European Network of Excellence.
Delphine Bernhard. 2008. Simple morpheme labelling
in unsupervised morpheme analysis. In Advances in
Multilingual and Multimodal Information Retrieval,
8th Workshop of the CLEF, volume 5152 of Lec-
ture Notes in Computer Science, pages 873–880.
Springer Berlin / Heidelberg.
</reference>
<figure confidence="0.989659333333333">
69.14%
53.40%
60.26%
66.90%
74.08%
70.31%
</figure>
<page confidence="0.991522">
85
</page>
<reference confidence="0.998865926605504">
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL’02, pages 21–30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical
Report A81, Publications in Computer and Informa-
tion Science, Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Mathias Creutz and Krister Lind´en. 2004. Morpheme
segmentation gold standards for Finnish and En-
glish. Technical Report A77, Publications in Com-
puter and Information Science, Helsinki University
of Technology.
Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo,
Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):1–29.
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In the annual conference of the North
American Chapter of the ACL (NAACL-HLT).
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 39(1):1–
38.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–189.
Kimmo Koskenniemi. 1983. Two-level morphology: A
general computational model for word-form recog-
nition and production. Ph.D. thesis, University of
Helsinki.
Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Budapest, Hungary, September 19-21, 2007, Re-
vised Selected Papers, Lecture Notes in Computer
Science, Vol. 5152, pages 864–873. Springer.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Wei Li and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In AAAI’05: Proceedings of the 20th na-
tional conference on Artificial intelligence, pages
813–818. AAAI Press.
Christian Monson, Alon Lavie, Jaime Carbonell, and
Lori Levin. 2004. Unsupervised induction of natu-
ral language morphology inflection classes. In Pro-
ceedings ofthe Workshop of the ACL Special Interest
Group in Computational Phonology (SIGPHON).
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Budapest, Hungary, September 19-21,
2007, Revised Selected Papers, Lecture Notes in
Computer Science, Vol. 5152. Springer.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ’09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 209–217. Association for Computational Lin-
guistics.
Jorma Rissanen. 1989. Stochastic Complexity in Sta-
tisticalInquiry, volume 15. World Scientific Series
in Computer Science, Singapore.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
AAAI’08: Proceedings of the 23rd national con-
ference on Artificial intelligence, pages 848–854.
AAAI Press.
Benjamin Snyder and Regina Barzilay. 2008b. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of ACL-08: HLT,
pages 737–745, Columbus, Ohio, June. Association
for Computational Linguistics.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In COLING ’08: Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics, pages 1017–1024, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Xiaojin Zhu. 2005. Semi-supervised Learning with
Graphs. Ph.D. thesis, CMU. Chapter 11, Semi-
supervised learning literature survey (updated online
version).
</reference>
<page confidence="0.998548">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.554827">
<title confidence="0.993886">Semi-supervised learning of concatenative morphology</title>
<author confidence="0.981736">Kohonen Virpioja</author>
<affiliation confidence="0.9553175">Aalto University School of Science and Adaptive Informatics Research</affiliation>
<address confidence="0.70461">P.O. Box 15400, FI-00076 AALTO,</address>
<abstract confidence="0.98970265">We consider morphology learning in a semi-supervised setting, where a small set of linguistic gold standard analyses is available. We extend Morfessor Baseline, which is a method for unsupervised morphological segmentation, to this task. We show that known linguistic segmentations can be exploited by adding them into the data likelihood function and optimizing separate weights for unlabeled and labeled data. Experiments on English and Finnish are presented with varying amount of labeled data. Results of the linguistic evaluation of Morpho Challenge improve rapidly already with small amounts of labeled data, surpassing the state-ofthe-art unsupervised methods at 1000 labeled words for English and at 100 labeled words for Finnish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
</authors>
<title>Unsupervised morphological segmentation based on segment predictability and word segments alignment.</title>
<date>2006</date>
<journal>PASCAL European Network of Excellence.</journal>
<booktitle>In Proceedings of the PASCAL Challenge Workshop on Unsupervised segmentation of words into morphemes,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="2438" citStr="Bernhard, 2006" startWordPosition="364" endWordPosition="365">of availability also apply to morphologically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may </context>
</contexts>
<marker>Bernhard, 2006</marker>
<rawString>Delphine Bernhard. 2006. Unsupervised morphological segmentation based on segment predictability and word segments alignment. In Proceedings of the PASCAL Challenge Workshop on Unsupervised segmentation of words into morphemes, Venice, Italy. PASCAL European Network of Excellence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
</authors>
<title>Simple morpheme labelling in unsupervised morpheme analysis.</title>
<date>2008</date>
<booktitle>In Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the CLEF,</booktitle>
<volume>5152</volume>
<pages>873--880</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="21667" citStr="Bernhard (2008)" startWordPosition="3628" endWordPosition="3630">unction, semi-supervised training improves the results somewhat, but it outperforms weighted unsupervised model only barely. With weighting, however, semi-supervised training improves the results significantly already for only 100 labeled training samples. For comparison, in Morpho Challenges (Kurimo et al., 2009), the unsupervised Morfessor Baseline and Morfessor Categories-MAP by Creutz and Lagus (2007) have achieved F-measures of 59.84% and 50.50%, respectively, and the all time best unsupervised result by a method that does not provide alternative analyses for words is 66.24%, obtained by Bernhard (2008).3 This best unsupervised result is surpassed by the semi-supervised algorithm at 1000 labeled samples. As shown in Figure 1, the supervised method obtains inconsistent scores for English with the 3Better results (68.71%) have been achieved by Monson et al. (2008), but as they were obtained by combining of two systems as alternative analyses, the comparison is not as meaningful. Figure 2: The F-measure for Finnish as a function of the number of labeled training samples. The semi-supervised and unsupervised lines overlap. smallest training data sizes. The supervised algorithm only knows the mor</context>
<context position="23359" citStr="Bernhard (2008)" startWordPosition="3893" endWordPosition="3894">o the F-measure already in the unsupervised case. This is mainly because the standard unsupervised Morfessor Baseline method does not, on average, segment words into as many segments as would be appropriate for Finnish. Without weighting, the semi-supervised method does not improve over the unsupervised one: The unlabeled training data is so much larger that the labeled data has no real effect. For Finnish, the unsupervised Morfessor Baseline and Categories-MAP obtain F-measures of 26.75% and 44.61%, respectively (Kurimo et al., 2009). The all time best for an unsupervised method is 52.45% by Bernhard (2008). With optimized likelihood weights, the semi-supervised Morfessor Baseline achieves higher F-measures with only 100 labeled training samples. Furthermore, the largest improvement for the semisupervised method is achieved already from 1000 labeled training samples. Unlike English, the supervised method is quite a lot worse than the unsupervised one for small training data. This is natural because of the more complex morphology 83 Figure 3: Precision-recall graph for English with varying amount of labeled training data. Parameters α and Q have been optimized for three different measures: F0.5, </context>
</contexts>
<marker>Bernhard, 2008</marker>
<rawString>Delphine Bernhard. 2008. Simple morpheme labelling in unsupervised morpheme analysis. In Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the CLEF, volume 5152 of Lecture Notes in Computer Science, pages 873–880. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Morphological and Phonological Learning of ACL’02,</booktitle>
<pages>21--30</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="2401" citStr="Creutz and Lagus, 2002" startWordPosition="356" endWordPosition="359">reely available. In many cases, the problems of availability also apply to morphologically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very hi</context>
<context position="4741" citStr="Creutz and Lagus, 2002" startWordPosition="733" endWordPosition="736">ween input data and labels, and therefore require labeled data. Both, however, can be extended to the semi-supervised case. For generative models, it is, in principle, very easy to use both labeled and unlabeled data. For unsupervised learning one can consider the labels as missing data and estimate their values using the Expectation Maximization (EM) algorithm (Dempster et al., 1977). In the semi-supervised case, some labels are available, and the rest are considered missing and estimated with EM. In this paper, we extend the Morfessor Baseline method for the semi-supervised case. Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005; Creutz and Lagus, 2007, etc.) is one of the well-established methods for morphological segmentation. It applies a simple generative model. The basic idea, inspired by the Minimum Description Length principle (Rissanen, 1989), is to encode the words in the training data with a lexicon of morphs, that are segments of the words. The number of bits needed to encode both the morph lexicon and the data using the lexicon should be minimized. Morfessor does not limit the number of morphemes per word form, making it suitable for modeling a large variety of agglutinative langua</context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Proceedings of the Workshop on Morphological and Phonological Learning of ACL’02, pages 21–30, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0.</title>
<date>2005</date>
<tech>Technical Report A81,</tech>
<institution>Publications in Computer and Information Science, Helsinki University of Technology.</institution>
<contexts>
<context position="4765" citStr="Creutz and Lagus, 2005" startWordPosition="737" endWordPosition="740">ls, and therefore require labeled data. Both, however, can be extended to the semi-supervised case. For generative models, it is, in principle, very easy to use both labeled and unlabeled data. For unsupervised learning one can consider the labels as missing data and estimate their values using the Expectation Maximization (EM) algorithm (Dempster et al., 1977). In the semi-supervised case, some labels are available, and the rest are considered missing and estimated with EM. In this paper, we extend the Morfessor Baseline method for the semi-supervised case. Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005; Creutz and Lagus, 2007, etc.) is one of the well-established methods for morphological segmentation. It applies a simple generative model. The basic idea, inspired by the Minimum Description Length principle (Rissanen, 1989), is to encode the words in the training data with a lexicon of morphs, that are segments of the words. The number of bits needed to encode both the morph lexicon and the data using the lexicon should be minimized. Morfessor does not limit the number of morphemes per word form, making it suitable for modeling a large variety of agglutinative languages irrespective of them</context>
<context position="11311" citStr="Creutz and Lagus, 2005" startWordPosition="1855" endWordPosition="1859">hs tokens in the observed data, v E Z+ • Morph strings (a1, ... , aµ), ai E E∗ • Morph counts (T1, ... , Tµ), Ti E J1,. . . , v}, Ei Ti = v. Normalized with v, these give the probabilities of the morphs. MDL-inspired and non-informative priors have been preferred. When using such priors, morph type count and morph token counts can be neglected when optimizing the model. The morph string prior is based on length distribution P(L) and distribution P(C) of characters over the character set E, both assumed to be known: P(ai) = P(L = |ai|) H|σi |P(C = aij) (3) j=1 We use the implicit length prior (Creutz and Lagus, 2005), which is obtained by removing P(L) and using end-of-word mark as an additional character in P(C). For morph counts, the noninformative prior 1 P(T1, ... , Tµ) v- 1/ (4) µ − 1 gives equal probability to each possible combination of the counts when µ and v are known, as there are (ν−1 ) possible ways to choose µ positive µ−1 integers that sum up to v. 2.3 Unsupervised learning In principle, unsupervised learning can be performed by looking for the MAP estimate with the EM-algorithm. In the case of Morfessor Baseline, this is problematic, because the prior only assigns higher probability to lex</context>
<context position="12551" citStr="Creutz and Lagus (2005)" startWordPosition="2076" endWordPosition="2079"> morphs have nonzero probabilities. The EM-algorithm has the property that it will not assign a zero probability to any morph, that has a nonzero likelihood in the previous step, and this will hold for all morphs |DW | = H j=1 |DW | = H j=1 80 that initially have a nonzero probability. In consequence, Morfessor Baseline instead uses a local search algorithm, which will assign zero probability to a large part of the potential morphs. This is memory-efficient, since only the morphs with nonzero probabilities need to be stored in memory. The training algorithm of Morfessor Baseline, described by Creutz and Lagus (2005), tries to minimize the cost function L(0, z, DW) = − ln P(0) − ln P(DW |z, 0) (5) by testing local changes to z, modifying the parameters according to each change, and selecting the best one. More specifically, one word is processed at a time, and the segmentation that minimizes the cost function with the optimal model parameters is selected: zj = arg min (t+1) zj nmin L(0, z(t), DW) o. (6) 0 Next, the parameters are updated: 0(t+1) = arg min nL(0, z(t+1), DW)}. (7) 0 J As neither of the steps can increase the cost function, this will converge to a local optimum. The initial parameters are ob</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0. Technical Report A81, Publications in Computer and Information Science, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4789" citStr="Creutz and Lagus, 2007" startWordPosition="741" endWordPosition="744">e labeled data. Both, however, can be extended to the semi-supervised case. For generative models, it is, in principle, very easy to use both labeled and unlabeled data. For unsupervised learning one can consider the labels as missing data and estimate their values using the Expectation Maximization (EM) algorithm (Dempster et al., 1977). In the semi-supervised case, some labels are available, and the rest are considered missing and estimated with EM. In this paper, we extend the Morfessor Baseline method for the semi-supervised case. Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005; Creutz and Lagus, 2007, etc.) is one of the well-established methods for morphological segmentation. It applies a simple generative model. The basic idea, inspired by the Minimum Description Length principle (Rissanen, 1989), is to encode the words in the training data with a lexicon of morphs, that are segments of the words. The number of bits needed to encode both the morph lexicon and the data using the lexicon should be minimized. Morfessor does not limit the number of morphemes per word form, making it suitable for modeling a large variety of agglutinative languages irrespective of them being more isolating or</context>
<context position="21460" citStr="Creutz and Lagus (2007)" startWordPosition="3592" endWordPosition="3595"> seen that optimizing the likelihood weight α alone does not improve much over the unsupervised case, implying that the Morfessor Baseline is well suited for English morphology. Without weighting of the likelihood function, semi-supervised training improves the results somewhat, but it outperforms weighted unsupervised model only barely. With weighting, however, semi-supervised training improves the results significantly already for only 100 labeled training samples. For comparison, in Morpho Challenges (Kurimo et al., 2009), the unsupervised Morfessor Baseline and Morfessor Categories-MAP by Creutz and Lagus (2007) have achieved F-measures of 59.84% and 50.50%, respectively, and the all time best unsupervised result by a method that does not provide alternative analyses for words is 66.24%, obtained by Bernhard (2008).3 This best unsupervised result is surpassed by the semi-supervised algorithm at 1000 labeled samples. As shown in Figure 1, the supervised method obtains inconsistent scores for English with the 3Better results (68.71%) have been achieved by Monson et al. (2008), but as they were obtained by combining of two systems as alternative analyses, the comparison is not as meaningful. Figure 2: T</context>
<context position="28145" citStr="Creutz and Lagus, 2007" startWordPosition="4701" endWordPosition="4704">61.75 56.38 58.24 3000 63.72 58.21 58.90 10000 66.58 60.26 57.24 Table 1: The F0.5, F1 and F2 measures for the semi-supervised + weighting method. English Finnish labeled data α Q α Q 0 0.75 - 0.01 - 100 0.75 750 0.01 500 300 1 500 0.005 5000 1000 1 500 0.05 2500 3000 1.75 350 0.1 1000 10000 1.75 175 0.1 500 Table 2: The values for the weights α and Q that the semisupervised algorithm chose for different amounts of labeled data when optimizing F1- measure. The semi-supervised extension could easily be applied to the other versions and extensions of Morfessor, such as Morfessor Categories-MAP (Creutz and Lagus, 2007) and Allomorfessor (Virpioja and Kohonen, 2009). Especially the modeling of allomorphy might benefit from even small amounts of labeled data, because those allomorphs that are hardest to find (affixes, stems with irregular orthographic changes) are often more common than the easy cases, and thus likely to be found even from a small labeled data set. Even without labeling, it will be interesting to see how well the semi-supervised morphology learning works in applications such as information retrieval. Compared to unsupervised learning, we obtained much higher recall for reasonably good levels </context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krister Lind´en</author>
</authors>
<title>Morpheme segmentation gold standards for Finnish and English.</title>
<date>2004</date>
<tech>Technical Report A77,</tech>
<institution>Publications in Computer and Information Science, Helsinki University of Technology.</institution>
<marker>Creutz, Lind´en, 2004</marker>
<rawString>Mathias Creutz and Krister Lind´en. 2004. Morpheme segmentation gold standards for Finnish and English. Technical Report A77, Publications in Computer and Information Science, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas Stolcke.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas Stolcke. 2007. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. ACM Transactions on Speech and Language Processing, 5(1):1–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Highperformance, language-independent morphological segmentation.</title>
<date>2007</date>
<booktitle>In the annual conference of the North American Chapter of the ACL (NAACL-HLT).</booktitle>
<contexts>
<context position="2461" citStr="Dasgupta and Ng, 2007" startWordPosition="366" endWordPosition="369">also apply to morphologically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias,</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. Highperformance, language-independent morphological segmentation. In the annual conference of the North American Chapter of the ACL (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<pages>38</pages>
<contexts>
<context position="4506" citStr="Dempster et al., 1977" startWordPosition="693" endWordPosition="696">ing of the ACL-SIGMORPHON, ACL 2010, pages 78–86, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics and supervised learning. In contrast, discriminative models only specify the conditional distribution between input data and labels, and therefore require labeled data. Both, however, can be extended to the semi-supervised case. For generative models, it is, in principle, very easy to use both labeled and unlabeled data. For unsupervised learning one can consider the labels as missing data and estimate their values using the Expectation Maximization (EM) algorithm (Dempster et al., 1977). In the semi-supervised case, some labels are available, and the rest are considered missing and estimated with EM. In this paper, we extend the Morfessor Baseline method for the semi-supervised case. Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005; Creutz and Lagus, 2007, etc.) is one of the well-established methods for morphological segmentation. It applies a simple generative model. The basic idea, inspired by the Minimum Description Length principle (Rissanen, 1989), is to encode the words in the training data with a lexicon of morphs, that are segments of the words. The number </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B (Methodological), 39(1):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2377" citStr="Goldsmith, 2001" startWordPosition="354" endWordPosition="355">or they are not freely available. In many cases, the problems of availability also apply to morphologically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages t</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level morphology: A general computational model for word-form recognition and production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="19614" citStr="Koskenniemi (1983)" startWordPosition="3305" endWordPosition="3306">ological segmentation were compared to a linguistic gold standard analysis. Precision measures whether the words that share morphemes in the proposed analysis have common morphemes also in the gold standard, and recall measures the opposite. The final score to optimize was F-measure, i.e, the harmonic mean of the precision and recall.2 In addition to the unweighted F1 score, we have applied F2 and F0.5 scores, which give more weight to recall and precision, respectively. Finnish gold standards are based on FINTWOL morphological analyzer from Lingsoft, Inc., that applies the two-level model by Koskenniemi (1983). English gold standards are from the CELEX English database. The final test sets are the same as in Morpho Challenge, based on 10 000 English word forms and 200 000 Finnish word forms. The test sets are divided into ten parts for calculating deviations and statistical significances. For parameter tuning, we applied a small held-out set containing 500 word forms that were not included in the test set. For supervised and semi-supervised training, we created sets of five different sizes: 100, 300, 1000, 3 000, and 10 000. They did not contain any of the word forms in the final test set, but were</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-level morphology: A general computational model for word-form recognition and production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Matti Varjokallio</author>
</authors>
<title>Morpho Challenge evaluation using a linguistic Gold Standard.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<booktitle>In Advances in Multilingual and MultiModal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,</booktitle>
<volume>5152</volume>
<pages>864--873</pages>
<publisher>Springer.</publisher>
<location>Budapest, Hungary,</location>
<contexts>
<context position="2700" citStr="Kurimo et al., 2008" startWordPosition="404" endWordPosition="407">ed, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias, for example, not analyzing frequent compound words as consisting of smaller parts could be beneficial in information retrieval. In many cases, even a small amount of labeled data can be used to adapt a method to a particular language and </context>
</contexts>
<marker>Kurimo, Creutz, Varjokallio, 2008</marker>
<rawString>Mikko Kurimo, Mathias Creutz, and Matti Varjokallio. 2008. Morpho Challenge evaluation using a linguistic Gold Standard. In Advances in Multilingual and MultiModal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, Budapest, Hungary, September 19-21, 2007, Revised Selected Papers, Lecture Notes in Computer Science, Vol. 5152, pages 864–873. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
<author>Graeme W Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Overview and results of Morpho Challenge</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece,</location>
<contexts>
<context position="6151" citStr="Kurimo et al., 2009" startWordPosition="969" endWordPosition="972">a large set of unlabeled data, the effect of the supervision on the results tends to be small. Thus, we add a discriminative weighting scheme, where a small set of word forms with gold standard analyzes are used for tuning the respective weights of the labeled and unlabeled data. The paper is organized as follows: First, we discuss related work on semi-supervised learning. Then we describe the Morfessor Baseline model and the unsupervised algorithm, followed by our semi-supervised extension. Finally, we present experimental results for English and Finnish using the Morpho Challenge data sets (Kurimo et al., 2009). 1.1 Related work There is surprisingly little work that consider improving the unsupervised models of morphology with small amounts of annotated data. In the related tasks that deal with sequential labeling (word segmentation, POS tagging, shallow parsing, named-entity recognition), semi-supervised learning is more common. Snyder and Barzilay (2008a; 2008b) consider learning morphological segmentation with nonparametric Bayesian model from multilingual data. For multilingual settings, they extract 6139 parallel short phrases from the Hebrew, Arabic, Aramaic and English bible. Using the align</context>
<context position="18538" citStr="Kurimo et al., 2009" startWordPosition="3122" endWordPosition="3126">ers α and Q are optimized using the the held-out set. All variations are evaluated using the linguistic gold standard evaluation of Morpho Challenge 2009. For supervised and semi-supervised methods, the amount of labeled data is varied between 100 and 10 000 words, whereas the heldout set has 500 gold standard analyzes. To obtain precision-recall curves, we calculated weighted F0.5 and F2 scores in addition to the normal F1 score. The parameters α and Q were optimized also for those. 3.1 Data and evaluation We used the English and Finnish data sets from Competition 1 of Morpho Challenge 2009 (Kurimo et al., 2009). Both are extracted from a three million sentence corpora. For English, there were 62185 728 word tokens and 384 903 word types. For Finnish, there were 36 207 308 word tokens and 2 206 719 word types. The complexity of Finnish morphology is indicated by the almost ten times larger number of word types than in English, while the number of word tokens is smaller. We applied also the evaluation method of the Morpho Challenge 2009: The results of the morphological segmentation were compared to a linguistic gold standard analysis. Precision measures whether the words that share morphemes in the p</context>
<context position="21367" citStr="Kurimo et al., 2009" startWordPosition="3580" endWordPosition="3583">the unsupervised, supervised and semi-supervised Morfessor Baseline for English. It can be seen that optimizing the likelihood weight α alone does not improve much over the unsupervised case, implying that the Morfessor Baseline is well suited for English morphology. Without weighting of the likelihood function, semi-supervised training improves the results somewhat, but it outperforms weighted unsupervised model only barely. With weighting, however, semi-supervised training improves the results significantly already for only 100 labeled training samples. For comparison, in Morpho Challenges (Kurimo et al., 2009), the unsupervised Morfessor Baseline and Morfessor Categories-MAP by Creutz and Lagus (2007) have achieved F-measures of 59.84% and 50.50%, respectively, and the all time best unsupervised result by a method that does not provide alternative analyses for words is 66.24%, obtained by Bernhard (2008).3 This best unsupervised result is surpassed by the semi-supervised algorithm at 1000 labeled samples. As shown in Figure 1, the supervised method obtains inconsistent scores for English with the 3Better results (68.71%) have been achieved by Monson et al. (2008), but as they were obtained by combi</context>
<context position="23284" citStr="Kurimo et al., 2009" startWordPosition="3878" endWordPosition="3881">r Finnish. The optimization of the likelihood weight gives a large improvement to the F-measure already in the unsupervised case. This is mainly because the standard unsupervised Morfessor Baseline method does not, on average, segment words into as many segments as would be appropriate for Finnish. Without weighting, the semi-supervised method does not improve over the unsupervised one: The unlabeled training data is so much larger that the labeled data has no real effect. For Finnish, the unsupervised Morfessor Baseline and Categories-MAP obtain F-measures of 26.75% and 44.61%, respectively (Kurimo et al., 2009). The all time best for an unsupervised method is 52.45% by Bernhard (2008). With optimized likelihood weights, the semi-supervised Morfessor Baseline achieves higher F-measures with only 100 labeled training samples. Furthermore, the largest improvement for the semisupervised method is achieved already from 1000 labeled training samples. Unlike English, the supervised method is quite a lot worse than the unsupervised one for small training data. This is natural because of the more complex morphology 83 Figure 3: Precision-recall graph for English with varying amount of labeled training data. </context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2009</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and results of Morpho Challenge 2009. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Semisupervised sequence modeling with syntactic topic models.</title>
<date>2005</date>
<booktitle>In AAAI’05: Proceedings of the 20th national conference on Artificial intelligence,</booktitle>
<pages>813--818</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="8027" citStr="Li and McCallum (2005)" startWordPosition="1266" endWordPosition="1269">arzilay (2008a) in both unsupervised and semi-supervised settings. For the latter, they use somewhat smaller proportions of annotated data, varying from 25% to 100% of the total data, but the amount of unlabeled data is still very small. Results are reported also for a larger 120 000 word Arabic data set, but only for unsupervised learning. A problem similar to morphological segmentation is word segmentation for the languages where orthography does not specify word boundaries. However, the amount of labeled data is usually large, and unlabeled data is just an additional source of information. Li and McCallum (2005) apply a semi-supervised approach to Chinese word segmentation where unlabeled data is utilized for forming word clusters, which are then used as features for a supervised classifier. Xu et al. (2008) adapt a Chinese word segmentation specifically to a machine translation task, by using the indirect supervision from a parallel corpus. 2 Method We present an extension of the Morfessor Baseline method to the semi-supervised setting. Morfessor Baseline is based on a generative probabilistic model. It is a method for modeling concatenative morphology, where the morphs—i.e., the sur79 face forms of</context>
</contexts>
<marker>Li, McCallum, 2005</marker>
<rawString>Wei Li and Andrew McCallum. 2005. Semisupervised sequence modeling with syntactic topic models. In AAAI’05: Proceedings of the 20th national conference on Artificial intelligence, pages 813–818. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Alon Lavie</author>
<author>Jaime Carbonell</author>
<author>Lori Levin</author>
</authors>
<title>Unsupervised induction of natural language morphology inflection classes.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON).</booktitle>
<contexts>
<context position="2422" citStr="Monson et al., 2004" startWordPosition="360" endWordPosition="363"> cases, the problems of availability also apply to morphologically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different a</context>
</contexts>
<marker>Monson, Lavie, Carbonell, Levin, 2004</marker>
<rawString>Christian Monson, Alon Lavie, Jaime Carbonell, and Lori Levin. 2004. Unsupervised induction of natural language morphology inflection classes. In Proceedings ofthe Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>ParaMor: Finding paradigms across morphology.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<booktitle>In Advances in Multilingual and MultiModal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,</booktitle>
<volume>5152</volume>
<publisher>Springer.</publisher>
<location>Budapest, Hungary,</location>
<contexts>
<context position="21931" citStr="Monson et al. (2008)" startWordPosition="3668" endWordPosition="3671">r comparison, in Morpho Challenges (Kurimo et al., 2009), the unsupervised Morfessor Baseline and Morfessor Categories-MAP by Creutz and Lagus (2007) have achieved F-measures of 59.84% and 50.50%, respectively, and the all time best unsupervised result by a method that does not provide alternative analyses for words is 66.24%, obtained by Bernhard (2008).3 This best unsupervised result is surpassed by the semi-supervised algorithm at 1000 labeled samples. As shown in Figure 1, the supervised method obtains inconsistent scores for English with the 3Better results (68.71%) have been achieved by Monson et al. (2008), but as they were obtained by combining of two systems as alternative analyses, the comparison is not as meaningful. Figure 2: The F-measure for Finnish as a function of the number of labeled training samples. The semi-supervised and unsupervised lines overlap. smallest training data sizes. The supervised algorithm only knows the morphs in the training set, and therefore is crucially dependent on the Viterbi segmentation algorithm for analyzing new data. Thus, overfitting to some small data sets is not surprising. At 10 000 labeled training samples it clearly outperforms the unsupervised algo</context>
</contexts>
<marker>Monson, Carbonell, Lavie, Levin, 2008</marker>
<rawString>Christian Monson, Jaime Carbonell, Alon Lavie, and Lori Levin. 2008. ParaMor: Finding paradigms across morphology. In Advances in Multilingual and MultiModal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, Budapest, Hungary, September 19-21, 2007, Revised Selected Papers, Lecture Notes in Computer Science, Vol. 5152. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>209--217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2509" citStr="Poon et al., 2009" startWordPosition="374" endWordPosition="377">king supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias, for example, not analyzing frequent compound wo</context>
<context position="7243" citStr="Poon et al. (2009)" startWordPosition="1136" endWordPosition="1139">lingual settings, they extract 6139 parallel short phrases from the Hebrew, Arabic, Aramaic and English bible. Using the aligned phrase pairs, the model can learn the segmentations for two languages at the same time. In one of the papers (2008a), they consider also semi-supervised scenarios, where annotated data is available either in only one language or both of the languages. However, the amount of annotated data is fixed to the half of the full data. This differs from our experimental setting, where the amount of unlabeled data is very large and the amount of labeled data relatively small. Poon et al. (2009) apply a log-linear, undirected generative model for learning the morphology of Arabic and Hebrew. They report results for the same small data set as Snyder and Barzilay (2008a) in both unsupervised and semi-supervised settings. For the latter, they use somewhat smaller proportions of annotated data, varying from 25% to 100% of the total data, but the amount of unlabeled data is still very small. Results are reported also for a larger 120 000 word Arabic data set, but only for unsupervised learning. A problem similar to morphological segmentation is word segmentation for the languages where or</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 209–217. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<date>1989</date>
<booktitle>Stochastic Complexity in StatisticalInquiry, volume 15. World Scientific Series in Computer Science,</booktitle>
<contexts>
<context position="4991" citStr="Rissanen, 1989" startWordPosition="773" endWordPosition="774">der the labels as missing data and estimate their values using the Expectation Maximization (EM) algorithm (Dempster et al., 1977). In the semi-supervised case, some labels are available, and the rest are considered missing and estimated with EM. In this paper, we extend the Morfessor Baseline method for the semi-supervised case. Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005; Creutz and Lagus, 2007, etc.) is one of the well-established methods for morphological segmentation. It applies a simple generative model. The basic idea, inspired by the Minimum Description Length principle (Rissanen, 1989), is to encode the words in the training data with a lexicon of morphs, that are segments of the words. The number of bits needed to encode both the morph lexicon and the data using the lexicon should be minimized. Morfessor does not limit the number of morphemes per word form, making it suitable for modeling a large variety of agglutinative languages irrespective of them being more isolating or synthetic. We show that the model can be trained in a similar fashion in the semi-supervised case as in the unsupervised case. However, with a large set of unlabeled data, the effect of the supervision</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>Jorma Rissanen. 1989. Stochastic Complexity in StatisticalInquiry, volume 15. World Scientific Series in Computer Science, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Crosslingual propagation for morphological analysis.</title>
<date>2008</date>
<booktitle>In AAAI’08: Proceedings of the 23rd national conference on Artificial intelligence,</booktitle>
<pages>848--854</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2488" citStr="Snyder and Barzilay, 2008" startWordPosition="370" endWordPosition="373">ically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias, for example, not analyzing</context>
<context position="6503" citStr="Snyder and Barzilay (2008" startWordPosition="1019" endWordPosition="1022">on semi-supervised learning. Then we describe the Morfessor Baseline model and the unsupervised algorithm, followed by our semi-supervised extension. Finally, we present experimental results for English and Finnish using the Morpho Challenge data sets (Kurimo et al., 2009). 1.1 Related work There is surprisingly little work that consider improving the unsupervised models of morphology with small amounts of annotated data. In the related tasks that deal with sequential labeling (word segmentation, POS tagging, shallow parsing, named-entity recognition), semi-supervised learning is more common. Snyder and Barzilay (2008a; 2008b) consider learning morphological segmentation with nonparametric Bayesian model from multilingual data. For multilingual settings, they extract 6139 parallel short phrases from the Hebrew, Arabic, Aramaic and English bible. Using the aligned phrase pairs, the model can learn the segmentations for two languages at the same time. In one of the papers (2008a), they consider also semi-supervised scenarios, where annotated data is available either in only one language or both of the languages. However, the amount of annotated data is fixed to the half of the full data. This differs from ou</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008a. Crosslingual propagation for morphological analysis. In AAAI’08: Proceedings of the 23rd national conference on Artificial intelligence, pages 848–854. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>737--745</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2488" citStr="Snyder and Barzilay, 2008" startWordPosition="370" endWordPosition="373">ically annotated corpora, making supervised learning infeasible. In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias, for example, not analyzing</context>
<context position="6503" citStr="Snyder and Barzilay (2008" startWordPosition="1019" endWordPosition="1022">on semi-supervised learning. Then we describe the Morfessor Baseline model and the unsupervised algorithm, followed by our semi-supervised extension. Finally, we present experimental results for English and Finnish using the Morpho Challenge data sets (Kurimo et al., 2009). 1.1 Related work There is surprisingly little work that consider improving the unsupervised models of morphology with small amounts of annotated data. In the related tasks that deal with sequential labeling (word segmentation, POS tagging, shallow parsing, named-entity recognition), semi-supervised learning is more common. Snyder and Barzilay (2008a; 2008b) consider learning morphological segmentation with nonparametric Bayesian model from multilingual data. For multilingual settings, they extract 6139 parallel short phrases from the Hebrew, Arabic, Aramaic and English bible. Using the aligned phrase pairs, the model can learn the segmentations for two languages at the same time. In one of the papers (2008a), they consider also semi-supervised scenarios, where annotated data is available either in only one language or both of the languages. However, the amount of annotated data is fixed to the half of the full data. This differs from ou</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008b. Unsupervised multilingual learning for morphological segmentation. In Proceedings of ACL-08: HLT, pages 737–745, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Oskar Kohonen</author>
</authors>
<title>Unsupervised morpheme analysis with Allomorfessor.</title>
<date>2009</date>
<booktitle>In Working notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece.</location>
<contexts>
<context position="16633" citStr="Virpioja and Kohonen (2009)" startWordPosition="2797" endWordPosition="2801">led samples, but now with the restriction that Zj E A(wj). The likelihood function for DW7→A is then equivalent to Equation 2. Because the recursive algorithm search assumes that a string is segmented in the same way irrespective of its context, the labeled data can still P(M = mjki |0). (9) |ajk| Y i=1 81 get zero probabilities. In practice, zero probabilities in the labeled data likelihood are treated as very large, but not infinite, costs. 2.5 Segmenting new words After training the model, a Viterbi-like algorithm can be applied to find the optimal segmentation of each word. As proposed by Virpioja and Kohonen (2009), also new morph types can be allowed by utilizing an approximate cost of adding them to the lexicon. As this enables reasonable results also when the training data is small, we use a similar technique. The cost is calculated from the decrease in the probabilities given in Equations 3 and 4 when a new morph is assumed to be in the lexicon. 3 Experiments In the experiments, we compare six different variants of the Morfessor Baseline algorithm: • Unsupervised: The classic, unsupervised Morfessor baseline. • Unsupervised + weighting: A held-out set is used for adjusting the weight of the likeliho</context>
<context position="28192" citStr="Virpioja and Kohonen, 2009" startWordPosition="4707" endWordPosition="4711">000 66.58 60.26 57.24 Table 1: The F0.5, F1 and F2 measures for the semi-supervised + weighting method. English Finnish labeled data α Q α Q 0 0.75 - 0.01 - 100 0.75 750 0.01 500 300 1 500 0.005 5000 1000 1 500 0.05 2500 3000 1.75 350 0.1 1000 10000 1.75 175 0.1 500 Table 2: The values for the weights α and Q that the semisupervised algorithm chose for different amounts of labeled data when optimizing F1- measure. The semi-supervised extension could easily be applied to the other versions and extensions of Morfessor, such as Morfessor Categories-MAP (Creutz and Lagus, 2007) and Allomorfessor (Virpioja and Kohonen, 2009). Especially the modeling of allomorphy might benefit from even small amounts of labeled data, because those allomorphs that are hardest to find (affixes, stems with irregular orthographic changes) are often more common than the easy cases, and thus likely to be found even from a small labeled data set. Even without labeling, it will be interesting to see how well the semi-supervised morphology learning works in applications such as information retrieval. Compared to unsupervised learning, we obtained much higher recall for reasonably good levels of precision, which should be beneficial to mos</context>
</contexts>
<marker>Virpioja, Kohonen, 2009</marker>
<rawString>Sami Virpioja and Oskar Kohonen. 2009. Unsupervised morpheme analysis with Allomorfessor. In Working notes for the CLEF 2009 Workshop, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>1017--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8227" citStr="Xu et al. (2008)" startWordPosition="1298" endWordPosition="1301">beled data is still very small. Results are reported also for a larger 120 000 word Arabic data set, but only for unsupervised learning. A problem similar to morphological segmentation is word segmentation for the languages where orthography does not specify word boundaries. However, the amount of labeled data is usually large, and unlabeled data is just an additional source of information. Li and McCallum (2005) apply a semi-supervised approach to Chinese word segmentation where unlabeled data is utilized for forming word clusters, which are then used as features for a supervised classifier. Xu et al. (2008) adapt a Chinese word segmentation specifically to a machine translation task, by using the indirect supervision from a parallel corpus. 2 Method We present an extension of the Morfessor Baseline method to the semi-supervised setting. Morfessor Baseline is based on a generative probabilistic model. It is a method for modeling concatenative morphology, where the morphs—i.e., the sur79 face forms of morphemes—of a word are its nonoverlapping segments. The model parameters 9 encode a morph lexicon, which includes the properties of the morphs, such as their string representations. Each morph m in </context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised chinese word segmentation for statistical machine translation. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 1017–1024, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised Learning with Graphs.</title>
<date>2005</date>
<tech>Ph.D. thesis, CMU. Chapter 11,</tech>
<note>Semisupervised learning literature survey (updated online version).</note>
<contexts>
<context position="3700" citStr="Zhu (2005)" startWordPosition="567" endWordPosition="568">requent compound words as consisting of smaller parts could be beneficial in information retrieval. In many cases, even a small amount of labeled data can be used to adapt a method to a particular language and task. Methodologically, this is referred to as semi-supervised learning. In semi-supervised learning, the learning system has access to both labeled and unlabeled data. Typically, the labeled data set is too small for supervised methods to be effective, but there is a large amount of unlabeled data available. There are many different approaches to this class of problems, as presented by Zhu (2005). One approach is to use generative models, which specify a join distribution over all variables in the model. They can be utilized both in unsupervised 78 Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 78–86, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics and supervised learning. In contrast, discriminative models only specify the conditional distribution between input data and labels, and therefore require labeled data. Both, however, can be extended to the semi-supervised case. For generative models, it is, in principle, very easy to </context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Xiaojin Zhu. 2005. Semi-supervised Learning with Graphs. Ph.D. thesis, CMU. Chapter 11, Semisupervised learning literature survey (updated online version).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>