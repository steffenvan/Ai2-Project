<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047138">
<title confidence="0.9987285">
Automatic Adaptation of Annotation Standards for Dependency Parsing
— Using Projected Treebank as Source Corpus
</title>
<author confidence="0.965373">
Wenbin Jiang and Qun Liu
</author>
<affiliation confidence="0.953150333333333">
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.730461">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.996215">
{jiangwenbin, liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.997281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970470588235">
We describe for dependency parsing an an-
notation adaptation strategy, which can au-
tomatically transfer the knowledge from
a source corpus with a different annota-
tion standard to the desired target parser,
with the supervision by a target corpus an-
notated in the desired standard. Further-
more, instead of a hand-annotated one, a
projected treebank derived from a bilin-
gual corpus is used as the source cor-
pus. This benefits the resource-scarce
languages which haven’t different hand-
annotated treebanks. Experiments show
that the target parser gains significant im-
provement over the baseline parser trained
on the target corpus only, when the target
corpus is smaller.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999758181818182">
Automatic annotation adaptation for sequence la-
beling (Jiang et al., 2009) aims to enhance a
tagger with one annotation standard by transfer-
ring knowledge from a source corpus annotated in
another standard. It would be valuable to adapt
this strategy to parsing, since for some languages
there are also several treebanks with different an-
notation standards, such as Chomskian-style Penn
Treebank (Marcus et al., 1993) and HPSG LinGo
Redwoods Treebank (Oepen et al., 2002) for En-
glish. However, we are not content with conduct-
ing annotation adaptation between existing differ-
ent treebanks, because it would be more valuable
to boost the parsers also for the resource-scarce
languages, rather than only for the resource-rich
ones that already have several treebanks.
Although hand-annotated treebanks are costly
and scarce, it is not difficult for many languages to
collect large numbers of bilingual sentence-pairs
aligned to English. According to the word align-
ment, the English parses can be projected across
to their translations, and the projected trees can be
leveraged to boost parsing. Many efforts are de-
voted to the research on projected treebanks, such
as (L¨u et al., 2002), (Hwa et al., 2005) and
(Ganchev et al., 2009), etc. Considering the fact
that a projected treebank partially inherits the En-
glish annotation standard, some hand-written rules
are designed to deal with the divergence between
languages such as in (Hwa et al., 2002). How-
ever, it will be more valuable and interesting to
adapt this divergence automatically and boost the
existing parsers with this projected treebank.
In this paper, we investigate the automatic anno-
tation adaptation strategy for Chinese dependency
parsing, where the source corpus for adaptation is
a projected treebank derived from a bilingual cor-
pus aligned to English with word alignment and
English trees. We also propose a novel, error-
tolerant tree-projecting algorithm, which dynam-
ically searches the project Chinese tree that has
the largest consistency with the corresponding En-
glish tree, according to an alignment matrix rather
than a single alignment. Experiments show that
when the target corpus is smaller, the projected
Chinese treebank, although with inevitable noise
caused by non-literal translation and word align-
ment error, can be successfully utilized and re-
sult in significant improvement over the baseline
model trained on the target corpus only.
In the rest of the paper, we first present the tree-
projecting algorithm (section 2), and then the an-
notation adaptation strategy (section 3). After dis-
cussing the related work (section 4) we show the
experiments (section 5).
</bodyText>
<sectionHeader confidence="0.9896535" genericHeader="method">
2 Error-Tolerant Tree-Projecting
Algorithm
</sectionHeader>
<bodyText confidence="0.989863">
Previous works making use of projected cor-
pus usually adopt the direct-mapping method for
structure projection (Yarowsky and Ngai, 2001;
Hwa et al., 2005; Ganchev et al., 2009), where
</bodyText>
<page confidence="0.971783">
25
</page>
<bodyText confidence="0.979022045454545">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25–28,
Paris, October 2009. c�2009 Association for Computational Linguistics
some filtering is needed to eliminate the inaccurate
or conflicting labels or dependency edges. Here
we propose a more robust algorithm for depen-
dency tree projection. According to the align-
ment matrix, this algorithm dynamically searches
the projected Chinese dependency tree which has
the largest consistency with the corresponding En-
glish tree.
We briefly introduce the alignment matrix be-
fore describing our projecting algorithm. Given
a Chinese sentence C1:M and its English transla-
tion E1:N, the alignment matrix A is an M x N
matrix with each element Ai,j denoting the proba-
bility of Chinese word Ci aligned to English word
Ej. Such structure potentially encodes many more
possible alignments.
Using C(TC|TE, A) to denote the degree of Chi-
nese tree TC being consistent with English tree TE
according to alignment matrix A, the projecting al-
gorithm aims to find
</bodyText>
<equation confidence="0.972239333333333">
ˆTC = argmax C(TC|TE,A) (1)
TC
C(TC|TE, A) can be factorized into each depen-
dency edge x → y in TC, that is to say
C(TC|TE, A) = 11 Ce(x → y|TE,A) (2)
x—yETC
</equation>
<bodyText confidence="0.9267705">
We can obtain Ce by simple accumulation across
all possible alignments
</bodyText>
<equation confidence="0.978995">
Ce(x → y|TE,A)
�= Ax,x′ x Ay,y′ x δ(x′, y′|TE) (3)
1&lt;x′,y′&lt;|E|
</equation>
<bodyText confidence="0.999947285714286">
where δ(x′, y′|TE) is a 0-1 function that equals 1
only if x′ → y′ exists in TE.
The searching procedure, argmax operation in
equation 1, can be effectively solved by a simple,
bottom-up dynamic algorithm with cube-pruning
speed-up (Huang and Chiang, 2005). We omit the
detailed algorithm here due to space restrictions.
</bodyText>
<sectionHeader confidence="0.936682" genericHeader="method">
3 Annotation Adaptation for
</sectionHeader>
<subsectionHeader confidence="0.561601">
Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999996">
The automatic annotation adaptation strategy for
sequence labeling (Jiang et al., 2009) aims to
strengthen a tagger trained on a corpus annotated
in one annotation standard with a larger assistant
corpus annotated in another standard. We can de-
fine the purpose of the automatic annotation adap-
tation for dependency parsing in the same way.
Similar to that in sequence labeling, the train-
ing corpus with the desired annotation standard is
called the target corpus while the assistant cor-
pus annotated in a different standard is called
the source corpus. For training, an intermediate
parser, called the source parser, is trained directly
on the source corpus and then used to parse the tar-
get corpus. After that a second parser, called the
target parser, is trained on the target corpus with
guide features extracted from the source parser’s
parsing results. For testing, a token sequence is
first parsed by the source parser to obtain an inter-
mediate parsing result with the source annotation
standard, and then parsed by the target parser with
the guide features extracted from the intermediate
parsing result to obtain the final result.
The design of the guide features is the most im-
portant, and is specific to the parsing algorithm of
the target parser. In this work we adopt the max-
imum spanning tree (MST) algorithm (McDon-
ald et al., 2005; McDonald and Pereira, 2006) for
both the source and the target parser, so the guide
features should be defined on dependency edges
in accordance with the edge-factored property of
MST models. In the decoding procedure of the
target parser, the degree of a dependency edge be-
ing supported can be adjusted by the relationship
between this edge’s head and modifier in the in-
termediate parsing result of the source parser. The
most intuitionistic relationship is whether the de-
pendency between head and modifier exists in this
intermediate result. Such a bi-valued relationship
is similar to that in the stacking method for com-
bining dependency parsers (Martins et al., 2008;
Nivre and McDonald, 2008). The guide features
are then defined as this relationship itself as well as
its combinations with the lexical features of MST
models.
Furthermore, in order to explore more de-
tailed knowledge from the source parser, we re-
define the relationship as a four-valued variable
which covers the following situations: parent-
child, child-parent, siblings and else. With the
guide features, the parameter tuning procedure of
the target parser will automatically learn the regu-
larity of using the source parser’s intermediate re-
sult to guide its decision making.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="method">
4 Related Works
</sectionHeader>
<bodyText confidence="0.930337">
Many works have been devoted to obtain pars-
ing knowledge from word aligned bilingual cor-
</bodyText>
<page confidence="0.980126">
26
</page>
<bodyText confidence="0.999817318181818">
pora. (L¨u et al., 2002) learns Chinese bracket-
ing knowledge via ITG alignment; (Hwa et al.,
2005) and (Ganchev et al., 2009) induces depen-
dency grammar via projection from aligned En-
glish, where some filtering is used to reduce the
noise and some hand-designed rules to handle lan-
guage heterogeneity.
Just recently, Smith and Eisner (2009) gave
an idea similar to ours. They perform depen-
dency projection and annotation adaptation with
Quasi-Synchronous Grammar (QG) Features. Al-
though both related to projection and annotation,
there are still important differences between these
two works. First, we design an error-tolerant
alignment-matrix-based tree-projecting algorithm
to perform whole-tree projection, while they re-
sort to QG features to score local configurations
of aligned source and target trees. Second, their
adaptation emphasizes to transform a tree from
one annotation standard to another, while our
adaptation emphasizes to strengthen the parser us-
ing a treebank annotated in a different standard.
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999756888888889">
The source corpus for annotation adaptation, that
is, the projected Chinese treebank, is derived from
5.6 millions LDC Chinese-English sentence pairs.
The Chinese side of the bilingual corpus is word-
segmented and POS-tagged by an implementation
of (Jiang et al., 2008), and the English sentences
are parsed by an implementation of (McDonald
and Pereira, 2006) which is instead trained on WSJ
section of Penn English Treebank (Marcus et al.,
1993). The alignment matrixes for sentence pairs
are obtained according to (Liu et al., 2009). The
English trees are then projected across to Chinese
using the algorithm in section 2. Out of these pro-
jected trees, we only select 500 thousands with
word count l s.t. 6 &lt; l &lt; 100 and with project-
ing confidence c = C(TC|TE, A)&apos;/l s.t. c ≥ 0.35.
While for the target corpus, we take Penn Chinese
Treebank (CTB) 1.0 and CTB 5.0 (Xue et al.,
2005) respectively, and follow the traditional cor-
pus splitting: chapters 271-300 for testing, chap-
ters 301-325 for development, and else for train-
ing.
We adopt the 2nd-order MST model (McDon-
ald et al., 2005) as the target parser for better
performance, and the 1st-order MST model as
the source parser for fast training. Both the two
parsers are trained with averaged perceptron algo-
</bodyText>
<table confidence="0.973896">
Model P% on CTB 1 P% on CTB 5
source parser 53.28 53.28
target parser 83.56 87.34
baseline parser 82.23 87.15
</table>
<tableCaption confidence="0.986111">
Table 1: Performances of annotation adaptation
</tableCaption>
<bodyText confidence="0.967326333333333">
with CTB 1.0 and CTB 5.0 as the target corpus re-
spectively, as well as of the baseline parsers (2nd-
order MST parsers trained on the target corpora).
</bodyText>
<figure confidence="0.968567">
100 1000 10000
sentence count of target corpus
</figure>
<figureCaption confidence="0.9485115">
Figure 1: Performance of the target parsers with
target corpora of different scales.
</figureCaption>
<bodyText confidence="0.994020137931035">
rithm (Collins, 2002). The development set of
CTB is also used to determine the best model for
the source parser, conditioned on the hypothesis
of larger isomorphisme between Chinese and En-
glish.
Table 1 shows that the experimental results of
annotation adaptation, with CTB 1.0 and CTB 5.0
as the target corpus respectively. We can see that
the source parsers, directly trained on the source
corpora of projected trees, performs poorly on
both CTB test sets (which are in fact the same).
This is partly due to the noise in the projected tree-
bank, and partly due to the heterogeneous between
the CTB trees and the projected trees. On the
contrary, automatic annotation adaptation effec-
tively transfers the knowledge to the target parsers,
achieving improvement on both target corpora.
Especially on CTB 1.0, an accuracy increment of
1.3 points is obtained over the baseline parser.
We observe that for the much larger CTB 5.0,
the performance of annotation adaptation is much
lower. To further investigate the adaptation perfor-
mances with target corpora of different scales, we
conduct annotation adaptation on a series of tar-
get corpora which consist of different amount of
dependency trees from CTB 5.0. Curves in Fig-
ure 1 shows the experimental results. We see that
the smaller the training corpus is, the more signif-
icant improvement can be obtained. For example,
</bodyText>
<figure confidence="0.996793428571428">
dependency accuracy
0.85
0.75
0.8
0.7
baseline
target parser
</figure>
<page confidence="0.995844">
27
</page>
<bodyText confidence="0.9901325">
with a target corpus composed of 2K trees, nearly
2 points of accuracy increment is achieved. This
is a good news to the resource-scarce languages.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering, volume 11, pages
311–325.
</bodyText>
<sectionHeader confidence="0.781005" genericHeader="conclusions">
6 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999981736842106">
This paper describes for dependency parsing an
automatic annotation adaptation strategy. What
is more important, we use a projected treebank,
rather than a hand-annotated one, as the source
corpus for adaptation. This is quite different from
previous works on projected trees (Hwa et al.,
2005; Ganchev et al., 2009), and is also more valu-
able than previous works of annotation adaptation
(Jiang et al., 2009). Experiments show that this
strategy gains improvement over baseline parsers
with target corpora of different scales, especially
the smaller ones. This provides a new strategy for
resource-scarce languages to train high-precision
dependency parsers. In the future, we will adapt
this strategy to constituent parsing, which is more
challenging and interesting due to the complexity
of projection between constituent trees, and due
to the obscurity of annotation adaptation for con-
stituent parsing.
</bodyText>
<sectionHeader confidence="0.964107" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999795625">
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are grateful to the anony-
mous reviewers for their valuable suggestions. We
also thank Yang Liu for sharing his codes of align-
ment matrix generation, and Liang Huang and
Haitao Mi for helpful discussions.
</bodyText>
<sectionHeader confidence="0.980529" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.66704125">
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1–8, Philadelphia, USA.
</bodyText>
<reference confidence="0.998795051724138">
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53–64.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Wenbin Jiang, Liang Huang, Yajuan L¨u, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging–a case study. In
Proceedings of the 47th ACL.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
Yajuan L¨u, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002. Learning chinese bracketing knowledge
based on a bilingual language model. In Proceed-
ings of the COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings ofEMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings ofACL, pages 91–
98.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings ofACL.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of COLING.
David Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of the NAACL.
</reference>
<page confidence="0.999069">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.626703">
<title confidence="0.990802">Automatic Adaptation of Annotation Standards for Dependency Parsing — Using Projected Treebank as Source Corpus</title>
<author confidence="0.92589">Wenbin Jiang</author>
<author confidence="0.92589">Qun Liu</author>
<affiliation confidence="0.8731515">Key Lab. of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.933838">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<abstract confidence="0.999182333333333">We describe for dependency parsing an annotation adaptation strategy, which can automatically transfer the knowledge from corpus a different annotastandard to the desired the supervision by a corpus annotated in the desired standard. Furthermore, instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus. This benefits the resource-scarce languages which haven’t different handannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="2236" citStr="Ganchev et al., 2009" startWordPosition="341" endWordPosition="344">more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across to their translations, and the projected trees can be leveraged to boost parsing. Many efforts are devoted to the research on projected treebanks, such as (L¨u et al., 2002), (Hwa et al., 2005) and (Ganchev et al., 2009), etc. Considering the fact that a projected treebank partially inherits the English annotation standard, some hand-written rules are designed to deal with the divergence between languages such as in (Hwa et al., 2002). However, it will be more valuable and interesting to adapt this divergence automatically and boost the existing parsers with this projected treebank. In this paper, we investigate the automatic annotation adaptation strategy for Chinese dependency parsing, where the source corpus for adaptation is a projected treebank derived from a bilingual corpus aligned to English with word</context>
<context position="3869" citStr="Ganchev et al., 2009" startWordPosition="592" endWordPosition="595"> non-literal translation and word alignment error, can be successfully utilized and result in significant improvement over the baseline model trained on the target corpus only. In the rest of the paper, we first present the treeprojecting algorithm (section 2), and then the annotation adaptation strategy (section 3). After discussing the related work (section 4) we show the experiments (section 5). 2 Error-Tolerant Tree-Projecting Algorithm Previous works making use of projected corpus usually adopt the direct-mapping method for structure projection (Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009), where 25 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25–28, Paris, October 2009. c�2009 Association for Computational Linguistics some filtering is needed to eliminate the inaccurate or conflicting labels or dependency edges. Here we propose a more robust algorithm for dependency tree projection. According to the alignment matrix, this algorithm dynamically searches the projected Chinese dependency tree which has the largest consistency with the corresponding English tree. We briefly introduce the alignment matrix before describing our projecting al</context>
<context position="8399" citStr="Ganchev et al., 2009" startWordPosition="1334" endWordPosition="1337">lore more detailed knowledge from the source parser, we redefine the relationship as a four-valued variable which covers the following situations: parentchild, child-parent, siblings and else. With the guide features, the parameter tuning procedure of the target parser will automatically learn the regularity of using the source parser’s intermediate result to guide its decision making. 4 Related Works Many works have been devoted to obtain parsing knowledge from word aligned bilingual cor26 pora. (L¨u et al., 2002) learns Chinese bracketing knowledge via ITG alignment; (Hwa et al., 2005) and (Ganchev et al., 2009) induces dependency grammar via projection from aligned English, where some filtering is used to reduce the noise and some hand-designed rules to handle language heterogeneity. Just recently, Smith and Eisner (2009) gave an idea similar to ours. They perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar (QG) Features. Although both related to projection and annotation, there are still important differences between these two works. First, we design an error-tolerant alignment-matrix-based tree-projecting algorithm to perform whole-tree projection, while they res</context>
<context position="13138" citStr="Ganchev et al., 2009" startWordPosition="2098" endWordPosition="2101">is achieved. This is a good news to the resource-scarce languages. Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. In Natural Language Engineering, volume 11, pages 311–325. 6 Conclusion and Future Works This paper describes for dependency parsing an automatic annotation adaptation strategy. What is more important, we use a projected treebank, rather than a hand-annotated one, as the source corpus for adaptation. This is quite different from previous works on projected trees (Hwa et al., 2005; Ganchev et al., 2009), and is also more valuable than previous works of annotation adaptation (Jiang et al., 2009). Experiments show that this strategy gains improvement over baseline parsers with target corpora of different scales, especially the smaller ones. This provides a new strategy for resource-scarce languages to train high-precision dependency parsers. In the future, we will adapt this strategy to constituent parsing, which is more challenging and interesting due to the complexity of projection between constituent trees, and due to the obscurity of annotation adaptation for constituent parsing. Acknowled</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the IWPT,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="5460" citStr="Huang and Chiang, 2005" startWordPosition="854" endWordPosition="857"> TE according to alignment matrix A, the projecting algorithm aims to find ˆTC = argmax C(TC|TE,A) (1) TC C(TC|TE, A) can be factorized into each dependency edge x → y in TC, that is to say C(TC|TE, A) = 11 Ce(x → y|TE,A) (2) x—yETC We can obtain Ce by simple accumulation across all possible alignments Ce(x → y|TE,A) �= Ax,x′ x Ay,y′ x δ(x′, y′|TE) (3) 1&lt;x′,y′&lt;|E| where δ(x′, y′|TE) is a 0-1 function that equals 1 only if x′ → y′ exists in TE. The searching procedure, argmax operation in equation 1, can be effectively solved by a simple, bottom-up dynamic algorithm with cube-pruning speed-up (Huang and Chiang, 2005). We omit the detailed algorithm here due to space restrictions. 3 Annotation Adaptation for Dependency Parsing The automatic annotation adaptation strategy for sequence labeling (Jiang et al., 2009) aims to strengthen a tagger trained on a corpus annotated in one annotation standard with a larger assistant corpus annotated in another standard. We can define the purpose of the automatic annotation adaptation for dependency parsing in the same way. Similar to that in sequence labeling, the training corpus with the desired annotation standard is called the target corpus while the assistant corpu</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the IWPT, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Okan Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2454" citStr="Hwa et al., 2002" startWordPosition="375" endWordPosition="378"> difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across to their translations, and the projected trees can be leveraged to boost parsing. Many efforts are devoted to the research on projected treebanks, such as (L¨u et al., 2002), (Hwa et al., 2005) and (Ganchev et al., 2009), etc. Considering the fact that a projected treebank partially inherits the English annotation standard, some hand-written rules are designed to deal with the divergence between languages such as in (Hwa et al., 2002). However, it will be more valuable and interesting to adapt this divergence automatically and boost the existing parsers with this projected treebank. In this paper, we investigate the automatic annotation adaptation strategy for Chinese dependency parsing, where the source corpus for adaptation is a projected treebank derived from a bilingual corpus aligned to English with word alignment and English trees. We also propose a novel, errortolerant tree-projecting algorithm, which dynamically searches the project Chinese tree that has the largest consistency with the corresponding English tree, </context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2002. Evaluating translational correspondence using annotation projection. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Jiang, Huang, L¨u, Liu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Yajuan L¨u, and Qun Liu. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="1080" citStr="Jiang et al., 2009" startWordPosition="157" endWordPosition="160"> with a different annotation standard to the desired target parser, with the supervision by a target corpus annotated in the desired standard. Furthermore, instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus. This benefits the resource-scarce languages which haven’t different handannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 1 Introduction Automatic annotation adaptation for sequence labeling (Jiang et al., 2009) aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard. It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank (Marcus et al., 1993) and HPSG LinGo Redwoods Treebank (Oepen et al., 2002) for English. However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce l</context>
<context position="5659" citStr="Jiang et al., 2009" startWordPosition="882" endWordPosition="885"> 11 Ce(x → y|TE,A) (2) x—yETC We can obtain Ce by simple accumulation across all possible alignments Ce(x → y|TE,A) �= Ax,x′ x Ay,y′ x δ(x′, y′|TE) (3) 1&lt;x′,y′&lt;|E| where δ(x′, y′|TE) is a 0-1 function that equals 1 only if x′ → y′ exists in TE. The searching procedure, argmax operation in equation 1, can be effectively solved by a simple, bottom-up dynamic algorithm with cube-pruning speed-up (Huang and Chiang, 2005). We omit the detailed algorithm here due to space restrictions. 3 Annotation Adaptation for Dependency Parsing The automatic annotation adaptation strategy for sequence labeling (Jiang et al., 2009) aims to strengthen a tagger trained on a corpus annotated in one annotation standard with a larger assistant corpus annotated in another standard. We can define the purpose of the automatic annotation adaptation for dependency parsing in the same way. Similar to that in sequence labeling, the training corpus with the desired annotation standard is called the target corpus while the assistant corpus annotated in a different standard is called the source corpus. For training, an intermediate parser, called the source parser, is trained directly on the source corpus and then used to parse the ta</context>
<context position="13231" citStr="Jiang et al., 2009" startWordPosition="2114" endWordPosition="2117">Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. In Natural Language Engineering, volume 11, pages 311–325. 6 Conclusion and Future Works This paper describes for dependency parsing an automatic annotation adaptation strategy. What is more important, we use a projected treebank, rather than a hand-annotated one, as the source corpus for adaptation. This is quite different from previous works on projected trees (Hwa et al., 2005; Ganchev et al., 2009), and is also more valuable than previous works of annotation adaptation (Jiang et al., 2009). Experiments show that this strategy gains improvement over baseline parsers with target corpora of different scales, especially the smaller ones. This provides a new strategy for resource-scarce languages to train high-precision dependency parsers. In the future, we will adapt this strategy to constituent parsing, which is more challenging and interesting due to the complexity of projection between constituent trees, and due to the obscurity of annotation adaptation for constituent parsing. Acknowledgement This project was supported by National Natural Science Foundation of China, Contracts </context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Tian Xia</author>
<author>Xinyan Xiao</author>
<author>Qun Liu</author>
</authors>
<title>Weighted alignment matrices for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="9838" citStr="Liu et al., 2009" startWordPosition="1550" endWordPosition="1553">gthen the parser using a treebank annotated in a different standard. 5 Experiments The source corpus for annotation adaptation, that is, the projected Chinese treebank, is derived from 5.6 millions LDC Chinese-English sentence pairs. The Chinese side of the bilingual corpus is wordsegmented and POS-tagged by an implementation of (Jiang et al., 2008), and the English sentences are parsed by an implementation of (McDonald and Pereira, 2006) which is instead trained on WSJ section of Penn English Treebank (Marcus et al., 1993). The alignment matrixes for sentence pairs are obtained according to (Liu et al., 2009). The English trees are then projected across to Chinese using the algorithm in section 2. Out of these projected trees, we only select 500 thousands with word count l s.t. 6 &lt; l &lt; 100 and with projecting confidence c = C(TC|TE, A)&apos;/l s.t. c ≥ 0.35. While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 (Xue et al., 2005) respectively, and follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. We adopt the 2nd-order MST model (McDonald et al., 2005) as the target parser for better performance, an</context>
</contexts>
<marker>Liu, Xia, Xiao, Liu, 2009</marker>
<rawString>Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009. Weighted alignment matrices for statistical machine translation. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan L¨u</author>
<author>Sheng Li</author>
<author>Tiejun Zhao</author>
<author>Muyun Yang</author>
</authors>
<title>Learning chinese bracketing knowledge based on a bilingual language model.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<marker>L¨u, Li, Zhao, Yang, 2002</marker>
<rawString>Yajuan L¨u, Sheng Li, Tiejun Zhao, and Muyun Yang. 2002. Learning chinese bracketing knowledge based on a bilingual language model. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="1423" citStr="Marcus et al., 1993" startWordPosition="211" endWordPosition="214">ndannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 1 Introduction Automatic annotation adaptation for sequence labeling (Jiang et al., 2009) aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard. It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank (Marcus et al., 1993) and HPSG LinGo Redwoods Treebank (Oepen et al., 2002) for English. However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across to thei</context>
<context position="9750" citStr="Marcus et al., 1993" startWordPosition="1536" endWordPosition="1539">rm a tree from one annotation standard to another, while our adaptation emphasizes to strengthen the parser using a treebank annotated in a different standard. 5 Experiments The source corpus for annotation adaptation, that is, the projected Chinese treebank, is derived from 5.6 millions LDC Chinese-English sentence pairs. The Chinese side of the bilingual corpus is wordsegmented and POS-tagged by an implementation of (Jiang et al., 2008), and the English sentences are parsed by an implementation of (McDonald and Pereira, 2006) which is instead trained on WSJ section of Penn English Treebank (Marcus et al., 1993). The alignment matrixes for sentence pairs are obtained according to (Liu et al., 2009). The English trees are then projected across to Chinese using the algorithm in section 2. Out of these projected trees, we only select 500 thousands with word count l s.t. 6 &lt; l &lt; 100 and with projecting confidence c = C(TC|TE, A)&apos;/l s.t. c ≥ 0.35. While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 (Xue et al., 2005) respectively, and follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. We adopt the 2nd</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="7588" citStr="Martins et al., 2008" startWordPosition="1203" endWordPosition="1206">d the target parser, so the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonald, 2008). The guide features are then defined as this relationship itself as well as its combinations with the lexical features of MST models. Furthermore, in order to explore more detailed knowledge from the source parser, we redefine the relationship as a four-valued variable which covers the following situations: parentchild, child-parent, siblings and else. With the guide features, the parameter tuning procedure of the target parser will automatically learn the regularity of using the source parser’s intermediate result to guide its decision making. 4 Related Works Many </context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="6945" citStr="McDonald and Pereira, 2006" startWordPosition="1098" endWordPosition="1101">parser, is trained on the target corpus with guide features extracted from the source parser’s parsing results. For testing, a token sequence is first parsed by the source parser to obtain an intermediate parsing result with the source annotation standard, and then parsed by the target parser with the guide features extracted from the intermediate parsing result to obtain the final result. The design of the guide features is the most important, and is specific to the parsing algorithm of the target parser. In this work we adopt the maximum spanning tree (MST) algorithm (McDonald et al., 2005; McDonald and Pereira, 2006) for both the source and the target parser, so the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in the stacking method for combini</context>
<context position="9663" citStr="McDonald and Pereira, 2006" startWordPosition="1521" endWordPosition="1524">figurations of aligned source and target trees. Second, their adaptation emphasizes to transform a tree from one annotation standard to another, while our adaptation emphasizes to strengthen the parser using a treebank annotated in a different standard. 5 Experiments The source corpus for annotation adaptation, that is, the projected Chinese treebank, is derived from 5.6 millions LDC Chinese-English sentence pairs. The Chinese side of the bilingual corpus is wordsegmented and POS-tagged by an implementation of (Jiang et al., 2008), and the English sentences are parsed by an implementation of (McDonald and Pereira, 2006) which is instead trained on WSJ section of Penn English Treebank (Marcus et al., 1993). The alignment matrixes for sentence pairs are obtained according to (Liu et al., 2009). The English trees are then projected across to Chinese using the algorithm in section 2. Out of these projected trees, we only select 500 thousands with word count l s.t. 6 &lt; l &lt; 100 and with projecting confidence c = C(TC|TE, A)&apos;/l s.t. c ≥ 0.35. While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 (Xue et al., 2005) respectively, and follow the traditional corpus splitting: chapters 271-300</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="6916" citStr="McDonald et al., 2005" startWordPosition="1093" endWordPosition="1097">ser, called the target parser, is trained on the target corpus with guide features extracted from the source parser’s parsing results. For testing, a token sequence is first parsed by the source parser to obtain an intermediate parsing result with the source annotation standard, and then parsed by the target parser with the guide features extracted from the intermediate parsing result to obtain the final result. The design of the guide features is the most important, and is specific to the parsing algorithm of the target parser. In this work we adopt the maximum spanning tree (MST) algorithm (McDonald et al., 2005; McDonald and Pereira, 2006) for both the source and the target parser, so the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in th</context>
<context position="10390" citStr="McDonald et al., 2005" startWordPosition="1650" endWordPosition="1654">trixes for sentence pairs are obtained according to (Liu et al., 2009). The English trees are then projected across to Chinese using the algorithm in section 2. Out of these projected trees, we only select 500 thousands with word count l s.t. 6 &lt; l &lt; 100 and with projecting confidence c = C(TC|TE, A)&apos;/l s.t. c ≥ 0.35. While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 (Xue et al., 2005) respectively, and follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. We adopt the 2nd-order MST model (McDonald et al., 2005) as the target parser for better performance, and the 1st-order MST model as the source parser for fast training. Both the two parsers are trained with averaged perceptron algoModel P% on CTB 1 P% on CTB 5 source parser 53.28 53.28 target parser 83.56 87.34 baseline parser 82.23 87.15 Table 1: Performances of annotation adaptation with CTB 1.0 and CTB 5.0 as the target corpus respectively, as well as of the baseline parsers (2ndorder MST parsers trained on the target corpora). 100 1000 10000 sentence count of target corpus Figure 1: Performance of the target parsers with target corpora of diff</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91– 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7615" citStr="Nivre and McDonald, 2008" startWordPosition="1207" endWordPosition="1210">o the guide features should be defined on dependency edges in accordance with the edge-factored property of MST models. In the decoding procedure of the target parser, the degree of a dependency edge being supported can be adjusted by the relationship between this edge’s head and modifier in the intermediate parsing result of the source parser. The most intuitionistic relationship is whether the dependency between head and modifier exists in this intermediate result. Such a bi-valued relationship is similar to that in the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonald, 2008). The guide features are then defined as this relationship itself as well as its combinations with the lexical features of MST models. Furthermore, in order to explore more detailed knowledge from the source parser, we redefine the relationship as a four-valued variable which covers the following situations: parentchild, child-parent, siblings and else. With the guide features, the parameter tuning procedure of the target parser will automatically learn the regularity of using the source parser’s intermediate result to guide its decision making. 4 Related Works Many works have been devoted to </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christopher Manning Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The lingo redwoods treebank: Motivation and preliminary applications. In</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1477" citStr="Oepen et al., 2002" startWordPosition="220" endWordPosition="223"> parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 1 Introduction Automatic annotation adaptation for sequence labeling (Jiang et al., 2009) aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard. It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank (Marcus et al., 1993) and HPSG LinGo Redwoods Treebank (Oepen et al., 2002) for English. However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks. Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English. According to the word alignment, the English parses can be projected across to their translations, and the projected trees can be leverag</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Flickinger, Brants, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning Dan Flickinger, and Thorsten Brants. 2002. The lingo redwoods treebank: Motivation and preliminary applications. In In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8614" citStr="Smith and Eisner (2009)" startWordPosition="1368" endWordPosition="1371">res, the parameter tuning procedure of the target parser will automatically learn the regularity of using the source parser’s intermediate result to guide its decision making. 4 Related Works Many works have been devoted to obtain parsing knowledge from word aligned bilingual cor26 pora. (L¨u et al., 2002) learns Chinese bracketing knowledge via ITG alignment; (Hwa et al., 2005) and (Ganchev et al., 2009) induces dependency grammar via projection from aligned English, where some filtering is used to reduce the noise and some hand-designed rules to handle language heterogeneity. Just recently, Smith and Eisner (2009) gave an idea similar to ours. They perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar (QG) Features. Although both related to projection and annotation, there are still important differences between these two works. First, we design an error-tolerant alignment-matrix-based tree-projecting algorithm to perform whole-tree projection, while they resort to QG features to score local configurations of aligned source and target trees. Second, their adaptation emphasizes to transform a tree from one annotation standard to another, while our adaptation emphasizes t</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="10187" citStr="Xue et al., 2005" startWordPosition="1618" endWordPosition="1621">8), and the English sentences are parsed by an implementation of (McDonald and Pereira, 2006) which is instead trained on WSJ section of Penn English Treebank (Marcus et al., 1993). The alignment matrixes for sentence pairs are obtained according to (Liu et al., 2009). The English trees are then projected across to Chinese using the algorithm in section 2. Out of these projected trees, we only select 500 thousands with word count l s.t. 6 &lt; l &lt; 100 and with projecting confidence c = C(TC|TE, A)&apos;/l s.t. c ≥ 0.35. While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 (Xue et al., 2005) respectively, and follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. We adopt the 2nd-order MST model (McDonald et al., 2005) as the target parser for better performance, and the 1st-order MST model as the source parser for fast training. Both the two parsers are trained with averaged perceptron algoModel P% on CTB 1 P% on CTB 5 source parser 53.28 53.28 target parser 83.56 87.34 baseline parser 82.23 87.15 Table 1: Performances of annotation adaptation with CTB 1.0 and CTB 5.0 as the target corpus respectively, as w</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL.</booktitle>
<contexts>
<context position="3828" citStr="Yarowsky and Ngai, 2001" startWordPosition="584" endWordPosition="587">k, although with inevitable noise caused by non-literal translation and word alignment error, can be successfully utilized and result in significant improvement over the baseline model trained on the target corpus only. In the rest of the paper, we first present the treeprojecting algorithm (section 2), and then the annotation adaptation strategy (section 3). After discussing the related work (section 4) we show the experiments (section 5). 2 Error-Tolerant Tree-Projecting Algorithm Previous works making use of projected corpus usually adopt the direct-mapping method for structure projection (Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009), where 25 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25–28, Paris, October 2009. c�2009 Association for Computational Linguistics some filtering is needed to eliminate the inaccurate or conflicting labels or dependency edges. Here we propose a more robust algorithm for dependency tree projection. According to the alignment matrix, this algorithm dynamically searches the projected Chinese dependency tree which has the largest consistency with the corresponding English tree. We briefly introduce the alignment m</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In Proceedings of the NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>