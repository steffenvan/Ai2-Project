<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001938">
<title confidence="0.927797">
Document Filtering Using Semantic Information
from a Machine Readable Dictionaryl
</title>
<author confidence="0.942737">
Elizabeth D. Liddy, Woojin Paik
</author>
<affiliation confidence="0.9723862">
School of Information Studies
Syracuse University
Edmund S. Yu
College of Engineering and Computer Science
Syracuse University
</affiliation>
<sectionHeader confidence="0.62242" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.964713742857143">
Large scale information retrieval systems need to refine the flow of documents which will receive further
fine-grain analysis to those documents with a high potential for relevance to their respective users. This
paper reports on research we have conducted into the usefulness of semantic codes from a machine
readable dictionary for filtering large sets of incoming documents for their broad subject appropriateness
to a topic of interest. The Subject Field Coder produces a summary-level semantic representation of a
text&apos;s contents by tagging each word in the document with the appropriate, disambiguated Subject Field
Code (SFC). The within-document SFCs are normalized to produce a vector of the SFCs representing that
document&apos;s contents. Queries are likewise represented as SFC vectors and then compared to SFC vectors of
incoming documents, which are then ranked according to similarity to the query SFC vector. Only those
documents whose SFC vectors exhibit a predetermined degree of similarity to the query SFC vector are
passed to later system components for more refined representation and matching. The assignment of SFCs
is fully automatic, efficient and has been empirically tested as a reasonable approach for ranking documents
from a very large incoming flow of documents. We report details of the implementation, as well as results
of an empirical testing of the Subject Field Coder on fifty queries.
1, information Filtering
Two realities regarding the current context of information retrieval motivate the research herein
reported: 1) Document collections from which individuals need to receive and/or retrieve relevant
information are immense in size and only likely to increase; 2) Given the size of both the daily influx of
documents and the document databases in which the daily input is then stored, a finer level of representa-
tion of both information needs and documents is necessary in order to ensure higher precision results.
Although precision has always been a concern in information retrieval, the problem assumes new signifi-
cance when low precision translates into thousands of non-relevant documents that each user must peruse.
Improved precision can be achieved by using a more conceptual level of representation of documents and
queries, so that the system provides to the user documents containing their concepts of interest, not just
the user&apos;s keywords. However, this level of analysis is computationally expensive and not reasonable to
perform on documents that are unlikely to be relevant. Therefore, preliminary filtering of documents in an
information retrieval system would permit later, finer levels of text analysis to be more efficiently
applied to a smaller subset of documents.
This suggests the view that information retrieval be approached as a multi-stage filtering process, with
the types and optimal number of filterings dependent on both the size of the document collection and the
desired granularity of filtering. We believe that intelligent filtering is needed in document detection
applications, where millions of documents are received daily by an organization, while only a relatively
small subset of documents is of potential interest to any individual user. Furthermore, we believe that a
1 support for this research was provided by DARPA Contract #91-F136100-00 under the auspices
of the TIPSTER Project.
</bodyText>
<page confidence="0.990942">
20
</page>
<bodyText confidence="0.999821466666667">
purely content-based document filter would be useful in delineating a subject-appropriate preliminary set
of documents for each user on which the system would then perform finer levels of analysis and matching.
The notion of filtering as used here, is to be distinguished from one sense of the term currently in use.
Belkin and Croft (1992) define information filtering broadly as &apos;a variety of processes involving the
delivery of information to people who need it&apos; (p.29). Defined as such, the work we are herein reporting,
fits the definition of filtering. However, Belkin and Croft describe a particular application of information
filtering which equates to Selective Dissemination of Information (SDI). This view of filtering is at a finer
grain of matching than our notion of filtering. In an SDI application, filtering is the full matching process,
while we conceive of filtering as a rougher-grain, first stage, topic-area matching. In a one-stage SDI
application, user-profiles may contain facets of description beyond the desired content of useful docu-
ments, whereas our preliminary filter relies solely on topic-based criteria. The goal of our filter is to
efficiently and effectively skim off those documents which possess the greatest likelihood of proving
relevant to a user&apos;s need, here conceived of as a natural language statement of their long-standing
information requirement. Later stages of processing in the system will perform the more refined
conceptual level of matching.
</bodyText>
<sectionHeader confidence="0.998059" genericHeader="categories and subject descriptors">
2. DR-LINK Project
</sectionHeader>
<bodyText confidence="0.89047003030303">
Our ongoing research into the development and implementation of an effective document filter has produced
a module used within a larger document detection system, the DR-LINK System (Liddy &amp; Myaeng, 1993).
The DR-LINK Project is research being conducted under the auspices of DARPA&apos;s TIPSTER Project whose
goal is the development of algorithms both for the detection of documents of interest and the extraction of
selected information from these documents for a large group of users. The DR-LINK system architecture is
modular in design, with six separate processing modules. These modules enhance the documents at every
stage by various semantic enrichments which are used to refine the flow of documents in terms of both
appropriateness to the query and pure numbers. Briefly summarized, the six modules&apos; processing is as
follows:
1) The Subject Field Coder uses semantic word knowledge to produce a summary-level topical
vector representation of a document&apos;s contents that is matched to a vector representation of a
query in order to select for further processing only those documents which have real potential
of being relevant. This subset of documents is then passed to the:
2) The Proper Noun Interpreter, which uses a variety of knowledge bases and context-based
heuristics to recognize, categorize, and standardize every proper noun in the text. The
similarity between a query&apos;s proper noun requirements and each document&apos;s Proper Noun Field
is computed at either the category level or by precise string matching. This similarity value is
combined with the similarity value from the Subject Field Coder for a reranking of all
documents in response to the query. Those documents which exceed an empirically determined
cut-off criterion based on this combined similarity value, are then passed to:
3) The Text Structurer, which sub-divides a text into its discourse-level segments in order to
focus matching on the appropriate discourse component in the documents in response to the
particular requirements of an information need. For example, for queries run against the
newspaper database that are seeking information about a particular possible future event (e.g.
Japanese acquisition of U.S. companies), the Text Structurer matching algorithm will weight
more highly those articles in which mention of the event occurs in an &apos;Expectation&apos; component.
When retrieved, the structured texts, with the appropriate components high-lighted, are passed
to the:
4) Relation-Concept Detector, which raises the level at which we do matching from a key-
word or key-phrase level to a more conceptual level by expanding terms in the query to all
terms which have been shown to be &apos;substitutable&apos; for them, and then by extracting semantic
relations between concepts in both documents and queries. This component produces concept-
relation-concept triples which are passed to the:
</bodyText>
<page confidence="0.459143">
23.
</page>
<listItem confidence="0.98986">
5) Conceptual Graph Generator which converts the triples into the Conceptual Graph (CG)
formalism, a representation similar to semantic networks, but with labelled arcs (Sowa,
1984). The resultant CGs are passed to the:
6) Conceptual Graph Matcher, which measures the degree to which a particular query CG and
candidate document CGs share a common structure, and then produces a final ranking of the
documents.
</listItem>
<bodyText confidence="0.979431533333333">
/
Since the later modules in our system require very complex processing in order to produce conceptually
enriched representations of documents and queries, preliminary filtering of the incoming flow of documents
by means of the Subject Field Coder has proven to be extremely useful. For while CGs enable us to do fine-
grained representation, such fine-grained representation is not necessary in order to determine, for
instance, that a document on &apos;computer games&apos; is not likely to be relevant to a query on &apos;merit pay&apos;.
Therefore, the SFCoder produces a first rough cut of those documents which have real potential for
matching a query as the first of a multi-stage model of retrieval. Because the SFCoder is based on the
implicit semantics of the words in the text, it has the ability to successfully eliminate non-topic relevant
documents during a preliminary stage without the attendant risks of filtering approaches which are based on
less semantically reliable characteristics of documents.
2., Representation Used in Filterino
Subject filtering is a difficult problem, given the richness and variety of natural language. In addition,
imposition of an overly stringent subject filter in too homogenous a document collection runs the risk of
excluding docu-ments which might match the query during a later, finer matching process. This is
particularly true if a lexical or keyword analysis of text is the basis of the filtering. However, if based on
an appropriate semantic representation combined with a reasonable cut-off criterion, a subject-based filter
offers the means of siphoning off from a large heterogenous stream of documents, smaller, more
appropriate sub-collections of documents for various users or user-groups, for which the system then
produces more conceptual representations and performs finer-grain matching.
The success of our filtering approach is attributable to the nature of the representation scheme we use for
every text (whether document or query). The representation of each text is a summary vector of the
Subject Field Codes (SFCs) from Lonaman&apos;s Dictionav of Contemporary Ertglish (LDOCE) representing the
correct sense of each word in the text that is in LDOCE and which has SFCs assigned in LDOCE. For example,
Figure 1 presents a short Wall Street Journal article and a humanly readable version of the normalized SFC
vector which serves as the document&apos;s semantic summary representation.
A U. S. magistrate in Florida ordered Carlos Lehder Rivas, described as among the world&apos;s
leading cocaine traffickers, held without bond on 11 drug-smuggling counts. Lehder, who
was captured last week in Colombia and immediately extradited to the U.S., pleaded innocent
to the charges in federal court in Jacksonville.
</bodyText>
<figure confidence="0.64313875">
LAW .2667 SOCIOLOGY .1333
BUSINESS .1333 ECONOMICS .0667
DRUGS .1333 MILITARY .0667
POLITICAL SCIENCE .1333 OCCUPATIONS .0667
</figure>
<figureCaption confidence="0.982801">
Fig. 1: Sample Wall Street Journal document and its SFC representation
</figureCaption>
<bodyText confidence="0.993821">
As can be seen by reading either the original text or the SFC vector values, the text&apos;s main topic is law,
while the topics of business, drugs, political science and sociology are equally, but less significantly
mentioned. The vector suggests a passing reference to the fields of economics, military, and occupations.
</bodyText>
<page confidence="0.976892">
22
</page>
<bodyText confidence="0.998017911111111">
The system would consider this document relevant to a query whose SFC representation was distributed
proportionately among the same SFCs slots on the vector. The important aspect of this representation is
that a document does not need to include any of the same words that are included in a query in order for a
high similarity to be found between the query and a document, since the matching is based on similarity of
SFC vectors, not according to the particular words used.
Therefore, it can be seen that the SFC representation, which is one level of abstraction above the actual
words in a text, implicitly handles both the synonymy (multiple words having the same meaning) and
polysemy (one word having multiple meanings) problems which have plagued the use of natural language in
information retrieval systems. This level of abstraction is an essential feature of the representation since
it has been shown (Furnas et al, 1987) that users&apos; information requests frequently share little vocabulary
overlap with the documents which actually contain relevant information.
1, Lonaman&apos;s Dictionary of Contemporary EngHet
Our text representation is based on the machine-readable version of Longman&apos;s Dictionary of Contemporary
Enalish (LDOCE), a British-produced learner&apos;s dictionary. The first edition of LDOCE has been used in a
number of investigations into natural language processing applications (Boguraev &amp; Briscoe, 1989). We are
using the second edition (1987) which contains 35,899 headwords and 53,838 senses. The machine-
readable tape of LDOCE contains several fields of information not visible in the hard-copy version which are
extremely useful in natural language processing tasks. Some of these are relevant for syntactic processing,
while others contain semantic information, which indicate the class of entities to which a noun belongs (e.g.
animate, abstract) or the semantic constraints for the arguments of a verb or an adjective, and the SFCs,
which are the basis of our text representation for document filtering.
The SFCs comprise a classification scheme of 124 major fields, based on an earlier classification scheme of
Merriam-Webster. SFCs are manually assigned to words in LDOCE by the Longman lexicographers. There
are two types of problems with the SFCs which we have resolved in order to use them computationally.
First, a particular word may function as more than one part of speech and secondly, if a word has more
than one sense, each of these senses may be tagged in the lexicon with different SFCs. Therefore, in order
for SFCs to provide a reasonable representation of texts, a system must ascertain both the grammatical
function and sense of a word in the text, so that the appropriate SFC for each orthographic form can be
chosen. We have incorporated in our system means for choosing amongst each word&apos;s syntactic categories
and senses found in LDOCE, thereby enabling the system to assign just one SFC to each word in a given text.
In related research, Walker and Amster used the Subject Field Codes to determine the appropriate subject
domains for a set of texts (1986). However, they used the most frequent SFC to characterize a document&apos;s
content, whereas we represent a document by a vector of frequencies of SFCs for words in that text.
Stator (1991) has taken the original 124 SFCs and added an additional layer of seven pragmatic classes to
the original two-level hierarchy. He has found the reconstructed hierarchy useful when attempting to
disambiguate multiple senses and SFCs attached to words. His metric for preferring one sense over another
relies on values within an individual text, whereas we add corpus correlation values as a further stage in
the disambiguation process. Krovetz (1991) has been exploring the effect of combining the evidence from
SFCs with evidence from other fields in LDOCE for selection of a correct word sense. His goal is to
represent documents by their appropriate senses rather than just the orthographic forms of words, for use
in an information retrieval system.
I, Subiect Field Coding of Texts
In the Subject Field Coder, the following stages of processing are done in order to generate a SFC vector
representation of each text:
In Stage 1 processing, we run the documents and query through POST, a probabilistic part of speech
</bodyText>
<page confidence="0.990971">
23
</page>
<bodyText confidence="0.993265181818182">
tagger (Meeter et al, 1991) in order to restrict candidate SFCs of a word to those of just the appropriate
syntactic category of each word as determined by POST.
Stage 2 processing consists of retrieving the SFCs of each word&apos;s correct part of speech from the lexical
database. The SFC retrieval process utilizes the Kelly &amp; Stone (1975) stemming algorithm to reduce
morphological variants to their simple stem as found in LDOCE. Kelly &amp; Stone&apos;s approach is one of weak
stemming and produces correct simple stems for look-up in LDOCE, rather than stripping suffixes.
Having selected candidate SFCs for each word&apos;s correct syntactic category, we begin sense disambiguation
at Stage 3, using sentence-level context-heuristics to determine a single word&apos;s correct SFC. We begin
with context-heuristics because empirical results have shown that local context is used by humans for
sense disambiguation (Choueka &amp; Lusignan, 1985) and context-heuristics have been experimentally tested
in Walker &amp; Amsler&apos;s (1986) and Slator&apos;s work (1991) with promising results. The input to Stage 3 is a
word, its part-of-speech tag, and the SFCs for each sense of that grammatical category. For some words,
no disambiguation may be necessary at this stage because the SFCs for the part-of-speech of the input word
may all be GENERAL or there may be no SFCs provided by LDOCE. However, for the majority of words in
each sentence there are multiple SFCs, so the input would be as seen in Figure 2.
State n POLITICAL SCIENCE4, ORDERS
companies n BUSINESS, MUSIC, THEATER
employ v LABOR, BUSINESS
about adv
one a dj
billion adj NUMBERS
people. n SOCIOLOGY, POLITICAL SCIENCE2, ANTHROPOLOGY
Fig 2: Subject Field Codes &amp; Frequencies (in Superscript) for Part-of-Speech Tagged Words
To select a single SFC for each word in a sentence, Stage 3 uses an ordered set of context-heuristics.
First, the SFCs attached to all words in a sentence are evaluated to determine at the sentence level: 1)
whether any words have only one SFC assigned to all senses of that word, and; 2) the SFCs which are
most frequently assigned across all words in the sentence. Each sentence may have more than one unique
SFC, as there may be more than one word whose senses have all been assigned a single SFC. In Figure 2,
NUMBERS is a unique SFC, being the only SFC assigned to the word billion&apos; and POLITICAL SCIENCE is the
highly frequent SFC for this sentence, being assigned to 6 senses in total. The unique SFCs and the highly
frequent SFCs have proven to be good local determinants of the subject domain of the sentence. We have
established the criterion that if no SFC has a frequency equal to or greater than three, a frequency-based
SFC for that particular sentence is not selected. Erppirical results show that SFCs having a within-sentence
frequency less than three do not accurately represEint the domain of the sentence.
Stage 4 evaluates the remaining words in the sentence, and for some words chooses a single SFC based on
the locally-important SFCs determined in Stage 3. The system scans the SFCs of each remaining word to
determine whether the SFCs which have been identified as unique or highly frequent occur amongst the
multiple SFCs which LDOCE lexicographers have assigned to that word. In Figure 2, for example, POLITICAL
SCIENCE would be selected as the appropriate SFC for &apos;people&apos; and &apos;state&apos; because POLITICAL SCIENCE was
determined in Stage 3 to be the most frequent SFC value for the sentence.
For the ambiguous words which have no SFC in common with the unique or highly frequent SFCs for that
sentence, Stage $ incorporates two global knowledge sources to complete the sense disambiguation task.
The primary source is a 122 x 122 correlation matrix computed from the SFC frequencies of the 442,059
words that occurred in a sample of 977 Wall Street Journal (WSJ) articles. The matrix reflects stable
</bodyText>
<page confidence="0.997242">
24
</page>
<bodyText confidence="0.998226326086957">
estimates of SFCs which co-occur within documents for this text type. The second source is the order in
which the senses of a word are listed in LDOCE. Ordering of senses in LDOCE is determined by Longman&apos;s
lexicographers based on frequency of use in the English language.
The correlation matrix was produced by running SAS on the SFC output of the 977 WSJ articles processed
through Stage 2. At that stage, each document is represented by a vector of SFCs of all senses of the
correct part-of-speech of each word as determined by POST. The observation unit is the document and the
variables being correlated are the 122 SFCs. The scores for the variables are the within-document
frequencies of each SFC. There are 255,709 scores across the 977 articles on which the matrix is
computed. The resulting values in the 122 x 122 matrix are the Pearson product moment correlation
coefficients between SFCs and range from a +1 to a -1, with 0 indicating no relationship between the SFCs.
For example, NET GAMES and COURT GAMES have the highest correlation, with ECONOMICS and BUSINESS
having the next highest correlation.
The matrix is then used in Stage 5, where each of the remaining ambiguous words is resolved a word at a
time, by accessing the matrix via the unique and highly frequent SFCs determined for a sentence in Stage 3.
The system evaluates the correlation coefficients between the unique/highly frequent SFCs of the sentence
and the multiple SFCs assigned to the word being disambiguated to determine which of the multiple SFCs has
the highest correlation with the unique and/or highly frequent SFCs. The system then selects that SFC as
the unambiguous representation of the sense of the word.
We have developed heuristics for three cases for selecting a single SFC for a word using the correlation
matrix. The three cases function better than handling all instances as a single case because of the special
treatment needed for words with the less-substantive GENERAL (XX) code. When XX is amongst the SFCs,
we take order of the SFCs into consideration, reflecting the fact that the first SFC listed is more likely to
be correct, since the most widely used sense is listed first in LDOCE. So, to overcome this likelihood, a
more substantive SFC listed later in the entry must have a much higher correlation with a sentence-
determined SFC in order to be selected over the GENERAL (XX) code.
The system implementation of the disambiguation procedures was tested on a sample of 1638 words from
WSJ having SFCs in LDOCE. The system selected a single SFC for each word, which was compared to the
sense-selections made by an independent judge who was instructed to read the sentences and the definitions
of the senses of each word and then to select that sense which was most correct. Overall, the SFC
disambiguator selected the correct SFC 89% of the time (Liddy &amp; Paik, 1992).
Stage 6 processing produces a vector of SFCs and their frequencies for each document and for the query.
At this point the non-substantive GENERAL SFCs are removed from the vector sums, since these contribute
nothing to a text&apos;s subject content representation.
In Stage 7, the vectors of each text are normalized using Sagees (1976) term weighting formula in order
to control for the effect of document length. The choice of Sager&apos;s (1976) term weighting formula and
Sager and Lockemann&apos;s (1976) similarity measure were based on an extensive study done by McGill, Koll &amp;
Noreault (1979) which empirically evaluated twenty one term-weighting formulae and twenty four
similarity measures. Using the coefficient of ranking effectiveness (CRE) measure, each term weighting
scheme was tested in combination with each similarity measure to determine which combination was best
for either controlled representations or free text representations. Since we consider SFCodes to be a form
of a controlled vocabulary (all free-text terms are reduced to 122 codes), we chose Sagees (1976) term
weighting scheme and Sager &amp; Lockemann&apos;s (1976) similarity measure since they were shown to be the
best formulae for use with controlled vocabulary representation (McGill et al, 1979).
At Stage 8, the document vectors are compared to the query vector using Sager &amp; Lockemann&apos;s (1976)
similarity measure and a ranked listing of the documents in decreasing order of similarity is produced.
Having created this ranked list of documents for each query, the system must determine how many of these
</bodyText>
<page confidence="0.995292">
25
</page>
<bodyText confidence="0.986613333333333">
documents should reasonably be passed on to the next module. The method used is an adaptive cut-off
criterion that predicts for each query and at each recall level, how many documents from the ranked list
should be forwarded. The cut-off criterion uses a multiple regression formula which was developed on
training data consisting of the odd-numbered Topic Statements (queries) from 1 to 50, used in both TIPSTER-
Phase I and TREC-1. The training corpora consisted of Wall Street Journal articles from 1986-1992, a
total of 173,255 separate documents. The regression formula used in Stage 8 is:
SPSV; = e0.9909 - (0.6112 • RL) + (0.5455 STDSVi) - 5
where: SPSV; is the Standardized Predicted Similarity Value
RL is the designated Recall Level
STDSV; is the Standardized Top-Ranked Document Similarity Value, logarithmically
transformed
is the Topic Statement whose cut-off criterion is being predicted
RL and STDSV; significantly predicted SPSVi on the training queries (R = .826, F = 265.42, df= 2,247, p
.0005). Using this standardized value (SPSV;), a linear transformation is used to produce the value which
will be used as the cut-off criterion:
</bodyText>
<equation confidence="0.772832">
PSV; = (SPSV; • s.d.i) + mean;
</equation>
<bodyText confidence="0.995435833333333">
where: PSVi is the Predicted Similarity Value
s.d.1 standard deviation
mean; mean
The PSVi is used by the system to establish the cut-off criterion for each recall level for each query. The
averaged results of the testing of the PSV; using the held-out, even-numbered Topic Statements are
provided in Table 1.
</bodyText>
<table confidence="0.999542538461538">
A. B. C. D.
Recall level Actual % of % of DB searched Recall level
DB searched based on PSV based on PSV
0.10 1.27 0.50 0.20
0.20 2.67 0.98 0.28
0.30 4.42 2.51 0.39
0.40 6.55 5.24 0.50
0.50 8.46 8.90 0.61
0.60 10.97 13.62 0.69
0.70 13.78 19.36 0.75
0.80 17.36 25.52 0.81
0.90 23.84 32.39 0.87
1.00 52.82 39.65 0.92
</table>
<tableCaption confidence="0.99994">
Table 1: Performance of the PSVi on 25 TODiC Statements and 173.255 documents
</tableCaption>
<page confidence="0.996028">
26
</page>
<bodyText confidence="0.997927730769231">
Column A lists the recall levels used in information retrieval testing. Column B shows for each of these
recall levels, what per cent of the database was actually searched to achieve that level of recall. These
percentages are based on post hoc information and are known because the relevance assessments made by
the trained analysts for these queries and documents were made available after TREC-1. Column C displays
what percent of the ranked list would need to be searched to achieve that row&apos;s recall level, when the
system uses the PSV as the cut-off criterion. Column D shows what the actual recall performance would be
when the system uses the PSV for that recall level as the cut-off criterion.
This means that on average, if the user was striving for 70% recall, 19.36 % of the 173,255 documents
would be passed on to the system&apos;s next module when the PSV; is used as the cut-off criterion. In actuality,
the PSV predicts slightly better than this, and the user would retrieve 75% of the relevant documents. And
if the user were really interested in total recall, use of the PSV would require that 39.65% of the ranked
documents be forwarded and these documents would in fact contain 92% of the relevant documents.
Testing and Results
Having produced a ranked listing of documents based on the similarity of their SFC vectors to a query
vector, the most illustrative evaluation of performance would be the results provided in Table 1. We
believe that these are quite reasonable filtering results. Earlier testings of the SFCoder have revealed that
the most important factor in improving its performance would be recognition that a query contains a
requirement that a particular proper noun be in a document in order for the document to be relevant.
Therefore, we have incorporated a second level of lexical-semantic processing as an extension of the
SFCoder. That is, the Proper Noun Interpreter (Paik et al; in press) includes algorithms for computing the
similarity between a query&apos;s proper noun requirements and each document&apos;s Proper Noun Field. The proper
noun similarity value is then combined with the similarity value produced by the SFCoder for a re-ranking
in relation to the query. In the 18th month TIPSTER evaluation of our system, this re-ranking of documents
based on the SFC values plus the Proper Noun values improved significantly the filtering power of the
system. We have not yet adapted the PSV for predicting the cut-off criterion on the combined similarity
values, but we will be doing so in the next few weeks.
</bodyText>
<sectionHeader confidence="0.797938" genericHeader="general terms">
L Conclusions
</sectionHeader>
<bodyText confidence="0.997759">
As a preliminary full implementation and testing of the SFCoder as a means for semantically representing
the content of texts for the purpose of delimiting a document set with a high likelihood of containing all
those relevant to an individual query, we find these results promising. In a large operational system, the
ability to filter out 61% of the incoming flux of millions of documents if the SFCoder alone is used, or 72%
of the documents if the SFCoder + Proper Noun Interpreter is used, will have a significant impact on the
system&apos;s performance.
In addition, we have also been experimenting with the SFC vectors as a means for automatically clustering
documents in an established database (Liddy, Paik &amp; Woelfel, 1992). To do this, the document vectors are
clustered using Ward&apos;s agglomerative clustering algorithm (Ward, 1963) to form classes in a document
database. For ad hoc retrieval, query SFC vectors are matched to the SFC vector of each cluster-centroid
in the database. Clusters whose centroid vectors exhibit a predetermined similarity to the query SFC
vector are either presented to the user as a semantically cohesive cluster on which to begin preliminary
browsing or, passed on to other system components for further processing. A qualitative analysis of the
clusters produced in this manner revealed that the use of SFCs combined with Ward&apos;s clustering algorithm
resulted in meaningful groupings of documents that were similar across concepts not directly encoded in
SFCs. Browsers find that documents seem to fit naturally int the cluster to which they are assigned by the
system.
Beyond its uses within the DR-LINK System, the Subject Field Coder has general applicability as a pre-filter
</bodyText>
<page confidence="0.99247">
27
</page>
<bodyText confidence="0.999796818181818">
for a wide range of other systems. The only adjustment required would be a recomputation of the
correlation matrix based on each new corpus. The recomputation is necessary due to the fact that different
corpora represent different domains and the tendencies of SFCs to correlate with other SFCs will vary
somewhat from domain to domain. We have used the SFC filter on various corpora and have quickly
recomputed a matrix for each.
Reiterating the opening argument of this paper, we believe that the current situation in information
retrieval could be effectively dealt with by considering document retrieval as a multi-stage process in
which the first modules of a system filter out those texts with no real likelihood of matching a user&apos;s need.
The filtering approach offers promise particularly to those systems which perform a more conceptual style
of representation which is very computationally expensive if applied to all documents regardless of the
likelihood that they might be relevant.
</bodyText>
<sectionHeader confidence="0.804051" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990588">
We wish to thank Longman Group, Ltd. for making the machine readable version of LDOCE, 2nd Edition
available to us and BBN for making POST available for our use on the DR-LINK Project.
</bodyText>
<sectionHeader confidence="0.988689" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99734364516129">
Belkin, N.J. &amp; Croft, W.B. (1992). Information filtering and information retrieval: Two sides of the same
coin? Communications of the ACM, 35 (12): 29-38.
Boguraev, B. &amp; Briscoe, T. (1989). Computational lexicography for natural lanauage orocessina.
London: Longman.
Choueka, Y. &amp; Lusignan, S. (1985). Disambiguation by short contexts. Computers and the Humanities, pp.
147-157.
Furnas, G.W., Landauer, T.K., Gomez, L.M. &amp; Dumais, S.T. (1987). The vocabulary problem in human-
system communication. Communications of the ACM, 30 (11):964-71.
Kelly, E. F. &amp; Stone, P. J. (1975). Computer recognition of English word senses. Amsterdam: North Holland
Publishing Co.
Krovetz, R. (1991). Lexical acquisition and information retrieval. In Zemik, U. (Ed.). Lexical acauisition:
exoloitina on-line resources to build a lexicon. Hillsdale, NJ: Lawrence Earlbaum.
Liddy, E.D. &amp; Myaeng, S. H. (1993). DR-LINK&apos;s linguistic-conceptual approach to document detection.
proceedinas of the First Text Retrieval Conference. NIST.
Liddy, E.D. &amp; Paik, W. (1992). Statistically-guided word sense disambiguation. In Proceedinas of AAAI Fall
&apos;92 Symposium on Probabilistic Approaches to Natural Lanauage. Boston.
Liddy, E.D., Paik, W. &amp; Woelfel, J. (1992). Use of subject field codes from a machine-readable dictionary
for automatic classification of documents. Proceedings of 3rd ASIS Classification Research Workshop.
McGill, M., Koll, M., &amp; Noreault, T. (1979). An evaluation of factors affecting document rankina by
information retrieval systems. Final report to National Science Foundation. Syracuse, NY: Syracuse
University.
Meteer, M., Schwartz, R. &amp; Weischedel, R. (1991). POST: Using probabilities in language processing.
Proceedinas of the Twelfth International Conference on Artificial Intelligence. Sydney, Australia.
Paik, W., Liddy, E.D., Yu, E.S. &amp; McKenna, M. (In press). Extracting and classifying proper nouns in
documents. Proceedings of the Human Lanauaae Technolaav Workshoo. Princeton, NJ: March, 1993.
Sager, W.K.H. &amp; Lockemann, P.C. (1976). Classification of ranking algorithms, international forum on
Information and Documentation. 1(4):2-25, 1976.
Stator, B. (1991). Using context for sense preference. In Zernik, U. (Ed.). Lexical acauisition: exoloiting on-
line resources to build a lexicon. Hillsdale, NJ: Lawrence Earlbaum.
Sowa, J. (1984). Conceptual Structures: Information Processina in Mind and Machine. Reading, MA:
Addison-Wesley.
</reference>
<page confidence="0.977046">
28
</page>
<reference confidence="0.9845864">
Walker, D. E. &amp; Amsler, R. A. (1986). The use of machine-readable dictionaries in sublanguage analysis. In
Grishman, R. &amp; Kittredge, R. (Eds). i • n
sind brocessinct. Hillsdale, NJ: Lawrence Earlbaum.
Ward, J. (1963). Hierarchical grouping to optimize an objection function. Journal of the American
Statistical Association. 58, p. 237-254.
</reference>
<figure confidence="0.9989644">
: • Z
IC
• • 1 : 1 1
• Is .6: OZ
• • I
</figure>
<page confidence="0.977747">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000760">
<title confidence="0.997431">Document Filtering Using Semantic Information Readable Dictionaryl</title>
<author confidence="0.999838">Elizabeth D Liddy</author>
<author confidence="0.999838">Woojin</author>
<affiliation confidence="0.9998275">School of Information Syracuse University</affiliation>
<author confidence="0.99944">Edmund S Yu</author>
<affiliation confidence="0.999894">College of Engineering and Computer Science Syracuse University</affiliation>
<abstract confidence="0.959063374655648">Large scale information retrieval systems need to refine the flow of documents which will receive further fine-grain analysis to those documents with a high potential for relevance to their respective users. This paper reports on research we have conducted into the usefulness of semantic codes from a machine readable dictionary for filtering large sets of incoming documents for their broad subject appropriateness to a topic of interest. The Subject Field Coder produces a summary-level semantic representation of a text&apos;s contents by tagging each word in the document with the appropriate, disambiguated Subject Field Code (SFC). The within-document SFCs are normalized to produce a vector of the SFCs representing that document&apos;s contents. Queries are likewise represented as SFC vectors and then compared to SFC vectors of incoming documents, which are then ranked according to similarity to the query SFC vector. Only those documents whose SFC vectors exhibit a predetermined degree of similarity to the query SFC vector are passed to later system components for more refined representation and matching. The assignment of SFCs is fully automatic, efficient and has been empirically tested as a reasonable approach for ranking documents from a very large incoming flow of documents. We report details of the implementation, as well as results of an empirical testing of the Subject Field Coder on fifty queries. Filtering Two realities regarding the current context of information retrieval motivate the research herein 1) Document collections from which need to receive and/or retrieve relevant information are immense in size and only likely to increase; 2) Given the size of both the daily influx of documents and the document databases in which the daily input is then stored, a finer level of representation of both information needs and documents is necessary in order to ensure higher precision results. Although precision has always been a concern in information retrieval, the problem assumes new significance when low precision translates into thousands of non-relevant documents that each user must peruse. Improved precision can be achieved by using a more conceptual level of representation of documents and so that the system provides to the user documents containing their of interest, just the user&apos;s keywords. However, this level of analysis is computationally expensive and not reasonable to perform on documents that are unlikely to be relevant. Therefore, preliminary filtering of documents in an information retrieval system would permit later, finer levels of text analysis to be more efficiently applied to a smaller subset of documents. This suggests the view that information retrieval be approached as a multi-stage filtering process, with the types and optimal number of filterings dependent on both the size of the document collection and the desired granularity of filtering. We believe that intelligent filtering is needed in document detection applications, where millions of documents are received daily by an organization, while only a relatively small subset of documents is of potential interest to any individual user. Furthermore, we believe that a support this research was provided by DARPA Contract #91-F136100-00 under the auspices of the TIPSTER Project. 20 purely content-based document filter would be useful in delineating a subject-appropriate preliminary set of documents for each user on which the system would then perform finer levels of analysis and matching. The notion of filtering as used here, is to be distinguished from one sense of the term currently in use. Belkin and Croft (1992) define information filtering broadly as &apos;a variety of processes involving the delivery of information to people who need it&apos; (p.29). Defined as such, the work we are herein reporting, fits the definition of filtering. However, Belkin and Croft describe a particular application of information filtering which equates to Selective Dissemination of Information (SDI). This view of filtering is at a finer grain of matching than our notion of filtering. In an SDI application, filtering is the full matching process, while we conceive of filtering as a rougher-grain, first stage, topic-area matching. In a one-stage SDI user-profiles may contain facets of description beyond the desired content of useful docuour preliminary filter relies solely on topic-based criteria. The goal of our filter is to efficiently and effectively skim off those documents which possess the greatest likelihood of proving relevant to a user&apos;s need, here conceived of as a natural language statement of their long-standing information requirement. Later stages of processing in the system will perform the more refined conceptual level of matching. Project Our ongoing research into the development and implementation of an effective document filter has produced a module used within a larger document detection system, the DR-LINK System (Liddy &amp; Myaeng, 1993). The DR-LINK Project is research being conducted under the auspices of DARPA&apos;s TIPSTER Project whose goal is the development of algorithms both for the detection of documents of interest and the extraction of selected information from these documents for a large group of users. The DR-LINK system architecture is modular in design, with six separate processing modules. These modules enhance the documents at every stage by various semantic enrichments which are used to refine the flow of documents in terms of both appropriateness to the query and pure numbers. Briefly summarized, the six modules&apos; processing is as follows: 1) The Subject Field Coder uses semantic word knowledge to produce a summary-level topical vector representation of a document&apos;s contents that is matched to a vector representation of a query in order to select for further processing only those documents which have real potential of being relevant. This subset of documents is then passed to the: 2) The Proper Noun Interpreter, which uses a variety of knowledge bases and context-based heuristics to recognize, categorize, and standardize every proper noun in the text. The similarity between a query&apos;s proper noun requirements and each document&apos;s Proper Noun Field is computed at either the category level or by precise string matching. This similarity value is combined with the similarity value from the Subject Field Coder for a reranking of all documents in response to the query. Those documents which exceed an empirically determined cut-off criterion based on this combined similarity value, are then passed to: 3) The Text Structurer, which sub-divides a text into its discourse-level segments in order to focus matching on the appropriate discourse component in the documents in response to the particular requirements of an information need. For example, for queries run against the newspaper database that are seeking information about a particular possible future event (e.g. Japanese acquisition of U.S. companies), the Text Structurer matching algorithm will weight more highly those articles in which mention of the event occurs in an &apos;Expectation&apos; component. When retrieved, the structured texts, with the appropriate components high-lighted, are passed to the: 4) Relation-Concept Detector, which raises the level at which we do matching from a keyword or key-phrase level to a more conceptual level by expanding terms in the query to all terms which have been shown to be &apos;substitutable&apos; for them, and then by extracting semantic relations between concepts in both documents and queries. This component produces conceptrelation-concept triples which are passed to the: 23. 5) Conceptual Graph Generator which converts the triples into the Conceptual Graph (CG) formalism, a representation similar to semantic networks, but with labelled arcs (Sowa, 1984). The resultant CGs are passed to the: 6) Conceptual Graph Matcher, which measures the degree to which a particular query CG and candidate document CGs share a common structure, and then produces a final ranking of the documents. / Since the later modules in our system require very complex processing in order to produce conceptually enriched representations of documents and queries, preliminary filtering of the incoming flow of documents by means of the Subject Field Coder has proven to be extremely useful. For while CGs enable us to do finegrained representation, such fine-grained representation is not necessary in order to determine, for instance, that a document on &apos;computer games&apos; is not likely to be relevant to a query on &apos;merit pay&apos;. Therefore, the SFCoder produces a first rough cut of those documents which have real potential for matching a query as the first of a multi-stage model of retrieval. Because the SFCoder is based on the implicit semantics of the words in the text, it has the ability to successfully eliminate non-topic relevant documents during a preliminary stage without the attendant risks of filtering approaches which are based on less semantically reliable characteristics of documents. Used in Filterino Subject filtering is a difficult problem, given the richness and variety of natural language. In addition, imposition of an overly stringent subject filter in too homogenous a document collection runs the risk of excluding docu-ments which might match the query during a later, finer matching process. This is particularly true if a lexical or keyword analysis of text is the basis of the filtering. However, if based on an appropriate semantic representation combined with a reasonable cut-off criterion, a subject-based filter offers the means of siphoning off from a large heterogenous stream of documents, smaller, more appropriate sub-collections of documents for various users or user-groups, for which the system then produces more conceptual representations and performs finer-grain matching. The success of our filtering approach is attributable to the nature of the representation scheme we use for every text (whether document or query). The representation of each text is a summary vector of the Field Codes (SFCs) from Dictionav of Contemporary Ertglish(LDOCE) representing the correct sense of each word in the text that is in LDOCE and which has SFCs assigned in LDOCE. For example, 1 presents a short Street Journalarticle and a humanly readable version of the normalized SFC vector which serves as the document&apos;s semantic summary representation. A U. S. magistrate in Florida ordered Carlos Lehder Rivas, described as among the world&apos;s leading cocaine traffickers, held without bond on 11 drug-smuggling counts. Lehder, who was captured last week in Colombia and immediately extradited to the U.S., pleaded innocent to the charges in federal court in Jacksonville. LAW .2667 SOCIOLOGY .1333 BUSINESS .1333 ECONOMICS .0667 DRUGS .1333 MILITARY .0667 POLITICAL SCIENCE .1333 OCCUPATIONS .0667 1: Sample Street Journaldocument and its SFC representation As can be seen by reading either the original text or the SFC vector values, the text&apos;s main topic is law, while the topics of business, drugs, political science and sociology are equally, but less significantly mentioned. The vector suggests a passing reference to the fields of economics, military, and occupations. 22 The system would consider this document relevant to a query whose SFC representation was distributed proportionately among the same SFCs slots on the vector. The important aspect of this representation is a document does to include any of the same words that are included in a query in order for a high similarity to be found between the query and a document, since the matching is based on similarity of SFC vectors, not according to the particular words used. Therefore, it can be seen that the SFC representation, which is one level of abstraction above the actual words in a text, implicitly handles both the synonymy (multiple words having the same meaning) and polysemy (one word having multiple meanings) problems which have plagued the use of natural language in information retrieval systems. This level of abstraction is an essential feature of the representation since it has been shown (Furnas et al, 1987) that users&apos; information requests frequently share little vocabulary overlap with the documents which actually contain relevant information. Dictionary of Contemporary EngHet text representation is based on the machine-readable version of Dictionary of Contemporary Enalish(LDOCE), a British-produced learner&apos;s dictionary. The first edition of LDOCE has been used in a number of investigations into natural language processing applications (Boguraev &amp; Briscoe, 1989). We are using the second edition (1987) which contains 35,899 headwords and 53,838 senses. The machinereadable tape of LDOCE contains several fields of information not visible in the hard-copy version which are extremely useful in natural language processing tasks. Some of these are relevant for syntactic processing, while others contain semantic information, which indicate the class of entities to which a noun belongs (e.g. abstract) or the semantic constraints for the arguments of a an adjective, and the SFCs, which are the basis of our text representation for document filtering. The SFCs comprise a classification scheme of 124 major fields, based on an earlier classification scheme of Merriam-Webster. SFCs are manually assigned to words in LDOCE by the Longman lexicographers. There are two types of problems with the SFCs which we have resolved in order to use them computationally. First, a particular word may function as more than one part of speech and secondly, if a word has more than one sense, each of these senses may be tagged in the lexicon with different SFCs. Therefore, in order for SFCs to provide a reasonable representation of texts, a system must ascertain both the grammatical function and sense of a word in the text, so that the appropriate SFC for each orthographic form can be chosen. We have incorporated in our system means for choosing amongst each word&apos;s syntactic categories and senses found in LDOCE, thereby enabling the system to assign just one SFC to each word in a given text. In related research, Walker and Amster used the Subject Field Codes to determine the appropriate subject domains for a set of texts (1986). However, they used the most frequent SFC to characterize a document&apos;s content, whereas we represent a document by a vector of frequencies of SFCs for words in that text. Stator (1991) has taken the original 124 SFCs and added an additional layer of seven pragmatic classes to the original two-level hierarchy. He has found the reconstructed hierarchy useful when attempting to disambiguate multiple senses and SFCs attached to words. His metric for preferring one sense over another relies on values within an individual text, whereas we add corpus correlation values as a further stage in the disambiguation process. Krovetz (1991) has been exploring the effect of combining the evidence from SFCs with evidence from other fields in LDOCE for selection of a correct word sense. His goal is to represent documents by their appropriate senses rather than just the orthographic forms of words, for use in an information retrieval system. Field Coding of Texts In the Subject Field Coder, the following stages of processing are done in order to generate a SFC vector representation of each text: 1 we run the documents and query through POST, a probabilistic part of speech 23 tagger (Meeter et al, 1991) in order to restrict candidate SFCs of a word to those of just the appropriate syntactic category of each word as determined by POST. Stage 2 processing consists of retrieving the SFCs of each word&apos;s correct part of speech from the lexical database. The SFC retrieval process utilizes the Kelly &amp; Stone (1975) stemming algorithm to reduce morphological variants to their simple stem as found in LDOCE. Kelly &amp; Stone&apos;s approach is one of weak stemming and produces correct simple stems for look-up in LDOCE, rather than stripping suffixes. Having selected candidate SFCs for each word&apos;s correct syntactic category, we begin sense disambiguation at Stage 3, using sentence-level context-heuristics to determine a single word&apos;s correct SFC. We begin with context-heuristics because empirical results have shown that local context is used by humans for sense disambiguation (Choueka &amp; Lusignan, 1985) and context-heuristics have been experimentally tested in Walker &amp; Amsler&apos;s (1986) and Slator&apos;s work (1991) with promising results. The input to Stage 3 is a word, its part-of-speech tag, and the SFCs for each sense of that grammatical category. For some words, no disambiguation may be necessary at this stage because the SFCs for the part-of-speech of the input word all be GENERAL or there may be provided by LDOCE. However, for the majority of words in each sentence there are multiple SFCs, so the input would be as seen in Figure 2. n POLITICAL ORDERS companies n BUSINESS, MUSIC, THEATER employ v LABOR, BUSINESS about adv one a dj billion adj NUMBERS n SOCIOLOGY, POLITICAL ANTHROPOLOGY Fig 2: Subject Field Codes &amp; Frequencies (in Superscript) for Part-of-Speech Tagged Words To select a single SFC for each word in a sentence, Stage 3 uses an ordered set of context-heuristics. First, the SFCs attached to all words in a sentence are evaluated to determine at the sentence level: 1) whether any words have only one SFC assigned to all senses of that word, and; 2) the SFCs which are most frequently assigned across all words in the sentence. Each sentence may have more than one unique SFC, as there may be more than one word whose senses have all been assigned a single SFC. In Figure 2, NUMBERS is a unique SFC, being the only SFC assigned to the word billion&apos; and POLITICAL SCIENCE is the highly frequent SFC for this sentence, being assigned to 6 senses in total. The unique SFCs and the highly frequent SFCs have proven to be good local determinants of the subject domain of the sentence. We have established the criterion that if no SFC has a frequency equal to or greater than three, a frequency-based SFC for that particular sentence is not selected. Erppirical results show that SFCs having a within-sentence frequency less than three do not accurately represEint the domain of the sentence. Stage 4 evaluates the remaining words in the sentence, and for some words chooses a single SFC based on the locally-important SFCs determined in Stage 3. The system scans the SFCs of each remaining word to determine whether the SFCs which have been identified as unique or highly frequent occur amongst the multiple SFCs which LDOCE lexicographers have assigned to that word. In Figure 2, for example, POLITICAL SCIENCE would be selected as the appropriate SFC for &apos;people&apos; and &apos;state&apos; because POLITICAL SCIENCE was determined in Stage 3 to be the most frequent SFC value for the sentence. For the ambiguous words which have no SFC in common with the unique or highly frequent SFCs for that sentence, Stage $ incorporates two global knowledge sources to complete the sense disambiguation task. The primary source is a 122 x 122 correlation matrix computed from the SFC frequencies of the 442,059 words that occurred in a sample of 977 Wall Street Journal (WSJ) articles. The matrix reflects stable 24 estimates of SFCs which co-occur within documents for this text type. The second source is the order in which the senses of a word are listed in LDOCE. Ordering of senses in LDOCE is determined by Longman&apos;s lexicographers based on frequency of use in the English language. The correlation matrix was produced by running SAS on the SFC output of the 977 WSJ articles processed through Stage 2. At that stage, each document is represented by a vector of SFCs of all senses of the correct part-of-speech of each word as determined by POST. The observation unit is the document and the variables being correlated are the 122 SFCs. The scores for the variables are the within-document frequencies of each SFC. There are 255,709 scores across the 977 articles on which the matrix is computed. The resulting values in the 122 x 122 matrix are the Pearson product moment correlation coefficients between SFCs and range from a +1 to a -1, with 0 indicating no relationship between the SFCs. For example, NET GAMES and COURT GAMES have the highest correlation, with ECONOMICS and BUSINESS having the next highest correlation. The matrix is then used in Stage 5, where each of the remaining ambiguous words is resolved a word at a time, by accessing the matrix via the unique and highly frequent SFCs determined for a sentence in Stage 3. The system evaluates the correlation coefficients between the unique/highly frequent SFCs of the sentence and the multiple SFCs assigned to the word being disambiguated to determine which of the multiple SFCs has the highest correlation with the unique and/or highly frequent SFCs. The system then selects that SFC as the unambiguous representation of the sense of the word. We have developed heuristics for three cases for selecting a single SFC for a word using the correlation matrix. The three cases function better than handling all instances as a single case because of the special treatment needed for words with the less-substantive GENERAL (XX) code. When XX is amongst the SFCs, we take order of the SFCs into consideration, reflecting the fact that the first SFC listed is more likely to be correct, since the most widely used sense is listed first in LDOCE. So, to overcome this likelihood, a more substantive SFC listed later in the entry must have a much higher correlation with a sentencedetermined SFC in order to be selected over the GENERAL (XX) code. The system implementation of the disambiguation procedures was tested on a sample of 1638 words from WSJ having SFCs in LDOCE. The system selected a single SFC for each word, which was compared to the sense-selections made by an independent judge who was instructed to read the sentences and the definitions of the senses of each word and then to select that sense which was most correct. Overall, the SFC disambiguator selected the correct SFC 89% of the time (Liddy &amp; Paik, 1992). a vector of SFCs and their frequencies for each document and for the query. At this point the non-substantive GENERAL SFCs are removed from the vector sums, since these contribute nothing to a text&apos;s subject content representation. In Stage 7, the vectors of each text are normalized using Sagees (1976) term weighting formula in order to control for the effect of document length. The choice of Sager&apos;s (1976) term weighting formula and Sager and Lockemann&apos;s (1976) similarity measure were based on an extensive study done by McGill, Koll &amp; Noreault (1979) which empirically evaluated twenty one term-weighting formulae and twenty four similarity measures. Using the coefficient of ranking effectiveness (CRE) measure, each term weighting scheme was tested in combination with each similarity measure to determine which combination was best for either controlled representations or free text representations. Since we consider SFCodes to be a form of a controlled vocabulary (all free-text terms are reduced to 122 codes), we chose Sagees (1976) term weighting scheme and Sager &amp; Lockemann&apos;s (1976) similarity measure since they were shown to be the best formulae for use with controlled vocabulary representation (McGill et al, 1979). 8, document vectors are compared to the query vector using Sager &amp; Lockemann&apos;s (1976) similarity measure and a ranked listing of the documents in decreasing order of similarity is produced. Having created this ranked list of documents for each query, the system must determine how many of these 25 documents should reasonably be passed on to the next module. The method used is an adaptive cut-off criterion that predicts for each query and at each recall level, how many documents from the ranked list should be forwarded. The cut-off criterion uses a multiple regression formula which was developed on training data consisting of the odd-numbered Topic Statements (queries) from 1 to 50, used in both TIPSTER- Phase I and TREC-1. The training corpora consisted of Wall Street Journal articles from 1986-1992, a total of 173,255 separate documents. The regression formula used in Stage 8 is: SPSV; = e0.9909 - (0.6112 • RL) + (0.5455 STDSVi) - 5 where: SPSV; is the Standardized Predicted Similarity Value RL is the designated Recall Level STDSV; is the Standardized Top-Ranked Document Similarity Value, logarithmically transformed is the Topic Statement whose cut-off criterion is being predicted and significantly predicted on the training queries (R = .826, F = 265.42, df= 2,247, p .0005). Using this standardized value (SPSV;), a linear transformation is used to produce the value which will be used as the cut-off criterion: PSV; = (SPSV; • s.d.i) + mean; where: is the Predicted Similarity Value s.d.1 standard deviation mean; mean is used by the system to establish the cut-off criterion for each recall level for each query. The averaged results of the testing of the PSV; using the held-out, even-numbered Topic Statements are provided in Table 1. A. B. C. D. Recall level Actual % % of DB searched based on PSV Recall DB searched based on PSV 0.10 1.27 0.50 0.20 0.20 2.67 0.98 0.28 0.30 4.42 2.51 0.39 0.40 6.55 5.24 0.50 0.50 8.46 8.90 0.61 0.60 10.97 13.62 0.69 0.70 13.78 19.36 0.75 0.80 17.36 25.52 0.81 0.90 23.84 32.39 0.87 1.00 52.82 39.65 0.92 1: Performance of the PSVi on 25 and 173.255 documents 26 Column A lists the recall levels used in information retrieval testing. Column B shows for each of these recall levels, what per cent of the database was actually searched to achieve that level of recall. These percentages are based on post hoc information and are known because the relevance assessments made by the trained analysts for these queries and documents were made available after TREC-1. Column C displays what percent of the ranked list would need to be searched to achieve that row&apos;s recall level, when the system uses the PSV as the cut-off criterion. Column D shows what the actual recall performance would be when the system uses the PSV for that recall level as the cut-off criterion. This means that on average, if the user was striving for 70% recall, 19.36 % of the 173,255 documents be passed on to the system&apos;s next module when the is used as the cut-off criterion. In actuality, the PSV predicts slightly better than this, and the user would retrieve 75% of the relevant documents. And if the user were really interested in total recall, use of the PSV would require that 39.65% of the ranked documents be forwarded and these documents would in fact contain 92% of the relevant documents. Testing and Results Having produced a ranked listing of documents based on the similarity of their SFC vectors to a query vector, the most illustrative evaluation of performance would be the results provided in Table 1. We believe that these are quite reasonable filtering results. Earlier testings of the SFCoder have revealed that the most important factor in improving its performance would be recognition that a query contains a requirement that a particular proper noun be in a document in order for the document to be relevant. Therefore, we have incorporated a second level of lexical-semantic processing as an extension of the SFCoder. That is, the Proper Noun Interpreter (Paik et al; in press) includes algorithms for computing the similarity between a query&apos;s proper noun requirements and each document&apos;s Proper Noun Field. The proper noun similarity value is then combined with the similarity value produced by the SFCoder for a re-ranking in relation to the query. In the 18th month TIPSTER evaluation of our system, this re-ranking of documents based on the SFC values plus the Proper Noun values improved significantly the filtering power of the system. We have not yet adapted the PSV for predicting the cut-off criterion on the combined similarity values, but we will be doing so in the next few weeks. As a preliminary full implementation and testing of the SFCoder as a means for semantically representing the content of texts for the purpose of delimiting a document set with a high likelihood of containing all those relevant to an individual query, we find these results promising. In a large operational system, the ability to filter out 61% of the incoming flux of millions of documents if the SFCoder alone is used, or 72% of the documents if the SFCoder + Proper Noun Interpreter is used, will have a significant impact on the system&apos;s performance. In addition, we have also been experimenting with the SFC vectors as a means for automatically clustering documents in an established database (Liddy, Paik &amp; Woelfel, 1992). To do this, the document vectors are clustered using Ward&apos;s agglomerative clustering algorithm (Ward, 1963) to form classes in a document database. For ad hoc retrieval, query SFC vectors are matched to the SFC vector of each cluster-centroid in the database. Clusters whose centroid vectors exhibit a predetermined similarity to the query SFC vector are either presented to the user as a semantically cohesive cluster on which to begin preliminary browsing or, passed on to other system components for further processing. A qualitative analysis of the clusters produced in this manner revealed that the use of SFCs combined with Ward&apos;s clustering algorithm resulted in meaningful groupings of documents that were similar across concepts not directly encoded in SFCs. Browsers find that documents seem to fit naturally int the cluster to which they are assigned by the system. Beyond its uses within the DR-LINK System, the Subject Field Coder has general applicability as a pre-filter 27 a wide range of other systems. The only required would be a recomputation of the matrix based on each new recomputation is necessary due to the fact that different corpora represent different domains and the tendencies of SFCs to correlate with other SFCs will vary somewhat from domain to domain. We have used the SFC filter on various corpora and have quickly recomputed a matrix for each. Reiterating the opening argument of this paper, we believe that the current situation in information retrieval could be effectively dealt with by considering document retrieval as a multi-stage process in which the first modules of a system filter out those texts with no real likelihood of matching a user&apos;s need. The filtering approach offers promise particularly to those systems which perform a more conceptual style of representation which is very computationally expensive if applied to all documents regardless of the likelihood that they might be relevant.</abstract>
<note confidence="0.948030042553192">Acknowledgments We wish to thank Longman Group, Ltd. for making the machine readable version of LDOCE, 2nd Edition available to us and BBN for making POST available for our use on the DR-LINK Project. References Belkin, N.J. &amp; Croft, W.B. (1992). Information filtering and information retrieval: Two sides of the same of the ACM,35 (12): 29-38. B. &amp; Briscoe, T. lexicography for natural lanauage orocessina. London: Longman. Y. &amp; Lusignan, S. (1985). Disambiguation by short contexts. and the Humanities,pp. 147-157. Furnas, G.W., Landauer, T.K., Gomez, L.M. &amp; Dumais, S.T. (1987). The vocabulary problem in humancommunication. of the ACM,30 (11):964-71. E. F. &amp; Stone, P. J. (1975). recognition of English word senses.Amsterdam: North Holland Publishing Co. R. (1991). Lexical acquisition and information retrieval. In Zemik, U. (Ed.). acauisition: on-line resources to build a lexicon.Hillsdale, NJ: Lawrence Earlbaum. Liddy, E.D. &amp; Myaeng, S. H. (1993). DR-LINK&apos;s linguistic-conceptual approach to document detection. the First Text Retrieval Conference.NIST. E.D. &amp; Paik, W. (1992). Statistically-guided word sense disambiguation. In of AAAI Fall Symposium on Probabilistic Approaches to Natural Lanauage.Boston. Liddy, E.D., Paik, W. &amp; Woelfel, J. (1992). Use of subject field codes from a machine-readable dictionary automatic classification of documents. of 3rd ASIS Classification Research Workshop. M., Koll, M., &amp; Noreault, T. (1979). evaluation of factors affecting document rankina by retrieval systems.Final report to National Science Foundation. Syracuse, NY: Syracuse University. Meteer, M., Schwartz, R. &amp; Weischedel, R. (1991). POST: Using probabilities in language processing. of the Twelfth International Conference on Artificial Intelligence.Sydney, Australia. Paik, W., Liddy, E.D., Yu, E.S. &amp; McKenna, M. (In press). Extracting and classifying proper nouns in of the Human Lanauaae Technolaav Workshoo.Princeton, NJ: March, 1993. W.K.H. &amp; Lockemann, P.C. (1976). Classification of ranking algorithms, forum on and Documentation.1(4):2-25, 1976. B. (1991). Using context for sense preference. In Zernik, U. (Ed.). acauisition: exoloiting onresources to build a lexicon.Hillsdale, NJ: Lawrence Earlbaum. J. (1984). Structures: Information Processina in Mind and Machine.Reading, MA: Addison-Wesley. 28 Walker, D. E. &amp; Amsler, R. A. (1986). The use of machine-readable dictionaries in sublanguage analysis. In Grishman, R. &amp; Kittredge, R. (Eds). i • brocessinct.Hillsdale, NJ: Lawrence Earlbaum. J. (1963). Hierarchical grouping to optimize an objection function. of the American Association.58, p. 237-254. : • Z IC • 1 : • • • I 29</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N J Belkin</author>
<author>W B Croft</author>
</authors>
<title>Information filtering and information retrieval: Two sides of the same coin?</title>
<date>1992</date>
<journal>Communications of the ACM,</journal>
<volume>35</volume>
<issue>12</issue>
<pages>29--38</pages>
<contexts>
<context position="3962" citStr="Belkin and Croft (1992)" startWordPosition="599" endWordPosition="602">ceived daily by an organization, while only a relatively small subset of documents is of potential interest to any individual user. Furthermore, we believe that a 1 support for this research was provided by DARPA Contract #91-F136100-00 under the auspices of the TIPSTER Project. 20 purely content-based document filter would be useful in delineating a subject-appropriate preliminary set of documents for each user on which the system would then perform finer levels of analysis and matching. The notion of filtering as used here, is to be distinguished from one sense of the term currently in use. Belkin and Croft (1992) define information filtering broadly as &apos;a variety of processes involving the delivery of information to people who need it&apos; (p.29). Defined as such, the work we are herein reporting, fits the definition of filtering. However, Belkin and Croft describe a particular application of information filtering which equates to Selective Dissemination of Information (SDI). This view of filtering is at a finer grain of matching than our notion of filtering. In an SDI application, filtering is the full matching process, while we conceive of filtering as a rougher-grain, first stage, topic-area matching. </context>
</contexts>
<marker>Belkin, Croft, 1992</marker>
<rawString>Belkin, N.J. &amp; Croft, W.B. (1992). Information filtering and information retrieval: Two sides of the same coin? Communications of the ACM, 35 (12): 29-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<title>Computational lexicography for natural lanauage orocessina.</title>
<date>1989</date>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="13115" citStr="Boguraev &amp; Briscoe, 1989" startWordPosition="2010" endWordPosition="2013">ieval systems. This level of abstraction is an essential feature of the representation since it has been shown (Furnas et al, 1987) that users&apos; information requests frequently share little vocabulary overlap with the documents which actually contain relevant information. 1, Lonaman&apos;s Dictionary of Contemporary EngHet Our text representation is based on the machine-readable version of Longman&apos;s Dictionary of Contemporary Enalish (LDOCE), a British-produced learner&apos;s dictionary. The first edition of LDOCE has been used in a number of investigations into natural language processing applications (Boguraev &amp; Briscoe, 1989). We are using the second edition (1987) which contains 35,899 headwords and 53,838 senses. The machinereadable tape of LDOCE contains several fields of information not visible in the hard-copy version which are extremely useful in natural language processing tasks. Some of these are relevant for syntactic processing, while others contain semantic information, which indicate the class of entities to which a noun belongs (e.g. animate, abstract) or the semantic constraints for the arguments of a verb or an adjective, and the SFCs, which are the basis of our text representation for document filt</context>
</contexts>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Boguraev, B. &amp; Briscoe, T. (1989). Computational lexicography for natural lanauage orocessina. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
<author>S Lusignan</author>
</authors>
<title>Disambiguation by short contexts.</title>
<date>1985</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>147--157</pages>
<contexts>
<context position="16928" citStr="Choueka &amp; Lusignan, 1985" startWordPosition="2628" endWordPosition="2631">cess utilizes the Kelly &amp; Stone (1975) stemming algorithm to reduce morphological variants to their simple stem as found in LDOCE. Kelly &amp; Stone&apos;s approach is one of weak stemming and produces correct simple stems for look-up in LDOCE, rather than stripping suffixes. Having selected candidate SFCs for each word&apos;s correct syntactic category, we begin sense disambiguation at Stage 3, using sentence-level context-heuristics to determine a single word&apos;s correct SFC. We begin with context-heuristics because empirical results have shown that local context is used by humans for sense disambiguation (Choueka &amp; Lusignan, 1985) and context-heuristics have been experimentally tested in Walker &amp; Amsler&apos;s (1986) and Slator&apos;s work (1991) with promising results. The input to Stage 3 is a word, its part-of-speech tag, and the SFCs for each sense of that grammatical category. For some words, no disambiguation may be necessary at this stage because the SFCs for the part-of-speech of the input word may all be GENERAL or there may be no SFCs provided by LDOCE. However, for the majority of words in each sentence there are multiple SFCs, so the input would be as seen in Figure 2. State n POLITICAL SCIENCE4, ORDERS companies n B</context>
</contexts>
<marker>Choueka, Lusignan, 1985</marker>
<rawString>Choueka, Y. &amp; Lusignan, S. (1985). Disambiguation by short contexts. Computers and the Humanities, pp. 147-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>L M Gomez</author>
<author>S T Dumais</author>
</authors>
<title>The vocabulary problem in humansystem communication.</title>
<date>1987</date>
<journal>Communications of the ACM,</journal>
<volume>30</volume>
<pages>11--964</pages>
<contexts>
<context position="12621" citStr="Furnas et al, 1987" startWordPosition="1945" endWordPosition="1948">igh similarity to be found between the query and a document, since the matching is based on similarity of SFC vectors, not according to the particular words used. Therefore, it can be seen that the SFC representation, which is one level of abstraction above the actual words in a text, implicitly handles both the synonymy (multiple words having the same meaning) and polysemy (one word having multiple meanings) problems which have plagued the use of natural language in information retrieval systems. This level of abstraction is an essential feature of the representation since it has been shown (Furnas et al, 1987) that users&apos; information requests frequently share little vocabulary overlap with the documents which actually contain relevant information. 1, Lonaman&apos;s Dictionary of Contemporary EngHet Our text representation is based on the machine-readable version of Longman&apos;s Dictionary of Contemporary Enalish (LDOCE), a British-produced learner&apos;s dictionary. The first edition of LDOCE has been used in a number of investigations into natural language processing applications (Boguraev &amp; Briscoe, 1989). We are using the second edition (1987) which contains 35,899 headwords and 53,838 senses. The machinerea</context>
</contexts>
<marker>Furnas, Landauer, Gomez, Dumais, 1987</marker>
<rawString>Furnas, G.W., Landauer, T.K., Gomez, L.M. &amp; Dumais, S.T. (1987). The vocabulary problem in humansystem communication. Communications of the ACM, 30 (11):964-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Kelly</author>
<author>P J Stone</author>
</authors>
<title>Computer recognition of English word senses.</title>
<date>1975</date>
<publisher>North Holland Publishing Co.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="16341" citStr="Kelly &amp; Stone (1975)" startWordPosition="2542" endWordPosition="2545">eval system. I, Subiect Field Coding of Texts In the Subject Field Coder, the following stages of processing are done in order to generate a SFC vector representation of each text: In Stage 1 processing, we run the documents and query through POST, a probabilistic part of speech 23 tagger (Meeter et al, 1991) in order to restrict candidate SFCs of a word to those of just the appropriate syntactic category of each word as determined by POST. Stage 2 processing consists of retrieving the SFCs of each word&apos;s correct part of speech from the lexical database. The SFC retrieval process utilizes the Kelly &amp; Stone (1975) stemming algorithm to reduce morphological variants to their simple stem as found in LDOCE. Kelly &amp; Stone&apos;s approach is one of weak stemming and produces correct simple stems for look-up in LDOCE, rather than stripping suffixes. Having selected candidate SFCs for each word&apos;s correct syntactic category, we begin sense disambiguation at Stage 3, using sentence-level context-heuristics to determine a single word&apos;s correct SFC. We begin with context-heuristics because empirical results have shown that local context is used by humans for sense disambiguation (Choueka &amp; Lusignan, 1985) and context-</context>
</contexts>
<marker>Kelly, Stone, 1975</marker>
<rawString>Kelly, E. F. &amp; Stone, P. J. (1975). Computer recognition of English word senses. Amsterdam: North Holland Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>Lexical acquisition and information retrieval. In</title>
<date>1991</date>
<location>Hillsdale, NJ: Lawrence Earlbaum.</location>
<contexts>
<context position="15430" citStr="Krovetz (1991)" startWordPosition="2389" endWordPosition="2390"> the most frequent SFC to characterize a document&apos;s content, whereas we represent a document by a vector of frequencies of SFCs for words in that text. Stator (1991) has taken the original 124 SFCs and added an additional layer of seven pragmatic classes to the original two-level hierarchy. He has found the reconstructed hierarchy useful when attempting to disambiguate multiple senses and SFCs attached to words. His metric for preferring one sense over another relies on values within an individual text, whereas we add corpus correlation values as a further stage in the disambiguation process. Krovetz (1991) has been exploring the effect of combining the evidence from SFCs with evidence from other fields in LDOCE for selection of a correct word sense. His goal is to represent documents by their appropriate senses rather than just the orthographic forms of words, for use in an information retrieval system. I, Subiect Field Coding of Texts In the Subject Field Coder, the following stages of processing are done in order to generate a SFC vector representation of each text: In Stage 1 processing, we run the documents and query through POST, a probabilistic part of speech 23 tagger (Meeter et al, 1991</context>
</contexts>
<marker>Krovetz, 1991</marker>
<rawString>Krovetz, R. (1991). Lexical acquisition and information retrieval. In Zemik, U. (Ed.). Lexical acauisition: exoloitina on-line resources to build a lexicon. Hillsdale, NJ: Lawrence Earlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D Liddy</author>
<author>S H Myaeng</author>
</authors>
<title>DR-LINK&apos;s linguistic-conceptual approach to document detection. proceedinas of the First Text Retrieval Conference.</title>
<date>1993</date>
<publisher>NIST.</publisher>
<contexts>
<context position="5340" citStr="Liddy &amp; Myaeng, 1993" startWordPosition="807" endWordPosition="810">r relies solely on topic-based criteria. The goal of our filter is to efficiently and effectively skim off those documents which possess the greatest likelihood of proving relevant to a user&apos;s need, here conceived of as a natural language statement of their long-standing information requirement. Later stages of processing in the system will perform the more refined conceptual level of matching. 2. DR-LINK Project Our ongoing research into the development and implementation of an effective document filter has produced a module used within a larger document detection system, the DR-LINK System (Liddy &amp; Myaeng, 1993). The DR-LINK Project is research being conducted under the auspices of DARPA&apos;s TIPSTER Project whose goal is the development of algorithms both for the detection of documents of interest and the extraction of selected information from these documents for a large group of users. The DR-LINK system architecture is modular in design, with six separate processing modules. These modules enhance the documents at every stage by various semantic enrichments which are used to refine the flow of documents in terms of both appropriateness to the query and pure numbers. Briefly summarized, the six module</context>
</contexts>
<marker>Liddy, Myaeng, 1993</marker>
<rawString>Liddy, E.D. &amp; Myaeng, S. H. (1993). DR-LINK&apos;s linguistic-conceptual approach to document detection. proceedinas of the First Text Retrieval Conference. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D Liddy</author>
<author>W Paik</author>
</authors>
<title>Statistically-guided word sense disambiguation.</title>
<date>1992</date>
<booktitle>In Proceedinas of AAAI Fall &apos;92 Symposium on Probabilistic Approaches to Natural Lanauage.</booktitle>
<location>Boston.</location>
<contexts>
<context position="22766" citStr="Liddy &amp; Paik, 1992" startWordPosition="3621" endWordPosition="3624"> later in the entry must have a much higher correlation with a sentencedetermined SFC in order to be selected over the GENERAL (XX) code. The system implementation of the disambiguation procedures was tested on a sample of 1638 words from WSJ having SFCs in LDOCE. The system selected a single SFC for each word, which was compared to the sense-selections made by an independent judge who was instructed to read the sentences and the definitions of the senses of each word and then to select that sense which was most correct. Overall, the SFC disambiguator selected the correct SFC 89% of the time (Liddy &amp; Paik, 1992). Stage 6 processing produces a vector of SFCs and their frequencies for each document and for the query. At this point the non-substantive GENERAL SFCs are removed from the vector sums, since these contribute nothing to a text&apos;s subject content representation. In Stage 7, the vectors of each text are normalized using Sagees (1976) term weighting formula in order to control for the effect of document length. The choice of Sager&apos;s (1976) term weighting formula and Sager and Lockemann&apos;s (1976) similarity measure were based on an extensive study done by McGill, Koll &amp; Noreault (1979) which empiri</context>
</contexts>
<marker>Liddy, Paik, 1992</marker>
<rawString>Liddy, E.D. &amp; Paik, W. (1992). Statistically-guided word sense disambiguation. In Proceedinas of AAAI Fall &apos;92 Symposium on Probabilistic Approaches to Natural Lanauage. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D Liddy</author>
<author>W Paik</author>
<author>J Woelfel</author>
</authors>
<title>Use of subject field codes from a machine-readable dictionary for automatic classification of documents.</title>
<date>1992</date>
<booktitle>Proceedings of 3rd ASIS Classification Research Workshop.</booktitle>
<marker>Liddy, Paik, Woelfel, 1992</marker>
<rawString>Liddy, E.D., Paik, W. &amp; Woelfel, J. (1992). Use of subject field codes from a machine-readable dictionary for automatic classification of documents. Proceedings of 3rd ASIS Classification Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M McGill</author>
<author>M Koll</author>
<author>T Noreault</author>
</authors>
<title>An evaluation of factors affecting document rankina by information retrieval systems. Final report to National Science Foundation.</title>
<date>1979</date>
<institution>Syracuse University.</institution>
<location>Syracuse, NY:</location>
<contexts>
<context position="24031" citStr="McGill et al, 1979" startWordPosition="3815" endWordPosition="3818">ulae and twenty four similarity measures. Using the coefficient of ranking effectiveness (CRE) measure, each term weighting scheme was tested in combination with each similarity measure to determine which combination was best for either controlled representations or free text representations. Since we consider SFCodes to be a form of a controlled vocabulary (all free-text terms are reduced to 122 codes), we chose Sagees (1976) term weighting scheme and Sager &amp; Lockemann&apos;s (1976) similarity measure since they were shown to be the best formulae for use with controlled vocabulary representation (McGill et al, 1979). At Stage 8, the document vectors are compared to the query vector using Sager &amp; Lockemann&apos;s (1976) similarity measure and a ranked listing of the documents in decreasing order of similarity is produced. Having created this ranked list of documents for each query, the system must determine how many of these 25 documents should reasonably be passed on to the next module. The method used is an adaptive cut-off criterion that predicts for each query and at each recall level, how many documents from the ranked list should be forwarded. The cut-off criterion uses a multiple regression formula whic</context>
</contexts>
<marker>McGill, Koll, Noreault, 1979</marker>
<rawString>McGill, M., Koll, M., &amp; Noreault, T. (1979). An evaluation of factors affecting document rankina by information retrieval systems. Final report to National Science Foundation. Syracuse, NY: Syracuse University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>POST: Using probabilities in language processing.</title>
<date>1991</date>
<booktitle>Proceedinas of the Twelfth International Conference on Artificial Intelligence.</booktitle>
<location>Sydney, Australia.</location>
<marker>Meteer, Schwartz, Weischedel, 1991</marker>
<rawString>Meteer, M., Schwartz, R. &amp; Weischedel, R. (1991). POST: Using probabilities in language processing. Proceedinas of the Twelfth International Conference on Artificial Intelligence. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Paik</author>
<author>E D Liddy</author>
<author>E S Yu</author>
<author>M McKenna</author>
</authors>
<title>Extracting and classifying proper nouns in documents.</title>
<date>1993</date>
<booktitle>Proceedings of the Human Lanauaae Technolaav Workshoo.</booktitle>
<location>Princeton, NJ:</location>
<marker>Paik, Liddy, Yu, McKenna, 1993</marker>
<rawString>Paik, W., Liddy, E.D., Yu, E.S. &amp; McKenna, M. (In press). Extracting and classifying proper nouns in documents. Proceedings of the Human Lanauaae Technolaav Workshoo. Princeton, NJ: March, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W K H Sager</author>
<author>P C Lockemann</author>
</authors>
<title>Classification of ranking algorithms,</title>
<date>1976</date>
<booktitle>international forum on Information and Documentation.</booktitle>
<pages>1--4</pages>
<marker>Sager, Lockemann, 1976</marker>
<rawString>Sager, W.K.H. &amp; Lockemann, P.C. (1976). Classification of ranking algorithms, international forum on Information and Documentation. 1(4):2-25, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Stator</author>
</authors>
<title>Using context for sense preference. In</title>
<date>1991</date>
<location>Hillsdale, NJ: Lawrence Earlbaum.</location>
<contexts>
<context position="14981" citStr="Stator (1991)" startWordPosition="2320" endWordPosition="2321">ord in the text, so that the appropriate SFC for each orthographic form can be chosen. We have incorporated in our system means for choosing amongst each word&apos;s syntactic categories and senses found in LDOCE, thereby enabling the system to assign just one SFC to each word in a given text. In related research, Walker and Amster used the Subject Field Codes to determine the appropriate subject domains for a set of texts (1986). However, they used the most frequent SFC to characterize a document&apos;s content, whereas we represent a document by a vector of frequencies of SFCs for words in that text. Stator (1991) has taken the original 124 SFCs and added an additional layer of seven pragmatic classes to the original two-level hierarchy. He has found the reconstructed hierarchy useful when attempting to disambiguate multiple senses and SFCs attached to words. His metric for preferring one sense over another relies on values within an individual text, whereas we add corpus correlation values as a further stage in the disambiguation process. Krovetz (1991) has been exploring the effect of combining the evidence from SFCs with evidence from other fields in LDOCE for selection of a correct word sense. His </context>
</contexts>
<marker>Stator, 1991</marker>
<rawString>Stator, B. (1991). Using context for sense preference. In Zernik, U. (Ed.). Lexical acauisition: exoloiting online resources to build a lexicon. Hillsdale, NJ: Lawrence Earlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sowa</author>
</authors>
<title>Conceptual Structures:</title>
<date>1984</date>
<booktitle>Information Processina in Mind and Machine.</booktitle>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="8194" citStr="Sowa, 1984" startWordPosition="1248" endWordPosition="1249">e passed to the: 4) Relation-Concept Detector, which raises the level at which we do matching from a keyword or key-phrase level to a more conceptual level by expanding terms in the query to all terms which have been shown to be &apos;substitutable&apos; for them, and then by extracting semantic relations between concepts in both documents and queries. This component produces conceptrelation-concept triples which are passed to the: 23. 5) Conceptual Graph Generator which converts the triples into the Conceptual Graph (CG) formalism, a representation similar to semantic networks, but with labelled arcs (Sowa, 1984). The resultant CGs are passed to the: 6) Conceptual Graph Matcher, which measures the degree to which a particular query CG and candidate document CGs share a common structure, and then produces a final ranking of the documents. / Since the later modules in our system require very complex processing in order to produce conceptually enriched representations of documents and queries, preliminary filtering of the incoming flow of documents by means of the Subject Field Coder has proven to be extremely useful. For while CGs enable us to do finegrained representation, such fine-grained representat</context>
</contexts>
<marker>Sowa, 1984</marker>
<rawString>Sowa, J. (1984). Conceptual Structures: Information Processina in Mind and Machine. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Walker</author>
<author>R A Amsler</author>
</authors>
<title>The use of machine-readable dictionaries in sublanguage analysis.</title>
<date>1986</date>
<booktitle>In Grishman,</booktitle>
<location>Hillsdale, NJ: Lawrence Earlbaum.</location>
<marker>Walker, Amsler, 1986</marker>
<rawString>Walker, D. E. &amp; Amsler, R. A. (1986). The use of machine-readable dictionaries in sublanguage analysis. In Grishman, R. &amp; Kittredge, R. (Eds). i • n sind brocessinct. Hillsdale, NJ: Lawrence Earlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ward</author>
</authors>
<title>Hierarchical grouping to optimize an objection function.</title>
<date>1963</date>
<journal>Journal of the American Statistical Association.</journal>
<volume>58</volume>
<pages>237--254</pages>
<contexts>
<context position="29624" citStr="Ward, 1963" startWordPosition="4752" endWordPosition="4753">t to an individual query, we find these results promising. In a large operational system, the ability to filter out 61% of the incoming flux of millions of documents if the SFCoder alone is used, or 72% of the documents if the SFCoder + Proper Noun Interpreter is used, will have a significant impact on the system&apos;s performance. In addition, we have also been experimenting with the SFC vectors as a means for automatically clustering documents in an established database (Liddy, Paik &amp; Woelfel, 1992). To do this, the document vectors are clustered using Ward&apos;s agglomerative clustering algorithm (Ward, 1963) to form classes in a document database. For ad hoc retrieval, query SFC vectors are matched to the SFC vector of each cluster-centroid in the database. Clusters whose centroid vectors exhibit a predetermined similarity to the query SFC vector are either presented to the user as a semantically cohesive cluster on which to begin preliminary browsing or, passed on to other system components for further processing. A qualitative analysis of the clusters produced in this manner revealed that the use of SFCs combined with Ward&apos;s clustering algorithm resulted in meaningful groupings of documents tha</context>
</contexts>
<marker>Ward, 1963</marker>
<rawString>Ward, J. (1963). Hierarchical grouping to optimize an objection function. Journal of the American Statistical Association. 58, p. 237-254.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>