<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001616">
<title confidence="0.832689666666667">
JU-SKNSB: Extended WordNet Based WSD on the English All-Words
Task at SemEval-1
Sudip Kumar Naskar
</title>
<author confidence="0.628405">
Computer Sc. &amp; Engg. Dept.,
</author>
<affiliation confidence="0.7045455">
Jadavpur University,
Kolkata, India
</affiliation>
<email confidence="0.996696">
sudip.naskar@gmail.com
</email>
<author confidence="0.661268">
Sivaji Bandyopadhyay
</author>
<affiliation confidence="0.636785666666667">
Computer Sc. &amp; Engg. Dept.,
Jadavpur University,
Kolkata, India
</affiliation>
<email confidence="0.997664">
sivaji_cse_ju@yahoo.com
</email>
<sectionHeader confidence="0.995613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999629105263158">
This paper presents an Extended WordNet
based word sense disambiguation system
using a major modification to the Lesk al-
gorithm. The algorithm tries to disambigu-
ate nouns, verbs and adjectives. The algo-
rithm relies on the POS-sense tagged syn-
set glosses provided by the Extended
WordNet. The basic unit of disambiguation
of our algorithm is the entire sentence un-
der consideration. It takes a global ap-
proach where all the words in the target
sentence are simultaneously disambigu-
ated. The context includes previous and
next sentence. The system assigns the de-
fault WordNet first sense to a word when
the algorithm fails to predict the sense of
the word. The system produces a precision
and recall of .402 on the SemEval-2007
English All-Words test data.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999558857142857">
In Senseval 1, most of the systems disambiguating
English words, were outperformed by a Lesk vari-
ant serving as baseline(Kilgariff &amp; Rosenzweig,
2000). On the other hand, during Senseval 2 and
Senseval 3, Lesk baselines were outperformed by
most of the systems in the lexical sample track
(Edmonds, 2002).
In this paper, we explore variants of the Lesk al-
gorithm on the English All Words SemEval 2007
test data (465 instances), as well as on the first 10
Semcor 2.0 files (9642 instances). The proposed
WSD algorithm is POS-sense-tagged gloss (from
Extended WordNet) based and is a major modifi-
cation of the original Lesk algorithm.
</bodyText>
<sectionHeader confidence="0.988358" genericHeader="method">
2 Extended WordNet
</sectionHeader>
<bodyText confidence="0.999966363636364">
The eXtended WordNet (Harabagiu et al., 1999)
project aims to transform the WordNet glosses into
a format that allows the derivation of additional
semantic and logic relations. It intends to syntacti-
cally parse the glosses, transform glosses into logi-
cal forms and tag semantically the nouns, verbs,
adjectives and adverbs of the glosses automati-
cally. The last release of the Extended WordNet is
based on WordNet 2.0 and has three stages: POS
tagging and parsing, logic form transformation,
and semantic disambiguation.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
3 Related Works
</sectionHeader>
<bodyText confidence="0.999235210526316">
Banerjee and Pedersen (2002) reports an adapta-
tion of Lesk’s dictionary-based WSD algorithm
which makes use of WordNet glosses and tests on
English lexical sample from SENSEVAL-2. They de-
fine overlap as the longest sequence of one or more
consecutive content words that occurs in both
glosses. Each overlap contributes a score equal to
the square of the number of words in the overlap.
A version of Lesk algorithm in combination
with WordNet has been reported for achieving
good results in (Ramakrishnan et al., 2004).
Vasilescu et al. (2004) carried on a series of ex-
periments on the Lesk algorithm, adapted to
WordNet, and on some variants. They studied the
effect of varying the number of words in the con-
texts, centered around the target word.
But till now no work has been reported which
makes use of Extended WordNet for Lesk-like
gloss-oriented approach.
</bodyText>
<page confidence="0.984031">
203
</page>
<bodyText confidence="0.7500125">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203–206,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.962804" genericHeader="method">
4 Proposed Sense Disambiguation Algo-
rithm
</sectionHeader>
<bodyText confidence="0.99984375">
The proposed sense disambiguation algorithm is a
major modification of the Lesk algorithm (Lesk,
1986). WordNet and Extended WordNet are the
main resources.
</bodyText>
<subsectionHeader confidence="0.998815">
4.1 Modifications to the Lesk Algorithm
</subsectionHeader>
<bodyText confidence="0.999972636363636">
We modify the Lesk algorithm (Lesk, 1986) in
several ways to create our baseline algorithm. The
Lesk algorithm relies on glosses found in tradi-
tional dictionaries which often do not have enough
words for the algorithm to work well. We choose
the lexical database WordNet, to take advantage of
the highly inter–connected set of relations among
different words that WordNet offers, and Extended
WordNet to capitalize on its (POS and sense)
tagged glosses.
The Lesk algorithm takes a local approach for
sense disambiguation. The disambiguation of the
various words in a sentence is a series of inde-
pendent problems and has no effect on each other.
We propose a global approach where all the words
(we mean by word, an open-class lemma) in the
context window are simultaneously disambiguated
in a bid to get the best combination of senses for
all the words in the window instead of only the
target word. The process can be thought of as sense
disambiguation of the whole context, instead of a
word.
The Lesk algorithm disambiguates words in short
phrases. But, the basic unit of disambiguation of
our algorithm is the entire sentence under consid-
eration. We later modify the context to include the
previous and next sentence.
Another major change is that the dictionary
definition or gloss of each of its senses is com-
pared to the glosses of every other word in the con-
text by the Lesk algorithm. But in the present
work, the words themselves are compared with the
glosses of every other word in the context.
</bodyText>
<subsectionHeader confidence="0.999481">
4.2 Choice of Which Glosses to Use
</subsectionHeader>
<bodyText confidence="0.999862444444445">
While Lesk’s algorithm restricts its comparisons to
the dictionary meanings of the words being disam-
biguated, our choice of dictionary allows us to also
compare the meanings (i.e., glosses) of the words,
as well as the words that are related to them
through various relationships defined in WordNet.
For each POS we choose a relation if links of its
kind form at least 5% of the total number of links
for that part of speech, with two exceptions. We
use the attribute relation although there are not
many links of its kind. But this relation links adjec-
tives, which are not well developed in WordNet, to
nouns which have a lot of data about them. This
potential to tap into the rich noun data prompted us
to use this relation. Another exception is the an-
tonymy relationship. Although there are sufficient
antonymy links for adjectives and adverbs, we
have not utilized these relations.
</bodyText>
<table confidence="0.999687333333333">
Noun Verb Adjective
Hypernym Hyponym Attribute
Hyponym Troponym Also see
Holonym Also see Similar to
Meronym Pertainym of
Attribute
</table>
<tableCaption confidence="0.983266">
Table 1. WordNet relations chosen for the disam-
biguation algorithm
</tableCaption>
<subsectionHeader confidence="0.990965">
4.3 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999979241379311">
The gloss bag is constructed for every sense of
every word in the sentence. The gloss-bag is con-
structed from the POS and sense tagged glosses of
synsets, obtained from the Extended WordNet. For
any synset, the words forming the synset and the
gloss definition contribute to the gloss-bag. The
non-content words are left out. Example sentences
do not contribute to the gloss bag since they are not
(POS and sense) tagged. Each word along with its
POS and sense-tag are stored in the gloss bag. For
words with different POS, different relations are
taken into account (according to Table 1) for build-
ing the corresponding gloss-bag.
This gloss-bag creation process can be per-
formed offline or online. It can be performed dy-
namically on a as-when-needed basis. Or, gloss-
bags can be created for all WordNet entries only
once and stored in a data file in prior. The issue is
time versus space.
Once, this gloss-bag creation process is over, the
comparison process starts. Each word (say Wi) in
the context is compared with each word in the
gloss-bag for every sense (say Sk) of every other
word (say W;) in the context. If a match is found,
they are checked further for part-of-speech match.
If the words match in part-of-speech as well, a
score is assigned to both the words: the word being
matched (Wi) and the word whose gloss-bag con-
tains the match (W;). This matching event indicates
</bodyText>
<page confidence="0.993398">
204
</page>
<bodyText confidence="0.993075864864865">
mutual confidence towards each other, so both
words are rewarded for this event. Two two-
dimensional (one for word index and the other for
sense index) vectors are maintained: sense_vote for
the word in context, and sense_score for the word
in gloss-bag. Say, for example, the context word
(Wi # noun) matches with gloss word (Wn # noun #
m) (i.e., Wi = Wn) in the gloss bag for kth sense of
Wj. Then, a score of 1/(gloss bag size of (Wjk)) is
assigned to both sense_vote[i][m] and
sense_score[j][k]. Scores are normalized before
assigning because of huge discrepancy in gloss-bag
sizes. This process continues until each context
word is matched against all gloss-bag words for
each sense of every other context words.
Once all the comparisons have been made, we add
sense_vote value with the sense_score linearly
value for each sense of every word to arrive at the
combination score for this word-sense pair.
The algorithm assigns a word the nth sense for
which the corresponding sense_vote and
sense_score produces the maximum sum, and it
does not assign a word any sense when the corre-
sponding sense_vote and sense_score values are 0,
even if the word has only one sense. In the event of
a tie, we choose the one that is more frequent, as
specified by WordNet.
Assuming that there are N words in the window
of context (i.e. the sentence), and that, on an aver-
age there are S senses per word, and G number of
gloss words in each gloss bag per sense, N * S
gloss bags need to be constructed, giving rise to a
total of N * S * G gloss words. Now these many
gloss words are compared against each of the N
context words. Thus, N2 * S * G pairs of word
comparisons need to be performed. Both, S and G
vary heavily.
</bodyText>
<sectionHeader confidence="0.849109" genericHeader="method">
5 Variants of the Algorithm
</sectionHeader>
<bodyText confidence="0.9998242">
The algorithm discussed thus far is our baseline
algorithm. We made some changes, as described in
the following two subsections, to investigate
whether the performance of the algorithm can be
improved.
</bodyText>
<subsectionHeader confidence="0.985883">
5.1 Increasing the Context Size
</subsectionHeader>
<bodyText confidence="0.999833375">
The poor performance of the algorithm perhaps
suggests that sentential context is not enough for
this algorithm to work. So we went for a larger
context: a context window containing the current
sentence under consideration (target sentence), its
preceding sentence and the succeeding sentence.
This increment in context size indeed performed
better than the baseline algorithm.
</bodyText>
<subsectionHeader confidence="0.999844">
5.2 Assigning Different Scores
</subsectionHeader>
<bodyText confidence="0.998985333333333">
When constructing the gloss-bags for a word-sense
pair, some words may appear in more than one
gloss (by gloss we mean to say synonyms as well
as gloss). So, we added another parameter with
every (word#pos#sense) in a gloss bag: noc - the
number of occurrence of this (word#pos#sense)
combination in this gloss-bag.
And, in case of a match of context word (say Wi)
with a gloss-bag word (of say kth sense of word
Wj), we scored the words in four ways to see if this
phenomenon has any effect on the sense disam-
biguation process. Say, for example, the context
word (Wi # noun) matches with gloss word (Wn #
noun # m # noc) in the gloss bag for kth sense of Wj
(i.e., the particular word appears noc times in the
said gloss-bag) and the gloss bag size is gbs. Then,
we reward Wi and Wj for this event in four ways
given below.
</bodyText>
<listItem confidence="0.927873916666667">
1. Assign 1/gbs to
sense_vote[i][m] and 1/gbs
to sense_score[j][k].
2. Assign 1/gbs to
sense_vote[i][m] and noc/gbs
to sense_score[j][k].
3. Assign noc/gbs to
sense_vote[i][m] and 1/gbs
to sense_score[j][k].
4. Assign noc/gbs to
sense_vote[i][m] and noc/gbs
to sense_score[j][k].
</listItem>
<bodyText confidence="0.999886444444444">
The results of this four-way scoring proved that
this indeed has influence on the disambiguation
process.
The WSD system is based on Extended Word-
Net version 2.0-1.1 (the latest release), which is in
turn based on WordNet version 2.0. So, the system
returns WordNet 2.0 sense indexes. These Word-
Net sense indexes are then mapped to WordNet 2.1
sense indexes using sensemap 2.0 to 2.1.
</bodyText>
<sectionHeader confidence="0.985021" genericHeader="method">
6 Evaluations
</sectionHeader>
<footnote confidence="0.497306">
The system has been evaluated on the SemEval-
2007 English All-Words Tasks (465 test in-
</footnote>
<page confidence="0.99782">
205
</page>
<bodyText confidence="0.999019454545455">
stances), as well as on the first 10 Semcor 2.0
files, which are manually disambiguated text
corpora using WordNet senses.
We compute F-Score as 2*P*R / (P+R). Ta-
ble 2 shows the performance of the four variants of
the system (with a context size of 3 sentences)
on the first 10 Semcor 2.0 files. From table 2, it
is clearly evident that model C produces the best
result (precision - .621, recall - .533) among the 4
scoring schemes. POS-wise evaluation results for
model C on Semcor 2.0 data is given in table 3.
</bodyText>
<table confidence="0.9962104">
Model
A B C D
Precision .618 .602 .621 .604
Recall .531 .517 .533 .519
F-Score .571 .556 .574 .558
</table>
<tableCaption confidence="0.878466">
Table 2. Evaluation of the four models on Sem-
cor Data
</tableCaption>
<table confidence="0.999667">
Noun Verb Adj Overall
Precision .6977 .4272 .6694 .6211
Recall .6179 .3947 .4602 .5335
F-Score .6554 .4103 .5454 .574
</table>
<tableCaption confidence="0.99892">
Table 3. POS-wise Evaluation for model C on
</tableCaption>
<subsectionHeader confidence="0.48455">
Semcor Data
</subsectionHeader>
<bodyText confidence="0.999259">
Model C produced a precision of .393 and a re-
call of .359 on the SemEval-2007 English All-
Words test data (465 test instances). Table 4
shows POS-wise evaluation results for this test
data.
</bodyText>
<table confidence="0.9987525">
Noun Verb Overall
Precision .507 .331 .393
Recall .472 .299 .359
F-Score .489 .314 .375
</table>
<tableCaption confidence="0.997488">
Table 3. POS-wise Evaluation on SemEval-2007
</tableCaption>
<subsectionHeader confidence="0.532745">
English All-Words test data
</subsectionHeader>
<bodyText confidence="0.999621285714286">
When default WordNet first senses were as-
signed to the (40) words for which the algorithm
failed to predict senses, both the precision and re-
call values went up to .402 (this result has been
submitted in SemEval-2007). The WSD system
stood 10th in the SemEval-2007 English All-
Words task.
</bodyText>
<sectionHeader confidence="0.999052" genericHeader="conclusions">
7 Discussions
</sectionHeader>
<bodyText confidence="0.99995955">
We believe that this somewhat poor showing can
be partially attributed to the brevity of definitions
in WordNet in particular and dictionaries in gen-
eral. The Lesk algorithm is crucially dependent on
the lengths of glosses. However lexicographers
aim to create short and precise definitions which,
though a desirable quality in dictionaries, is disad-
vantageous to this algorithm. Nouns have the long-
est average glosses in WordNet, and indeed the
highest recall obtained is on nouns. The character-
istics of the gloss bags need to be further investi-
gated. Again many of the sense tagged gloss words
in Extended WordNet, which are determinant fac-
tors in this algorithm, are of “silver” or “normal”
quality. And finally, since the system returns
WordNet 2.0 sense indexes which are mapped to
WordNet 2.1 indexes with certain amount of con-
fidence using sensemap 2.0 to 2.1, there may be
some loss of information during this mapping
process.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999725777777778">
A. Kilgarriff, and J. Rosenzweig. 2000. Framework and
Results for English SENSEVAL. Computers and the
Humanities, 34, 15-48.
Florentina Vasilescu, Philippe Langlais, and Guy La-
palme. 2004. Evaluating Variants of the Lesk Ap-
proach for Disambiguating Words. LREC, Portugal.
G. Ramakrishnan, B. Prithviraj, and P. Bhattacharyya.
2004. A Gloss Centered Algorithm for Word Sense
Disambiguation. Proceedings of the ACL SEN-
SEVAL 2004, Barcelona, Spain, 217-221.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from a ice cream cone. Proceedings of SIGDOC
’86.
P. Edmonds. 2002. SENSEVAL : The Evaluation of
Word Sense Disambiguation Systems, ELRA News-
letter, Vol. 7, No. 3.
S. Banerjee. 2002. Adapting the Lesk Algorithm for
Word Sense Disambiguation to WordNet. MS Thesis,
University of Minnesota.
S. Banerjee, and T. Pedersen. 2002. An Adapted Lesk
Algorithm for Word Sense Disambiguation Using
WordNet. CICLing, Mexico.
S. Harabagiu, G. Miller, and D. Moldovan. 1999.
WordNet2 - a morphologically and semantically en-
hanced resource. Proceedings of SIGLEX-99, Univ of
Mariland. 1-8.
</reference>
<page confidence="0.998889">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.362394">
<title confidence="0.916257">JU-SKNSB: Extended WordNet Based WSD on the English All-Words Task at SemEval-1</title>
<author confidence="0.964082">Sudip Kumar Naskar</author>
<affiliation confidence="0.9509525">Computer Sc. &amp; Engg. Dept., Jadavpur University,</affiliation>
<address confidence="0.893449">Kolkata, India</address>
<email confidence="0.999752">sudip.naskar@gmail.com</email>
<author confidence="0.990244">Sivaji Bandyopadhyay</author>
<affiliation confidence="0.935946">Computer Sc. &amp; Engg. Dept., Jadavpur University,</affiliation>
<address confidence="0.890514">Kolkata, India</address>
<email confidence="0.999885">sivaji_cse_ju@yahoo.com</email>
<abstract confidence="0.98266175">This paper presents an Extended WordNet based word sense disambiguation system using a major modification to the Lesk algorithm. The algorithm tries to disambiguate nouns, verbs and adjectives. The algorithm relies on the POS-sense tagged synset glosses provided by the Extended WordNet. The basic unit of disambiguation of our algorithm is the entire sentence under consideration. It takes a global approach where all the words in the target sentence are simultaneously disambiguated. The context includes previous and next sentence. The system assigns the default WordNet first sense to a word when the algorithm fails to predict the sense of the word. The system produces a precision and recall of .402 on the SemEval-2007 English All-Words test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<date>2000</date>
<booktitle>Framework and Results for English SENSEVAL. Computers and the Humanities,</booktitle>
<volume>34</volume>
<pages>15--48</pages>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff, and J. Rosenzweig. 2000. Framework and Results for English SENSEVAL. Computers and the Humanities, 34, 15-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florentina Vasilescu</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>Evaluating Variants of the Lesk Approach for Disambiguating Words.</title>
<date>2004</date>
<location>LREC,</location>
<contexts>
<context position="2805" citStr="Vasilescu et al. (2004)" startWordPosition="442" endWordPosition="445">parsing, logic form transformation, and semantic disambiguation. 3 Related Works Banerjee and Pedersen (2002) reports an adaptation of Lesk’s dictionary-based WSD algorithm which makes use of WordNet glosses and tests on English lexical sample from SENSEVAL-2. They define overlap as the longest sequence of one or more consecutive content words that occurs in both glosses. Each overlap contributes a score equal to the square of the number of words in the overlap. A version of Lesk algorithm in combination with WordNet has been reported for achieving good results in (Ramakrishnan et al., 2004). Vasilescu et al. (2004) carried on a series of experiments on the Lesk algorithm, adapted to WordNet, and on some variants. They studied the effect of varying the number of words in the contexts, centered around the target word. But till now no work has been reported which makes use of Extended WordNet for Lesk-like gloss-oriented approach. 203 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203–206, Prague, June 2007. c�2007 Association for Computational Linguistics 4 Proposed Sense Disambiguation Algorithm The proposed sense disambiguation algorithm is a major modificati</context>
</contexts>
<marker>Vasilescu, Langlais, Lapalme, 2004</marker>
<rawString>Florentina Vasilescu, Philippe Langlais, and Guy Lapalme. 2004. Evaluating Variants of the Lesk Approach for Disambiguating Words. LREC, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ramakrishnan</author>
<author>B Prithviraj</author>
<author>P Bhattacharyya</author>
</authors>
<title>A Gloss Centered Algorithm for Word Sense Disambiguation.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL SENSEVAL 2004,</booktitle>
<pages>217--221</pages>
<location>Barcelona,</location>
<contexts>
<context position="2780" citStr="Ramakrishnan et al., 2004" startWordPosition="438" endWordPosition="441">ree stages: POS tagging and parsing, logic form transformation, and semantic disambiguation. 3 Related Works Banerjee and Pedersen (2002) reports an adaptation of Lesk’s dictionary-based WSD algorithm which makes use of WordNet glosses and tests on English lexical sample from SENSEVAL-2. They define overlap as the longest sequence of one or more consecutive content words that occurs in both glosses. Each overlap contributes a score equal to the square of the number of words in the overlap. A version of Lesk algorithm in combination with WordNet has been reported for achieving good results in (Ramakrishnan et al., 2004). Vasilescu et al. (2004) carried on a series of experiments on the Lesk algorithm, adapted to WordNet, and on some variants. They studied the effect of varying the number of words in the contexts, centered around the target word. But till now no work has been reported which makes use of Extended WordNet for Lesk-like gloss-oriented approach. 203 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203–206, Prague, June 2007. c�2007 Association for Computational Linguistics 4 Proposed Sense Disambiguation Algorithm The proposed sense disambiguation algori</context>
</contexts>
<marker>Ramakrishnan, Prithviraj, Bhattacharyya, 2004</marker>
<rawString>G. Ramakrishnan, B. Prithviraj, and P. Bhattacharyya. 2004. A Gloss Centered Algorithm for Word Sense Disambiguation. Proceedings of the ACL SENSEVAL 2004, Barcelona, Spain, 217-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone.</title>
<date>1986</date>
<booktitle>Proceedings of SIGDOC ’86.</booktitle>
<contexts>
<context position="3442" citStr="Lesk, 1986" startWordPosition="541" endWordPosition="542">periments on the Lesk algorithm, adapted to WordNet, and on some variants. They studied the effect of varying the number of words in the contexts, centered around the target word. But till now no work has been reported which makes use of Extended WordNet for Lesk-like gloss-oriented approach. 203 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203–206, Prague, June 2007. c�2007 Association for Computational Linguistics 4 Proposed Sense Disambiguation Algorithm The proposed sense disambiguation algorithm is a major modification of the Lesk algorithm (Lesk, 1986). WordNet and Extended WordNet are the main resources. 4.1 Modifications to the Lesk Algorithm We modify the Lesk algorithm (Lesk, 1986) in several ways to create our baseline algorithm. The Lesk algorithm relies on glosses found in traditional dictionaries which often do not have enough words for the algorithm to work well. We choose the lexical database WordNet, to take advantage of the highly inter–connected set of relations among different words that WordNet offers, and Extended WordNet to capitalize on its (POS and sense) tagged glosses. The Lesk algorithm takes a local approach for sense</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone. Proceedings of SIGDOC ’86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Edmonds</author>
</authors>
<title>SENSEVAL : The Evaluation of Word Sense Disambiguation Systems,</title>
<date>2002</date>
<journal>ELRA Newsletter,</journal>
<volume>7</volume>
<contexts>
<context position="1378" citStr="Edmonds, 2002" startWordPosition="212" endWordPosition="213">ltaneously disambiguated. The context includes previous and next sentence. The system assigns the default WordNet first sense to a word when the algorithm fails to predict the sense of the word. The system produces a precision and recall of .402 on the SemEval-2007 English All-Words test data. 1 Introduction In Senseval 1, most of the systems disambiguating English words, were outperformed by a Lesk variant serving as baseline(Kilgariff &amp; Rosenzweig, 2000). On the other hand, during Senseval 2 and Senseval 3, Lesk baselines were outperformed by most of the systems in the lexical sample track (Edmonds, 2002). In this paper, we explore variants of the Lesk algorithm on the English All Words SemEval 2007 test data (465 instances), as well as on the first 10 Semcor 2.0 files (9642 instances). The proposed WSD algorithm is POS-sense-tagged gloss (from Extended WordNet) based and is a major modification of the original Lesk algorithm. 2 Extended WordNet The eXtended WordNet (Harabagiu et al., 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. It intends to syntactically parse the glosses, transform glosses into logic</context>
</contexts>
<marker>Edmonds, 2002</marker>
<rawString>P. Edmonds. 2002. SENSEVAL : The Evaluation of Word Sense Disambiguation Systems, ELRA Newsletter, Vol. 7, No. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
</authors>
<title>Adapting the Lesk Algorithm for Word Sense Disambiguation to WordNet. MS Thesis,</title>
<date>2002</date>
<institution>University of Minnesota.</institution>
<marker>Banerjee, 2002</marker>
<rawString>S. Banerjee. 2002. Adapting the Lesk Algorithm for Word Sense Disambiguation to WordNet. MS Thesis, University of Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2002</date>
<tech>CICLing, Mexico.</tech>
<contexts>
<context position="2291" citStr="Banerjee and Pedersen (2002)" startWordPosition="357" endWordPosition="360">odification of the original Lesk algorithm. 2 Extended WordNet The eXtended WordNet (Harabagiu et al., 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. It intends to syntactically parse the glosses, transform glosses into logical forms and tag semantically the nouns, verbs, adjectives and adverbs of the glosses automatically. The last release of the Extended WordNet is based on WordNet 2.0 and has three stages: POS tagging and parsing, logic form transformation, and semantic disambiguation. 3 Related Works Banerjee and Pedersen (2002) reports an adaptation of Lesk’s dictionary-based WSD algorithm which makes use of WordNet glosses and tests on English lexical sample from SENSEVAL-2. They define overlap as the longest sequence of one or more consecutive content words that occurs in both glosses. Each overlap contributes a score equal to the square of the number of words in the overlap. A version of Lesk algorithm in combination with WordNet has been reported for achieving good results in (Ramakrishnan et al., 2004). Vasilescu et al. (2004) carried on a series of experiments on the Lesk algorithm, adapted to WordNet, and on </context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>S. Banerjee, and T. Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. CICLing, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>G Miller</author>
<author>D Moldovan</author>
</authors>
<title>WordNet2 - a morphologically and semantically enhanced resource.</title>
<date>1999</date>
<booktitle>Proceedings of SIGLEX-99, Univ of Mariland.</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1771" citStr="Harabagiu et al., 1999" startWordPosition="276" endWordPosition="279">erformed by a Lesk variant serving as baseline(Kilgariff &amp; Rosenzweig, 2000). On the other hand, during Senseval 2 and Senseval 3, Lesk baselines were outperformed by most of the systems in the lexical sample track (Edmonds, 2002). In this paper, we explore variants of the Lesk algorithm on the English All Words SemEval 2007 test data (465 instances), as well as on the first 10 Semcor 2.0 files (9642 instances). The proposed WSD algorithm is POS-sense-tagged gloss (from Extended WordNet) based and is a major modification of the original Lesk algorithm. 2 Extended WordNet The eXtended WordNet (Harabagiu et al., 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. It intends to syntactically parse the glosses, transform glosses into logical forms and tag semantically the nouns, verbs, adjectives and adverbs of the glosses automatically. The last release of the Extended WordNet is based on WordNet 2.0 and has three stages: POS tagging and parsing, logic form transformation, and semantic disambiguation. 3 Related Works Banerjee and Pedersen (2002) reports an adaptation of Lesk’s dictionary-based WSD algorithm which makes use </context>
</contexts>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>S. Harabagiu, G. Miller, and D. Moldovan. 1999. WordNet2 - a morphologically and semantically enhanced resource. Proceedings of SIGLEX-99, Univ of Mariland. 1-8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>