<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.881861333333333">
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech
Drahomira “johanka” Spoustov´a
</title>
<author confidence="0.8909125">
Jan Hajiˇc
Jan Votrubec
</author>
<affiliation confidence="0.995864">
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
</affiliation>
<email confidence="0.496671">
{johanka,hajic,votrubec}@
ufal.mff.cuni.cz
</email>
<note confidence="0.527531666666667">
Pavel Krbec
IBM Czech Republic,
Voice Technologies and Systems,
</note>
<address confidence="0.641687">
Prague, Czech Republic,
</address>
<email confidence="0.956685">
pavel krbec@cz.ibm.com
</email>
<author confidence="0.996487">
Pavel Kvˇetoˇn
</author>
<affiliation confidence="0.9978705">
Institute of the Czech Language,
Academy of Sciences of the Czech Republic
</affiliation>
<email confidence="0.973977">
Pavel.Kveton@seznam.cz
</email>
<sectionHeader confidence="0.992451" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999442">
Several hybrid disambiguation methods are
described which combine the strength of
hand-written disambiguation rules and sta-
tistical taggers. Three different statistical
(HMM, Maximum-Entropy and Averaged
Perceptron) taggers are used in a tagging
experiment using Prague Dependency Tree-
bank. The results of the hybrid systems are
better than any other method tried for Czech
tagging so far.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975181818182">
Inflective languages pose a specific problem in tag-
ging due to two phenomena: highly inflective na-
ture (causing sparse data problem in any statistically
based system), and free word order (causing fixed-
context systems, such as n-gram HMMs, to be even
less adequate than for English).
The average tagset contains about 1,000 – 2,000
distinct tags; the size of the set of possible and plau-
sible tags can reach several thousands. There have
been attempts at solving this problem for some of
the highly inflective European languages, such as
</bodyText>
<page confidence="0.99183">
67
</page>
<bodyText confidence="0.9972215">
(Daelemans, 1996), (Erjavec, 1999) for Slovenian
and (Hajiˇc, 2000) for five Central and Eastern Euro-
pean languages.
Several taggers already exist for Czech, e.g.
(Hajiˇc et al., 2001b), (Smith, 2005), (Hajiˇc et al.,
2006) and (Votrubec, 2006). The last one reaches
the best accuracy for Czech so far (95.12 %). Hence
no system has reached – in the absolute terms – a
performance comparable to English tagging (such as
(Ratnaparkhi, 1996)), which stands above 97 %.
We are using the Prague Dependency Treebank
(Hajiˇc et al., 2006) (PDT) with about 1.8 million
hand annotated tokens of Czech for training and test-
ing. The tagging experiments in this paper all use
the Czech morphological (pre)processor, which in-
cludes a guesser for “unknown” tokens and which is
available from the PDT website (PDT Guide, 2006)
to disambiguate only among those tags which are
morphologically plausible.
The meaning of the Czech tags (each tag has 15
positions) we are using is explained in Table 1. The
detailed linguistic description of the individual posi-
tions can be found in the documentation to the PDT
(Hajiˇc et al., 2006).
</bodyText>
<subsubsectionHeader confidence="0.590911">
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 67–74,
</subsubsectionHeader>
<bodyText confidence="0.444402">
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<note confidence="0.9899945625">
Name Description
1 POS Part of Speech
2 SUBPOS Detailed POS
3 GENDER Gender
4 NUMBER Number
5 CASE Case
6 POSSGENDER Possessor’s Gender
7 POSSNUMBER Possessor’s Number
8 PERSON Person
9 TENSE Tense
10 GRADE Degree of comparison
11 NEGATION Negation
12 VOICE Voice
13 RESERVE1 Unused
14 RESERVE2 Unused
15 VAR Variant
</note>
<tableCaption confidence="0.99405">
Table 1: Czech Morphology and the Positional Tags
</tableCaption>
<sectionHeader confidence="0.347544" genericHeader="method">
2 Components of the hybrid system
</sectionHeader>
<subsectionHeader confidence="0.951798">
2.1 The HMM tagger
</subsectionHeader>
<bodyText confidence="0.974089">
The HMM tagger is based on the well known for-
mula of HMM tagging:
</bodyText>
<equation confidence="0.9992195">
T = arg max P(T)P(W  |T) (1)
T
</equation>
<bodyText confidence="0.536955">
where
</bodyText>
<equation confidence="0.998353">
P(W|T) ^ Hni=1 P(wi  |ti, ti−1) (2)
P(T) ^ IIZ1 P(ti  |ti−1, ti−2).
</equation>
<bodyText confidence="0.989436666666667">
The trigram probability P(W  |T) in formula 2
replaces (Hajiˇc et al., 2001b) the common (and less
accurate) bigram approach. We will use this tagger
as a baseline system for further improvements.
Initially, we change the formula 1 by introduc-
ing a scaling mechanism1: T� = arg maxT(AT �
</bodyText>
<equation confidence="0.942966">
logP(T) + logP(W  |T)).
</equation>
<bodyText confidence="0.99907675">
We tag the word sequence from right to left, i.e.
we change the trigram probability P(W  |T) from
formula 2 to P(wi  |ti, ti+1).
Both the output probability P(wi  |ti, ti+1) and
the transition probability P(T) suffer a lot due to
the data sparseness problem. We introduce a com-
ponent P(endingi  |ti, ti+1), where ending con-
sists of the last three characters of wi. Also, we in-
troduce another component P(t∗i  |t∗i+1, t∗i+2) based
on a reduced tagset T∗ that contains positions POS,
GENDER, NUMBER and CASE only (chosen on
linguistic grounds).
</bodyText>
<footnote confidence="0.9545535">
1The optimum value of the scaling parameter AT can be
tuned using held-out data.
</footnote>
<bodyText confidence="0.999540888888889">
We upgrade all trigrams to fourgrams; the
smoothing mechanism for fourgrams is history-
based bucketing (Krbec, 2005).
The final fine-tuned HMM tagger thus uses all
the enhancements and every component contains its
scaling factor which has been computed using held-
out data. The total error rate reduction is 13.98 %
relative on development data, measured against the
baseline HMM tagger.
</bodyText>
<subsectionHeader confidence="0.999844">
2.2 Morˇce
</subsectionHeader>
<bodyText confidence="0.99986315">
The Morˇce2 tagger assumes some of the HMM prop-
erties at runtime, namely those that allow the Viterbi
algorithm to be used to find the best tag sequence for
a given text. However, the transition weights are not
probabilities. They are estimated by an Averaged
Perceptron described in (Collins, 2002). Averaged
Perceptron works with features which describe the
current tag and its context.
Features can be derived from any information we
already have about the text. Every feature can be
true or false in a given context, so we can regard
current true features as a description of the current
tag context.
For every feature, the Averaged Perceptron stores
its weight coefficient, which is typically an integer
number. The whole task of Averaged Perceptron is
to sum all the coefficients of true features in a given
context. The result is passed to the Viterbi algorithm
as a transition weight for a given tag. Mathemati-
cally, we can rewrite it as:
</bodyText>
<equation confidence="0.992475">
n
w(C,T) = αi.Oi(C,T) (3)
i=1
</equation>
<bodyText confidence="0.9988203">
where w(C, T) is the transition weight for tag T in
context C, n is number of features, αi is the weight
coefficient of ith feature and O(C, T)i is evaluation
of ith feature for context C and tag T.
Weight coefficients (α) are estimated on training
data, cf. (Votrubec, 2006). The training algorithm
is very simple, therefore it can be quickly retrained
and it gives a possibility to test many different sets of
features (Votrubec, 2005). As a result, Morˇce gives
the best accuracy from the standalone taggers.
</bodyText>
<footnote confidence="0.998691">
2The name Morˇce stands for “MORfologie ˇCEˇstiny”
(“Czech morphology”).
</footnote>
<page confidence="0.999015">
68
</page>
<subsectionHeader confidence="0.999356">
2.3 The Feature-Based Tagger
</subsectionHeader>
<bodyText confidence="0.99899925">
The Feature-based tagger, taken also from the PDT
(Hajiˇc et al., 2006) distribution used in our exper-
iments uses a general log-linear model in its basic
formulation:
</bodyText>
<equation confidence="0.999529666666667">
exp(�� i�1 Aifi(y,x))
pAC(y  |x) = (4)
Z(x)
</equation>
<bodyText confidence="0.999983533333333">
where fi(y, x) is a binary-valued feature of the event
value being predicted and its context, Ai is a weight
of the feature fi, and the Z(x) is the natural normal-
ization factor.
The weights Ai are approximated by Maximum
Likelihood (using the feature counts relative to all
feature contexts found), reducing the model essen-
tially to Naive Bayes. The approximation is nec-
essary due to the millions of the possible features
which make the usual entropy maximization infeasi-
ble. The model makes heavy use of single-category
Ambiguity Classes (AC)3, which (being indepen-
dent on the tagger’s intermediate decisions) can be
included in both left and right contexts of the fea-
tures.
</bodyText>
<subsectionHeader confidence="0.999505">
2.4 The rule-based component
</subsectionHeader>
<bodyText confidence="0.999982090909091">
The approach to tagging (understood as a stand-
alone task) using hand-written disambiguation rules
has been proposed and implemented for the first
time in the form of Constraint-Based Grammars
(Karlsson, 1995). On a larger scale, this aproach was
applied to English, (Karlsson, 1995) and (Samuels-
son, 1997), and French (Chanod, 1995). Also (Bick,
2000) uses manually written disambiguation rules
for tagging Brazilian Portuguese, (Karlsson, 1985)
and (Koskenniemi, 1990) for Finish and (Oflazer,
1997) reports the same for Turkish.
</bodyText>
<subsectionHeader confidence="0.825225">
2.4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999787375">
In the hybrid tagging system presented in this pa-
per, the rule-based component is used to further re-
duce the ambiguity (the number of tags) of tokens
in an input sentence, as output by the morphological
processor (see Sect. 1). The core of the component
is a hand-written grammar (set of rules).
Each rule represents a portion of knowledge of
the language system (in particular, of Czech). The
</bodyText>
<footnote confidence="0.703807">
3If a token can be a N(oun), V(erb) or A(djective), its (major
POS) Ambiguity Class is the value “ANV”.
</footnote>
<bodyText confidence="0.9999226">
knowledge encoded in each rule is formally defined
in two parts: a sequence of tokens that is searched
for in an input sentence and the tags that can be
deleted if the sequence of tokens is found.
The overall strategy of this “negative” grammar is
to keep the highest recall possible (i.e. 100 %) and
gradually improve precision. In other words, when-
ever a rule deletes a tag, it is (almost) 100% safe that
the deleted tag is “incorrect” in the sentence, i.e. the
tag cannot be present in any correct tagging of the
sentence.
Such an (virtually) “error-free” grammar can par-
tially disambiguate any input and prevent the subse-
quent taggers (stochastic, in our case) to choose tags
that are “safely incorrect”.
</bodyText>
<subsectionHeader confidence="0.658386">
2.4.2 The rules
</subsectionHeader>
<bodyText confidence="0.9996613">
Formally, each rule consists of the description of
the context (sequence of tokens with some special
property), and the action to be performed given the
context (which tags are to be discarded). The length
of context is not limited by any constant; however,
for practical purposes, the context cannot cross over
sentence boundaries.
For example: in Czech, two finite verbs cannot
appear within one clause. This fact can be used to
define the following disambiguation rule:
</bodyText>
<listItem confidence="0.997061375">
• context: unambiguous finite verb, fol-
lowed/preceded by a sequence of tokens
containing neither a comma nor a coordinat-
ing conjunction, at either side of a word x
ambiguous between a finite verb and another
reading;
• action: delete the finite verb reading(s) at the
word x.
</listItem>
<bodyText confidence="0.999953909090909">
It is obvious that no rule can contain knowledge
of the whole language system. In particular, each
rule is focused on at most a few special phenomena
of the language. But whenever a rule deletes a tag
from a sentence, the information about the sentence
structure “increases”. This can help other rules to be
applied and to delete more and more tags.
For example, let’s have an input sentence with two
finite verbs within one clause, both of them ambigu-
ous with some other (non-finite-verbal) tags. In this
situation, the sample rule above cannot be applied.
</bodyText>
<page confidence="0.996206">
69
</page>
<bodyText confidence="0.9999602">
On the other hand, if some other rule exists in the
grammar that can delete non-finite-verbal tags from
one of the tokens, then the way for application of the
sample rule is opened.
The rules operate in a loop in which (theoreti-
cally) all rules are applied again whenever a rule
deletes a tag in the partially disambiguated sentence.
Since deletion is a monotonic operation, the algo-
rithm is guaranteed to terminate; effective imple-
mentation has also been found in (Kvˇetoˇn, 2006).
</bodyText>
<subsectionHeader confidence="0.887831">
2.4.3 Grammar used in tests
</subsectionHeader>
<bodyText confidence="0.9933475">
The grammar is being developed since 2000 as
a standalone module that performs Czech morpho-
logical disambiguation. There are two ways of rule
development:
</bodyText>
<listItem confidence="0.817735833333333">
• the rules developed by syntactic introspection:
such rules are subsequently verified on the cor-
pus material, then implemented and the imple-
mented rules are tested on a testing corpus;
• the rules are derived from the corpus by intro-
spection and subsequently implemented.
</listItem>
<bodyText confidence="0.9929048">
In particular, the rules are not based on examina-
tion of errors of stochastic taggers.
The set of rules is (manually) divided into two
(disjoint) reliability classes — safe rules (100% re-
liable rules) and heuristics (highly reliable rules, but
obscure exceptions can be found). The safe rules re-
flect general syntactic regularities of Czech; for in-
stance, no word form in the nominative case can fol-
low an unambiguous preposition. The less reliable
heuristic rules can be exemplified by those account-
ing for some special intricate relations of grammati-
cal agreement in Czech.
The grammar consists of 1727 safe rules and 504
heuristic rules. The system has been used in two
ways:
</bodyText>
<listItem confidence="0.98084825">
• safe rules only: in this mode, safe rules are ex-
ecuted in the loop until some tags are being
deleted. The system terminates as soon as no
rule can delete any tag.
• all rules: safe rules are executed first (see safe
rules only mode). Then heuristic rules start
to operate in the loop (similarly to the safe
rules). Any time a heuristic rule deletes a tag,
</listItem>
<bodyText confidence="0.9890704">
the safe rules only mode is entered as a sub-
procedure. When safe rules’ execution termi-
nates, the loop of heuristic rules continues. The
disambiguation is finished when no heuristic
rule can delete any tag.
The rules are written in the fast LanGR formalism
(Kvˇetoˇn, 2006) which is a subset of more general
LanGR formalism (Kvˇetoˇn, 2005). The LanGR for-
malism has been developed specially for writing and
implementing disambiguation rules.
</bodyText>
<sectionHeader confidence="0.858704" genericHeader="method">
3 Methods of combination
</sectionHeader>
<subsectionHeader confidence="0.992852">
3.1 Serial combination
</subsectionHeader>
<bodyText confidence="0.9983592">
The simplest way of combining a hand-written dis-
ambiguation grammar with a stochastic tagger is to
let the grammar reduce the ambiguity of the tagger’s
input. Formally, an input text is processed as fol-
lows:
</bodyText>
<listItem confidence="0.987737285714286">
1. morphological analysis (every input token gets
all tags that are plausible without looking at
context);
2. rule-based component (partially disambiguates
the input, i.e. deletes some tags);
3. the stochastic tagger (gets partially disam-
biguated text on its input).
</listItem>
<bodyText confidence="0.993603222222222">
This algorithm was already used in (Hajiˇc et
al., 2001b), only components were changed — the
ruled-based component was significantly improved
and two different sets of rules were tried, as well
as three different statistical taggers. The best result
was (not surprisingly) achieved with set of safe rules
followed by the Morˇce tagger.
An identical approach was used in (Tapanainen,
1994) for English.
</bodyText>
<subsectionHeader confidence="0.9773905">
3.2 Serial combination with SUBPOS
pre-processing
</subsectionHeader>
<bodyText confidence="0.999520833333333">
Manual inspection of the output of the application of
the hand-written rules on the development data (as
used in the serial combination described in the pre-
vious section) discovered that certain types of dead-
locked (“cross-dependent”) rules prevent successful
disambiguation.
</bodyText>
<page confidence="0.990088">
70
</page>
<bodyText confidence="0.998684">
Cross-dependence means that a rule A can not
apply because of some remaining ambiguity, which
could be resolved by a rule B, but the operation of B
is still dependent on the application of A. In particu-
lar, ambiguity in the Part-of-Speech category is very
problematic. For example, only a few safe rules can
apply to a three-word sentence where all three words
are ambiguous between finite verbs and something
else.
If the Part-of-Speech ambiguity of the input is al-
ready resolved, precision of the rule-based compo-
nent and also of the final result after applying any of
the statistical taggers improves. Full Part-of-Speech
information is represented by the first two categories
of the Czech morphology tagset — POS and SUB-
POS, which deals with different types of pronouns,
adverbs etc. As POS is uniquely determined by
SUBPOS (Hajiˇc et al., 2006), it is sufficient to re-
solve the SUBPOS ambiguity only.
All three taggers achieve more than 99% accuracy
in SUBPOS disambiguation. For SUBPOS disam-
biguation, we use the taggers in usual way (i.e. they
determine the whole tag) and then we put back all
tags having the same SUBPOS as the tag chosen by
the tagger.
Thus, the method with SUBPOS pre-processing
operates in four steps:
</bodyText>
<listItem confidence="0.99786075">
1. morphological analysis;
2. SUBPOS disambiguation (any tagger);
3. rule-based component;
4. final disambiguation (the same tagger4).
</listItem>
<bodyText confidence="0.978789">
The best results were again achieved with the tag-
ger Morˇce and set of safe rules.
</bodyText>
<subsectionHeader confidence="0.999229">
3.3 Combining more taggers in parallel
</subsectionHeader>
<bodyText confidence="0.998603666666667">
This method is quite different from previous ones,
because it essentially needs more than one tagger. It
consists of the following steps:
</bodyText>
<footnote confidence="0.706894333333333">
1. morphological analysis;
4This limitation is obviously not necessary, but we treat this
combination primarily as a one-tagger method. Results of em-
ploying two different taggers are only slightly better, but still
much worse than results of other methods presented later be-
low.
</footnote>
<listItem confidence="0.9980065">
2. running N taggers independently;
3. merging the results from the previous step —
each token ends up with between 1 and N tags,
a union of the taggers’ outputs;
4. (optional: the rule-based component;)
5. final disambiguation (single tagger).
</listItem>
<bodyText confidence="0.999931647058824">
The best results were achieved with two taggers
in Step 1 (Feature-based and Morˇce), set of all rules
in Step 3 and the HMM tagger in Step 4.
This method is based on an assumption that dif-
ferent stochastic taggers make complementary mis-
takes, so that the recall of the “union” of taggers
is almost 100 %. Several existing language mod-
els are based on this assumption — (Brill, 1998)
for tagging English, (Borin, 2000) for tagging Ger-
man and (Vidov´a-Hladk´a, 2000) for tagging inflec-
tive languages. All these models perform some kind
of “voting” — for every token, one tagger is selected
as the most appropriate to supply the correct tag.
The model presented in this paper, however, entrusts
the selection of the correct tag to another tagger that
already operates on the partially disambiguated in-
put.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999458764705882">
All the methods presented in this paper have been
trained and tested on the PDT version 2.05. Tag-
gers were trained on PDT 2.0 training data set
(1,539,241 tokens), the results were achieved on
PDT 2.0 evaluation-test data set (219,765 tokens),
except Table 6, where PDT 2.0 development-test
data set (201,651 tokens) was used. The morpholog-
ical analysis processor and all the taggers were used
in versions from April 2006 (Hajiˇc et al., 2006), the
rule-based component is from September 2006.
For evaluation, we use both precision and recall
(and the corresponding F-measure) and accuracy,
since we also want to evaluate the partial disam-
biguation achieved by the hand-written rules alone.
Let t denote the number of tokens in the test data,
let c denote the number of tags assigned to all to-
kens by a disambiguation process and let h denote
</bodyText>
<footnote confidence="0.995823">
5The results cannot be simply (number-to-number) com-
pared to previous results on Czech tagging, because different
training and testing data (PDT 2.0 instead of PDT 1.0) are used
since 2006.
</footnote>
<page confidence="0.999201">
71
</page>
<bodyText confidence="0.9957035">
the number of tokens where the manually assigned
tag is present in the output of the process.
</bodyText>
<listItem confidence="0.892559454545454">
• In case of the morphological analysis processor
and the standalone rule-based component, the
output can contain more than one tag for ev-
ery token. Then precision (p), recall (r) and F-
measure (f) characteristics are defined as fol-
lows:
p = h/c r = h/t f = 2pr/(p + r).
• The output of the stochastic taggers contains al-
ways exactly one tag for every token — then
p = r = f = h/t holds and this ratio is de-
noted as accuracy.
</listItem>
<bodyText confidence="0.9999088125">
Table 2 shows the performance of the morpholog-
ical analysis processor and the standalone rule-based
component. Table 3 shows the performance of the
standalone taggers. The improvement of the combi-
nation methods is presented in Table 4.
Table 5 shows the relative error rate reduction.
The best method presented by this paper (parallel
combination of taggers with all rules) reaches the
relative error rate decrease of 11.48 % in compari-
son with the tagger Morˇce (which achieves the best
results for Czech so far).
Table 6 shows error rate (100 % − accuracy) of
various methods6 on particular positions of the tags
(13 and 14 are omitted). The most problematic posi-
tion is CASE (5), whose error rate was significantly
reduced.
</bodyText>
<sectionHeader confidence="0.99386" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.986389830188679">
We have presented several variations of a novel
method for combining statistical and hand-written
rule-based tagging. In all cases, the rule-based
component brings an improvement — the smaller
the involvement of the statistical component(s) is,
the bigger. The smallest gain can be observed
in the case of the parallel combination of taggers
(which by itself brings an expected improvement).
The best variation improved the accuracy of the
best-performing standalone statistical tagger by over
6F-b stands for feature-based taggeer, Par for parallel com-
bination without rules and Par+Rul for parallel combination
with rules.
11 % (in terms of relative error rate reduction), and
the inclusion of the rule-component itself improved
the best statistical-only combination by over 3.5 %
relative.
This might actually lead to pessimism regarding
the rule-based component. Most other inflective lan-
guages however have much smaller datasets avail-
able than Czech has today; in those cases, we expect
that the contribution of the rule-based component
(which does not depend on the training data size, ob-
viously) will be much more substantial.
The LanGR formalism, now well-developed,
could be used for relatively fast development for
other languages. We are, of course, unable to give
exact figures of what will take less effort — whether
to annotate more data or to develop the rule-based
component for a particular language. Our feeling is
that the jury is actually still out on this issue, de-
spite some people saying that annotation is always
cheaper: annotation for morphologically complex
(e.g., inflective) languages is not cheap, and rule-
based development efforts have not been previously
using (unannotated) corpora so extensively (which
is what LanGR supports for “testing” the developed
rules, leading to more reliable rules and more effec-
tive development cycle).
On the other hand, the rule-based component has
also two obvious and well-known disadvantages: it
is language dependent, and the application of the
rules is slower than even the baseline HMM tagger
despite the “fast” version of the LanGR implemen-
tation we are using7.
In any case, our experiments produced a software
suite which gives the all-time best results in Czech
tagging, and we have offered to apply it to re-tag the
existing 200 mil. word Czech National Corpus. It
should significantly improve the user experience (for
searching the corpus) and allow for more precise ex-
periments with parsing and other NLP applications
that use that corpus.
</bodyText>
<footnote confidence="0.959062666666667">
7In the tests presented in this paper, the speed of the op-
eration of each stochastic tagger (and the parallel combination
without rules) is several hundreds of tokens processed per sec-
ond (running on a 2.2GHz Opteron processor). The operation of
the standalone rule-based component, however, is cca 10 times
slower — about 40 tokens per second. The parallel combination
with all rules processes about 60 tokens per second — the rules
operate faster here because their input in parallel combination
is already partially disambiguated.
</footnote>
<page confidence="0.985795">
72
</page>
<table confidence="0.99906425">
Method p r f
Morphology 25.72 % 99.39 % 40.87 %
Safe rules 57.90 % 98.83 % 73.02 %
All rules 66.35 % 98.03 % 79.14 %
</table>
<tableCaption confidence="0.986871">
Table 2: Evaluation of rules alone
</tableCaption>
<table confidence="0.99728475">
Tagger accuracy
Feature-based 94.04 %
HMM 94.82 %
Morˇce 95.12 %
</table>
<tableCaption confidence="0.998397">
Table 3: Evaluation of the taggers alone
</tableCaption>
<table confidence="0.9987732">
Combination method accuracy
Serial (safe rules+Morˇce) 95.34 %
SUBPOS serial (safe rules+Morˇce) 95.44 %
Parallel without rules 95.52 %
Parallel with all rules 95.68 %
</table>
<tableCaption confidence="0.997535">
Table 4: Evaluation of the combinations
</tableCaption>
<table confidence="0.9998036">
Method Morˇce Parallel
without
rules
Parallel without rules 8.20 % —
Parallel with all rules 11.48 % 3.57 %
</table>
<tableCaption confidence="0.987303">
Table 5: Relative error rate reduction
</tableCaption>
<table confidence="0.999982642857143">
F-b HMM Morˇce Par Par+Rul
1 0.61 0.70 0.66 0.57 0.57
2 0.69 0.78 0.75 0.64 0.64
3 1.82 1.49 1.66 1.39 1.37
4 1.56 1.30 1.38 1.18 1.15
5 4.03 3.53 3.08 2.85 2.62
6 0.02 0.03 0.03 0.02 0.02
7 0.01 0.01 0.01 0.01 0.01
8 0.06 0.07 0.08 0.06 0.05
9 0.05 0.08 0.07 0.05 0.04
10 0.29 0.28 0.30 0.26 0.27
11 0.29 0.31 0.33 0.28 0.28
12 0.05 0.08 0.06 0.05 0.04
15 0.31 0.31 0.31 0.28 0.29
</table>
<tableCaption confidence="0.998275">
Table 6: Error rate [%] on particular positions of tags
</tableCaption>
<sectionHeader confidence="0.989304" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997876">
The research described here was supported by the
projects MSM0021620838 and LC536 of Ministry of
Eduation, Youth and Sports of the Czech Republic,
GA405/06/0589 of the Grant Agency of the Czech
Republic and 1ET100610409 Diagnostic and Eval-
uation Tools for Linguistic Software of the Informa-
tion Society Programme of the National Research
Programme of the Czech Republic.
</bodyText>
<sectionHeader confidence="0.995909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991595277777778">
Eckhard Bick. 2000. The parsing system “Palavras”
— automatic grammatical analysis of Portuguese in a
constraint grammar framework. In: Proceedings of the
2nd International Conference on Language Resources
and Evaluation, TELRI. Athens
Lars Borin. 2000. Something borrowed, something blue:
Rule-based combination of POS taggers. In: Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation, Vol. 1, pp. 21–26. Athens
Eric Brill and Jun Wu. 1998. Classifier combination
for improved lexical disambiguation. In: Proceedings
of the 17th international conference on Computational
linguistics, Vol. 1, pp. 191–195. Montreal, Quebec
Jean-Pierre Chanod and Pasi Tapanainen. 1995. Tagging
French — comparing a statistical and a constraint-
based method. In: Proceedings of EACL-95, pp. 149–
157. Dublin
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In: Proceedings
of EMNLP’02, July 2002, pp. 1–8. Philadelphia
W. Daelemans and Jakub Zavrel and Peter Berck and
Steven Gillis. 1996. MBT: A memory-based part of
speech tagger-generator. In: Proceedings of the 4th
WVLC, pp. 14–27. Copenhagen
Tomaz Erjavec and Saso Dzeroski and Jakub Zavrel.
1999. Morphosyntactic Tagging of Slovene: Evaluat-
ing PoS Taggers and Tagsets. Technical Report, Dept.
for Intelligent Systems, Jozef Stefan Institute. Ljubl-
jana
Jan Hajiˇc and Barbora Hladk´a. 1997. Tagging of in-
flective languages: a comparison. In: Proceedings of
ANLP ‘97, pp. 136–143. Washington, DC.
Jan Hajiˇc 2000. Morphological tagging: Data vs. dic-
tionaries. In: Proceedings of the 6th ANLP / 1st
NAACL’00, pp. 94–101. Seattle, WA
</reference>
<page confidence="0.983553">
73
</page>
<reference confidence="0.9950608">
Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva and
Vladimfr Petkeviˇc. 2001. Serial Combination of
Rules and Statistics: A Case Study in Czech Tag-
ging. In: Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics. CNRS
– Institut de Recherche en Informatique de Toulouse
and Universit´e des Sciences Sociales, pp. 260–267.
Toulouse
Jan Hajiˇc, Eva Hajiˇcov´a, Jarmila Panevov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka
and Marie Mikulov´a. 2006. Prague De-
pendency Treebank v2.0. CDROM. Linguis-
tic Data Consortium, Cat. LDC2006T01. Philadel-
phia. ISBN 1-58563-370-4. Documentation also at
http://ufal.ms.mff.cuni.cz/pdt2.0.
Fred Karlsson. 1985. Parsing Finnish in terms of a pro-
cess grammar. In: Fred Karlsson (ed.): Computational
Morphosyntax: Report on Research 1981-84, Univer-
sity of Helsinki, Department of General Linguistics
Publications No. 13, pp. 137–176.
Fred Karlsson and Atro Voutilainen and Juha Heikkil¨a
and Arto Anttila (eds.). 1995. Constraint Grammar: a
language-independent system for parsing unrestricted
text. Natural Language Processing. Vol. 4, Mouton
de Gruyter, Berlin and New York.
Kimmo Koskenniemi. 1990. Finite-State Parsing and
Disambiguation. In: Proceedings of Coling-90, Uni-
versity of Helsinki, 1990, pp. 229–232. Helsinki
Pavel Krbec. 2005. Language Modelling for Speech
Recognition of Czech. PhD Thesis, MFF, Charles Uni-
versity Prague.
Pavel Kvˇetoˇn. 2005. Rule-based Morphological Dis-
ambiguation. PhD Thesis, MFF, Charles University
Prague.
Pavel Kvˇetoˇn. 2006. Rule-based morphological dis-
ambiguation: On computational complexity of the
LanGR formalism. In: The Prague Bulletin of Mathe-
matical Linguistics, Vol. 85, pp. 57–72. Prague
Kemal Oflazer and G¨okhan T¨ur. 1997. Morphological
disambiguation by voting constraints. In: Proceedings
of the 8th conference on European chapter of the As-
sociation for Computational Linguistics, pp. 222–229.
Madrid
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In: Proceedings of the 1st
EMNLP, May 1996, pp. 133–142. Philadelphia
Christer Samuelsson and Atro Voluntainen. 1997. Com-
paring a linguistic and a stochastic tagger. In: Pro-
ceedings of ACL/EACL Joint Converence, pp. 246–
252. Madrid
Noah A. Smith and David A. Smith and Roy W.
Tromble. 2005. Context-Based Morphological Dis-
ambiguation with Random Fields. In: Proceedings of
HLT/EMNLP, pp. 475–482. Vancouver
Drahomfra “johanka” Spoustov´a. in prep. Kombino-
van´e statisticko-pravidlov´e metody znaˇckov´an!ˇceˇstiny.
(Combining Statistical and Rule-Based Approaches to
Morphological Tagging of Czech Texts). PhD Thesis,
MFF UK, in prep.
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging
accurately: don’t guess if you know. In: Proceedings
of the 4th conference on Applied Natural Language
Processing, pp. 47–52. Stuttgart
Barbora Vidov´a-Hladk´a. 2000. Czech Language Tag-
ging. PhD thesis, ´UFAL MFF UK. Prague
Jan Votrubec. 2005. Volba vhodn´ych rys˚u pro morfolog-
ick´e znaˇckov´an!ˇceˇstiny. (Feature Selection for Mor-
phological Tagging of Czech.) Master thesis, MFF,
Charles University, Prague.
Jan Votrubec. 2006. Morphological Tagging Based on
Averaged Perceptron. In: WDS’06 Proceedings of
Contributed Papers, MFF UK, pp. 191–195. Prague
Karel Oliva, Milena Hn´atkov´a, Vladimfr Petkeviˇc and
Pavel Kvˇetoˇn. 2000. The Linguistic Basis of a Rule-
Based Tagger of Czech. In: Sojka P., Kopeˇcek I.,
Pala K. (eds.): Proceedings of the Conference ”Text,
Speech and Dialogue 2000”, Lecture Notes in Artifi-
cial Intelligence, Vol. 1902. Springer-Verlag, pp. 3–8.
Berlin-Heidelberg
PDTGuide. http://ufal.ms.mff.cuni.cz/pdt2.0
</reference>
<page confidence="0.998746">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.123170">
<title confidence="0.948487666666667">The Best of Two Worlds: Cooperation of and Rule-Based Taggers for Czech Drahomira “johanka”</title>
<author confidence="0.97881">Jan Jan</author>
<affiliation confidence="0.876555">Institute of Formal and Applied Faculty of Mathematics and Charles University Prague, Czech</affiliation>
<email confidence="0.940225">ufal.mff.cuni.cz</email>
<author confidence="0.968599">Pavel</author>
<affiliation confidence="0.976725">IBM Czech Voice Technologies and</affiliation>
<address confidence="0.717882">Prague, Czech</address>
<email confidence="0.846873">pavelkrbec@cz.ibm.com</email>
<author confidence="0.925721">Pavel</author>
<affiliation confidence="0.9191415">Institute of the Czech Academy of Sciences of the Czech</affiliation>
<email confidence="0.432721">Pavel.Kveton@seznam.cz</email>
<abstract confidence="0.998332727272727">Several hybrid disambiguation methods are described which combine the strength of hand-written disambiguation rules and statistical taggers. Three different statistical (HMM, Maximum-Entropy and Averaged Perceptron) taggers are used in a tagging experiment using Prague Dependency Treebank. The results of the hybrid systems are better than any other method tried for Czech tagging so far.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eckhard Bick</author>
</authors>
<title>The parsing system “Palavras” — automatic grammatical analysis of Portuguese in a constraint grammar framework. In:</title>
<date>2000</date>
<booktitle>Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<location>TELRI. Athens</location>
<contexts>
<context position="7518" citStr="Bick, 2000" startWordPosition="1219" endWordPosition="1220">tropy maximization infeasible. The model makes heavy use of single-category Ambiguity Classes (AC)3, which (being independent on the tagger’s intermediate decisions) can be included in both left and right contexts of the features. 2.4 The rule-based component The approach to tagging (understood as a standalone task) using hand-written disambiguation rules has been proposed and implemented for the first time in the form of Constraint-Based Grammars (Karlsson, 1995). On a larger scale, this aproach was applied to English, (Karlsson, 1995) and (Samuelsson, 1997), and French (Chanod, 1995). Also (Bick, 2000) uses manually written disambiguation rules for tagging Brazilian Portuguese, (Karlsson, 1985) and (Koskenniemi, 1990) for Finish and (Oflazer, 1997) reports the same for Turkish. 2.4.1 Overview In the hybrid tagging system presented in this paper, the rule-based component is used to further reduce the ambiguity (the number of tags) of tokens in an input sentence, as output by the morphological processor (see Sect. 1). The core of the component is a hand-written grammar (set of rules). Each rule represents a portion of knowledge of the language system (in particular, of Czech). The 3If a token</context>
</contexts>
<marker>Bick, 2000</marker>
<rawString>Eckhard Bick. 2000. The parsing system “Palavras” — automatic grammatical analysis of Portuguese in a constraint grammar framework. In: Proceedings of the 2nd International Conference on Language Resources and Evaluation, TELRI. Athens</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Borin</author>
</authors>
<title>Something borrowed, something blue: Rule-based combination of POS taggers. In:</title>
<date>2000</date>
<booktitle>Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<volume>1</volume>
<pages>21--26</pages>
<location>Athens</location>
<contexts>
<context position="16486" citStr="Borin, 2000" startWordPosition="2703" endWordPosition="2704">results from the previous step — each token ends up with between 1 and N tags, a union of the taggers’ outputs; 4. (optional: the rule-based component;) 5. final disambiguation (single tagger). The best results were achieved with two taggers in Step 1 (Feature-based and Morˇce), set of all rules in Step 3 and the HMM tagger in Step 4. This method is based on an assumption that different stochastic taggers make complementary mistakes, so that the recall of the “union” of taggers is almost 100 %. Several existing language models are based on this assumption — (Brill, 1998) for tagging English, (Borin, 2000) for tagging German and (Vidov´a-Hladk´a, 2000) for tagging inflective languages. All these models perform some kind of “voting” — for every token, one tagger is selected as the most appropriate to supply the correct tag. The model presented in this paper, however, entrusts the selection of the correct tag to another tagger that already operates on the partially disambiguated input. 4 Results All the methods presented in this paper have been trained and tested on the PDT version 2.05. Taggers were trained on PDT 2.0 training data set (1,539,241 tokens), the results were achieved on PDT 2.0 eva</context>
</contexts>
<marker>Borin, 2000</marker>
<rawString>Lars Borin. 2000. Something borrowed, something blue: Rule-based combination of POS taggers. In: Proceedings of the 2nd International Conference on Language Resources and Evaluation, Vol. 1, pp. 21–26. Athens</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Jun Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation. In:</title>
<date>1998</date>
<booktitle>Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<volume>1</volume>
<pages>191--195</pages>
<location>Montreal, Quebec</location>
<marker>Brill, Wu, 1998</marker>
<rawString>Eric Brill and Jun Wu. 1998. Classifier combination for improved lexical disambiguation. In: Proceedings of the 17th international conference on Computational linguistics, Vol. 1, pp. 191–195. Montreal, Quebec</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Pierre Chanod</author>
<author>Pasi Tapanainen</author>
</authors>
<title>Tagging French — comparing a statistical and a constraintbased method. In:</title>
<date>1995</date>
<booktitle>Proceedings of EACL-95,</booktitle>
<pages>149--157</pages>
<location>Dublin</location>
<marker>Chanod, Tapanainen, 1995</marker>
<rawString>Jean-Pierre Chanod and Pasi Tapanainen. 1995. Tagging French — comparing a statistical and a constraintbased method. In: Proceedings of EACL-95, pp. 149– 157. Dublin</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In:</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP’02,</booktitle>
<pages>1--8</pages>
<location>Philadelphia</location>
<contexts>
<context position="4957" citStr="Collins, 2002" startWordPosition="795" endWordPosition="796">torybased bucketing (Krbec, 2005). The final fine-tuned HMM tagger thus uses all the enhancements and every component contains its scaling factor which has been computed using heldout data. The total error rate reduction is 13.98 % relative on development data, measured against the baseline HMM tagger. 2.2 Morˇce The Morˇce2 tagger assumes some of the HMM properties at runtime, namely those that allow the Viterbi algorithm to be used to find the best tag sequence for a given text. However, the transition weights are not probabilities. They are estimated by an Averaged Perceptron described in (Collins, 2002). Averaged Perceptron works with features which describe the current tag and its context. Features can be derived from any information we already have about the text. Every feature can be true or false in a given context, so we can regard current true features as a description of the current tag context. For every feature, the Averaged Perceptron stores its weight coefficient, which is typically an integer number. The whole task of Averaged Perceptron is to sum all the coefficients of true features in a given context. The result is passed to the Viterbi algorithm as a transition weight for a g</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In: Proceedings of EMNLP’02, July 2002, pp. 1–8. Philadelphia</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger-generator. In:</title>
<date>1996</date>
<booktitle>Proceedings of the 4th WVLC,</booktitle>
<pages>14--27</pages>
<location>Copenhagen</location>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans and Jakub Zavrel and Peter Berck and Steven Gillis. 1996. MBT: A memory-based part of speech tagger-generator. In: Proceedings of the 4th WVLC, pp. 14–27. Copenhagen</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaz Erjavec</author>
<author>Saso Dzeroski</author>
<author>Jakub Zavrel</author>
</authors>
<title>Morphosyntactic Tagging of Slovene: Evaluating PoS Taggers and Tagsets.</title>
<date>1999</date>
<tech>Technical Report,</tech>
<institution>Dept. for Intelligent Systems, Jozef Stefan Institute.</institution>
<location>Ljubljana</location>
<marker>Erjavec, Dzeroski, Zavrel, 1999</marker>
<rawString>Tomaz Erjavec and Saso Dzeroski and Jakub Zavrel. 1999. Morphosyntactic Tagging of Slovene: Evaluating PoS Taggers and Tagsets. Technical Report, Dept. for Intelligent Systems, Jozef Stefan Institute. Ljubljana</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Barbora Hladk´a</author>
</authors>
<title>Tagging of inflective languages: a comparison. In:</title>
<date>1997</date>
<booktitle>Proceedings of ANLP ‘97,</booktitle>
<pages>136--143</pages>
<location>Washington, DC.</location>
<marker>Hajiˇc, Hladk´a, 1997</marker>
<rawString>Jan Hajiˇc and Barbora Hladk´a. 1997. Tagging of inflective languages: a comparison. In: Proceedings of ANLP ‘97, pp. 136–143. Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Morphological tagging: Data vs. dictionaries. In:</title>
<date>2000</date>
<booktitle>Proceedings of the 6th ANLP / 1st NAACL’00,</booktitle>
<pages>94--101</pages>
<location>Seattle, WA</location>
<marker>Hajiˇc, 2000</marker>
<rawString>Jan Hajiˇc 2000. Morphological tagging: Data vs. dictionaries. In: Proceedings of the 6th ANLP / 1st NAACL’00, pp. 94–101. Seattle, WA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
<author>Karel Oliva</author>
<author>Vladimfr Petkeviˇc</author>
</authors>
<title>Serial Combination of Rules and Statistics: A Case Study in Czech Tagging. In:</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics. CNRS – Institut de Recherche en Informatique de Toulouse and Universit´e des Sciences Sociales,</booktitle>
<pages>260--267</pages>
<location>Toulouse</location>
<marker>Hajiˇc, Krbec, Kvˇetoˇn, Oliva, Petkeviˇc, 2001</marker>
<rawString>Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva and Vladimfr Petkeviˇc. 2001. Serial Combination of Rules and Statistics: A Case Study in Czech Tagging. In: Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics. CNRS – Institut de Recherche en Informatique de Toulouse and Universit´e des Sciences Sociales, pp. 260–267. Toulouse</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>JiˇrfHavelka</author>
<author>Marie Mikulov´a</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank v2.0. CDROM. Linguistic Data Consortium, Cat. LDC2006T01. Philadelphia. ISBN</booktitle>
<pages>1--58563</pages>
<note>Documentation also at http://ufal.ms.mff.cuni.cz/pdt2.0.</note>
<marker>Hajiˇc, Hajiˇcov´a, Panevov´a, Sgall, Pajas, ˇStˇep´anek, JiˇrfHavelka, Mikulov´a, 2006</marker>
<rawString>Jan Hajiˇc, Eva Hajiˇcov´a, Jarmila Panevov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka and Marie Mikulov´a. 2006. Prague Dependency Treebank v2.0. CDROM. Linguistic Data Consortium, Cat. LDC2006T01. Philadelphia. ISBN 1-58563-370-4. Documentation also at http://ufal.ms.mff.cuni.cz/pdt2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Parsing Finnish in terms of a process grammar.</title>
<date>1985</date>
<booktitle>Computational Morphosyntax: Report on Research 1981-84, University of Helsinki, Department of General Linguistics Publications No. 13,</booktitle>
<pages>137--176</pages>
<editor>In: Fred Karlsson (ed.):</editor>
<contexts>
<context position="7612" citStr="Karlsson, 1985" startWordPosition="1230" endWordPosition="1231">ses (AC)3, which (being independent on the tagger’s intermediate decisions) can be included in both left and right contexts of the features. 2.4 The rule-based component The approach to tagging (understood as a standalone task) using hand-written disambiguation rules has been proposed and implemented for the first time in the form of Constraint-Based Grammars (Karlsson, 1995). On a larger scale, this aproach was applied to English, (Karlsson, 1995) and (Samuelsson, 1997), and French (Chanod, 1995). Also (Bick, 2000) uses manually written disambiguation rules for tagging Brazilian Portuguese, (Karlsson, 1985) and (Koskenniemi, 1990) for Finish and (Oflazer, 1997) reports the same for Turkish. 2.4.1 Overview In the hybrid tagging system presented in this paper, the rule-based component is used to further reduce the ambiguity (the number of tags) of tokens in an input sentence, as output by the morphological processor (see Sect. 1). The core of the component is a hand-written grammar (set of rules). Each rule represents a portion of knowledge of the language system (in particular, of Czech). The 3If a token can be a N(oun), V(erb) or A(djective), its (major POS) Ambiguity Class is the value “ANV”. k</context>
</contexts>
<marker>Karlsson, 1985</marker>
<rawString>Fred Karlsson. 1985. Parsing Finnish in terms of a process grammar. In: Fred Karlsson (ed.): Computational Morphosyntax: Report on Research 1981-84, University of Helsinki, Department of General Linguistics Publications No. 13, pp. 137–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen and Juha Heikkil¨a and Arto Anttila (eds.).</title>
<date>1995</date>
<booktitle>Natural Language Processing. Vol. 4, Mouton de Gruyter,</booktitle>
<location>Berlin and New York.</location>
<contexts>
<context position="7375" citStr="Karlsson, 1995" startWordPosition="1196" endWordPosition="1197"> reducing the model essentially to Naive Bayes. The approximation is necessary due to the millions of the possible features which make the usual entropy maximization infeasible. The model makes heavy use of single-category Ambiguity Classes (AC)3, which (being independent on the tagger’s intermediate decisions) can be included in both left and right contexts of the features. 2.4 The rule-based component The approach to tagging (understood as a standalone task) using hand-written disambiguation rules has been proposed and implemented for the first time in the form of Constraint-Based Grammars (Karlsson, 1995). On a larger scale, this aproach was applied to English, (Karlsson, 1995) and (Samuelsson, 1997), and French (Chanod, 1995). Also (Bick, 2000) uses manually written disambiguation rules for tagging Brazilian Portuguese, (Karlsson, 1985) and (Koskenniemi, 1990) for Finish and (Oflazer, 1997) reports the same for Turkish. 2.4.1 Overview In the hybrid tagging system presented in this paper, the rule-based component is used to further reduce the ambiguity (the number of tags) of tokens in an input sentence, as output by the morphological processor (see Sect. 1). The core of the component is a han</context>
</contexts>
<marker>Karlsson, 1995</marker>
<rawString>Fred Karlsson and Atro Voutilainen and Juha Heikkil¨a and Arto Anttila (eds.). 1995. Constraint Grammar: a language-independent system for parsing unrestricted text. Natural Language Processing. Vol. 4, Mouton de Gruyter, Berlin and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Finite-State Parsing and Disambiguation. In:</title>
<date>1990</date>
<booktitle>Proceedings of Coling-90,</booktitle>
<pages>229--232</pages>
<location>University of Helsinki,</location>
<contexts>
<context position="7636" citStr="Koskenniemi, 1990" startWordPosition="1233" endWordPosition="1234">ng independent on the tagger’s intermediate decisions) can be included in both left and right contexts of the features. 2.4 The rule-based component The approach to tagging (understood as a standalone task) using hand-written disambiguation rules has been proposed and implemented for the first time in the form of Constraint-Based Grammars (Karlsson, 1995). On a larger scale, this aproach was applied to English, (Karlsson, 1995) and (Samuelsson, 1997), and French (Chanod, 1995). Also (Bick, 2000) uses manually written disambiguation rules for tagging Brazilian Portuguese, (Karlsson, 1985) and (Koskenniemi, 1990) for Finish and (Oflazer, 1997) reports the same for Turkish. 2.4.1 Overview In the hybrid tagging system presented in this paper, the rule-based component is used to further reduce the ambiguity (the number of tags) of tokens in an input sentence, as output by the morphological processor (see Sect. 1). The core of the component is a hand-written grammar (set of rules). Each rule represents a portion of knowledge of the language system (in particular, of Czech). The 3If a token can be a N(oun), V(erb) or A(djective), its (major POS) Ambiguity Class is the value “ANV”. knowledge encoded in each</context>
</contexts>
<marker>Koskenniemi, 1990</marker>
<rawString>Kimmo Koskenniemi. 1990. Finite-State Parsing and Disambiguation. In: Proceedings of Coling-90, University of Helsinki, 1990, pp. 229–232. Helsinki</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Krbec</author>
</authors>
<title>Language Modelling for Speech Recognition of Czech.</title>
<date>2005</date>
<tech>PhD Thesis,</tech>
<institution>MFF, Charles University Prague.</institution>
<contexts>
<context position="4376" citStr="Krbec, 2005" startWordPosition="700" endWordPosition="701">+1). Both the output probability P(wi |ti, ti+1) and the transition probability P(T) suffer a lot due to the data sparseness problem. We introduce a component P(endingi |ti, ti+1), where ending consists of the last three characters of wi. Also, we introduce another component P(t∗i |t∗i+1, t∗i+2) based on a reduced tagset T∗ that contains positions POS, GENDER, NUMBER and CASE only (chosen on linguistic grounds). 1The optimum value of the scaling parameter AT can be tuned using held-out data. We upgrade all trigrams to fourgrams; the smoothing mechanism for fourgrams is historybased bucketing (Krbec, 2005). The final fine-tuned HMM tagger thus uses all the enhancements and every component contains its scaling factor which has been computed using heldout data. The total error rate reduction is 13.98 % relative on development data, measured against the baseline HMM tagger. 2.2 Morˇce The Morˇce2 tagger assumes some of the HMM properties at runtime, namely those that allow the Viterbi algorithm to be used to find the best tag sequence for a given text. However, the transition weights are not probabilities. They are estimated by an Averaged Perceptron described in (Collins, 2002). Averaged Perceptr</context>
</contexts>
<marker>Krbec, 2005</marker>
<rawString>Pavel Krbec. 2005. Language Modelling for Speech Recognition of Czech. PhD Thesis, MFF, Charles University Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Kvˇetoˇn</author>
</authors>
<title>Rule-based Morphological Disambiguation.</title>
<date>2005</date>
<tech>PhD Thesis,</tech>
<institution>MFF, Charles University Prague.</institution>
<marker>Kvˇetoˇn, 2005</marker>
<rawString>Pavel Kvˇetoˇn. 2005. Rule-based Morphological Disambiguation. PhD Thesis, MFF, Charles University Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Kvˇetoˇn</author>
</authors>
<title>Rule-based morphological disambiguation: On computational complexity of the LanGR formalism.</title>
<date>2006</date>
<journal>In: The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>85</volume>
<pages>57--72</pages>
<location>Prague</location>
<marker>Kvˇetoˇn, 2006</marker>
<rawString>Pavel Kvˇetoˇn. 2006. Rule-based morphological disambiguation: On computational complexity of the LanGR formalism. In: The Prague Bulletin of Mathematical Linguistics, Vol. 85, pp. 57–72. Prague</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Morphological disambiguation by voting constraints. In:</title>
<date>1997</date>
<booktitle>Proceedings of the 8th conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>222--229</pages>
<location>Madrid</location>
<marker>Oflazer, T¨ur, 1997</marker>
<rawString>Kemal Oflazer and G¨okhan T¨ur. 1997. Morphological disambiguation by voting constraints. In: Proceedings of the 8th conference on European chapter of the Association for Computational Linguistics, pp. 222–229. Madrid</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging. In:</title>
<date>1996</date>
<booktitle>Proceedings of the 1st EMNLP,</booktitle>
<pages>133--142</pages>
<location>Philadelphia</location>
<contexts>
<context position="1922" citStr="Ratnaparkhi, 1996" startWordPosition="285" endWordPosition="286"> set of possible and plausible tags can reach several thousands. There have been attempts at solving this problem for some of the highly inflective European languages, such as 67 (Daelemans, 1996), (Erjavec, 1999) for Slovenian and (Hajiˇc, 2000) for five Central and Eastern European languages. Several taggers already exist for Czech, e.g. (Hajiˇc et al., 2001b), (Smith, 2005), (Hajiˇc et al., 2006) and (Votrubec, 2006). The last one reaches the best accuracy for Czech so far (95.12 %). Hence no system has reached – in the absolute terms – a performance comparable to English tagging (such as (Ratnaparkhi, 1996)), which stands above 97 %. We are using the Prague Dependency Treebank (Hajiˇc et al., 2006) (PDT) with about 1.8 million hand annotated tokens of Czech for training and testing. The tagging experiments in this paper all use the Czech morphological (pre)processor, which includes a guesser for “unknown” tokens and which is available from the PDT website (PDT Guide, 2006) to disambiguate only among those tags which are morphologically plausible. The meaning of the Czech tags (each tag has 15 positions) we are using is explained in Table 1. The detailed linguistic description of the individual p</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In: Proceedings of the 1st EMNLP, May 1996, pp. 133–142. Philadelphia</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
<author>Atro Voluntainen</author>
</authors>
<title>Comparing a linguistic and a stochastic tagger. In:</title>
<date>1997</date>
<booktitle>Proceedings of ACL/EACL Joint Converence,</booktitle>
<pages>246--252</pages>
<location>Madrid</location>
<marker>Samuelsson, Voluntainen, 1997</marker>
<rawString>Christer Samuelsson and Atro Voluntainen. 1997. Comparing a linguistic and a stochastic tagger. In: Proceedings of ACL/EACL Joint Converence, pp. 246– 252. Madrid</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-Based Morphological Disambiguation with Random Fields. In:</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP,</booktitle>
<pages>475--482</pages>
<location>Vancouver</location>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith and David A. Smith and Roy W. Tromble. 2005. Context-Based Morphological Disambiguation with Random Fields. In: Proceedings of HLT/EMNLP, pp. 475–482. Vancouver</rawString>
</citation>
<citation valid="false">
<title>Drahomfra “johanka” Spoustov´a. in prep. Kombinovan´e statisticko-pravidlov´e metody znaˇckov´an!ˇceˇstiny. (Combining Statistical and Rule-Based Approaches to Morphological Tagging of Czech Texts). PhD Thesis, MFF UK,</title>
<note>in prep.</note>
<marker></marker>
<rawString>Drahomfra “johanka” Spoustov´a. in prep. Kombinovan´e statisticko-pravidlov´e metody znaˇckov´an!ˇceˇstiny. (Combining Statistical and Rule-Based Approaches to Morphological Tagging of Czech Texts). PhD Thesis, MFF UK, in prep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Atro Voutilainen</author>
</authors>
<title>Tagging accurately: don’t guess if you know. In:</title>
<date>1994</date>
<booktitle>Proceedings of the 4th conference on Applied Natural Language Processing,</booktitle>
<pages>47--52</pages>
<location>Stuttgart</location>
<marker>Tapanainen, Voutilainen, 1994</marker>
<rawString>Pasi Tapanainen and Atro Voutilainen. 1994. Tagging accurately: don’t guess if you know. In: Proceedings of the 4th conference on Applied Natural Language Processing, pp. 47–52. Stuttgart</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbora Vidov´a-Hladk´a</author>
</authors>
<title>Czech Language Tagging.</title>
<date>2000</date>
<tech>PhD thesis,</tech>
<institution>UFAL MFF UK. Prague</institution>
<marker>Vidov´a-Hladk´a, 2000</marker>
<rawString>Barbora Vidov´a-Hladk´a. 2000. Czech Language Tagging. PhD thesis, ´UFAL MFF UK. Prague</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Votrubec</author>
</authors>
<title>Volba vhodn´ych rys˚u pro morfologick´e znaˇckov´an!ˇceˇstiny. (Feature Selection for Morphological Tagging of Czech.) Master thesis, MFF,</title>
<date>2005</date>
<institution>Charles University,</institution>
<location>Prague.</location>
<contexts>
<context position="6072" citStr="Votrubec, 2005" startWordPosition="988" endWordPosition="989">res in a given context. The result is passed to the Viterbi algorithm as a transition weight for a given tag. Mathematically, we can rewrite it as: n w(C,T) = αi.Oi(C,T) (3) i=1 where w(C, T) is the transition weight for tag T in context C, n is number of features, αi is the weight coefficient of ith feature and O(C, T)i is evaluation of ith feature for context C and tag T. Weight coefficients (α) are estimated on training data, cf. (Votrubec, 2006). The training algorithm is very simple, therefore it can be quickly retrained and it gives a possibility to test many different sets of features (Votrubec, 2005). As a result, Morˇce gives the best accuracy from the standalone taggers. 2The name Morˇce stands for “MORfologie ˇCEˇstiny” (“Czech morphology”). 68 2.3 The Feature-Based Tagger The Feature-based tagger, taken also from the PDT (Hajiˇc et al., 2006) distribution used in our experiments uses a general log-linear model in its basic formulation: exp(�� i�1 Aifi(y,x)) pAC(y |x) = (4) Z(x) where fi(y, x) is a binary-valued feature of the event value being predicted and its context, Ai is a weight of the feature fi, and the Z(x) is the natural normalization factor. The weights Ai are approximated </context>
</contexts>
<marker>Votrubec, 2005</marker>
<rawString>Jan Votrubec. 2005. Volba vhodn´ych rys˚u pro morfologick´e znaˇckov´an!ˇceˇstiny. (Feature Selection for Morphological Tagging of Czech.) Master thesis, MFF, Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Votrubec</author>
</authors>
<title>Morphological Tagging Based on Averaged Perceptron. In:</title>
<date>2006</date>
<booktitle>WDS’06 Proceedings of Contributed Papers, MFF UK,</booktitle>
<pages>191--195</pages>
<location>Prague</location>
<contexts>
<context position="1727" citStr="Votrubec, 2006" startWordPosition="251" endWordPosition="252">d free word order (causing fixedcontext systems, such as n-gram HMMs, to be even less adequate than for English). The average tagset contains about 1,000 – 2,000 distinct tags; the size of the set of possible and plausible tags can reach several thousands. There have been attempts at solving this problem for some of the highly inflective European languages, such as 67 (Daelemans, 1996), (Erjavec, 1999) for Slovenian and (Hajiˇc, 2000) for five Central and Eastern European languages. Several taggers already exist for Czech, e.g. (Hajiˇc et al., 2001b), (Smith, 2005), (Hajiˇc et al., 2006) and (Votrubec, 2006). The last one reaches the best accuracy for Czech so far (95.12 %). Hence no system has reached – in the absolute terms – a performance comparable to English tagging (such as (Ratnaparkhi, 1996)), which stands above 97 %. We are using the Prague Dependency Treebank (Hajiˇc et al., 2006) (PDT) with about 1.8 million hand annotated tokens of Czech for training and testing. The tagging experiments in this paper all use the Czech morphological (pre)processor, which includes a guesser for “unknown” tokens and which is available from the PDT website (PDT Guide, 2006) to disambiguate only among thos</context>
<context position="5910" citStr="Votrubec, 2006" startWordPosition="962" endWordPosition="963">Perceptron stores its weight coefficient, which is typically an integer number. The whole task of Averaged Perceptron is to sum all the coefficients of true features in a given context. The result is passed to the Viterbi algorithm as a transition weight for a given tag. Mathematically, we can rewrite it as: n w(C,T) = αi.Oi(C,T) (3) i=1 where w(C, T) is the transition weight for tag T in context C, n is number of features, αi is the weight coefficient of ith feature and O(C, T)i is evaluation of ith feature for context C and tag T. Weight coefficients (α) are estimated on training data, cf. (Votrubec, 2006). The training algorithm is very simple, therefore it can be quickly retrained and it gives a possibility to test many different sets of features (Votrubec, 2005). As a result, Morˇce gives the best accuracy from the standalone taggers. 2The name Morˇce stands for “MORfologie ˇCEˇstiny” (“Czech morphology”). 68 2.3 The Feature-Based Tagger The Feature-based tagger, taken also from the PDT (Hajiˇc et al., 2006) distribution used in our experiments uses a general log-linear model in its basic formulation: exp(�� i�1 Aifi(y,x)) pAC(y |x) = (4) Z(x) where fi(y, x) is a binary-valued feature of the</context>
</contexts>
<marker>Votrubec, 2006</marker>
<rawString>Jan Votrubec. 2006. Morphological Tagging Based on Averaged Perceptron. In: WDS’06 Proceedings of Contributed Papers, MFF UK, pp. 191–195. Prague</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel Oliva</author>
</authors>
<title>Milena Hn´atkov´a, Vladimfr Petkeviˇc and Pavel Kvˇetoˇn.</title>
<date>2000</date>
<booktitle>Proceedings of the Conference ”Text, Speech and Dialogue 2000”, Lecture Notes in Artificial Intelligence,</booktitle>
<volume>Vol.</volume>
<pages>3--8</pages>
<editor>Sojka P., Kopeˇcek I., Pala K. (eds.):</editor>
<publisher>Springer-Verlag,</publisher>
<marker>Oliva, 2000</marker>
<rawString>Karel Oliva, Milena Hn´atkov´a, Vladimfr Petkeviˇc and Pavel Kvˇetoˇn. 2000. The Linguistic Basis of a RuleBased Tagger of Czech. In: Sojka P., Kopeˇcek I., Pala K. (eds.): Proceedings of the Conference ”Text, Speech and Dialogue 2000”, Lecture Notes in Artificial Intelligence, Vol. 1902. Springer-Verlag, pp. 3–8. Berlin-Heidelberg</rawString>
</citation>
<citation valid="false">
<note>PDTGuide. http://ufal.ms.mff.cuni.cz/pdt2.0</note>
<marker></marker>
<rawString>PDTGuide. http://ufal.ms.mff.cuni.cz/pdt2.0</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>