<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.635223">
<title confidence="0.99439">
Structured Belief Propagation for NLP
</title>
<author confidence="0.998236">
Matthew R. Gormley Jason Eisner
</author>
<affiliation confidence="0.96205">
Department of Computer Science
Johns Hopkins University, Baltimore, MD
</affiliation>
<email confidence="0.992607">
{mrg,jason}@cs.jhu.edu
</email>
<sectionHeader confidence="0.993431" genericHeader="abstract">
1 Tutorial Overview
</sectionHeader>
<bodyText confidence="0.999965">
Statistical natural language processing relies on
probabilistic models of linguistic structure. More
complex models can help capture our intuitions
about language, by adding linguistically meaning-
ful interactions and latent variables. However, in-
ference and learning in the models we want often
poses a serious computational challenge.
Belief propagation (BP) and its variants pro-
vide an attractive approximate solution, especially
using recent training methods. These approaches
can handle joint models of interacting compo-
nents, are computationally efficient, and have ex-
tended the state-of-the-art on a number of com-
mon NLP tasks, including dependency parsing,
modeling of morphological paradigms, CCG pars-
ing, phrase extraction, semantic role labeling, and
information extraction (Smith and Eisner, 2008;
Dreyer and Eisner, 2009; Auli and Lopez, 2011;
Burkett and Klein, 2012; Naradowsky et al., 2012;
Stoyanov and Eisner, 2012).
This tutorial delves into BP with an emphasis on
recent advances that enable state-of-the-art perfor-
mance in a variety of tasks. Our goal is to eluci-
date how these approaches can easily be applied
to new problems. We also cover the theory under-
lying them. Our target audience is researchers in
human language technologies; we do not assume
familiarity with BP.
In the first three sections, we discuss applica-
tions of BP to NLP problems, the basics of mod-
eling with factor graphs and message passing, and
the theoretical underpinnings of “what BP is do-
ing” and how it relates to other inference tech-
niques. In the second three sections, we cover
key extensions to the standard BP algorithm to en-
able modeling of linguistic structure, efficient in-
ference, and approximation-aware training. We
survey a variety of software tools and introduce
a new software framework that incorporates many
of the modern approaches covered in this tutorial.
</bodyText>
<sectionHeader confidence="0.892981" genericHeader="keywords">
2 Outline
</sectionHeader>
<listItem confidence="0.998301825">
1. Probabilistic Modeling [15 min., Eisner]
• Intro: Modeling with factor graphs
• Constituency and dependency parsing
• Joint CCG Parsing and supertagging
• Transliteration; Morphology
• Alignment; Phrase extraction
• Joint models for NLP; Semantic role label-
ing; Targeted sentiment
• Variable-centric view of the world
2. Belief Propagation Basics [40 min., Eisner]
• Messages and beliefs
• Sum-product algorithm
• Relation to the forward-backward and
Viterbi algorithms
• BP as dynamic programming
• Acyclic vs. loopy graphs
3. Theory [25 min., Gormley]
• From sum-product to max-product
• From arc consistency to BP
• From Gibbs sampling to particle BP to BP
• Convergence properties
• Bethe free energy
4. Incorporating Structure into Factors and Vari-
ables [30 min., Gormley]
• Embedding dynamic programs (e.g.
inside-outside) within factors
• String-valued variables and finite state ma-
chines
5. Message approximation and scheduling [20
min., Eisner]
• Computing fewer messages
• Pruning messages
• Expectation Propagation and Penalized EP
6. Approximation-aware Training [30 min., Gorm-
ley]
• Empirical risk minimization under approx-
imations (ERMA)
• BP as a computational expression graph
• Automatic differentiation (AD)
7. Software [10 min., Gormley]
</listItem>
<page confidence="0.890847">
5
</page>
<note confidence="0.976784">
Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 5–6,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.993439" genericHeader="introduction">
3 Instructors
</sectionHeader>
<bodyText confidence="0.999940173913043">
Matt Gormley is a PhD student at Johns Hopkins
University working with Mark Dredze and Jason
Eisner. His current research focuses on joint mod-
eling of multiple linguistic strata in learning set-
tings where supervised resources are scarce. He
has authored papers in a variety of areas including
topic modeling, global optimization, semantic role
labeling, relation extraction, and grammar induc-
tion.
Jason Eisner is a Professor in Computer Sci-
ence and Cognitive Science at Johns Hopkins Uni-
versity, where he has received two school-wide
awards for excellence in teaching. His 90+ pa-
pers have presented many models and algorithms
spanning numerous areas of NLP. His goal is to
develop the probabilistic modeling, inference, and
learning techniques needed for a unified model of
all kinds of linguistic structure. In particular, he
and his students introduced structured belief prop-
agation (which incorporates classical NLP models
and their associated dynamic programming algo-
rithms), as well as loss-calibrated training for use
with belief propagation.
</bodyText>
<sectionHeader confidence="0.998062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99895395">
Michael Auli and Adam Lopez. 2011. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proceedings of ACL.
David Burkett and Dan Klein. 2012. Fast inference in
phrase extraction models with belief propagation. In
Proceedings of NAACL.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of
EMNLP.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization
of hidden syntactic structure. In Proceedings of
EMNLP 2012.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate CRF-Based NLP sys-
tems. In Proceedings of NAACL-HLT.
</reference>
<page confidence="0.998785">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000964">
<title confidence="0.999751">Structured Belief Propagation for NLP</title>
<author confidence="0.999998">Matthew R Gormley Jason Eisner</author>
<affiliation confidence="0.755853">Department of Computer Science Johns Hopkins University, Baltimore, MD</affiliation>
<abstract confidence="0.901459817073171">1 Tutorial Overview Statistical natural language processing relies on probabilistic models of linguistic structure. More complex models can help capture our intuitions about language, by adding linguistically meaningful interactions and latent variables. However, inand learning in the models we poses a serious computational challenge. propagation and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it relates to other inference techniques. In the second three sections, we cover key extensions to the standard BP algorithm to enable modeling of linguistic structure, efficient inference, and approximation-aware training. We survey a variety of software tools and introduce a new software framework that incorporates many of the modern approaches covered in this tutorial. 2 Outline Probabilistic Modeling min., • Intro: Modeling with factor graphs • Constituency and dependency parsing • Joint CCG Parsing and supertagging • Transliteration; Morphology • Alignment; Phrase extraction • Joint models for NLP; Semantic role labeling; Targeted sentiment • Variable-centric view of the world Belief Propagation Basics min., • Messages and beliefs • Sum-product algorithm • Relation to the forward-backward and Viterbi algorithms • BP as dynamic programming • Acyclic vs. loopy graphs Theory min., • From sum-product to max-product • From arc consistency to BP • From Gibbs sampling to particle BP to BP • Convergence properties • Bethe free energy 4. Incorporating Structure into Factors and Varimin., • Embedding dynamic programs (e.g. inside-outside) within factors • String-valued variables and finite state machines Message approximation and scheduling • Computing fewer messages • Pruning messages • Expectation Propagation and Penalized EP Approximation-aware Training min., Gorm- • Empirical risk minimization under approximations (ERMA) • BP as a computational expression graph • Automatic differentiation (AD) Software min., 5 of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th pages</abstract>
<address confidence="0.284841">China, July 26-31, 2015. Association for Computational Linguistics</address>
<title confidence="0.424843">3 Instructors</title>
<author confidence="0.984481">Matt Gormley is a PhD student at Johns Hopkins</author>
<affiliation confidence="0.885768">University working with Mark Dredze and Jason</affiliation>
<abstract confidence="0.942613461538462">Eisner. His current research focuses on joint modeling of multiple linguistic strata in learning settings where supervised resources are scarce. He has authored papers in a variety of areas including topic modeling, global optimization, semantic role labeling, relation extraction, and grammar induction. Jason Eisner is a Professor in Computer Science and Cognitive Science at Johns Hopkins University, where he has received two school-wide awards for excellence in teaching. His 90+ papers have presented many models and algorithms spanning numerous areas of NLP. His goal is to develop the probabilistic modeling, inference, and learning techniques needed for a unified model of all kinds of linguistic structure. In particular, he and his students introduced structured belief propagation (which incorporates classical NLP models and their associated dynamic programming algorithms), as well as loss-calibrated training for use with belief propagation. References Michael Auli and Adam Lopez. 2011. A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing. of David Burkett and Dan Klein. 2012. Fast inference in phrase extraction models with belief propagation. In of Markus Dreyer and Jason Eisner. 2009. Graphical over multiple strings. In of Jason Naradowsky, Sebastian Riedel, and David Smith. 2012. Improving NLP through marginalization hidden syntactic structure. In of David A. Smith and Jason Eisner. 2008. Dependency by belief propagation. In of Veselin Stoyanov and Jason Eisner. 2012. Minimumrisk training of approximate CRF-Based NLP sys- In of</abstract>
<intro confidence="0.446257">6</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1038" citStr="Auli and Lopez, 2011" startWordPosition="139" endWordPosition="142"> However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical </context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Fast inference in phrase extraction models with belief propagation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1063" citStr="Burkett and Klein, 2012" startWordPosition="143" endWordPosition="146">d learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP</context>
</contexts>
<marker>Burkett, Klein, 2012</marker>
<rawString>David Burkett and Dan Klein. 2012. Fast inference in phrase extraction models with belief propagation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Graphical models over multiple strings.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1016" citStr="Dreyer and Eisner, 2009" startWordPosition="135" endWordPosition="138">ons and latent variables. However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing</context>
</contexts>
<marker>Dreyer, Eisner, 2009</marker>
<rawString>Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Sebastian Riedel</author>
<author>David Smith</author>
</authors>
<title>Improving NLP through marginalization of hidden syntactic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="1088" citStr="Naradowsky et al., 2012" startWordPosition="147" endWordPosition="150">we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it rel</context>
</contexts>
<marker>Naradowsky, Riedel, Smith, 2012</marker>
<rawString>Jason Naradowsky, Sebastian Riedel, and David Smith. 2012. Improving NLP through marginalization of hidden syntactic structure. In Proceedings of EMNLP 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="991" citStr="Smith and Eisner, 2008" startWordPosition="131" endWordPosition="134">lly meaningful interactions and latent variables. However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor g</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Minimumrisk training of approximate CRF-Based NLP systems.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="1116" citStr="Stoyanov and Eisner, 2012" startWordPosition="151" endWordPosition="154">ious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it relates to other inference tech</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Veselin Stoyanov and Jason Eisner. 2012. Minimumrisk training of approximate CRF-Based NLP systems. In Proceedings of NAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>