<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003142">
<title confidence="0.9966415">
ECNUCS: A Surface Information Based System Description of Sentiment
Analysis in Twitter in the SemEval-2013 (Task 2)
</title>
<author confidence="0.994587">
Tian Tian ZHU and Fang Xi ZHANG and Man LAN∗
</author>
<affiliation confidence="0.970221">
Department of Computer Science and Technology
East China Normal University
</affiliation>
<email confidence="0.996376">
51111201046,51111201041@ecnu.edu.cn; mlan@cs.ecnu.edu.cn
</email>
<sectionHeader confidence="0.993805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999809076923077">
This paper briefly reports our submissions
to the two subtasks of Semantic Analysis in
Twitter task in SemEval 2013 (Task 2), i.e.,
the Contextual Polarity Disambiguation task
(an expression-level task) and the Message
Polarity Classification task (a message-level
task). We extract features from surface infor-
mation of tweets, i.e., content features, Micro-
blogging features, emoticons, punctuation and
sentiment lexicon, and adopt SVM to build
classifier. For subtask A, our system on twit-
ter data ranks 2 on unconstrained rank and on
SMS data ranks 1 on unconstrained rank.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999823375">
Micro-blogging today has become a very popular
communication tool among Internet users. Millions
of messages are appearing daily in popular web sites
that provide services for Micro-blogging and one
popularly known is Twitter1. Through the twitter
platform, users share either information or opin-
ions about personalities, politicians, products, com-
panies, events (Prentice and Huffman, 2008) etc. As
a result of the rapidly increasing number of tweets,
mining sentiments expressed in tweets has attracted
more and more attention, which is also one of the
basic analysis utility functions needed by various ap-
plications.
The task of Sentiment Analysis in Twitter is
to identify the sentiment of tweets and get a bet-
ter understanding of how sentiment is conveyed in
</bodyText>
<footnote confidence="0.938529">
1http://www.twitter.com
</footnote>
<bodyText confidence="0.999865">
tweets and texts, which consists of two sub-tasks,
i.e., the Contextual Polarity Disambiguation task
(an expression-level task) and the Message Polarity
Classification task (a message-level task). The con-
textual polarity disambiguation task (subtask A) is
to determine whether a given message containing a
marked instance of a word or a phrase is positive,
negative or neutral in that context. The message
polarity classification task (subtask B) is to decide
whether a given message is of positive, negative, or
neutral sentiment and for messages conveying both
a positive and negative sentiment, whichever is the
stronger sentiment should be chosen (Wilson et al.,
2013). We participate in these two tasks.
In recent years, many researchers have proposed
methods to analyze sentiment in twitter. For exam-
ple, (Pak and Paroubek, 2010) used a Part of Speech
(POS) tagger on the tweets and found that some POS
taggers can help identify the sentiment of tweets.
They found that objective tweets often contain more
nouns than subjective tweets. However, subjective
tweets may carry more adjectives and adverbs than
objective tweets. Besides, (Davidov et al., 2010)
proved that emoticon and punctuation like excla-
mation mark are good features when distinguishing
the sentiment of tweets. In addition, some senti-
ment lexicons like SentiWordNet (Baccianella et al.,
2010) and MPQA Subjectivity Lexicon (Wilson et
al., 2009) have been adopted to calculate the senti-
ment score of tweets (Zirn et al., 2011).
The rest of this paper is organized as follows. Sec-
tion 2 describes our approach for subtask 1, i.e.,
the Contextual Polarity Disambiguation task. Sec-
tion 3 describes our approach for subtask 2, i.e., the
</bodyText>
<page confidence="0.966869">
408
</page>
<bodyText confidence="0.90906325">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 408–413, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
message polarity classification task. Concluding re-
marks is in Section 4.
</bodyText>
<sectionHeader confidence="0.89565" genericHeader="method">
2 System Description of Contextual
</sectionHeader>
<subsectionHeader confidence="0.634544">
Polarity Disambiguation
</subsectionHeader>
<bodyText confidence="0.99994525">
For the Contextual Polarity Disambiguation task,
we first extract features from multiple aspects, i.e.,
punctuation, emoticons, POS tags, instance length
and sentiment lexicon features. Then we adopt poly-
nomial SVM to build classification models. Accord-
ing to the definition of this task, the given instance
has been marked by a start position and an end posi-
tion rather than a whole tweet. So we first record the
frequency of the first three kinds of features in this
given instance. To avoid interference from the num-
ber of words in given instance, we then normalize
the feature values by the length of instance.
</bodyText>
<subsectionHeader confidence="0.982901">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999876217391304">
Typically, most tweets contain informal language
expressions, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, such as,
“RT” for “re-tweet” and #hashtags, which are a type
of tagging for Twitter messages. Therefore, working
with these informal text genres presents challenges
for natural language processing beyond those typ-
ically encountered when working with more tradi-
tional text genres, such as newswire data. So we
perform text preprocessing in order to remedy as
many informal texts as possible. Firstly, we per-
form normalization to convert creative spelling and
misspelling into its right spelling. For example, any
repetition of more than 3 continuous letters are re-
duced back to 1 letter (e.g. “noooo” is reduced to
“no”). In addition, according to the Internet slang
dictionary2, we convert each slang to its complete
form, for example, “aka” is rewritten as “also known
as”. After that, we use the Stanford parser3 for to-
kenization and the Stanford POS Tagger4 for POS
tagging. Finally, Natural Language Toolkit5 is used
for WordNet based Lemmatization.
</bodyText>
<footnote confidence="0.99987425">
2http://www.noslang.com
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/tagger.shtml
5http://nltk.org/
</footnote>
<subsectionHeader confidence="0.9117455">
2.2 Features
2.2.1 Punctuation
</subsectionHeader>
<bodyText confidence="0.9999906">
Typically, punctuation may express user’s senti-
ment to a certain extent. For example, many excla-
mation marks (!) in tweet may indicate strong feel-
ings or high volume (shouting). Therefore, given
a marked instance, we record the frequency of the
following four types of punctuation: (1) exclama-
tion mark (!), (2) question mark (?), (3) double or
single quotation marks( ” and “”), (4) sum of the
above three punctuation. Then the punctuation fea-
ture value is normalized by the length of instance.
</bodyText>
<subsectionHeader confidence="0.698121">
2.2.2 Emoticons
</subsectionHeader>
<bodyText confidence="0.992079">
We create two features that capture the number of
positive and negative emoticons. Table 1 lists the
two types of emoticons. We also use the union of
the two emoticon sets as a feature. In total, we have
three emoticon features.
</bodyText>
<table confidence="0.927248">
Positive Emoticons Negative Emoticons
:-) : ) :D :-D =) ;) :( :-( : ( ;(
;-) ; );D ;-D (; :) ;-( ; ( ):
:-P ;-P XD (-: (-; :o) ;o) -/ :/ ;-/ ;/
:0) ;0) &amp;quot; &amp;quot; T T T0T ToT
</table>
<tableCaption confidence="0.998445">
Table 1: List of emoticons
</tableCaption>
<subsectionHeader confidence="0.563541">
2.2.3 POS
</subsectionHeader>
<bodyText confidence="0.999983083333333">
According to the finding of (Pak and Paroubek,
2010), POS taggers help to identify the sentiment
of tweets. Therefore, we record the frequency of
the following four POS features, i.e., noun (“NN”,
“NNP”, “NNS” and “NNPS” POS tags are grouped
into noun feature), verb (“VB”, “VBD”, “VBG”,
“VBN”, “VBP” and “VBZ” POS tags are grouped
into verb feature), adjective (“JJ”, “JJR” and “JJS”
POS tags are grouped into adjective feature) and
adverb (“RB”, “RBR” and “RBS” POS tags are
grouped into adverb feature). Then we normalize
them by the length of given instance.
</bodyText>
<subsectionHeader confidence="0.7781">
2.2.4 Sentiment lexicon Features
</subsectionHeader>
<bodyText confidence="0.999916666666667">
For each word in a given instance, we use three
sentiment lexicons to identify its sentiment polarity
and calculate its sentiment weight, i.e., SentiWord-
Net (Baccianella et al., 2010), MPQA Subjectivity
Lexicon (Wilson et al., 2009) and an Unigram Lex-
icon made from the Large Movie Review Dataset
</bodyText>
<page confidence="0.995644">
409
</page>
<bodyText confidence="0.989536333333333">
v1.0 (Maas et al., 2011). To calculate the sentiment
score for this instance, we use the following formula
to sum up the sentiment score of each word:
</bodyText>
<equation confidence="0.998754333333333">
Num(w) ∗ Senti weight
(1)
Length(I)
</equation>
<bodyText confidence="0.999970802816901">
where I represents the given instance and w repre-
sents each word in I. The Senti weight is calcu-
lated based on the word in the instance and the cho-
sen sentiment lexicon. That is, for each word in the
instance, we have different Senti weight values for
it since we use different sentiment lexicons. Below
we describe the calculation of Senti weight values
for a word in three sentiment lexicons. Note that
Num(w) is always 1 since most words appear one
time in a instance.
SentiWordIet. SentiWordNet is a lexical resource
for sentiment analysis, which assigns each synset of
WordNet (Stark and Riesenfeld, 1998) three senti-
ment scores: positivity, negativity, objectivity (e.g.
living#a#3, positivity: 0.5, negativity: 0.125, ob-
jectivity: 0.375), where sum of these three scores
is always 1. For one concept, if its positive score
and negative score are all 0, we treat it as objective
concept; otherwise, we treat it as subjective concept.
And we take the first sense as the concept of each
word.
We extract three features from SentiWordNet, i.e.,
SUBWordNet, POSWordNet and NEGWordNet.
The Senti weight of SUBWordNet records
whether a word is subjective. If it is subjective,
we set Senti weight as 1, otherwise 0. Similarly,
the Senti weight values of POSWordNet and
NEGWordNet indicate the positive score and the
negative score of the given word. Considering
some negation terms may reverse the sentiment
orientation of instance, we manually generate a
negation term list (e.g. “not”, “never”, etc.,) and if a
negation term appears in the instance, we switch the
POSWordNet to NEGWordNet and vice versa. Be-
sides, we adopt another feature to record the ratio of
POSWordNet/NEGWordNet. If the denominator is
0, i.e., NEGWordNet = 0, that means, the word has
the strongest positive sentiment orientation, then we
set 10*POSWordNet as its feature value.
MPQA. The MPQA Subjectivity Lexicon contains
about 8, 000 subjective words. Each word in the
lexicon has two types of sentiment strength: strong
subjective and weak subjective, and four kinds of
sentiment polarity: positive, negative, both (positive
and negative) and neutral. Therefore we calculate
three features from this lexicon, i.e., SUBMPQA,
POSMPQA and NEGMPQA. For the SUBMPQA
feature, if the word has strong or weak subjective,
we set its Senti weight as 1 or 0.5 accordingly.
For the POSMPQA (NEGMPQA) feature, we set
Senti weight as 1, or 0.5 or 0 if the word has strong
positive (negative), or weak positive (negative) or
neutral. We also reverse the sentiment orientation
of POSMPQA and NEGMPQA if a negation term
appears.
Unigram Lexicon. Unlike the above two lexicons
in themselves which provide sentiment polarity and
sentiment strength for each word, we also utilize the
third lexicon to calculate the sentiment information
statistically. Therefore we generate an unigram lex-
icon by ourselves from a large Movie Review data
set(Maas et al., 2011) which contains 25, 000 posi-
tive and 25, 000 negative movie reviews. We calcu-
late the Senti weight of each word appears in the
data set as the ratio of the frequency of this word
in positive reviews to that in negative reviews and
record this feature as SentiUL.
Clearly, since we use additional data set to de-
velop a sentiment lexicon which is used to generate
this SentiUL feature, this feature is worked with all
other features to train the unconstrained system.
</bodyText>
<subsubsectionHeader confidence="0.678932">
2.2.5 Other features
</subsubsectionHeader>
<bodyText confidence="0.999975">
In addition, we collect three other features: (1)
length of instance, (2) uppercase word (e.g. “WTO”
or “Machine Learning”), (3) URL. For the uppercase
word and URL features, we record the frequency of
them and then normalize them by the instance length
as well.
</bodyText>
<subsectionHeader confidence="0.9924635">
2.3 Experiment and Results
2.3.1 Classification Algorithm
</subsectionHeader>
<bodyText confidence="0.9999322">
We adopt LibSVM6 to build polynomial kernel-
based SVM classifiers. We have also tried linear ker-
nel but get no improvement. To obtain the optimal
parameters for SVM, such as c and g, we perform
grid search with 10-fold cross validation on training
</bodyText>
<equation confidence="0.90893375">
6http://www.csie.ntu.edu.tw/ cjlin/libsvm/
1]
Senti(I) =
w∈I
</equation>
<page confidence="0.963955">
410
</page>
<bodyText confidence="0.900969">
data.
</bodyText>
<sectionHeader confidence="0.427009" genericHeader="method">
2.3.2 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999983511111111">
In section 2, we obtained 22 features in total. To
train the constrained model, we used the above de-
scribed 21 features (except 5entiUL) and used all
above 22 features to train the unconstrained model.
We combined the provided training and develop-
ment data by the organizers as our final training
data. And we should apologize for our misunder-
standing of the definitions of the constrained and
unconstrained condition. As the official definition
of unconstrained model, participates are allowed
to add other data to expand the training data sets,
but our unconstrained model only adds one fea-
ture (5entiUL) which is got from other data set.
Therefore, we actually submitted two results of con-
strained model. But we still refer this model trained
on all features as unconstrained model for it ap-
peared in the unconstrained list of official results.
There are two kinds of test data: 4,435 twitter in-
stances and 2,334 SMS message instances. Table
2 list the F-score and averaged F-score of positive,
negative and neutral class of each test data set.
On one hand, from the table we can see that
whether on constrained or unconstrained model, the
results on twitter data are slightly better than those
of SMS data. However, this difference is not signifi-
cant. This indicates that the model trained on twitter
data performs well on SMS data. And it also shows
that twitter data and SMS data are linguistically sim-
ilar with each other in nature. On the other hand, we
find that on each test data set, there is little differ-
ence between the constrained model and the uncon-
strained model, which indicates the 5entiUL feature
does not have discriminating power by itself. How-
ever, since we had not used other labeled or unla-
beled data to extend the training data set, we cannot
draw a conclusion on this. Besides, our results con-
tain no neutral items even though the classifier we
used is multivariate. One reason may be the neutral
instances in training data is too sparse for the classi-
fier to learn.
On twitter data, our system ranks 2 under un-
constrained model and ranks 10 under constrained
model. On SMS data, our system ranks first under
unconstrained model and ranks 7 under constrained
model.
</bodyText>
<sectionHeader confidence="0.894094" genericHeader="method">
3 System Description of Message Polarity
Classification
</sectionHeader>
<bodyText confidence="0.999940166666667">
Unlike the previous subtask, the Message Polarity
classification task focuses on the whole tweet rather
than a marked sequence of given instance. Firstly,
we perform text preprocessing as Task A. Besides
the previous described features, we also extract fol-
lowing features.
</bodyText>
<subsectionHeader confidence="0.962952">
3.1 Features
3.1.1 Micro-blogging features
</subsectionHeader>
<bodyText confidence="0.99997675">
We adopted three tweet domain-specific features,
i.e., #hashtags, @USERS, URLs. We calculate the
frequency of the three features and normalize them
by the length of instance.
</bodyText>
<subsubsectionHeader confidence="0.56948">
3.1.2 n-gram features
</subsubsectionHeader>
<bodyText confidence="0.999573">
We used unigrams to capture the content of
tweets.
</bodyText>
<subsectionHeader confidence="0.999197">
3.2 Classification Algorithm
</subsectionHeader>
<bodyText confidence="0.999863833333333">
We adopted two different classifiers in preliminary
experiments, i.e., maximum entropy and SVM. We
used the Mallet tool (McCallum, 2002) to perform
Maximum Entropy classification and LibSVM7 with
a linear kernel, where the default setting is adopted
in all experiments.
</bodyText>
<subsectionHeader confidence="0.985541">
3.3 Results on Training Data
</subsectionHeader>
<bodyText confidence="0.999470411764706">
In the first experiment, we used only content fea-
tures and LibSVM classifier to do our experiments.
The results were listed in Table 3. From Table 3,
we found that the system with unigram without re-
moving stop words performs the best. The possible
reason was that Microblogs are always short (con-
strained in 140 words) and removing stop words
would cause information missing in such a short
text. In addition, although bigrams improved the
performance to some extern, they added the feature
space many more and might affect other features. So
in our final systems, we used only unigram feature
and did not remove stop words.
In the second experiment, we compared all fea-
tures described before with two learning algorithms.
The results were shown in Table 4, where 1 indi-
cates unigram, 2 indicates micro-blog, 3 indicates
</bodyText>
<footnote confidence="0.989072">
7http://www.csie.ntu.edu.tw/ cjlin/libsvm/
</footnote>
<page confidence="0.989476">
411
</page>
<table confidence="0.9992528">
System F-pos F-neg F-neu average F(pos and neg)
twitter-constrained 0.8506 0.7390 0.0 0.7948
twitter-unconstrained 0.8561 0.7468 0.0 0.8015
SMS-constrained 0.7727 0.7611 0.0 0.7669
SMS-unconstrained 0.7645 0.7824 0.0 0.7734
</table>
<tableCaption confidence="0.982478">
Table 2: Results of our systems on subtask A test data
</tableCaption>
<table confidence="0.999204">
features F-pos F-neg F-neu average F(pos and neg) acc(%)
unigrams 0.6356 0.3381 0.7122 0.4869 63.75
unigrams(remove stop words) 0.6046 0.3453 0.6988 0.4750 62.13
bigrams 0.5186 0.0196 0.6625 0.2691 55.85
unigrams+bigrams 0.6234 0.3724 0.7043 0.4979 63.18
</table>
<tableCaption confidence="0.998008">
Table 3: Results of our systems on on subtask B training data using content features
</tableCaption>
<table confidence="0.999392857142857">
features F-pos F-neg F-neu average F(pos and neg) acc(%)
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
1 0.6178 0.6356 0.3696 0.3381 0.6848 0.7122 0.4937 0.4869 61.56 63.75
1+2 0.6403 0.6339 0.4207 0.4310 0.6990 0.7184 0.5305 0.5324 63.75 64.89
1+2+3 0.6328 0.6512 0.4051 0.4371 0.6975 0.7232 0.5190 0.5442 63.18 65.75
1+2+3+4 0.6488 0.6593 0.4587 0.4481 0.7083 0.7288 0.5538 0.5537 64.89 66.41
2+3+4 0.5290 0.5201 0.2897 0.2643 0.6503 0.6411 0.4093 0.3922 55.85 54.80
</table>
<tableCaption confidence="0.999848">
Table 4: Results of our systems on subtask B training data using all features and two learning algorithms
</tableCaption>
<bodyText confidence="0.999554666666667">
punctuation, 4 indicates sentiment lexicon features.
From Table 4, the best performance was obtained
by using all these features. Since the performance
of Maximum Entropy and SVM in terms of F-score
was comparable to each other, we finally chose SVM
since it achieved a better accuracy than MaxEnt.
</bodyText>
<sectionHeader confidence="0.518751" genericHeader="evaluation">
3.4 Results on Test Data
</sectionHeader>
<bodyText confidence="0.999980142857143">
We combined the provided training and develop-
ment data by the organizers as our final training data.
There were two kinds of test data: 3,813 tweets and
2,094 SMS messages . Table 5 listed the results of
our final systems on the tweet and SMS data sets by
using all above described features and SVM algo-
rithm.
From Table 5, on one hand, we can see that the
overall performance of SMS test data is inferior to
twitter data, for the reason may be that the domain
of features are all based on twitter data, and maybe
not quite suitable for SMS data. However, this dif-
ferent is not significant. On the other hand, we also
can find that there is no obvious distinction between
the constrained and the unconstrained model on each
test data.Also from Table 5, the F-score for positive
instances is higher than negative instances, and it
is interesting that most of other participants’systems
results show the same consequence. One of the rea-
son may be the positive instance in training data are
more than negative instances both in training data
and test data.
Our result on twitter message is 0.5842 , while
on SMS is 0.5477. Compared with the highest av-
erage F-score 0.6902 in twitter data and 0.6848 in
SMS data, our system does not perform very well.
On the one hand , pre-processing was roughly , then
features extracted were not suited in classification
stage. On the other hand, in classification stage all
parameters were default when used LibSVM. These
might cause low performance. In future, we may
overcome the insufficient described above and take
hashtags’ sentiment inclination and the source files
of URLs into consideration to enhance the perfor-
mance.
</bodyText>
<page confidence="0.994045">
412
</page>
<table confidence="0.9979202">
System F-pos F-neg F-neu average F(pos and neg)
twitter-constrained 0.6671 0.4338 0.7124 0.5505
twitter-unconstrained 0.6775 0.4908 0.7204 0.5842
SMS-constrained 0.5796 0.4846 0.7801 0.5321
SMS-unconstrained 0.5818 0.5137 0.7612 0.5477
</table>
<tableCaption confidence="0.99953">
Table 5: Results of our systems on subtask B test data
</tableCaption>
<sectionHeader confidence="0.996888" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999968833333333">
In this work we extracted features from four aspects,
including surface information of twitters and senti-
ment lexicons like SentiWordNet and MPQA Lexi-
con. On the contextual polarity disambiguation task,
our system ranks 2 on twitter (unconstrained) rank
and ranks 1 on SMS (unconstrained) rank.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998628">
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improves the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No. 20090076120029)
and Shanghai Knowledge Service Platform Project
(No. ZF1213).
</bodyText>
<sectionHeader confidence="0.999036" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99949608">
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 241–249. Association for Computational Lin-
guistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 142–150, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings ofLREC, volume 2010.
Sara Prentice and Ethan Huffman. 2008. Social medias
new role in emergency management. Idaho National
Laboratory, pages 1–5.
Michael M Stark and Richard F Riesenfeld. 1998. Word-
net: An electronic lexical database. In Proceedings of
11th Eurographics Workshop on Rendering. Citeseer.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational linguistics, 35(3):399–433.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
C¨acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of
the 5th international Joint conference on natural Lan-
guage Processing (iJcnLP-2011), volume 167.
</reference>
<page confidence="0.998977">
413
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342224">
<title confidence="0.872684">ECNUCS: A Surface Information Based System Description of Sentiment Analysis in Twitter in the SemEval-2013 (Task 2)</title>
<author confidence="0.999248">Tian ZHU Xi ZHANG</author>
<affiliation confidence="0.932186">Department of Computer Science and East China Normal</affiliation>
<email confidence="0.597003">51111201046,51111201041@ecnu.edu.cn;mlan@cs.ecnu.edu.cn</email>
<abstract confidence="0.989248285714286">This paper briefly reports our submissions to the two subtasks of Semantic Analysis in Twitter task in SemEval 2013 (Task 2), i.e., the Contextual Polarity Disambiguation task (an expression-level task) and the Message Polarity Classification task (a message-level task). We extract features from surface information of tweets, i.e., content features, Microblogging features, emoticons, punctuation and sentiment lexicon, and adopt SVM to build classifier. For subtask A, our system on twitter data ranks 2 on unconstrained rank and on SMS data ranks 1 on unconstrained rank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="3047" citStr="Baccianella et al., 2010" startWordPosition="455" endWordPosition="458">rs have proposed methods to analyze sentiment in twitter. For example, (Pak and Paroubek, 2010) used a Part of Speech (POS) tagger on the tweets and found that some POS taggers can help identify the sentiment of tweets. They found that objective tweets often contain more nouns than subjective tweets. However, subjective tweets may carry more adjectives and adverbs than objective tweets. Besides, (Davidov et al., 2010) proved that emoticon and punctuation like exclamation mark are good features when distinguishing the sentiment of tweets. In addition, some sentiment lexicons like SentiWordNet (Baccianella et al., 2010) and MPQA Subjectivity Lexicon (Wilson et al., 2009) have been adopted to calculate the sentiment score of tweets (Zirn et al., 2011). The rest of this paper is organized as follows. Section 2 describes our approach for subtask 1, i.e., the Contextual Polarity Disambiguation task. Section 3 describes our approach for subtask 2, i.e., the 408 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 408–413, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics messa</context>
<context position="7447" citStr="Baccianella et al., 2010" startWordPosition="1154" endWordPosition="1157">owing four POS features, i.e., noun (“NN”, “NNP”, “NNS” and “NNPS” POS tags are grouped into noun feature), verb (“VB”, “VBD”, “VBG”, “VBN”, “VBP” and “VBZ” POS tags are grouped into verb feature), adjective (“JJ”, “JJR” and “JJS” POS tags are grouped into adjective feature) and adverb (“RB”, “RBR” and “RBS” POS tags are grouped into adverb feature). Then we normalize them by the length of given instance. 2.2.4 Sentiment lexicon Features For each word in a given instance, we use three sentiment lexicons to identify its sentiment polarity and calculate its sentiment weight, i.e., SentiWordNet (Baccianella et al., 2010), MPQA Subjectivity Lexicon (Wilson et al., 2009) and an Unigram Lexicon made from the Large Movie Review Dataset 409 v1.0 (Maas et al., 2011). To calculate the sentiment score for this instance, we use the following formula to sum up the sentiment score of each word: Num(w) ∗ Senti weight (1) Length(I) where I represents the given instance and w represents each word in I. The Senti weight is calculated based on the word in the instance and the chosen sentiment lexicon. That is, for each word in the instance, we have different Senti weight values for it since we use different sentiment lexicon</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2843" citStr="Davidov et al., 2010" startWordPosition="425" endWordPosition="428">essages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen (Wilson et al., 2013). We participate in these two tasks. In recent years, many researchers have proposed methods to analyze sentiment in twitter. For example, (Pak and Paroubek, 2010) used a Part of Speech (POS) tagger on the tweets and found that some POS taggers can help identify the sentiment of tweets. They found that objective tweets often contain more nouns than subjective tweets. However, subjective tweets may carry more adjectives and adverbs than objective tweets. Besides, (Davidov et al., 2010) proved that emoticon and punctuation like exclamation mark are good features when distinguishing the sentiment of tweets. In addition, some sentiment lexicons like SentiWordNet (Baccianella et al., 2010) and MPQA Subjectivity Lexicon (Wilson et al., 2009) have been adopted to calculate the sentiment score of tweets (Zirn et al., 2011). The rest of this paper is organized as follows. Section 2 describes our approach for subtask 1, i.e., the Contextual Polarity Disambiguation task. Section 3 describes our approach for subtask 2, i.e., the 408 Second Joint Conference on Lexical and Computational</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241–249. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<date>2011</date>
<contexts>
<context position="7589" citStr="Maas et al., 2011" startWordPosition="1179" endWordPosition="1182">d “VBZ” POS tags are grouped into verb feature), adjective (“JJ”, “JJR” and “JJS” POS tags are grouped into adjective feature) and adverb (“RB”, “RBR” and “RBS” POS tags are grouped into adverb feature). Then we normalize them by the length of given instance. 2.2.4 Sentiment lexicon Features For each word in a given instance, we use three sentiment lexicons to identify its sentiment polarity and calculate its sentiment weight, i.e., SentiWordNet (Baccianella et al., 2010), MPQA Subjectivity Lexicon (Wilson et al., 2009) and an Unigram Lexicon made from the Large Movie Review Dataset 409 v1.0 (Maas et al., 2011). To calculate the sentiment score for this instance, we use the following formula to sum up the sentiment score of each word: Num(w) ∗ Senti weight (1) Length(I) where I represents the given instance and w represents each word in I. The Senti weight is calculated based on the word in the instance and the chosen sentiment lexicon. That is, for each word in the instance, we have different Senti weight values for it since we use different sentiment lexicons. Below we describe the calculation of Senti weight values for a word in three sentiment lexicons. Note that Num(w) is always 1 since most wo</context>
<context position="10669" citStr="Maas et al., 2011" startWordPosition="1678" endWordPosition="1681">ts Senti weight as 1 or 0.5 accordingly. For the POSMPQA (NEGMPQA) feature, we set Senti weight as 1, or 0.5 or 0 if the word has strong positive (negative), or weak positive (negative) or neutral. We also reverse the sentiment orientation of POSMPQA and NEGMPQA if a negation term appears. Unigram Lexicon. Unlike the above two lexicons in themselves which provide sentiment polarity and sentiment strength for each word, we also utilize the third lexicon to calculate the sentiment information statistically. Therefore we generate an unigram lexicon by ourselves from a large Movie Review data set(Maas et al., 2011) which contains 25, 000 positive and 25, 000 negative movie reviews. We calculate the Senti weight of each word appears in the data set as the ratio of the frequency of this word in positive reviews to that in negative reviews and record this feature as SentiUL. Clearly, since we use additional data set to develop a sentiment lexicon which is used to generate this SentiUL feature, this feature is worked with all other features to train the unconstrained system. 2.2.5 Other features In addition, we collect three other features: (1) length of instance, (2) uppercase word (e.g. “WTO” or “Machine </context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.</rawString>
</citation>
<citation valid="true">
<title>Learning word vectors for sentiment analysis.</title>
<date></date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker></marker>
<rawString>Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="14802" citStr="McCallum, 2002" startWordPosition="2362" endWordPosition="2363">ked sequence of given instance. Firstly, we perform text preprocessing as Task A. Besides the previous described features, we also extract following features. 3.1 Features 3.1.1 Micro-blogging features We adopted three tweet domain-specific features, i.e., #hashtags, @USERS, URLs. We calculate the frequency of the three features and normalize them by the length of instance. 3.1.2 n-gram features We used unigrams to capture the content of tweets. 3.2 Classification Algorithm We adopted two different classifiers in preliminary experiments, i.e., maximum entropy and SVM. We used the Mallet tool (McCallum, 2002) to perform Maximum Entropy classification and LibSVM7 with a linear kernel, where the default setting is adopted in all experiments. 3.3 Results on Training Data In the first experiment, we used only content features and LibSVM classifier to do our experiments. The results were listed in Table 3. From Table 3, we found that the system with unigram without removing stop words performs the best. The possible reason was that Microblogs are always short (constrained in 140 words) and removing stop words would cause information missing in such a short text. In addition, although bigrams improved t</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<volume>volume</volume>
<contexts>
<context position="2517" citStr="Pak and Paroubek, 2010" startWordPosition="373" endWordPosition="376">polarity disambiguation task (subtask A) is to determine whether a given message containing a marked instance of a word or a phrase is positive, negative or neutral in that context. The message polarity classification task (subtask B) is to decide whether a given message is of positive, negative, or neutral sentiment and for messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen (Wilson et al., 2013). We participate in these two tasks. In recent years, many researchers have proposed methods to analyze sentiment in twitter. For example, (Pak and Paroubek, 2010) used a Part of Speech (POS) tagger on the tweets and found that some POS taggers can help identify the sentiment of tweets. They found that objective tweets often contain more nouns than subjective tweets. However, subjective tweets may carry more adjectives and adverbs than objective tweets. Besides, (Davidov et al., 2010) proved that emoticon and punctuation like exclamation mark are good features when distinguishing the sentiment of tweets. In addition, some sentiment lexicons like SentiWordNet (Baccianella et al., 2010) and MPQA Subjectivity Lexicon (Wilson et al., 2009) have been adopted</context>
<context position="6720" citStr="Pak and Paroubek, 2010" startWordPosition="1037" endWordPosition="1040">“”), (4) sum of the above three punctuation. Then the punctuation feature value is normalized by the length of instance. 2.2.2 Emoticons We create two features that capture the number of positive and negative emoticons. Table 1 lists the two types of emoticons. We also use the union of the two emoticon sets as a feature. In total, we have three emoticon features. Positive Emoticons Negative Emoticons :-) : ) :D :-D =) ;) :( :-( : ( ;( ;-) ; );D ;-D (; :) ;-( ; ( ): :-P ;-P XD (-: (-; :o) ;o) -/ :/ ;-/ ;/ :0) ;0) &amp;quot; &amp;quot; T T T0T ToT Table 1: List of emoticons 2.2.3 POS According to the finding of (Pak and Paroubek, 2010), POS taggers help to identify the sentiment of tweets. Therefore, we record the frequency of the following four POS features, i.e., noun (“NN”, “NNP”, “NNS” and “NNPS” POS tags are grouped into noun feature), verb (“VB”, “VBD”, “VBG”, “VBN”, “VBP” and “VBZ” POS tags are grouped into verb feature), adjective (“JJ”, “JJR” and “JJS” POS tags are grouped into adjective feature) and adverb (“RB”, “RBR” and “RBS” POS tags are grouped into adverb feature). Then we normalize them by the length of given instance. 2.2.4 Sentiment lexicon Features For each word in a given instance, we use three sentimen</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings ofLREC, volume 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Prentice</author>
<author>Ethan Huffman</author>
</authors>
<title>Social medias new role in emergency management. Idaho National Laboratory,</title>
<date>2008</date>
<pages>1--5</pages>
<contexts>
<context position="1285" citStr="Prentice and Huffman, 2008" startWordPosition="181" endWordPosition="184">roblogging features, emoticons, punctuation and sentiment lexicon, and adopt SVM to build classifier. For subtask A, our system on twitter data ranks 2 on unconstrained rank and on SMS data ranks 1 on unconstrained rank. 1 Introduction Micro-blogging today has become a very popular communication tool among Internet users. Millions of messages are appearing daily in popular web sites that provide services for Micro-blogging and one popularly known is Twitter1. Through the twitter platform, users share either information or opinions about personalities, politicians, products, companies, events (Prentice and Huffman, 2008) etc. As a result of the rapidly increasing number of tweets, mining sentiments expressed in tweets has attracted more and more attention, which is also one of the basic analysis utility functions needed by various applications. The task of Sentiment Analysis in Twitter is to identify the sentiment of tweets and get a better understanding of how sentiment is conveyed in 1http://www.twitter.com tweets and texts, which consists of two sub-tasks, i.e., the Contextual Polarity Disambiguation task (an expression-level task) and the Message Polarity Classification task (a message-level task). The co</context>
</contexts>
<marker>Prentice, Huffman, 2008</marker>
<rawString>Sara Prentice and Ethan Huffman. 2008. Social medias new role in emergency management. Idaho National Laboratory, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael M Stark</author>
<author>Richard F Riesenfeld</author>
</authors>
<title>Wordnet: An electronic lexical database.</title>
<date>1998</date>
<booktitle>In Proceedings of 11th Eurographics Workshop on Rendering. Citeseer.</booktitle>
<contexts>
<context position="8362" citStr="Stark and Riesenfeld, 1998" startWordPosition="1312" endWordPosition="1315">ht (1) Length(I) where I represents the given instance and w represents each word in I. The Senti weight is calculated based on the word in the instance and the chosen sentiment lexicon. That is, for each word in the instance, we have different Senti weight values for it since we use different sentiment lexicons. Below we describe the calculation of Senti weight values for a word in three sentiment lexicons. Note that Num(w) is always 1 since most words appear one time in a instance. SentiWordIet. SentiWordNet is a lexical resource for sentiment analysis, which assigns each synset of WordNet (Stark and Riesenfeld, 1998) three sentiment scores: positivity, negativity, objectivity (e.g. living#a#3, positivity: 0.5, negativity: 0.125, objectivity: 0.375), where sum of these three scores is always 1. For one concept, if its positive score and negative score are all 0, we treat it as objective concept; otherwise, we treat it as subjective concept. And we take the first sense as the concept of each word. We extract three features from SentiWordNet, i.e., SUBWordNet, POSWordNet and NEGWordNet. The Senti weight of SUBWordNet records whether a word is subjective. If it is subjective, we set Senti weight as 1, otherwi</context>
</contexts>
<marker>Stark, Riesenfeld, 1998</marker>
<rawString>Michael M Stark and Richard F Riesenfeld. 1998. Wordnet: An electronic lexical database. In Proceedings of 11th Eurographics Workshop on Rendering. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<booktitle>Computational linguistics,</booktitle>
<pages>35--3</pages>
<contexts>
<context position="3099" citStr="Wilson et al., 2009" startWordPosition="463" endWordPosition="466"> For example, (Pak and Paroubek, 2010) used a Part of Speech (POS) tagger on the tweets and found that some POS taggers can help identify the sentiment of tweets. They found that objective tweets often contain more nouns than subjective tweets. However, subjective tweets may carry more adjectives and adverbs than objective tweets. Besides, (Davidov et al., 2010) proved that emoticon and punctuation like exclamation mark are good features when distinguishing the sentiment of tweets. In addition, some sentiment lexicons like SentiWordNet (Baccianella et al., 2010) and MPQA Subjectivity Lexicon (Wilson et al., 2009) have been adopted to calculate the sentiment score of tweets (Zirn et al., 2011). The rest of this paper is organized as follows. Section 2 describes our approach for subtask 1, i.e., the Contextual Polarity Disambiguation task. Section 3 describes our approach for subtask 2, i.e., the 408 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 408–413, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics message polarity classification task. Concluding remarks </context>
<context position="7496" citStr="Wilson et al., 2009" startWordPosition="1161" endWordPosition="1164">” and “NNPS” POS tags are grouped into noun feature), verb (“VB”, “VBD”, “VBG”, “VBN”, “VBP” and “VBZ” POS tags are grouped into verb feature), adjective (“JJ”, “JJR” and “JJS” POS tags are grouped into adjective feature) and adverb (“RB”, “RBR” and “RBS” POS tags are grouped into adverb feature). Then we normalize them by the length of given instance. 2.2.4 Sentiment lexicon Features For each word in a given instance, we use three sentiment lexicons to identify its sentiment polarity and calculate its sentiment weight, i.e., SentiWordNet (Baccianella et al., 2010), MPQA Subjectivity Lexicon (Wilson et al., 2009) and an Unigram Lexicon made from the Large Movie Review Dataset 409 v1.0 (Maas et al., 2011). To calculate the sentiment score for this instance, we use the following formula to sum up the sentiment score of each word: Num(w) ∗ Senti weight (1) Length(I) where I represents the given instance and w represents each word in I. The Senti weight is calculated based on the word in the instance and the chosen sentiment lexicon. That is, for each word in the instance, we have different Senti weight values for it since we use different sentiment lexicons. Below we describe the calculation of Senti wei</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2354" citStr="Wilson et al., 2013" startWordPosition="347" endWordPosition="350">.e., the Contextual Polarity Disambiguation task (an expression-level task) and the Message Polarity Classification task (a message-level task). The contextual polarity disambiguation task (subtask A) is to determine whether a given message containing a marked instance of a word or a phrase is positive, negative or neutral in that context. The message polarity classification task (subtask B) is to decide whether a given message is of positive, negative, or neutral sentiment and for messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen (Wilson et al., 2013). We participate in these two tasks. In recent years, many researchers have proposed methods to analyze sentiment in twitter. For example, (Pak and Paroubek, 2010) used a Part of Speech (POS) tagger on the tweets and found that some POS taggers can help identify the sentiment of tweets. They found that objective tweets often contain more nouns than subjective tweets. However, subjective tweets may carry more adjectives and adverbs than objective tweets. Besides, (Davidov et al., 2010) proved that emoticon and punctuation like exclamation mark are good features when distinguishing the sentiment</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C¨acilia Zirn</author>
<author>Mathias Niepert</author>
<author>Heiner Stuckenschmidt</author>
<author>Michael Strube</author>
</authors>
<title>Fine-grained sentiment analysis with structural features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th international Joint conference on natural Language Processing (iJcnLP-2011),</booktitle>
<volume>167</volume>
<contexts>
<context position="3180" citStr="Zirn et al., 2011" startWordPosition="478" endWordPosition="481">eets and found that some POS taggers can help identify the sentiment of tweets. They found that objective tweets often contain more nouns than subjective tweets. However, subjective tweets may carry more adjectives and adverbs than objective tweets. Besides, (Davidov et al., 2010) proved that emoticon and punctuation like exclamation mark are good features when distinguishing the sentiment of tweets. In addition, some sentiment lexicons like SentiWordNet (Baccianella et al., 2010) and MPQA Subjectivity Lexicon (Wilson et al., 2009) have been adopted to calculate the sentiment score of tweets (Zirn et al., 2011). The rest of this paper is organized as follows. Section 2 describes our approach for subtask 1, i.e., the Contextual Polarity Disambiguation task. Section 3 describes our approach for subtask 2, i.e., the 408 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 408–413, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics message polarity classification task. Concluding remarks is in Section 4. 2 System Description of Contextual Polarity Disambiguation For t</context>
</contexts>
<marker>Zirn, Niepert, Stuckenschmidt, Strube, 2011</marker>
<rawString>C¨acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis with structural features. In Proceedings of the 5th international Joint conference on natural Language Processing (iJcnLP-2011), volume 167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>