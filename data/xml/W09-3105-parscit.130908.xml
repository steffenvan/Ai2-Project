<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002685">
<title confidence="0.9981815">
Active Learning of Extractive Reference Summaries
for Lecture Speech Summarization
</title>
<author confidence="0.98902">
Justin Jian Zhang and Pascale Fung
</author>
<affiliation confidence="0.996983666666667">
Human Language Technology Center
Department of Electronic and Computer Engineering
University of Science and Technology (HKUST)
</affiliation>
<address confidence="0.821805">
Clear Water Bay,Hong Kong
</address>
<email confidence="0.998764">
{zjustin,pascale}@ece.ust.hk
</email>
<sectionHeader confidence="0.994802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993005791666667">
We propose using active learning for tag-
ging extractive reference summary of lec-
ture speech. The training process of
feature-based summarization model usu-
ally requires a large amount of train-
ing data with high-quality reference sum-
maries. Human production of such sum-
maries is tedious, and since inter-labeler
agreement is low, very unreliable. Ac-
tive learning helps assuage this problem by
automatically selecting a small amount of
unlabeled documents for humans to hand
correct. Our method chooses the unla-
beled documents according to the similar-
ity score between the document and the
comparable resource—PowerPoint slides.
After manual correction, the selected doc-
uments are returned to the training pool.
Summarization results show an increasing
learning curve of ROUGE-L F-measure,
from 0.44 to 0.514, consistently higher
than that of using randomly chosen train-
ing samples.
Index Terms: active learning, summarization
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973188679246">
The need for the summarization of classroom lec-
tures, conference speeches, political speeches is
ever increasing with the advent of remote learning,
distributed collaboration and electronic archiving.
These user needs cannot be sufficiently met by
short abstracts. In recent years, virtually all sum-
marization systems are extractive - compiling bul-
let points from the document using some saliency
criteria. Reference summaries are often manu-
ally compiled by one or multiple human annota-
tors (Fujii et al., 2008; Nenkova et al., 2007). Un-
like for speech recognition where the reference
sentence is clear and unambiguous, and unlike
for machine translation where there are guidelines
for manual translating reference sentences, there
is no clear guideline for compiling a good ref-
erence summary. As a result, one of the most
important challenges in speech summarization re-
mains the difficulty to compile, evaluate and thus
to learn what a good summary is. Human judges
tend to agree on obviously good and very bad
summaries but cannot agree on borderline cases.
Consequently, annotator agreement is low. Refer-
ence summary generation is a tedious and low ef-
ficiency task. On the other hand, supervised learn-
ing of extractive summarization requires a large
amount of training data of reference summaries.
To reduce the amount of human annotation effort
and improve annotator agreement on the reference
summaries, we propose that active learning (selec-
tive sampling) is one possible solution.
Active learning has been applied to NLP tasks
such as spoken language understanding (Tur et al.,
2005), information extraction (Shen et al., 2004),
and text classification (Lewis and Catlett, 1994;
McCallum and Nigam, 1998; Tong and Koller,
2002). Different from supervised learning which
needs the entire corpus with manual labeling re-
sult, active learning selects the most useful exam-
ples for labeling and requires manual labeling of
training dataset to re-train model.
In this paper, we suggest a framework of refer-
ence summary annotation with relatively high in-
ter labeler agreement based on the rhetorical struc-
ture in presentation slides. Based on this frame-
work, we further propose a certainty-based active
learning method to alleviate the burden of human
annotation of training data.
The rest of this paper is organized as follows:
Section 2 depicts the corpus for our experiments,
the extractive summarizer, and outlines the acous-
tic/prosodic, and linguistic feature sets for repre-
senting each sentence. Section 3 depicts how to
</bodyText>
<page confidence="0.986997">
23
</page>
<note confidence="0.963474">
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 23–26,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999973333333333">
compile reference summaries with high inter la-
beler agreement by using the RDTW algorithm
and our active learning algorithm for tagging ex-
tractive reference summary. We describe our ex-
periments and evaluate the results in Section 4.
Our conclusion follows in Section 5.
</bodyText>
<sectionHeader confidence="0.980833" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.967946">
2.1 The Corpus
</subsectionHeader>
<bodyText confidence="0.999985578947368">
Our lecture speech corpus (Zhang et al., 2008)
contains 111 presentations recorded from the
NCMMSC2005 and NCMMSC2007 conferences
for evaluating our approach. The man-
ual transcriptions and the comparable corpus—
PowerPoint slides are also collected. Each presen-
tation lasts for 15 minutes on average. We select
71 of the 111 presentations with well organized
PowerPoint slides that always have clear sketches
and evidently aligned with the transcriptions. We
use about 90% of the lecture corpus from the 65
presentations as original unlabeled data U and the
remaining 6 presentations as held-out test set. We
randomly select 5 presentations from U as our
seed presentations. Reference summaries of the
seed presentations and the presentations of test set
are generated from the PowerPoint slides and pre-
sentation transcriptions using RDTW followed by
manual correction, as described in Section 3.
</bodyText>
<subsectionHeader confidence="0.999776">
2.2 SVM Classifier and the Feature Set
</subsectionHeader>
<bodyText confidence="0.99997896">
While (Ribeiro and de Matos, 2007) has shown
that MMR (maximum marginal relevance) ap-
proach is superior to feature-based classifica-
tion for summarizing Portuguese broadcast news
data, another work on Japanese lecture speech
drew the opposite conclusion (Fujii et al., 2008)
that feature-based classification method is bet-
ter. Therefore we continue to use the feature-
based method in our work. We consider the ex-
tractive summarization as a binary classification
problem, we predict whether each sentence of the
lecture transcription should be in a summary or
not. We use Radial Basis Function (RBF) ker-
nel for constructing SVM classifier, which is pro-
vided by LIBSVM, a library for support vector
machines (Chang and Lin, 2001). We represent
each sentence by a feature vector which consists of
acoustic features: duration of the sentence, aver-
age syllable Duration, F0 information features, en-
ergy information features; and linguistic features:
length of the sentence counted by word and TFIDF
information features, as shown in (Zhang et al.,
2008). We then build the SVM classifier as our
summarizer based on these sentence feature vec-
tors.
</bodyText>
<sectionHeader confidence="0.9954035" genericHeader="method">
3 Active Learning for Tagging Reference
Summary and Summarization
</sectionHeader>
<bodyText confidence="0.9991928">
Similar to (Hayama et al., 2005; Kan, 2007), we
have previously proposed how presentation slides
are used to compile reference summaries automat-
ically (Zhang et al., 2008). The motivations be-
hind this procedure are:
</bodyText>
<listItem confidence="0.964490444444445">
• presentation slides are compiled by the au-
thors themselves and therefore provide a
good standard summary of their work;
• presentation slides contain the hierarchical
rhetorical structure of lecture speech as the ti-
tles, subtitles, page breaks, bullet points pro-
vide an enriched set of discourse information
that are otherwise not apparent in the spoken
lecture transcriptions.
</listItem>
<bodyText confidence="0.993630307692308">
We propose a Relaxed Dynamic Time Warping
(RDTW) procedure, which is identical to Dy-
namic Programming and Edit Distance, to align
sentences from the slides to those in the lecture
speech transcriptions, resulting in automatically
extracted reference summaries.
We calculate the similarity scores
matrix Sim = (sij), where sij =
similarity(Senttrans[i], Sentslides[j]), be-
tween the sentences in the transcription and
the sentences in the slides. We then obtain
the distance matrix Dist = (dij), where
dij = 1−sij. We calculate the initial warp path P:
</bodyText>
<equation confidence="0.95171125">
P = (pini 1 , ...,pini n , ...,pini
N )
is represented by sentence pair(iini
n , jini
</equation>
<bodyText confidence="0.983040222222222">
n ): one
from transcription, the other from slides. Con-
sidering that the lecturer often doesn’t follow the
flow of his/her slides strictly, we adopt Relaxed
Dynamic Time Warping (RDTW) for finding the
optimal warp path, by the following equation.
We consider the transcription sentences on this
path as reference summary sentences. We then
obtain the optimal path (popt
</bodyText>
<equation confidence="0.988734333333333">
1 , ..., popt
n , ..., popt
N ),
</equation>
<bodyText confidence="0.90641425">
where popt
n is represented by (iopt
n , jopt
n ) and C
</bodyText>
<figure confidence="0.562200833333333">
iopt
n
= iini
n
jini
n +C
jopt
n = argmin
j=jini
n −C
(1)
diopt
n ,j
⎧
⎨⎪
⎪⎩
by DTW, where pini
n
</figure>
<page confidence="0.996282">
24
</page>
<bodyText confidence="0.997944145454545">
is the capacity to relax the path. We then select
the sentences iopt
n of the transcription whose sim-
ilarity scores of sentence pairs: (iopt
n , jopt
n ), are
higher than the pre-defined threshold as the refer-
ence summary sentences. The advantage of using
these summaries as references is that it circum-
vents the disagreement between multiple human
annotators.
We have compared these reference summaries
to human-labeled summaries. When asked to ”se-
lect the most salient sentences for a summary”, we
found that inter-annotator agreement ranges from
30% to 50% only. Sometimes even a single per-
son might choose different sentences at different
times (Nenkova et al., 2007). However, when in-
structed to follow the structure and points in the
presentation slides, inter-annotator agreement in-
creased to 80%. The agreement between auto-
matically extracted reference summary and hu-
mans also reaches 75%. Based on this high degree
of agreement, we generate reference summaries
by asking a human to manually correct those ex-
tracted by the RDTW algorithm. Our reference
summaries therefore make for more reliable train-
ing and test data.
For a transcribed presentation D with a se-
quence of recognized sentences {s1, s2, ..., sN},
we want to find the sentences to be classified
as summary sentences by using the salient sen-
tence classification function c(). In a probabilis-
tic framework, the extractive summarization task
is equivalent to estimating P(c(−→s n) = 1|D) of
each sentence sn, where →−s n is the feature vec-
tor with acoustic and linguistic features of the sen-
tence sn.
We propose an active learning approach where a
small set of transcriptions as seeds with reference
summaries, created by the RDTW algorithm and
human correction, are used to train the seed model
for the summarization classifier, and then the clas-
sifier is used to label data from a unlabel pool. At
each iteration, human annotators choose the unla-
beled documents whose similarity scores between
the extracted summary sentences and the Power-
Point slides sentences are top-N highest for label-
ing summary sentences. Formally, this approach
is described in Algorithm 1.
Given document D: {s1, s2, ..., sN}, we cal-
culate the similarity score between the extracted
summary sentences: {s,1,s,2, ..., s, K} and the Pow-
erPoint slide sentences: {ppts1, ppts2, ..., pptsL},
by equation 2.
</bodyText>
<sectionHeader confidence="0.859512" genericHeader="evaluation">
4 Experimental Results and Evaluation
</sectionHeader>
<bodyText confidence="0.9834765">
Algorithm 1 Active learning for tagging extrac-
tive reference summary and summarization
</bodyText>
<subsectionHeader confidence="0.374483">
Initialization
</subsectionHeader>
<bodyText confidence="0.890201">
For an unlabeled data set: Uall, i = 0
</bodyText>
<listItem confidence="0.97429156">
(1) Randomly choose a small set of data X{i}
from Uall; U{i} = Uall − X{i}
(2) Manually label each sentence in X{i} as
summary or non-summary by RDTW and hu-
man correction and save these sentences and
their labels in L{i}
Active Learning Process
(3) X{i} = null
(4) Train the classifier M{i} using L{i}
(5) Test U{i} by M{i}
(6) Calculate similarity score of given docu-
ment D between the extracted summary sen-
tences and the PowerPoint slides sentences by
equation 2
(7) Select the documents with top-five highest
similarity scores from U{i}
(8) Save selected samples into X{i}
(9) Manually correct each sentence label in
X{i} as summary or non-summary
(10) L{i + 1} = L{i} + X{i}
(11) U{i + 1} = U{i} − X{i}
(12) Evaluate M{i} on the testing set E
(13) i = i + 1, and repeat from (3) until U{i} is
empty or M{i} obtains satisfying performance
(14) M{i} is produced and the process ends
</listItem>
<equation confidence="0.994847">
Scoresim(D) = 1K
</equation>
<bodyText confidence="0.973697090909091">
We start our experiments by randomly choosing
six documents for manual labeling. We gradually
increase the training data pool by choosing five
more documents each time for manual correction.
We carry out two sets of experiments for compar-
ing our algorithm and random selection. We evalu-
ate the summarizer by ROUGE-L (summary-level
Longest Common Subsequence) F-measure (Lin,
2004).
The performance of our algorithm is illustrated
by the increasing ROUGE-L F-measure curve in
</bodyText>
<figureCaption confidence="0.589477">
Figure 1. It is shown to be consistently higher than
</figureCaption>
<figure confidence="0.6365046">
K L
1:
n=1 j=1
,
Sim(sn, pptsj) (2)
</figure>
<page confidence="0.856789">
25
</page>
<figureCaption confidence="0.999885">
Figure 1: Active learning vs. random selection
</figureCaption>
<bodyText confidence="0.999483916666667">
using randomly chosen samples. We also find that
by using only 51 documents for training, the per-
formance of the summarization model achieved
by our approach is better than that of the model
trained by random selection using all 65 presen-
tations (0.514 vs. 0.512 ROUGE-L F-measure).
This shows that our active learning approach re-
quires 22% less training data. Besides, acoustic
features can improve the performance of active
learning of speech summarization. Without acous-
tic features, our summarizer only performs 0.47
ROUGE-L F-measure.
</bodyText>
<sectionHeader confidence="0.991083" genericHeader="conclusions">
5 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999911333333334">
In this paper, we propose using active learning re-
duce the need for human annotation for tagging
extractive reference summary of lecture speech
summarization. We use RDTW to extract sen-
tences from transcriptions according to Power-
Point slides, and these sentences are then hand
corrected as reference summaries. The unlabeled
documents are selected whose similarity scores
between the extracted summary sentences and the
PowerPoint slides sentences are top-N highest for
labeling summary sentences. We then use an SVM
classifier to extract summary sentences. Summa-
rization results show an increasing learning curve
of F-measure, from 0.44 to 0.514, consistently
higher than that of using randomly chosen train-
ing data samples. Besides, acoustic features play
a significant role in active learning of speech sum-
marization. In our future work, we will try to ap-
ply different criteria, such as uncertainty-based or
committee-based criteria, for selecting samples to
be labeled, and compare the effectiveness of them.
</bodyText>
<sectionHeader confidence="0.997808" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9860575">
This work is partially supported by GRF612806 of
the Hong Kong RGC.
</bodyText>
<sectionHeader confidence="0.982842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99969982">
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie. ntu. edu. tw/cjlin/libsvm, 80:604–611.
Y. Fujii, K. Yamamoto, N. Kitaoka, and S. Nakagawa. 2008.
Class Lecture Summarization Taking into Account Con-
secutiveness of Important Sentences. In Proceedings of
Interspeech, pages 2438–2441.
T. Hayama, H. Nanba, and S. Kunifuji. 2005. Alignment
between a technical paper and presentation sheets using
a hidden markov model. In Active Media Technology,
2005.(AMT 2005). Proceedings of the 2005 International
Conference on, pages 102–106.
M.Y. Kan. 2007. SlideSeer: A digital library of aligned
document and presentation pairs. In Proceedings of the
7th ACM/IEEE-CS joint conference on Digital libraries,
pages 81–90. ACM New York, NY, USA.
D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty
sampling for supervised learning. In Proceedings of the
Eleventh International Conference on Machine Learning,
pages 148–156. Morgan Kaufmann.
C.Y. Lin. 2004. Rouge: A Package for Automatic Evalua-
tion of Summaries. Proceedings of the Workshop on Text
Summarization Branches Out (WAS 2004), pages 25–26.
A. McCallum and K. Nigam. 1998. Employing EM in Pool-
based Active Learning for Text Classification. In Proceed-
ings of ICML, pages 350–358.
A. Nenkova, R. Passonneau, and K. McKeown. 2007. The
Pyramid Method: Incorporating human content selection
variation in summarization evaluation. ACM Transactions
on Speech and Language Processing (TSLP), 4(2).
R. Ribeiro and D.M. de Matos. 2007. Extractive Summa-
rization of Broadcast News: Comparing Strategies for Eu-
ropean Portuguese. Lecture Notes in Computer Science,
4629:115.
D. Shen, J. Zhang, J. Su, G. Zhou, and C.L. Tan. 2004.
Multi-criteria-based Active Learning for Named Entity
Recognition. In Proceedings of 42th Annual Meeting of
the Association for Computational Linguistics. Associa-
tion for Computational Linguistics Morristown, NJ, USA.
S. Tong and D. Koller. 2002. Support vector machine ac-
tive learning with applications to text classification. The
Journal of Machine Learning Research, 2:45–66.
G. Tur, D. Hakkani-Tr, and R. E. Schapiro. 2005. Combin-
ing Active and Semi-supervised Learning for Spoken Lan-
guage Understanding. Speech Communications, 45:171–
186.
J.J. Zhang, S. Huang, and P. Fung. 2008. RSHMM++ for
extractive lecture speech summarization. In IEEE Spoken
Language Technology Workshop, 2008. SLT 2008, pages
161–164.
</reference>
<page confidence="0.998044">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.456051">
<title confidence="0.998953">Active Learning of Extractive Reference for Lecture Speech Summarization</title>
<author confidence="0.731635">Justin Jian Zhang</author>
<author confidence="0.731635">Pascale Human Language Technology</author>
<affiliation confidence="0.9998065">Department of Electronic and Computer University of Science and Technology</affiliation>
<address confidence="0.988028">Clear Water Bay,Hong</address>
<abstract confidence="0.999964791666667">We propose using active learning for tagging extractive reference summary of lecture speech. The training process of feature-based summarization model usually requires a large amount of training data with high-quality reference summaries. Human production of such summaries is tedious, and since inter-labeler agreement is low, very unreliable. Active learning helps assuage this problem by automatically selecting a small amount of unlabeled documents for humans to hand correct. Our method chooses the unlabeled documents according to the similarity score between the document and the comparable resource—PowerPoint slides. After manual correction, the selected documents are returned to the training pool. Summarization results show an increasing learning curve of ROUGE-L F-measure, from 0.44 to 0.514, consistently higher than that of using randomly chosen training samples.</abstract>
<intro confidence="0.979477">active learning, summarization</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.</title>
<date>2001</date>
<pages>80--604</pages>
<note>csie. ntu. edu. tw/cjlin/libsvm,</note>
<contexts>
<context position="5904" citStr="Chang and Lin, 2001" startWordPosition="900" endWordPosition="903">o feature-based classification for summarizing Portuguese broadcast news data, another work on Japanese lecture speech drew the opposite conclusion (Fujii et al., 2008) that feature-based classification method is better. Therefore we continue to use the featurebased method in our work. We consider the extractive summarization as a binary classification problem, we predict whether each sentence of the lecture transcription should be in a summary or not. We use Radial Basis Function (RBF) kernel for constructing SVM classifier, which is provided by LIBSVM, a library for support vector machines (Chang and Lin, 2001). We represent each sentence by a feature vector which consists of acoustic features: duration of the sentence, average syllable Duration, F0 information features, energy information features; and linguistic features: length of the sentence counted by word and TFIDF information features, as shown in (Zhang et al., 2008). We then build the SVM classifier as our summarizer based on these sentence feature vectors. 3 Active Learning for Tagging Reference Summary and Summarization Similar to (Hayama et al., 2005; Kan, 2007), we have previously proposed how presentation slides are used to compile re</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.C. Chang and C.J. Lin. 2001. LIBSVM: a library for support vector machines. Software available at http://www. csie. ntu. edu. tw/cjlin/libsvm, 80:604–611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Fujii</author>
<author>K Yamamoto</author>
<author>N Kitaoka</author>
<author>S Nakagawa</author>
</authors>
<title>Class Lecture Summarization Taking into Account Consecutiveness of Important Sentences.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>2438--2441</pages>
<contexts>
<context position="1759" citStr="Fujii et al., 2008" startWordPosition="251" endWordPosition="254"> that of using randomly chosen training samples. Index Terms: active learning, summarization 1 Introduction The need for the summarization of classroom lectures, conference speeches, political speeches is ever increasing with the advent of remote learning, distributed collaboration and electronic archiving. These user needs cannot be sufficiently met by short abstracts. In recent years, virtually all summarization systems are extractive - compiling bullet points from the document using some saliency criteria. Reference summaries are often manually compiled by one or multiple human annotators (Fujii et al., 2008; Nenkova et al., 2007). Unlike for speech recognition where the reference sentence is clear and unambiguous, and unlike for machine translation where there are guidelines for manual translating reference sentences, there is no clear guideline for compiling a good reference summary. As a result, one of the most important challenges in speech summarization remains the difficulty to compile, evaluate and thus to learn what a good summary is. Human judges tend to agree on obviously good and very bad summaries but cannot agree on borderline cases. Consequently, annotator agreement is low. Referenc</context>
<context position="5452" citStr="Fujii et al., 2008" startWordPosition="826" endWordPosition="829">d-out test set. We randomly select 5 presentations from U as our seed presentations. Reference summaries of the seed presentations and the presentations of test set are generated from the PowerPoint slides and presentation transcriptions using RDTW followed by manual correction, as described in Section 3. 2.2 SVM Classifier and the Feature Set While (Ribeiro and de Matos, 2007) has shown that MMR (maximum marginal relevance) approach is superior to feature-based classification for summarizing Portuguese broadcast news data, another work on Japanese lecture speech drew the opposite conclusion (Fujii et al., 2008) that feature-based classification method is better. Therefore we continue to use the featurebased method in our work. We consider the extractive summarization as a binary classification problem, we predict whether each sentence of the lecture transcription should be in a summary or not. We use Radial Basis Function (RBF) kernel for constructing SVM classifier, which is provided by LIBSVM, a library for support vector machines (Chang and Lin, 2001). We represent each sentence by a feature vector which consists of acoustic features: duration of the sentence, average syllable Duration, F0 inform</context>
</contexts>
<marker>Fujii, Yamamoto, Kitaoka, Nakagawa, 2008</marker>
<rawString>Y. Fujii, K. Yamamoto, N. Kitaoka, and S. Nakagawa. 2008. Class Lecture Summarization Taking into Account Consecutiveness of Important Sentences. In Proceedings of Interspeech, pages 2438–2441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hayama</author>
<author>H Nanba</author>
<author>S Kunifuji</author>
</authors>
<title>Alignment between a technical paper and presentation sheets using a hidden markov model.</title>
<date>2005</date>
<booktitle>In Active Media Technology, 2005.(AMT 2005). Proceedings of the 2005 International Conference on,</booktitle>
<pages>102--106</pages>
<contexts>
<context position="6416" citStr="Hayama et al., 2005" startWordPosition="980" endWordPosition="983">ng SVM classifier, which is provided by LIBSVM, a library for support vector machines (Chang and Lin, 2001). We represent each sentence by a feature vector which consists of acoustic features: duration of the sentence, average syllable Duration, F0 information features, energy information features; and linguistic features: length of the sentence counted by word and TFIDF information features, as shown in (Zhang et al., 2008). We then build the SVM classifier as our summarizer based on these sentence feature vectors. 3 Active Learning for Tagging Reference Summary and Summarization Similar to (Hayama et al., 2005; Kan, 2007), we have previously proposed how presentation slides are used to compile reference summaries automatically (Zhang et al., 2008). The motivations behind this procedure are: • presentation slides are compiled by the authors themselves and therefore provide a good standard summary of their work; • presentation slides contain the hierarchical rhetorical structure of lecture speech as the titles, subtitles, page breaks, bullet points provide an enriched set of discourse information that are otherwise not apparent in the spoken lecture transcriptions. We propose a Relaxed Dynamic Time W</context>
</contexts>
<marker>Hayama, Nanba, Kunifuji, 2005</marker>
<rawString>T. Hayama, H. Nanba, and S. Kunifuji. 2005. Alignment between a technical paper and presentation sheets using a hidden markov model. In Active Media Technology, 2005.(AMT 2005). Proceedings of the 2005 International Conference on, pages 102–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Y Kan</author>
</authors>
<title>SlideSeer: A digital library of aligned document and presentation pairs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries,</booktitle>
<pages>81--90</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6428" citStr="Kan, 2007" startWordPosition="984" endWordPosition="985">ich is provided by LIBSVM, a library for support vector machines (Chang and Lin, 2001). We represent each sentence by a feature vector which consists of acoustic features: duration of the sentence, average syllable Duration, F0 information features, energy information features; and linguistic features: length of the sentence counted by word and TFIDF information features, as shown in (Zhang et al., 2008). We then build the SVM classifier as our summarizer based on these sentence feature vectors. 3 Active Learning for Tagging Reference Summary and Summarization Similar to (Hayama et al., 2005; Kan, 2007), we have previously proposed how presentation slides are used to compile reference summaries automatically (Zhang et al., 2008). The motivations behind this procedure are: • presentation slides are compiled by the authors themselves and therefore provide a good standard summary of their work; • presentation slides contain the hierarchical rhetorical structure of lecture speech as the titles, subtitles, page breaks, bullet points provide an enriched set of discourse information that are otherwise not apparent in the spoken lecture transcriptions. We propose a Relaxed Dynamic Time Warping (RDTW</context>
</contexts>
<marker>Kan, 2007</marker>
<rawString>M.Y. Kan. 2007. SlideSeer: A digital library of aligned document and presentation pairs. In Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries, pages 81–90. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2933" citStr="Lewis and Catlett, 1994" startWordPosition="435" endWordPosition="438">onsequently, annotator agreement is low. Reference summary generation is a tedious and low efficiency task. On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries. To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution. Active learning has been applied to NLP tasks such as spoken language understanding (Tur et al., 2005), information extraction (Shen et al., 2004), and text classification (Lewis and Catlett, 1994; McCallum and Nigam, 1998; Tong and Koller, 2002). Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model. In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides. Based on this framework, we further propose a certainty-based active learning method to alleviate the burden of human annotation of training data. The </context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the Eleventh International Conference on Machine Learning, pages 148–156. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>Rouge: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>Proceedings of the Workshop on Text Summarization Branches Out (WAS</booktitle>
<pages>25--26</pages>
<contexts>
<context position="11987" citStr="Lin, 2004" startWordPosition="1918" endWordPosition="1919">+ X{i} (11) U{i + 1} = U{i} − X{i} (12) Evaluate M{i} on the testing set E (13) i = i + 1, and repeat from (3) until U{i} is empty or M{i} obtains satisfying performance (14) M{i} is produced and the process ends Scoresim(D) = 1K We start our experiments by randomly choosing six documents for manual labeling. We gradually increase the training data pool by choosing five more documents each time for manual correction. We carry out two sets of experiments for comparing our algorithm and random selection. We evaluate the summarizer by ROUGE-L (summary-level Longest Common Subsequence) F-measure (Lin, 2004). The performance of our algorithm is illustrated by the increasing ROUGE-L F-measure curve in Figure 1. It is shown to be consistently higher than K L 1: n=1 j=1 , Sim(sn, pptsj) (2) 25 Figure 1: Active learning vs. random selection using randomly chosen samples. We also find that by using only 51 documents for training, the performance of the summarization model achieved by our approach is better than that of the model trained by random selection using all 65 presentations (0.514 vs. 0.512 ROUGE-L F-measure). This shows that our active learning approach requires 22% less training data. Besid</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.Y. Lin. 2004. Rouge: A Package for Automatic Evaluation of Summaries. Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004), pages 25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>Employing EM in Poolbased Active Learning for Text Classification.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>350--358</pages>
<contexts>
<context position="2959" citStr="McCallum and Nigam, 1998" startWordPosition="439" endWordPosition="442">reement is low. Reference summary generation is a tedious and low efficiency task. On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries. To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution. Active learning has been applied to NLP tasks such as spoken language understanding (Tur et al., 2005), information extraction (Shen et al., 2004), and text classification (Lewis and Catlett, 1994; McCallum and Nigam, 1998; Tong and Koller, 2002). Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model. In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides. Based on this framework, we further propose a certainty-based active learning method to alleviate the burden of human annotation of training data. The rest of this paper is orga</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. Employing EM in Poolbased Active Learning for Text Classification. In Proceedings of ICML, pages 350–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
<author>K McKeown</author>
</authors>
<title>The Pyramid Method: Incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="1782" citStr="Nenkova et al., 2007" startWordPosition="255" endWordPosition="258">mly chosen training samples. Index Terms: active learning, summarization 1 Introduction The need for the summarization of classroom lectures, conference speeches, political speeches is ever increasing with the advent of remote learning, distributed collaboration and electronic archiving. These user needs cannot be sufficiently met by short abstracts. In recent years, virtually all summarization systems are extractive - compiling bullet points from the document using some saliency criteria. Reference summaries are often manually compiled by one or multiple human annotators (Fujii et al., 2008; Nenkova et al., 2007). Unlike for speech recognition where the reference sentence is clear and unambiguous, and unlike for machine translation where there are guidelines for manual translating reference sentences, there is no clear guideline for compiling a good reference summary. As a result, one of the most important challenges in speech summarization remains the difficulty to compile, evaluate and thus to learn what a good summary is. Human judges tend to agree on obviously good and very bad summaries but cannot agree on borderline cases. Consequently, annotator agreement is low. Reference summary generation is</context>
<context position="8849" citStr="Nenkova et al., 2007" startWordPosition="1391" endWordPosition="1394">the sentences iopt n of the transcription whose similarity scores of sentence pairs: (iopt n , jopt n ), are higher than the pre-defined threshold as the reference summary sentences. The advantage of using these summaries as references is that it circumvents the disagreement between multiple human annotators. We have compared these reference summaries to human-labeled summaries. When asked to ”select the most salient sentences for a summary”, we found that inter-annotator agreement ranges from 30% to 50% only. Sometimes even a single person might choose different sentences at different times (Nenkova et al., 2007). However, when instructed to follow the structure and points in the presentation slides, inter-annotator agreement increased to 80%. The agreement between automatically extracted reference summary and humans also reaches 75%. Based on this high degree of agreement, we generate reference summaries by asking a human to manually correct those extracted by the RDTW algorithm. Our reference summaries therefore make for more reliable training and test data. For a transcribed presentation D with a sequence of recognized sentences {s1, s2, ..., sN}, we want to find the sentences to be classified as s</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing (TSLP), 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ribeiro</author>
<author>D M de Matos</author>
</authors>
<title>Extractive Summarization of Broadcast News: Comparing Strategies for European Portuguese.</title>
<date>2007</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>4629--115</pages>
<marker>Ribeiro, de Matos, 2007</marker>
<rawString>R. Ribeiro and D.M. de Matos. 2007. Extractive Summarization of Broadcast News: Comparing Strategies for European Portuguese. Lecture Notes in Computer Science, 4629:115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C L Tan</author>
</authors>
<title>Multi-criteria-based Active Learning for Named Entity Recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of 42th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2883" citStr="Shen et al., 2004" startWordPosition="428" endWordPosition="431">aries but cannot agree on borderline cases. Consequently, annotator agreement is low. Reference summary generation is a tedious and low efficiency task. On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries. To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution. Active learning has been applied to NLP tasks such as spoken language understanding (Tur et al., 2005), information extraction (Shen et al., 2004), and text classification (Lewis and Catlett, 1994; McCallum and Nigam, 1998; Tong and Koller, 2002). Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model. In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides. Based on this framework, we further propose a certainty-based active learning method to alleviate the</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>D. Shen, J. Zhang, J. Su, G. Zhou, and C.L. Tan. 2004. Multi-criteria-based Active Learning for Named Entity Recognition. In Proceedings of 42th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tong</author>
<author>D Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--45</pages>
<contexts>
<context position="2983" citStr="Tong and Koller, 2002" startWordPosition="443" endWordPosition="446">summary generation is a tedious and low efficiency task. On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries. To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution. Active learning has been applied to NLP tasks such as spoken language understanding (Tur et al., 2005), information extraction (Shen et al., 2004), and text classification (Lewis and Catlett, 1994; McCallum and Nigam, 1998; Tong and Koller, 2002). Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model. In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides. Based on this framework, we further propose a certainty-based active learning method to alleviate the burden of human annotation of training data. The rest of this paper is organized as follows: Sectio</context>
</contexts>
<marker>Tong, Koller, 2002</marker>
<rawString>S. Tong and D. Koller. 2002. Support vector machine active learning with applications to text classification. The Journal of Machine Learning Research, 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>D Hakkani-Tr</author>
<author>R E Schapiro</author>
</authors>
<title>Combining Active and Semi-supervised Learning for Spoken Language Understanding. Speech Communications,</title>
<date>2005</date>
<pages>45--171</pages>
<contexts>
<context position="2839" citStr="Tur et al., 2005" startWordPosition="422" endWordPosition="425">o agree on obviously good and very bad summaries but cannot agree on borderline cases. Consequently, annotator agreement is low. Reference summary generation is a tedious and low efficiency task. On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries. To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution. Active learning has been applied to NLP tasks such as spoken language understanding (Tur et al., 2005), information extraction (Shen et al., 2004), and text classification (Lewis and Catlett, 1994; McCallum and Nigam, 1998; Tong and Koller, 2002). Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model. In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides. Based on this framework, we further propose a certainty-b</context>
</contexts>
<marker>Tur, Hakkani-Tr, Schapiro, 2005</marker>
<rawString>G. Tur, D. Hakkani-Tr, and R. E. Schapiro. 2005. Combining Active and Semi-supervised Learning for Spoken Language Understanding. Speech Communications, 45:171– 186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Zhang</author>
<author>S Huang</author>
<author>P Fung</author>
</authors>
<title>RSHMM++ for extractive lecture speech summarization.</title>
<date>2008</date>
<booktitle>In IEEE Spoken Language Technology Workshop,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="4289" citStr="Zhang et al., 2008" startWordPosition="649" endWordPosition="652"> acoustic/prosodic, and linguistic feature sets for representing each sentence. Section 3 depicts how to 23 Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 23–26, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP compile reference summaries with high inter labeler agreement by using the RDTW algorithm and our active learning algorithm for tagging extractive reference summary. We describe our experiments and evaluate the results in Section 4. Our conclusion follows in Section 5. 2 Experimental Setup 2.1 The Corpus Our lecture speech corpus (Zhang et al., 2008) contains 111 presentations recorded from the NCMMSC2005 and NCMMSC2007 conferences for evaluating our approach. The manual transcriptions and the comparable corpus— PowerPoint slides are also collected. Each presentation lasts for 15 minutes on average. We select 71 of the 111 presentations with well organized PowerPoint slides that always have clear sketches and evidently aligned with the transcriptions. We use about 90% of the lecture corpus from the 65 presentations as original unlabeled data U and the remaining 6 presentations as held-out test set. We randomly select 5 presentations from </context>
<context position="6225" citStr="Zhang et al., 2008" startWordPosition="949" endWordPosition="952">ation as a binary classification problem, we predict whether each sentence of the lecture transcription should be in a summary or not. We use Radial Basis Function (RBF) kernel for constructing SVM classifier, which is provided by LIBSVM, a library for support vector machines (Chang and Lin, 2001). We represent each sentence by a feature vector which consists of acoustic features: duration of the sentence, average syllable Duration, F0 information features, energy information features; and linguistic features: length of the sentence counted by word and TFIDF information features, as shown in (Zhang et al., 2008). We then build the SVM classifier as our summarizer based on these sentence feature vectors. 3 Active Learning for Tagging Reference Summary and Summarization Similar to (Hayama et al., 2005; Kan, 2007), we have previously proposed how presentation slides are used to compile reference summaries automatically (Zhang et al., 2008). The motivations behind this procedure are: • presentation slides are compiled by the authors themselves and therefore provide a good standard summary of their work; • presentation slides contain the hierarchical rhetorical structure of lecture speech as the titles, s</context>
</contexts>
<marker>Zhang, Huang, Fung, 2008</marker>
<rawString>J.J. Zhang, S. Huang, and P. Fung. 2008. RSHMM++ for extractive lecture speech summarization. In IEEE Spoken Language Technology Workshop, 2008. SLT 2008, pages 161–164.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>