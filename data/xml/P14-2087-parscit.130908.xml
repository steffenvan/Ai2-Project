<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015304">
<title confidence="0.996825">
Applying a Naive Bayes Similarity Measure to
Word Sense Disambiguation
</title>
<author confidence="0.999669">
Tong Wang Graeme Hirst
</author>
<affiliation confidence="0.99993">
University of Toronto University of Toronto
</affiliation>
<email confidence="0.998228">
tong@cs.toronto.edu gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.99388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978416666667">
We replace the overlap mechanism of the
Lesk algorithm with a simple, general-
purpose Naive Bayes model that mea-
sures many-to-many association between
two sets of random variables. Even with
simple probability estimates such as max-
imum likelihood, the model gains signifi-
cant improvement over the Lesk algorithm
on word sense disambiguation tasks. With
additional lexical knowledge from Word-
Net, performance is further improved to
surpass the state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865071428571">
To disambiguate a homonymous word in a given
context, Lesk (1986) proposed a method that mea-
sured the degree of overlap between the glosses
of the target and context words. Known as the
Lesk algorithm, this simple and intuitive method
has since been extensively cited and extended in
the word sense disambiguation (WSD) commu-
nity. Nonetheless, its performance in several WSD
benchmarks is less than satisfactory (Kilgarriff
and Rosenzweig, 2000; Vasilescu et al., 2004).
Among the popular explanations is a key limita-
tion of the algorithm, that “Lesk’s approach is very
sensitive to the exact wording of definitions, so the
absence of a certain word can radically change the
results.” (Navigli, 2009).
Compounding this problem is the fact that many
Lesk variants limited the concept of overlap to
the literal interpretation of string matching (with
their own variants such as length-sensitive match-
ing (Baner ee and Pedersen, 2002), etc.), and it
was not until recently that overlap started to take
on other forms such as tree-matching (Chen et al.,
2009) and vector space models (Abdalgader and
Skabar, 2012; Raviv et al., 2012; Patwardhan and
Pedersen, 2006). To address this limitation, a
Naive Bayes model (NBM) is proposed in this
study as a novel, probabilistic treatment of over-
lap in gloss-based WSD.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962305555556">
In the extraordinarily rich literature on WSD, we
focus our review on those closest to the topic of
Lesk and NBM. In particular, we opt for the “sim-
plified Lesk” (Kilgarriff and Rosenzweig, 2000),
where inventory senses are assessed by gloss-
context overlap rather than gloss-gloss overlap.
This particular variant prevents proliferation of
gloss comparison on larger contexts (Mihalcea
et al., 2004) and is shown to outperform the origi-
nal Lesk algorithm (Vasilescu et al., 2004).
To the best of our knowledge, NBMs have been
employed exclusively as classifiers in WSD —
that is, in contrast to their use as a similarity mea-
sure in this study. Gale et al. (1992) used NB
classifier resembling an information retrieval sys-
tem: a WSD instance is regarded as a document d,
and candidate senses are scored in terms of “rel-
evance” to d. When evaluated on a WSD bench-
mark (Vasilescu et al., 2004), the algorithm com-
pared favourably to Lesk variants (as expected
for a supervised method). Pedersen (2000) pro-
posed an ensemble model with multiple NB clas-
sifiers differing by context window size. Hristea
(2009) trained an unsupervised NB classifier using
the EM algorithm and empirically demonstrated
the benefits of WordNet-assisted (Fellbaum, 1998)
feature selection over local syntactic features.
Among Lesk variants, Baner ee and Pedersen
(2002) extended the gloss of both inventory senses
and the context words to include words in their re-
lated synsets in WordNet. Senses were scored by
the sum of overlaps across all relation pairs, and
the effect of individual relation pairs was evalu-
ated in a later work (Baner ee and Pedersen, 2003).
Overlap was assessed by string matching, with the
number of matching words squared so as to assign
</bodyText>
<page confidence="0.966296">
531
</page>
<bodyText confidence="0.992166313725491">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531–537,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
higher scores to multi-word overlaps.
Breaking away from string matching, Wilks
et al. (1990) measured overlap as similarity be-
tween gloss- and context-vectors, which were ag-
gregated word vectors encoding second order co-
occurrence information in glosses. An extension
by Patwardhan and Pedersen (2006) differentiated
context word senses and extended shorter glosses
with related glosses in WordNet. Patwardhan et al.
(2003) measured overlap by concept similarity
(Budanitsky and Hirst, 2006) between each inven-
tory sense and the context words. Gloss overlaps
from their earlier work actually out-performed all
five similarity-based methods.
More recently, Chen et al. (2009) pro-
posed a tree-matching algorithm that measured
gloss-context overlap as the weighted sum of
dependency-induced lexical distance. Abdalgader
and Skabar (2012) constructed a sentential simi-
larity measure (Li et al., 2006) using lexical simi-
larity measures (Budanitsky and Hirst, 2006), and
overlap was measured by the cosine of their re-
spective sentential vectors. A related approach
(Raviv et al., 2012) also used Wikipedia-induced
concepts to encoded sentential vectors. These sys-
tems compared favourably to existing methods in
WSD performance, although by using sense fre-
quency information, they are essentially super-
vised methods.
Distributional methods have been used in many
WSD systems in quite different flavours than the
current study. Kilgarriff and Rosenzweig (2000)
proposed a Lesk variant where each gloss word is
weighted by its idf score in relation to all glosses,
and gloss-context association was incremented by
these weights rather than binary, overlap counts.
Miller et al. (2012) used distributional thesauri as a
knowledge base to increase overlaps, which were,
again, assessed by string matching.
In conclusion, the majority of Lesk variants
focused on extending the gloss to increase the
chance of overlapping, while the proposed NBM
aims to make better use of the limited lexical
knowledge available. In contrast to string match-
ing, the probabilistic nature of our model offers
a “softer” measurement of gloss-context associa-
tion, resulting in a novel approach to unsupervised
WSD with state-of-the-art performance in more
than one WSD benchmark (Section 4).
</bodyText>
<sectionHeader confidence="0.902382" genericHeader="method">
3 Model and Task Descriptions
</sectionHeader>
<subsectionHeader confidence="0.99958">
3.1 The Naive Bayes Model
</subsectionHeader>
<bodyText confidence="0.999407142857143">
Formally, given two sets e = {ei} and f = { fj}
each consisting of multiple random events, the
proposed model measures the probabilistic asso-
ciation p(f|e) between e and f. Under the assump-
tion of conditional independence among the events
in each set, a Naive Bayes treatment of the mea-
sure can be formulated as:
</bodyText>
<equation confidence="0.9966226">
p({ei} |fj)p(fj)
p({ei})
∏j[p(fj)∏i p(ei |fj)]
∏j ∏i p(ei)
(1)
</equation>
<bodyText confidence="0.999738166666667">
In the second expression, Bayes’s rule is applied
not only to take advantage of the conditional inde-
pendence among ei’s, but also to facilitate proba-
bility estimation, since p({ei} |fj) is easier to esti-
mate in the context of WSD, where sample spaces
of e and f become asymmetric (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999887">
3.2 Model Application in WSD
</subsectionHeader>
<bodyText confidence="0.999970833333333">
In the context of WSD, e can be regarded as an
instance of a polysemous word w, while f repre-
sents certain lexical knowledge about the sense s
of w manifested by e.1 WSD is thus formulated as
identifying the sense s∗ in the sense inventory Y
of w s.t.:
</bodyText>
<equation confidence="0.968273">
s∗ = argmax p(f|e) (2)
s∈S
</equation>
<bodyText confidence="0.95202105">
In one of their simplest forms, ei’s correspond
to co-occurring words in the instance of w, and
fj’s consist of the gloss words of sense s. Conse-
quently, p(f|e) is essentially measuring the asso-
ciation between context words of w and definition
texts of s, i.e., the gloss-context association in the
simplified Lesk algorithm (Kilgarriff and Rosen-
zweig, 2000). A major difference, however, is that
instead of using hard, overlap counts between the
two sets of words from the gloss and the context,
this probabilistic treatment can implicitly model
the distributional similarity among the elements ei
and fj (and consequently between the sets e and
f) over a wider range of contexts. The result is a
“softer” proxy of association than the binary view
of overlaps in existing Lesk variants.
The foregoing discussion offers a second mo-
tivation for applying Bayes’s rule on the second
1Think of the notations e and f mnemonically as exem-
plars and features, respectively.
</bodyText>
<equation confidence="0.960569">
p(f|e) =∏ p(fj|{ei}) = ∏
j j
</equation>
<page confidence="0.986541">
532
</page>
<table confidence="0.988866166666667">
Senses Hypernyms Hyponyms Synonyms
factory building brewery, works,
complex, factory, industrial
complex mill, ... plant
life form organism, perennial, flora,
being crop... plant life
</table>
<tableCaption confidence="0.990927">
Table 1: Lexical knowledge for the word plant un-
</tableCaption>
<bodyText confidence="0.941967285714286">
der its two meanings factory and life form.
expression in Equation (1): it is easier to estimate
p(ei |fj) than p(fj|ei), since the vocabulary for the
lexical knowledge features (fj) is usually more
limited than that of the contexts (ei) and hence esti-
mation of the former suffices on a smaller amount
of data than that of the latter.
</bodyText>
<subsectionHeader confidence="0.9613515">
3.3 Incorporating Additional Lexical
Knowledge
</subsectionHeader>
<bodyText confidence="0.999059094339623">
The input of the proposed NBM is bags of words,
and thus it is straightforward to incorporate var-
ious forms of lexical knowledge (LK) for word
senses: by concatenating a tokenized knowledge
source to the existing knowledge representation f,
while the similarity measure remains unchanged.
The availability of LK largely depends on the
sense inventory used in a WSD task. WordNet
senses are often used in Senseval and SemEval
tasks, and hence senses (or synsets, and possibly
their corresponding word forms) that are seman-
tic related to the inventory senses under WordNet
relations are easily obtainable and have been ex-
ploited by many existing studies.
As pointed out by Patwardhan et al. (2003),
however, “not all of these relations are equally
helpful.” Relation pairs involving hyponyms were
shown to result in better F-measure when used
in gloss overlaps (Banerjee and Pedersen, 2003).
The authors attributed the phenomenon to the the
multitude of hyponyms compared to other rela-
tions. We further hypothesize that, beyond sheer
numbers, synonyms and hyponyms offer stronger
semantic specification that helps distinguish the
senses of a given ambiguous word, and thus are
more effective knowledge sources for WSD.
Take the word plant for example. Selected hy-
pernyms, hyponyms, and synonyms pertaining to
its two senses factory and life form are listed in
Table 1. Hypernyms can be overly general terms
(e.g., being). Although conceptually helpful for
humans in coarse-grained WSD, this generality is
likely to inflate the hypernyms’ probabilistic esti-
mation. Hyponyms, on the other hand, help spec-
ify their corresponding senses with information
that is possibly missing from the often overly brief
glosses: the many technical terms as hyponyms
in Table 1 — though rare — are likely to occur
in the (possibly domain-specific) contexts that are
highly typical of the corresponding senses. Par-
ticularly for the NBM, the co-occurrence is likely
to result in stronger gloss-definition associations
when similar contexts appear in a WSD instance.
We also observe that some semantically related
words appear under rare senses (e.g., still as an
alcohol-manufacturing plant, and annual as a one-
year-life-cycle plant; omitted from Table 1). This
is a general phenomenon in gloss-based WSD and
is beyond the scope of the current discussion.2
Overall, all three sources of LK may complement
each other in WSD tasks, with hyponyms particu-
larly promising in both quantity and quality com-
pared to hypernyms and synonyms.3
</bodyText>
<subsectionHeader confidence="0.983422">
3.4 Probability Estimation
</subsectionHeader>
<bodyText confidence="0.9999786">
A most open-ended question is how to estimate the
probabilities in Equation (1). In WSD in particu-
lar, the estimation concerns the marginal and con-
ditional probabilities of and between word tokens.
Many options are available to this end in statis-
tical machine learning (MLE, MAP, etc.), infor-
mation theory (Church and Hanks, 1990; Turney,
2001), as well as the rich body of research in lex-
ical semantic similarity Resnik, 1995; Jiang and
Conrath, 1997; Budanitsky and Hirst, 2006).
Here we choose maximum likelihood — not
only for its simplicity, but also to demonstrate
model strength with a relatively crude probability
estimation. To avoid underflow, Equation (1) is
estimated as the following log probability:
</bodyText>
<equation confidence="0.943737">
∑ log c(fj) +∑i∑log c(ei, fj) − |f|∑log c(ei)
i c(·) j c(fj) j c(·)
=(1− |e|)∑logc(f j) − |f|∑ logc(ei)
i j
logc(ei, fj)+ |f|(|e |−1)logc(·),
</equation>
<bodyText confidence="0.948886">
where c(x) is the count of word x, c(·) is the corpus
</bodyText>
<footnote confidence="0.866793666666667">
2We do, however, refer curious readers to the work of Ra-
viv et al. (2012) for a novel treatment of a similar problem.
3Note that LK expansion is a feature of our model rather
than a requirement. What type of knowledge to include is
eventually a decision made by the user based on the applica-
tion and LK availability.
</footnote>
<figure confidence="0.7746535">
∑
j
+∑
i
</figure>
<page confidence="0.992947">
533
</page>
<bodyText confidence="0.999900384615385">
size, c(x,y) is the joint count of x and y, and JvJ is
the dimension of vector v.
Nonetheless, we do investigate how model per-
formance responds to estimation quality. Specif-
ically in WSD, a source corpus is defined as the
source of the majority of the WSD instances in a
given dataset, and a baseline corpus of a smaller
size and less resemblance to the instances is used
for all datasets. The assumption is that a source
corpus offers better estimates for the model than
the baseline corpus, and difference in model per-
formance is expected when using probability esti-
mation of different quality.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999529">
4.1 Data, Scoring, and Pre-processing
</subsectionHeader>
<bodyText confidence="0.988846773584906">
Various aspects of the model discussed in Section
3 are evaluated in the English lexical sample tasks
from Senseval-2 (Edmonds and Cotton, 2001) and
SemEval-2007 (Pradhan et al., 2007). Training
sections are used as development data and test
sections held out for final testing. Model perfor-
mance is evaluated in terms of WSD accuracy us-
ing Equation (2) as the scoring function. Accu-
racy is defined as the number of correct responses
over the number of instances. Because it is a rare
event for the NBM to produce identical scores,4
the model always proposes a unique answer and
accuracy is thus equivalent to F-score commonly
used in existing reports.
Multiword expressions (MWEs) in the
Senseval-2 sense inventory are not explicitly
marked in the contexts. Several of the top-ranking
systems implemented their own MWE detection
algorithms (Kilgarriff and Rosenzweig, 2000;
Litkowski, 2002). Without digressing to the
details of MWE detection — and meanwhile,
to ensure fair comparison with existing systems
— we implement two variants of the prediction
module, one completely ignorant of MWE and
defaulting to INCORRECT for all MWE-related
answers, while the other assuming perfect MWE
detection and performing regular disambiguation
algorithm on the MWE-related senses (not de-
faulting to CORRECT). All results reported for
Senseval-2 below are harmonic means of the two
outcomes.
Each inventory sense is represented by a set of
LK tokens (e.g., definition texts, synonyms, etc.)
4This has never occurred in the hundreds of thousands of
runs in our development process.
from their corresponding WordNet synset (or in
the coarse-grained case, a concatenation of tokens
from all synsets in a sense group). The MIT-JWI
library (Finlayson, 2014) is used for accessing
WordNet. Usage examples in glosses (included by
the library by default) are removed in our experi-
ments.5
Basic pre-processing is performed on the con-
texts and the glosses, including lower-casing, stop-
word removal, lemmatization on both datasets,
and tokenization on the Senseval-2 instances.6
Stanford CoreNLP7 is used for lemmatization and
tokenization. Identical procedures are applied to
all corpora used for probability estimation.
Binomial test is used for significance testing,
and with one exception explicitly noted in Sec-
tion 4.3, all differences presented are statistically
highly significant (p &lt; 0.001).
</bodyText>
<subsectionHeader confidence="0.998137">
4.2 Comparing Lexical Knowledge Sources
</subsectionHeader>
<bodyText confidence="0.999782368421053">
To study the effect of different types of LK in
WSD (Section 3.3), for each inventory sense, we
choose synonyms (syn), hypernyms (hpr), and hy-
ponyms (hpo) as extended LK in addition to its
gloss. The WSD model is evaluated with gloss-
only (glo), individual extended LK sources, and
the combination of all four sources (all). The re-
sults are listed in Table 2 together with existing re-
sults (1st and 2nd correspond to the results of the
top two unsupervised methods in each dataset).8
By using only glosses, the proposed model
already shows statistically significant improve-
ment over the basic Lesk algorithm (92.4%
and 140.5% relative improvement in Senseval-
2 coarse- and fine-grained tracks, respectively).9
Moreover, comparison between coarse- and fine-
grained tracks reveals interesting properties of dif-
ferent LK sources. Previous hypotheses (Section
3.3) are empirically confirmed that WSD perfor-
</bodyText>
<footnote confidence="0.9799574375">
5We also compared the two Lesk baselines (with and
without usage examples) on the development data but did not
observe significant differences as reported by Kilgarriff and
Rosenzweig (2000).
6The SemEval-2007 instances are already tokenized.
7http://nlp.stanford.edu/software/
corenlp.shtml.
8We excluded the results of UNED (Fern´andez-Amor´os
et al., 2001) in Senseval-2 because, by using sense frequency
information that is only obtainable from sense-annotated cor-
pora, it is essentially a supervised system.
9Comparisons are made against the simplified Lesk al-
gorithm (Kilgarriff and Rosenzweig, 2000) without usage
examples. The comparison is unavailable in SemEval2007
since we have not found existing experiments with this exact
configuration.
</footnote>
<page confidence="0.997329">
534
</page>
<tableCaption confidence="0.9006585">
Table 2: Lexical knowledge sources and WSD performance (F-measure) on the Senseval-2 (fine- and
coarse-grained) and the SemEval-2007 dataset.
</tableCaption>
<figure confidence="0.999198888888889">
Lesk
Dataset
glo syn hpr hpo all 1st 2nd
Senseval-2 Coarse
.475
.469 .367
.262
.478 .494 .518 .523
Senseval-2 Fine
.362
.412 .293
.163
.371 .326 .379 .388
SemEval-2007
.538 .521
.494
–
.511 .507 .550 .573
</figure>
<figureCaption confidence="0.999881">
Figure 1: Model response to probability esti-
</figureCaption>
<bodyText confidence="0.985869115384616">
mates of different quality on the SemEval-2007
dataset. Error bars indicate confidence intervals
(p &lt; .001), and the dashed line corresponds to the
best reported result.
mance benefits most from hyponyms and least
from hypernyms. Specifically, highly similar, fine-
grained sense candidates apparently share more
hypernyms in the fine-grained case than in the
coarse-grained case; adding to the generality of
hypernyms (both semantic and distributional), we
postulate that their probability in the NBM is uni-
formly inflated among many sense candidates, and
hence they decrease in distinguishability. Syn-
onyms might help with regard to semantic spec-
ification, though their limited quantity also limits
their benefits. These patterns on the LK types are
consistent in all three experiments.
When including all four LK sources, our model
outperforms the state-of-the-art systems with sta-
tistical significance in both coarse-grained tasks.
For the fine-grained track, it achieves 2nd place
after that of Tugwell and Kilgarriff (2001), which
used a decision list (Yarowsky, 1995) on manu-
ally selected corpora evidence for each inventory
sense, and thus is not subject to loss of distin-
guishability in the glosses as Lesk variants are.
</bodyText>
<subsectionHeader confidence="0.999401">
4.3 Probability Estimation
</subsectionHeader>
<bodyText confidence="0.999887615384615">
To evaluate model response to probability esti-
mation of different quality (Section 3.4), source
corpora are chosen as the majority value of the
doc-source attribute of instances in each dataset,
namely, the British National Corpus for Senseval-
2 (94%) and the Wall Street Journal for SemEval-
2007 (86%). The Brown Corpus is shared by both
datasets as the baseline corpus. Figure 1 shows the
comparison on the SemEval-2007 dataset. Across
all experiments, higher WSD accuracy is consis-
tently witnessed using the source corpus; differ-
ences are statistically highly significant except for
hpo (which is significant with p &lt; 0.01).
</bodyText>
<sectionHeader confidence="0.993759" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999789857142857">
We have proposed a general-purpose Naive Bayes
model for measuring association between two sets
of random events. The model replaced string
matching in the Lesk algorithm for word sense dis-
ambiguation with a probabilistic measure of gloss-
context overlap. The base model on average more
than doubled the accuracy of Lesk in Senseval-2
on both fine- and coarse-grained tracks. With ad-
ditional lexical knowledge, the model also outper-
formed state of the art results with statistical sig-
nificance on two coarse-grained WSD tasks.
For future work, we plan to apply the model
in other shared tasks, including open-text WSD,
so as to compare with more recent Lesk variants.
We would also like to explore how to incorpo-
rate syntactic features and employ alternative sta-
tistical methods (e.g., parametric models) to im-
prove probability estimation and inference. Other
NLP problems involving compositionality in gen-
eral might also benefit from the proposed many-
to-many similarity measure.
</bodyText>
<sectionHeader confidence="0.998292" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989573833333333">
This study is funded by the Natural Sciences and
Engineering Research Council of Canada. We
thank Afsaneh Fazly, Navdeep Jaitly, and Varada
Kolhatkar for the many inspiring discussions, as
well as the anonymous reviewers for their con-
structive advice.
</bodyText>
<page confidence="0.997523">
535
</page>
<sectionHeader confidence="0.982311" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972800552">
Khaled Abdalgader and Andrew Skabar. Unsupervised
similarity-based word sense disambiguation using context
vectors and sentential word importance. ACM Transac-
tions on Speech and Language Processing, 9(1):2:1–2:21,
May 2012.
Satanjeev Banerjee and Ted Pedersen. An adapted Lesk al-
gorithm for word sense disambiguation using WordNet.
In Computational Linguistics and Intelligent Text Process-
ing, pages 136–145. Springer, 2002.
Satanjeev Banerjee and Ted Pedersen. Extended gloss over-
laps as a measure of semantic relatedness. In Proceedings
of the 18th International Joint Conference on Artificial In-
telligence, volume 3, pages 805–810, 2003.
Alexander Budanitsky and Graeme Hirst. Evaluating
WordNet-based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13–47, 2006.
Ping Chen, Wei Ding, Chris Bowes, and David Brown. A
fully unsupervised word sense disambiguation method us-
ing dependency knowledge. In Proceedings of Human
Language Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for Com-
putational Linguistics, pages 28–36, Stroudsburg, PA,
USA, 2009.
Kenneth Ward Church and Patrick Hanks. Word association
norms, mutual information, and lexicography. Computa-
tional Linguistics, 16(1):22–29, 1990.
Philip Edmonds and Scott Cotton. Senseval-2: Overview. In
Proceedings of the 2nd International Workshop on Eval-
uating Word Sense Disambiguation Systems, pages 1–5.
Association for Computational Linguistics, 2001.
Christiane Fellbaum. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA, 1998.
David Fern´andez-Amor´os, Julio Gonzalo, and Felisa Verdejo.
The UNED systems at Senseval-2. In The Proceedings of
the Second International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 75–78. Association
for Computational Linguistics, 2001.
Mark Alan Finlayson. Java libraries for accessing the Prince-
ton WordNet: Comparison and evaluation. In Proceed-
ings of the 7th Global Wordnet Conference, Tartu, Estonia,
2014.
William Gale, Kenneth Church, and David Yarowsky. A
method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26(5-6):415–439, 1992.
Florentina Hristea. Recent advances concerning the usage of
the Naive Bayes model in unsupervised word sense dis-
ambiguation. International Review on Computers &amp; Soft-
ware, 4(1), 2009.
Jay Jiang and David Conrath. Semantic similarity based on
corpus statistics and lexical taxonomy. Proceedings of
International Conference on Research in Computational
Linguistics, 1997.
Adam Kilgarriff and Joseph Rosenzweig. Framework and
results for English Senseval. Computers and the Humani-
ties, 34(1-2):15–48, 2000.
Michael Lesk. Automatic sense disambiguation using ma-
chine readable dictionaries: how to tell a pine cone from
an ice cream cone. In Proceedings of the 5th Annual In-
ternational Conference on Systems Documentation, pages
24–26, New York, New York, USA, 1986.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O’Shea, and Keeley Crockett. Sentence similarity based
on semantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138–1150,
2006.
Kenneth C. Litkowski. Sense information for disambigua-
tion: Confluence of supervised and unsupervised methods.
In Proceedings of the ACL-02 Workshop on Word Sense
Disambiguation: Recent Successes and Future Directions,
pages 47–53. Association for Computational Linguistics,
July 2002.
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. PageRank on
semantic networks, with application to word sense disam-
biguation. In Proceedings of the 20th International Con-
ference on Computational Linguistics, 2004.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna
Gurevych. Using distributional similarity for lexical ex-
pansion in knowledge-based word sense disambiguation.
In Proceedings of the 24th International Conference on
Computational Linguistics, pages 1781–1796, 2012.
Roberto Navigli. Word sense disambiguation: A survey.
ACM Computing Surveys, 41(2):10:1–10:69, 2009.
Siddharth Patwardhan and Ted Pedersen. Using WordNet-
based context vectors to estimate the semantic relatedness
of concepts. Proceedings of the EACL 2006 Workshop
Making Sense of Sense-Bringing Computational Linguis-
tics and Psycholinguistics Together, 1501:1–8, 2006.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the 4th Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 241–257, 2003.
Ted Pedersen. A simple approach to building ensembles of
Naive Bayesian classifiers for word sense disambiguation.
In Proceedings of the 1st Conference of North American
Chapter of the Association for Computational Linguistics,
pages 63–69. Association for Computational Linguistics,
2000.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. SemEval-2007 task 17: English lexical
sample, SRL and all words. In Proceedings of the 4th
International Workshop on Semantic Evaluations, pages
87–92. Association for Computational Linguistics, 2007.
Ariel Raviv, Shaul Markovitch, and Sotirios-Efstathios
Maneas. Concept-based approach to word sense disam-
biguation. In Proceedings of the 26th Conference on Arti-
ficial Intelligence, 2012.
Philip Resnik. Using information content to evaluate seman-
tic similarity in a taxonomy. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence -
Volume 1, IJCAI’95, pages 448–453, San Francisco, CA,
USA, 1995.
David Tugwell and Adam Kilgarriff. Wasp-bench: a lex-
icographic tool supporting word sense disambiguation.
In The Proceedings of the Second International Work-
shop on Evaluating Word Sense Disambiguation Systems,
pages 151–154. Association for Computational Linguis-
tics, 2001.
Peter Turney. Mining the web for synonyms: PMI-IR versus
LSA on TOEFL. In Proceedings of the 12th European
Conference on Machine Learning, pages 491–502, 2001.
Florentina Vasilescu, Philippe Langlais, and Guy Lapalme.
Evaluating variants of the Lesk approach for disambiguat-
ing words. In Proceedings of the 4th International Con-
ference on Language Resources and Evaluation, 2004.
</reference>
<page confidence="0.984019">
536
</page>
<reference confidence="0.998245625">
Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E. McDon-
ald, Tony Plate, and Brian M. Slator. Providing machine
tractable dictionary tools. Machine Translation, 5(2):99–
154, 1990.
David Yarowsky. Unsupervised word sense disambiguation
rivaling supervised methods. In Proceedings of the 33rd
annual meeting on Association for Computational Lin-
guistics, pages 189–196, 1995.
</reference>
<page confidence="0.997384">
537
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989850">
<title confidence="0.997642">Applying a Naive Bayes Similarity Measure Word Sense Disambiguation</title>
<author confidence="0.999875">Tong Wang Graeme Hirst</author>
<affiliation confidence="0.999963">University of Toronto University of Toronto</affiliation>
<email confidence="0.999661">tong@cs.toronto.edugh@cs.toronto.edu</email>
<abstract confidence="0.999542692307692">We replace the overlap mechanism of the Lesk algorithm with a simple, generalpurpose Naive Bayes model that meabetween two sets of random variables. Even with simple probability estimates such as maximum likelihood, the model gains significant improvement over the Lesk algorithm on word sense disambiguation tasks. With additional lexical knowledge from Word- Net, performance is further improved to surpass the state-of-the-art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Khaled Abdalgader</author>
<author>Andrew Skabar</author>
</authors>
<title>Unsupervised similarity-based word sense disambiguation using context vectors and sentential word importance.</title>
<date>2012</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="1776" citStr="Abdalgader and Skabar, 2012" startWordPosition="268" endWordPosition="271">popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Baner ee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector space models (Abdalgader and Skabar, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD. 2 Related Work In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents proliferation of gloss comparison on larger contexts (Mihalce</context>
<context position="4776" citStr="Abdalgader and Skabar (2012)" startWordPosition="736" endWordPosition="739">econd order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet. Patwardhan et al. (2003) measured overlap by concept similarity (Budanitsky and Hirst, 2006) between each inventory sense and the context words. Gloss overlaps from their earlier work actually out-performed all five similarity-based methods. More recently, Chen et al. (2009) proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance. Abdalgader and Skabar (2012) constructed a sentential similarity measure (Li et al., 2006) using lexical similarity measures (Budanitsky and Hirst, 2006), and overlap was measured by the cosine of their respective sentential vectors. A related approach (Raviv et al., 2012) also used Wikipedia-induced concepts to encoded sentential vectors. These systems compared favourably to existing methods in WSD performance, although by using sense frequency information, they are essentially supervised methods. Distributional methods have been used in many WSD systems in quite different flavours than the current study. Kilgarriff and</context>
</contexts>
<marker>Abdalgader, Skabar, 2012</marker>
<rawString>Khaled Abdalgader and Andrew Skabar. Unsupervised similarity-based word sense disambiguation using context vectors and sentential word importance. ACM Transactions on Speech and Language Processing, 9(1):2:1–2:21, May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer,</publisher>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. An adapted Lesk algorithm for word sense disambiguation using WordNet. In Computational Linguistics and Intelligent Text Processing, pages 136–145. Springer, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>3</volume>
<pages>805--810</pages>
<contexts>
<context position="9738" citStr="Banerjee and Pedersen, 2003" startWordPosition="1547" endWordPosition="1550">y measure remains unchanged. The availability of LK largely depends on the sense inventory used in a WSD task. WordNet senses are often used in Senseval and SemEval tasks, and hence senses (or synsets, and possibly their corresponding word forms) that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies. As pointed out by Patwardhan et al. (2003), however, “not all of these relations are equally helpful.” Relation pairs involving hyponyms were shown to result in better F-measure when used in gloss overlaps (Banerjee and Pedersen, 2003). The authors attributed the phenomenon to the the multitude of hyponyms compared to other relations. We further hypothesize that, beyond sheer numbers, synonyms and hyponyms offer stronger semantic specification that helps distinguish the senses of a given ambiguous word, and thus are more effective knowledge sources for WSD. Take the word plant for example. Selected hypernyms, hyponyms, and synonyms pertaining to its two senses factory and life form are listed in Table 1. Hypernyms can be overly general terms (e.g., being). Although conceptually helpful for humans in coarse-grained WSD, this</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, volume 3, pages 805–810, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="4433" citStr="Budanitsky and Hirst, 2006" startWordPosition="688" endWordPosition="691"> Linguistics (Short Papers), pages 531–537, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics higher scores to multi-word overlaps. Breaking away from string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet. Patwardhan et al. (2003) measured overlap by concept similarity (Budanitsky and Hirst, 2006) between each inventory sense and the context words. Gloss overlaps from their earlier work actually out-performed all five similarity-based methods. More recently, Chen et al. (2009) proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance. Abdalgader and Skabar (2012) constructed a sentential similarity measure (Li et al., 2006) using lexical similarity measures (Budanitsky and Hirst, 2006), and overlap was measured by the cosine of their respective sentential vectors. A related approach (Raviv et al., 2012) also used W</context>
<context position="11868" citStr="Budanitsky and Hirst, 2006" startWordPosition="1882" endWordPosition="1885">er in WSD tasks, with hyponyms particularly promising in both quantity and quality compared to hypernyms and synonyms.3 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log c(fj) +∑i∑log c(ei, fj) − |f|∑log c(ei) i c(·) j c(fj) j c(·) =(1− |e|)∑logc(f j) − |f|∑ logc(ei) i j logc(ei, fj)+ |f|(|e |−1)logc(·), where c(x) is the count of word x, c(·) is the corpus 2We do, however, refer curious readers to the work of Raviv et al. (2012) for a novel treatment of a similar problem. 3Note that LK expansion is a feature of our model ra</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Chen</author>
<author>Wei Ding</author>
<author>Chris Bowes</author>
<author>David Brown</author>
</authors>
<title>A fully unsupervised word sense disambiguation method using dependency knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1723" citStr="Chen et al., 2009" startWordPosition="260" endWordPosition="263">g, 2000; Vasilescu et al., 2004). Among the popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Baner ee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector space models (Abdalgader and Skabar, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD. 2 Related Work In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents prolifer</context>
<context position="4616" citStr="Chen et al. (2009)" startWordPosition="715" endWordPosition="718"> string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet. Patwardhan et al. (2003) measured overlap by concept similarity (Budanitsky and Hirst, 2006) between each inventory sense and the context words. Gloss overlaps from their earlier work actually out-performed all five similarity-based methods. More recently, Chen et al. (2009) proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance. Abdalgader and Skabar (2012) constructed a sentential similarity measure (Li et al., 2006) using lexical similarity measures (Budanitsky and Hirst, 2006), and overlap was measured by the cosine of their respective sentential vectors. A related approach (Raviv et al., 2012) also used Wikipedia-induced concepts to encoded sentential vectors. These systems compared favourably to existing methods in WSD performance, although by using sense frequency information, they </context>
</contexts>
<marker>Chen, Ding, Bowes, Brown, 2009</marker>
<rawString>Ping Chen, Wei Ding, Chris Bowes, and David Brown. A fully unsupervised word sense disambiguation method using dependency knowledge. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28–36, Stroudsburg, PA, USA, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="11717" citStr="Church and Hanks, 1990" startWordPosition="1857" endWordPosition="1860">general phenomenon in gloss-based WSD and is beyond the scope of the current discussion.2 Overall, all three sources of LK may complement each other in WSD tasks, with hyponyms particularly promising in both quantity and quality compared to hypernyms and synonyms.3 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log c(fj) +∑i∑log c(ei, fj) − |f|∑log c(ei) i c(·) j c(fj) j c(·) =(1− |e|)∑logc(f j) − |f|∑ logc(ei) i j logc(ei, fj)+ |f|(|e |−1)logc(·), where c(x) is the count of word x, c(·) is the corpus 2We do, however, re</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Scott Cotton</author>
</authors>
<title>Senseval-2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>pages</pages>
<contexts>
<context position="13420" citStr="Edmonds and Cotton, 2001" startWordPosition="2155" endWordPosition="2158">Specifically in WSD, a source corpus is defined as the source of the majority of the WSD instances in a given dataset, and a baseline corpus of a smaller size and less resemblance to the instances is used for all datasets. The assumption is that a source corpus offers better estimates for the model than the baseline corpus, and difference in model performance is expected when using probability estimation of different quality. 4 Evaluation 4.1 Data, Scoring, and Pre-processing Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval-2 (Edmonds and Cotton, 2001) and SemEval-2007 (Pradhan et al., 2007). Training sections are used as development data and test sections held out for final testing. Model performance is evaluated in terms of WSD accuracy using Equation (2) as the scoring function. Accuracy is defined as the number of correct responses over the number of instances. Because it is a rare event for the NBM to produce identical scores,4 the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. Multiword expressions (MWEs) in the Senseval-2 sense inventory are not explicitly marked in</context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>Philip Edmonds and Scott Cotton. Senseval-2: Overview. In Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems, pages 1–5. Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="3234" citStr="Fellbaum, 1998" startWordPosition="509" endWordPosition="510">e in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. Hristea (2009) trained an unsupervised NB classifier using the EM algorithm and empirically demonstrated the benefits of WordNet-assisted (Fellbaum, 1998) feature selection over local syntactic features. Among Lesk variants, Baner ee and Pedersen (2002) extended the gloss of both inventory senses and the context words to include words in their related synsets in WordNet. Senses were scored by the sum of overlaps across all relation pairs, and the effect of individual relation pairs was evaluated in a later work (Baner ee and Pedersen, 2003). Overlap was assessed by string matching, with the number of matching words squared so as to assign 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers),</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Fern´andez-Amor´os</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>The UNED systems at Senseval-2.</title>
<date>2001</date>
<booktitle>In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>75--78</pages>
<marker>Fern´andez-Amor´os, Gonzalo, Verdejo, 2001</marker>
<rawString>David Fern´andez-Amor´os, Julio Gonzalo, and Felisa Verdejo. The UNED systems at Senseval-2. In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 75–78. Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Alan Finlayson</author>
</authors>
<title>Java libraries for accessing the Princeton WordNet: Comparison and evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th Global Wordnet Conference,</booktitle>
<location>Tartu, Estonia,</location>
<contexts>
<context position="15019" citStr="Finlayson, 2014" startWordPosition="2406" endWordPosition="2407">ll MWE-related answers, while the other assuming perfect MWE detection and performing regular disambiguation algorithm on the MWE-related senses (not defaulting to CORRECT). All results reported for Senseval-2 below are harmonic means of the two outcomes. Each inventory sense is represented by a set of LK tokens (e.g., definition texts, synonyms, etc.) 4This has never occurred in the hundreds of thousands of runs in our development process. from their corresponding WordNet synset (or in the coarse-grained case, a concatenation of tokens from all synsets in a sense group). The MIT-JWI library (Finlayson, 2014) is used for accessing WordNet. Usage examples in glosses (included by the library by default) are removed in our experiments.5 Basic pre-processing is performed on the contexts and the glosses, including lower-casing, stopword removal, lemmatization on both datasets, and tokenization on the Senseval-2 instances.6 Stanford CoreNLP7 is used for lemmatization and tokenization. Identical procedures are applied to all corpora used for probability estimation. Binomial test is used for significance testing, and with one exception explicitly noted in Section 4.3, all differences presented are statist</context>
</contexts>
<marker>Finlayson, 2014</marker>
<rawString>Mark Alan Finlayson. Java libraries for accessing the Princeton WordNet: Comparison and evaluation. In Proceedings of the 7th Global Wordnet Conference, Tartu, Estonia, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--5</pages>
<contexts>
<context position="2654" citStr="Gale et al. (1992)" startWordPosition="415" endWordPosition="418">D, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents proliferation of gloss comparison on larger contexts (Mihalcea et al., 2004) and is shown to outperform the original Lesk algorithm (Vasilescu et al., 2004). To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD — that is, in contrast to their use as a similarity measure in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. Hristea (2009) trained an unsupervised NB classifier using the EM algorithm and empirically demonstrated the benefits of WordNet-assisted (Fellbaum, 1998) feature selection o</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26(5-6):415–439, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florentina Hristea</author>
</authors>
<title>Recent advances concerning the usage of the Naive Bayes model in unsupervised word sense disambiguation.</title>
<date>2009</date>
<journal>International Review on Computers &amp; Software,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="3094" citStr="Hristea (2009)" startWordPosition="491" endWordPosition="492">best of our knowledge, NBMs have been employed exclusively as classifiers in WSD — that is, in contrast to their use as a similarity measure in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. Hristea (2009) trained an unsupervised NB classifier using the EM algorithm and empirically demonstrated the benefits of WordNet-assisted (Fellbaum, 1998) feature selection over local syntactic features. Among Lesk variants, Baner ee and Pedersen (2002) extended the gloss of both inventory senses and the context words to include words in their related synsets in WordNet. Senses were scored by the sum of overlaps across all relation pairs, and the effect of individual relation pairs was evaluated in a later work (Baner ee and Pedersen, 2003). Overlap was assessed by string matching, with the number of matchi</context>
</contexts>
<marker>Hristea, 2009</marker>
<rawString>Florentina Hristea. Recent advances concerning the usage of the Naive Bayes model in unsupervised word sense disambiguation. International Review on Computers &amp; Software, 4(1), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>Proceedings of International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="11839" citStr="Jiang and Conrath, 1997" startWordPosition="1878" endWordPosition="1881">K may complement each other in WSD tasks, with hyponyms particularly promising in both quantity and quality compared to hypernyms and synonyms.3 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log c(fj) +∑i∑log c(ei, fj) − |f|∑log c(ei) i c(·) j c(fj) j c(·) =(1− |e|)∑logc(f j) − |f|∑ logc(ei) i j logc(ei, fj)+ |f|(|e |−1)logc(·), where c(x) is the count of word x, c(·) is the corpus 2We do, however, refer curious readers to the work of Raviv et al. (2012) for a novel treatment of a similar problem. 3Note that LK expansion</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath. Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of International Conference on Research in Computational Linguistics, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for English Senseval. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="1112" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="160" endWordPosition="163">Lesk algorithm on word sense disambiguation tasks. With additional lexical knowledge from WordNet, performance is further improved to surpass the state-of-the-art results. 1 Introduction To disambiguate a homonymous word in a given context, Lesk (1986) proposed a method that measured the degree of overlap between the glosses of the target and context words. Known as the Lesk algorithm, this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation (WSD) community. Nonetheless, its performance in several WSD benchmarks is less than satisfactory (Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004). Among the popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Baner ee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et</context>
<context position="5394" citStr="Kilgarriff and Rosenzweig (2000)" startWordPosition="828" endWordPosition="831"> Skabar (2012) constructed a sentential similarity measure (Li et al., 2006) using lexical similarity measures (Budanitsky and Hirst, 2006), and overlap was measured by the cosine of their respective sentential vectors. A related approach (Raviv et al., 2012) also used Wikipedia-induced concepts to encoded sentential vectors. These systems compared favourably to existing methods in WSD performance, although by using sense frequency information, they are essentially supervised methods. Distributional methods have been used in many WSD systems in quite different flavours than the current study. Kilgarriff and Rosenzweig (2000) proposed a Lesk variant where each gloss word is weighted by its idf score in relation to all glosses, and gloss-context association was incremented by these weights rather than binary, overlap counts. Miller et al. (2012) used distributional thesauri as a knowledge base to increase overlaps, which were, again, assessed by string matching. In conclusion, the majority of Lesk variants focused on extending the gloss to increase the chance of overlapping, while the proposed NBM aims to make better use of the limited lexical knowledge available. In contrast to string matching, the probabilistic n</context>
<context position="7595" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="1199" endWordPosition="1203"> context of WSD, e can be regarded as an instance of a polysemous word w, while f represents certain lexical knowledge about the sense s of w manifested by e.1 WSD is thus formulated as identifying the sense s∗ in the sense inventory Y of w s.t.: s∗ = argmax p(f|e) (2) s∈S In one of their simplest forms, ei’s correspond to co-occurring words in the instance of w, and fj’s consist of the gloss words of sense s. Consequently, p(f|e) is essentially measuring the association between context words of w and definition texts of s, i.e., the gloss-context association in the simplified Lesk algorithm (Kilgarriff and Rosenzweig, 2000). A major difference, however, is that instead of using hard, overlap counts between the two sets of words from the gloss and the context, this probabilistic treatment can implicitly model the distributional similarity among the elements ei and fj (and consequently between the sets e and f) over a wider range of contexts. The result is a “softer” proxy of association than the binary view of overlaps in existing Lesk variants. The foregoing discussion offers a second motivation for applying Bayes’s rule on the second 1Think of the notations e and f mnemonically as exemplars and features, respec</context>
<context position="14149" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="2271" endWordPosition="2274">sections held out for final testing. Model performance is evaluated in terms of WSD accuracy using Equation (2) as the scoring function. Accuracy is defined as the number of correct responses over the number of instances. Because it is a rare event for the NBM to produce identical scores,4 the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. Multiword expressions (MWEs) in the Senseval-2 sense inventory are not explicitly marked in the contexts. Several of the top-ranking systems implemented their own MWE detection algorithms (Kilgarriff and Rosenzweig, 2000; Litkowski, 2002). Without digressing to the details of MWE detection — and meanwhile, to ensure fair comparison with existing systems — we implement two variants of the prediction module, one completely ignorant of MWE and defaulting to INCORRECT for all MWE-related answers, while the other assuming perfect MWE detection and performing regular disambiguation algorithm on the MWE-related senses (not defaulting to CORRECT). All results reported for Senseval-2 below are harmonic means of the two outcomes. Each inventory sense is represented by a set of LK tokens (e.g., definition texts, synonym</context>
<context position="16787" citStr="Kilgarriff and Rosenzweig (2000)" startWordPosition="2673" endWordPosition="2676">ethods in each dataset).8 By using only glosses, the proposed model already shows statistically significant improvement over the basic Lesk algorithm (92.4% and 140.5% relative improvement in Senseval2 coarse- and fine-grained tracks, respectively).9 Moreover, comparison between coarse- and finegrained tracks reveals interesting properties of different LK sources. Previous hypotheses (Section 3.3) are empirically confirmed that WSD perfor5We also compared the two Lesk baselines (with and without usage examples) on the development data but did not observe significant differences as reported by Kilgarriff and Rosenzweig (2000). 6The SemEval-2007 instances are already tokenized. 7http://nlp.stanford.edu/software/ corenlp.shtml. 8We excluded the results of UNED (Fern´andez-Amor´os et al., 2001) in Senseval-2 because, by using sense frequency information that is only obtainable from sense-annotated corpora, it is essentially a supervised system. 9Comparisons are made against the simplified Lesk algorithm (Kilgarriff and Rosenzweig, 2000) without usage examples. The comparison is unavailable in SemEval2007 since we have not found existing experiments with this exact configuration. 534 Table 2: Lexical knowledge sources</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. Framework and results for English Senseval. Computers and the Humanities, 34(1-2):15–48, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference on Systems Documentation,</booktitle>
<pages>24--26</pages>
<location>New York, New York, USA,</location>
<contexts>
<context position="733" citStr="Lesk (1986)" startWordPosition="103" endWordPosition="104">sity of Toronto tong@cs.toronto.edu gh@cs.toronto.edu Abstract We replace the overlap mechanism of the Lesk algorithm with a simple, generalpurpose Naive Bayes model that measures many-to-many association between two sets of random variables. Even with simple probability estimates such as maximum likelihood, the model gains significant improvement over the Lesk algorithm on word sense disambiguation tasks. With additional lexical knowledge from WordNet, performance is further improved to surpass the state-of-the-art results. 1 Introduction To disambiguate a homonymous word in a given context, Lesk (1986) proposed a method that measured the degree of overlap between the glosses of the target and context words. Known as the Lesk algorithm, this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation (WSD) community. Nonetheless, its performance in several WSD benchmarks is less than satisfactory (Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004). Among the popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically c</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation, pages 24–26, New York, New York, USA, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A Bandar, James D O’Shea, and Keeley Crockett. Sentence similarity based on semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138–1150, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
</authors>
<title>Sense information for disambiguation: Confluence of supervised and unsupervised methods.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>47--53</pages>
<contexts>
<context position="14167" citStr="Litkowski, 2002" startWordPosition="2275" endWordPosition="2276">ng. Model performance is evaluated in terms of WSD accuracy using Equation (2) as the scoring function. Accuracy is defined as the number of correct responses over the number of instances. Because it is a rare event for the NBM to produce identical scores,4 the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. Multiword expressions (MWEs) in the Senseval-2 sense inventory are not explicitly marked in the contexts. Several of the top-ranking systems implemented their own MWE detection algorithms (Kilgarriff and Rosenzweig, 2000; Litkowski, 2002). Without digressing to the details of MWE detection — and meanwhile, to ensure fair comparison with existing systems — we implement two variants of the prediction module, one completely ignorant of MWE and defaulting to INCORRECT for all MWE-related answers, while the other assuming perfect MWE detection and performing regular disambiguation algorithm on the MWE-related senses (not defaulting to CORRECT). All results reported for Senseval-2 below are harmonic means of the two outcomes. Each inventory sense is represented by a set of LK tokens (e.g., definition texts, synonyms, etc.) 4This has</context>
</contexts>
<marker>Litkowski, 2002</marker>
<rawString>Kenneth C. Litkowski. Sense information for disambiguation: Confluence of supervised and unsupervised methods. In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 47–53. Association for Computational Linguistics, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
<author>Elizabeth Figa</author>
</authors>
<title>PageRank on semantic networks, with application to word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="2391" citStr="Mihalcea et al., 2004" startWordPosition="366" endWordPosition="369">r, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD. 2 Related Work In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents proliferation of gloss comparison on larger contexts (Mihalcea et al., 2004) and is shown to outperform the original Lesk algorithm (Vasilescu et al., 2004). To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD — that is, in contrast to their use as a similarity measure in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) p</context>
</contexts>
<marker>Mihalcea, Tarau, Figa, 2004</marker>
<rawString>Rada Mihalcea, Paul Tarau, and Elizabeth Figa. PageRank on semantic networks, with application to word sense disambiguation. In Proceedings of the 20th International Conference on Computational Linguistics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
</authors>
<title>Torsten Zesch, and Iryna Gurevych. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>1781--1796</pages>
<marker>Miller, Biemann, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1781–1796, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1368" citStr="Navigli, 2009" startWordPosition="204" endWordPosition="205">at measured the degree of overlap between the glosses of the target and context words. Known as the Lesk algorithm, this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation (WSD) community. Nonetheless, its performance in several WSD benchmarks is less than satisfactory (Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004). Among the popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Baner ee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector space models (Abdalgader and Skabar, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-ba</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2):10:1–10:69, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNetbased context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>Proceedings of the EACL 2006 Workshop Making Sense of Sense-Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1501--1</pages>
<contexts>
<context position="1828" citStr="Patwardhan and Pedersen, 2006" startWordPosition="276" endWordPosition="279">lgorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Baner ee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector space models (Abdalgader and Skabar, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD. 2 Related Work In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents proliferation of gloss comparison on larger contexts (Mihalcea et al., 2004) and is shown to outperform the origi</context>
<context position="4243" citStr="Patwardhan and Pedersen (2006)" startWordPosition="662" endWordPosition="665">dersen, 2003). Overlap was assessed by string matching, with the number of matching words squared so as to assign 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531–537, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics higher scores to multi-word overlaps. Breaking away from string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet. Patwardhan et al. (2003) measured overlap by concept similarity (Budanitsky and Hirst, 2006) between each inventory sense and the context words. Gloss overlaps from their earlier work actually out-performed all five similarity-based methods. More recently, Chen et al. (2009) proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance. Abdalgader and Skabar (2012) constructed a sentential similarity measure (Li et al., 2006) usin</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. Using WordNetbased context vectors to estimate the semantic relatedness of concepts. Proceedings of the EACL 2006 Workshop Making Sense of Sense-Bringing Computational Linguistics and Psycholinguistics Together, 1501:1–8, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<contexts>
<context position="4365" citStr="Patwardhan et al. (2003)" startWordPosition="679" endWordPosition="682">s of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531–537, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics higher scores to multi-word overlaps. Breaking away from string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet. Patwardhan et al. (2003) measured overlap by concept similarity (Budanitsky and Hirst, 2006) between each inventory sense and the context words. Gloss overlaps from their earlier work actually out-performed all five similarity-based methods. More recently, Chen et al. (2009) proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance. Abdalgader and Skabar (2012) constructed a sentential similarity measure (Li et al., 2006) using lexical similarity measures (Budanitsky and Hirst, 2006), and overlap was measured by the cosine of their respective sen</context>
<context position="9545" citStr="Patwardhan et al. (2003)" startWordPosition="1518" endWordPosition="1521">forward to incorporate various forms of lexical knowledge (LK) for word senses: by concatenating a tokenized knowledge source to the existing knowledge representation f, while the similarity measure remains unchanged. The availability of LK largely depends on the sense inventory used in a WSD task. WordNet senses are often used in Senseval and SemEval tasks, and hence senses (or synsets, and possibly their corresponding word forms) that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies. As pointed out by Patwardhan et al. (2003), however, “not all of these relations are equally helpful.” Relation pairs involving hyponyms were shown to result in better F-measure when used in gloss overlaps (Banerjee and Pedersen, 2003). The authors attributed the phenomenon to the the multitude of hyponyms compared to other relations. We further hypothesize that, beyond sheer numbers, synonyms and hyponyms offer stronger semantic specification that helps distinguish the senses of a given ambiguous word, and thus are more effective knowledge sources for WSD. Take the word plant for example. Selected hypernyms, hyponyms, and synonyms pe</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the 4th International Conference on Intelligent Text Processing and Computational Linguistics, pages 241–257, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of Naive Bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="2989" citStr="Pedersen (2000)" startWordPosition="474" endWordPosition="475">cea et al., 2004) and is shown to outperform the original Lesk algorithm (Vasilescu et al., 2004). To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD — that is, in contrast to their use as a similarity measure in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. Hristea (2009) trained an unsupervised NB classifier using the EM algorithm and empirically demonstrated the benefits of WordNet-assisted (Fellbaum, 1998) feature selection over local syntactic features. Among Lesk variants, Baner ee and Pedersen (2002) extended the gloss of both inventory senses and the context words to include words in their related synsets in WordNet. Senses were scored by the sum of overlaps across all relation pairs, and the effect of individual relation pairs was evaluated in a lat</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. A simple approach to building ensembles of Naive Bayesian classifiers for word sense disambiguation. In Proceedings of the 1st Conference of North American Chapter of the Association for Computational Linguistics, pages 63–69. Association for Computational Linguistics, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval-2007 task 17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>87--92</pages>
<contexts>
<context position="13460" citStr="Pradhan et al., 2007" startWordPosition="2161" endWordPosition="2164">ned as the source of the majority of the WSD instances in a given dataset, and a baseline corpus of a smaller size and less resemblance to the instances is used for all datasets. The assumption is that a source corpus offers better estimates for the model than the baseline corpus, and difference in model performance is expected when using probability estimation of different quality. 4 Evaluation 4.1 Data, Scoring, and Pre-processing Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval-2 (Edmonds and Cotton, 2001) and SemEval-2007 (Pradhan et al., 2007). Training sections are used as development data and test sections held out for final testing. Model performance is evaluated in terms of WSD accuracy using Equation (2) as the scoring function. Accuracy is defined as the number of correct responses over the number of instances. Because it is a rare event for the NBM to produce identical scores,4 the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. Multiword expressions (MWEs) in the Senseval-2 sense inventory are not explicitly marked in the contexts. Several of the top-rankin</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. SemEval-2007 task 17: English lexical sample, SRL and all words. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 87–92. Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Raviv</author>
</authors>
<title>Shaul Markovitch, and Sotirios-Efstathios Maneas. Concept-based approach to word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Conference on Artificial Intelligence,</booktitle>
<marker>Raviv, 2012</marker>
<rawString>Ariel Raviv, Shaul Markovitch, and Sotirios-Efstathios Maneas. Concept-based approach to word sense disambiguation. In Proceedings of the 26th Conference on Artificial Intelligence, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence -Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="11814" citStr="Resnik, 1995" startWordPosition="1876" endWordPosition="1877">e sources of LK may complement each other in WSD tasks, with hyponyms particularly promising in both quantity and quality compared to hypernyms and synonyms.3 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log c(fj) +∑i∑log c(ei, fj) − |f|∑log c(ei) i c(·) j c(fj) j c(·) =(1− |e|)∑logc(f j) − |f|∑ logc(ei) i j logc(ei, fj)+ |f|(|e |−1)logc(·), where c(x) is the count of word x, c(·) is the corpus 2We do, however, refer curious readers to the work of Raviv et al. (2012) for a novel treatment of a similar problem</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence -Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Tugwell</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Wasp-bench: a lexicographic tool supporting word sense disambiguation.</title>
<date>2001</date>
<booktitle>In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>151--154</pages>
<contexts>
<context position="18771" citStr="Tugwell and Kilgarriff (2001)" startWordPosition="2963" endWordPosition="2966">ality of hypernyms (both semantic and distributional), we postulate that their probability in the NBM is uniformly inflated among many sense candidates, and hence they decrease in distinguishability. Synonyms might help with regard to semantic specification, though their limited quantity also limits their benefits. These patterns on the LK types are consistent in all three experiments. When including all four LK sources, our model outperforms the state-of-the-art systems with statistical significance in both coarse-grained tasks. For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. 4.3 Probability Estimation To evaluate model response to probability estimation of different quality (Section 3.4), source corpora are chosen as the majority value of the doc-source attribute of instances in each dataset, namely, the British National Corpus for Senseval2 (94%) and the Wall Street Journal for SemEval2007 (86%). The Brown Corpus is shared by both datasets as the baseline corpus. Fi</context>
</contexts>
<marker>Tugwell, Kilgarriff, 2001</marker>
<rawString>David Tugwell and Adam Kilgarriff. Wasp-bench: a lexicographic tool supporting word sense disambiguation. In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 151–154. Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="11732" citStr="Turney, 2001" startWordPosition="1861" endWordPosition="1862">oss-based WSD and is beyond the scope of the current discussion.2 Overall, all three sources of LK may complement each other in WSD tasks, with hyponyms particularly promising in both quantity and quality compared to hypernyms and synonyms.3 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log c(fj) +∑i∑log c(ei, fj) − |f|∑log c(ei) i c(·) j c(fj) j c(·) =(1− |e|)∑logc(f j) − |f|∑ logc(ei) i j logc(ei, fj)+ |f|(|e |−1)logc(·), where c(x) is the count of word x, c(·) is the corpus 2We do, however, refer curious rea</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the 12th European Conference on Machine Learning, pages 491–502, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florentina Vasilescu</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>Evaluating variants of the Lesk approach for disambiguating words.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<contexts>
<context position="1137" citStr="Vasilescu et al., 2004" startWordPosition="164" endWordPosition="167">mbiguation tasks. With additional lexical knowledge from WordNet, performance is further improved to surpass the state-of-the-art results. 1 Introduction To disambiguate a homonymous word in a given context, Lesk (1986) proposed a method that measured the degree of overlap between the glosses of the target and context words. Known as the Lesk algorithm, this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation (WSD) community. Nonetheless, its performance in several WSD benchmarks is less than satisfactory (Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004). Among the popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Baner ee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector sp</context>
<context position="2471" citStr="Vasilescu et al., 2004" startWordPosition="380" endWordPosition="383">mitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD. 2 Related Work In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents proliferation of gloss comparison on larger contexts (Mihalcea et al., 2004) and is shown to outperform the original Lesk algorithm (Vasilescu et al., 2004). To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD — that is, in contrast to their use as a similarity measure in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context wind</context>
</contexts>
<marker>Vasilescu, Langlais, Lapalme, 2004</marker>
<rawString>Florentina Vasilescu, Philippe Langlais, and Guy Lapalme. Evaluating variants of the Lesk approach for disambiguating words. In Proceedings of the 4th International Conference on Language Resources and Evaluation, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Dan Fass</author>
<author>Cheng-Ming Guo</author>
<author>James E McDonald</author>
<author>Tony Plate</author>
<author>Brian M Slator</author>
</authors>
<title>Providing machine tractable dictionary tools.</title>
<date>1990</date>
<journal>Machine Translation,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="4035" citStr="Wilks et al. (1990)" startWordPosition="632" endWordPosition="635"> in their related synsets in WordNet. Senses were scored by the sum of overlaps across all relation pairs, and the effect of individual relation pairs was evaluated in a later work (Baner ee and Pedersen, 2003). Overlap was assessed by string matching, with the number of matching words squared so as to assign 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531–537, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics higher scores to multi-word overlaps. Breaking away from string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet. Patwardhan et al. (2003) measured overlap by concept similarity (Budanitsky and Hirst, 2006) between each inventory sense and the context words. Gloss overlaps from their earlier work actually out-performed all five similarity-based methods. More recently, Chen et al. (2009) proposed a tree-ma</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1990</marker>
<rawString>Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E. McDonald, Tony Plate, and Brian M. Slator. Providing machine tractable dictionary tools. Machine Translation, 5(2):99– 154, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="18816" citStr="Yarowsky, 1995" startWordPosition="2972" endWordPosition="2973">ostulate that their probability in the NBM is uniformly inflated among many sense candidates, and hence they decrease in distinguishability. Synonyms might help with regard to semantic specification, though their limited quantity also limits their benefits. These patterns on the LK types are consistent in all three experiments. When including all four LK sources, our model outperforms the state-of-the-art systems with statistical significance in both coarse-grained tasks. For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. 4.3 Probability Estimation To evaluate model response to probability estimation of different quality (Section 3.4), source corpora are chosen as the majority value of the doc-source attribute of instances in each dataset, namely, the British National Corpus for Senseval2 (94%) and the Wall Street Journal for SemEval2007 (86%). The Brown Corpus is shared by both datasets as the baseline corpus. Figure 1 shows the comparison on the SemEval-20</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189–196, 1995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>