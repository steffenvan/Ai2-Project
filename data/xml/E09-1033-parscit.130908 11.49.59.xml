<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.977742">
Rich bitext projection features for parse reranking
</title>
<author confidence="0.989807">
Alexander Fraser Renjing Wang Hinrich Sch¨utze
</author>
<affiliation confidence="0.994173">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.991386">
{fraser,wangrg}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.99461" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880538461538">
Many different types of features have
been shown to improve accuracy in parse
reranking. A class of features that thus far
has not been considered is based on a pro-
jection of the syntactic structure of a trans-
lation of the text to be parsed. The intu-
ition for using this type of bitext projec-
tion feature is that ambiguous structures
in one language often correspond to un-
ambiguous structures in another. We show
that reranking based on bitext projection
features increases parsing accuracy signif-
icantly.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966416666666">
Parallel text or bitext is an important knowledge
source for solving many problems such as ma-
chine translation, cross-language information re-
trieval, and the projection of linguistic resources
from one language to another. In this paper, we
show that bitext-based features are effective in ad-
dressing another NLP problem, increasing the ac-
curacy of statistical parsing. We pursue this ap-
proach for a number of reasons. First, one lim-
iting factor for syntactic approaches to statistical
machine translation is parse quality (Quirk and
Corston-Oliver, 2006). Improved parses of bi-
text should result in improved machine translation.
Second, as more and more texts are available in
several languages, it will be increasingly the case
that a text to be parsed is itself part of a bitext.
Third, we hope that the improved parses of bitext
will serve as higher quality training data for im-
proving monolingual parsing using a process sim-
ilar to self-training (McClosky et al., 2006).
It is well known that different languages encode
different types of grammatical information (agree-
ment, case, tense etc.) and that what can be left
unspecified in one language must be made explicit
</bodyText>
<figure confidence="0.8953705">
NP
NP SBAR
</figure>
<figureCaption confidence="0.999756">
Figure 1: English parse with high attachment
</figureCaption>
<bodyText confidence="0.99995975">
in another. This information can be used for syn-
tactic disambiguation. However, it is surprisingly
hard to do this well. We use parses and alignments
that are automatically generated and hence imper-
fect. German parse quality is considered to be
worse than English parse quality, and the annota-
tion style is different, e.g., NP structure in German
is flatter.
We conduct our research in the framework of
N-best parse reranking, but apply it to bitext and
add only features based on syntactic projection
from German to English. We test the idea that,
generally, English parses with more isomorphism
with respect to the projected German parse are bet-
ter. The system takes as input (i) English sen-
tences with a list of automatically generated syn-
tactic parses, (ii) a translation of the English sen-
tences into German, (iii) an automatically gen-
erated parse of the German translation, and (iv)
an automatically generated word alignment. We
achieve a significant improvement of 0.66 F1 (ab-
solute) on test data.
The paper is organized as follows. Section 2
outlines our approach and section 3 introduces the
model. Section 4 describes training and section 5
presents the data and experimental results. In sec-
tion 6, we discuss previous work. Section 7 ana-
lyzes our results and section 8 concludes.
</bodyText>
<figure confidence="0.997766916666667">
CC
NP
NP
DT
NN
and
DT
NN
who had gray hair
a
baby
a woman
</figure>
<note confidence="0.8601425">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 282–290,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.985299">
282
</page>
<figure confidence="0.9866805">
NP
a woman
</figure>
<figureCaption confidence="0.946964">
Figure 2: English parse with low attachment
</figureCaption>
<figure confidence="0.809005">
CNP
</figure>
<figureCaption confidence="0.997674">
Figure 3: German parse with low attachment
</figureCaption>
<sectionHeader confidence="0.992991" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.986495538461539">
Consider the English sentence “He saw a baby and
a woman who had gray hair”. Suppose that the
baseline parser generates two parses, containing
the NPs shown in figures 1 and 2, respectively, and
that the semantically more plausible second parse
in figure 2 is correct. How can we determine that
the second parse should be favored? Since we are
parsing bitext, we can observe the German trans-
lation which is “Er sah ein Baby und eine Frau,
die graue Haare hatte” (glossed: “he saw a baby
and a woman, who gray hair had”). The singular
verb in the subordinate clause (“hatte”: “had”) in-
dicates that the subordinate S must be attached low
to “woman” (“Frau”) as shown in figure 3.
We follow Collins’ (2000) approach to discrim-
inative reranking (see also (Riezler et al., 2002)).
Given a new sentence to parse, we first select the
best N parse trees according to a generative model.
Then we use new features to learn discriminatively
how to rerank the parses in this N-best list. We
use features derived using projections of the 1-best
German parse onto the hypothesized English parse
under consideration.
In more detail, we take the 100 best English
parses from the BitPar parser (Schmid, 2004) and
rerank them. We have a good chance of finding the
optimal parse among the 100-best1. An automati-
cally generated word alignment determines trans-
lational correspondence between German and En-
glish. We use features which measure syntactic di-
1Using an oracle to select the best parse results in an Fl
of 95.90, an improvement of 8.01 absolute over the baseline.
vergence between the German and English trees to
try to rank the English trees which have less diver-
gence higher. Our test set is 3718 sentences from
the English Penn treebank (Marcus et al., 1993)
which were translated into German. We hold out
these sentences, and train BitPar on the remain-
ing Penn treebank training sentences. The average
Fi parsing accuracy of BitPar on this test set is
87.89%, which is our baseline2. We implement
features based on projecting the German parse to
each of the English 100-best parses in turn via the
word alignment. By performing cross-validation
and measuring test performance within each fold,
we compare our new system with the baseline on
the 3718 sentence set. The overall test accuracy
we reach is 88.55%, a statistically significant im-
provement over baseline of 0.66.
Given a word alignment of the bitext, the sys-
tem performs the following steps for each English
sentence to be parsed:
</bodyText>
<listItem confidence="0.968208111111111">
(i) run BitPar trained on English to generate 100-
best parses for the English sentence
(ii) run BitPar trained on German to generate the
1-best parse for the German sentence
(iii) calculate feature function values which mea-
sure different kinds of syntactic divergence
(iv) apply a model that combines the feature func-
tion values to score each of the 100-best parses
(v) pick the best parse according to the model
</listItem>
<sectionHeader confidence="0.978419" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.998143333333333">
We use a log-linear model to choose the best En-
glish parse. The feature functions are functions
on the hypothesized English parse e, the German
parse g, and the word alignment a, and they as-
sign a score (varying between 0 and infinity) that
measures syntactic divergence. The alignment of
a sentence pair is a function that, for each English
word, returns a set of German words that the En-
glish word is aligned with as shown here for the
sentence pair from section 2:
Er sah ein Baby und eine Frau , die graue Haare
hatte
</bodyText>
<equation confidence="0.955537">
He{1} saw{2} a{3} baby{4} and{5} a{6}
woman{7} who{9} had{12} gray{10} hair{11}
</equation>
<bodyText confidence="0.998731333333333">
Feature function values are calculated either by
taking the negative log of a probability, or by using
a heuristic function which scales in a similar fash-
</bodyText>
<footnote confidence="0.837328">
2The test set is very challenging, containing English sen-
tences of up to 99 tokens.
</footnote>
<figure confidence="0.998178814814815">
NP
NP
CC
NN
DT
DT
NN
baby
a
and
NP
SBAR
who had gray hair
NP KON NP
und
ART
NN
ART
ein
Baby
NN
,
S
die...
Frau
eine
,
</figure>
<page confidence="0.998539">
283
</page>
<bodyText confidence="0.99714875">
ion3. The form of the log-linear model is shown in
eq. 1. There are M feature functions h1, ... , hM.
The vector λ is used to control the contribution of
each feature function.
</bodyText>
<equation confidence="0.959645">
exp(− Ei λihi(e, g, a))
pA(e|g, a) = (1)
Ee′ exp(− Ei λihi(e′, g, a))
</equation>
<bodyText confidence="0.999462">
Given a vector of weights λ, the best English
parse eˆ can be found by solving eq. 2. The model
is trained by finding the weight vector λ which
maximizes accuracy (see section 4).
</bodyText>
<equation confidence="0.984644333333333">
eˆ = argmax pA(e|g, a)
e
λihi(e, g, a)) (2)
</equation>
<subsectionHeader confidence="0.996137">
3.1 Feature Functions
</subsectionHeader>
<bodyText confidence="0.978481855263158">
The basic idea behind our feature functions is that
any constituent in a sentence should play approx-
imately the same syntactic role and have a similar
span as the corresponding constituent in a trans-
lation. If there is an obvious disagreement, it
is probably caused by wrong attachment or other
syntactic mistakes in parsing. Sometimes in trans-
lation the syntactic role of a given semantic consti-
tutent changes; we assume that our model penal-
izes all hypothesized parses equally in this case.
For the initial experiments, we used a set of 34
probabilistic and heuristic feature functions.
BitParLogProb (the only monolingual feature)
is the negative log probability assigned by BitPar
to the English parse. If we set λ1 = 1 and λi = 0
for all i =� 1 and evaluate eq. 2, we will select the
parse ranked best by BitPar.
In order to define our feature functions, we first
introduce auxiliary functions operating on indi-
vidual word positions or sets of word positions.
Alignment functions take an alignment a as an ar-
gument. In the descriptions of these functions we
omit a as it is held constant for a sentence pair (i.e.,
an English sentence and its German translation).
f(i) returns the set of word positions of German
words aligned with an English word at position i.
f′(i) returns the leftmost word position of the
German words aligned with an English word at po-
sition i, or zero if the English word is unaligned.
f−1(i) returns the set of positions of English
3For example, a probability of 1 is a feature value of 0,
while a low probability is a feature value which is ≫ 0.
words aligned with a German word at position i.
f′−1(i) returns the leftmost word position of the
English words aligned with a German word at po-
sition i, or zero if the German word is unaligned.
We overload the above functions to allow the ar-
gument i to be a set, in which case union is used,
for example, f(i) = UjEif(j). Positions in a
tree are denoted with integers. First, the POS tags
are numbered from 1 to the length of the sentence
(i.e., the same as the word positions). Constituents
higher in the tree are also indexed using consecu-
tive integers. We refer to the constituent that has
been assigned index i in the tree t as “constituent i
in tree t” or simply as “constituent i”. The follow-
ing functions have the English and German trees
as an implicit argument; it should be obvious from
the argument to the function whether the index
i refers to the German tree or the English tree.
When we say “constituents”, we include nodes
on the POS level of the tree. Our syntactic trees
are annotated with a syntactic head for each con-
stituent. Finally, the tag at position 0 is NULL.
mid2sib(i) returns 0 if i is 0, returns 1 if i has
exactly two siblings, one on the left of i and one
on the right, and otherwise returns 0.
head(i) returns the index of the head of i. The
head of a POS tag is its own position.
tag(i) returns the tag of i.
left(i) returns the index of the leftmost sibling of
i.
right(i) returns the index of the rightmost sibling.
up(i) returns the index of i’s parent.
A(i) returns the set of word positions covered by
i. If i is a set, A returns all word positions between
the leftmost position covered by any constituent in
the set and the rightmost position covered by any
constituent in the set (inclusive).
n(A) returns the size of the set A.
c(A) returns the number of characters (including
punctuation and excluding spaces) covered by the
constituents in set A.
Q7r� is 1 if 7r is true, and 0 otherwise.
l and m are the lengths in words of the English and
German sentences, respectively.
</bodyText>
<subsectionHeader confidence="0.89967">
3.1.1 Count Feature Functions
</subsectionHeader>
<bodyText confidence="0.999940166666667">
Feature CrdBin counts binary events involving
the heads of coordinated phrases. If in the English
parse we have a coordination where the English
CC is aligned only with a German KON, and both
have two siblings, then the value contributed to
CrdBin is 1 (indicating a constraint violation) un-
</bodyText>
<equation confidence="0.4697055">
= argmin �exp(
e i
</equation>
<page confidence="0.969934">
284
</page>
<bodyText confidence="0.999938">
less the head of the English left conjunct is aligned
with the head of the German left conjunct and like-
wise the right conjuncts are aligned. Eq. 3 calcu-
lates the value of CrdBin.
</bodyText>
<equation confidence="0.9998616">
�l J(tag(i) = CCKJ(n(f(i)) = 1K mid2sib(i)
Z=1
mid2sib(f′(i)) Jtag(f′(i)) = KON-CDK
J[head(left(f′(i))) =6 f′(head(left(i)))] OR
[head(right(f′(i))) =6 f′(head(right(i)))]K (3)
</equation>
<bodyText confidence="0.9849242">
Feature Q simply captures a mismatch between
questions and statements. If an English sentence is
parsed as a question but the parallel German sen-
tence is not, or vice versa, the feature value is 1;
otherwise the value is 0.
</bodyText>
<subsubsectionHeader confidence="0.748158">
3.1.2 Span Projection Feature Functions
</subsubsectionHeader>
<bodyText confidence="0.999959">
Span projection features calculate the percentage
difference between a constituent’s span and the
span of its projection. Span size is measured in
characters or words. To project a constituent in
a parse, we use the word alignment to project all
word positions covered by the constituent and then
look for the smallest covering constituent in the
parse of the parallel sentence.
CrdPrj is a feature that measures the diver-
gence in the size of coordination constituents and
their projections. If we have a constituent (XP1
CC XP2) in English that is projected to a German
coordination, we expect the English and German
left conjuncts to span a similar percentage of their
respective sentences, as should the right conjuncts.
The feature computes a character-based percent-
age difference as shown in eq. 4.
</bodyText>
<equation confidence="0.9986619">
�l Jtag(i) = CCKJn(f(i)) = 1K (4)
Z=1
Jtag(f′(i)) = KON-CDK
mid2sib(i)mid2sib(f′(i))
(|c(A(left(i)))
r − s
c(A(left(f′(i)))) |
+|c(A(right(i)))
r − s
c(A(right(f′(i)))) |)
</equation>
<bodyText confidence="0.998496666666667">
r and s are the lengths in characters of the En-
glish and German sentences, respectively. In the
English parse in figure 1, the left conjunct has 5
characters and the right conjunct has 6, while in
figure 2 the left conjunct has 5 characters and the
right conjunct has 20. In the German parse (fig-
ure 3) the left conjunct has 7 characters and the
right conjunct has 27. Finally, r = 33 and s = 42.
Thus, the value of CrdPrj is 0.48 for the first hy-
pothesized parse and 0.05 for the second, which
captures the higher divergence of the first English
parse from the German parse.
POSParentPrj is based on computing the span
difference between all the parent constituents of
POS tags in a German parse and their respective
coverage in the corresponding hypothesized parse.
The feature value is the sum of all the differences.
POSPar(i) is true if i immediately dominates a
POS tag. The projection direction is from German
to English, and the feature computes a percentage
difference which is character-based. The value of
the feature is calculated in eq. 5, where M is the
number of constituents (including POS tags) in the
German tree.
</bodyText>
<equation confidence="0.704263333333333">
JPOSPar(i)K|c(A(i))
s
(5)
</equation>
<bodyText confidence="0.999874590909091">
The right conjunct in figure 3 is a POSParent
that corresponds to the coordination NP in fig-
ure 1, contributing a score of 0.21, and to the right
conjunct in figure 2, contributing a score of 0.04.
For the two parses of the full sentences contain-
ing the NPs in figure 1 and figure 2, we sum over
7 POSParents and get a value of 0.27 for parse 1
and 0.11 for parse 2. The lower value for parse
2 correctly captures the fact that the first English
parse has higher divergence than the second due to
incorrect high attachment.
AbovePOSPrj is similar to POSParentPrj, but
it is word-based and the projection direction is
from English to German. Unlike POSParentPrj
the feature value is calculated over all constituents
above the POS level in the English tree.
Another span projection feature function is
DTNNPrj, which projects English constituents of
the form (NP(DT)(NN)). DTNN(i) is true if i
is an NP immediately dominating only DT and
NN. The feature computes a percentage difference
which is word-based, shown in eq. 6.
</bodyText>
<equation confidence="0.6410115">
JDTNN(i)K|n(A(i)) − n (A (f (A(i)))) |(6)
l m
</equation>
<bodyText confidence="0.5678555">
L is the number of constituents in the English
tree. This feature is designed to disprefer parses
</bodyText>
<equation confidence="0.946737375">
M
Z=1
c(A(f−1(A(i))))
−
r
|
L
Z=1
</equation>
<page confidence="0.981875">
285
</page>
<bodyText confidence="0.999941">
where constituents starting with “DT NN”, e.g.,
(NP (DT NN NN NN)), are incorrectly split into
two NPs, e.g., (NP (DT NN)) and (NP (NN NN)).
This feature fires in this case, and projects the (NP
(DT NN)) into German. If the German projection
is a surprisingly large number of words (as should
be the case if the German also consists of a deter-
miner followed by several nouns) then the penalty
paid by this feature is large. This feature is impor-
tant as (NP (DT NN)) is a very common construc-
tion.
</bodyText>
<subsectionHeader confidence="0.772012">
3.1.3 Probabilistic Feature Functions
</subsectionHeader>
<bodyText confidence="0.999984045454545">
We use Europarl (Koehn, 2005), from which we
extract a parallel corpus of approximately 1.22
million sentence pairs, to estimate the probabilis-
tic feature functions described in this section.
For the PDepth feature, we estimate English
parse depth probability conditioned on German
parse depth from Europarl by calculating a sim-
ple probability distribution over the 1-best parse
pairs for each parallel sentence. A very deep Ger-
man parse is unlikely to correspond to a flat En-
glish parse and we can penalize such a parse using
PDepth. The index i refers to a sentence pair in
Europarl, as does j. Let li and mi be the depths
of the top BitPar ranked parses of the English and
German sentences, respectively. We calculate the
probability of observing an English tree of depth
l′ given German tree of depth m′ as the maxi-
mum likelihood estimate, shown in eq. 7, where
δ(z, z′) = 1 if z = z′ and 0 otherwise. To avoid
noisy feature values due to outliers and parse er-
rors, we bound the value of PDepth at 5 as shown
in eq. 84.
</bodyText>
<equation confidence="0.999479333333333">
p(l′ |m′) = Ei δ(l′, li)δ(m′, mi) (7)
Ej δ(m′, mj)
min(5, − log10(p(l′|m′))) (8)
</equation>
<bodyText confidence="0.9997241">
The full parse of the sentence containing the En-
glish high attachment has a parse depth of 8 while
the full parse of the sentence containing the En-
glish low attachment has a depth of 9. Their fea-
ture values given the German parse depth of 6 are
−log10(0.12) = 0.93 and − log10(0.14) = 0.84.
The wrong parse is assigned a higher feature value
indicating its higher divergence.
The feature PTagEParentGPOSGParent mea-
sures tagging inconsistency based on estimating
</bodyText>
<footnote confidence="0.55107">
4Throughout this paper, assume log(0) = −∞.
</footnote>
<bodyText confidence="0.998129666666667">
the probability that for an English word at posi-
tion i, the parent of its POS tag has a particular
label. The feature value is calculated in eq. 10.
</bodyText>
<equation confidence="0.998324">
q(i,j) = p(tag(up(i))|tag(j), tag(up(j))) (9)
�l min 5, Ej∈ f(i) − log10(q(i, j)) 10)
i=1 min(
n(f (i)) )
</equation>
<bodyText confidence="0.988237692307692">
Consider (S(NP(NN fruit))(VP(V flies))) and
(NP(NN fruit)(NNS flies)) with the translation
(NP(NNS Fruchtfliegen)). Assume that “fruit”
and “flies” are aligned with the German com-
pound noun “Fruchtfliegen”. In the incorrect En-
glish parse the parent of the POS of “fruit” is
NP and the parent of the POS of “flies” is VP,
while in the correct parse the parent of the POS of
“fruit” is NP and the parent of the POS of “flies”
is NP. In the German parse the compound noun
is POS-tagged as an NNS and the parent is an
NP. The probabilities considered for the two En-
glish parses are p(NP|NNS, NP) for “fruit” in both
parses, p(VP|NNS, NP) for “flies” in the incorrect
parse, and p(NP|NNS, NP) for “flies” in the cor-
rect parse. A German NNS in an NP has a higher
probability of being aligned with a word in an En-
glish NP than with a word in an English VP, so the
second parse will be preferred.
As with the PDepth feature, we use relative
frequency to estimate this feature. When an En-
glish word is aligned with two words, estimation is
more complex. We heuristically give each English
and German pair one count. The value calculated
by the feature function is the geometric mean5 of
the pairwise probabilities, see eq. 10.
</bodyText>
<subsectionHeader confidence="0.688941">
3.1.4 Other Features
</subsectionHeader>
<bodyText confidence="0.999966">
Our best system uses the nine features we have
described in detail so far. In addition, we imple-
mented the following 25 other features, which did
not improve performance (see section 7): (i) 7
“ptag” features similar to PTagEParentGPOSG-
Parent but predicting and conditioning on differ-
ent combinations of tags (POS tag, parent of POS,
grandparent of POS)
(ii) 10 “prj” features similar to POSParentPrj
measuring different combinations of character and
word percentage differences at the POS parent and
</bodyText>
<footnote confidence="0.549315">
5Each English word has the same weight regardless of
whether it was aligned with one or with more German words.
</footnote>
<page confidence="0.99669">
286
</page>
<bodyText confidence="0.862805">
POS grandparent levels, projecting from both En-
glish and German
(iii) 3 variants of the DTNN feature function
</bodyText>
<listItem confidence="0.980095833333333">
(iv) A NPPP feature function, similar to the
DTNN feature function but trying to counteract a
bias towards (NP (NP) (PP)) units
(v) A feature function which penalizes aligning
clausal units to non-clausal units
(vi) The BitPar rank
</listItem>
<sectionHeader confidence="0.97698" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.99996725">
Log-linear models are often trained using the
Maximum Entropy criterion, but we train our
model directly to maximize F1. We score F1 by
comparing hypothesized parses for the discrimi-
native training set with the gold standard. To try
to find the optimal A vector, we perform direct ac-
curacy maximization, meaning that we search for
the A vector which directly optimizes F1 on the
training set.
Och (2003) has described an efficient exact one-
dimensional accuracy maximization technique for
a similar search problem in machine translation.
The technique involves calculating an explicit
representation of the piecewise constant function
gm(x) which evaluates the accuracy of the hy-
potheses which would be picked by eq. 2 from a
set of hypotheses if we hold all weights constant,
except for the weight Am, which is set to x. This
is calculated in one pass over the data.
The algorithm for training is initialized with a
choice for A and is described in figure 4. The func-
tion F1(A) returns F1 of the parses selected using
A. Due to space we do not describe step 8 in detail
(see (Och, 2003)). In step 9 the algorithm per-
forms approximate normalization, where feature
weights are forced towards zero. The implemen-
tation of step 9 is straight-forward given the M
explicit functions gm(x) created in step 8.
</bodyText>
<sectionHeader confidence="0.977571" genericHeader="method">
5 Data and Experiments
</sectionHeader>
<bodyText confidence="0.9999218">
We used the subset of the Wall Street Journal
investigated in (Atterer and Sch¨utze, 2007) for
our experiments, which consists of all sentences
that have at least one prepositional phrase attach-
ment ambiguity. This difficult subset of sentences
seems particularly interesting when investigating
the potential of information in bitext for improv-
ing parsing performance. The first 500 sentences
of this set were translated from English to German
by a graduate student and an additional 3218 sen-
</bodyText>
<listItem confidence="0.96515375">
1: Algorithm TRAIN(A)
2: repeat
3: add A to the set s
4: let t be a set of 1000 randomly generated vectors
5: let A = argmaxρ∈(s∪t) F1(�)
6: let A′ = A
7: repeat
8: repeatedly run one-dimensional error minimiza-
tion step (updating a single scalar of the vector A)
until no further error reduction
9: adjust each scalar of A in turn towards 0 such that
there is no increase in error (if possible)
10: until no scalar in A changes in last two steps (8 and
9)
11: until A = A′
12: return A
</listItem>
<figureCaption confidence="0.99914">
Figure 4: Sketch of the training algorithm
</figureCaption>
<bodyText confidence="0.998468083333334">
tences by a translation bureau. We withheld these
3718 English sentences (and an additional 1000
reserved sentences) when we trained BitPar on the
Penn treebank.
Parses. We use the BitPar parser (Schmid,
2004) which is based on a bit-vector im-
plementation (cf. (Graham et al., 1980)) of
the Cocke-Younger-Kasami algorithm (Kasami,
1965; Younger, 1967). It computes a compact
parse forest for all possible analyses. As all pos-
sible analyses are computed, any number of best
parses can be extracted. In contrast, other treebank
parsers use sophisticated search strategies to find
the most probable analysis without examining the
set of all possible analyses (Charniak et al., 1998;
Klein and Manning, 2003). BitPar is particularly
useful for N-best parsing as the N-best parses can
be computed efficiently.
For the 3718 sentences in the translated set, we
created 100-best English parses and 1-best Ger-
man parses. The German parser was trained on
the TIGER treebank. For the Europarl corpus, we
created 1-best parses for both languages.
Word Alignment. We use a word alignment
of the translated sentences from the Penn tree-
bank, as well as a word alignment of the Europarl
corpus. We align these two data sets together
with data from the JRC Acquis (Steinberger et al.,
2006) to try to obtain better quality alignments (it
is well known that alignment quality improves as
the amount of data increases (Fraser and Marcu,
2007)). We aligned approximately 3.08 million
sentence pairs. We tried to obtain better alignment
quality as alignment quality is a problem in many
cases where syntactic projection would otherwise
work well (Fossum and Knight, 2008).
</bodyText>
<page confidence="0.990989">
287
</page>
<table confidence="0.9982925">
System Train +base Test +base
1 Baseline 87.89 87.89
2 Contrastive 88.70 0.82 88.45 0.56
(5 trials/fold)
3 Contrastive 88.82 0.93 88.55 0.66
(greedy selection)
</table>
<tableCaption confidence="0.999881">
Table 1: Average F1 of 7-way cross-validation
</tableCaption>
<bodyText confidence="0.999876911111111">
To generate the alignments, we used Model 4
(Brown et al., 1993), as implemented in GIZA++
(Och and Ney, 2003). As is standard practice, we
trained Model 4 with English as the source lan-
guage, and then trained Model 4 with German as
the source language, resulting in two Viterbi align-
ments. These were combined using the Grow Diag
Final And symmetrization heuristic (Koehn et al.,
2003).
Experiments. We perform 7-way cross-
validation on 3718 sentences. In each fold of the
cross-validation, the training set is 3186 sentences,
while the test set is 532 sentences. Our results are
shown in table 1. In row 1, we take the hypothesis
ranked best by BitPar. In row 2, we train using the
algorithm outlined in section 4. To cancel out any
effect caused by a particularly effective or ineffec-
tive starting A value, we perform 5 trials each time.
Columns 3 and 5 report the improvement over the
baseline on train and test respectively. We reach
an improvement of 0.56 over the baseline using
the algorithm as described in section 4.
Our initial experiments used many highly cor-
related features. For our next experiment we use
greedy feature selection. We start with a A vector
that is zero for all features, and then run the error
minimization without the random generation of
vectors (figure 4, line 4). This means that we add
one feature at a time. This greedy algorithm winds
up producing a vector with many zero weights. In
row 3 of table 1, we used the greedy feature selec-
tion algorithm and trained using F1, resulting in
a performance of 0.66 over the baseline which is
our best result. We performed a planned one-tailed
paired t-test on the F1 scores of the parses selected
by the baseline and this system for the 3718 sen-
tences (parses were taken from the test portion
of each fold). We found that there is a signifi-
cant difference with the baseline (t(3717) = 6.42,
p &lt; .01). We believe that using the full set of 34
features (many of which are very similar to one
another) made the training problem harder with-
out improving the fit to the training data, and that
greedy feature selection helps with this (see also
section 7).
</bodyText>
<sectionHeader confidence="0.990144" genericHeader="method">
6 Previous Work
</sectionHeader>
<bodyText confidence="0.999995617021277">
As we mentioned in section 2, work on parse
reranking is relevant, but a vital difference is that
we use features based only on syntactic projection
of the two languages in a bitext. For an overview
of different types of features that have been used in
parse reranking see Charniak and Johnson (2005).
Like Collins (2000) we use cross-validation to
train our model, but we have access to much less
data (3718 sentences total, which is less than 1/10
of the data Collins used). We use rich feature func-
tions which were designed by hand to specifically
address problems in English parses which can be
disambiguated using the German translation.
Syntactic projection has been used to bootstrap
treebanks in resource poor languages. Some ex-
amples of projection of syntactic parses from En-
glish to a resource poor language for which no
parser is available are the works of Yarowsky and
Ngai (2001), Hwa et al. (2005) and Goyal and
Chatterjee (2006). Our work differs from theirs
in that we are performing a parse reranking task
in English using knowledge gained from German
parses, and parsing accuracy is generally thought
to be worse in German than in English.
Hopkins and Kuhn (2006) conducted research
with goals similar to ours. They showed how to
build a powerful generative model which flexibly
incorporates features from parallel text in four lan-
guages, but were not able to show an improvement
in parsing performance. After the submission of
our paper for review, two papers outlining relevant
work were published. Burkett and Klein (2008)
describe a system for simultaneously improving
Chinese and English parses of a Chinese/English
bitext. This work is complementary to ours. The
system is trained using gold standard trees in both
Chinese and English, in contrast with our system
which only has access to gold standard trees in En-
glish. Their system uses a tree alignment which
varies within training, but this does not appear to
make a large difference in performance. They use
coarsely defined features which are language in-
dependent. We use several features similar to their
two best performing sets of features, but in con-
trast with their work, we also define features which
are specifically aimed at English disambiguation
problems that we have observed can be resolved
</bodyText>
<page confidence="0.990699">
288
</page>
<bodyText confidence="0.999838652173913">
using German parses. They use an in-domain
Chinese parser and out-of-domain English parser,
while for us the English parser is in-domain and
the German parser is out-of-domain, both of which
make improving the English parse more difficult.
Their Maximum Entropy training is more appro-
priate for their numerous coarse features, while
we use Minimum Error Rate Training, which is
much faster. Finally, we are projecting from a sin-
gle German parse which is a more difficult prob-
lem. Fossum and Knight (2008) outline a system
for using Chinese/English word alignments to de-
termine ambiguous English PP-attachments. They
first use an oracle to choose PP-attachment deci-
sions which are ambiguous in the English side of a
Chinese/English bitext, and then build a classifier
which uses information from a word alignment to
make PP-attachment decisions. No Chinese syn-
tactic information is required. We use automati-
cally generated German parses to improve English
syntactic parsing, and have not been able to find a
similar phenomenon for which only a word align-
ment would suffice.
</bodyText>
<sectionHeader confidence="0.991819" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999882173913043">
We looked at the weights assigned during the
cross-validation performed to obtain our best re-
sult. The weights of many of the 34 features we
defined were frequently set to zero. We sorted
the features by the number of times the relevant
λ scalar was zero (i.e., the number of folds of
the cross-validation for which they were zero; the
greedy feature selection is deterministic and so we
do not run multiple trials). We then reran the same
greedy feature selection algorithm as was used in
table 1, row 3, but this time using only the top
9 feature values, which were the features which
were active on 4 or more folds6. The result was an
improvement on train of 0.84 and an improvement
on test of 0.73. This test result may be slightly
overfit, but the result supports the inference that
these 9 feature functions are the most important.
We chose these feature functions to be described
in detail in section 3. We observed that the variants
of the similar features POSParentPrj and Above-
POSPrj projected in opposite directions and mea-
sured character and word differences, respectively,
and this complementarity seems to help.
</bodyText>
<footnote confidence="0.97126575">
6We saw that many features canceled one another out on
different folds. For instance either the word-based or the
character-based version of DTNN was active in each fold,
but never at the same time as one another.
</footnote>
<bodyText confidence="0.999921545454545">
We also tried to see if our results depended
strongly on the log-linear model and training algo-
rithm, by using the SVM-Light ranker (Joachims,
2002). In order to make the experiment tractable,
we limited ourselves to the 8-best parses (rather
than 100-best). Our training algorithm and model
was 0.74 better than the baseline on train and 0.47
better on test, while SVM-Light was 0.54 better
than baseline on train and 0.49 better on test (us-
ing linear kernels). We believe that the results are
not unduly influenced by the training algorithm.
</bodyText>
<sectionHeader confidence="0.998279" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999988">
We have shown that rich bitext projection features
can improve parsing accuracy. This confirms the
hypothesis that the divergence in what information
different languages encode grammatically can be
exploited for syntactic disambiguation. Improved
parsing due to bitext projection features should be
helpful in syntactic analysis of bitexts (by way of
mutual syntactic disambiguation) and in comput-
ing syntactic analyses of texts that have transla-
tions in other languages available.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999025">
This work was supported in part by Deutsche
Forschungsgemeinschaft Grant SFB 732. We
would like to thank Helmut Schmid for support of
BitPar and for his many helpful comments on our
work. We would also like to thank the anonymous
reviewers.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999367058823529">
Michaela Atterer and Hinrich Sch¨utze. 2007. Preposi-
tional phrase attachment without oracles. Computa-
tional Linguistics, 33(4).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2).
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
</reference>
<page confidence="0.988963">
289
</page>
<reference confidence="0.966139402777778">
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In EMNLP.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve
PP-attachment ambiguity in English. In AMTA.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3).
Shailly Goyal and Niladri Chatterjee. 2006. Parsing
aligned parallel corpus by projecting syntactic re-
lations from annotated source corpus. In Proceed-
ings of the COLING/ACL main conference poster
sessions.
Susan L. Graham, Michael A. Harrison, and Walter L.
Ruzzo. 1980. An improved context-free recognizer.
ACM Transactions on Programming Languages and
Systems, 2(3).
Mark Hopkins and Jonas Kuhn. 2006. A framework
for incorporating alignment information in parsing.
In Proceedings of the EACL 2006 Workshop on
Cross-Language Knowledge Induction.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3).
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD.
Takao Kasami. 1965. An efficient recognition and syn-
tax analysis algorithm for context-free languages.
Technical Report AFCRL-65-7558, Air Force Cam-
bridge Research Laboratory.
Dan Klein and Christopher Manning. 2003. A* pars-
ing: fast exact viterbi parse selection. In HLT-
NAACL.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT Summit X.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics, 19(2).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard S. Crouch, John T. Maxwell III, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a lexical-functional grammar and discriminative
estimation techniques. In ACL.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: a multilin-
gual aligned parallel corpus with 20+ languages. In
LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In NAACL.
Daniel H. Younger. 1967. Recognition of context-free
languages in time n3. Information and Control, 10.
</reference>
<page confidence="0.997125">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.729087">
<title confidence="0.992301">Rich bitext projection features for parse reranking</title>
<author confidence="0.998207">Alexander Fraser Renjing Wang Hinrich Sch¨utze</author>
<affiliation confidence="0.997564">Institute for Natural Language Processing University of Stuttgart</affiliation>
<abstract confidence="0.981282214285714">Many different types of features have been shown to improve accuracy in parse reranking. A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed. The intufor using this type of projecfeature that ambiguous structures in one language often correspond to unambiguous structures in another. We show that reranking based on bitext projection features increases parsing accuracy significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michaela Atterer</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Prepositional phrase attachment without oracles.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Atterer, Sch¨utze, 2007</marker>
<rawString>Michaela Atterer and Hinrich Sch¨utze. 2007. Prepositional phrase attachment without oracles. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="24744" citStr="Brown et al., 1993" startWordPosition="4268" endWordPosition="4271">lignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). 287 System Train +base Test +base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 (5 trials/fold) 3 Contrastive 88.82 0.93 88.55 0.66 (greedy selection) Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments. These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). Experiments. We perform 7-way crossvalidation on 3718 sentences. In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences. Our results are shown in table 1. In row 1, we take the hypothesis ranked best by BitPar. In row 2, </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28369" citStr="Burkett and Klein (2008)" startWordPosition="4895" endWordPosition="4898">al and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were published. Burkett and Klein (2008) describe a system for simultaneously improving Chinese and English parses of a Chinese/English bitext. This work is complementary to ours. The system is trained using gold standard trees in both Chinese and English, in contrast with our system which only has access to gold standard trees in English. Their system uses a tree alignment which varies within training, but this does not appear to make a large difference in performance. They use coarsely defined features which are language independent. We use several features similar to their two best performing sets of features, but in contrast wit</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="27126" citStr="Charniak and Johnson (2005)" startWordPosition="4690" endWordPosition="4693">ant difference with the baseline (t(3717) = 6.42, p &lt; .01). We believe that using the full set of 34 features (many of which are very similar to one another) made the training problem harder without improving the fit to the training data, and that greedy feature selection helps with this (see also section 7). 6 Previous Work As we mentioned in section 2, work on parse reranking is relevant, but a vital difference is that we use features based only on syntactic projection of the two languages in a bitext. For an overview of different types of features that have been used in parse reranking see Charniak and Johnson (2005). Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="23496" citStr="Charniak et al., 1998" startWordPosition="4064" endWordPosition="4067">glish sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to t</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998. Edge-based best-first chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parse quality on syntactically-informed statistical machine translation.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1287" citStr="Quirk and Corston-Oliver, 2006" startWordPosition="192" endWordPosition="195">rojection features increases parsing accuracy significantly. 1 Introduction Parallel text or bitext is an important knowledge source for solving many problems such as machine translation, cross-language information retrieval, and the projection of linguistic resources from one language to another. In this paper, we show that bitext-based features are effective in addressing another NLP problem, increasing the accuracy of statistical parsing. We pursue this approach for a number of reasons. First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver, 2006). Improved parses of bitext should result in improved machine translation. Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext. Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006). It is well known that different languages encode different types of grammatical information (agreement, case, tense etc.) and that what can be left unspecified in one language mus</context>
</contexts>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="27147" citStr="Collins (2000)" startWordPosition="4695" endWordPosition="4696">t(3717) = 6.42, p &lt; .01). We believe that using the full set of 34 features (many of which are very similar to one another) made the training problem harder without improving the fit to the training data, and that greedy feature selection helps with this (see also section 7). 6 Previous Work As we mentioned in section 2, work on parse reranking is relevant, but a vital difference is that we use features based only on syntactic projection of the two languages in a bitext. For an overview of different types of features that have been used in parse reranking see Charniak and Johnson (2005). Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Using bilingual Chinese-English word alignments to resolve PP-attachment ambiguity in English.</title>
<date>2008</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="24468" citStr="Fossum and Knight, 2008" startWordPosition="4223" endWordPosition="4226">nguages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). 287 System Train +base Test +base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 (5 trials/fold) 3 Contrastive 88.82 0.93 88.55 0.66 (greedy selection) Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments. These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). </context>
<context position="29617" citStr="Fossum and Knight (2008)" startWordPosition="5098" endWordPosition="5101">fine features which are specifically aimed at English disambiguation problems that we have observed can be resolved 288 using German parses. They use an in-domain Chinese parser and out-of-domain English parser, while for us the English parser is in-domain and the German parser is out-of-domain, both of which make improving the English parse more difficult. Their Maximum Entropy training is more appropriate for their numerous coarse features, while we use Minimum Error Rate Training, which is much faster. Finally, we are projecting from a single German parse which is a more difficult problem. Fossum and Knight (2008) outline a system for using Chinese/English word alignments to determine ambiguous English PP-attachments. They first use an oracle to choose PP-attachment decisions which are ambiguous in the English side of a Chinese/English bitext, and then build a classifier which uses information from a word alignment to make PP-attachment decisions. No Chinese syntactic information is required. We use automatically generated German parses to improve English syntactic parsing, and have not been able to find a similar phenomenon for which only a word alignment would suffice. 7 Analysis We looked at the wei</context>
</contexts>
<marker>Fossum, Knight, 2008</marker>
<rawString>Victoria Fossum and Kevin Knight. 2008. Using bilingual Chinese-English word alignments to resolve PP-attachment ambiguity in English. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="24241" citStr="Fraser and Marcu, 2007" startWordPosition="4189" endWordPosition="4192">tly. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). 287 System Train +base Test +base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 (5 trials/fold) 3 Contrastive 88.82 0.93 88.55 0.66 (greedy selection) Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with </context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shailly Goyal</author>
<author>Niladri Chatterjee</author>
</authors>
<title>Parsing aligned parallel corpus by projecting syntactic relations from annotated source corpus.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL main conference</booktitle>
<note>poster sessions.</note>
<contexts>
<context position="27769" citStr="Goyal and Chatterjee (2006)" startWordPosition="4798" endWordPosition="4801">2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were published. Burkett and Klein (2008)</context>
</contexts>
<marker>Goyal, Chatterjee, 2006</marker>
<rawString>Shailly Goyal and Niladri Chatterjee. 2006. Parsing aligned parallel corpus by projecting syntactic relations from annotated source corpus. In Proceedings of the COLING/ACL main conference poster sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Graham</author>
<author>Michael A Harrison</author>
<author>Walter L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="23100" citStr="Graham et al., 1980" startWordPosition="4004" endWordPosition="4007">onal error minimization step (updating a single scalar of the vector A) until no further error reduction 9: adjust each scalar of A in turn towards 0 such that there is no increase in error (if possible) 10: until no scalar in A changes in last two steps (8 and 9) 11: until A = A′ 12: return A Figure 4: Sketch of the training algorithm tences by a translation bureau. We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English par</context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Susan L. Graham, Michael A. Harrison, and Walter L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonas Kuhn</author>
</authors>
<title>A framework for incorporating alignment information in parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Cross-Language Knowledge Induction.</booktitle>
<contexts>
<context position="28007" citStr="Hopkins and Kuhn (2006)" startWordPosition="4838" endWordPosition="4841"> problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were published. Burkett and Klein (2008) describe a system for simultaneously improving Chinese and English parses of a Chinese/English bitext. This work is complementary to ours. The system is trained using gold standard trees in both Chinese and English, in contrast with our </context>
</contexts>
<marker>Hopkins, Kuhn, 2006</marker>
<rawString>Mark Hopkins and Jonas Kuhn. 2006. A framework for incorporating alignment information in parsing. In Proceedings of the EACL 2006 Workshop on Cross-Language Knowledge Induction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="27737" citStr="Hwa et al. (2005)" startWordPosition="4793" endWordPosition="4796">(2005). Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were pub</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Nat. Lang. Eng., 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD.</booktitle>
<contexts>
<context position="31685" citStr="Joachims, 2002" startWordPosition="5447" endWordPosition="5448"> be described in detail in section 3. We observed that the variants of the similar features POSParentPrj and AbovePOSPrj projected in opposite directions and measured character and word differences, respectively, and this complementarity seems to help. 6We saw that many features canceled one another out on different folds. For instance either the word-based or the character-based version of DTNN was active in each fold, but never at the same time as one another. We also tried to see if our results depended strongly on the log-linear model and training algorithm, by using the SVM-Light ranker (Joachims, 2002). In order to make the experiment tractable, we limited ourselves to the 8-best parses (rather than 100-best). Our training algorithm and model was 0.74 better than the baseline on train and 0.47 better on test, while SVM-Light was 0.54 better than baseline on train and 0.49 better on test (using linear kernels). We believe that the results are not unduly influenced by the training algorithm. 8 Conclusion We have shown that rich bitext projection features can improve parsing accuracy. This confirms the hypothesis that the divergence in what information different languages encode grammatically </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takao Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-7558,</tech>
<institution>Air Force Cambridge Research Laboratory.</institution>
<contexts>
<context position="23153" citStr="Kasami, 1965" startWordPosition="4012" endWordPosition="4013"> vector A) until no further error reduction 9: adjust each scalar of A in turn towards 0 such that there is no increase in error (if possible) 10: until no scalar in A changes in last two steps (8 and 9) 11: until A = A′ 12: return A Figure 4: Sketch of the training algorithm tences by a translation bureau. We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was t</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Takao Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-7558, Air Force Cambridge Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>A* parsing: fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="23522" citStr="Klein and Manning, 2003" startWordPosition="4068" endWordPosition="4071"> additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better qualit</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. A* parsing: fast exact viterbi parse selection. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="25066" citStr="Koehn et al., 2003" startWordPosition="4323" endWordPosition="4326">sum and Knight, 2008). 287 System Train +base Test +base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 (5 trials/fold) 3 Contrastive 88.82 0.93 88.55 0.66 (greedy selection) Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments. These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). Experiments. We perform 7-way crossvalidation on 3718 sentences. In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences. Our results are shown in table 1. In row 1, we take the hypothesis ranked best by BitPar. In row 2, we train using the algorithm outlined in section 4. To cancel out any effect caused by a particularly effective or ineffective starting A value, we perform 5 trials each time. Columns 3 and 5 report the improvement over the baseline on train and test respectively. We reach an improvement of 0.56 over the baseline using t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: a parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X.</booktitle>
<contexts>
<context position="16427" citStr="Koehn, 2005" startWordPosition="2843" endWordPosition="2844">er parses M Z=1 c(A(f−1(A(i)))) − r | L Z=1 285 where constituents starting with “DT NN”, e.g., (NP (DT NN NN NN)), are incorrectly split into two NPs, e.g., (NP (DT NN)) and (NP (NN NN)). This feature fires in this case, and projects the (NP (DT NN)) into German. If the German projection is a surprisingly large number of words (as should be the case if the German also consists of a determiner followed by several nouns) then the penalty paid by this feature is large. This feature is important as (NP (DT NN)) is a very common construction. 3.1.3 Probabilistic Feature Functions We use Europarl (Koehn, 2005), from which we extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate the probabilistic feature functions described in this section. For the PDepth feature, we estimate English parse depth probability conditioned on German parse depth from Europarl by calculating a simple probability distribution over the 1-best parse pairs for each parallel sentence. A very deep German parse is unlikely to correspond to a flat English parse and we can penalize such a parse using PDepth. The index i refers to a sentence pair in Europarl, as does j. Let li and mi be the depths of t</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5354" citStr="Marcus et al., 1993" startWordPosition="885" endWordPosition="888">100 best English parses from the BitPar parser (Schmid, 2004) and rerank them. We have a good chance of finding the optimal parse among the 100-best1. An automatically generated word alignment determines translational correspondence between German and English. We use features which measure syntactic di1Using an oracle to select the best parse results in an Fl of 95.90, an improvement of 8.01 absolute over the baseline. vergence between the German and English trees to try to rank the English trees which have less divergence higher. Our test set is 3718 sentences from the English Penn treebank (Marcus et al., 1993) which were translated into German. We hold out these sentences, and train BitPar on the remaining Penn treebank training sentences. The average Fi parsing accuracy of BitPar on this test set is 87.89%, which is our baseline2. We implement features based on projecting the German parse to each of the English 100-best parses in turn via the word alignment. By performing cross-validation and measuring test performance within each fold, we compare our new system with the baseline on the 3718 sentence set. The overall test accuracy we reach is 88.55%, a statistically significant improvement over ba</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1706" citStr="McClosky et al., 2006" startWordPosition="264" endWordPosition="267">tistical parsing. We pursue this approach for a number of reasons. First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver, 2006). Improved parses of bitext should result in improved machine translation. Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext. Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006). It is well known that different languages encode different types of grammatical information (agreement, case, tense etc.) and that what can be left unspecified in one language must be made explicit NP NP SBAR Figure 1: English parse with high attachment in another. This information can be used for syntactic disambiguation. However, it is surprisingly hard to do this well. We use parses and alignments that are automatically generated and hence imperfect. German parse quality is considered to be worse than English parse quality, and the annotation style is different, e.g., NP structure in Germ</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="24790" citStr="Och and Ney, 2003" startWordPosition="4276" endWordPosition="4279">ity improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). 287 System Train +base Test +base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 (5 trials/fold) 3 Contrastive 88.82 0.93 88.55 0.66 (greedy selection) Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments. These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). Experiments. We perform 7-way crossvalidation on 3718 sentences. In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences. Our results are shown in table 1. In row 1, we take the hypothesis ranked best by BitPar. In row 2, we train using the algorithm outlined in secti</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="20876" citStr="Och (2003)" startWordPosition="3621" endWordPosition="3622">r to the DTNN feature function but trying to counteract a bias towards (NP (NP) (PP)) units (v) A feature function which penalizes aligning clausal units to non-clausal units (vi) The BitPar rank 4 Training Log-linear models are often trained using the Maximum Entropy criterion, but we train our model directly to maximize F1. We score F1 by comparing hypothesized parses for the discriminative training set with the gold standard. To try to find the optimal A vector, we perform direct accuracy maximization, meaning that we search for the A vector which directly optimizes F1 on the training set. Och (2003) has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation. The technique involves calculating an explicit representation of the piecewise constant function gm(x) which evaluates the accuracy of the hypotheses which would be picked by eq. 2 from a set of hypotheses if we hold all weights constant, except for the weight Am, which is set to x. This is calculated in one pass over the data. The algorithm for training is initialized with a choice for A and is described in figure 4. The function F1(A) returns F1 of the parses </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard S Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4377" citStr="Riezler et al., 2002" startWordPosition="720" endWordPosition="723">hown in figures 1 and 2, respectively, and that the semantically more plausible second parse in figure 2 is correct. How can we determine that the second parse should be favored? Since we are parsing bitext, we can observe the German translation which is “Er sah ein Baby und eine Frau, die graue Haare hatte” (glossed: “he saw a baby and a woman, who gray hair had”). The singular verb in the subordinate clause (“hatte”: “had”) indicates that the subordinate S must be attached low to “woman” (“Frau”) as shown in figure 3. We follow Collins’ (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). Given a new sentence to parse, we first select the best N parse trees according to a generative model. Then we use new features to learn discriminatively how to rerank the parses in this N-best list. We use features derived using projections of the 1-best German parse onto the hypothesized English parse under consideration. In more detail, we take the 100 best English parses from the BitPar parser (Schmid, 2004) and rerank them. We have a good chance of finding the optimal parse among the 100-best1. An automatically generated word alignment determines translational correspondence between Ge</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard S. Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="4795" citStr="Schmid, 2004" startWordPosition="792" endWordPosition="793">d”) indicates that the subordinate S must be attached low to “woman” (“Frau”) as shown in figure 3. We follow Collins’ (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). Given a new sentence to parse, we first select the best N parse trees according to a generative model. Then we use new features to learn discriminatively how to rerank the parses in this N-best list. We use features derived using projections of the 1-best German parse onto the hypothesized English parse under consideration. In more detail, we take the 100 best English parses from the BitPar parser (Schmid, 2004) and rerank them. We have a good chance of finding the optimal parse among the 100-best1. An automatically generated word alignment determines translational correspondence between German and English. We use features which measure syntactic di1Using an oracle to select the best parse results in an Fl of 95.90, an improvement of 8.01 absolute over the baseline. vergence between the German and English trees to try to rank the English trees which have less divergence higher. Our test set is 3718 sentences from the English Penn treebank (Marcus et al., 1993) which were translated into German. We ho</context>
<context position="23027" citStr="Schmid, 2004" startWordPosition="3993" endWordPosition="3994">∈(s∪t) F1(�) 6: let A′ = A 7: repeat 8: repeatedly run one-dimensional error minimization step (updating a single scalar of the vector A) until no further error reduction 9: adjust each scalar of A in turn towards 0 such that there is no increase in error (if possible) 10: until no scalar in A changes in last two steps (8 and 9) 11: until A = A′ 12: return A Figure 4: Sketch of the training algorithm tences by a translation bureau. We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For </context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
</authors>
<title>Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis,</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<location>and</location>
<marker>Steinberger, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Daniel Varga. 2006. The JRC-Acquis: a multilingual aligned parallel corpus with 20+ languages. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In NAACL.</title>
<date>2001</date>
<contexts>
<context position="27718" citStr="Yarowsky and Ngai (2001)" startWordPosition="4789" endWordPosition="4792"> see Charniak and Johnson (2005). Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining rel</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<contexts>
<context position="23169" citStr="Younger, 1967" startWordPosition="4014" endWordPosition="4015">il no further error reduction 9: adjust each scalar of A in turn towards 0 such that there is no increase in error (if possible) 10: until no scalar in A changes in last two steps (8 and 9) 11: until A = A′ 12: return A Figure 4: Sketch of the training algorithm tences by a translation bureau. We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TI</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H. Younger. 1967. Recognition of context-free languages in time n3. Information and Control, 10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>