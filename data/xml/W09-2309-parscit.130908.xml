<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.998003">
Reordering Model Using Syntactic Information of a Source Tree for
Statistical Machine Translation
</title>
<author confidence="0.9787705">
Kei Hashimoto*&apos;, Hirohumi Yamamoto*2*3, Hideo Okuma*2*4,
Eiichiro Sumita*2*4, and Keiichi Tokuda*&apos;*2
</author>
<affiliation confidence="0.9205938">
*&apos;Nagoya Institute of Technology Department of Computer Science and Engineering
/ Gokiso-cho Syouwa-ku Nagoya-city Aichi Japan
*2National Institute of Information and Communications Technology
*3Kinki University School of Science and Engineering Department of Informaiton
*4ATR Spoken Language Communication Research Labs.
</affiliation>
<sectionHeader confidence="0.991976" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988425">
This paper presents a reordering model us-
ing syntactic information of a source tree for
phrase-based statistical machine translation.
The proposed model is an extension of IST-
ITG (imposing source tree on inversion trans-
duction grammar) constraints. In the pro-
posed method, the target-side word order is
obtained by rotating nodes of the source-side
parse-tree. We modeled the node rotation,
monotone or swap, using word alignments
based on a training parallel corpus and source-
side parse-trees. The model efficiently sup-
presses erroneous target word orderings, espe-
cially global orderings. Furthermore, the pro-
posed method conducts a probabilistic evalu-
ation of target word reorderings. In English-
to-Japanese and English-to-Chinese transla-
tion experiments, the proposed method re-
sulted in a 0.49-point improvement (29.31 to
29.80) and a 0.33-point improvement (18.60
to 18.93) in word BLEU-4 compared with
IST-ITG constraints, respectively. This indi-
cates the validity of the proposed reordering
model.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951">
Statistical machine translation has been wiedely ap-
plied in many state-of-the-art translation systems. A
popular statistical machine translation paradigms is
the phrase-based model (Koehn et al., 2003; Och
and Ney, 2004). In phrase-based statistical ma-
chine translation, errors in word reordering, espe-
cially global reordering, are one of the most se-
rious problems. To resolve this problem, many
</bodyText>
<page confidence="0.989665">
69
</page>
<bodyText confidence="0.999887882352941">
word-reordering constraint techniques have been
proposed. These techniques are categorized into
two types. The first type is linguistically syntax-
based. In this approach, tree structures for the source
(Quirk et al., 2005; Huang et al., 2006), target (Ya-
mada and Knight, 2000; Marcu et al., 2006), or both
(Melamed, 2004) are used for model training. The
second type is formal constraints on word permuta-
tions. IBM constraints (Berger et al., 1996), the lex-
ical word reordering model (Tillmann, 2004), and
inversion transduction grammar (ITG) constraints
(Wu, 1995; Wu, 1997) belong to this type of ap-
proach. For ITG constraints, the target-side word
order is obtained by rotating nodes of the source-
side binary tree. In these node rotations, the source
binary tree instance is not considered. Imposing
a source tree on ITG (IST-ITG) constraints (Ya-
mamoto et al., 2008) is an extension of ITG con-
straints and a hybrid of the first and second type of
approach. IST-ITG constraints directly introduce a
source sentence tree structure. Therefore, IST-ITG
can obtain stronger constraints for word reordering
than the original ITG constraints. For example, IST-
ITG constraints allows only eight word orderings for
a four-word sentence, even though twenty-two word
orderings are possible with respect to the original
ITG constraints. Although IST-ITG constraints ef-
ficiently suppress erroneous target word orderings,
the method cannot assign the probability to the tar-
get word orderings.
This paper presents a reordering model using syn-
tactic information of a source tree for phrase-based
statistical machine translation. The proposed re-
ordering model is an extension of IST-ITG con-
</bodyText>
<note confidence="0.9650995">
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 69–77,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999792764705883">
straints. In the proposed method, the target-side
word order is obtained by rotating nodes of a source-
side parse-tree in a similar fashion to IST-ITG con-
straints. We modeled the rotating positions, mono-
tone or swap, from word alignments of a training
parallel corpus and source-side parse-trees. The pro-
posed method conducts a probabilistic evaluation of
target word orderings using syntactic information of
the source tree.
The rest of this paper is organized as follows.
Section 2 describes the previous approach to re-
solving erroneous word reordering. In Section 3,
the reordering model using syntactic information of
a source tree is presented. Section 4 shows ex-
perimental results. Finally, Section 5 presnts the
summary and some concluding remarks and future
works.
</bodyText>
<sectionHeader confidence="0.988625" genericHeader="introduction">
2 Previous Works
</sectionHeader>
<bodyText confidence="0.999613666666667">
First, we introduce two previous studies on related
word reordering constraints, ITG and IST-ITG con-
straints.
</bodyText>
<subsectionHeader confidence="0.955183">
2.1 ITG Constraints
</subsectionHeader>
<bodyText confidence="0.834177625">
In one-to-one word-alignment, the source word fz
is translated into the target word ez. The source
sentence [f1, f2, · · · , fN] is translated into the tar-
get sentence which is the reordered target word se-
quence [e1, e2, · · · , eN]. The number of reorderings
is N!. When ITG constraints are introduced, this
combination N! can be reduced in accordance with
the following constraints.
</bodyText>
<listItem confidence="0.99844375">
• All possible binary tree structures are generated
from the source word sequence.
• The target sentence is obtained by rotating any
node of the binary trees.
</listItem>
<bodyText confidence="0.977665857142857">
When N = 4, the ITG constraints can reduce
the number of combinations from 4! = 24 to
22 by rejecting the combinations [e3, e1, e4, e2]
and [e2, e4, e1, e3]. For a four-word sentence, the
search space is reduced to 92% (22/24), but for
a 10-word sentence, the search space is only 6%
(206,098/3,628,800) of the original full space.
</bodyText>
<subsectionHeader confidence="0.963543">
2.2 IST-ITG Constraints
</subsectionHeader>
<bodyText confidence="0.998753714285714">
In ITG constraints, the source-side binary tree in-
stance is not considered. Therefore, if a source sen-
tence tree structure is utilized, stronger constraints
than the original ITG constraints can be created.
IST-ITG constraints directly introduce a source sen-
tence tree structure. The target sentence is obtained
with the following constraints.
</bodyText>
<listItem confidence="0.9975735">
• A source sentence tree structure is generated
from the source sentence.
• The target sentence is obtained by rotating any
node of the source sentence tree structure.
</listItem>
<bodyText confidence="0.9960078125">
By parsing the source sentence, the parse-tree is
obtained. After parsing the source sentence, a
bracketed sentence is obtained by removing the
node syntactic labels; this bracketed sentence can
then be converted into a tree structure. For example,
the parse-tree “(S1 (S (NP (DT This)) (VP (AUX
is) (NP (DT a) (NN pen)))))” is obtained from the
source sentence “This is a pen,” which consists of
four words. By removing the node syntactic labels,
the bracketed sentence “((This) ((is) ((a) (pen))))”
is obtained. Such a bracketed sentence can be used
to produce constraints. If IST-ITG constraints is
applied, the number of word orderings in N = 4
is reduced to 8, down from 22 with ITG cn-
straints. For example, for the source-side bracketed
tree “((f1f2) (f3f4)),” the eight target sequences
</bodyText>
<equation confidence="0.945622714285714">
[e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3],
[e2, e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1],
[e4, e3, e1, e2], and [e4, e3, e2, e1] are accepted. For
the source-side bracketed tree “(((f1f2) f3) f4),”
the eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4],
[e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3],
[e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2, e1] are
</equation>
<listItem confidence="0.78238225">
accepted. When the source sentence tree structure
is a binary tree, the number of word orderings is
reduced to 2N−1. The parsing results sometimes do
not produce binary trees. In this case, some subtrees
have more than two child nodes. For a non-binary
subtree, any reordering of child nodes is allowed. If
a subtree has three child nodes, six reorderings of
the nodes are accepted.
</listItem>
<bodyText confidence="0.998517666666667">
In phrase-based statistical machine translation, a
source “phrase” is translated into a target “phrase”.
However, with IST-ITG constraints, “word” must be
</bodyText>
<page confidence="0.996148">
70
</page>
<bodyText confidence="0.9994305">
used for the constraint unit since the parse unit is a
“word”. To absorb different units between transla-
tion models and IST-ITG constraints, a new limita-
tion for word reordering is applied.
</bodyText>
<listItem confidence="0.974147">
• Word ordering that destroys a phrase is not al-
lowed.
</listItem>
<bodyText confidence="0.814551">
When this limitation is applied, the translated word
ordering is obtained from the bracketed source sen-
tence tree by reordering the nodes in the tree, which
is the same as for one-to-one word-alignment.
</bodyText>
<sectionHeader confidence="0.995821" genericHeader="method">
3 Reordering Model Using Syntactic
</sectionHeader>
<subsectionHeader confidence="0.652343">
Information of the Source Tree
</subsectionHeader>
<bodyText confidence="0.999767666666667">
In this section, we present a new reordering model
using syntactic information of a source-side parse-
tree.
</bodyText>
<subsectionHeader confidence="0.999892">
3.1 Abstract of Proposed Method
</subsectionHeader>
<bodyText confidence="0.999711035714286">
The IST-ITG constraints method efficiently sup-
presses erroneous target word orderings. However,
IST-ITG constraints cannot evaluate the accuracy of
the target word orderings; i.e., IST-ITG constraints
assign an equal probability to all target word order-
ings. This paper proposes a reordering model us-
ing syntactic information of the source tree as an
extension of IST-ITG constraints. The proposed re-
ordering model conducts a probabilistic evaluation
of target word orderings using syntactic information
of the source-side parse-tree.
In the proposed method, the target-side word or-
der is obtained by rotating nodes of the source-
side parse-tree in a similar fashion to IST-ITG con-
straints. Reordering probabilities are assigned to
each subtree of source-side parse-tree 5 by reorder-
ing the positions into two types: monotone and
swap. If the subtree has more than two child nodes,
the number of child node order is more than two.
However, we assume the child node order other than
monotone to be swap. The source-side parse-tree
5 consists of subtrees {s1, s2, • • • , sK}, where K
is the number of subtrees included in the source-
side parse-tree. The subtree sk is which is repre-
sented by the parent node’s syntactic label and the
order, from sentence head to sentence tail, of the
child node’s syntactic labels. For example, Fig-
ure 1 shows a source-side parse-tree for a four-word
</bodyText>
<figure confidence="0.969358833333334">
Source-side parse-tree
S
NP VP
AUX NP
DT NN
Source sentence
</figure>
<figureCaption confidence="0.9997105">
Figure 1: Example of a source-side parse-tree fo a four-
word source sentence consisting of three subtrees.
</figureCaption>
<bodyText confidence="0.999850857142857">
source sentence consisting of three subtrees. In Fig-
ure 1, the subtrees s1, s2, and s3 are represented by
S+NP+VP, VP+AUX+NP, and NP+DT+NN, re-
spectively. Each subtree has a probability P(t  |sk),
where t is monotone (m) or swap (s). The proba-
bility of the target word reordering is calculated as
follows.
</bodyText>
<equation confidence="0.997104666666667">
K
Pr = ∏ P(t  |sk) (1)
k=1
</equation>
<bodyText confidence="0.999816666666667">
Each target candidate is assigned the different re-
ordering probability by Equation (1). Since the pro-
posed reordering model uses the syntactic labels,
which is not considered in IST-ITG constraints, the
different parse-tree assigns the different reordering
probability. The proposed model is effective for
global word reordering, because reordering proba-
bilities are also assigned to higher-level subtrees of
the source-side parse-tree.
</bodyText>
<subsectionHeader confidence="0.999772">
3.2 Training of the Proposed Model
</subsectionHeader>
<bodyText confidence="0.993588625">
We modeled monotone or swap node rotating auto-
matically from word alignments of a training paral-
lel corpus and source-side parse-trees. The training
algorithm for the proposed reordering model is as
follows.
1. The training process begins with a word-
aligned corpus. We obtained the word align-
ments using Koehn et al.’s method (2003),
</bodyText>
<page confidence="0.980297">
71
</page>
<figure confidence="0.926933117647059">
Subtree type Monotone probability
S+PP+,+NP+VP+. 0.764
PP+IN+NP 0.816
NP+DT+NN+NN 0.664
VP+AUX+VP 0.864
VP+VBN+PP 0.837
NP+NP+PP 0.805
NP+DT+JJ+NN 0.653
NP+DT+JJ+VBP+NN 0.412
NP+DT+NN+CC+VB 0.357
Table 1: Example of proposed reordering models.
4
2,3,4
2,3
2
3
1
</figure>
<figureCaption confidence="0.98157">
Figure 2: Example of a source-side parse-tree with word
alignments using the training algorithm of the proposed
model.
</figureCaption>
<bodyText confidence="0.999648666666667">
which is based on Och and Ney’s work (2004).
This involves running GIZA++ (Och and Ney,
2003) on the corpus in both directions, and ap-
plying refinement rules (the variant they desig-
nate is “final-and”) to obtain a single many-to-
many word alignment for each sentence.
</bodyText>
<listItem confidence="0.971114181818182">
2. Source-side parse-trees are created using a
source language phrase structure parser, which
annotates each node with a syntactic label. A
source-side parse-tree consists of several sub-
trees with syntactic labels. For example, the
parse-tree “(S1 (S (NP (DT This)) (VP (AUX
is) (NP (DT a) (NN pen)))))” is obtained from
the source sentence “This is a pen” which con-
sists of four words.
3. Word alignments and source-side parse-trees
are combined. Leaf nodes are assigned target
word positions obtained from word alignments.
Via the bottom-up process, target word posi-
tions are assigned to all nodes. For example,
in Figure 2, the left-side (sentence head) child
node of subtree s2 is assigned the target word
position “4,” and the right-side (sentence tail)
child node is assigned the target word positions
“2” and “3,” which are assigned to the child
nodes of subtree s3.
4. The monotone and swap reordering positions
are checked and counted for each subtree. By
</listItem>
<bodyText confidence="0.915994818181818">
comparing the target word positions, which are
assigned in the above step, the reordering posi-
tion is determined. If the target word position
of the left-side child node is smaller than one of
the right-side child node, the reordering posi-
tion determined as monotone. For example, in
Figure 2, the subtrees s1, s2 and s3 are mono-
tone, swap, and monotone, respectively.
5. The reordering probability of the subtree can
be directly estimated by counting the reorder-
ing positions in the training data.
</bodyText>
<equation confidence="0.993522">
P(t  |s) = ct (s) (2)
∑t ct(s)
</equation>
<bodyText confidence="0.931808916666667">
where ct(s) is the count of reordering positon t
included all training samples for the subtree s.
The parsing results sometimes do not produce bi-
nary trees. For a non-binary subtree, any reorder-
ing of child nodes is allowed. However, the pro-
posed reordering model assumes that reordering po-
sitions are only two, monotone and swap. That
is, the reordering position which the order of child
nodes do not change is monotone, and the other po-
sitions are swap. Therefore, the probability of swap
P(s  |sk) is derived from the probability of mono-
tone P(m  |sk) as follows.
</bodyText>
<equation confidence="0.995908">
P(s  |sk) = 1.0 − P(m  |sk) (3)
</equation>
<bodyText confidence="0.9971645">
Table 1 shows the example of proposed reordering
models.
If a subtree is represented by a binary-tree, there
are L3 possible subtrees, where L is the number of
</bodyText>
<page confidence="0.99494">
72
</page>
<figureCaption confidence="0.999364">
Figure 3: Example of a target word order which is not
derived from rotating the nodes of source-side parse trees.
Figure 4: Example of a target candidate including a
phrase.
</figureCaption>
<bodyText confidence="0.9991398">
syntactic labels. However, in the possible subtrees,
there are subtrees observed only a few times in train-
ing sentences, especially when the subtree consists
of more than three child nodes. Although a large
number of subtree models can capture variations in
the training samples, too many models lead to the
over-fitting problem. Therefore, subtrees where the
number of training samples is less than a heuristic
threshold and unseen subtrees are clustered to deal
with the data sparseness problem for robust model
estimations.
After creating word alignments of a training par-
allel corpus, there are target word orders which are
not derived from rotating nodes of source-side parse-
trees. Figure 3 shows a sample which is not derived
from rotating nodes. Some are due to linguistic rea-
sons, structual differences such as negation (French
“ne...pas” and English “not”), adverb, modal and so
on. Others are due to non-linguistic reasons, er-
rors of automatic word alignments, syntactic anal-
ysis, or human translation (Fox, 2002). The pro-
posed method discards such problematic cases. In
Figure 3, the subtree s1 is then removed from train-
ing samples, and the subtrees s2 and s3 are used as
training samples.
</bodyText>
<subsectionHeader confidence="0.964166">
3.3 Decoding Using the Proposed Reordering
Model
</subsectionHeader>
<bodyText confidence="0.999881">
In this section, we describe a one-pass phrase-based
decoding algorithm that uses the proposed reorder-
ing model in the decoder. The translation target sen-
tence is sequentially generated from left (sentence
head) to right (sentence tail), and all reordering is
conducted on the source side. To introduce the pro-
posed reordering model into the decoder, the target
candidate must be checked for whether the reorder-
ing position of a subtree is either monotone or swap
whenever a new phrase is selected to extend a target
candidate. The checking algorithm is as follows.
</bodyText>
<listItem confidence="0.9450308">
1. For old translation candidates, the subtree s,
which includes both translated and untranslated
words, and its untranslated part u are calcu-
lated.
2. When a new target phrase e¯ is generated, the
source phrase f and the untranslated part u cal-
culated in the above step are compared. If the
source phrase f does not include the untrans-
lated part u and is not included u, the new can-
didate is rejected.
3. In the accepted candidate, the reordering po-
sitions for all subtrees included the source side
parse-tree are checked by comparing the source
phrase f¯ with the source phrase sequence used
before.
</listItem>
<bodyText confidence="0.998843888888889">
Subtrees checked reordering positions are assigned a
probability–monotone or swap–by the proposed re-
ordering model, and the target word order is evalu-
ated by Equation (1).
Phrase-based statistical machine translation uses
a “phrase” as the translation unit. However, the pro-
posed reordering model needs a “word” order. Be-
cause “word” alignments form the source phrase to
target phrase are not clear, we cannot determine the
</bodyText>
<page confidence="0.99733">
73
</page>
<table confidence="0.998419285714286">
English Japanese
Train Sentences 1.0M
Words 24.6M 24.6M
Dev Sentences 2.0K
Words 50.1K 58.7K
Test Sentences 2.0K
Words 49.5K 58.0K
</table>
<tableCaption confidence="0.980366">
Table 2: Statistics of training, development and test cor-
pus for E-J translation.
</tableCaption>
<figureCaption confidence="0.9988365">
Figure 5: Example of a non-binary subtree including a
phrase.
</figureCaption>
<bodyText confidence="0.999971111111111">
reordering position of subtree included in a phrase.
Therefore, in the decoding process using the pro-
posed reordering model, we define that higher prob-
ability, monotone or swap, are assigned to subtrees
included in a source phrase. For example, in Fig-
ure 4, the source sentence [[f1, f2], f3, f4] is trans-
lated into the target sentence [[e1, e2], e4, e3], where
[f1, f2] and [e1, e2] are used as phrases. Then, the
source phrase [f1, f2] includes the subtree s2. If the
monotone probabilities of subtrees s1, s2, and s3 are
0.8, 0.4 and 0.7, the proposed reordering probabil-
ity is 0.8 × 0.6 × 0.3 = 0.144. If a source phrase
is [f1, f2, f3, f4] and a source-side parse-tree has the
same tree structure used in Figure 4, the subtrees s1,
s2, and s3 are assigned higher reordering probabili-
ties. If the source phrase [f1, f2, f3, f4] used in Fig-
ure 4, the subtrees s1, s2, and s3 are assigned higher
reordering probabilities.
Non-binary subtrees are often observed in the
source-side parse-tree. When a source phrase f¯ is
included in a non-binary subtree and does not in-
clude a non-binary subtree, we cannot determine the
reordering position. For example, the reordering po-
sition of subtree s2 in Figure 5, which includes the
phrase [f3, f4], can not be determined. In this case,
we define that such subtrees are also to be assigned
a higher probability.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999391">
To evaluate the proposed model, we conducted two
experiments: English-to-Japanese and English-to-
Chinese translation.
</bodyText>
<subsectionHeader confidence="0.9897245">
4.1 English-to-Japanese Paper Abstract
Translation Experiments
</subsectionHeader>
<bodyText confidence="0.99995646875">
The first experiment was the English-to-Japanese
(E-J) translation. Table 2 shows the training, de-
velopment and test corpus statistics. JST Japanese-
English paper abstract corpus consists of 1.0M
parallel sentences were used for model training.
This corpus was constructed from 2.0M Japanese-
English paper abstract corpus belongs to JST by
NICT using the method of Uchiyama and Isahara
(2007). For phrase-based translation model training,
we used the GIZA++ toolkit (Och and Ney, 2003),
and 1.0M bilingual sentences. For language model
training, we used the SRI language model toolkit
(Stolcke, 2002), and 1.0M sentences for the trans-
lation model training. The language model type was
word 5-gram smoothed by Kneser-Ney discounting
(Kneser and Ney, 1995). To tune the decoder pa-
rameters, we conducted minimum error rate training
(Och, 2003) with respect to the word BLEU score
(Papineni et al., 2002) using 2.0K development sen-
tence pairs. The test set with 2.0K sentences is used.
In the evaluation and development sets, a single ref-
erence was used. For the creation of English sen-
tence parse trees and segmentation of the English,
we used the Charniak parser (Charniak, 2000). We
used Chasen for segmentation of the Japanese sen-
tences. For decoding, we used an in-house decoder
that is a close relative of the Moses decoder. The
performance of this decoder was configured to be
the same as Moses. Other conditions were the same
as the default conditions of the Moses decoder.
In this experiment, the following three methods
were compared.
</bodyText>
<listItem confidence="0.9887185">
• Baseline : The IBM constraints and the lexi-
cal reordering model were used for target word
</listItem>
<page confidence="0.992651">
74
</page>
<table confidence="0.84992">
Baseline IST-ITG Proposed
BLEU 27.87 29.31 29.80
</table>
<tableCaption confidence="0.8893135">
Table 3: BLEU score results for E-J translation. (1-
reference)
</tableCaption>
<bodyText confidence="0.581294">
reordering.
</bodyText>
<listItem confidence="0.9161112">
• IST-ITG : The IST-ITG constraints, the IBM
constraints, and the lexical reordering model
were used for target word reordering.
• Proposed : The proposed reordering model,
the IBM constraints, and the lexical reordering
</listItem>
<bodyText confidence="0.9950645">
model were used for target word reordering.
During minimum error training, each method used
each reordering model and reordering constraint.
The proposed reordering model are trained from
1.0M bilingual sentences for the translation model
training. The amount of available training samples
represented by subtrees was 9.8M. In the available
training samples, there were 54K subtree types. The
heuristic threshold was 10, and subtrees with train-
ing samples of less than 10 were clustered. The pro-
posed reordering model consisted of 5,960 subtrees
types and one clustered model “other”. The models
not including “other” covered 99.29% of all training
samples.
The BLEU scores are presented in Table 3.
In comparing “Baseline” method with “IST-ITG”
method, the improvement in BLEU was a 1.44-
point. Furthermore, in comparing “IST-ITG”
method with “Proposed” method, the improvement
in BLEU was a 0.49-point. Both the IST-ITG con-
straints and the proposed reordering model fixed the
phrase position for the global reorderings. How-
ever, the proposed method can conduct a probabilis-
tic evaluation of target word reorderings which the
IST-ITG constraints cannot. Therefore, “Proposed”
method resulted in a better BLEU.
</bodyText>
<subsectionHeader confidence="0.785065">
4.2 NIST MT08 English-to-Chinese
Translation Experiments
</subsectionHeader>
<bodyText confidence="0.9999386">
Next, we conducted English-to-Chinese (E-C) news-
paper translation experiments for different lan-
guage pairs. The NIST MT08 evaluation campaign
English-to-Chinese translation track was used for
the training and evaluation corpora. Table 4 shows
</bodyText>
<table confidence="0.9985108">
English Chinese
Train Sentences 4.6M
Words
79.6M 73.4M
Dev Sentences 1.6K
Words
46.4K 39.0K
Test Sentences 1.9K
Words
45.7K 47.0K (Ave.)
</table>
<tableCaption confidence="0.989722">
Table 4: Statistics of training, development and test cor-
pus for E-C translation.
</tableCaption>
<table confidence="0.998699">
Baseline IST-ITG Proposed
BLEU 17.54 18.60 18.93
</table>
<tableCaption confidence="0.9573105">
Table 5: BLEU score results for E-C translation. (4-
reference)
</tableCaption>
<bodyText confidence="0.999901193548387">
the training, development and test corpus statistics.
For the translation model training, we used 4.6M
bilingual sentences. For the language model train-
ing, we used 4.6M sentences which are used for
the translation model training. The language model
type was word 3-gram smoothed by Kneser-Ney
discounting. A development set with 1.6K sen-
tences was used as evaluation data in the Chinese-to-
English translation track for the NIST MT07 eval-
uation campaign. A single reference was used in
the development set. The evaluation set with 1.9K
sentences is the same as the MT08 evaluation data,
with 4 references. In this experiment, the compared
methods were the same as in the E-J experiment.
The proposed reordering model are trained from
4.6M bilingual sentences for the translation model
training. The amount of available training samples
represented by subtrees was 39.6M. In the available
training samples, there were 193K subtree types.
As in the E-J experiments, the heuristic threshold
was 10. The proposed reordering model consisted
of 18,955 subtree types and one clustered model
“other.” The models not including “other” covered
99.45% of all training samples.
The BLEU scores are presented in Table 5.
In comparing “Baseline” method with “IST-ITG”
method, the improvement in BLEU was a 1.06-
point. In comparing “IST-ITG” method with “Pro-
posed” method, the improvement in BLEU was a
0.33-point. As in the E-J experiments, “Proposed”
method performed the highest BLEU. We demon-
</bodyText>
<page confidence="0.995959">
75
</page>
<bodyText confidence="0.999937875">
strated that the proposed method is effective for mul-
tiple language pairs. However, the improvement
of BLEU score in E-C translation is smaller than
the improvement in E-J translation, because English
and Chinese are similar sentence structures, such as
SVO-languages (Japanese is SOV-language). When
the sentence structures are different, the proposed re-
ordering model is effective.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985777777778">
This paper proposed a new word reordering model
using syntactic information of a source tree for
phrase-based statistical machine translation. The
proposed model is an extension of the IST-ITG con-
straints. In both IST-ITG constraints and the pro-
posed method, the target-side word order is obtained
by rotating nodes of the source-side tree structure.
Both the IST-ITG constraints and the proposed re-
ordering model fix the phrase position for the global
reorderings. However, the proposed method can
conduct a probabilistic evaluation of target word re-
orderings which the IST-ITG constraints cannot. In
E-J and E-C translation experiments, the proposed
method resulted in a 0.49-point improvement (29.31
to 29.80) and a 0.33-point improvement (18.60 to
18.93) in word BLEU-4 compared with IST-ITG
constraints, respectively. This indicates the validity
of the proposed reordering model.
Future work will focus on a reduction of com-
putational cost of decoding including the proposed
reordering model, and a simultaneous training of
translation and reordering models. Moreover, we
will deal with difference between source and target
in multi level like in Gally et al. (2004).
The improvement could clearly be seen from vi-
sual inspection of the output, a few examples of
which are presented in the following Appendix.
</bodyText>
<sectionHeader confidence="0.9655995" genericHeader="method">
A Samples from the English-to-Japanese
Translation
</sectionHeader>
<subsectionHeader confidence="0.7072">
A.1 Sentence 1
</subsectionHeader>
<bodyText confidence="0.9739026">
Source: Aggravation was obvious from the latter
half of March to the end of April, and he contracted
the disease in February to the beginning of May.
Baseline:
Proposed:
</bodyText>
<sectionHeader confidence="0.552033" genericHeader="method">
A.2 Sentence 2
</sectionHeader>
<bodyText confidence="0.9989345">
Source: The value of TF, on the other hand, was
higher in the reverse order, indicating that high ox-
idation rate causes severe defects on the surface of
Ni crystallites.
</bodyText>
<sectionHeader confidence="0.588277666666667" genericHeader="method">
Baseline:
Reference:
Proposed:
</sectionHeader>
<subsectionHeader confidence="0.376947">
A.3 Sentence 3
</subsectionHeader>
<bodyText confidence="0.95719475">
Source: After diagnosing the pleural effusion and
ascites, vein catheter was left in place under the echo
guide, and after removing the pleural effusion and
ascites, OK-432 was administered locally.
</bodyText>
<sectionHeader confidence="0.640272666666667" genericHeader="method">
Baseline:
Reference:
Proposed:
</sectionHeader>
<subsectionHeader confidence="0.35097">
A.4 Sentence 4
</subsectionHeader>
<bodyText confidence="0.975505">
Source: From result of the consideration, it was
pointed that radiation from the loop elements was
weak.
</bodyText>
<sectionHeader confidence="0.82446" genericHeader="method">
Baseline:
Reference:
Proposed:
Reference:
</sectionHeader>
<page confidence="0.979172">
76
</page>
<sectionHeader confidence="0.998246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999581064935065">
Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,
Vincent J. Della Pietra, Andrew S. Kehler, and Robert
L. Mercer 1996. Language translation apparatus
and method of using context-based translation models.
United States patent, patent number 5510981.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL 2000, pages 132–
139.
Chasen
http://chasen-legacy.sourceforge.jp/
Heidi J. Fox, 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP, pages
304–311.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Extended
Domain of Locality. In Proceedings of AMTA.
Japanese-English paper abstract corpus
http://www.jst.go.jp
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language model In Proceed-
ings of ICASSP 1995, pages 181–184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127–133.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Machine
Translation with Syntactified Target Language Phrases
In Proceedings of EMNLP2006, pages 44–52.
Dan Melamed. 2004. Statistical machine translation by
parsing In Proceedings of ACL, pages 653–660.
Moses
http://www.statmt.org/moses/
Franz josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1), pages 19–51.
Franz josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Franz josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4), pages 417–
449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL, pages 271–279.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318.
Andreas Stolcke. 2002. SRILM - An Ex-
tensible Language Model Toolkit In Pro-
ceedings of ICSLP2002, pages 901–904.
http://www.speech.sri.com/projects/srilm/
Christopher Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proceed-
ings of HLT-NAACL, pages 101–104.
Masao Uchiyama and Hitoshi Isahara. 2007. 2007. A
japanese-english patent parallel corpus. In MT sum-
mit XI, pages 475–482.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAI, pages 1328–1334.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguiatics, 23(3), pages 377–403.
Kenji Yamada and Kevin Knight. 2000. A syntax-based
statistical translation model In Proceedings of ACL,
pages 523–530.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing Constraints from the Source
Tree on ITG Constraints for SMT. In Proceedings of
ACL : HLT Second Workshop on Syntax and Structure
in Statistical Translation (SSST-2), pages 1–9.
</reference>
<page confidence="0.999032">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.375332">
<title confidence="0.9991055">Reordering Model Using Syntactic Information of a Source Tree Statistical Machine Translation</title>
<author confidence="0.761597">Hirohumi Hideo</author>
<affiliation confidence="0.8859115">and Keiichi Institute of Technology Department of Computer Science and / Gokiso-cho Syouwa-ku Nagoya-city Aichi Institute of Information and Communications University School of Science and Engineering Department of Spoken Language Communication Research Labs.</affiliation>
<abstract confidence="0.99896332">This paper presents a reordering model using syntactic information of a source tree for phrase-based statistical machine translation. The proposed model is an extension of IST- ITG (imposing source tree on inversion transduction grammar) constraints. In the proposed method, the target-side word order is obtained by rotating nodes of the source-side parse-tree. We modeled the node rotation, monotone or swap, using word alignments based on a training parallel corpus and sourceside parse-trees. The model efficiently suppresses erroneous target word orderings, especially global orderings. Furthermore, the proposed method conducts a probabilistic evaluation of target word reorderings. In Englishto-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Andrew S Kehler</author>
<author>Robert L Mercer</author>
</authors>
<title>Language translation apparatus and method of using context-based translation models. United States patent, patent number 5510981.</title>
<date>1996</date>
<contexts>
<context position="2396" citStr="Berger et al., 1996" startWordPosition="340" endWordPosition="343">rase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the first and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Kehler, Mercer, 1996</marker>
<rawString>Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and Robert L. Mercer 1996. Language translation apparatus and method of using context-based translation models. United States patent, patent number 5510981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL 2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="20116" citStr="Charniak, 2000" startWordPosition="3244" endWordPosition="3245">language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this decoder was configured to be the same as Moses. Other conditions were the same as the default conditions of the Moses decoder. In this experiment, the following three methods were compared. • Baseline : The IBM constraints and the lexical reordering model were used for target word 74 Baseline IST-ITG Proposed BLEU 27.87 29.31 29.80 Table 3: BLEU score results for E-J translation. (1- reference) reordering. • IST-ITG : The </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of NAACL 2000, pages 132– 139.</rawString>
</citation>
<citation valid="false">
<note>Chasen http://chasen-legacy.sourceforge.jp/</note>
<marker></marker>
<rawString>Chasen http://chasen-legacy.sourceforge.jp/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="15295" citStr="Fox, 2002" startWordPosition="2452" endWordPosition="2453">hreshold and unseen subtrees are clustered to deal with the data sparseness problem for robust model estimations. After creating word alignments of a training parallel corpus, there are target word orders which are not derived from rotating nodes of source-side parsetrees. Figure 3 shows a sample which is not derived from rotating nodes. Some are due to linguistic reasons, structual differences such as negation (French “ne...pas” and English “not”), adverb, modal and so on. Others are due to non-linguistic reasons, errors of automatic word alignments, syntactic analysis, or human translation (Fox, 2002). The proposed method discards such problematic cases. In Figure 3, the subtree s1 is then removed from training samples, and the subtrees s2 and s3 are used as training samples. 3.3 Decoding Using the Proposed Reordering Model In this section, we describe a one-pass phrase-based decoding algorithm that uses the proposed reordering model in the decoder. The translation target sentence is sequentially generated from left (sentence head) to right (sentence tail), and all reordering is conducted on the source side. To introduce the proposed reordering model into the decoder, the target candidate </context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox, 2002. Phrasal cohesion and statistical machine translation. In Proceedings of EMNLP, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL-04.</booktitle>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT/NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical Syntax-Directed Translation with Extended Domain of Locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="2190" citStr="Huang et al., 2006" startWordPosition="305" endWordPosition="308">anslation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constrai</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical Syntax-Directed Translation with Extended Domain of Locality. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="false">
<note>Japanese-English paper abstract corpus http://www.jst.go.jp</note>
<marker></marker>
<rawString>Japanese-English paper abstract corpus http://www.jst.go.jp</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language model</title>
<date>1995</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<pages>181--184</pages>
<contexts>
<context position="19693" citStr="Kneser and Ney, 1995" startWordPosition="3170" endWordPosition="3173">ics. JST JapaneseEnglish paper abstract corpus consists of 1.0M parallel sentences were used for model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this dec</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language model In Proceedings of ICASSP 1995, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1749" citStr="Koehn et al., 2003" startWordPosition="237" endWordPosition="240">e, the proposed method conducts a probabilistic evaluation of target word reorderings. In Englishto-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model. 1 Introduction Statistical machine translation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word perm</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases In</title>
<date>2006</date>
<booktitle>Proceedings of EMNLP2006,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="2244" citStr="Marcu et al., 2006" startWordPosition="315" endWordPosition="318">e-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG con</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases In Proceedings of EMNLP2006, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>653--660</pages>
<contexts>
<context position="2269" citStr="Melamed, 2004" startWordPosition="321" endWordPosition="322">opular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of </context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>Dan Melamed. 2004. Statistical machine translation by parsing In Proceedings of ACL, pages 653–660. Moses</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context position="11672" citStr="Och and Ney, 2003" startWordPosition="1845" endWordPosition="1848">is as follows. 1. The training process begins with a wordaligned corpus. We obtained the word alignments using Koehn et al.’s method (2003), 71 Subtree type Monotone probability S+PP+,+NP+VP+. 0.764 PP+IN+NP 0.816 NP+DT+NN+NN 0.664 VP+AUX+VP 0.864 VP+VBN+PP 0.837 NP+NP+PP 0.805 NP+DT+JJ+NN 0.653 NP+DT+JJ+VBP+NN 0.412 NP+DT+NN+CC+VB 0.357 Table 1: Example of proposed reordering models. 4 2,3,4 2,3 2 3 1 Figure 2: Example of a source-side parse-tree with word alignments using the training algorithm of the proposed model. which is based on Och and Ney’s work (2004). This involves running GIZA++ (Och and Ney, 2003) on the corpus in both directions, and applying refinement rules (the variant they designate is “final-and”) to obtain a single many-tomany word alignment for each sentence. 2. Source-side parse-trees are created using a source language phrase structure parser, which annotates each node with a syntactic label. A source-side parse-tree consists of several subtrees with syntactic labels. For example, the parse-tree “(S1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen” which consists of four words. 3. Word alignments and source-side pars</context>
<context position="19424" citStr="Och and Ney, 2003" startWordPosition="3129" endWordPosition="3132">two experiments: English-to-Japanese and English-toChinese translation. 4.1 English-to-Japanese Paper Abstract Translation Experiments The first experiment was the English-to-Japanese (E-J) translation. Table 2 shows the training, development and test corpus statistics. JST JapaneseEnglish paper abstract corpus consists of 1.0M parallel sentences were used for model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English senten</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1), pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19779" citStr="Och, 2003" startWordPosition="3186" endWordPosition="3187">model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this decoder was configured to be the same as Moses. Other conditions were the same as the def</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>417--449</pages>
<contexts>
<context position="1769" citStr="Och and Ney, 2004" startWordPosition="241" endWordPosition="244">od conducts a probabilistic evaluation of target word reorderings. In Englishto-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model. 1 Introduction Statistical machine translation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constr</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4), pages 417– 449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="2169" citStr="Quirk et al., 2005" startWordPosition="301" endWordPosition="304">atistical machine translation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on I</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of ACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19839" citStr="Papineni et al., 2002" startWordPosition="3195" endWordPosition="3198"> 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this decoder was configured to be the same as Moses. Other conditions were the same as the default conditions of the Moses decoder. In this experiment, th</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Model Toolkit In</title>
<date>2002</date>
<booktitle>Proceedings of ICSLP2002,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="19539" citStr="Stolcke, 2002" startWordPosition="3148" endWordPosition="3149">ion Experiments The first experiment was the English-to-Japanese (E-J) translation. Table 2 shows the training, development and test corpus statistics. JST JapaneseEnglish paper abstract corpus consists of 1.0M parallel sentences were used for model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for se</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Model Toolkit In Proceedings of ICSLP2002, pages 901–904. http://www.speech.sri.com/projects/srilm/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="2448" citStr="Tillmann, 2004" startWordPosition="350" endWordPosition="351">d reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the first and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reorder</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christopher Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Uchiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A japanese-english patent parallel corpus.</title>
<date>2007</date>
<booktitle>In MT summit XI,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="19331" citStr="Uchiyama and Isahara (2007)" startWordPosition="3115" endWordPosition="3118"> also to be assigned a higher probability. 4 Experiments To evaluate the proposed model, we conducted two experiments: English-to-Japanese and English-toChinese translation. 4.1 English-to-Japanese Paper Abstract Translation Experiments The first experiment was the English-to-Japanese (E-J) translation. Table 2 shows the training, development and test corpus statistics. JST JapaneseEnglish paper abstract corpus consists of 1.0M parallel sentences were used for model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the eva</context>
</contexts>
<marker>Uchiyama, Isahara, 2007</marker>
<rawString>Masao Uchiyama and Hitoshi Isahara. 2007. 2007. A japanese-english patent parallel corpus. In MT summit XI, pages 475–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1328--1334</pages>
<contexts>
<context position="2512" citStr="Wu, 1995" startWordPosition="358" endWordPosition="359"> problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the first and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reordering than the original ITG constraints. For example, ISTITG const</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proceedings of IJCAI, pages 1328–1334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguiatics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="2523" citStr="Wu, 1997" startWordPosition="360" endWordPosition="361"> To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the first and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reordering than the original ITG constraints. For example, ISTITG constraints allo</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguiatics, 23(3), pages 377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2223" citStr="Yamada and Knight, 2000" startWordPosition="310" endWordPosition="314">plied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The first type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an</context>
</contexts>
<marker>Yamada, Knight, 2000</marker>
<rawString>Kenji Yamada and Kevin Knight. 2000. A syntax-based statistical translation model In Proceedings of ACL, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Hideo Okuma</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Imposing Constraints from the Source Tree on ITG Constraints for SMT.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL : HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>1--9</pages>
<contexts>
<context position="2817" citStr="Yamamoto et al., 2008" startWordPosition="407" endWordPosition="411">get (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the first and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reordering than the original ITG constraints. For example, ISTITG constraints allows only eight word orderings for a four-word sentence, even though twenty-two word orderings are possible with respect to the original ITG constraints. Although IST-ITG constraints efficiently suppress erroneous target word orderings, the method cannot assign the probability to the target word</context>
</contexts>
<marker>Yamamoto, Okuma, Sumita, 2008</marker>
<rawString>Hirofumi Yamamoto, Hideo Okuma, and Eiichiro Sumita. 2008. Imposing Constraints from the Source Tree on ITG Constraints for SMT. In Proceedings of ACL : HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 1–9.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>