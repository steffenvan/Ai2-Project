<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.188163">
<title confidence="0.992109">
Using the Wiktionary Graph Structure for Synonym Detection
</title>
<author confidence="0.999907">
Timothy Weale, Chris Brew, Eric Fosler-Lussier
</author>
<affiliation confidence="0.9949035">
Department of Computer Science and Engineering
The Ohio State University
</affiliation>
<email confidence="0.997583">
{weale,cbrew,fosler}@cse.ohio-state.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582454545455">
This paper presents our work on using
the graph structure of Wiktionary for syn-
onym detection. We implement seman-
tic relatedness metrics using both a direct
measure of information flow on the graph
and a comparison of the list of vertices
found to be “close” to a given vertex. Our
algorithms, evaluated on ESL 50, TOEFL
80 and RDWP 300 data sets, perform bet-
ter than or comparable to existing seman-
tic relatedness measures.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980142857143">
The recent creation of large-scale, collabora-
tively constructed semantic resources provides re-
searchers with cheap, easily accessible informa-
tion. Previous metrics used for synonym detec-
tion had to be built using co-occurrence statistics
of collected corpora (Higgins, 2004) or expensive,
expert-created resources such as WordNet or Ro-
get’s Thesaurus (Jarmasz and Szpakowicz, 2003).
Here, we evaluate the effectiveness of Wiktionary,
a collaboratively constructed resource, as a source
of semantic relatedness information for the syn-
onym detection problem.
Researching these metrics is important because
they have been empirically shown to improve per-
formance in a variety of NLP applications, includ-
ing word sense disambiguation (Turdakov and Ve-
likhov, 2008), real-world spelling errors (Budan-
itsky and Hirst, 2006) and coreference resolution
(Strube and Ponzetto, 2006).
Synonym detection is a recognized testbed
for comparing semantic relatedness metrics (e.g
(Zesch et al., 2008)). In this task, a target word
or phrase is presented to the system, which is then
presented with four alternative words or phrases.
The goal of the system is to pick the alternative
most related to the target. Example questions can
be found in Figure 1.
Through the Wikimedia Foundation,1 volun-
teers have created two large-scale, collaborative
resources that have been used in previous related-
ness research – Wikipedia (an encyclopedia) and
Wiktionary (a dictionary). These sources have
been used for synonym detection and replicating
human relatedness evaluations using the category
structure (Strube and Ponzetto, 2006), local link
structure (Milne and Witten, 2008) and (Turdakov
and Velikhov, 2008) and global features (Zesch et
al., 2008). They contain related information but
focus on different information needs; which infor-
mation source provides better results depends on
the needs of the task. We use Wiktionary which,
due to its role as a dictionary, focuses on common
words and definitions – the type of information
found in our synonym detection problems.
Both Wikipedia and Wiktionary are organized
around a basic “page” unit, containing informa-
tion about an individual word, phrase or entity
in the world – definitions, thesaurus entries, pro-
nunciation guides and translations in Wiktionary
and general biographical, organizational or philo-
sophical information in Wikipedia. In both data
sets, pages are linked to each each other and to
a user-created category structure – a graph struc-
ture where pages are vertices of the graph and page
links are the graph edges. We will leverage this
graph for determining relatedness.
</bodyText>
<footnote confidence="0.954983">
1http://www.wikimedia.org/
</footnote>
<subsectionHeader confidence="0.980959">
Source Word Alternative Words
</subsectionHeader>
<bodyText confidence="0.968171333333333">
make earn, print, trade, borrow
flawed imperfect, tiny, lustrous, crude
solitary alone, alert, restless, fearless
</bodyText>
<figureCaption confidence="0.996658">
Figure 1: Example TOEFL Questions
</figureCaption>
<page confidence="0.98598">
28
</page>
<note confidence="0.992072">
Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 28–31,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.934577" genericHeader="method">
2 Extracting Relatedness Measures
</sectionHeader>
<bodyText confidence="0.9999756">
We define relatedness based on information flow
through the entire Wiktionary graph, rather than
by any local in-bound or out-bound link structure.
This provides a global measurement of vertex im-
portance, as we do not limit the approach to com-
paring immediate neighbors.
To do this, we first run the PageRank algorithm
(Brin and Page, 1998) iteratively over the graph
until convergence to measure the a priori impor-
tance of each vertex in graph:
</bodyText>
<equation confidence="0.9194415">
� )
P~ Rt+1 = α x P~ Rt - E + (1 − α) x J~ (1)
</equation>
<bodyText confidence="0.9996415">
In this, E contains the edge transition probabilities,
set to a uniform out-bound probability. P~R holds
the PageRank value for each vertex and J~ is uni-
form vector used to randomly transition between
vertices. Traditionally, α = 0.85 and is used to
tradeoff between a strict transition model and the
random-walk model.
We then adopt the extensions proposed in (Ol-
livier and Senellart, 2007) (OS) to determine re-
latedness given a source vertex:
</bodyText>
<equation confidence="0.858267">
� )
~Rt+1 = αx ~Rt - E + (~S − P~ R) +(1−α)x J~
(2)
S~ is a vector that contains zeros except for a one
~
</equation>
<bodyText confidence="0.999776090909091">
at our source vertex, and PR removes an overall
value of 1 based on the a priori PageRank value of
the vertex. In this way, vertices close to the source
are rewarded with weight and vertices that have a
high a priori importance are penalized. When R~
converges, it contains measures of importance for
vertices based on the source vertex.
Final relatedness values are then calculated
from the vector generated by Equation 2 and the
a priori importance of the vector based on the
PageRank from Equation 1:
</bodyText>
<equation confidence="0.999534">
relOS(w,a) = Rw[a] x log (PR[a]1/ (3)
</equation>
<bodyText confidence="0.999959923076923">
w is the vertex for the source word and a is the
alternative word vertex. The PR[a] penalty is used
to further ensure that our alternative vertex is not
highly valued simply because it is well-connected.
Applying Equation 3 provides comparable se-
mantic relatedness performance (see Tables 1 and
2). However, cases exist where a single data value
is insufficient to make an adequate determination
of word relatedness because of small differences
for candidate words. We can incorporate addi-
tional relatedness information about our vertices
by leveraging information about the set of vertices
deemed “most related” to our current vertex.
</bodyText>
<subsectionHeader confidence="0.993175">
2.1 Integrating N-Best Neighbors
</subsectionHeader>
<bodyText confidence="0.999987529411765">
We add information by looking at the similarity
between the n-best related words for each vertex.
Intuitively, given a source word w and candidate
alternatives a1 and a2,2 we look at the set of words
that are semantically related to each of the can-
didates (represented as vectors W, A1 and A2).
If the overlap between elements of W and A1 is
greater than W and A2, A1 is more likely to be the
synonym of W.
Highly-ranked shared elements are good indi-
cators of relatedness and should contribute more
than low-ranked related words. Lists with many
low-ranked words could be an artifact of the data
set and should not be ranked higher than ones con-
taining a few high-ranked words.
Our ranked-list comparison metric (NB) is a se-
lective mean reciprocal ranking function:
</bodyText>
<equation confidence="0.914318">
x δ(Wr E ~A) (4)
~
W is the n-best list based on the source vertex
</equation>
<bodyText confidence="0.9997072">
and A~ is the n-best list based on the alternative
vertex. Values are added to our relatedness metric
based on the position of a vertex in the target list
and the traditional Dirac δ-function, which has a
value of one if the target vertex appears anywhere
in our candidate list and a zero in all other cases.
Each metric (OS and NB) will have different
ranges. We therefore normalize the reported value
by scaling each based on the maximum value for
that portion in order to achieve a uniform scale.
Our final metric (OS+NB) is created by aver-
aging the two normalized scores. In this work,
both scores are given equal weighting. Deriving
weightings for combining the two scores will be
part of our future work.
</bodyText>
<equation confidence="0.97861325">
OS(ci, cj) + NB(ci, cj, n)
relOS+NB(wi,j ) =
2
(5)
</equation>
<bodyText confidence="0.9999364">
In this, OS() returns the normalized relOS()
value and NB() returns the normalized relNB
value. The maximum relP+N() value of 1.0 is
achieved if cj has the highest PageRank-based
value and the highest N-Best value.
</bodyText>
<footnote confidence="0.80275">
2See Figure 1
</footnote>
<equation confidence="0.997403333333333">
relNB(W, A, n) =
n 1
r=1 r
</equation>
<page confidence="0.997738">
29
</page>
<table confidence="0.999687428571429">
Source ESL TOEFL
Acc. (%) Acc. (%)
JPL 82 78.8
LC-IR 78 81.3
OS 86 88.8
NB 80 88.8
OS+NB 88 93.8
</table>
<tableCaption confidence="0.999815">
Table 1: ESL and TOEFL Performance
</tableCaption>
<sectionHeader confidence="0.995181" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999858421052632">
We present performance results on three data sets.
The first, ESL, uses 50 questions from the English
as a Second Language test (Turney, 2001). Next,
an 80 question data set from the Test of English
as a Foreign Language (TOEFL) is used (Lan-
dauer and Dumais, 1997). Finally, we evaluate
on the Reader’s Digest WordPower (RDWP) data
set (Jarmasz and Szpakowicz, 2003). This is a set
of 300 synonym detection problems gathered from
the Word Power game of the Canadian edition of
Reader’s Digest Word from 2000 – 2001.
We use the Feb. 03, 2009 version of the English
Wiktionary data set 3 for extracting graph structure
and relatedness information.
Table 1 presents the performance of our algo-
rithm on the ESL and TOEFL test sets. Our results
are compared to Jarmasz and Szpakowicz (2003),
which uses a path-based cost on the structure
of Roget’s Thesaurus (JPL) and a cooccurence-
based metric, LC-IR (Higgins, 2004), which con-
strained context to only consider adjacent words in
structured web queries.
Information about our algorithm’s performance
on the RDWP test set is found in Table 2. Our re-
sults are compared to the previously mentioned al-
gorithms and also the work of Zesch et al. (2008).
Their first metric (ZPL) uses the path length be-
tween two graph vertices for relatedness determi-
nation. The second, (ZCV), creates concept vec-
tors based on a distribution of pages that contain a
particular word.
RDWP is not only larger then the previous two,
but also more complictated. TOEFL and ESL
average 1.0 and 1.008 number of words in each
source and alternative, respectively. For RDWP
each entry averages 1.4 words.
We map words and phrases to graph vertices by
first matching against the page title. If there is no
</bodyText>
<footnote confidence="0.908873">
3http://download.wikimedia.org
</footnote>
<bodyText confidence="0.9999825625">
match, we follow the approach outlined in (Zesch
et al., 2008). Common words are removed from
the phrase4 and for every remaining word in the
phrase, we determine the page mapping for that
individual word. The relatedness of the phrase
is then set to be the maximum relatedness value
attributed to any of the individual words in the
phrase.
Random guessing by an algorithm could in-
crease algorithm performance through random
chance. Therefore, we present both a overall
percentage and also a precision-based percentage.
The first (Raw) is defined as the correct number of
guesses over all questions. The second (Prec) is
defined as the correct number of guesses divided
by only those questions that were attempted.
</bodyText>
<subsectionHeader confidence="0.981361">
3.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999934903225806">
For NB and OS+NB, we set n = 3000 based on
TOEFL data set training.5 Testing was then per-
formed on the ESL and RDWP data set.
As shown in Table 1, the OS algorithm per-
forms better on the task than the comparison sys-
tems. On its own, NB relatedness performs well –
at or slightly worse than OS. Combining the two
measures increases performance on both data sets.
While our TOEFL results are below the reported
performance of (Turney et al., 2003) (97.5%), we
do not use any task-dependent learning for our re-
sults and our algorithms have better performance
than any individual module in their system.
Combining OS with NB mitigates the influence
of OS when it is not confident. OS correctly picks
‘pinnacle’ as a synonym of ’zenith’ with a relat-
edness value 126,000 times larger than its next
competitor. For ‘consumed’, OS is wrong, giving
‘bred’ a higher score than ‘eaten’ – but only by
a value 1.2 times that of ‘eaten’. The latter case
is overcome by the addition of n-best information
while the former is unaffected.
Table 2 demonstrates that we have results com-
parable to existing state-of-the-art measures. Our
choice of n resulted in reduced scores on this task
when compared to using the OS metric by itself.
But, our algorithm still outperforms both the ZPL
and ZCV metrics for our data set in raw scores and
in three out of the four precision measures. Fur-
ther refinement of the RDWP data set mapping or
changing our metric score to a weighted sum of
</bodyText>
<footnote confidence="0.968507666666667">
4Defined here as: {and, or, to, be, the, a, an, of, on, in, for,
with, by, into, is, no}
5Out of 1.1 million vertices
</footnote>
<page confidence="0.979785">
30
</page>
<table confidence="0.9994414">
Metric Source Attempted Score # Ties Raw Prec
JPL Roget’s 300 223 0 .74 .74
LC-IR Web 300 224.33 - .75 .75
ZPL 226 88.33 96 .29 .39
ZCV Wikipedia 288 165.83 2 .55 .58
ZPL 201 103.7 55 .35 .52
ZCV Wiktionary 174 147.3 3 .49 .85
OS 300 234 0 .78 .78
NB Wiktionary 300 212 0 .71 .71
OS+NB 300 227 0 .76 .76
</table>
<tableCaption confidence="0.999512">
Table 2: Reader’s Digest WordPower 300 Overall Performance
</tableCaption>
<bodyText confidence="0.999458857142857">
sorts (rather than a raw maximum) could result in
increased performance.
Wiktionary’s coverage enables all words in the
first two tasks to be found (with the exception of
‘bipartisanly’). Enough of the words in the RDWP
task are found to enable the algorithm to attempt
all synonym detection questions.
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999963214285714">
In this paper, we have demonstrated the effective-
ness of Wiktionary as a source of relatedness in-
formation when coupled with metrics based on
information flow using synonym detection as our
evaluation testbed.
Our immediate work will be in learning weights
for the combination measure, using (Turney et al.,
2003) as our guideline. Additional work will be in
automatically determining an effective value for n
across all data sets.
Long-term work will be in modifying the page
transition values to achieve non-uniform transition
values. Links are of differing quality, and the tran-
sition probabilities should reflect that.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999480170212766">
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search En-
gine. Computer Networks and ISDN Systems, 30(1–
7):107–117.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13–47.
Derrick Higgins. 2004. Which Statistics Reflect Se-
mantics? Rethinking Synonymy and Word Similar-
ity. In Proceedings of the International Conference
on Linguistic Evidence.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
Thesaurus and Semantic Similarity. In Proceedings
of Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2003).
Thomas K. Landauer and Susan T. Dumais. 1997. A
Solution to Plato’s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. Psychological Review.
David Milne and Ian H. Witten. 2008. An Effec-
tive, Low-Cost Measure of Semantic Relatedness
Obtained from Wikipedia Links. In Proceedings of
AAAI 2008.
Yann Ollivier and Pierre Senellart. 2007. Finding Re-
lated Pages Using Green Measures: An Illustration
with Wikipedia. In Proceedings of AAAI 2007.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! Computing Semantic Relatedness Us-
ing Wikipedia. In AAAI.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
relatedness metric for Wikipedia concepts based on
link analysis and its application to word sense dis-
ambiguation. In Proceedings of CEUR.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining Indepen-
dent Modules in Lexical Multiple-Choice Problems.
In RecentAdvances in Natural Language Processing
III: Selected Papers from RANLP 2003.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491–502, Freidburg,
Germany.
Torsten Zesch, Christof Muller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of AAAI 2008.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.424106">
<title confidence="0.999995">Using the Wiktionary Graph Structure for Synonym Detection</title>
<author confidence="0.999867">Timothy Weale</author>
<author confidence="0.999867">Chris Brew</author>
<author confidence="0.999867">Eric</author>
<affiliation confidence="0.7151125">Department of Computer Science and The Ohio State</affiliation>
<abstract confidence="0.998553333333333">This paper presents our work on using the graph structure of Wiktionary for synonym detection. We implement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be “close” to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The Anatomy of a Large-Scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>7--107</pages>
<contexts>
<context position="4019" citStr="Brin and Page, 1998" startWordPosition="603" endWordPosition="606">, lustrous, crude solitary alone, alert, restless, fearless Figure 1: Example TOEFL Questions 28 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 28–31, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP 2 Extracting Relatedness Measures We define relatedness based on information flow through the entire Wiktionary graph, rather than by any local in-bound or out-bound link structure. This provides a global measurement of vertex importance, as we do not limit the approach to comparing immediate neighbors. To do this, we first run the PageRank algorithm (Brin and Page, 1998) iteratively over the graph until convergence to measure the a priori importance of each vertex in graph: � ) P~ Rt+1 = α x P~ Rt - E + (1 − α) x J~ (1) In this, E contains the edge transition probabilities, set to a uniform out-bound probability. P~R holds the PageRank value for each vertex and J~ is uniform vector used to randomly transition between vertices. Traditionally, α = 0.85 and is used to tradeoff between a strict transition model and the random-walk model. We then adopt the extensions proposed in (Ollivier and Senellart, 2007) (OS) to determine relatedness given a source vertex: � </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks and ISDN Systems, 30(1– 7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1484" citStr="Budanitsky and Hirst, 2006" startWordPosition="213" endWordPosition="217">n had to be built using co-occurrence statistics of collected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 2008)). In this task, a target word or phrase is presented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Through the Wikimedia Foundation,1 volunteers have created two large-scale, collaborative resources that have been used in previous relatedness research – Wikipedia (an encyclope</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
</authors>
<title>Which Statistics Reflect Semantics? Rethinking Synonymy and Word Similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Linguistic Evidence.</booktitle>
<contexts>
<context position="942" citStr="Higgins, 2004" startWordPosition="138" endWordPosition="139">plement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be “close” to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures. 1 Introduction The recent creation of large-scale, collaboratively constructed semantic resources provides researchers with cheap, easily accessible information. Previous metrics used for synonym detection had to be built using co-occurrence statistics of collected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). S</context>
<context position="8845" citStr="Higgins, 2004" startWordPosition="1460" endWordPosition="1461">er’s Digest WordPower (RDWP) data set (Jarmasz and Szpakowicz, 2003). This is a set of 300 synonym detection problems gathered from the Word Power game of the Canadian edition of Reader’s Digest Word from 2000 – 2001. We use the Feb. 03, 2009 version of the English Wiktionary data set 3 for extracting graph structure and relatedness information. Table 1 presents the performance of our algorithm on the ESL and TOEFL test sets. Our results are compared to Jarmasz and Szpakowicz (2003), which uses a path-based cost on the structure of Roget’s Thesaurus (JPL) and a cooccurencebased metric, LC-IR (Higgins, 2004), which constrained context to only consider adjacent words in structured web queries. Information about our algorithm’s performance on the RDWP test set is found in Table 2. Our results are compared to the previously mentioned algorithms and also the work of Zesch et al. (2008). Their first metric (ZPL) uses the path length between two graph vertices for relatedness determination. The second, (ZCV), creates concept vectors based on a distribution of pages that contain a particular word. RDWP is not only larger then the previous two, but also more complictated. TOEFL and ESL average 1.0 and 1.</context>
</contexts>
<marker>Higgins, 2004</marker>
<rawString>Derrick Higgins. 2004. Which Statistics Reflect Semantics? Rethinking Synonymy and Word Similarity. In Proceedings of the International Conference on Linguistic Evidence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s Thesaurus and Semantic Similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of Conference on Recent Advances in Natural Language Processing (RANLP</booktitle>
<contexts>
<context position="1049" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="151" endWordPosition="154">graph and a comparison of the list of vertices found to be “close” to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures. 1 Introduction The recent creation of large-scale, collaboratively constructed semantic resources provides researchers with cheap, easily accessible information. Previous metrics used for synonym detection had to be built using co-occurrence statistics of collected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 200</context>
<context position="8299" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="1365" endWordPosition="1368"> if cj has the highest PageRank-based value and the highest N-Best value. 2See Figure 1 relNB(W, A, n) = n 1 r=1 r 29 Source ESL TOEFL Acc. (%) Acc. (%) JPL 82 78.8 LC-IR 78 81.3 OS 86 88.8 NB 80 88.8 OS+NB 88 93.8 Table 1: ESL and TOEFL Performance 3 Evaluation We present performance results on three data sets. The first, ESL, uses 50 questions from the English as a Second Language test (Turney, 2001). Next, an 80 question data set from the Test of English as a Foreign Language (TOEFL) is used (Landauer and Dumais, 1997). Finally, we evaluate on the Reader’s Digest WordPower (RDWP) data set (Jarmasz and Szpakowicz, 2003). This is a set of 300 synonym detection problems gathered from the Word Power game of the Canadian edition of Reader’s Digest Word from 2000 – 2001. We use the Feb. 03, 2009 version of the English Wiktionary data set 3 for extracting graph structure and relatedness information. Table 1 presents the performance of our algorithm on the ESL and TOEFL test sets. Our results are compared to Jarmasz and Szpakowicz (2003), which uses a path-based cost on the structure of Roget’s Thesaurus (JPL) and a cooccurencebased metric, LC-IR (Higgins, 2004), which constrained context to only consider adjacent </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s Thesaurus and Semantic Similarity. In Proceedings of Conference on Recent Advances in Natural Language Processing (RANLP 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review.</title>
<date>1997</date>
<contexts>
<context position="8197" citStr="Landauer and Dumais, 1997" startWordPosition="1349" endWordPosition="1353">() value and NB() returns the normalized relNB value. The maximum relP+N() value of 1.0 is achieved if cj has the highest PageRank-based value and the highest N-Best value. 2See Figure 1 relNB(W, A, n) = n 1 r=1 r 29 Source ESL TOEFL Acc. (%) Acc. (%) JPL 82 78.8 LC-IR 78 81.3 OS 86 88.8 NB 80 88.8 OS+NB 88 93.8 Table 1: ESL and TOEFL Performance 3 Evaluation We present performance results on three data sets. The first, ESL, uses 50 questions from the English as a Second Language test (Turney, 2001). Next, an 80 question data set from the Test of English as a Foreign Language (TOEFL) is used (Landauer and Dumais, 1997). Finally, we evaluate on the Reader’s Digest WordPower (RDWP) data set (Jarmasz and Szpakowicz, 2003). This is a set of 300 synonym detection problems gathered from the Word Power game of the Canadian edition of Reader’s Digest Word from 2000 – 2001. We use the Feb. 03, 2009 version of the English Wiktionary data set 3 for extracting graph structure and relatedness information. Table 1 presents the performance of our algorithm on the ESL and TOEFL test sets. Our results are compared to Jarmasz and Szpakowicz (2003), which uses a path-based cost on the structure of Roget’s Thesaurus (JPL) and </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI</booktitle>
<contexts>
<context position="2320" citStr="Milne and Witten, 2008" startWordPosition="340" endWordPosition="343">esented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Through the Wikimedia Foundation,1 volunteers have created two large-scale, collaborative resources that have been used in previous relatedness research – Wikipedia (an encyclopedia) and Wiktionary (a dictionary). These sources have been used for synonym detection and replicating human relatedness evaluations using the category structure (Strube and Ponzetto, 2006), local link structure (Milne and Witten, 2008) and (Turdakov and Velikhov, 2008) and global features (Zesch et al., 2008). They contain related information but focus on different information needs; which information source provides better results depends on the needs of the task. We use Wiktionary which, due to its role as a dictionary, focuses on common words and definitions – the type of information found in our synonym detection problems. Both Wikipedia and Wiktionary are organized around a basic “page” unit, containing information about an individual word, phrase or entity in the world – definitions, thesaurus entries, pronunciation g</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links. In Proceedings of AAAI 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Ollivier</author>
<author>Pierre Senellart</author>
</authors>
<title>Finding Related Pages Using Green Measures: An Illustration with Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI</booktitle>
<contexts>
<context position="4563" citStr="Ollivier and Senellart, 2007" startWordPosition="702" endWordPosition="706">iate neighbors. To do this, we first run the PageRank algorithm (Brin and Page, 1998) iteratively over the graph until convergence to measure the a priori importance of each vertex in graph: � ) P~ Rt+1 = α x P~ Rt - E + (1 − α) x J~ (1) In this, E contains the edge transition probabilities, set to a uniform out-bound probability. P~R holds the PageRank value for each vertex and J~ is uniform vector used to randomly transition between vertices. Traditionally, α = 0.85 and is used to tradeoff between a strict transition model and the random-walk model. We then adopt the extensions proposed in (Ollivier and Senellart, 2007) (OS) to determine relatedness given a source vertex: � ) ~Rt+1 = αx ~Rt - E + (~S − P~ R) +(1−α)x J~ (2) S~ is a vector that contains zeros except for a one ~ at our source vertex, and PR removes an overall value of 1 based on the a priori PageRank value of the vertex. In this way, vertices close to the source are rewarded with weight and vertices that have a high a priori importance are penalized. When R~ converges, it contains measures of importance for vertices based on the source vertex. Final relatedness values are then calculated from the vector generated by Equation 2 and the a priori </context>
</contexts>
<marker>Ollivier, Senellart, 2007</marker>
<rawString>Yann Ollivier and Pierre Senellart. 2007. Finding Related Pages Using Green Measures: An Illustration with Wikipedia. In Proceedings of AAAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>WikiRelate! Computing Semantic Relatedness Using Wikipedia.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1539" citStr="Strube and Ponzetto, 2006" startWordPosition="221" endWordPosition="224">ected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 2008)). In this task, a target word or phrase is presented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Through the Wikimedia Foundation,1 volunteers have created two large-scale, collaborative resources that have been used in previous relatedness research – Wikipedia (an encyclopedia) and Wiktionary (a dictionary). These sources have </context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate! Computing Semantic Relatedness Using Wikipedia. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Turdakov</author>
<author>Pavel Velikhov</author>
</authors>
<title>Semantic relatedness metric for Wikipedia concepts based on link analysis and its application to word sense disambiguation.</title>
<date>2008</date>
<booktitle>In Proceedings of CEUR.</booktitle>
<contexts>
<context position="1427" citStr="Turdakov and Velikhov, 2008" startWordPosition="205" endWordPosition="209">le information. Previous metrics used for synonym detection had to be built using co-occurrence statistics of collected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 2008)). In this task, a target word or phrase is presented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Through the Wikimedia Foundation,1 volunteers have created two large-scale, collaborative resources that have been used i</context>
</contexts>
<marker>Turdakov, Velikhov, 2008</marker>
<rawString>Denis Turdakov and Pavel Velikhov. 2008. Semantic relatedness metric for Wikipedia concepts based on link analysis and its application to word sense disambiguation. In Proceedings of CEUR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining Independent Modules in Lexical Multiple-Choice Problems.</title>
<date>2003</date>
<booktitle>In RecentAdvances in Natural Language Processing III: Selected Papers from RANLP</booktitle>
<contexts>
<context position="10857" citStr="Turney et al., 2003" startWordPosition="1800" endWordPosition="1803">orrect number of guesses over all questions. The second (Prec) is defined as the correct number of guesses divided by only those questions that were attempted. 3.1 Discussion For NB and OS+NB, we set n = 3000 based on TOEFL data set training.5 Testing was then performed on the ESL and RDWP data set. As shown in Table 1, the OS algorithm performs better on the task than the comparison systems. On its own, NB relatedness performs well – at or slightly worse than OS. Combining the two measures increases performance on both data sets. While our TOEFL results are below the reported performance of (Turney et al., 2003) (97.5%), we do not use any task-dependent learning for our results and our algorithms have better performance than any individual module in their system. Combining OS with NB mitigates the influence of OS when it is not confident. OS correctly picks ‘pinnacle’ as a synonym of ’zenith’ with a relatedness value 126,000 times larger than its next competitor. For ‘consumed’, OS is wrong, giving ‘bred’ a higher score than ‘eaten’ – but only by a value 1.2 times that of ‘eaten’. The latter case is overcome by the addition of n-best information while the former is unaffected. Table 2 demonstrates th</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining Independent Modules in Lexical Multiple-Choice Problems. In RecentAdvances in Natural Language Processing III: Selected Papers from RANLP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<location>Freidburg, Germany.</location>
<contexts>
<context position="8075" citStr="Turney, 2001" startWordPosition="1329" endWordPosition="1330">ur future work. OS(ci, cj) + NB(ci, cj, n) relOS+NB(wi,j ) = 2 (5) In this, OS() returns the normalized relOS() value and NB() returns the normalized relNB value. The maximum relP+N() value of 1.0 is achieved if cj has the highest PageRank-based value and the highest N-Best value. 2See Figure 1 relNB(W, A, n) = n 1 r=1 r 29 Source ESL TOEFL Acc. (%) Acc. (%) JPL 82 78.8 LC-IR 78 81.3 OS 86 88.8 NB 80 88.8 OS+NB 88 93.8 Table 1: ESL and TOEFL Performance 3 Evaluation We present performance results on three data sets. The first, ESL, uses 50 questions from the English as a Second Language test (Turney, 2001). Next, an 80 question data set from the Test of English as a Foreign Language (TOEFL) is used (Landauer and Dumais, 1997). Finally, we evaluate on the Reader’s Digest WordPower (RDWP) data set (Jarmasz and Szpakowicz, 2003). This is a set of 300 synonym detection problems gathered from the Word Power game of the Canadian edition of Reader’s Digest Word from 2000 – 2001. We use the Feb. 03, 2009 version of the English Wiktionary data set 3 for extracting graph structure and relatedness information. Table 1 presents the performance of our algorithm on the ESL and TOEFL test sets. Our results ar</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502, Freidburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof Muller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using Wiktionary for Computing Semantic Relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI</booktitle>
<contexts>
<context position="1651" citStr="Zesch et al., 2008" startWordPosition="237" endWordPosition="240">Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 2008)). In this task, a target word or phrase is presented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Through the Wikimedia Foundation,1 volunteers have created two large-scale, collaborative resources that have been used in previous relatedness research – Wikipedia (an encyclopedia) and Wiktionary (a dictionary). These sources have been used for synonym detection and replicating human relatedness evaluations using the category structure (Stru</context>
<context position="9124" citStr="Zesch et al. (2008)" startWordPosition="1506" endWordPosition="1509">ry data set 3 for extracting graph structure and relatedness information. Table 1 presents the performance of our algorithm on the ESL and TOEFL test sets. Our results are compared to Jarmasz and Szpakowicz (2003), which uses a path-based cost on the structure of Roget’s Thesaurus (JPL) and a cooccurencebased metric, LC-IR (Higgins, 2004), which constrained context to only consider adjacent words in structured web queries. Information about our algorithm’s performance on the RDWP test set is found in Table 2. Our results are compared to the previously mentioned algorithms and also the work of Zesch et al. (2008). Their first metric (ZPL) uses the path length between two graph vertices for relatedness determination. The second, (ZCV), creates concept vectors based on a distribution of pages that contain a particular word. RDWP is not only larger then the previous two, but also more complictated. TOEFL and ESL average 1.0 and 1.008 number of words in each source and alternative, respectively. For RDWP each entry averages 1.4 words. We map words and phrases to graph vertices by first matching against the page title. If there is no 3http://download.wikimedia.org match, we follow the approach outlined in </context>
</contexts>
<marker>Zesch, Muller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof Muller, and Iryna Gurevych. 2008. Using Wiktionary for Computing Semantic Relatedness. In Proceedings of AAAI 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>