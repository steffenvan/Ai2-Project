<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005147">
<title confidence="0.829428">
SENSEVAL-2 Japanese Dictionary Task
</title>
<author confidence="0.992666">
Kiyoaki Shirai
</author>
<affiliation confidence="0.993954">
School of Information Science, Japan Advanced Institute of Science and Technology
</affiliation>
<email confidence="0.990107">
kshirai@jaist.ac.jp
</email>
<sectionHeader confidence="0.979511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930333333333">
This paper reports an overview of the
SENSEVAL-2 Japanese dictionary task. It was
a lexical sample task, and word senses are de-
fined according to a Japanese dictionary, the
Iwanami Kokugo Jiten. The Iwanami Kokugo
Jaen and a training corpus were distributed to
all participants. The number of target words
was 100, 50 nouns and 50 verbs. One hundred
instances of each target word were provided,
making for a total of 10,000 instances for eval-
uation. Seven systems of three organizations
participated in this task.
</bodyText>
<sectionHeader confidence="0.995505" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960965517241">
In SENSEVAL-2, there are two Japanese tasks,
a translation task and a dictionary task. This
paper describes the details of the dictionary
task.
First of all, let me introduce an overview of
the Japanese dictionary task. This task is a
lexical sample task. Word senses were defined
according to the Iwanami Kokugo Jiten (Nishio
et al., 1994), a Japanese dictionary published by
Iwanami Shoten. It was distributed to all par-
ticipants as a sense inventory. Training data,
a corpus consisting of 3,000 newspaper articles
and manually annotated with sense IDs, was
also distributed to participants. For evaluation,
we distributed newspaper articles with marked
target words as test documents. Participants
were required to assign one or more sense IDs
to each target word, optionally with associated
probabilities. The number of target words was
100, 50 nouns and 50 verbs. One hundred in-
stances of each target word were provided, mak-
ing for a total of 10,000 instances.
In what follows, Section 2 describes details
of data used in the Japanese dictionary task.
Section 3 describes the process to construct the
gold standard data, including the analysis of
inter-tagger agreement. Section 4 briefly intro-
duces participating systems and their results.
Finally, Section 5 concludes this paper.
</bodyText>
<sectionHeader confidence="0.954389" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999129">
In the Japanese dictionary task, three data were
distributed to all participants: sense inventory,
training data and evaluation data.
</bodyText>
<subsectionHeader confidence="0.99462">
2.1 Sense Inventory
</subsectionHeader>
<bodyText confidence="0.995654875">
As described in Section 1, word senses are de-
fined according to a Japanese dictionary, the
Iwanami Kokugo Jiten. The number of head-
words and word senses in the Iwanami Kokugo
Jiten is 60,321 and 85,870, respectively.
Figure 1 shows an example of word sense de-
scriptions in the Iwanami Kokugo Jiten, the
sense set of the Japanese noun &amp;quot;MURI.&amp;quot;
</bodyText>
<sectionHeader confidence="0.458593" genericHeader="method">
MURI
</sectionHeader>
<subsectionHeader confidence="0.340671">
1. lack of reasonableness
</subsectionHeader>
<bodyText confidence="0.668392166666667">
1-a. something not to be rational, not to be sen-
sible [kimi go okoru no wa MURI mo nai
(It is natural for you to be angry)]
1-b. to do something compulsorily [sigoto no
MURI de byouki ni naru (I become ill from
overwork)]
</bodyText>
<figureCaption confidence="0.997615">
Figure 1: Sense set of &amp;quot;MURI&amp;quot;
</figureCaption>
<bodyText confidence="0.999912636363636">
As shown in Figure 1, there are hierarchical
structures in word sense descriptions. For ex-
ample, word sense 1 subsumes 1-a and 1-b. The
number of layers of hierarchy in the Iwanami
Kokugo Jiten is at most 3. Word sense dis-
tinctions in the lowest level are rather fine or
subtle. Furthermore, a word sense description
sometimes contains example sentences including
a headword, indicated by italics in Figure 1.
The Iwanami Kokugo Jiten was provided to
all participants. For each sense description, a
</bodyText>
<page confidence="0.998456">
33
</page>
<bodyText confidence="0.9997712">
corresponding sense ID and morphological in-
formation were supplied. All morphological in-
formation, which included word segmentation,
part-of-speech (POS) tag, base form and read-
ing, was manually post-edited.
</bodyText>
<subsectionHeader confidence="0.999347">
2.2 Training Data
</subsectionHeader>
<bodyText confidence="0.993683666666667">
An annotated corpus was distributed as the
training data. It was made up of 3,000 news-
paper articles extracted from the 1994 Mainichi
Shimbun, consisting of 888,000 words. The an-
notated information in the training corpus was
as follows:
</bodyText>
<listItem confidence="0.968786">
• Morphological information
</listItem>
<bodyText confidence="0.996091">
The text was annotated with morphologi-
cal information (word segmentation, POS
tag, base form and reading) for all words.
All morphological information was manu-
ally post-edited.
</bodyText>
<listItem confidence="0.9596">
• UDC code
</listItem>
<bodyText confidence="0.696554">
Each article was assigned a code represent-
ing the text class. The classification code
system was the third version (INFOSTA,
</bodyText>
<listItem confidence="0.834906666666667">
1994) of Universal Decimal Classification
(UDC) code (Organization, 1993).
• Word sense IDs
</listItem>
<bodyText confidence="0.826628">
Only 148,558 words in the text were anno-
tated for sense. Words assigned with sense
IDs satisfied the following conditions:
</bodyText>
<listItem confidence="0.9964095">
1. Their POSs were noun, verb or adjec-
tive.
2. The Iwanami Kokugo Jiten gave sense
descriptions for them.
</listItem>
<bodyText confidence="0.928230166666667">
3. They were ambiguous, i.e. there are
more than two word senses in the dic-
tionary.
Word sense IDs were manually annotated.
However, only one annotator assigned a
sense ID for each word.
</bodyText>
<subsectionHeader confidence="0.996972">
2.3 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999764166666667">
The evaluation data was made up of 2,130 news-
paper articles extracted from the 1994 Mainichi
Shimbun. The articles used for the training and
evaluation data were mutually exclusive. The
annotated information in the evaluation data
was as follows:
</bodyText>
<listItem confidence="0.980203">
• Morphological information
</listItem>
<bodyText confidence="0.999989375">
The text was annotated with morphologi
cal information (word segmentation, POE
tag, base form and reading) for all words
Note that morphological information in thE
training data was manually post-edited,
but not in the evaluation data. So partici-
pants might ignore morphological informa-
tion in the evaluation data.
</bodyText>
<listItem confidence="0.97362625">
• UDC code
As in the training data. each article was
assigned a UDC code
• Word sense IDs (gold standard data)
</listItem>
<bodyText confidence="0.999148416666667">
Word sense IDs were annotated manually
for the target words 1. Note that word
sense IDs in the evaluation and training
data were given in different ways: (1) a
sense ID was assigned for each word by at
least two annotators in the evaluation data,
while by only one annotator in the training
data, (2) only 10,000 instances in the arti-
cles were annotated with sense IDs in the
evaluation data, while all words were an-
notated which satisfied the conditions de-
scribed in 2.2 in the training data.
</bodyText>
<sectionHeader confidence="0.985642" genericHeader="method">
3 Gold Standard Data
</sectionHeader>
<bodyText confidence="0.999936222222222">
Except for the gold standard data, the data de-
scribed in Section 2 have been developed by
Real World Computing Partnership (Hasida et
al., 1998; Shirai et al., 2001) and already re-
leased to public domain 2. On the other hand,
the gold standard data was newly developed for
the SENSEVAL-2. This section presents the
process of preparing the gold standard data, and
the analysis of inter-tagger agreement.
</bodyText>
<subsectionHeader confidence="0.998859">
3.1 Sampling Target Words
</subsectionHeader>
<bodyText confidence="0.9877255">
When we chose target words, we considered the
following:
</bodyText>
<listItem confidence="0.995219">
• POSs of target words were either nouns or
verbs.
• Words were chosen which occurred more
than 50 times in the training data.
</listItem>
<footnote confidence="0.989875">
1They were hidden from participants at the contest.
2Notice that the training data had been released to
the public before the contest began. This violated the
SENSEVAL-2 schedule constraint that answer submis-
sion should not occur more than 21 days after down-
loading the training data.
</footnote>
<page confidence="0.996628">
34
</page>
<tableCaption confidence="0.999557">
Table 1: Number of Target Words
</tableCaption>
<table confidence="0.971743125">
D„ Di, D, all
nouns 10 20 20 50
verbs (9.1/1.19) (3.7/0.723) (3.3/0.248) (4.6/0.627)
10 20 20 50
(18/1.77) (6.7/0.728) (5.2/0.244) (8.3/0.743)
all 20 - 40 40 100
(14/1.48) (5.2/0.725) (4.2/0.246) (6.5/0.685)
(average polysemy / average entropy)
</table>
<listItem confidence="0.987235272727273">
• The relative &amp;quot;difficulty&amp;quot; in disambiguating
the sense of words was considered. Diffi-
culty of the word w was defined by the en-
tropy of the word sense distribution E(w)
in the training data. Obviously, the higher
E(w) was, the more difficult the WSD for
w was.
We set up three word classes, Da (E(w) &gt;
1), Db (0.5 &lt; E(w) &lt; 1) and Da (E(w) &lt;
0.5), and chose target words evenly from
them.
</listItem>
<bodyText confidence="0.983144714285714">
Table 1 reveals details of numbers of target
words. Average polysemy (i.e. average num-
ber of word senses per headword) and average
entropy are also indicated.
One hundred instances of each target word
were selected from newspaper articles, making
for a total of 10,000 instances.
</bodyText>
<subsectionHeader confidence="0.995515">
3.2 Manual Annotation
</subsectionHeader>
<bodyText confidence="0.999824375">
Six annotators assigned the correct word sense
IDs for 10,000 instances. They were not experts,
but had knowledge of linguistics or lexicography
to some degree. The process of manual anno-
tating was as follows:
Step 1. Two annotators chose a sense ID for
each instance separately in accordance with
the following guidelines:
</bodyText>
<listItem confidence="0.988573111111111">
• Only one sense ID was to be chosen for
each instance.
• Sense IDs at any layers in hierarchical
structures could be assignable.
• The &amp;quot;UNASSIGNABLE&amp;quot; tag was to
be chosen only when all sense IDs
weren&apos;t absolutely applicable. Other-
wise, choose one of sense IDs in the
dictionary.
</listItem>
<tableCaption confidence="0.987962">
Table 2: Inter-tagger Agreement
</tableCaption>
<table confidence="0.96691725">
Da Db Da (all)
nouns 0.809 0.786 0.957 0.859
verbs 0.699 0.896 0.922 0.867
all 0.754 0.841 0.939 0.863
</table>
<bodyText confidence="0.985521933333333">
Step 2. If the sense IDs selected by 2 annota-
tors agreed, we considered it to be a correct
sense ID for an instance.
Step 3. If they did not agree, the third anno-
tator chose the correct sense ID between
them. If the third annotator judged both of
them to be wrong and chose another sense
ID as correct, we considered that all 3 word
sense IDs were correct.
According to Step 3., the number of words for
which 3 annotators assigned different sense IDs
from one another was a quite few, 28 (0.3%).
Table 2 indicates the inter-tagger agreement
of two annotators in Step 1. Agreement ratio
for all 10,000 instances was 86.3%.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="evaluation">
4 Results for Participating Systems
</sectionHeader>
<bodyText confidence="0.9961425">
In the Japanese dictionary task, the following 7
systems of 3 organizations submitted answers.
Notice that all systems used supervised learning
techniques.
</bodyText>
<listItem confidence="0.8956295">
• Communications Research Laboratory and
New York University (CRL1 CRL4)
</listItem>
<bodyText confidence="0.9957885">
The learning schemes were simple Bayes
and support vector machine (SVM), and
two kinds of hybrid models of simple Bayes
and SVM.
</bodyText>
<listItem confidence="0.909399">
• Tokyo Institute of Technology (Titechl,
Titech2)
</listItem>
<bodyText confidence="0.998072333333333">
Decision lists were learned from the train-
ing data. The features used in the decision
lists were content words and POS tags in a
window, and content words in example sen-
tences contained in word sense descriptions
in the Iwanami Kokugo Jiten.
</bodyText>
<listItem confidence="0.99065">
• Nara Institute of Science and Technology
(Naist)
</listItem>
<bodyText confidence="0.928888">
The learning algorithm was SVM. The fea-
ture space was reconstructed using Princi-
ple Component Analysis(PCA) and Inde-
pendent Component Analysis(ICA).
</bodyText>
<page confidence="0.994619">
35
</page>
<figure confidence="0.809004">
3 coarse-grained iv mixed-grained # fine-grainedl
</figure>
<figureCaption confidence="0.876552">
Figure 2: Results
</figureCaption>
<figure confidence="0.98606325">
c,C&amp;quot;
s,5&amp;quot; ts°
eP e
4`•
</figure>
<figureCaption confidence="0.997750333333333">
Figure 3: Mixed-grained scores for nouns and
verbs
Figure 4: Mixed-grained scores for word classes
</figureCaption>
<bodyText confidence="0.99727437037037">
The results of all systems are shown in Fig-
ure 2. &amp;quot;Baseline&amp;quot; indicates the system which
always selects the most frequent word sense ID,
while &amp;quot;Agreement&amp;quot; indicates the agreement ra-
tio between two annotators. All systems outper-
formed the baseline, and there was no remark-
able difference between their scores (differences
were 3 % at most).
Figure 3 indicates the mixed-grained scores
for nouns and verbs. Comparing baseline sys-
tem scores, the score for verbs was greater than
that for nouns, even though the average entropy
of verbs was higher than that of nouns (Table 1).
The situation was the same in CRL systems, bt
not in Titech and Naist. The reason why the al
erage entropy was not coincident with the scot
of the baseline was that the entropy of som
verbs was so great that it raised the average er
tropy disproportionately. Actually, the entrop
of 7 verbs was greater than the maximum er
tropy of nouns.
Figure 4 indicates the mixed-grained score
for each word class. For word class Dc, ther
was hardly any difference among scores of a
systems, including Baseline system and Agree
ment. On the other hand, appreciable differenc
was found for Da and Db.
</bodyText>
<sectionHeader confidence="0.99927" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999890166666667">
This paper reports an overview of th
SENSEVAL-2 Japanese dictionary task. Th
data used in this task are available on th
SENSEVAL-2 web site. I hope this valuabl,
data helps all researchers to improve their WSI
systems.
</bodyText>
<sectionHeader confidence="0.9737" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99992275">
I wish to express my gratitude to Mainich
Newspapers for providing articles. I would als(
like to thank Prof. Takenobu Tokunaga (Toky(
Institute of Technology) and Prof. Sadao Kuro
hashi (University of Tokyo) for valuable advis(
about task organization, the annotators for con.
structing gold standard data, and all partici.
pants.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997278823529412">
Koiti Hasida et al. 1998. The RWC texi
databases. In Proceedings of the the firs.
International Conference on Language Re-
sources and Evaluation, pages 457-462.
INFOSTA. 1994. Universal Decimal Classifica-
tion. Maruzen, Tokyo. (in Japanese).
Minoru Nishio, Etsutaro Iwabuchi, and Shizuc
Mizutani. 1994. Iwanami Kokugo Jiten Da
Go Han. Iwanami Publisher. (in Japanese).
British Standards Organization. 1993. Guide tc
the Universal Decimal Classification (UDC).
BSI, London.
Kiyoaki Shirai et al. 2001. Text database with
word sense tags defined by Iwanami Japanese
dictionary. SIG notes of Information Pro-
cessing Society of Japan, 2001(9):117-122.
(in Japanese).
</reference>
<figure confidence="0.9983955">
0.9
0.8
0.7
0.6
c)•
61 Da la Db 4 Dc I
1
0.9
0.8
0.7
0.6
0.5
0.4
0.9
0.85
0.8
0.75
0.7
0.65
0.6
</figure>
<page confidence="0.959427">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.724809">
<title confidence="0.990482">SENSEVAL-2 Japanese Dictionary Task</title>
<author confidence="0.999909">Kiyoaki Shirai</author>
<affiliation confidence="0.999508">School of Information Science, Japan Advanced Institute of Science and</affiliation>
<email confidence="0.79128">kshirai@jaist.ac.jp</email>
<abstract confidence="0.993642">This paper reports an overview of the SENSEVAL-2 Japanese dictionary task. It was a lexical sample task, and word senses are defined according to a Japanese dictionary, the Iwanami Kokugo Jiten. The Iwanami Kokugo Jaen and a training corpus were distributed to all participants. The number of target words 50 and 50 verbs. One hundred instances of each target word were provided, making for a total of 10,000 instances for evaluation. Seven systems of three organizations participated in this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Koiti Hasida</author>
</authors>
<title>The RWC texi databases.</title>
<date>1998</date>
<booktitle>In Proceedings of the the firs. International Conference on Language Resources and Evaluation,</booktitle>
<pages>457--462</pages>
<marker>Hasida, 1998</marker>
<rawString>Koiti Hasida et al. 1998. The RWC texi databases. In Proceedings of the the firs. International Conference on Language Resources and Evaluation, pages 457-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>INFOSTA</author>
</authors>
<title>Universal Decimal Classification.</title>
<date>1994</date>
<location>Maruzen, Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="4075" citStr="INFOSTA, 1994" startWordPosition="654" endWordPosition="655">post-edited. 2.2 Training Data An annotated corpus was distributed as the training data. It was made up of 3,000 newspaper articles extracted from the 1994 Mainichi Shimbun, consisting of 888,000 words. The annotated information in the training corpus was as follows: • Morphological information The text was annotated with morphological information (word segmentation, POS tag, base form and reading) for all words. All morphological information was manually post-edited. • UDC code Each article was assigned a code representing the text class. The classification code system was the third version (INFOSTA, 1994) of Universal Decimal Classification (UDC) code (Organization, 1993). • Word sense IDs Only 148,558 words in the text were annotated for sense. Words assigned with sense IDs satisfied the following conditions: 1. Their POSs were noun, verb or adjective. 2. The Iwanami Kokugo Jiten gave sense descriptions for them. 3. They were ambiguous, i.e. there are more than two word senses in the dictionary. Word sense IDs were manually annotated. However, only one annotator assigned a sense ID for each word. 2.3 Evaluation Data The evaluation data was made up of 2,130 newspaper articles extracted from th</context>
</contexts>
<marker>INFOSTA, 1994</marker>
<rawString>INFOSTA. 1994. Universal Decimal Classification. Maruzen, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minoru Nishio</author>
<author>Etsutaro Iwabuchi</author>
<author>Shizuc Mizutani</author>
</authors>
<title>Iwanami Kokugo Jiten Da Go Han. Iwanami Publisher.</title>
<date>1994</date>
<note>(in Japanese).</note>
<contexts>
<context position="1031" citStr="Nishio et al., 1994" startWordPosition="159" endWordPosition="162">tributed to all participants. The number of target words was 100, 50 nouns and 50 verbs. One hundred instances of each target word were provided, making for a total of 10,000 instances for evaluation. Seven systems of three organizations participated in this task. 1 Introduction In SENSEVAL-2, there are two Japanese tasks, a translation task and a dictionary task. This paper describes the details of the dictionary task. First of all, let me introduce an overview of the Japanese dictionary task. This task is a lexical sample task. Word senses were defined according to the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary published by Iwanami Shoten. It was distributed to all participants as a sense inventory. Training data, a corpus consisting of 3,000 newspaper articles and manually annotated with sense IDs, was also distributed to participants. For evaluation, we distributed newspaper articles with marked target words as test documents. Participants were required to assign one or more sense IDs to each target word, optionally with associated probabilities. The number of target words was 100, 50 nouns and 50 verbs. One hundred instances of each target word were provided, making for a t</context>
</contexts>
<marker>Nishio, Iwabuchi, Mizutani, 1994</marker>
<rawString>Minoru Nishio, Etsutaro Iwabuchi, and Shizuc Mizutani. 1994. Iwanami Kokugo Jiten Da Go Han. Iwanami Publisher. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>British Standards Organization</author>
</authors>
<title>Guide tc the Universal Decimal Classification (UDC).</title>
<date>1993</date>
<publisher>BSI,</publisher>
<location>London.</location>
<contexts>
<context position="4143" citStr="Organization, 1993" startWordPosition="662" endWordPosition="663">ted as the training data. It was made up of 3,000 newspaper articles extracted from the 1994 Mainichi Shimbun, consisting of 888,000 words. The annotated information in the training corpus was as follows: • Morphological information The text was annotated with morphological information (word segmentation, POS tag, base form and reading) for all words. All morphological information was manually post-edited. • UDC code Each article was assigned a code representing the text class. The classification code system was the third version (INFOSTA, 1994) of Universal Decimal Classification (UDC) code (Organization, 1993). • Word sense IDs Only 148,558 words in the text were annotated for sense. Words assigned with sense IDs satisfied the following conditions: 1. Their POSs were noun, verb or adjective. 2. The Iwanami Kokugo Jiten gave sense descriptions for them. 3. They were ambiguous, i.e. there are more than two word senses in the dictionary. Word sense IDs were manually annotated. However, only one annotator assigned a sense ID for each word. 2.3 Evaluation Data The evaluation data was made up of 2,130 newspaper articles extracted from the 1994 Mainichi Shimbun. The articles used for the training and eval</context>
</contexts>
<marker>Organization, 1993</marker>
<rawString>British Standards Organization. 1993. Guide tc the Universal Decimal Classification (UDC). BSI, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
</authors>
<title>Text database with word sense tags defined by Iwanami Japanese dictionary.</title>
<date>2001</date>
<booktitle>SIG notes of Information Processing Society of Japan,</booktitle>
<pages>2001--9</pages>
<note>(in Japanese).</note>
<marker>Shirai, 2001</marker>
<rawString>Kiyoaki Shirai et al. 2001. Text database with word sense tags defined by Iwanami Japanese dictionary. SIG notes of Information Processing Society of Japan, 2001(9):117-122. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>