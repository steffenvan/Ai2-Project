<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000606">
<title confidence="0.9974155">
Improving Text Classification by a Sense Spectrum Approach to Term
Expansion
</title>
<author confidence="0.998344">
Peter Wittek S´andor Dar´anyi Chew Lim Tan
</author>
<affiliation confidence="0.999706">
Department of Computer Science Swedish School of Library Department of Computer Science
National University of Singapore and Information Science National University of Singapore
</affiliation>
<address confidence="0.8955295">
Computing 1, Law Link G¨oteborg University &amp; Computing 1, Law Link
Singapore 117590 University of Bor˚as Singapore 117590
wittek@comp.nus.edu.sg All´egatan 1 tancl@comp.nus.edu.sg
50190 Bor˚as, Sweden
</address>
<email confidence="0.998589">
sandor.daranyi@hb.se
</email>
<sectionHeader confidence="0.996651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99939975">
Experimenting with different mathematical
objects for text representation is an important
step of building text classification models. In
order to be efficient, such objects of a for-
mal model, like vectors, have to reasonably re-
produce language-related phenomena such as
word meaning inherent in index terms. We in-
troduce an algorithm for sense-based seman-
tic ordering of index terms which approxi-
mates Cruse’s description of a sense spectrum.
Following semantic ordering, text classifica-
tion by support vector machines can benefit
from semantic smoothing kernels that regard
semantic relations among index terms while
computing document similarity. Adding ex-
pansion terms to the vector representation can
also improve effectiveness. This paper pro-
poses a new kernel which discounts less im-
portant expansion terms based on lexical re-
latedness.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801673913044">
Generally, building an automated text classification
system consists of two key subtasks. The first task
is text representation which converts the content of
documents into compact format so that they can be
further processed by the text classifiers. Another
task is to learn the model of a text classifier which
is used to classify the unlabeled documents. This
paper proposes a substantially new model for text
representation to improve effectiveness of text clas-
sification by semantic ordering.
Our motivation for the research presented here
came from (Dorrer et al., 2001) who demonstrated
the viability of database searching by visible light
using a quantum algorithm, albeit on meaningless
items. The question was, what kind of document
representation would be necessary to extend their
in-principle results to include semantics, one that
has been leading us to test both periodic and non-
periodic functions for this purpose. Since represen-
tation and retrieval by colors was implied in their
method, we speculated that the following compo-
nents could be useful in a rephrased model: (a)
a metaphorically presented spectral expression of
lexical semantic phenomena, (b) a ranked one-
dimensional condensate of multidimensional sense
structure, and (c) representation of documents and
queries by functions in L2 space with a similarity
measure. Our anticipation was that by matching
these components, a new model could demonstrate
new capacities in general, and contribute to comput-
ing meaning by waves in particular.
Semantic ordering (component b) is an approxi-
mation of what (Cruse, 1986) referred to as a sense
spectrum, i.e. a series of points - called local senses
and constituting lexical units -, in a one-dimensional
semantic continuum (component a). Apart from dif-
ferentiating between the conceptual content of the
same word in terms of its senses in word pairs, i.e.
their semantic relatedness, it also compresses the
result in spectral form. The scalar values of this
spectrum have the double potential of being a con-
densed measure for semantic weighting, and, ten-
tatively, they can play the role of mass in experi-
ments where gravity is called in as a metaphor for
text categorization and information retrieval (Paij-
mans, 1997; Shi et al., 2005; Wittek et al., 2009).
</bodyText>
<page confidence="0.987963">
183
</page>
<note confidence="0.989927">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183–191,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999893293478261">
This paper addresses text categorization by means
of non-periodical functions only.
In support of Cruse’s point, recently it has been
demonstrated by measurements that sense classifi-
cation errors made by their maximum entropy based
word sense disambiguation system were partly
remedied once instead of a fine-grained view, a more
coarse-grained view of senses was adopted (Palmer
et al., 2006). Improvement of sense classification ac-
curacy linked with “zooming out” in terms of obser-
vation granularity indicates, in our eyes, the “fluid”,
perhaps spectral nature of sense inasmuch as it is
impossible to precisely distinguish between the bor-
derlines and some fuzziness is implied both in the
phenomenon and its perception. This “fluidity of
language”, as Palmer et al. call it, is in accord
with the theory of shared semantic representations in
psycholinguistics (Rodd et al., 2002), according to
which related senses share a portion of their mean-
ing representation in the mental lexicon; it also sup-
ports an earlier observation of two of the present au-
thors based on the same methodology as outlined in
this paper, namely that using continuous functions
for information retrieval leads to content representa-
tion without exact term or document locations, one
which is regional in its nature and subject to a math-
ematical uncertainty principle (Wittek and Dar´anyi,
2007).
We approach our problem in three steps: (1)
whether distributional semantics alone is enough for
the representation of word meaning, (2) whether se-
mantic relatedness between word pairs can be ex-
pressed in an ordered form while preserving lexical
field structure, and if (3) the uniqueness of entries in
such an order can be expressed by functions rather
than scalars such as distance. As we will show, this
line of thought leads to performance improvement
in text classification by using kernel-based feature
weighting.
Since the early days of the vector space model,
it has been debated whether it is a proper carrier
of meaning of texts (Raghavan and Wong, 1986),
arguing if distributional similarity is an adequate
proxy for lexical semantic relatedness (Budanitsky
and Hirst, 2006). We argue for the need to enrich
distributional semantics-based text representation by
other components because with the statistical, i.e.
devoid of word semantics approaches there is gen-
erally no way to improve both precision and recall
at the same time, increasing one is done at the ex-
pense of the other. For example, casting a wider net
of search terms to improve recall of relevant items
will also bring in an even greater proportion of ir-
relevant items, lowering precision. In the mean-
time, practical approaches have been proliferating,
especially with developments in kernel methods in
the last decade (Joachims, 1998; Cristianini et al.,
2002). Some researchers suggested a more general
mathematical framework to accommodate the needs
that the vector space model cannot satisfy (van Rijs-
bergen, 2004). This paper explores the opportunities
of this representation in the domain of text classifi-
cation by introducing it as a new nonlinear semantic
kernel.
Another aspect of the same problem is term ex-
pansion for document classification and retrieval.
By automatically selecting expansion terms for a
text classification system to expand a document vec-
tor by adding terms that are related to the terms
already in the document, performance can be im-
proved (Hu et al., 2008). Such new terms can ei-
ther be statistically related to the original terms or
chosen from lexical resources such as thesauri, con-
trolled vocabularies, ontologies and the like.
However, in doing so the fundamental question
often overlooked is whether the expansion terms ex-
tracted are equally related to the document and are
useful for text classification. In what follows we
propose a form of term expansion with decreasing
importance of those terms that are less related, as
contrasted with rigid term expansion. This can be
carried out by a combination of semantic ordering
and using function space for classification.
This paper is organized as follows. Section 2
overviews text classification by support vector ma-
chines, expanding on traditional text similarity mea-
sures (Section 2.1), semantic smoothing kernels
(Section 2.2), term expansion strategies (Section
2.3), and finally introduces our semantic kernels in
the L2 space (Section 2.4). Section 3 discusses ex-
perimental results and Section 4 concludes the pa-
per.
</bodyText>
<page confidence="0.99122">
184
</page>
<bodyText confidence="0.954668545454546">
2 Text Classification with Support Vector
Machines
similarity of their corresponding term vectors which
is defined as:
Text categorization is the task of assigning unlabeled
documents into predefined categories. Given a col-
lection of {d1, d2, ... , dN} documents, and a C =
{c1, c2, ..., cjCj} set of predefined categories, the
task is, for each document dj (j E {1, 2, ... , N}),
to assign a decision to file dj under ci or a deci-
sion not to file dj under ci (ci E C) by virtue of
a function Φ, where the function Φ is also referred
to as the classifier, or model, or hypothesis, or rule.
Supervised text classification is a machine learning
technique for creating the function Φ from training
data. The training data consist of pairs of input doc-
uments, and desired outputs (i.e., classes).
Support vector machines have been found the
most effective by several authors (Joachims, 1998).
The proposed semantic text classification method is
grounded in the kernel methods underlying support
vector machines.
A support vector machine is a kind of supervised
learning algorithm. In its simplest, linear form, a
support vector machine is a hyperplane that sepa-
rates a set of positive examples from a set of nega-
tive examples with maximum margin (Shawe-Taylor
and Cristianini, 2004). The strength of kernel meth-
ods is that they allow a mapping φ(.) of x to a higher
dimensional space. In the dual formulation of the
mathematical programming problem, only the ker-
nel matrix K(xi,xj) = φ(xi)&apos;φ(xi) is needed in
the calculations.
</bodyText>
<subsectionHeader confidence="0.984767">
2.1 Traditional Text Similarity Measure
</subsectionHeader>
<bodyText confidence="0.999792571428571">
Intuitively, if a text fragment of two documents ad-
dress similar topics, it is highly possible that they
share lots of substantive terms. After having re-
moved the stopwords and stemmed the rest, the
stemmed terms construct a vector representation for
each text document. Let aj be a document vector in
the vector space model, that is, aj = EMk�1 akjek,
where M is the number of index terms, akj is some
weighting (e.g., term frequency), and ek is a basis
vector of the M-dimensional Euclidean space. This
representation is also referred to as the bag-of-words
(BOW) model.
Given this representation, semantic relatedness of
a pair of text fragments is computed as the cosine
</bodyText>
<equation confidence="0.939287">
aiaj
S(ai, aj) =
</equation>
<bodyText confidence="0.593465">
|ai||a|j
</bodyText>
<subsectionHeader confidence="0.998497">
2.2 Linear Semantic Kernels
</subsectionHeader>
<bodyText confidence="0.999949642857143">
One enrichment strategy is to use a semantic
smoothing kernel while calculating the similarity
between two documents. Any linear kernel for texts
is characterized by K(ai, aj) = a&apos;iS&apos;Saj, where
S is an appropriately shaped matrix commonly re-
ferred to as semantic smoothing matrix (Siolas and
d’Alch´e Buc, 2000; Shawe-Taylor and Cristianini,
2004; Basili et al., 2005; Mavroeidis et al., 2005;
Bloehdorn et al., 2006). The presence of S changes
the orthogonality of the vector space model, as this
mapping should introduce term dependence. A re-
cent attempt tried to manually construct S with the
help of a lexical resource (Siolas and d’Alch´e Buc,
2000). The entries in the symmetric matrix S ex-
press the semantic similarity between the terms i and
j. Entries in this matrix are inversely proportional
to the length of the WordNet hierarchy path linking
the two terms. The performance, measured over the
20NewsGroups corpus, showed an improvement of
2 % over the the basic vector space method. More-
over, the semantic matrix S is almost fully dense,
hence computational issues arise. In a similar con-
struction, (Bloehdorn et al., 2006) defined the ma-
trix entries as weights of superconcepts of the two
terms in the WordNet hierarchy. Focusing on special
subcategories of Reuters-21578 and on the TREC
Question Answering Dataset, they showed consis-
tent improvement over the baseline. As (Mavroei-
dis et al., 2005) pointed out, polysemy will remain
a problem in semantic smoothing kernels. A more
complex way of calculating the semantic similarity
as the matrix entries was also proposed (Basili et al.,
2005). For a more general discussion on semantic
similarity see Section 2.4.1.
An early attempt to overcome the untenable or-
thogonality assumption of the vector space model
was proposed under the name of generalized vec-
tor space model (Wong et al., 1985). The article
which proposed the model did not provide empiri-
cal results, and since then the model has been re-
garded of large theoretical importance with less im-
pact on actual applications. The model takes a distri-
</bodyText>
<equation confidence="0.984858">
. (1)
</equation>
<page confidence="0.98676">
185
</page>
<bodyText confidence="0.999970319148936">
butional approach, focusing on term co-occurrences.
The underlying assumption is that term correlations
are captured by the co-occurrence information. That
is, two terms are semantically related if they co-
occur often in the same documents. By eliminat-
ing orthogonality, documents can be seen as similar
even if they do not share any terms. The term co-
occurrence matrix is AA&apos;, hence the model takes A&apos;
as the semantic similarity matrix S. A major draw-
back of the generalized vector space model is that it
replaces the orthogonality assumption with another
questionable assumption. The computational needs
are tremendous too, if the dimensions of A are con-
sidered. Moreover, the co-occurrence matrix is not
sparse anymore.
Latent semantic indexing (or latent semantic anal-
ysis) was another attempt to bring more linguis-
tic and psychological aspects to language process-
ing via a kernel. Conceptually, latent semantic in-
dexing is similar to the generalized vector space
model, it measures semantic information through
co-occurrence analysis in the corpus. From the al-
gorithmic perspective it is an enormous problem that
textual data have a large number of relevant fea-
tures. This results in huge computational needs and
the classification models may overfit the data. The
number of features can be reduced by multivariate
feature extraction methods. In latent semantic in-
dexing, the dimension of the vector space is reduced
by singular value decomposition (Deerwester et al.,
1990).
Using rank reduction, terms that occur together
very often in the same documents are merged into
a single dimension of the feature space. The di-
mensions of the reduced space correspond to the
axes of greatest variance. For latent semantic in-
dexing, by dual representation the kernel matrix is
K = V E2kV &apos;, where Ek is a diagonal matrix con-
taining the k largest singular values of the singu-
lar value decomposition of the vector space, and V
holds the right singular vectors of the decomposi-
tion. The new kernel matrix can be obtained directly
from K by applying an eigenvalue decomposition
of K (Cristianini et al., 2002). The computational
complexity of performing an eigenvalue decompo-
sition on the kernel matrix is a major drawback of
latent semantic indexing.
</bodyText>
<subsectionHeader confidence="0.712605">
2.3 Text Representation Enrichment Strategies
by Term Expansion
</subsectionHeader>
<bodyText confidence="0.999976944444444">
In order to eliminate the bottleneck of the traditional
BOW representation, previous approaches in term
expansion enriched this convention by external lexi-
cal resources such as WordNet.
As a first step, these methods generate new fea-
tures for each document in the dataset. These new
features can be synonyms or homonyms of docu-
ment terms as in (Hotho et al., 2003; Rodriguez
and Hidalgo, 1997), or expanded features for terms,
sentences and documents as in (Gabrilovich and
Markovitch, 2005), or term context information for
word sense disambiguation such as topic signatures
(Agirre and De Lacalle, 2003; Agirre et al., 2004).
Then, the generated new features replace the old
ones or are appended to the document representa-
tion, and construct a new vector representation ai
for each text document. The similarity measure of
document pairs is defined as:
</bodyText>
<subsectionHeader confidence="0.962655">
2.4 Our Framework
</subsectionHeader>
<bodyText confidence="0.999920555555556">
The basic assumption of our framework is that terms
can be arranged in an order such that consecutive
terms are semantically related. Hence each term ac-
quires a unique position, and this position ties the
term to its semantically related neighbors. However,
given a BOW representation with a cosine similarity
measure, this position would not improve classifica-
tion performance. Therefore we suggest to associate
a mathematical function with each term, thus map-
ping terms and documents to the L2 space, and using
the inner product of this space to express similar-
ity. The choice of function will determine to which
extent neighboring terms, i.e., the enriching terms,
are considered in calculating the similarity between
two documents. This section first introduces an al-
gorithm that produces the aforementioned semantic
order, then the semantic kernels in the L2 space are
discussed.
</bodyText>
<sectionHeader confidence="0.442125" genericHeader="method">
2.4.1 An Algorithm for a Semantic Ordering of
Terms
</sectionHeader>
<bodyText confidence="0.998508">
The proposed kernels assume that there is a se-
mantic order between terms. Let V denote a set of
</bodyText>
<figure confidence="0.8992615">
SAI Sj) =
(2)
��j|.
aiaj
</figure>
<page confidence="0.982711">
186
</page>
<bodyText confidence="0.997215575757576">
terms {t1, t2, ... , tn} and let d(ti, tj) denote the se-
mantic distance between the terms ti and tj. The
initial order of the terms is not relevant, though it is
assumed to be alphabetic. Let G = (V, E) denote
a weighted undirected graph, where the weights in
the set E are defined by the distances between the
terms.
Various lexical resource-based (Budanitsky and
Hirst, 2006) and distributional measures (Moham-
mad and Hirst, 2005) have been proposed to mea-
sure semantic relatedness and distance between
terms. Terms can be corpus- or genre-specific. Man-
ually constructed general-purpose lexical resources
include many usages that are infrequent in a partic-
ular corpus or genre of documents. For example,
one of the 8 senses of company in WordNet is a
visitor/visitant, which is a hyponym of person (Lin,
1998). This sense of the term is practically never
used in newspaper articles, hence distributional at-
tributes should be taken into consideration. Com-
posite measures that combine the advantages of both
approaches have also been developed (Resnik, 1995;
Jiang and Conrath, 1997). This paper relies on the
Jiang-Conrath composite measure (Jiang and Con-
rath, 1997), which has been shown to be superior
to other measures (Budanitsky and Hirst, 2006), and
we also found that this measure works the best for
the purpose. The Jiang-Conrath metric measures
the distance between two senses by using the hier-
archy of WordNet. By denoting the lowest super-
ordinate of two senses s1 and s2 in the hierarchy
with LSuper(s1,s2), the metric is calculated as fol-
lows:
</bodyText>
<equation confidence="0.904954">
d(s1, s2) = IC(s1)+IC(s2)−2IC(LSuper(s1, s2)),
</equation>
<bodyText confidence="0.99961668">
where IC(s) is the information content of a sense
s based on a corpus. Distance between two terms
is calculated according to the following equation:
d(t1, t2) = maxs1Esen(t1),s2Esen(t2) d(s1, s2), where
t1 and t2 are two terms, and sen(ti) is the set of
senses of ti. The distance between two terms is
usually defined as the minimum of the sense dis-
tances. We chose maximum because it ensures that
only closely related terms will be placed to adjacent
positions by the algorithm below.
Finding a semantic ordering of terms can be trans-
lated to a graph problem: a minimum-weight Hamil-
tonian path G&apos; of G gives the ordering by reading
the nodes from one of the paths to the other. G is
a complete graph, therefore such a path always ex-
ists, but finding it is an NP-complete problem. The
following greedy algorithm is similar to the nearest
neighbor heuristic for the solution of the traveling
salesman problem. It creates a graph G&apos; = (V &apos;, E&apos;),
where V &apos; = V and E&apos; C E. This G&apos; graph is a
spanning tree of G in which the maximum degree of
a node is two, that is, the minimum spanning tree is
a path between two nodes.
Step 1 Find the term at the highest stage of the hi-
erarchy in a lexical resource.
</bodyText>
<equation confidence="0.939433">
ts = argmintzEV depth(ti).
</equation>
<bodyText confidence="0.9783395">
This seed term is the first element of V &apos;, V &apos; =
{ts}. Remove it from the set V :
</bodyText>
<equation confidence="0.996355">
V := V \{ts}.
</equation>
<bodyText confidence="0.9990198">
Using WordNet, this seed term is entity, if the
vocabulary of the text collection contains it.
Step 2 Let tl denote the leftmost term of the order-
ing and tr the rightmost one. Find the next two
elements of the ordering:
</bodyText>
<equation confidence="0.786341333333333">
t&apos;l = argmintzEV d(ti, tl),
t&apos;r = argmintzEV \{t1,1d(ti, tr).
Step 3 If d(tl, t&apos;l) &lt; d(tr, t&apos;r) then add t&apos;l to V &apos;,
E&apos; := E&apos; U {e(tl, t&apos;l)}, and V := V \{t&apos;l}.
Else add t&apos;r to V &apos;, E&apos; := E&apos; U {e(tr, t&apos;r)} and
V := V \{t&apos;r}.
</equation>
<bodyText confidence="0.9733605">
Step 4 Repeat from Step 2 until V = ∅.
The above algorithm can be thought of as a modi-
fied Prim’s algorithm, but it does not find the optimal
minimum-weight spanning tree.
</bodyText>
<subsectionHeader confidence="0.876219">
2.4.2 Semantic Kernels in the L2 Space
</subsectionHeader>
<bodyText confidence="0.997154">
The L2 space shares resemblance with a real
vector space. Real-valued vectors are replaced by
square-integrable functions, and the dot product is
replaced by the following inner product: (fi, fj) =
f fifjdx, for some fi, fj in the given L2 space.
Lately, Hoenkamp has also pointed out that the
L2 space can be used for information retrieval when
</bodyText>
<page confidence="0.996082">
187
</page>
<bodyText confidence="0.9998203">
he introduced a Haar basis for the document space
(Hoenkamp, 2003). He utilized a signal processing
framework within the context of latent semantic in-
dexing. In order to apply an L2 representation for
text classification, the problem is approached from a
different angle than by Hoenkamp, taking discount-
ing expansion terms as our point of departure.
Assigning a function w(x − k) to the term in the
kth position in a semantic order, a document j can
be expressed as follows:
</bodyText>
<equation confidence="0.998810666666667">
M
fj(x) = E akjw(x − k), (3)
k=1
</equation>
<bodyText confidence="0.999715153846154">
where x is in [1, M], and it is the variable of inte-
gration in calculating the inner product of the L2;
x can be regarded as a “dummy” variable carrying
no meaning in itself. The above formula will be re-
ferred to as a document function. In the experiments,
the function exp(−bx2) was used as w(x), with b as
a free parameter reflecting the width of the function
expressing how many neighboring expansion terms
are considered.
The inner product of the L2[1, M] space is applied
to express similarity between two documents in sim-
ilar vein as the dot product does in a real-valued vec-
tor space:
</bodyText>
<equation confidence="0.996148">
�(fi,fj) = [1,M] fi(x)fj(x)dx, (4)
</equation>
<bodyText confidence="0.999331">
where fi and fj are the representations of the docu-
ments in the L2 space (fi, fj E L2([1, M])).
</bodyText>
<figureCaption confidence="0.800188">
Figure 1: Two documents with matching term brand
name. Dotted line: Document-1. Dashed line:
Document-2. Solid line: Their product.
</figureCaption>
<bodyText confidence="0.763943357142857">
With the above formula, a matching term in two
documents will be counted to its full term frequency
or tfidf score, while semantically related terms will
be counted less and less according their semantic
similarity to the matching term. Assuming that the
terms brand, brand name, and trade name follow
each other in the semantic order, consider the fol-
lowing example. The first document has the term
brand name, and so does the second document. In
Figure 1, it can be seen brand name is counted the
same way as it would be in a BOW model with its
full term frequency score, brand and trade name are
counted to a lesser extent, while other related terms
are considered even less.
</bodyText>
<page confidence="0.291702">
2
</page>
<figure confidence="0.989282">
1.5
1
0.5
</figure>
<figureCaption confidence="0.99205975">
Figure 2: Two documents with no matching term but
with related terms brand and trade name. Dotted line:
Document-1. Dashed line: Document-2. Solid line:
Their product.
</figureCaption>
<bodyText confidence="0.999841142857143">
Now if the two documents do not share the exact
term, only related terms occur, for instance, trade
name and brand, respectively, then the term brand
name, placed between trade name and brand in the
s semantic order, will be considered only to some
extent for the calculation of similarity (see Figure
2).
</bodyText>
<sectionHeader confidence="0.9902" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999803875">
The most widely used benchmark corpus is the
Reuters-21578 collection. For benchmarking pur-
poses, the ModApte split was adopted. 9603 doc-
uments were used as the training set and 3299 as the
test set in the experiments. Only those ninety text
categories which had at least one positive example
in the training set were included in the benchmark.
Another benchmark data corpus we used was the 20
</bodyText>
<figure confidence="0.998079357142857">
brand
brand
name
trade
name
2
1.5
1
0.5
brand
trade
name
brand
name
</figure>
<page confidence="0.993175">
188
</page>
<bodyText confidence="0.999699708333333">
Newsgroups corpus, which is a collection of approx-
imate 20,000 newsgroup documents nearly evenly
divided among 20 discussion groups and each doc-
ument is labeled as one of the 20 categories corre-
sponding to the name of the newsgroup that the doc-
ument was posted to.
In preparing the index terms, we restricted the vo-
cabulary to the terms of WordNet 3.0 in order to be
able to calculate the similarity score between any
two terms. Stop words were removed in advance.
Multiple word expressions were used to fully utilize
WordNet. We used the built-in stemmer of WordNet,
which is able to distinguish between different parts-
of-speeches if the form of the word is unambiguous.
For example, {accommodates, accommodated, ac-
commodation} was stemmed to {accommodate, ac-
commodate, accommodation}. We used term fre-
quency as term weighting.
Prior to the semantic ordering, terms were as-
sumed to be in alphabetic order. Measuring the
Jiang-Conrath distance between adjacent terms, the
average distance was 1.68 on the Reuters corpus.
Note that the Jiang-Conrath distance was normal-
ized to the interval [0, 2]. There were few terms with
zero or little distance between them. This is due to
terms which are related and start with the same word
or stem. For example, account, account executive,
account for, accountable.
The same average distance after reordering the
terms with the proposed algorithm and the Jiang-
Conrath distance was 0.56 on the same corpus.
About one third of the terms had very little distance
between each other. Nevertheless, over 10 % of the
total terms still had the maximum distance. This is
due to the non-optimal nature of the proposed term-
ordering algorithm. These terms add noise to the
classification. The noisy terms occur typically at the
two sides of the scale, that is, the leftmost terms and
the rightmost terms. While it is easy to find close
terms in the beginning, as the algorithm proceeds,
fewer terms remain in the pool to be chosen. For in-
stance, brand, brand name, trade name, label are in
the 33rd, 34th, 35th and 36th position on the left side
counting from the seed respectively, while windy,
widespread, willingly, whatsoever, worried, worth-
while close the left side, apparently sharing little in
common. The noise can be reduced by the appropri-
ate choice of the parameter b in exp(−bx2), so that
</bodyText>
<table confidence="0.9996104">
Kernel Reuters Reuters 20News 20News
Micro Macro Micro Macro
Linear 0.900 0.826 0.801 0.791
Poly 0.903 0.824 0.796 0.788
L2 0.911 0.835 0.813 0.799
</table>
<tableCaption confidence="0.999864">
Table 1: Micro- and macro-average Fl results
</tableCaption>
<bodyText confidence="0.991568666666667">
the impact of adjacent but distantly related terms can
be minimized.
Table 1 shows the results on the two benchmark
corpora with the baseline linear kernel. Precision
and recall with regard to a class ck, the F1 score
shown is their average. For all the kernels, the results
with the best parameter settings are shown. Polyno-
mial kernels were benchmarked between degrees 2
and 5. L2 kernels were benchmarked with width b
between 1 and 8, the performance peaking at 4 in
all cases. The model is able to outperform the base-
line kernels, and the differences in micro-averaged
results are statistically significant. In all cases of the
L2 kernel, the increase of F1 was due the increase in
both precision and recall.
</bodyText>
<sectionHeader confidence="0.996465" genericHeader="method">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999975181818182">
Information systems are in great need of automated
intelligent tools, but existing algorithms and meth-
ods cannot be pushed much further. Most tech-
niques in current use are impaired by the semanti-
cally poor but widespread representation of informa-
tion and knowledge. For this reason, we propose a
new formalism that combines Cruse’s idea about a
sense spectrum, approximated by semantic ordering,
and its calculation by functions.
The suggested model combines term expansion
with the semantic relations and semantic relatedness
used in semantic smoothing kernels. This slightly
unusual approach needs to transform the real vector
representation to the L2 space, and the experimen-
tal results show that this new representation can im-
prove text classification effectiveness.
Our new model also blends insights from differ-
ent approaches to lexical semantics theory at its dif-
ferent levels. First, during the semantic ordering
of terms the distributional hypothesis meets hand-
crafted lexical resources of word meaning that relate
to term occurrences as if they were their referents,
</bodyText>
<page confidence="0.996821">
189
</page>
<bodyText confidence="0.99992775">
a component external to term context. While high-
quality lexical resources enable such an ordering in
themselves, the procedure can benefit from data de-
rived from the specific corpora in study – seman-
tic relatedness measures such as the Jiang-Conrath
similarity operate this way. Secondly, once the or-
dering is done and a sense spectrum is constructed,
weights expressing statistical relationships between
terms and documents are borrowed from the vector
space model to form the basis for constructing hypo-
thetical signals of content, documents as continuous
functions.
</bodyText>
<sectionHeader confidence="0.94167" genericHeader="method">
5 Future research
</sectionHeader>
<figureCaption confidence="0.999178">
Figure 3: A hypothetical spectrum of terms.
</figureCaption>
<bodyText confidence="0.999538857142857">
As we have shown, a spectral interpretation of
sense granularity can lead to improved text catego-
rization results by utilizing L2 space for informa-
tion representation. Whether non-periodic functions
other than the variant tested in this paper can be ap-
plied to the same end needs to be explored.
Turning back to the use of the spectrum of visi-
ble light for representing meaning, this raises new
research questions. On the one hand, translat-
ing one-dimensional semantic ordering into colors
is straightforward. Consider the following map-
ping. Assume that a language has a finite N num-
ber of terms, so the 1-dimensional result is an or-
dered list o1, o2, ... , oN. Calculate the following:
</bodyText>
<equation confidence="0.779529333333333">
N−
1
A = �z=1 d(oz, oz+1), where A is the sum of
</equation>
<bodyText confidence="0.990394333333333">
distances between consecutive words. Further let
F : [0, A] → [400, 700] be the following map-
ping: F(x) = 400 + x300Δ . The visible spectrum
is between 400 and 700 nm, F maps the cumulative
distances of terms from [0, A] to the visible spec-
trum congruently, i.e. without distorting the dis-
tances. With this bijective (one-to-one) mapping,
each term is assigned a physical wavelength and fre-
quency. Figure 3 shows an example of such a term
spectrum.
On the other hand, we have only begun to test the
applicability of periodic functions in L2 space (Wit-
tek and Dar´anyi, 2007), hence a well-established
link to semantic computing by waves is missing for
the time being. A possible compromise between the
non-periodic vs. periodic approaches can be to ap-
ply wavelets instead of waves, a direction where our
ongoing research shows promising results. These
will be reported elsewhere. In a broader frame of
thought, we are also working on the optical equiv-
alents of the vector space model and the general-
ized vector space model as a first step toward coding
more semantics in mathematical objects, and putting
them to work in novel computing environments.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999812666666667">
The authors are grateful to Martha Palmer (Univer-
sity of Colorado Boulder) for her inspiring sugges-
tions and advice on sense granularity.
</bodyText>
<sectionHeader confidence="0.99784" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997324409090909">
E. Agirre and O.L. De Lacalle. 2003. Clustering Word-
Net word senses. In Proceedings of RANLP-03, 4th
International Conference on Recent Advances in Nat-
ural Language Processing, pages 121–130.
E. Agirre, E. Alfonseca, and O.L. de Lacalle. 2004. Ap-
proximating hierarchy-based similarity for WordNet
nominal synsets using topic signatures. In Proceed-
ings of GWC-04, 2nd Global WordNet Conference,
pages 15–22.
R. Basili, M. Cammisa, and A. Moschitti. 2005. Effec-
tive use of WordNet semantics via kernel-based learn-
ing. In Proceedings of CoNLL-05, 9th Conference on
Computational Natural Language Learning, pages 1–
8.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. Proceed-
ings of ICDM-06, 6th IEEE International Conference
on Data Mining.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13–47.
</reference>
<page confidence="0.988721">
190
</page>
<reference confidence="0.99942081372549">
N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. La-
tent semantic kernels. Journal of Intelligent Informa-
tion Systems, 18(2):127–152.
D.A. Cruse. 1986. Lexical semantics.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society forInforma-
tion Science, 41(6):391–407.
C. Dorrer, P. Londero, M. Anderson, S. Wallentowitz, and
IA Walmsley. 2001. Computing with interference:
all-optical single-query 50-element database search.
In Proceedings of QELS-01, Quantum Electronics and
Laser Science Conference, pages 149–150.
E. Gabrilovich and S. Markovitch. 2005. Feature gen-
eration for text categorization using world knowledge.
In Proceedings of IJCAI-05, 19th International Joint
Conference on Artificial Intelligence, volume 19.
E. Hoenkamp. 2003. Unitary operators on the document
space. Journal of the American Society for Informa-
tion Science and Technology, 54(4):314–320.
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet
improves text document clustering. In Proceedings of
SIGIR-03, 26th ACM International Conference on Re-
search and Development in Information Retrieval.
J. Hu, L. Fang, Y. Cao, H.J. Zeng, H. Li, Q. Yang, and
Z. Chen. 2008. Enhancing text clustering by lever-
aging Wikipedia semantics. In Proceedings of SIGIR-
08, 31st ACM International Conference on Research
and Development in Information Retrieval, pages 179–
186.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, pages 19–33.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of ECML-98, 10th European Confer-
ence on Machine Learning, pages 137–142.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL Workshop
on Usage of WordNet in Natural Language Processing
Systems, volume 98, pages 768–773.
D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis,
M. Theobald, and G. Weikum. 2005. Word sense
disambiguation for exploiting hierarchical thesauri
in text classification. Proceedings of PKDD-05,
9th European Conference on the Principles of Data
Mining and Knowledge Discovery, pages 181–192.
S. Mohammad and G. Hirst. 2005. Distributional mea-
sures as proxies for semantic relatedness.
H. Paijmans. 1997. Gravity wells of meaning: detecting
information-rich passages in scientific texts. Journal
of Documentation, 53(5):520–536.
M. Palmer, H.T. Dang, and C. Fellbaum. 2006. Mak-
ing fine-grained and coarse-grained sense distinctions,
both manually and automatically. Natural Language
Engineering, 13(02):137–163.
V.V. Raghavan and S.K.M. Wong. 1986. A critical anal-
ysis of vector space model for information retrieval.
Journal of the American Society for Information Sci-
ence, 37(5):279–287.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
IJCAI-95, 14th International Joint Conference on Ar-
tificial Intelligence, volume 1, pages 448–453.
J. Rodd, G. Gaskell, and W. Marslen-Wilson. 2002.
Making sense of semantic ambiguity: Semantic com-
petition in lexical access. Journal of Memory and Lan-
guage, 46(2):245–266.
M.D.E.B. Rodriguez and J.M.G. Hidalgo. 1997. Using
WordNet to complement training information in text
categorisation. In Procedings of RANLP-97, 2nd In-
ternational Conference on RecentAdvances in Natural
Language Processing.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis.
S. Shi, J.R. Wen, Q. Yu, R. Song, and W.Y. Ma. 2005.
Gravitation-based model for information retrieval. In
Proceedings of SIGIR-05, 28th ACM International
Conference on Research and Development in Informa-
tion Retrieval, pages 488–495. ACM New York, NY,
USA.
G. Siolas and F. d’Alch´e Buc. 2000. Support vector ma-
chines based on a semantic kernel for text categoriza-
tion. In Proceedings of IJCNN-00, IEEE International
Joint Conference on Neural Networks.
C. J. van Rijsbergen. 2004. The Geometry of Information
Retrieval.
P. Wittek and S. Dar´anyi. 2007. Representing word
semantics for IR by continuous functions. In S. Do-
minich and F. Kiss, editors, Proceedings of ICTIR-07,
1st International Conference of the Theory of Informa-
tion Retrieval, pages 149–155.
P. Wittek, C.L. Tan, and S. Dar´anyi. 2009. An or-
dering of terms based on semantic relatedness. In
H. Bunt, editor, Proceedings of IWCS-09, 8th Inter-
national Conference on Computational Semantics.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of SIGIR-85, 8th ACM International
Conference on Research and Development in Informa-
tion Retrieval, pages 18–25.
</reference>
<page confidence="0.998476">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226254">
<title confidence="0.998957">Improving Text Classification by a Sense Spectrum Approach to Term Expansion</title>
<author confidence="0.999851">Peter Wittek S´andor Dar´anyi Chew Lim Tan</author>
<affiliation confidence="0.9995925">Department of Computer Science Swedish School of Library Department of Computer National University of Singapore and Information Science National University of Singapore</affiliation>
<address confidence="0.726658">Computing 1, Law Link G¨oteborg University &amp; Computing 1, Law Link</address>
<date confidence="0.5761485">Singapore 117590 University of Bor˚as Singapore 117590 1</date>
<address confidence="0.97933">50190 Bor˚as, Sweden</address>
<email confidence="0.990157">sandor.daranyi@hb.se</email>
<abstract confidence="0.993141809523809">Experimenting with different mathematical objects for text representation is an important step of building text classification models. In order to be efficient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in index terms. We introduce an algorithm for sense-based semantic ordering of index terms which approximates Cruse’s description of a sense spectrum. Following semantic ordering, text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among index terms while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O L De Lacalle</author>
</authors>
<title>Clustering WordNet word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-03, 4th International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>121--130</pages>
<marker>Agirre, De Lacalle, 2003</marker>
<rawString>E. Agirre and O.L. De Lacalle. 2003. Clustering WordNet word senses. In Proceedings of RANLP-03, 4th International Conference on Recent Advances in Natural Language Processing, pages 121–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>O L de Lacalle</author>
</authors>
<title>Approximating hierarchy-based similarity for WordNet nominal synsets using topic signatures.</title>
<date>2004</date>
<booktitle>In Proceedings of GWC-04, 2nd Global WordNet Conference,</booktitle>
<pages>15--22</pages>
<marker>Agirre, Alfonseca, de Lacalle, 2004</marker>
<rawString>E. Agirre, E. Alfonseca, and O.L. de Lacalle. 2004. Approximating hierarchy-based similarity for WordNet nominal synsets using topic signatures. In Proceedings of GWC-04, 2nd Global WordNet Conference, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M Cammisa</author>
<author>A Moschitti</author>
</authors>
<title>Effective use of WordNet semantics via kernel-based learning.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-05, 9th Conference on Computational Natural Language Learning,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10982" citStr="Basili et al., 2005" startWordPosition="1726" endWordPosition="1729">ional Euclidean space. This representation is also referred to as the bag-of-words (BOW) model. Given this representation, semantic relatedness of a pair of text fragments is computed as the cosine aiaj S(ai, aj) = |ai||a|j 2.2 Linear Semantic Kernels One enrichment strategy is to use a semantic smoothing kernel while calculating the similarity between two documents. Any linear kernel for texts is characterized by K(ai, aj) = a&apos;iS&apos;Saj, where S is an appropriately shaped matrix commonly referred to as semantic smoothing matrix (Siolas and d’Alch´e Buc, 2000; Shawe-Taylor and Cristianini, 2004; Basili et al., 2005; Mavroeidis et al., 2005; Bloehdorn et al., 2006). The presence of S changes the orthogonality of the vector space model, as this mapping should introduce term dependence. A recent attempt tried to manually construct S with the help of a lexical resource (Siolas and d’Alch´e Buc, 2000). The entries in the symmetric matrix S express the semantic similarity between the terms i and j. Entries in this matrix are inversely proportional to the length of the WordNet hierarchy path linking the two terms. The performance, measured over the 20NewsGroups corpus, showed an improvement of 2 % over the the</context>
<context position="12221" citStr="Basili et al., 2005" startWordPosition="1929" endWordPosition="1932"> method. Moreover, the semantic matrix S is almost fully dense, hence computational issues arise. In a similar construction, (Bloehdorn et al., 2006) defined the matrix entries as weights of superconcepts of the two terms in the WordNet hierarchy. Focusing on special subcategories of Reuters-21578 and on the TREC Question Answering Dataset, they showed consistent improvement over the baseline. As (Mavroeidis et al., 2005) pointed out, polysemy will remain a problem in semantic smoothing kernels. A more complex way of calculating the semantic similarity as the matrix entries was also proposed (Basili et al., 2005). For a more general discussion on semantic similarity see Section 2.4.1. An early attempt to overcome the untenable orthogonality assumption of the vector space model was proposed under the name of generalized vector space model (Wong et al., 1985). The article which proposed the model did not provide empirical results, and since then the model has been regarded of large theoretical importance with less impact on actual applications. The model takes a distri. (1) 185 butional approach, focusing on term co-occurrences. The underlying assumption is that term correlations are captured by the co-</context>
</contexts>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>R. Basili, M. Cammisa, and A. Moschitti. 2005. Effective use of WordNet semantics via kernel-based learning. In Proceedings of CoNLL-05, 9th Conference on Computational Natural Language Learning, pages 1– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bloehdorn</author>
<author>R Basili</author>
<author>M Cammisa</author>
<author>A Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>Proceedings of ICDM-06, 6th IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="11032" citStr="Bloehdorn et al., 2006" startWordPosition="1734" endWordPosition="1737">also referred to as the bag-of-words (BOW) model. Given this representation, semantic relatedness of a pair of text fragments is computed as the cosine aiaj S(ai, aj) = |ai||a|j 2.2 Linear Semantic Kernels One enrichment strategy is to use a semantic smoothing kernel while calculating the similarity between two documents. Any linear kernel for texts is characterized by K(ai, aj) = a&apos;iS&apos;Saj, where S is an appropriately shaped matrix commonly referred to as semantic smoothing matrix (Siolas and d’Alch´e Buc, 2000; Shawe-Taylor and Cristianini, 2004; Basili et al., 2005; Mavroeidis et al., 2005; Bloehdorn et al., 2006). The presence of S changes the orthogonality of the vector space model, as this mapping should introduce term dependence. A recent attempt tried to manually construct S with the help of a lexical resource (Siolas and d’Alch´e Buc, 2000). The entries in the symmetric matrix S express the semantic similarity between the terms i and j. Entries in this matrix are inversely proportional to the length of the WordNet hierarchy path linking the two terms. The performance, measured over the 20NewsGroups corpus, showed an improvement of 2 % over the the basic vector space method. Moreover, the semantic</context>
</contexts>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. Proceedings of ICDM-06, 6th IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="6016" citStr="Budanitsky and Hirst, 2006" startWordPosition="917" endWordPosition="920">tedness between word pairs can be expressed in an ordered form while preserving lexical field structure, and if (3) the uniqueness of entries in such an order can be expressed by functions rather than scalars such as distance. As we will show, this line of thought leads to performance improvement in text classification by using kernel-based feature weighting. Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts (Raghavan and Wong, 1986), arguing if distributional similarity is an adequate proxy for lexical semantic relatedness (Budanitsky and Hirst, 2006). We argue for the need to enrich distributional semantics-based text representation by other components because with the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical approaches have been proliferating, especially with developments in kernel methods in the la</context>
<context position="17314" citStr="Budanitsky and Hirst, 2006" startWordPosition="2762" endWordPosition="2765">semantic order, then the semantic kernels in the L2 space are discussed. 2.4.1 An Algorithm for a Semantic Ordering of Terms The proposed kernels assume that there is a semantic order between terms. Let V denote a set of SAI Sj) = (2) ��j|. aiaj 186 terms {t1, t2, ... , tn} and let d(ti, tj) denote the semantic distance between the terms ti and tj. The initial order of the terms is not relevant, though it is assumed to be alphabetic. Let G = (V, E) denote a weighted undirected graph, where the weights in the set E are defined by the distances between the terms. Various lexical resource-based (Budanitsky and Hirst, 2006) and distributional measures (Mohammad and Hirst, 2005) have been proposed to measure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
<author>H Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2002</date>
<journal>Journal of Intelligent Information Systems,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="6668" citStr="Cristianini et al., 2002" startWordPosition="1022" endWordPosition="1025">enrich distributional semantics-based text representation by other components because with the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical approaches have been proliferating, especially with developments in kernel methods in the last decade (Joachims, 1998; Cristianini et al., 2002). Some researchers suggested a more general mathematical framework to accommodate the needs that the vector space model cannot satisfy (van Rijsbergen, 2004). This paper explores the opportunities of this representation in the domain of text classification by introducing it as a new nonlinear semantic kernel. Another aspect of the same problem is term expansion for document classification and retrieval. By automatically selecting expansion terms for a text classification system to expand a document vector by adding terms that are related to the terms already in the document, performance can be</context>
<context position="14795" citStr="Cristianini et al., 2002" startWordPosition="2350" endWordPosition="2353">0). Using rank reduction, terms that occur together very often in the same documents are merged into a single dimension of the feature space. The dimensions of the reduced space correspond to the axes of greatest variance. For latent semantic indexing, by dual representation the kernel matrix is K = V E2kV &apos;, where Ek is a diagonal matrix containing the k largest singular values of the singular value decomposition of the vector space, and V holds the right singular vectors of the decomposition. The new kernel matrix can be obtained directly from K by applying an eigenvalue decomposition of K (Cristianini et al., 2002). The computational complexity of performing an eigenvalue decomposition on the kernel matrix is a major drawback of latent semantic indexing. 2.3 Text Representation Enrichment Strategies by Term Expansion In order to eliminate the bottleneck of the traditional BOW representation, previous approaches in term expansion enriched this convention by external lexical resources such as WordNet. As a first step, these methods generate new features for each document in the dataset. These new features can be synonyms or homonyms of document terms as in (Hotho et al., 2003; Rodriguez and Hidalgo, 1997)</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2002</marker>
<rawString>N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. Latent semantic kernels. Journal of Intelligent Information Systems, 18(2):127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<date>1986</date>
<note>Lexical semantics.</note>
<contexts>
<context position="2979" citStr="Cruse, 1986" startWordPosition="439" endWordPosition="440"> implied in their method, we speculated that the following components could be useful in a rephrased model: (a) a metaphorically presented spectral expression of lexical semantic phenomena, (b) a ranked onedimensional condensate of multidimensional sense structure, and (c) representation of documents and queries by functions in L2 space with a similarity measure. Our anticipation was that by matching these components, a new model could demonstrate new capacities in general, and contribute to computing meaning by waves in particular. Semantic ordering (component b) is an approximation of what (Cruse, 1986) referred to as a sense spectrum, i.e. a series of points - called local senses and constituting lexical units -, in a one-dimensional semantic continuum (component a). Apart from differentiating between the conceptual content of the same word in terms of its senses in word pairs, i.e. their semantic relatedness, it also compresses the result in spectral form. The scalar values of this spectrum have the double potential of being a condensed measure for semantic weighting, and, tentatively, they can play the role of mass in experiments where gravity is called in as a metaphor for text categoriz</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>D.A. Cruse. 1986. Lexical semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society forInformation Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="14172" citStr="Deerwester et al., 1990" startWordPosition="2241" endWordPosition="2244">guage processing via a kernel. Conceptually, latent semantic indexing is similar to the generalized vector space model, it measures semantic information through co-occurrence analysis in the corpus. From the algorithmic perspective it is an enormous problem that textual data have a large number of relevant features. This results in huge computational needs and the classification models may overfit the data. The number of features can be reduced by multivariate feature extraction methods. In latent semantic indexing, the dimension of the vector space is reduced by singular value decomposition (Deerwester et al., 1990). Using rank reduction, terms that occur together very often in the same documents are merged into a single dimension of the feature space. The dimensions of the reduced space correspond to the axes of greatest variance. For latent semantic indexing, by dual representation the kernel matrix is K = V E2kV &apos;, where Ek is a diagonal matrix containing the k largest singular values of the singular value decomposition of the vector space, and V holds the right singular vectors of the decomposition. The new kernel matrix can be obtained directly from K by applying an eigenvalue decomposition of K (Cr</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society forInformation Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dorrer</author>
<author>P Londero</author>
<author>M Anderson</author>
<author>S Wallentowitz</author>
<author>IA Walmsley</author>
</authors>
<title>Computing with interference: all-optical single-query 50-element database search.</title>
<date>2001</date>
<booktitle>In Proceedings of QELS-01, Quantum Electronics and Laser Science Conference,</booktitle>
<pages>149--150</pages>
<contexts>
<context position="1964" citStr="Dorrer et al., 2001" startWordPosition="282" endWordPosition="285">ased on lexical relatedness. 1 Introduction Generally, building an automated text classification system consists of two key subtasks. The first task is text representation which converts the content of documents into compact format so that they can be further processed by the text classifiers. Another task is to learn the model of a text classifier which is used to classify the unlabeled documents. This paper proposes a substantially new model for text representation to improve effectiveness of text classification by semantic ordering. Our motivation for the research presented here came from (Dorrer et al., 2001) who demonstrated the viability of database searching by visible light using a quantum algorithm, albeit on meaningless items. The question was, what kind of document representation would be necessary to extend their in-principle results to include semantics, one that has been leading us to test both periodic and nonperiodic functions for this purpose. Since representation and retrieval by colors was implied in their method, we speculated that the following components could be useful in a rephrased model: (a) a metaphorically presented spectral expression of lexical semantic phenomena, (b) a r</context>
</contexts>
<marker>Dorrer, Londero, Anderson, Wallentowitz, Walmsley, 2001</marker>
<rawString>C. Dorrer, P. Londero, M. Anderson, S. Wallentowitz, and IA Walmsley. 2001. Computing with interference: all-optical single-query 50-element database search. In Proceedings of QELS-01, Quantum Electronics and Laser Science Conference, pages 149–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Feature generation for text categorization using world knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-05, 19th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>19</volume>
<contexts>
<context position="15493" citStr="Gabrilovich and Markovitch, 2005" startWordPosition="2458" endWordPosition="2461">osition on the kernel matrix is a major drawback of latent semantic indexing. 2.3 Text Representation Enrichment Strategies by Term Expansion In order to eliminate the bottleneck of the traditional BOW representation, previous approaches in term expansion enriched this convention by external lexical resources such as WordNet. As a first step, these methods generate new features for each document in the dataset. These new features can be synonyms or homonyms of document terms as in (Hotho et al., 2003; Rodriguez and Hidalgo, 1997), or expanded features for terms, sentences and documents as in (Gabrilovich and Markovitch, 2005), or term context information for word sense disambiguation such as topic signatures (Agirre and De Lacalle, 2003; Agirre et al., 2004). Then, the generated new features replace the old ones or are appended to the document representation, and construct a new vector representation ai for each text document. The similarity measure of document pairs is defined as: 2.4 Our Framework The basic assumption of our framework is that terms can be arranged in an order such that consecutive terms are semantically related. Hence each term acquires a unique position, and this position ties the term to its s</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2005</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2005. Feature generation for text categorization using world knowledge. In Proceedings of IJCAI-05, 19th International Joint Conference on Artificial Intelligence, volume 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hoenkamp</author>
</authors>
<title>Unitary operators on the document space.</title>
<date>2003</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>54</volume>
<issue>4</issue>
<contexts>
<context position="20937" citStr="Hoenkamp, 2003" startWordPosition="3417" endWordPosition="3418">t from Step 2 until V = ∅. The above algorithm can be thought of as a modified Prim’s algorithm, but it does not find the optimal minimum-weight spanning tree. 2.4.2 Semantic Kernels in the L2 Space The L2 space shares resemblance with a real vector space. Real-valued vectors are replaced by square-integrable functions, and the dot product is replaced by the following inner product: (fi, fj) = f fifjdx, for some fi, fj in the given L2 space. Lately, Hoenkamp has also pointed out that the L2 space can be used for information retrieval when 187 he introduced a Haar basis for the document space (Hoenkamp, 2003). He utilized a signal processing framework within the context of latent semantic indexing. In order to apply an L2 representation for text classification, the problem is approached from a different angle than by Hoenkamp, taking discounting expansion terms as our point of departure. Assigning a function w(x − k) to the term in the kth position in a semantic order, a document j can be expressed as follows: M fj(x) = E akjw(x − k), (3) k=1 where x is in [1, M], and it is the variable of integration in calculating the inner product of the L2; x can be regarded as a “dummy” variable carrying no m</context>
</contexts>
<marker>Hoenkamp, 2003</marker>
<rawString>E. Hoenkamp. 2003. Unitary operators on the document space. Journal of the American Society for Information Science and Technology, 54(4):314–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hotho</author>
<author>S Staab</author>
<author>G Stumme</author>
</authors>
<title>WordNet improves text document clustering.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR-03, 26th ACM International Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="15365" citStr="Hotho et al., 2003" startWordPosition="2440" endWordPosition="2443">ue decomposition of K (Cristianini et al., 2002). The computational complexity of performing an eigenvalue decomposition on the kernel matrix is a major drawback of latent semantic indexing. 2.3 Text Representation Enrichment Strategies by Term Expansion In order to eliminate the bottleneck of the traditional BOW representation, previous approaches in term expansion enriched this convention by external lexical resources such as WordNet. As a first step, these methods generate new features for each document in the dataset. These new features can be synonyms or homonyms of document terms as in (Hotho et al., 2003; Rodriguez and Hidalgo, 1997), or expanded features for terms, sentences and documents as in (Gabrilovich and Markovitch, 2005), or term context information for word sense disambiguation such as topic signatures (Agirre and De Lacalle, 2003; Agirre et al., 2004). Then, the generated new features replace the old ones or are appended to the document representation, and construct a new vector representation ai for each text document. The similarity measure of document pairs is defined as: 2.4 Our Framework The basic assumption of our framework is that terms can be arranged in an order such that </context>
</contexts>
<marker>Hotho, Staab, Stumme, 2003</marker>
<rawString>A. Hotho, S. Staab, and G. Stumme. 2003. WordNet improves text document clustering. In Proceedings of SIGIR-03, 26th ACM International Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hu</author>
<author>L Fang</author>
<author>Y Cao</author>
<author>H J Zeng</author>
<author>H Li</author>
<author>Q Yang</author>
<author>Z Chen</author>
</authors>
<title>Enhancing text clustering by leveraging Wikipedia semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR08, 31st ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>179--186</pages>
<contexts>
<context position="7295" citStr="Hu et al., 2008" startWordPosition="1122" endWordPosition="1125">archers suggested a more general mathematical framework to accommodate the needs that the vector space model cannot satisfy (van Rijsbergen, 2004). This paper explores the opportunities of this representation in the domain of text classification by introducing it as a new nonlinear semantic kernel. Another aspect of the same problem is term expansion for document classification and retrieval. By automatically selecting expansion terms for a text classification system to expand a document vector by adding terms that are related to the terms already in the document, performance can be improved (Hu et al., 2008). Such new terms can either be statistically related to the original terms or chosen from lexical resources such as thesauri, controlled vocabularies, ontologies and the like. However, in doing so the fundamental question often overlooked is whether the expansion terms extracted are equally related to the document and are useful for text classification. In what follows we propose a form of term expansion with decreasing importance of those terms that are less related, as contrasted with rigid term expansion. This can be carried out by a combination of semantic ordering and using function space</context>
</contexts>
<marker>Hu, Fang, Cao, Zeng, Li, Yang, Chen, 2008</marker>
<rawString>J. Hu, L. Fang, Y. Cao, H.J. Zeng, H. Li, Q. Yang, and Z. Chen. 2008. Enhancing text clustering by leveraging Wikipedia semantics. In Proceedings of SIGIR08, 31st ACM International Conference on Research and Development in Information Retrieval, pages 179– 186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="18020" citStr="Jiang and Conrath, 1997" startWordPosition="2872" endWordPosition="2875">ure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that combine the advantages of both approaches have also been developed (Resnik, 1995; Jiang and Conrath, 1997). This paper relies on the Jiang-Conrath composite measure (Jiang and Conrath, 1997), which has been shown to be superior to other measures (Budanitsky and Hirst, 2006), and we also found that this measure works the best for the purpose. The Jiang-Conrath metric measures the distance between two senses by using the hierarchy of WordNet. By denoting the lowest superordinate of two senses s1 and s2 in the hierarchy with LSuper(s1,s2), the metric is calculated as follows: d(s1, s2) = IC(s1)+IC(s2)−2IC(LSuper(s1, s2)), where IC(s) is the information content of a sense s based on a corpus. Distance</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="6641" citStr="Joachims, 1998" startWordPosition="1020" endWordPosition="1021">for the need to enrich distributional semantics-based text representation by other components because with the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical approaches have been proliferating, especially with developments in kernel methods in the last decade (Joachims, 1998; Cristianini et al., 2002). Some researchers suggested a more general mathematical framework to accommodate the needs that the vector space model cannot satisfy (van Rijsbergen, 2004). This paper explores the opportunities of this representation in the domain of text classification by introducing it as a new nonlinear semantic kernel. Another aspect of the same problem is term expansion for document classification and retrieval. By automatically selecting expansion terms for a text classification system to expand a document vector by adding terms that are related to the terms already in the d</context>
<context position="9210" citStr="Joachims, 1998" startWordPosition="1440" endWordPosition="1441">= {c1, c2, ..., cjCj} set of predefined categories, the task is, for each document dj (j E {1, 2, ... , N}), to assign a decision to file dj under ci or a decision not to file dj under ci (ci E C) by virtue of a function Φ, where the function Φ is also referred to as the classifier, or model, or hypothesis, or rule. Supervised text classification is a machine learning technique for creating the function Φ from training data. The training data consist of pairs of input documents, and desired outputs (i.e., classes). Support vector machines have been found the most effective by several authors (Joachims, 1998). The proposed semantic text classification method is grounded in the kernel methods underlying support vector machines. A support vector machine is a kind of supervised learning algorithm. In its simplest, linear form, a support vector machine is a hyperplane that separates a set of positive examples from a set of negative examples with maximum margin (Shawe-Taylor and Cristianini, 2004). The strength of kernel methods is that they allow a mapping φ(.) of x to a higher dimensional space. In the dual formulation of the mathematical programming problem, only the kernel matrix K(xi,xj) = φ(xi)&apos;φ</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<volume>98</volume>
<pages>768--773</pages>
<contexts>
<context position="17748" citStr="Lin, 1998" startWordPosition="2833" endWordPosition="2834">) denote a weighted undirected graph, where the weights in the set E are defined by the distances between the terms. Various lexical resource-based (Budanitsky and Hirst, 2006) and distributional measures (Mohammad and Hirst, 2005) have been proposed to measure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that combine the advantages of both approaches have also been developed (Resnik, 1995; Jiang and Conrath, 1997). This paper relies on the Jiang-Conrath composite measure (Jiang and Conrath, 1997), which has been shown to be superior to other measures (Budanitsky and Hirst, 2006), and we also found that this measure works the best for the purpose. The Jiang-Conrath metric measures the distance between two senses by using the hierarchy o</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems, volume 98, pages 768–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mavroeidis</author>
<author>G Tsatsaronis</author>
<author>M Vazirgiannis</author>
<author>M Theobald</author>
<author>G Weikum</author>
</authors>
<title>Word sense disambiguation for exploiting hierarchical thesauri in text classification.</title>
<date>2005</date>
<booktitle>Proceedings of PKDD-05, 9th European Conference on the Principles of Data Mining and Knowledge Discovery,</booktitle>
<pages>181--192</pages>
<contexts>
<context position="11007" citStr="Mavroeidis et al., 2005" startWordPosition="1730" endWordPosition="1733">. This representation is also referred to as the bag-of-words (BOW) model. Given this representation, semantic relatedness of a pair of text fragments is computed as the cosine aiaj S(ai, aj) = |ai||a|j 2.2 Linear Semantic Kernels One enrichment strategy is to use a semantic smoothing kernel while calculating the similarity between two documents. Any linear kernel for texts is characterized by K(ai, aj) = a&apos;iS&apos;Saj, where S is an appropriately shaped matrix commonly referred to as semantic smoothing matrix (Siolas and d’Alch´e Buc, 2000; Shawe-Taylor and Cristianini, 2004; Basili et al., 2005; Mavroeidis et al., 2005; Bloehdorn et al., 2006). The presence of S changes the orthogonality of the vector space model, as this mapping should introduce term dependence. A recent attempt tried to manually construct S with the help of a lexical resource (Siolas and d’Alch´e Buc, 2000). The entries in the symmetric matrix S express the semantic similarity between the terms i and j. Entries in this matrix are inversely proportional to the length of the WordNet hierarchy path linking the two terms. The performance, measured over the 20NewsGroups corpus, showed an improvement of 2 % over the the basic vector space metho</context>
</contexts>
<marker>Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald, Weikum, 2005</marker>
<rawString>D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis, M. Theobald, and G. Weikum. 2005. Word sense disambiguation for exploiting hierarchical thesauri in text classification. Proceedings of PKDD-05, 9th European Conference on the Principles of Data Mining and Knowledge Discovery, pages 181–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>G Hirst</author>
</authors>
<title>Distributional measures as proxies for semantic relatedness.</title>
<date>2005</date>
<contexts>
<context position="17369" citStr="Mohammad and Hirst, 2005" startWordPosition="2769" endWordPosition="2773"> are discussed. 2.4.1 An Algorithm for a Semantic Ordering of Terms The proposed kernels assume that there is a semantic order between terms. Let V denote a set of SAI Sj) = (2) ��j|. aiaj 186 terms {t1, t2, ... , tn} and let d(ti, tj) denote the semantic distance between the terms ti and tj. The initial order of the terms is not relevant, though it is assumed to be alphabetic. Let G = (V, E) denote a weighted undirected graph, where the weights in the set E are defined by the distances between the terms. Various lexical resource-based (Budanitsky and Hirst, 2006) and distributional measures (Mohammad and Hirst, 2005) have been proposed to measure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that combine the advantages of both approaches have also bee</context>
</contexts>
<marker>Mohammad, Hirst, 2005</marker>
<rawString>S. Mohammad and G. Hirst. 2005. Distributional measures as proxies for semantic relatedness.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paijmans</author>
</authors>
<title>Gravity wells of meaning: detecting information-rich passages in scientific texts.</title>
<date>1997</date>
<journal>Journal of Documentation,</journal>
<volume>53</volume>
<issue>5</issue>
<contexts>
<context position="3626" citStr="Paijmans, 1997" startWordPosition="546" endWordPosition="548"> i.e. a series of points - called local senses and constituting lexical units -, in a one-dimensional semantic continuum (component a). Apart from differentiating between the conceptual content of the same word in terms of its senses in word pairs, i.e. their semantic relatedness, it also compresses the result in spectral form. The scalar values of this spectrum have the double potential of being a condensed measure for semantic weighting, and, tentatively, they can play the role of mass in experiments where gravity is called in as a metaphor for text categorization and information retrieval (Paijmans, 1997; Shi et al., 2005; Wittek et al., 2009). 183 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183–191, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics This paper addresses text categorization by means of non-periodical functions only. In support of Cruse’s point, recently it has been demonstrated by measurements that sense classification errors made by their maximum entropy based word sense disambiguation system were partly remedied once instead of a fine-grained view, a more coarse-grained view of senses was ado</context>
</contexts>
<marker>Paijmans, 1997</marker>
<rawString>H. Paijmans. 1997. Gravity wells of meaning: detecting information-rich passages in scientific texts. Journal of Documentation, 53(5):520–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>H T Dang</author>
<author>C Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="4252" citStr="Palmer et al., 2006" startWordPosition="635" endWordPosition="638">et al., 2005; Wittek et al., 2009). 183 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183–191, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics This paper addresses text categorization by means of non-periodical functions only. In support of Cruse’s point, recently it has been demonstrated by measurements that sense classification errors made by their maximum entropy based word sense disambiguation system were partly remedied once instead of a fine-grained view, a more coarse-grained view of senses was adopted (Palmer et al., 2006). Improvement of sense classification accuracy linked with “zooming out” in terms of observation granularity indicates, in our eyes, the “fluid”, perhaps spectral nature of sense inasmuch as it is impossible to precisely distinguish between the borderlines and some fuzziness is implied both in the phenomenon and its perception. This “fluidity of language”, as Palmer et al. call it, is in accord with the theory of shared semantic representations in psycholinguistics (Rodd et al., 2002), according to which related senses share a portion of their meaning representation in the mental lexicon; it a</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2006</marker>
<rawString>M. Palmer, H.T. Dang, and C. Fellbaum. 2006. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(02):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V V Raghavan</author>
<author>S K M Wong</author>
</authors>
<title>A critical analysis of vector space model for information retrieval.</title>
<date>1986</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>37</volume>
<issue>5</issue>
<contexts>
<context position="5895" citStr="Raghavan and Wong, 1986" startWordPosition="901" endWordPosition="904">(1) whether distributional semantics alone is enough for the representation of word meaning, (2) whether semantic relatedness between word pairs can be expressed in an ordered form while preserving lexical field structure, and if (3) the uniqueness of entries in such an order can be expressed by functions rather than scalars such as distance. As we will show, this line of thought leads to performance improvement in text classification by using kernel-based feature weighting. Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts (Raghavan and Wong, 1986), arguing if distributional similarity is an adequate proxy for lexical semantic relatedness (Budanitsky and Hirst, 2006). We argue for the need to enrich distributional semantics-based text representation by other components because with the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision</context>
</contexts>
<marker>Raghavan, Wong, 1986</marker>
<rawString>V.V. Raghavan and S.K.M. Wong. 1986. A critical analysis of vector space model for information retrieval. Journal of the American Society for Information Science, 37(5):279–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95, 14th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>448--453</pages>
<contexts>
<context position="17994" citStr="Resnik, 1995" startWordPosition="2870" endWordPosition="2871">oposed to measure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that combine the advantages of both approaches have also been developed (Resnik, 1995; Jiang and Conrath, 1997). This paper relies on the Jiang-Conrath composite measure (Jiang and Conrath, 1997), which has been shown to be superior to other measures (Budanitsky and Hirst, 2006), and we also found that this measure works the best for the purpose. The Jiang-Conrath metric measures the distance between two senses by using the hierarchy of WordNet. By denoting the lowest superordinate of two senses s1 and s2 in the hierarchy with LSuper(s1,s2), the metric is calculated as follows: d(s1, s2) = IC(s1)+IC(s2)−2IC(LSuper(s1, s2)), where IC(s) is the information content of a sense s b</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI-95, 14th International Joint Conference on Artificial Intelligence, volume 1, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rodd</author>
<author>G Gaskell</author>
<author>W Marslen-Wilson</author>
</authors>
<title>Making sense of semantic ambiguity: Semantic competition in lexical access.</title>
<date>2002</date>
<journal>Journal of Memory and Language,</journal>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="4741" citStr="Rodd et al., 2002" startWordPosition="712" endWordPosition="715"> were partly remedied once instead of a fine-grained view, a more coarse-grained view of senses was adopted (Palmer et al., 2006). Improvement of sense classification accuracy linked with “zooming out” in terms of observation granularity indicates, in our eyes, the “fluid”, perhaps spectral nature of sense inasmuch as it is impossible to precisely distinguish between the borderlines and some fuzziness is implied both in the phenomenon and its perception. This “fluidity of language”, as Palmer et al. call it, is in accord with the theory of shared semantic representations in psycholinguistics (Rodd et al., 2002), according to which related senses share a portion of their meaning representation in the mental lexicon; it also supports an earlier observation of two of the present authors based on the same methodology as outlined in this paper, namely that using continuous functions for information retrieval leads to content representation without exact term or document locations, one which is regional in its nature and subject to a mathematical uncertainty principle (Wittek and Dar´anyi, 2007). We approach our problem in three steps: (1) whether distributional semantics alone is enough for the represent</context>
</contexts>
<marker>Rodd, Gaskell, Marslen-Wilson, 2002</marker>
<rawString>J. Rodd, G. Gaskell, and W. Marslen-Wilson. 2002. Making sense of semantic ambiguity: Semantic competition in lexical access. Journal of Memory and Language, 46(2):245–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D E B Rodriguez</author>
<author>J M G Hidalgo</author>
</authors>
<title>Using WordNet to complement training information in text categorisation.</title>
<date>1997</date>
<booktitle>In Procedings of RANLP-97, 2nd International Conference on RecentAdvances in Natural Language Processing.</booktitle>
<contexts>
<context position="15395" citStr="Rodriguez and Hidalgo, 1997" startWordPosition="2444" endWordPosition="2447">K (Cristianini et al., 2002). The computational complexity of performing an eigenvalue decomposition on the kernel matrix is a major drawback of latent semantic indexing. 2.3 Text Representation Enrichment Strategies by Term Expansion In order to eliminate the bottleneck of the traditional BOW representation, previous approaches in term expansion enriched this convention by external lexical resources such as WordNet. As a first step, these methods generate new features for each document in the dataset. These new features can be synonyms or homonyms of document terms as in (Hotho et al., 2003; Rodriguez and Hidalgo, 1997), or expanded features for terms, sentences and documents as in (Gabrilovich and Markovitch, 2005), or term context information for word sense disambiguation such as topic signatures (Agirre and De Lacalle, 2003; Agirre et al., 2004). Then, the generated new features replace the old ones or are appended to the document representation, and construct a new vector representation ai for each text document. The similarity measure of document pairs is defined as: 2.4 Our Framework The basic assumption of our framework is that terms can be arranged in an order such that consecutive terms are semantic</context>
</contexts>
<marker>Rodriguez, Hidalgo, 1997</marker>
<rawString>M.D.E.B. Rodriguez and J.M.G. Hidalgo. 1997. Using WordNet to complement training information in text categorisation. In Procedings of RANLP-97, 2nd International Conference on RecentAdvances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<contexts>
<context position="9601" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1499" endWordPosition="1502">technique for creating the function Φ from training data. The training data consist of pairs of input documents, and desired outputs (i.e., classes). Support vector machines have been found the most effective by several authors (Joachims, 1998). The proposed semantic text classification method is grounded in the kernel methods underlying support vector machines. A support vector machine is a kind of supervised learning algorithm. In its simplest, linear form, a support vector machine is a hyperplane that separates a set of positive examples from a set of negative examples with maximum margin (Shawe-Taylor and Cristianini, 2004). The strength of kernel methods is that they allow a mapping φ(.) of x to a higher dimensional space. In the dual formulation of the mathematical programming problem, only the kernel matrix K(xi,xj) = φ(xi)&apos;φ(xi) is needed in the calculations. 2.1 Traditional Text Similarity Measure Intuitively, if a text fragment of two documents address similar topics, it is highly possible that they share lots of substantive terms. After having removed the stopwords and stemmed the rest, the stemmed terms construct a vector representation for each text document. Let aj be a document vector in the vector sp</context>
<context position="10961" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1722" endWordPosition="1725">ek is a basis vector of the M-dimensional Euclidean space. This representation is also referred to as the bag-of-words (BOW) model. Given this representation, semantic relatedness of a pair of text fragments is computed as the cosine aiaj S(ai, aj) = |ai||a|j 2.2 Linear Semantic Kernels One enrichment strategy is to use a semantic smoothing kernel while calculating the similarity between two documents. Any linear kernel for texts is characterized by K(ai, aj) = a&apos;iS&apos;Saj, where S is an appropriately shaped matrix commonly referred to as semantic smoothing matrix (Siolas and d’Alch´e Buc, 2000; Shawe-Taylor and Cristianini, 2004; Basili et al., 2005; Mavroeidis et al., 2005; Bloehdorn et al., 2006). The presence of S changes the orthogonality of the vector space model, as this mapping should introduce term dependence. A recent attempt tried to manually construct S with the help of a lexical resource (Siolas and d’Alch´e Buc, 2000). The entries in the symmetric matrix S express the semantic similarity between the terms i and j. Entries in this matrix are inversely proportional to the length of the WordNet hierarchy path linking the two terms. The performance, measured over the 20NewsGroups corpus, showed an improvemen</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shi</author>
<author>J R Wen</author>
<author>Q Yu</author>
<author>R Song</author>
<author>W Y Ma</author>
</authors>
<title>Gravitation-based model for information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR-05, 28th ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>488--495</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3644" citStr="Shi et al., 2005" startWordPosition="549" endWordPosition="552">f points - called local senses and constituting lexical units -, in a one-dimensional semantic continuum (component a). Apart from differentiating between the conceptual content of the same word in terms of its senses in word pairs, i.e. their semantic relatedness, it also compresses the result in spectral form. The scalar values of this spectrum have the double potential of being a condensed measure for semantic weighting, and, tentatively, they can play the role of mass in experiments where gravity is called in as a metaphor for text categorization and information retrieval (Paijmans, 1997; Shi et al., 2005; Wittek et al., 2009). 183 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183–191, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics This paper addresses text categorization by means of non-periodical functions only. In support of Cruse’s point, recently it has been demonstrated by measurements that sense classification errors made by their maximum entropy based word sense disambiguation system were partly remedied once instead of a fine-grained view, a more coarse-grained view of senses was adopted (Palmer et al</context>
</contexts>
<marker>Shi, Wen, Yu, Song, Ma, 2005</marker>
<rawString>S. Shi, J.R. Wen, Q. Yu, R. Song, and W.Y. Ma. 2005. Gravitation-based model for information retrieval. In Proceedings of SIGIR-05, 28th ACM International Conference on Research and Development in Information Retrieval, pages 488–495. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Siolas</author>
<author>F d’Alch´e Buc</author>
</authors>
<title>Support vector machines based on a semantic kernel for text categorization.</title>
<date>2000</date>
<booktitle>In Proceedings of IJCNN-00, IEEE International Joint Conference on Neural Networks.</booktitle>
<marker>Siolas, Buc, 2000</marker>
<rawString>G. Siolas and F. d’Alch´e Buc. 2000. Support vector machines based on a semantic kernel for text categorization. In Proceedings of IJCNN-00, IEEE International Joint Conference on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>The Geometry of Information Retrieval.</title>
<date>2004</date>
<marker>van Rijsbergen, 2004</marker>
<rawString>C. J. van Rijsbergen. 2004. The Geometry of Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wittek</author>
<author>S Dar´anyi</author>
</authors>
<title>Representing word semantics for IR by continuous functions.</title>
<date>2007</date>
<booktitle>Proceedings of ICTIR-07, 1st International Conference of the Theory of Information Retrieval,</booktitle>
<pages>149--155</pages>
<editor>In S. Dominich and F. Kiss, editors,</editor>
<marker>Wittek, Dar´anyi, 2007</marker>
<rawString>P. Wittek and S. Dar´anyi. 2007. Representing word semantics for IR by continuous functions. In S. Dominich and F. Kiss, editors, Proceedings of ICTIR-07, 1st International Conference of the Theory of Information Retrieval, pages 149–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wittek</author>
<author>C L Tan</author>
<author>S Dar´anyi</author>
</authors>
<title>An ordering of terms based on semantic relatedness.</title>
<date>2009</date>
<booktitle>Proceedings of IWCS-09, 8th International Conference on Computational Semantics.</booktitle>
<editor>In H. Bunt, editor,</editor>
<marker>Wittek, Tan, Dar´anyi, 2009</marker>
<rawString>P. Wittek, C.L. Tan, and S. Dar´anyi. 2009. An ordering of terms based on semantic relatedness. In H. Bunt, editor, Proceedings of IWCS-09, 8th International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>W Ziarko</author>
<author>P C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of SIGIR-85, 8th ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="12470" citStr="Wong et al., 1985" startWordPosition="1970" endWordPosition="1973">using on special subcategories of Reuters-21578 and on the TREC Question Answering Dataset, they showed consistent improvement over the baseline. As (Mavroeidis et al., 2005) pointed out, polysemy will remain a problem in semantic smoothing kernels. A more complex way of calculating the semantic similarity as the matrix entries was also proposed (Basili et al., 2005). For a more general discussion on semantic similarity see Section 2.4.1. An early attempt to overcome the untenable orthogonality assumption of the vector space model was proposed under the name of generalized vector space model (Wong et al., 1985). The article which proposed the model did not provide empirical results, and since then the model has been regarded of large theoretical importance with less impact on actual applications. The model takes a distri. (1) 185 butional approach, focusing on term co-occurrences. The underlying assumption is that term correlations are captured by the co-occurrence information. That is, two terms are semantically related if they cooccur often in the same documents. By eliminating orthogonality, documents can be seen as similar even if they do not share any terms. The term cooccurrence matrix is AA&apos;,</context>
</contexts>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of SIGIR-85, 8th ACM International Conference on Research and Development in Information Retrieval, pages 18–25.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>