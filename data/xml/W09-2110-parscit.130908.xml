<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044599">
<title confidence="0.992186">
Using First and Second Language Models to Correct Preposition Errors
in Second Language Authoring
</title>
<author confidence="0.996374">
Matthieu Hermet Alain Désilets
</author>
<affiliation confidence="0.991191333333333">
School of Information Technology and Institute for Information Technology
Engineering
University of Ottawa National Research Council of Canada
</affiliation>
<address confidence="0.5479795">
800, King Edward, Ottawa, Bldg M-50, Montreal Road, Ottawa, K1A 0R6,
Canada Canada
</address>
<email confidence="0.993833">
mhermet@site.uottawa.ca alain.desilets@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.995562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870545454545">
In this paper, we investigate a novel approach
to correcting grammatical and lexical errors
in texts written by second language authors.
Contrary to previous approaches which tend
to use unilingual models of the user&apos;s second
language (L2), this new approach uses a sim-
ple roundtrip Machine Translation method
which leverages information about both the
author’s first (L1) and second languages. We
compare the repair rate of this roundtrip
translation approach to that of an existing ap-
proach based on a unilingual L2 model with
shallow syntactic pruning, on a series of
preposition choice errors. We find no statisti-
cally significant difference between the two
approaches, but find that a hybrid combina-
tion of both does perform significantly better
than either one in isolation. Finally, we illus-
trate how the translation approach has the po-
tential of repairing very complex errors
which would be hard to treat without leverag-
ing knowledge of the author&apos;s L1.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999914060606061">
In this paper, we investigate a novel approach to
correcting grammatical and lexical errors in texts
written by second language learners or authors.
Contrary to previous approaches which tend to use
unilingual models of the user&apos;s second language
(L2), this new approach uses a translation model
based on both the user&apos;s first (L1) and second lan-
guages. It has the advantage of being able to model
linguistic interference phenomena, that is, errors
which are produced through literal translation from
the author&apos;s first language. Although we apply this
method in the context of French-as-a-Second-Lan-
guage, its principles are largely independent of lan-
guage, and could also be extended to other classes
of errors. Note that this is preliminary work which,
in a first step, focuses on error correction, and ig-
nores for now the preliminary step of error detec-
tion which is left for future research.
This work is of interest to applications in Comput-
er-Assisted-Language-Learning (CALL) and Intel-
ligent Tutoring Systems (ITS), where tutoring ma-
terial often consists of drills such as fill-in-the-
blanks or multiple-choice-questions. These require
very little use of a learner&apos;s language production
capacities, and in order to support richer free-text
assessment capabilities, ITS systems thus need to
use error detection and correction functionalities
(Heift and Schulze, 2007).
Editing Aids (EA) are tools which assist a user in
producing written compositions. They typically use
rules for grammar checking as well as lexical
heuristics to suggest stylistic tips, synonyms or fal-
lacious collocations. Advanced examples of such
</bodyText>
<page confidence="0.993269">
64
</page>
<note confidence="0.9922145">
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 64–72,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999052592592593">
tools include Antidote1 for French and StyleWriter2
for English. Text Editors like MS Word and Word
Perfect also include grammar checkers, but their
style checking capabilities tend to be limited. All
these tools can provide useful assistance to editing
style, but they were not designed to assist with
many errors found typically in highly non-idiomat-
ic sentences produced by L2 authors.
Recent work in the field of error correction, espe-
cially as applied to English in the context of En-
glish as a Second Language (ESL), show an in-
creasing use of corpora and language models.
These have the advantage of offering a model of
correctness based on common usage, independent-
ly of any meta-information on correctness. Corpus-
based approaches are also able to correct higher
level lexical-syntactic errors, such as the choice of
preposition which is often semantically governed
by other parts of the sentence.
The reminder of this paper is organized as follows.
In section 2, we give a detailed account of the
problem of preposition errors in a Second Lan-
guage Learning (SLL) context. Related work is re-
viewed in section 3 and the algorithmic framework
is presented in section 4. An evaluation is dis-
cussed in section 5, and conclusions and directions
for future research are presented in section 6.
</bodyText>
<sectionHeader confidence="0.972675" genericHeader="introduction">
2 The Preposition Problem
</sectionHeader>
<bodyText confidence="0.9907525">
Prepositions constitute 14% of all tokens produced
in most languages (Fort &amp; Guillaume 2007). They
are reported as yielding among the highest error
class rates across various languages (Izumi, 2004,
for Japanese, Granger et al., 2001, for French). In
their analysis of a small corpus of advanced-inter-
mediate French as a Second Language (FSL) learn-
ers, Hermet et al. (2008) found that preposition
choice accounted for 17.2 % of all errors. Preposi-
tions can be seen as a special class of cognates, in
the sense that the same L1 preposition used in dif-
ferent L1 sentences, could translate to several dif-
ferent L2 prepositions.
Automatic error detection/correction methods often
process prepositions and determiners in the same
way because they both fall in the class of function-
</bodyText>
<footnote confidence="0.9966105">
1 www.druide.com
2 www.stylewriter-usa.com
</footnote>
<bodyText confidence="0.999924291666667">
words. However, one can make the argument that
preposition errors deserve a different and deeper
kind of treatment, because they tend to be more se-
mantically motivated (event hough some preposi-
tions governed by verbs draw a purely functional
relation). In contrast, determiners are not semanti-
cally motivated and only vary on the register of
quantity (or genre in some languages).
For example, there are 37 determiners in French,
most of which can be used interchangeably without
significantly affecting the syntax of a sentence, and
often, not even its meaning (&amp;quot;I&apos;ll have one
coffee&amp;quot;/&amp;quot;I&apos;ll have a coffee&amp;quot;/&amp;quot;I&apos;ll have some
coffee&amp;quot;/&amp;quot;I&apos;ll have my coffee&amp;quot;/&amp;quot;I&apos;ll have coffee&amp;quot; are
all rather alike). Comparatively, there are 85 sim-
ple prepositions and 222 compounds ones and they
cannot be used interchangeably without signifi-
cantly modifying the sense of an utterance, except
for cases of synonymy.
In this paper, we focus our attention on preposition
correction only, as it seems to be a more complex
problem than determiners. While in principle the
methods described here could handle determiner
errors, we feel that our framework, which involves
parsing in combination with a very large language
model and Machine Translation, constitutes heav-
ier machinery than is warranted for that simpler
problem.
There are two major causes of preposition errors in
a SLL context. The first kind is caused by lexical
confusion within the second language itself. For
example, a L2 author writing in English may erro-
neously use a location preposition like &amp;quot;at&amp;quot; where
another location preposition like &amp;quot;in&amp;quot; would have
been more appropriate. The second kind involves
linguistic interference between prepositions in L1
and prepositions in L2 (Granger et al., 2001). For
example, a Second Language Learner who wants
to render the following two English sentences in
French &amp;quot;I go to Montreal&amp;quot; and &amp;quot;I go to Argentina&amp;quot;,
might use the same French preposition &amp;quot;d&amp;quot; for &amp;quot;to&amp;quot;,
when in fact, French usage dictates that you write
&amp;quot;d Montréal”, and &amp;quot;en Argentine”. Note that the
situation varies greatly from language to language.
The same two English sentences rendered in Italian
and German would in fact employ a same preposi-
tion, whereas in Spanish, different prepositions
would also be required as in French.
</bodyText>
<page confidence="0.999272">
65
</page>
<bodyText confidence="0.999971045454545">
Studies have found that the majority of errors made
by L2 authors (especially intermediate to advanced
ones) are caused by such linguistic interference
(Wang and Garigliano, 1992, Cowan, 1983, p 109).
Note that this kind of linguistic interference can of-
ten lead to much more severe and hard to repair er-
rors, as illustrated by the following example, taken
from an actual SLL corpus. Say a native English
author wants to render &amp;quot;Police arrived at the scene
of the crime&amp;quot; into French (her L2). Because she is
not fluent in French, she translates the last part of
the sentence to &amp;quot;à la scène de la crime&amp;quot;. This liter-
al translation turns out to be highly unidiomatic in
French, and should instead be written as &amp;quot;sur les
lieux du crime&amp;quot; (which in English, would translate
literally to &amp;quot;on the location of the crime&amp;quot;).
One might suspect that preposition errors of the
first type would be solvable using unilingual L2
language models, but that the second type might
benefit from a language model which also takes L1
into account. This is the main question investigated
in this paper.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999977450704225">
Historically, grammatical error correction has been
done through parsing-based techniques such as
syntactic constraint-relaxation (L&apos;haire &amp; Vande-
venter-Feltin, 2003), or mal-rules modeling
(Schneider and McCoy, 1998). But generating the
rule-bases needed by these types of approaches in-
volves a lot of manual work, and may still in the
end be too imprecise to convey information on the
nature and solution of an error. Recently, more ef-
fort has been put in methods that rely on automati-
cally built language models. Typically, this kind of
work will focus either on a restricted class of errors
or on specific domains. Seneff and Lee (2006) pro-
pose a two-phased generation-based framework
where a n-gram model re-ranked by a stochastic
context-free-grammar model is used to correct sen-
tence-level errors in the language domain of flight
reservation. Brockett et al. (2006) used a Brown
noise channel translation model to record patterns
of determiner error correction on a small set of
mass-nouns, and reducing the error spectrum in
both class and semantic domain, but adding detec-
tion capabilities. Note that although they use a
translation model, it processes only text that is in
one language. More specifically, the system
learned to &amp;quot;translate&amp;quot; from poorly written English
into correctly written English.
Chodorow et al. (2007) employed a maximum en-
tropy model to estimate the probability of 34
prepositions based on 25 local context features
ranging from words to NP/VP chunks. They use
lemmatization as a means of generalization and
trained their model over 7 million prepositional
contexts, achieving results of 84% precision and
19% recall in preposition error detection in the best
of the system&apos;s configurations. Gamon et al. (2008)
worked on a similar approach using only tagged
trigram left and right contexts: a model of preposi-
tions uses serves to identify preposition errors and
the Web provides examples of correct form. They
evaluate their framework on the task of preposition
identification and report results ranging from 74 to
45% precision on a set of 13 prepositions.
Yi et al. (2008) use the Web as corpus and send
segments of sentences of varying length as bag-of-
constituents queries to retrieve occurrence con-
texts. The number of the queried segments is a PoS
condition of &amp;quot;check-points&amp;quot; sensitive to typical er-
rors made by L2 authors. The contexts retrieved
are in turn analyzed for correspondence with the
original input. The detection and correction meth-
ods differ according to the class of the error. Deter-
miner errors call for distinct detection and correc-
tion procedures while collocation errors use the
same procedure for both. Determiner errors are dis-
covered by thresholds ratios on search hits statis-
tics, taking into account probable ambiguities,
since multiple forms of determiners can be valid in
a single context. Collocation errors on the other
hand, are assessed only by a threshold on absolute
counts, that is, a form different from the input au-
tomatically signals an error and provides its correc-
tion. This suggests that detection and correction
procedures coincide when the error ceases to bear
on a function word.
Similarly, Hermet et al. (2008) use a Web as cor-
pus based approach to address the correction of
preposition errors in a French-as-a-Second-Lan-
guage (FSL) context. Candidate prepositions are
substituted for erroneous ones following a taxono-
my of semantic classes, which produces a set of al-
</bodyText>
<page confidence="0.946499">
66
</page>
<bodyText confidence="0.999981457142857">
ternate sentences for each error. The main interest
of their study is the use of a syntax-based sentence
generalization method to maximize the likelihood
that at least one of the alternatives will have at
least one hits on the Web. They achieve accuracy
of 69% in error repair (no error detection), on a
small set of clauses written by FSL Learners.
Very little work has been done to actually exploit
knowledge of a L2 author&apos;s first language, in cor-
recting errors. Several authors (Wang and
Garigliano, 1992, Anderson, 1995, La Torre,
1999, Somers, 2001) have suggested that students
may learn by analyzing erroneous sentences pro-
duced by a MT system, and reflecting on the prob-
able cause of errors, especially in terms of interfer-
ence between the two languages. In this context
however, the MT system is used only to generate
exercises, as opposed to helping the student find
and correct errors in texts that he produces.
Although it is not based on an MT model, Wang
and Garigliano propose an algorithm which uses a
hand-crafted, domain-specific, mixed L1 and L2
grammar, in order to identify L1 interference errors
in L2 sentences. L2 sentences are parsed with this
mixed grammar, giving priority to L2 rules, and
only employing L1 rules as a last resort. Parts of
the sentence which required the user of L1 rules
are labeled as errors caused by L1 interference.
The paper does not present an actual evaluation of
the algorithm.
Finally, a patent by Dymetman and Isabelle (2005)
describes several ways in which MT technology
could be used to correct L2 errors, but to our
knowledge, none of them has been implemented
and evaluated yet.
</bodyText>
<sectionHeader confidence="0.997036" genericHeader="method">
4 Algorithmic Framework
</sectionHeader>
<bodyText confidence="0.999890428571429">
As discussed in section 2, L2 authoring errors can
be caused by confusions within the L2 itself, or by
linguistic interference between L1 and L2. In order
to account for this duality, we investigate the use
of two correction strategies, one which is based on
unilingual models of L2, and one which is based
on translation models between L1 and L2.
</bodyText>
<figure confidence="0.707811125">
Input Sentence
Il y a une grande fenêtre qui permet au soleil &lt;à&gt;
entrer
(there is a large window which lets the sun come in)
Syntactic Pruning and Lemmatization
permettre &lt;à&gt; entrer
(let come in)
Generation of alternate prepositions
</figure>
<figureCaption confidence="0.310263">
semantically related: dans, en, chez, sur, sous,
au, dans, après, avant, en, vers
</figureCaption>
<bodyText confidence="0.91977775">
most common: de, avec, par, pour
Query and sort alternative phrases
permettre d&apos;entrer: 119 000 hits
permettre avant entrer: 12 hits
permettre à entrer: 4 hits
permettre en entrer: 2 hits
...
→ preposition &lt;d&apos;&gt; is returned as correction
</bodyText>
<figureCaption confidence="0.995295">
Figure 1. Typical processing carried out by the Unilingual
approach.
</figureCaption>
<bodyText confidence="0.99993988">
The first approach, called the Unilingual strategy,
is illustrated by the example in Figure 1. It uses a
web search engine (Yahoo) as a simple, unilingual
language model, where the probability of a L2
phrase is estimated simply by counting its number
of occurrences in Web pages of that language. A
severe limitation of this kind of model is that it can
only estimate the probability of phrases that appear
at least once on the Web. In contrast, an N-gram
model (for example) is able to estimate the proba-
bility of phrases that it has never seen in the train-
ing corpus. In order to deal with this limitation,
syntactic pruning is therefore applied to the phrase
before it is sent to the search engine, in order to
eliminate parts which are not core to the context of
use of the preposition, thus increasing the odds
that the pruned sentence will have at least one oc-
currence on the Web.
This pruning and generalization is done by carry-
ing out syntactic analysis with the Xerox Incre-
mental Parser for the syntactic analysis (Ref XIP).
XIP is an error robust, symbolic, dependency pars-
er, which outputs syntactic information at the con-
stituency and dependency levels. Its ability to pro-
duce syntactic analyses in the presence of errors is
</bodyText>
<page confidence="0.997643">
67
</page>
<table confidence="0.999486083333333">
Category Prepositions
Localization in front, behind, after, before, above,
in, at, on, below, above...
Temporal at, in, after, before, for, during,
since...
Cause for, because of
Goal for, at
Manner in, by, with, according to...
Material in, of
Possession/Rela- to, at, with respect to...
tion
Most common to, at, on, with, by, for
</table>
<tableCaption confidence="0.994591">
Table 1. Categories of prepositions – the list is given in En-
glish, and non exhaustive for space reasons.
</tableCaption>
<bodyText confidence="0.99997045">
particularly interesting in the context of second
language authoring where the sentences produced
by the authors can be quite far from grammatical
correctness. The input sentence is fed to the parser
as two segments split at error point (in this case, at
the location of the erroneous preposition). This en-
sures that the parses are correct and not affected at
dependency level by the presence of error. The
syntactic analyses are needed to perform syntactic
pruning, which is a crucial step in our framework,
following Hermet et. al (2008). Pruning is per-
formed by way of chunking heuristics, which are
controlled by grammatical features, provided by
XIP&apos;s morphological analysis (PoS tagger). The
heuristics are designed to suppress syntactically
extraneous material in the sentence, such as ad-
verbs, some adjectives and some NPs. Adverbs are
removed in all cases, while adjectives are only re-
moved when they are not in a position to govern a
Prepositional Phrase. NPs are suppressed in con-
trolled cases, based on the verb sub-categorization
frame, when a PP can be attached directly to the
preceding verb. In case of ambiguity in the attach-
ment of the PP, two versions of the pruned sen-
tence can be produced reflecting two different PP
attachments. Lemmatization of verbs is also car-
ried out in the pruning step.
After pruning, the right and left sides of the sen-
tences are re-assembled with alternate prepositions.
The replacement of prepositions is controlled by
way of semantics. Since prepositions are richer in
sense than strict function words, they can therefore
be categorized according to semantics. Saint-Dizier
(2007) proposes such a taxonomy, and in our
framework, prepositions have been grouped in 7
non-exclusive categories. Table 1 provides details
of this categorization. The input preposition is
mapped to all the sets it belongs to, and corre-
sponding alternates are retrieved as correction can-
didates. The 6 most frequent French preposition
are also added automatically to the candidates list.
The resulting sentences are then sent to the Yahoo
Search Engine and hits are counted. The number of
hits returned by each of the queries is used as deci-
sion criteria, and the preposition contained in the
query with the most hits is selected as the correc-
tion candidate.
While the above Unilingual strategy might work
for simple cases of L1 interference, one would not
expect it to work as well in more complex cases
where both the preposition and its governing parts
have been translated too literally. For example, in
the case of the example from section 2, while the
Unilingual strategy might be able to effect correc-
tion “sur la scène du crime” which is marginally
better than the original “h la scène du crime” (12K
hits versus 1K), it cannot address the root of the
problem, that is, the unidiomatic expression
“scène du crime” which should instead be ren-
dered as “lieux du crime” (38K hits). In this partic-
ular case, it is not really an issue because it so hap-
pens that “sur” is the correct preposition to use for
both “lieux du crime” and “scène du crime”, but
in our experience, that is not always the case. Note
also that the Unilingual approach can only deal
with preposition errors (although it would be easy
enough to extend it to other kinds of function
words), and cannot deal with more semantically
deep L1 interference.
To address these issues, we experimented with a
second strategy which we will refer to as the
Roundtrip Machine Translation approach (or
Roundtrip MT for short). Note that our approach is
different from that of Brockett et al. (2006), as we
do make use of a truly multi-lingual translation
model. In contrast, Brockett’s translation model
was trained on texts that were written in the same
language, with the sources being ill-written text in
the same language as the properly-formed target
texts. One drawback of our approach however is
</bodyText>
<page confidence="0.998684">
68
</page>
<bodyText confidence="0.999938602040816">
that it may require different translation models for
speakers with different first languages.
There are many ways in which error-correction
could be carried out using MT techniques. Several
of these have been described in a patent by Dymet-
man and Isabelle (2005), but to our knowledge,
none of them have yet been implemented and eval-
uated. In this paper, we use the simplest possible
implementation of this concept, namely, we carry
out a single round-trip translation. Given a poten-
tially erroneous L2 sentence written by a second
language author, we translate it to the author&apos;s L1
language, and then back to L2. Even with this sim-
ple approach, we often find that errors which were
present in the original L2 sentence have been re-
paired in the roundtrip version. This may sound
surprising, since one would expect the roundtrip
sentence to be worse than the original, on account
of the &amp;quot;Chinese Whisper&amp;quot; effect. Our current theo-
ry for why this is not the case in practice goes as
follows. In the course of translating the original L2
sentence to L1, when the MT system encounters a
part that is ill-formed, it will tend to use single
word entries from its phrase table, because longer
phrases will not have been represented in the well-
formed L2 training data. In other words, the system
tends to generate a word for word translation of ill-
formed parts, which mirrors exactly what L2 au-
thors do when they write poorly formed L2 sen-
tences by translating too literally from their L1
thought. As a result, the L1 sentence produced by
the MT system is often well formed for that lan-
guage. Subsequently, when the MT system tries to
translate that well-formed L1 sentence back to L2,
it is therefore able to use longer entries from its
phrase table, and hence produce a better L2 trans-
lation of that part than what the author originally
produced.
We use Google Translate as a translation engine
for matter of simplicity. A drawback of using such
an online service is that it is essentially a closed
box, and we therefore have little control over the
translation process, and no access to lower level
data generated by the system in the course of trans-
lation (e.g. phrase alignments between source and
target sentences). In particular, this means that we
can only generate one alternative L2 sentence, and
have no way of assessing which parts of this single
alternative have a high probability of being better
than their corresponding parts in the original L2
sentence written by the author. In other words, we
have no way of telling which changes are likely to
be false positives, and which changes are likely to
be true positives. This is the main reason why we
focus only on error repair in this preliminary work.
The roundtrip sentences generated with Google
Translate often differ significantly from the origi-
nal L2 sentence, and in more ways than just the er-
roneous preposition used by the author. For exam-
ple, the (pruned) clause &amp;quot;avoir du succès en le re-
crutement&amp;quot; (&amp;quot;to be successful in recruiting&amp;quot;) might
come back as as &amp;quot;réussir d recruter&amp;quot; (&amp;quot;to succeed
in recruiting&amp;quot;). Here, the translation is acceptable,
but the preposition used by the MT system is not
appropriate for use in the original sentence as writ-
ten by the L2 author. Conversely, a roundtrip
translation can be ill-formed, yet use a preposition
which would be correct in the original L2 sentence.
For example, &amp;quot;regarder d des films&amp;quot; (&amp;quot;look at some
movies&amp;quot;) might come back as &amp;quot;inspecter des films&amp;quot;
(&amp;quot;inspect some films&amp;quot;). Here, the original meaning
is somewhat lost, but the system correctly suggest-
ed that there should be no preposition before “des
films”.
Hence, in the context of the Roundtrip MT ap-
proach, we need two ways of measuring appropri-
ateness of the suggested corrections for given
clauses. The first approach, which we call the
Clause criteria, looks at whether or not the whole
clause has been restored to a correct idiomatic
form (including correct use of preposition) which
also preserves the meaning intended by the author
of the original sentence. Hence, according to this
approach, an MT alternative may be deemed cor-
rect, even if it chooses a preposition which would
have been incorrect if substituted in the original L2
sentence as is. In the second approach, called the
Prep criteria, we only look at whether the preposi-
tion used by the MT system in the roundtrip trans-
lation, corresponds to the correct preposition to be
used in the original L2 clause. Hence, with this ap-
proach, an MT alternative may be deemed correct,
even if the preposition chosen by the MT system is
actually inappropriate in the context of the generat-
ed roundtrip translation, or, even worse, if the
roundtrip modified the clause to a point where it
actually means something different than what the
author actually intended.
</bodyText>
<page confidence="0.998433">
69
</page>
<bodyText confidence="0.999954605263158">
Of course, in the case of the Prep evaluation crite-
ria, having the MT system return a sentence which
employs the proper preposition to use in the con-
text of the original L2 sentence is not the end of
the process. In an error correction context, one
must also isolate the correct preposition and insert
it in the appropriate place in the original L2 sen-
tence. This part of the processing chain is not cur-
rently implemented, but would be easy to do, if we
used an MT system that provided us with the align-
ment information between the source sentence and
the target sentence generated. The accuracy figures
which we present in this paper assume that this
mapping has been implemented and that this par-
ticular part of the process can be done with 100%
accuracy (a claim which, while plausible, still
needs to be demonstrated in future work).
We also investigate a third strategy called Hybrid,
which uses the Roundtrip MT approach as a back-
up for cases where the Unilingual approach is un-
able to distinguish between different choices of
preposition. The latter typically occurs when the
system is not able to sufficiently prune and gener-
alize the phrase, resulting in a situation where all
pruned variants yield zero hits on the Web, no mat-
ter what preposition is used. One could of course
also use the Unilingual approach as a backup for
the Roundtrip MT approach, but this would be
harder to implement since the MT system always
returns an answer, and our use of the online
Google Translate system precludes any attempt to
estimate the confidence level of that answer.
In conclusion to this section, we use three preposi-
tion correction strategies: Unilingual, Roundtrip
MT and Hybrid, and in the case of the Roundtrip
MT approach, appropriateness of the corrections
can be evaluated using two criteria: Prep and
Clause.
</bodyText>
<sectionHeader confidence="0.995838" genericHeader="evaluation">
5 Evaluation and Results
</sectionHeader>
<subsectionHeader confidence="0.99952">
5.1 Corpus and Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9990274">
For evaluation, we extracted clauses containing
preposition errors from a small corpus of texts
written by advanced-intermediate French as a Sec-
ond Language (FSL) student in the course of one
semester. The corpus contained about 50, 000
</bodyText>
<table confidence="0.999754">
Algorithm Repair rate (%)
Unilingual 68.7
Roundtrip MT (Clause) 44.8
Roundtrip MT (Prep) 66.4
Hybrid (Prep) 82.1
</table>
<tableCaption confidence="0.999533">
Table 2. Results for 3 algorithms on 133 sentences.
</tableCaption>
<bodyText confidence="0.99991947826087">
words and 133 unique preposition errors. While
relatively small, we believe this set to be suffi-
ciently rich to test the approach. Most clauses also
presented other errors, including orthographic,
tense, agreement, morphologic and auxiliary er-
rors, of which only the last two affect parsing. The
clauses were fed as is to the correction algorithms,
without first fixing the other types of errors. But to
our surprise, XIP&apos;s robust parsing has proven resis-
tant in that it produced enough information to en-
able correct pruning based on chunking informa-
tion, and we report no pruning errors. Chodorow et
al. (2008) stress the importance of agreement be-
tween annotators when retrieving or correcting
preposition errors. In our case, our policy has been
to only retain errors reported by both authors of
this paper, and correction of these errors has raised
little matter of dispute.
We evaluated the various algorithms in terms of re-
pair rate, that is, the percentage of times that the al-
gorithm proposed an appropriate fix (the absence
of a suggestion was taken to be an inappropriate
fix). These figures are reported in Table 2.
</bodyText>
<subsectionHeader confidence="0.94761">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.99995">
ANOVA of the data summarized in Table 2 reveals
a statistically significant (p &lt; 0.001) effect of the
algorithm on repair rate. Although Roundtrip MT
performed slightly worse than Unilingual (66.4%
versus 68.7%), this difference was not found to be
statistically significant. On one hand, we found
that round-trip translation sometimes result in
spectacular restorations of long and clumsy phrases
caused by complex linguistic interference. Howev-
er, too often the Chinese whispers effect destroyed
the sense of the original phrase, resulting in inap-
propriate suggestions. This is evidenced by the fact
that repair rate of the Roundtrip MT approach was
significantly lower (p &lt; 0.001) when using the
</bodyText>
<page confidence="0.992796">
70
</page>
<bodyText confidence="0.999980717948718">
Clause criteria (44.8%) than when using the Prep
criteria (66.4%). It seems that, in the case of prepo-
sition correction, roundtrip MT is best used as a
way to to generate an L2 alternative from which to
mine the correct preposition. Indeed, flawed as
they are, these distorted roundtrip segments cor-
rected prepositions errors in 66.4% of the cases.
However, for a full picture, the approach should be
tried on more data, and on other classes of errors.
Particularly, we currently lack sufficient data to
test the hypothesis that the approach could address
the correction of more complex literal translations
by SL Learners.
In the Unilingual approach, the Yahoo Web search
engine proved to be an insufficient language model
for 31 cases out of 133, meaning that even the
pruned and generalized phrases got zero hits, no
matter what alternative preposition was used. In
those cases, the Hybrid approach would then at-
tempt correction using MT Roundtrip approach.
This turned out to work quite well, since it resulted
in an overall accuracy of 82.1%. ANOVA on the
data for Hybrid and the two pure approaches re-
veals a significant effect (p &lt; 0.001) of the algo-
rithm factor. Individual t-tests between the Hybrid
approach and each of the two pure approaches also
reveal statistically significant differences (p &lt;
0.001). The improvements provided by the hybrid
approach are fairly substantial, and represent rela-
tive gains of 19.5% over the pure Unilingual ap-
proach, and 23.6% over the pure Roundtrip MT ap-
proach. The success of this combined approach
might be attributable to the fact that the two ap-
proaches follow different paradigms. Roundtrip
MT uses a model of controlled incorrectness (er-
rors of anglicism) and Unilingual a model of cor-
rectness (occurrences of correct forms). In this re-
spect, the relatively low agreement between the
two approaches (65.4%) is not surprising.
</bodyText>
<sectionHeader confidence="0.997623" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999988392857143">
In this paper, we have demonstrated for the first
time that a bilingual Machine Translation approach
can be used to good effect to correct errors in texts
written by Second Language Authors or Learners.
In the case of preposition error correction we found
that, while the MT approach on its own did not
perform significantly better than a unilingual ap-
proach, a hybrid combination of both performed
much better than the unilingual approach alone.
More work needs to be carried out in order to fully
evaluate the potential of the MT approach. In par-
ticular, we plan to experiment with this kind of ap-
proach to deal with more complex cases of L1 in-
terference which result in severely damaged L2
sentences.
In this paper, we compared the bilingual MT ap-
proach to a unilingual baseline which used a rela-
tively simple Web as a corpus algorithm, whose
accuracy is comparable to that reported in the liter-
ature for a similar preposition correction algorithm
(Yi et al, 2008). Notwithstanding the fact that such
simple Web as a corpus approaches have often
been shown to be competitive with (if not better
than) more complex algorithms which cannot
leverage the full extent of the web (Halevy et al.,
2009), it would be interesting to compare the bilin-
gual MT approach to more sophisticated unilingual
algorithms for preposition correction, many of
which are referenced in section 3.
Error detection is another area for future research.
In this paper, we limited ourselves to error correc-
tion, since it could be solved through a very simple
round-trip translation, without requiring a detailed
control of the MT system, or access to lower level
information generated by the system in the course
of translation (for example, intermediate hypothe-
ses with probabilities and alignment information
between source and target sentences). In contrast,
we believe that error detection with an MT ap-
proach will require this kind of finer control and
access to the guts of the MT system. We plan to in-
vestigate this using the PORTAGE MT system
(Ueffing et al., 2007). Essentially, we plan to use
the MT system&apos;s internal information to assign
confidence scores to various segments of the
roundtrip translation, and label them as corrections
if this confidence is above a certain threshold. In
doing this, we will be following in the footsteps of
Yi et al. (2008) who use the same algorithm for er-
ror detection and error correction. The process of
detecting an error is simply one of determining
whether the system&apos;s topmost alternative is differ-
ent from what appeared in the original sentence,
and whether the system&apos;s confidence in that alter-
native is sufficiently high to take the risk of pre-
senting it to the user as a suggested correction.
</bodyText>
<page confidence="0.998127">
71
</page>
<sectionHeader confidence="0.998817" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999313">
The authors are indebted to the following people
(all from NRC) for helpful advice on how to best
exploit MT for second language correction: Pierre
Isabelle, George Foster and Eric Joanis.
</bodyText>
<sectionHeader confidence="0.996353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999289625">
Anderson, D. D. 1995. Machine Translation As a Tool
in Second Language Learning. CALICO Journal, v13
n1 p68-97.
Brockett C., Dolan W. B., and Gamon M.. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proc. 21st International Conf. On Computational Lin-
guistics and the 44th annual meeting of the ACL, p.
249–256, Sydney, Australia.
Chodorow M., Tetreault J. R. and Han N.-R.. 2007.
Detection of Grammatical Errors Involving Preposi-
tions. In Proc. ACL-SIGSEM Workshop on Preposi-
tions. Prague, Czech Republic.
Cowan, J. R. 1983. Towards a Psychological Theory of
Interference in Second Language Learning. In Sec-
ond Language Learning: Contrastive Analysis, Error
Analysis, and Related Aspects, edited by B. W. Robi-
nett, J. Schachter, pp 109-119, The Univ. of Michi-
gan Press.
Dymetman M., Isabelle, P. 2007. Second language writ-
ing advisor. US Patent #20070033002 , Feb 8, 2007.
Fort K., Guillaume B. 2007. PrepLex: un lexique des
prépositions du français pour l&apos;analyse syntaxique.
TALN 2007, Toulouse, June 5-8.
Gamon M., Gao J. F., Brockett C., Klementiev A.,
Dolan W. B., and Vanderwende L. 2008. Using con-
textual speller techniques and language modeling for
ESL error correction. In Proceedings of IJCNLP
2008, Hyderabad, India, January.
Granger S., Vandeventer A. &amp; Hamel M. J. 2001.
Analyse de corpus d&apos;apprenants pour l&apos;ELAO basé
sur le TAL. TAL 42(2), 609-621.
Halevy, A., Norvig, P., Pereira, F. 2009. &amp;quot;The Unrea-
sonable Effectiveness of Data.&amp;quot;, IEEE Intelligent
Systems, March/April 2009, pp 8-12.
Heift, T. &amp; Schulze, M. 2007. Errors and Intelligence
in Computer-Assisted Language Learning: Parsers
and Pedagogues. Routledge.
Hermet, M., Désilets, A., Szpakowicz, S. 2008. Using
the Web as a Linguistic Resource to Automatically
Correct Lexico-Syntactic Errors. In Proceedings of
the LREC&apos;08. Marrakech, Morroco.
Izumi, E., K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of Japanese learn-
er English and evaluation through the experiment on
automatic detection of learners’ errors. In LREC.
La Torre, M. D. 1999. A web-based resource to imprive
translation skills. ReCALL, Vol 11, No3, pp. 41-49.
Lee J. and Seneff S. 2006. Automatic grammar correc-
tion for second-language learners. In Interspeech.
ICSLP. p. 1978-1981. Pittsburgh.
L&apos;haire S. &amp; Vandeventer Faltin A. 2003. Error diagno-
sis in the FreeText project. CALICO 20(3), 481-495,
special Issue Error Analysis and Error Correction in
Computer-Assisted Language Learning, T. Heift &amp;
M. Schulze (eds.).
Schneider, D. and McCoy, K. F. 1998. Recognizing syn-
tactic errors in the writing of second language learn-
ers. In Proceedings of COLING/ACL 98.
Saint-Dizier, P. 2007. Regroupement des Prépositions
par sens. Undated Report. IRIT. Toulouse.
http://www.irit.fr/recherches/ILPL/Site-
Equipe/publi_fichier/prepLCS.doc
Somers, Harold. 2001. Three Perspectives on MT in the
Classroom, MT SUMMIT VIII Workshop on
Teaching Machine Translation, Santiago de
Compostela, pages 25-29.
Tetreault J. and Chodorow M. 2008. The Ups and
Downs of Preposition Error Detection. COLING,
Manchester.
Ueffing, N., Simard, M., Larkin, S., Johnson, J. H.
(2007), NRC&apos;s PORTAGE system for WMT 2007,
ACL-2007 Workshop on SMT, Prague, Czech Re-
public 2007.
Wang, Y. and Garigliano, R. 1992. An Intelligent Lan-
guage Tutoring System for Handling Errors caused
by Transfer. In Proceedings of ITS-92, pp. 395-404.
Yi X., Gao J. F., Dolan W. B., 2008. A Web-based En-
glish Proofing System for English as a Second Lan-
guage Users. In Proceedings of IJCNLP 2008, Hy-
derabad, India, January.
</reference>
<page confidence="0.998725">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440983">
<title confidence="0.999049">Using First and Second Language Models to Correct Preposition Errors in Second Language Authoring</title>
<author confidence="0.998515">Matthieu Hermet Alain Désilets</author>
<affiliation confidence="0.994144">School of Information Technology and Institute for Information Engineering University of Ottawa National Research Council of Canada</affiliation>
<address confidence="0.7803745">800, King Edward, Ottawa, Bldg M-50, Montreal Road, Ottawa, K1A 0R6, Canada Canada</address>
<email confidence="0.942819">mhermet@site.uottawa.caalain.desilets@nrc-cnrc.gc.ca</email>
<abstract confidence="0.992039217391304">In this paper, we investigate a novel approach to correcting grammatical and lexical errors in texts written by second language authors. Contrary to previous approaches which tend to use unilingual models of the user&apos;s second language (L2), this new approach uses a simple roundtrip Machine Translation method which leverages information about both the author’s first (L1) and second languages. We compare the repair rate of this roundtrip translation approach to that of an existing approach based on a unilingual L2 model with shallow syntactic pruning, on a series of preposition choice errors. We find no statistically significant difference between the two approaches, but find that a hybrid combination of both does perform significantly better than either one in isolation. Finally, we illustrate how the translation approach has the potential of repairing very complex errors which would be hard to treat without leveraging knowledge of the author&apos;s L1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D D Anderson</author>
</authors>
<title>Machine Translation As a Tool in Second Language Learning.</title>
<date>1995</date>
<journal>CALICO Journal,</journal>
<volume>13</volume>
<pages>68--97</pages>
<contexts>
<context position="12709" citStr="Anderson, 1995" startWordPosition="2024" endWordPosition="2025">for erroneous ones following a taxonomy of semantic classes, which produces a set of al66 ternate sentences for each error. The main interest of their study is the use of a syntax-based sentence generalization method to maximize the likelihood that at least one of the alternatives will have at least one hits on the Web. They achieve accuracy of 69% in error repair (no error detection), on a small set of clauses written by FSL Learners. Very little work has been done to actually exploit knowledge of a L2 author&apos;s first language, in correcting errors. Several authors (Wang and Garigliano, 1992, Anderson, 1995, La Torre, 1999, Somers, 2001) have suggested that students may learn by analyzing erroneous sentences produced by a MT system, and reflecting on the probable cause of errors, especially in terms of interference between the two languages. In this context however, the MT system is used only to generate exercises, as opposed to helping the student find and correct errors in texts that he produces. Although it is not based on an MT model, Wang and Garigliano propose an algorithm which uses a hand-crafted, domain-specific, mixed L1 and L2 grammar, in order to identify L1 interference errors in L2</context>
</contexts>
<marker>Anderson, 1995</marker>
<rawString>Anderson, D. D. 1995. Machine Translation As a Tool in Second Language Learning. CALICO Journal, v13 n1 p68-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W B Dolan</author>
<author>M Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>In Proc. 21st International Conf. On Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>249--256</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="9605" citStr="Brockett et al. (2006)" startWordPosition="1517" endWordPosition="1520">eeded by these types of approaches involves a lot of manual work, and may still in the end be too imprecise to convey information on the nature and solution of an error. Recently, more effort has been put in methods that rely on automatically built language models. Typically, this kind of work will focus either on a restricted class of errors or on specific domains. Seneff and Lee (2006) propose a two-phased generation-based framework where a n-gram model re-ranked by a stochastic context-free-grammar model is used to correct sentence-level errors in the language domain of flight reservation. Brockett et al. (2006) used a Brown noise channel translation model to record patterns of determiner error correction on a small set of mass-nouns, and reducing the error spectrum in both class and semantic domain, but adding detection capabilities. Note that although they use a translation model, it processes only text that is in one language. More specifically, the system learned to &amp;quot;translate&amp;quot; from poorly written English into correctly written English. Chodorow et al. (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP</context>
<context position="20080" citStr="Brockett et al. (2006)" startWordPosition="3267" endWordPosition="3270">e because it so happens that “sur” is the correct preposition to use for both “lieux du crime” and “scène du crime”, but in our experience, that is not always the case. Note also that the Unilingual approach can only deal with preposition errors (although it would be easy enough to extend it to other kinds of function words), and cannot deal with more semantically deep L1 interference. To address these issues, we experimented with a second strategy which we will refer to as the Roundtrip Machine Translation approach (or Roundtrip MT for short). Note that our approach is different from that of Brockett et al. (2006), as we do make use of a truly multi-lingual translation model. In contrast, Brockett’s translation model was trained on texts that were written in the same language, with the sources being ill-written text in the same language as the properly-formed target texts. One drawback of our approach however is 68 that it may require different translation models for speakers with different first languages. There are many ways in which error-correction could be carried out using MT techniques. Several of these have been described in a patent by Dymetman and Isabelle (2005), but to our knowledge, none o</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Brockett C., Dolan W. B., and Gamon M.. 2006. Correcting ESL errors using phrasal SMT techniques. In Proc. 21st International Conf. On Computational Linguistics and the 44th annual meeting of the ACL, p. 249–256, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J R Tetreault</author>
<author>N-R Han</author>
</authors>
<title>Detection of Grammatical Errors Involving Prepositions.</title>
<date>2007</date>
<booktitle>In Proc. ACL-SIGSEM Workshop on Prepositions.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10065" citStr="Chodorow et al. (2007)" startWordPosition="1589" endWordPosition="1592">re-ranked by a stochastic context-free-grammar model is used to correct sentence-level errors in the language domain of flight reservation. Brockett et al. (2006) used a Brown noise channel translation model to record patterns of determiner error correction on a small set of mass-nouns, and reducing the error spectrum in both class and semantic domain, but adding detection capabilities. Note that although they use a translation model, it processes only text that is in one language. More specifically, the system learned to &amp;quot;translate&amp;quot; from poorly written English into correctly written English. Chodorow et al. (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. They use lemmatization as a means of generalization and trained their model over 7 million prepositional contexts, achieving results of 84% precision and 19% recall in preposition error detection in the best of the system&apos;s configurations. Gamon et al. (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct </context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Chodorow M., Tetreault J. R. and Han N.-R.. 2007. Detection of Grammatical Errors Involving Prepositions. In Proc. ACL-SIGSEM Workshop on Prepositions. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Cowan</author>
</authors>
<title>Towards a Psychological Theory of Interference in Second Language Learning. In Second Language Learning: Contrastive Analysis, Error Analysis, and Related Aspects,</title>
<date>1983</date>
<pages>109--119</pages>
<institution>The Univ. of Michigan Press.</institution>
<note>edited by</note>
<contexts>
<context position="7831" citStr="Cowan, 1983" startWordPosition="1223" endWordPosition="1224">real&amp;quot; and &amp;quot;I go to Argentina&amp;quot;, might use the same French preposition &amp;quot;d&amp;quot; for &amp;quot;to&amp;quot;, when in fact, French usage dictates that you write &amp;quot;d Montréal”, and &amp;quot;en Argentine”. Note that the situation varies greatly from language to language. The same two English sentences rendered in Italian and German would in fact employ a same preposition, whereas in Spanish, different prepositions would also be required as in French. 65 Studies have found that the majority of errors made by L2 authors (especially intermediate to advanced ones) are caused by such linguistic interference (Wang and Garigliano, 1992, Cowan, 1983, p 109). Note that this kind of linguistic interference can often lead to much more severe and hard to repair errors, as illustrated by the following example, taken from an actual SLL corpus. Say a native English author wants to render &amp;quot;Police arrived at the scene of the crime&amp;quot; into French (her L2). Because she is not fluent in French, she translates the last part of the sentence to &amp;quot;à la scène de la crime&amp;quot;. This literal translation turns out to be highly unidiomatic in French, and should instead be written as &amp;quot;sur les lieux du crime&amp;quot; (which in English, would translate literally to &amp;quot;on the lo</context>
</contexts>
<marker>Cowan, 1983</marker>
<rawString>Cowan, J. R. 1983. Towards a Psychological Theory of Interference in Second Language Learning. In Second Language Learning: Contrastive Analysis, Error Analysis, and Related Aspects, edited by B. W. Robinett, J. Schachter, pp 109-119, The Univ. of Michigan Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
<author>P Isabelle</author>
</authors>
<title>Second language writing advisor.</title>
<date>2007</date>
<tech>US Patent #20070033002 ,</tech>
<marker>Dymetman, Isabelle, 2007</marker>
<rawString>Dymetman M., Isabelle, P. 2007. Second language writing advisor. US Patent #20070033002 , Feb 8, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fort</author>
<author>B Guillaume</author>
</authors>
<title>PrepLex: un lexique des prépositions du français pour l&apos;analyse syntaxique. TALN</title>
<date>2007</date>
<pages>5--8</pages>
<location>Toulouse,</location>
<contexts>
<context position="4628" citStr="Fort &amp; Guillaume 2007" startWordPosition="712" endWordPosition="715">ic errors, such as the choice of preposition which is often semantically governed by other parts of the sentence. The reminder of this paper is organized as follows. In section 2, we give a detailed account of the problem of preposition errors in a Second Language Learning (SLL) context. Related work is reviewed in section 3 and the algorithmic framework is presented in section 4. An evaluation is discussed in section 5, and conclusions and directions for future research are presented in section 6. 2 The Preposition Problem Prepositions constitute 14% of all tokens produced in most languages (Fort &amp; Guillaume 2007). They are reported as yielding among the highest error class rates across various languages (Izumi, 2004, for Japanese, Granger et al., 2001, for French). In their analysis of a small corpus of advanced-intermediate French as a Second Language (FSL) learners, Hermet et al. (2008) found that preposition choice accounted for 17.2 % of all errors. Prepositions can be seen as a special class of cognates, in the sense that the same L1 preposition used in different L1 sentences, could translate to several different L2 prepositions. Automatic error detection/correction methods often process preposit</context>
</contexts>
<marker>Fort, Guillaume, 2007</marker>
<rawString>Fort K., Guillaume B. 2007. PrepLex: un lexique des prépositions du français pour l&apos;analyse syntaxique. TALN 2007, Toulouse, June 5-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J F Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W B Dolan</author>
<author>L Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP 2008,</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="10476" citStr="Gamon et al. (2008)" startWordPosition="1654" endWordPosition="1657">use a translation model, it processes only text that is in one language. More specifically, the system learned to &amp;quot;translate&amp;quot; from poorly written English into correctly written English. Chodorow et al. (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. They use lemmatization as a means of generalization and trained their model over 7 million prepositional contexts, achieving results of 84% precision and 19% recall in preposition error detection in the best of the system&apos;s configurations. Gamon et al. (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct form. They evaluate their framework on the task of preposition identification and report results ranging from 74 to 45% precision on a set of 13 prepositions. Yi et al. (2008) use the Web as corpus and send segments of sentences of varying length as bag-ofconstituents queries to retrieve occurrence contexts. The number of the queried segments is a PoS condition of &amp;quot;check-points&amp;quot; sensitive to typical errors m</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Vanderwende, 2008</marker>
<rawString>Gamon M., Gao J. F., Brockett C., Klementiev A., Dolan W. B., and Vanderwende L. 2008. Using contextual speller techniques and language modeling for ESL error correction. In Proceedings of IJCNLP 2008, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>A Vandeventer</author>
<author>M J Hamel</author>
</authors>
<title>Analyse de corpus d&apos;apprenants pour l&apos;ELAO basé sur le TAL.</title>
<date>2001</date>
<journal>TAL</journal>
<volume>42</volume>
<issue>2</issue>
<pages>609--621</pages>
<contexts>
<context position="4769" citStr="Granger et al., 2001" startWordPosition="734" endWordPosition="737"> organized as follows. In section 2, we give a detailed account of the problem of preposition errors in a Second Language Learning (SLL) context. Related work is reviewed in section 3 and the algorithmic framework is presented in section 4. An evaluation is discussed in section 5, and conclusions and directions for future research are presented in section 6. 2 The Preposition Problem Prepositions constitute 14% of all tokens produced in most languages (Fort &amp; Guillaume 2007). They are reported as yielding among the highest error class rates across various languages (Izumi, 2004, for Japanese, Granger et al., 2001, for French). In their analysis of a small corpus of advanced-intermediate French as a Second Language (FSL) learners, Hermet et al. (2008) found that preposition choice accounted for 17.2 % of all errors. Prepositions can be seen as a special class of cognates, in the sense that the same L1 preposition used in different L1 sentences, could translate to several different L2 prepositions. Automatic error detection/correction methods often process prepositions and determiners in the same way because they both fall in the class of function1 www.druide.com 2 www.stylewriter-usa.com words. However</context>
<context position="7100" citStr="Granger et al., 2001" startWordPosition="1102" endWordPosition="1105">work, which involves parsing in combination with a very large language model and Machine Translation, constitutes heavier machinery than is warranted for that simpler problem. There are two major causes of preposition errors in a SLL context. The first kind is caused by lexical confusion within the second language itself. For example, a L2 author writing in English may erroneously use a location preposition like &amp;quot;at&amp;quot; where another location preposition like &amp;quot;in&amp;quot; would have been more appropriate. The second kind involves linguistic interference between prepositions in L1 and prepositions in L2 (Granger et al., 2001). For example, a Second Language Learner who wants to render the following two English sentences in French &amp;quot;I go to Montreal&amp;quot; and &amp;quot;I go to Argentina&amp;quot;, might use the same French preposition &amp;quot;d&amp;quot; for &amp;quot;to&amp;quot;, when in fact, French usage dictates that you write &amp;quot;d Montréal”, and &amp;quot;en Argentine”. Note that the situation varies greatly from language to language. The same two English sentences rendered in Italian and German would in fact employ a same preposition, whereas in Spanish, different prepositions would also be required as in French. 65 Studies have found that the majority of errors made by L2 au</context>
</contexts>
<marker>Granger, Vandeventer, Hamel, 2001</marker>
<rawString>Granger S., Vandeventer A. &amp; Hamel M. J. 2001. Analyse de corpus d&apos;apprenants pour l&apos;ELAO basé sur le TAL. TAL 42(2), 609-621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Halevy</author>
<author>P Norvig</author>
<author>F Pereira</author>
</authors>
<title>The Unreasonable Effectiveness of Data.&amp;quot;,</title>
<date>2009</date>
<journal>IEEE Intelligent Systems, March/April</journal>
<pages>8--12</pages>
<contexts>
<context position="32408" citStr="Halevy et al., 2009" startWordPosition="5343" endWordPosition="5346"> with this kind of approach to deal with more complex cases of L1 interference which result in severely damaged L2 sentences. In this paper, we compared the bilingual MT approach to a unilingual baseline which used a relatively simple Web as a corpus algorithm, whose accuracy is comparable to that reported in the literature for a similar preposition correction algorithm (Yi et al, 2008). Notwithstanding the fact that such simple Web as a corpus approaches have often been shown to be competitive with (if not better than) more complex algorithms which cannot leverage the full extent of the web (Halevy et al., 2009), it would be interesting to compare the bilingual MT approach to more sophisticated unilingual algorithms for preposition correction, many of which are referenced in section 3. Error detection is another area for future research. In this paper, we limited ourselves to error correction, since it could be solved through a very simple round-trip translation, without requiring a detailed control of the MT system, or access to lower level information generated by the system in the course of translation (for example, intermediate hypotheses with probabilities and alignment information between sourc</context>
</contexts>
<marker>Halevy, Norvig, Pereira, 2009</marker>
<rawString>Halevy, A., Norvig, P., Pereira, F. 2009. &amp;quot;The Unreasonable Effectiveness of Data.&amp;quot;, IEEE Intelligent Systems, March/April 2009, pp 8-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Heift</author>
<author>M Schulze</author>
</authors>
<title>Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues.</title>
<date>2007</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="2768" citStr="Heift and Schulze, 2007" startWordPosition="415" endWordPosition="418">which, in a first step, focuses on error correction, and ignores for now the preliminary step of error detection which is left for future research. This work is of interest to applications in Computer-Assisted-Language-Learning (CALL) and Intelligent Tutoring Systems (ITS), where tutoring material often consists of drills such as fill-in-theblanks or multiple-choice-questions. These require very little use of a learner&apos;s language production capacities, and in order to support richer free-text assessment capabilities, ITS systems thus need to use error detection and correction functionalities (Heift and Schulze, 2007). Editing Aids (EA) are tools which assist a user in producing written compositions. They typically use rules for grammar checking as well as lexical heuristics to suggest stylistic tips, synonyms or fallacious collocations. Advanced examples of such 64 Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 64–72, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tools include Antidote1 for French and StyleWriter2 for English. Text Editors like MS Word and Word Perfect also include grammar checkers, but their sty</context>
</contexts>
<marker>Heift, Schulze, 2007</marker>
<rawString>Heift, T. &amp; Schulze, M. 2007. Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hermet</author>
<author>A Désilets</author>
<author>S Szpakowicz</author>
</authors>
<title>Using the Web as a Linguistic Resource to Automatically Correct Lexico-Syntactic Errors.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC&apos;08.</booktitle>
<location>Marrakech, Morroco.</location>
<contexts>
<context position="4909" citStr="Hermet et al. (2008)" startWordPosition="758" endWordPosition="761">text. Related work is reviewed in section 3 and the algorithmic framework is presented in section 4. An evaluation is discussed in section 5, and conclusions and directions for future research are presented in section 6. 2 The Preposition Problem Prepositions constitute 14% of all tokens produced in most languages (Fort &amp; Guillaume 2007). They are reported as yielding among the highest error class rates across various languages (Izumi, 2004, for Japanese, Granger et al., 2001, for French). In their analysis of a small corpus of advanced-intermediate French as a Second Language (FSL) learners, Hermet et al. (2008) found that preposition choice accounted for 17.2 % of all errors. Prepositions can be seen as a special class of cognates, in the sense that the same L1 preposition used in different L1 sentences, could translate to several different L2 prepositions. Automatic error detection/correction methods often process prepositions and determiners in the same way because they both fall in the class of function1 www.druide.com 2 www.stylewriter-usa.com words. However, one can make the argument that preposition errors deserve a different and deeper kind of treatment, because they tend to be more semantica</context>
<context position="11924" citStr="Hermet et al. (2008)" startWordPosition="1891" endWordPosition="1894">ction and correction procedures while collocation errors use the same procedure for both. Determiner errors are discovered by thresholds ratios on search hits statistics, taking into account probable ambiguities, since multiple forms of determiners can be valid in a single context. Collocation errors on the other hand, are assessed only by a threshold on absolute counts, that is, a form different from the input automatically signals an error and provides its correction. This suggests that detection and correction procedures coincide when the error ceases to bear on a function word. Similarly, Hermet et al. (2008) use a Web as corpus based approach to address the correction of preposition errors in a French-as-a-Second-Language (FSL) context. Candidate prepositions are substituted for erroneous ones following a taxonomy of semantic classes, which produces a set of al66 ternate sentences for each error. The main interest of their study is the use of a syntax-based sentence generalization method to maximize the likelihood that at least one of the alternatives will have at least one hits on the Web. They achieve accuracy of 69% in error repair (no error detection), on a small set of clauses written by FSL</context>
</contexts>
<marker>Hermet, Désilets, Szpakowicz, 2008</marker>
<rawString>Hermet, M., Désilets, A., Szpakowicz, S. 2008. Using the Web as a Linguistic Resource to Automatically Correct Lexico-Syntactic Errors. In Proceedings of the LREC&apos;08. Marrakech, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<title>The overview of the sst speech corpus of Japanese learner English and evaluation through the experiment on automatic detection of learners’ errors.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>Izumi, E., K. Uchimoto, and H. Isahara. 2004. The overview of the sst speech corpus of Japanese learner English and evaluation through the experiment on automatic detection of learners’ errors. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>La Torre</author>
<author>M D</author>
</authors>
<title>A web-based resource to imprive translation skills.</title>
<date>1999</date>
<journal>ReCALL, Vol</journal>
<volume>11</volume>
<pages>41--49</pages>
<marker>Torre, D, 1999</marker>
<rawString>La Torre, M. D. 1999. A web-based resource to imprive translation skills. ReCALL, Vol 11, No3, pp. 41-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Automatic grammar correction for second-language learners.</title>
<date>2006</date>
<booktitle>In Interspeech. ICSLP.</booktitle>
<pages>1978--1981</pages>
<location>Pittsburgh.</location>
<marker>Lee, Seneff, 2006</marker>
<rawString>Lee J. and Seneff S. 2006. Automatic grammar correction for second-language learners. In Interspeech. ICSLP. p. 1978-1981. Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L&apos;haire</author>
<author>Vandeventer Faltin A</author>
</authors>
<title>Error diagnosis in the FreeText project.</title>
<date>2003</date>
<journal>CALICO</journal>
<booktitle>special Issue Error Analysis and Error Correction in Computer-Assisted Language Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<pages>481--495</pages>
<editor>T. Heift &amp; M. Schulze (eds.).</editor>
<marker>L&apos;haire, A, 2003</marker>
<rawString>L&apos;haire S. &amp; Vandeventer Faltin A. 2003. Error diagnosis in the FreeText project. CALICO 20(3), 481-495, special Issue Error Analysis and Error Correction in Computer-Assisted Language Learning, T. Heift &amp; M. Schulze (eds.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schneider</author>
<author>K F McCoy</author>
</authors>
<title>Recognizing syntactic errors in the writing of second language learners.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL 98.</booktitle>
<contexts>
<context position="8950" citStr="Schneider and McCoy, 1998" startWordPosition="1407" endWordPosition="1410">d instead be written as &amp;quot;sur les lieux du crime&amp;quot; (which in English, would translate literally to &amp;quot;on the location of the crime&amp;quot;). One might suspect that preposition errors of the first type would be solvable using unilingual L2 language models, but that the second type might benefit from a language model which also takes L1 into account. This is the main question investigated in this paper. 3 Related Work Historically, grammatical error correction has been done through parsing-based techniques such as syntactic constraint-relaxation (L&apos;haire &amp; Vandeventer-Feltin, 2003), or mal-rules modeling (Schneider and McCoy, 1998). But generating the rule-bases needed by these types of approaches involves a lot of manual work, and may still in the end be too imprecise to convey information on the nature and solution of an error. Recently, more effort has been put in methods that rely on automatically built language models. Typically, this kind of work will focus either on a restricted class of errors or on specific domains. Seneff and Lee (2006) propose a two-phased generation-based framework where a n-gram model re-ranked by a stochastic context-free-grammar model is used to correct sentence-level errors in the langua</context>
</contexts>
<marker>Schneider, McCoy, 1998</marker>
<rawString>Schneider, D. and McCoy, K. F. 1998. Recognizing syntactic errors in the writing of second language learners. In Proceedings of COLING/ACL 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Saint-Dizier</author>
</authors>
<title>Regroupement des Prépositions par sens. Undated Report. IRIT.</title>
<date>2007</date>
<location>Toulouse. http://www.irit.fr/recherches/ILPL/SiteEquipe/publi_fichier/prepLCS.doc</location>
<contexts>
<context position="18126" citStr="Saint-Dizier (2007)" startWordPosition="2933" endWordPosition="2934">d on the verb sub-categorization frame, when a PP can be attached directly to the preceding verb. In case of ambiguity in the attachment of the PP, two versions of the pruned sentence can be produced reflecting two different PP attachments. Lemmatization of verbs is also carried out in the pruning step. After pruning, the right and left sides of the sentences are re-assembled with alternate prepositions. The replacement of prepositions is controlled by way of semantics. Since prepositions are richer in sense than strict function words, they can therefore be categorized according to semantics. Saint-Dizier (2007) proposes such a taxonomy, and in our framework, prepositions have been grouped in 7 non-exclusive categories. Table 1 provides details of this categorization. The input preposition is mapped to all the sets it belongs to, and corresponding alternates are retrieved as correction candidates. The 6 most frequent French preposition are also added automatically to the candidates list. The resulting sentences are then sent to the Yahoo Search Engine and hits are counted. The number of hits returned by each of the queries is used as decision criteria, and the preposition contained in the query with </context>
</contexts>
<marker>Saint-Dizier, 2007</marker>
<rawString>Saint-Dizier, P. 2007. Regroupement des Prépositions par sens. Undated Report. IRIT. Toulouse. http://www.irit.fr/recherches/ILPL/SiteEquipe/publi_fichier/prepLCS.doc</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<date>2001</date>
<booktitle>Three Perspectives on MT in the Classroom, MT SUMMIT VIII Workshop on Teaching Machine Translation, Santiago de Compostela,</booktitle>
<pages>25--29</pages>
<contexts>
<context position="12740" citStr="Somers, 2001" startWordPosition="2029" endWordPosition="2030">axonomy of semantic classes, which produces a set of al66 ternate sentences for each error. The main interest of their study is the use of a syntax-based sentence generalization method to maximize the likelihood that at least one of the alternatives will have at least one hits on the Web. They achieve accuracy of 69% in error repair (no error detection), on a small set of clauses written by FSL Learners. Very little work has been done to actually exploit knowledge of a L2 author&apos;s first language, in correcting errors. Several authors (Wang and Garigliano, 1992, Anderson, 1995, La Torre, 1999, Somers, 2001) have suggested that students may learn by analyzing erroneous sentences produced by a MT system, and reflecting on the probable cause of errors, especially in terms of interference between the two languages. In this context however, the MT system is used only to generate exercises, as opposed to helping the student find and correct errors in texts that he produces. Although it is not based on an MT model, Wang and Garigliano propose an algorithm which uses a hand-crafted, domain-specific, mixed L1 and L2 grammar, in order to identify L1 interference errors in L2 sentences. L2 sentences are pa</context>
</contexts>
<marker>Somers, 2001</marker>
<rawString>Somers, Harold. 2001. Three Perspectives on MT in the Classroom, MT SUMMIT VIII Workshop on Teaching Machine Translation, Santiago de Compostela, pages 25-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The Ups and Downs of Preposition Error Detection.</title>
<date>2008</date>
<location>COLING, Manchester.</location>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Tetreault J. and Chodorow M. 2008. The Ups and Downs of Preposition Error Detection. COLING, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>M Simard</author>
<author>S Larkin</author>
<author>J H Johnson</author>
</authors>
<date>2007</date>
<booktitle>NRC&apos;s PORTAGE system for WMT 2007, ACL-2007 Workshop on SMT,</booktitle>
<location>Prague, Czech Republic</location>
<contexts>
<context position="33257" citStr="Ueffing et al., 2007" startWordPosition="5481" endWordPosition="5484">h. In this paper, we limited ourselves to error correction, since it could be solved through a very simple round-trip translation, without requiring a detailed control of the MT system, or access to lower level information generated by the system in the course of translation (for example, intermediate hypotheses with probabilities and alignment information between source and target sentences). In contrast, we believe that error detection with an MT approach will require this kind of finer control and access to the guts of the MT system. We plan to investigate this using the PORTAGE MT system (Ueffing et al., 2007). Essentially, we plan to use the MT system&apos;s internal information to assign confidence scores to various segments of the roundtrip translation, and label them as corrections if this confidence is above a certain threshold. In doing this, we will be following in the footsteps of Yi et al. (2008) who use the same algorithm for error detection and error correction. The process of detecting an error is simply one of determining whether the system&apos;s topmost alternative is different from what appeared in the original sentence, and whether the system&apos;s confidence in that alternative is sufficiently </context>
</contexts>
<marker>Ueffing, Simard, Larkin, Johnson, 2007</marker>
<rawString>Ueffing, N., Simard, M., Larkin, S., Johnson, J. H. (2007), NRC&apos;s PORTAGE system for WMT 2007, ACL-2007 Workshop on SMT, Prague, Czech Republic 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>R Garigliano</author>
</authors>
<title>An Intelligent Language Tutoring System for Handling Errors caused by Transfer.</title>
<date>1992</date>
<booktitle>In Proceedings of ITS-92,</booktitle>
<pages>395--404</pages>
<contexts>
<context position="7818" citStr="Wang and Garigliano, 1992" startWordPosition="1219" endWordPosition="1222">ces in French &amp;quot;I go to Montreal&amp;quot; and &amp;quot;I go to Argentina&amp;quot;, might use the same French preposition &amp;quot;d&amp;quot; for &amp;quot;to&amp;quot;, when in fact, French usage dictates that you write &amp;quot;d Montréal”, and &amp;quot;en Argentine”. Note that the situation varies greatly from language to language. The same two English sentences rendered in Italian and German would in fact employ a same preposition, whereas in Spanish, different prepositions would also be required as in French. 65 Studies have found that the majority of errors made by L2 authors (especially intermediate to advanced ones) are caused by such linguistic interference (Wang and Garigliano, 1992, Cowan, 1983, p 109). Note that this kind of linguistic interference can often lead to much more severe and hard to repair errors, as illustrated by the following example, taken from an actual SLL corpus. Say a native English author wants to render &amp;quot;Police arrived at the scene of the crime&amp;quot; into French (her L2). Because she is not fluent in French, she translates the last part of the sentence to &amp;quot;à la scène de la crime&amp;quot;. This literal translation turns out to be highly unidiomatic in French, and should instead be written as &amp;quot;sur les lieux du crime&amp;quot; (which in English, would translate literally </context>
<context position="12693" citStr="Wang and Garigliano, 1992" startWordPosition="2020" endWordPosition="2023">epositions are substituted for erroneous ones following a taxonomy of semantic classes, which produces a set of al66 ternate sentences for each error. The main interest of their study is the use of a syntax-based sentence generalization method to maximize the likelihood that at least one of the alternatives will have at least one hits on the Web. They achieve accuracy of 69% in error repair (no error detection), on a small set of clauses written by FSL Learners. Very little work has been done to actually exploit knowledge of a L2 author&apos;s first language, in correcting errors. Several authors (Wang and Garigliano, 1992, Anderson, 1995, La Torre, 1999, Somers, 2001) have suggested that students may learn by analyzing erroneous sentences produced by a MT system, and reflecting on the probable cause of errors, especially in terms of interference between the two languages. In this context however, the MT system is used only to generate exercises, as opposed to helping the student find and correct errors in texts that he produces. Although it is not based on an MT model, Wang and Garigliano propose an algorithm which uses a hand-crafted, domain-specific, mixed L1 and L2 grammar, in order to identify L1 interfere</context>
</contexts>
<marker>Wang, Garigliano, 1992</marker>
<rawString>Wang, Y. and Garigliano, R. 1992. An Intelligent Language Tutoring System for Handling Errors caused by Transfer. In Proceedings of ITS-92, pp. 395-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yi</author>
<author>J F Gao</author>
<author>W B Dolan</author>
</authors>
<title>A Web-based English Proofing System for English as a Second Language Users.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP 2008,</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="10840" citStr="Yi et al. (2008)" startWordPosition="1715" endWordPosition="1718">e lemmatization as a means of generalization and trained their model over 7 million prepositional contexts, achieving results of 84% precision and 19% recall in preposition error detection in the best of the system&apos;s configurations. Gamon et al. (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct form. They evaluate their framework on the task of preposition identification and report results ranging from 74 to 45% precision on a set of 13 prepositions. Yi et al. (2008) use the Web as corpus and send segments of sentences of varying length as bag-ofconstituents queries to retrieve occurrence contexts. The number of the queried segments is a PoS condition of &amp;quot;check-points&amp;quot; sensitive to typical errors made by L2 authors. The contexts retrieved are in turn analyzed for correspondence with the original input. The detection and correction methods differ according to the class of the error. Determiner errors call for distinct detection and correction procedures while collocation errors use the same procedure for both. Determiner errors are discovered by thresholds</context>
<context position="32177" citStr="Yi et al, 2008" startWordPosition="5304" endWordPosition="5307">proach, a hybrid combination of both performed much better than the unilingual approach alone. More work needs to be carried out in order to fully evaluate the potential of the MT approach. In particular, we plan to experiment with this kind of approach to deal with more complex cases of L1 interference which result in severely damaged L2 sentences. In this paper, we compared the bilingual MT approach to a unilingual baseline which used a relatively simple Web as a corpus algorithm, whose accuracy is comparable to that reported in the literature for a similar preposition correction algorithm (Yi et al, 2008). Notwithstanding the fact that such simple Web as a corpus approaches have often been shown to be competitive with (if not better than) more complex algorithms which cannot leverage the full extent of the web (Halevy et al., 2009), it would be interesting to compare the bilingual MT approach to more sophisticated unilingual algorithms for preposition correction, many of which are referenced in section 3. Error detection is another area for future research. In this paper, we limited ourselves to error correction, since it could be solved through a very simple round-trip translation, without re</context>
<context position="33553" citStr="Yi et al. (2008)" startWordPosition="5530" endWordPosition="5533">e hypotheses with probabilities and alignment information between source and target sentences). In contrast, we believe that error detection with an MT approach will require this kind of finer control and access to the guts of the MT system. We plan to investigate this using the PORTAGE MT system (Ueffing et al., 2007). Essentially, we plan to use the MT system&apos;s internal information to assign confidence scores to various segments of the roundtrip translation, and label them as corrections if this confidence is above a certain threshold. In doing this, we will be following in the footsteps of Yi et al. (2008) who use the same algorithm for error detection and error correction. The process of detecting an error is simply one of determining whether the system&apos;s topmost alternative is different from what appeared in the original sentence, and whether the system&apos;s confidence in that alternative is sufficiently high to take the risk of presenting it to the user as a suggested correction. 71 Acknowledgments The authors are indebted to the following people (all from NRC) for helpful advice on how to best exploit MT for second language correction: Pierre Isabelle, George Foster and Eric Joanis. References</context>
</contexts>
<marker>Yi, Gao, Dolan, 2008</marker>
<rawString>Yi X., Gao J. F., Dolan W. B., 2008. A Web-based English Proofing System for English as a Second Language Users. In Proceedings of IJCNLP 2008, Hyderabad, India, January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>