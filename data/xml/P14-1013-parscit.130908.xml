<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.946087">
Learning Topic Representation for SMT with Neural Networks∗
</title>
<author confidence="0.997661">
Lei Cui1, Dongdong Zhang2, Shujie Liu2, Qiming Chen3, Mu Li2, Ming Zhou2, and Muyun Yang1
</author>
<affiliation confidence="0.999549">
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, P.R. China
</affiliation>
<email confidence="0.569705">
leicui@hit.edu.cn, ymy@mtlab.hit.edu.cn
</email>
<address confidence="0.553146">
2Microsoft Research, Beijing, P.R. China
</address>
<email confidence="0.941993">
{dozhang,shujliu,muli,mingzhou}@microsoft.com
</email>
<affiliation confidence="0.832201">
3Shanghai Jiao Tong University, Shanghai, P.R. China
</affiliation>
<email confidence="0.991994">
simoncqm@gmail.com
</email>
<sectionHeader confidence="0.994621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988873">
Statistical Machine Translation (SMT)
usually utilizes contextual information
to disambiguate translation candidates.
However, it is often limited to contexts
within sentence boundaries, hence broader
topical information cannot be leveraged.
In this paper, we propose a novel approach
to learning topic representation for paral-
lel data using a neural network architec-
ture, where abundant topical contexts are
embedded via topic relevant monolingual
data. By associating each translation rule
with the topic representation, topic rele-
vant rules are selected according to the dis-
tributional similarity with the source text
during SMT decoding. Experimental re-
sults show that our method significantly
improves translation accuracy in the NIST
Chinese-to-English translation task com-
pared to a state-of-the-art baseline.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996799568965517">
Making translation decisions is a difficult task in
many Statistical Machine Translation (SMT) sys-
tems. Current translation modeling approaches
usually use context dependent information to dis-
ambiguate translation candidates. For exam-
ple, translation sense disambiguation approaches
(Carpuat and Wu, 2005; Carpuat and Wu,
2007) are proposed for phrase-based SMT sys-
tems. Meanwhile, for hierarchical phrase-based
or syntax-based SMT systems, there is also much
work involving rich contexts to guide rule selec-
tion (He et al., 2008; Liu et al., 2008; Marton
and Resnik, 2008; Xiong et al., 2009). Although
these methods are effective and proven successful
in many SMT systems, they only leverage within-
∗This work was done while the first and fourth authors
were visiting Microsoft Research.
sentence contexts which are insufficient in explor-
ing broader information. For example, the word
driver often means “the operator of a motor ve-
hicle” in common texts. But in the sentence “Fi-
nally, we write the user response to the buffer, i.e.,
pass it to our driver”, we understand that driver
means “computer program”. In this case, people
understand the meaning because of the IT topical
context which goes beyond sentence-level analy-
sis and requires more relevant knowledge. There-
fore, it is important to leverage topic information
to learn smarter translation models and achieve
better translation performance.
Topic modeling is a useful mechanism for dis-
covering and characterizing various semantic con-
cepts embedded in a collection of documents. At-
tempts on topic-based translation modeling in-
clude topic-specific lexicon translation models
(Zhao and Xing, 2006; Zhao and Xing, 2007),
topic similarity models for synchronous rules
(Xiao et al., 2012), and document-level translation
with topic coherence (Xiong and Zhang, 2013). In
addition, topic-based approaches have been used
in domain adaptation for SMT (Tam et al., 2007;
Su et al., 2012), where they view different topics
as different domains. One typical property of these
approaches in common is that they only utilize
parallel data where document boundaries are ex-
plicitly given. In this way, the topic of a sentence
can be inferred with document-level information
using off-the-shelf topic modeling toolkits such
as Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) or Hidden Topic Markov Model (HTMM)
(Gruber et al., 2007). Most of them also assume
that the input must be in document level. However,
this situation does not always happen since there is
considerable amount of parallel data which does
not have document boundaries. In addition, con-
temporary SMT systems often works on sentence
level rather than document level due to the effi-
ciency. Although we can easily apply LDA at the
</bodyText>
<page confidence="0.987771">
133
</page>
<note confidence="0.8318025">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999055096153846">
sentence level, it is quite difficult to infer the topic
accurately with only a few words in the sentence.
This makes previous approaches inefficient when
applied them in real-world commercial SMT sys-
tems. Therefore, we need to devise a systematical
approach to enriching the sentence and inferring
its topic more accurately.
In this paper, we propose a novel approach to
learning topic representations for sentences. Since
the information within the sentence is insufficient
for topic modeling, we first enrich sentence con-
texts via Information Retrieval (IR) methods using
content words in the sentence as queries, so that
topic-related monolingual documents can be col-
lected. These topic-related documents are utilized
to learn a specific topic representation for each
sentence using a neural network based approach.
Neural network is an effective technique for learn-
ing different levels of data representations. The
levels inferred from neural network correspond to
distinct levels of concepts, where high-level rep-
resentations are obtained from low-level bag-of-
words input. It is able to detect correlations among
any subset of input features through non-linear
transformations, which demonstrates the superior-
ity of eliminating the effect of noisy words which
are irrelevant to the topic. Our problem fits well
into the neural network framework and we expect
that it can further improve inferring the topic rep-
resentations for sentences.
To incorporate topic representations as trans-
lation knowledge into SMT, our neural network
based approach directly optimizes similarities be-
tween the source language and target language in a
compact topic space. This underlying topic space
is learned from sentence-level parallel data in or-
der to share topic information across the source
and target languages as much as possible. Addi-
tionally, our model can be discriminatively trained
with a large number of training instances, without
expensive sampling methods such as in LDA or
HTMM, thus it is more practicable and scalable.
Finally, we associate the learned representation to
each bilingual translation rule. Topic-related rules
are selected according to distributional similarity
with the source text, which helps hypotheses gen-
eration in SMT decoding. We integrate topic simi-
larity features in the log-linear model and evaluate
the performance on the NIST Chinese-to-English
translation task. Experimental results demonstrate
that our model significantly improves translation
accuracy over a state-of-the-art baseline.
</bodyText>
<sectionHeader confidence="0.924113" genericHeader="method">
2 Background: Deep Learning
</sectionHeader>
<bodyText confidence="0.999976">
Deep learning is an active topic in recent years
which has triumphed in many machine learning
research areas. This technique began raising pub-
lic awareness in the mid-2000s after researchers
showed how a multi-layer feed-forward neural
network can be effectively trained. The train-
ing procedure often involves two phases: a layer-
wise unsupervised pre-training phase and a su-
pervised fine-tuning phase. For pre-training, Re-
stricted Boltzmann Machine (RBM) (Hinton et
al., 2006), auto-encoding (Bengio et al., 2006)
and sparse coding (Lee et al., 2006) are most fre-
quently used. Unsupervised pre-training trains the
network one layer at a time and helps guide the pa-
rameters of the layer towards better regions in pa-
rameter space (Bengio, 2009). Followed by fine-
tuning in this parameter region, deep learning is
able to achieve state-of-the-art performance in var-
ious research areas, including breakthrough results
on the ImageNet dataset for objective recognition
(Krizhevsky et al., 2012), significant error reduc-
tion in speech recognition (Dahl et al., 2012), etc.
Deep learning has also been successfully ap-
plied in a variety of NLP tasks such as part-of-
speech tagging, chunking, named entity recog-
nition, semantic role labeling (Collobert et al.,
2011), parsing (Socher et al., 2011a), sentiment
analysis (Socher et al., 2011b), etc. Most NLP
research converts a high-dimensional and sparse
binary representation into a low-dimensional and
real-valued representation. This low-dimensional
representation is usually learned from huge
amount of monolingual texts in the pre-training
phase, and then fine-tuned towards task-specific
criterion. Inspired by previous successful re-
search, we first learn sentence representations us-
ing topic-related monolingual texts in the pre-
training phase, and then optimize the bilingual
similarity by leveraging sentence-level parallel
data in the fine-tuning phase.
</bodyText>
<sectionHeader confidence="0.9318555" genericHeader="method">
3 Topic Similarity Model with Neural
Network
</sectionHeader>
<bodyText confidence="0.9999136">
In this section, we explain our neural network
based topic similarity model in detail, as well as
how to incorporate the topic similarity features
into SMT decoding procedure. Figure 1 sketches
the high-level overview which illustrates how to
</bodyText>
<page confidence="0.998792">
134
</page>
<figureCaption confidence="0.994703">
Figure 1: Overview of neural network based topic
similarity model.
</figureCaption>
<bodyText confidence="0.999701125">
learn topic representations using sentence-level
parallel data. Given a parallel sentence pair Y, e),
the first step is to treat f and e as queries, and
use IR methods to retrieve relevant documents to
enrich contextual information for them. Specifi-
cally, the ranking model we used is a Vector Space
Model (VSM), where the query and document are
converted into tf-idf weighted vectors. The most
relevant N documents df and de are retrieved and
converted to a high-dimensional, bag-of-words in-
put f and e for the representation learning1.
There are two phases in our neural network
training process: pre-training and fine-tuning. In
the pre-training phase (Section 3.1), we build two
neural networks with the same structure but differ-
ent parameters to learn a low-dimensional repre-
sentation for sentences in two different languages.
Then, in the fine-tuning phase (Section 3.2), our
model directly optimizes the similarity of two low-
dimensional representations, so that it highly cor-
relates to SMT decoding. Finally, the learned rep-
resentation is used to calculate similarities which
are integrated as features in SMT decoding proce-
dure (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.9836455">
3.1 Pre-training using denoising
auto-encoder
</subsectionHeader>
<bodyText confidence="0.999975666666667">
In the pre-training phase, we leverage neural
network structures to transform high-dimensional
sparse vectors to low-dimensional dense vectors.
The topic similarity is calculated on top of the
learned dense vectors. This dense representation
should preserve the information from the bag-of-
</bodyText>
<footnote confidence="0.8835885">
1We use f and e to denote the n-of-V vector converted
from the retrieved documents.
</footnote>
<bodyText confidence="0.999773367346939">
words input, meanwhile alleviate data sparse prob-
lem. Therefore, we use a specially designed mech-
anism called auto-encoder to solve this problem.
Auto-encoder (Bengio et al., 2006) is one of the
basic building blocks of deep learning. Assum-
ing that the input is a n-of-V binary vector x rep-
resenting the bag-of-words (V is the vocabulary
size), an auto-encoder consists of an encoding pro-
cess g(x) and a decoding process h(g(x)). The
objective of the auto-encoder is to minimize the
reconstruction error L(h(g(x)), x). Our goal is to
learn a low-dimensional vector which can preserve
information from the original n-of-V vector.
One problem with auto-encoder is that it treats
all words in the same way, making no distinguish-
ment between function words and content words.
The representation learned by auto-encoders tends
to be influenced by the function words, thereby it
is not robust. To alleviate this problem, Vincent et
al. (2008) proposed the Denoising Auto-Encoder
(DAE), which aims to reconstruct a clean, “re-
paired” input from a corrupted, partially destroyed
vector. This is done by corrupting the initial in-
put x to get a partially destroyed version ˜x. DAE
is capable of capturing the global structure of the
input while ignoring the noise. In our task, for
each sentence, we treat the retrieved N relevant
documents as a single large document and convert
it to a bag-of-words vector x in Figure 2. With
DAE, the input x is manually corrupted by apply-
ing masking noise (randomly mask 1 to 0) and get-
ting ˜x. Denoising training is considered as “filling
in the blanks” (Vincent et al., 2010), which means
the masking components can be recovered from
the non-corrupted components. For example, in
IT related texts, if the word driver is masked, it
should be predicted through hidden units in neural
networks by active signals such as “buffer”, “user
response”, etc.
In our case, the encoding process transforms
the corrupted input x˜ into g(˜x) with two layers:
a linear layer connected with a non-linear layer.
Assuming that the dimension of the g(˜x) is L,
the linear layer forms a L x V matrix W which
projects the n-of-V vector to a L-dimensional hid-
den layer. After the bag-of-words input has been
transformed, they are fed into a subsequent layer
to model the highly non-linear relations among
words:
</bodyText>
<equation confidence="0.71176">
z = f(Wx˜+ b) (1)
</equation>
<bodyText confidence="0.98381">
where z is the output of the non-linear layer, b is a
</bodyText>
<figure confidence="0.992009821428572">
sim(f,e)
f e
Neural Network
Training
Data
Preprocessing
de
df
cos(Zf, ZJ
Ze = g(e)
Zf = g(f)
English
document
collection
Chinese
document
collection
Parallel
sentence
e
f
IR IR
135
h(gXt)
g (Xt)
Xt
L(h g Xt , X)
X
</figure>
<figureCaption confidence="0.9978725">
Figure 2: Denoising auto-encoder with a bag-of-
words input.
</figureCaption>
<bodyText confidence="0.999928833333333">
L-length bias vector. f(·) is a non-linear function,
where common choices include sigmoid function,
hyperbolic function, “hard” hyperbolic function,
rectifier function, etc. In this work, we use the
rectifier function as our non-linear function due to
its efficiency and better performance (Glorot et al.,
</bodyText>
<equation confidence="0.9187742">
2011):
�
x if x &gt; 0
rec(x) = (2)
0 otherwise
</equation>
<bodyText confidence="0.999980375">
The decoding process consists of a linear layer
and a non-linear layer with similar network struc-
tures, but different parameters. It transforms the
L-dimensional vector g(˜x) to a V-dimensional
vector h(g(˜x)). To minimize reconstruction error
with respect to ˜x, we define the loss function as
the L2-norm of the difference between the uncor-
rupted input and reconstructed input:
</bodyText>
<equation confidence="0.998833">
L(h(g(˜x)),x) = Ih(g(˜x)) − xI2 (3)
</equation>
<bodyText confidence="0.999549222222222">
Multi-layer neural networks are trained with the
standard back-propagation algorithm (Rumelhart
et al., 1988). The gradient of the loss function
is calculated and back-propagated to the previous
layer to update its parameters. Training neural net-
works involves many factors such as the learning
rate and the length of hidden layers. We will dis-
cuss the optimization of these parameters in Sec-
tion 4.
</bodyText>
<subsectionHeader confidence="0.999942">
3.2 Fine-tuning with parallel data
</subsectionHeader>
<bodyText confidence="0.999986357142857">
In the fine-tuning phase, we stack another layer on
top of the two low-dimensional vectors to maxi-
mize the similarity between source and target lan-
guages. The similarity scores are integrated into
the standard log-linear model for making transla-
tion decisions. Since the vectors from DAE are
trained using information from monolingual train-
ing data independently, these vectors may be in-
adequate to measure bilingual topic similarity due
to their different topic spaces. Therefore, in this
stage, parallel sentence pairs are used to help con-
necting the vectors from different languages be-
cause they express the same topic. In fact, the ob-
jective of fine-tuning is to discover a latent topic
space which is shared by both languages as much
as possible. This shared topic space is particularly
useful when the SMT decoder tries to match the
source texts and translation candidates in the tar-
get language.
Given a parallel sentence pair (f, e), the DAE
learns representations for f and e respectively, as
zf = g(f) and ze = g(e) in Figure 1. We then take
two vectors as the input to calculate their similar-
ity. Consequently, the whole neural network can
be fine-tuned towards the supervised criteria with
the help of parallel data. The similarity score of
the representation pair (zf, ze) is defined as the co-
sine similarity of the two vectors:
</bodyText>
<equation confidence="0.989943">
sim(f, e) = COS(zf, ze)
zf · ze (4)
IzfIIzeI
</equation>
<bodyText confidence="0.94582608">
Since a parallel sentence pair should have the
same topic, our goal is to maximize the similar-
ity score between the source sentence and target
sentence. Inspired by the contrastive estimation
method (Smith and Eisner, 2005), for each paral-
lel sentence pair (f, e) as a positive instance, we
select another sentence pair (f&apos;, e&apos;) from the train-
ing data and treat (f, e&apos;) as a negative instance. To
make the similarity of the positive instance larger
than the negative instance by some margin q, we
utilize the following pairwise ranking loss:
L(f, e) = max{0, q − sim(f, e) + sim(f, e&apos;)}
(5)
where q =12 − sim(f, f&apos;). The rationale behind
this criterion is, the smaller sim(f, f&apos;) is, the more
we should penalize negative instances.
To effectively train the model in this task, neg-
ative instances must be selected carefully. Since
different sentences may have very similar topic
distributions, we select negative instances that are
dissimilar with the positive instances based on the
following criteria:
1. For each positive instance (f, e), we select e&apos;
which contains at least 30% different content
words from e.
</bodyText>
<page confidence="0.958066">
136
</page>
<bodyText confidence="0.994825333333333">
2. If we cannot find such e&apos;, remove (f, e) from
the training instances for network learning.
The model minimizes the pairwise ranking loss
across all training instances:
We also consider the topic sensitivity estimation
since general rules have flatter distributions while
topic-specific rules have sharper distributions. A
standard entropy metric is used to measure the sen-
sitivity of the source-side of (α, γ) as:
</bodyText>
<equation confidence="0.973342333333333">
X L(f, e) (6) Sen(α) = − X |zα |zαi X log zαi (11)
L = i=1
(f,e)
</equation>
<bodyText confidence="0.9992045">
We used standard back-propagation algorithm
to further fine-tune the neural network parameters
W and b in Equation (1). The learned neural net-
works are used to obtain sentence topic representa-
tions, which will be further leveraged to infer topic
representations of bilingual translation rules.
</bodyText>
<subsectionHeader confidence="0.971816">
3.3 Integration into SMT decoding
</subsectionHeader>
<bodyText confidence="0.999997181818182">
We incorporate the learned topic similarity scores
into the standard log-linear framework for SMT.
When a synchronous rule (α, γ) is extracted from
a sentence pair (f, e), a triple instance Z =
((α, γ), (f, e), c) is collected for inferring the
topic representation of (α, γ), where c is the count
of rule occurrence. Following (Chiang, 2007), we
give a count of one for each phrase pair occurrence
and a fractional count for each hierarchical phrase
pair. The topic representation of (α, γ) is then cal-
culated as the weighted average:
</bodyText>
<equation confidence="0.9933955">
P
((α,γ),(f,e),c)ET Ic X zel
P
((α,γ),(f,e),c)ET W
</equation>
<bodyText confidence="0.999988181818182">
where T denotes all instances for the rule (α, γ),
zα and zγ are the source-side and target-side topic
vectors respectively.
By measuring the similarity between the source
texts and bilingual translation rules, the SMT de-
coder is able to encourage topic relevant transla-
tion candidates and penalize topic irrelevant candi-
dates. Therefore, it helps to train a smarter transla-
tion model with the embedded topic information.
Given a source sentence s to be translated, we de-
fine the similarity as follows:
</bodyText>
<equation confidence="0.934345">
Sim(zs, zα) = cos(zs, zα) (9)
Sim(zs, zγ) = cos(zs, zγ) (10)
</equation>
<bodyText confidence="0.999529272727273">
where zs is the topic representation of s. The
similarity calculated against zα or zγ denotes the
source-to-source or the source-to-target similarity.
where zαi is a component in the vector zα. The
target-side sensitivity Sen(γ) can be calculated in
a similar way. The larger the sensitivity is, the
more topic-specific the rule manifests.
In addition to traditional SMT features, we add
new topic-related features into the standard log-
linear framework. For the SMT system, the best
translation candidate eˆ is given by:
</bodyText>
<equation confidence="0.991373">
eˆ = arg max P(elf) (12)
e
</equation>
<bodyText confidence="0.935528">
where the translation probability is given by:
</bodyText>
<equation confidence="0.99050175">
P(elf) a X wi · log φi(f, e)
i
X
= wj · log φj(f, e)
j
 |{z }
Standard
(13)
</equation>
<bodyText confidence="0.949609230769231">
where φj(f, e) is the standard feature function and
wj is the corresponding feature weight. φk(f, e)
is the topic-related feature function and wk is the
feature weight. The detailed feature description is
as follows:
Standard features: Translation model, includ-
ing translation probabilities and lexical weights
for both directions (4 features), 5-gram language
model (1 feature), word count (1 feature), phrase
count (1 feature), NULL penalty (1 feature), num-
ber of hierarchical rules used (1 feature).
Topic-related features: rule similarity scores
(2 features), rule sensitivity scores (2 features).
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.916837">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999929428571429">
We evaluate the performance of our neural net-
work based topic similarity model on a Chinese-
to-English machine translation task. In neural net-
work training, a large number of monolingual doc-
uments are collected in both source and target lan-
guages. The documents are mainly from two do-
mains: news and weblog. We use Chinese and
</bodyText>
<equation confidence="0.760723833333333">
(7)
P((α,γ),(f,e),c)ET W
zα =
P
((α,γ),(f,e),c)ET {c X zfI
zγ =
</equation>
<figure confidence="0.758196166666667">
(8)
X wk · log φk(f, e)
+
k
 |{z }
Topic related
</figure>
<page confidence="0.974792">
137
</page>
<bodyText confidence="0.9783408">
English Gigaword corpus (Version 5) which are
mainly from news domain. In addition, we also
collect weblog documents with a variety of top-
ics from the web. The total data statistics are
presented in Table 1. These documents are built
in the format of inverted index using Lucene2,
which can be efficiently retrieved by the paral-
lel sentence pairs. The most relevant N docu-
ments are collected, where we experiment with
N = {1, 5, 10, 20, 50}.
</bodyText>
<table confidence="0.9995872">
Domain Chinese English
Docs Words Docs Words
News 5.7M 5.4B 9.9M 25.6B
Weblog 2.1M 8B 1.2M 2.9B
Total 7.8M 13.4B 11.1M 28.5B
</table>
<tableCaption confidence="0.99826">
Table 1: Statistics of monolingual data, in num-
</tableCaption>
<bodyText confidence="0.972264580645161">
bers of documents and words (main content). “M”
refers to million and “B” refers to billion.
We implement a distributed framework to speed
up the training process of neural networks. The
network is learned with mini-batch asynchronous
gradient descent with the adaptive learning rate
procedure called AdaGrad (Duchi et al., 2011).
We use 32 model replicas in each iteration during
the training. The model parameters are averaged
after each iteration and sent to each replica for the
next iteration. The vocabulary size for the input
layer is 100,000, and we choose different lengths
for the hidden layer as L = {100, 300, 600,1000}
in the experiments. In the pre-training phase, all
parallel data is fed into two neural networks re-
spectively for DAE training, where network pa-
rameters W and b are randomly initialized. In
the fine-tuning phase, for each parallel sentence
pair, we randomly select other ten sentence pairs
which satisfy the criterion as negative instances.
These training instances are leveraged to optimize
the similarity of two vectors.
In SMT training, an in-house hierarchical
phrase-based SMT decoder is implemented for our
experiments. The CKY decoding algorithm is
used and cube pruning is performed with the same
default parameter settings as in Chiang (2007).
The parallel data we use is released by LDC3. In
total, the datasets contain nearly 1.1 million sen-
tence pairs. Translation models are trained over
the parallel data that is automatically word-aligned
</bodyText>
<footnote confidence="0.62184225">
2http://lucene.apache.org/
3LDC2003E14, LDC2002E18, LDC2003E07,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34,
LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09
</footnote>
<bodyText confidence="0.9993973125">
using GIZA++ in both directions, and the diag-
grow-final heuristic is used to refine symmetric
word alignment. An in-house language modeling
toolkit is used to train the 5-gram language model
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). The English monolingual data used
for language modeling is the same as in Table
1. The NIST 2003 dataset is the development
data. The testing data consists of NIST 2004,
2005, 2006 and 2008 datasets. The evaluation
metric for the overall translation quality is case-
insensitive BLEU4 (Papineni et al., 2002). The
reported BLEU scores are averaged over 5 times
of running MERT (Och, 2003). A statistical sig-
nificance test is performed using the bootstrap re-
sampling method (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.847213">
4.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999968388888889">
The baseline is a re-implementation of the Hiero
system (Chiang, 2007). The phrase pairs that ap-
pear only once in the parallel data are discarded
because most of them are noisy. We also use
the fix-discount method in Foster et al. (2006)
for phrase table smoothing. This implementation
makes the system perform much better and the
translation model size is much smaller.
We compare our method with the LDA-based
approach proposed by Xiao et al. (2012). In (Xiao
et al., 2012), the topic of each sentence pair is ex-
actly the same as the document it belongs to. Since
some of our parallel data does not have document-
level information, we rely on the IR method to
retrieve the most relevant document and simulate
this approach. The PLDA toolkit (Liu et al., 2011)
is used to infer topic distributions, which takes
34.5 hours to finish.
</bodyText>
<subsectionHeader confidence="0.9195225">
4.3 Effect of retrieved documents and length
of hidden layers
</subsectionHeader>
<bodyText confidence="0.999834153846154">
We illustrate the relationship among translation
accuracy (BLEU), the number of retrieved docu-
ments (N) and the length of hidden layers (L) on
different testing datasets. The results are shown in
Figure 3. The best translation accuracy is achieved
when N=10 for most settings. This confirms that
enriching the source text with topic-related doc-
uments is very useful in determining topic repre-
sentations, thereby help to guide the synchronous
rule selection. However, we find that as N be-
comes larger in the experiments, e.g. N=50, the
translation accuracy drops drastically. As more
documents are retrieved, less relevant information
</bodyText>
<page confidence="0.985772">
138
</page>
<figure confidence="0.990331543859649">
BLEU
BLEU
Number of Retrieved Documents (N)
Number of Retrieved Documents (N)
Number of Retrieved Documents (N)
Number of Retrieved Documents (N)
NIST 2004
42.8
42.6
42.4
42.2
43
42
0 5 10 20 50
L=100
L=300
L=600
L=1000
NIST 2006
BLEU
39.2
38.8
38.6
38.4
38.2
37.80 5 10 20 50
39
38
L=100
L=300
L=600
L=1000
NIST 2005
42
L=100
L=300
L=600
L=1000
41.6
41.4
41.2
0 5 10 20 50
41
41.8
NIST 2008
32
L=100
L=300
L=600
L=1000
31.6
31.4
31.2
0 5 10 20 50
BLEU
31
31.8
</figure>
<figureCaption confidence="0.988442">
Figure 3: End-to-end translation results (BLEU%) using all standard and topic-related features, with
different settings on the number of retrieved documents N and the length of hidden layers L.
</figureCaption>
<bodyText confidence="0.998496619047619">
is also used to train the neural networks. Irrel-
evant documents bring so many unrelated topic
words hence degrade neural network learning per-
formance.
Another important factor is the length of hid-
den layers L in the network. In deep learning, this
parameter is often empirically tuned with human
efforts. As shown in Figure 3, the translation accu-
racy is better when L is relatively small. Actually,
there is no obvious distinction of the performance
when L is less than 600. However, when L equals
1,000, the translation accuracy is inferior to other
settings. The main reason is that parameters in
the neural networks are too many to be effectively
trained. As we know when L=1000, there are a
total of 100, 000 × 1, 000 parameters between the
linear and non-linear layers in the network. Lim-
ited training data prevents the model from getting
close to the global optimum. Therefore, the model
is likely to fall in local optima and lead to unac-
ceptable representations.
</bodyText>
<subsectionHeader confidence="0.999523">
4.4 Effect of topic related features
</subsectionHeader>
<bodyText confidence="0.999968114285714">
We evaluate the performance of adding new topic-
related features to the log-linear model and com-
pare the translation accuracy with the method in
(Xiao et al., 2012). To make different methods
comparable, we set the dimension of topic rep-
resentation as 100 for all settings. This takes 10
hours in pre-training phase and 22 hours in fine-
tuning phase. Table 2 shows how the accuracy is
improved with more features added. The results
confirm that topic information is indispensable for
SMT since both (Xiao et al., 2012) and our neural
network based method significantly outperforms
the baseline system. Our method improves 0.86
BLEU points at most and 0.76 BLEU points on
average over the baseline. We observe that source-
side similarity is more effective than target-side
similarity, but their contributions are cumulative.
This proves that bilingually induced topic repre-
sentation with neural network helps the SMT sys-
tem disambiguate translation candidates. Further-
more, rule sensitivity features improve SMT per-
formance compared with only using similarity fea-
tures. Because topic-specific rules usually have a
larger sensitivity score, they can beat general rules
when they obtain the same similarity score against
the input sentence. Finally, when all new fea-
tures are integrated, the performance is the best,
preforming substantially better than (Xiao et al.,
2012) with 0.39 BLEU points on average.
It is worth mentioning that the performance
of (Xiao et al., 2012) is similar to the settings
with N=1 and L=100 in Figure 3. This is not
simply coincidence since we can interpret their
approach as a special case in our neural net-
work method: when a parallel sentence pair has
</bodyText>
<page confidence="0.995684">
139
</page>
<table confidence="0.997222222222222">
Settings NIST 2004 NIST 2005 NIST 2006 NIST 2008 Average
Baseline 42.25 41.21 38.05 31.16 38.17
(Xiao et al., 2012) 42.58 41.61 38.39 31.58 38.54
Sim(Src) 42.51 41.55 38.53 31.57 38.54
Sim(Trg) 42.43 41.48 38.4 31.49 38.45
Sim(Src+Trg) 42.7 41.66 38.66 31.66 38.67
Sim(Src+Trg)+Sen(Src) 42.77 41.81 38.85 31.73 38.79
Sim(Src+Trg)+Sen(Trg) 42.85 41.79 38.76 31.7 38.78
Sim(Src+Trg)+Sen(Src+Trg) 42.95 41.97 38.91 31.88 38.93
</table>
<tableCaption confidence="0.995531">
Table 2: Effectiveness of different features in BLEU% (p &lt; 0.05), with N=10 and L=100. “Sim”
</tableCaption>
<bodyText confidence="0.935518333333333">
denotes the rule similarity feature and “Sen” denotes rule sensitivity feature. “Src” and “Trg” means
utilizing source-side/target-side rule topic vectors to calculate similarity or sensitivity, respectively. The
“Average” setting is the averaged result of four datasets.
document-level information, that document will
be retrieved for training; otherwise, the most rel-
evant document will be retrieved from the mono-
lingual data. Therefore, our method can be viewed
as a more general framework than previous LDA-
based approaches.
</bodyText>
<subsectionHeader confidence="0.951815">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999978173913043">
In this section, we give a case study to explain
why our method works. An example of transla-
tion rule disambiguation for a sentence from the
NIST 2005 dataset is shown in Figure 4. We find
that the topic of this sentence is about “rescue af-
ter a natural disaster”. Under this topic, the Chi-
nese rule “&amp;送 X” should be translated to “de-
liver X” or “distribute X”. However, the baseline
system prefers “send X” rather than those two can-
didates. Although the translation probability of
“send X” is much higher, it is inappropriate in this
context since it is usually used in IT texts. For
example, (&amp;送邮件, send emails), (&amp;送信息,
send messages) and (&amp;送数据, send data). In
contrast, with our neural network based approach,
the learned topic distributions of “deliver X” or
“distribute X” are more similar with the input sen-
tence than “send X”, which is shown in Figure 4.
The similarity scores indicate that “deliver X” and
“distribute X” are more appropriate to translate the
sentence. Therefore, adding topic-related features
is able to keep the topic consistency and substan-
tially improve the translation accuracy.
</bodyText>
<sectionHeader confidence="0.999877" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999979069767442">
Topic modeling was first leveraged to improve
SMT performance in (Zhao and Xing, 2006; Zhao
and Xing, 2007). They proposed a bilingual
topical admixture approach for word alignment
and assumed that each word-pair follows a topic-
specific model. They reported extensive empir-
ical analysis and improved word alignment ac-
curacy as well as translation quality. Follow-
ing this work, (Xiao et al., 2012) extended topic-
specific lexicon translation models to hierarchical
phrase-based translation models, where the topic
information of synchronous rules was directly in-
ferred with the help of document-level informa-
tion. Experiments show that their approach not
only achieved better translation performance but
also provided a faster decoding speed compared
with previous lexicon-based LDA methods.
Another direction of approaches leveraged topic
modeling techniques for domain adaptation. Tam
et al. (2007) used bilingual LSA to learn latent
topic distributions across different languages and
enforce one-to-one topic correspondence during
model training. They incorporated the bilingual
topic information into language model adaptation
and lexicon translation model adaptation, achiev-
ing significant improvements in the large-scale
evaluation. (Su et al., 2012) investigated the rela-
tionship between out-of-domain bilingual data and
in-domain monolingual data via topic mapping
using HTMM methods. They estimated phrase-
topic distributions in translation model adaptation
and generated better translation quality. Recently,
Chen et al. (2013) proposed using vector space
model for adaptation where genre resemblance is
leveraged to improve translation accuracy. We
also investigated multi-domain adaptation where
explicit topic information is used to train domain
specific models (Cui et al., 2013).
Generally, most previous research has leveraged
conventional topic modeling techniques such as
LDA or HTMM. In our work, a novel neural net-
work based approach is proposed to infer topic
representations for parallel data. The advantage of
</bodyText>
<page confidence="0.996428">
140
</page>
<figureCaption confidence="0.9908225">
Figure 4: An example from the NIST 2005 dataset. We illustrate the normalized topic representations of
the source sentence and three ambiguous synchronous rules. Details are explained in Section 4.5.
</figureCaption>
<bodyText confidence="0.866840785714286">
our method is that it is applicable to both sentence-
level and document-level SMT, since we do not
place any restrictions on the input. In addition, our
method directly maximizes the similarity between
parallel sentence pairs, which is ideal for SMT de-
coding. Compared to document-level topic mod-
eling which uses the topic of a document for all
sentences within the document (Xiao et al., 2012),
our contributions are:
• We proposed a more general approach to
leveraging topic information for SMT by us-
ing IR methods to get a collection of related
documents, regardless of whether or not doc-
ument boundaries are explicitly given.
</bodyText>
<listItem confidence="0.968127875">
• We used neural networks to learn topic repre-
sentations more accurately, with more practi-
cable and scalable modeling techniques.
• We directly optimized bilingual topic simi-
larity in the deep learning framework with
the help of sentence-level parallel data, so
that the learned representation could be easily
used in SMT decoding procedure.
</listItem>
<sectionHeader confidence="0.985014" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99997672">
In this paper, we propose a neural network based
approach to learning bilingual topic representa-
tion for SMT. We enrich contexts of parallel sen-
tence pairs with topic related monolingual data
and obtain a set of documents to represent sen-
tences. These documents are converted to a bag-
of-words input and fed into neural networks. The
learned low-dimensional vector is used to obtain
the topic representations of synchronous rules. In
SMT decoding, appropriate rules are selected to
best match source texts according to their similar-
ity in the topic space. Experimental results show
that our approach is promising for SMT systems to
learn a better translation model. It is a significant
improvement over the state-of-the-art Hiero sys-
tem, as well as a conventional LDA-based method.
In the future research, we will extend our neural
network methods to address document-level trans-
lation, where topic transition between sentences is
a crucial problem to be solved. Since the transla-
tion of the current sentence is usually influenced
by the topic of previous sentences, we plan to
leverage recurrent neural networks to model this
phenomenon, where the history translation infor-
mation is naturally combined in the model.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999099833333333">
We are grateful to the anonymous reviewers for
their insightful comments. We also thank Fei
Huang (BBN), Nan Yang, Yajuan Duan, Hong Sun
and Duyu Tang for the helpful discussions. This
work is supported by the National Natural Science
Foundation of China (Granted No. 61272384)
</bodyText>
<page confidence="0.99782">
141
</page>
<sectionHeader confidence="0.983369" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999830212962963">
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise train-
ing of deep networks. In B. Sch¨olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153–160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Found. Trends Mach. Learn., 2(1):1–127, Jan-
uary.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05),
pages 387–394, Ann Arbor, Michigan, June. Asso-
ciation for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. Proceedings of Machine Trans-
lation Summit XI, pages 73–80.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1285–
1293, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,
Mu Li, and Ming Zhou. 2013. Multi-domain adap-
tation for SMT using multi-task learning. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1055–
1065, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech and Language
Processing, 20(1):30–42, January.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53–61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&amp;CP Vol-
ume, volume 15, pages 315–323.
Amit Gruber, Michal Rosen-zvi, and Yair Weiss. 2007.
Hidden topic markov models. In In Proceedings of
Artificial Intelligence and Statistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321–328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527–1554, July.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In P. Bartlett, F.C.N. Pereira,
C.J.C. Burges, L. Bottou, and K.Q. Weinberger, ed-
itors, Advances in Neural Information Processing
Systems 25, pages 1106–1114.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y. Ng. 2006. Efficient sparse coding algo-
rithms. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 801–808. MIT Press, Cambridge,
MA.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
89–97, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
</reference>
<page confidence="0.978659">
142
</page>
<reference confidence="0.99941226">
Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based transla-
tion. In Proceedings of ACL-08: HLT, pages 1003–
1011, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696–699. MIT
Press, Cambridge, MA, USA.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 354–362, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011a. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 151–161, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 459–468, Jeju Island, Korea,
July. Association for Computational Linguistics.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187–
207, December.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th International
Conference on Machine Learning, ICML ’08, pages
1096–1103, New York, NY, USA. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res., 11:3371–
3408, December.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for
hierarchical phrase-based translation. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 750–758, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
AAAI.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315–
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969–976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Bing Zhao and Eric P. Xing. 2007. Hm-bitam: Bilin-
gual topic exploration, word alignment, and trans-
lation. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1689–1696. MIT
Press, Cambridge, MA.
</reference>
<page confidence="0.999132">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.100261">
<title confidence="0.97252">Topic Representation for SMT with Neural</title>
<note confidence="0.382178">of Computer Science and Technology, Harbin Institute of Technology, Harbin, P.R. leicui@hit.edu.cn, Research, Beijing, P.R. Jiao Tong University, Shanghai, P.R.</note>
<email confidence="0.996086">simoncqm@gmail.com</email>
<abstract confidence="0.987709380952381">Statistical Machine Translation (SMT) usually utilizes contextual information to disambiguate translation candidates. However, it is often limited to contexts within sentence boundaries, hence broader topical information cannot be leveraged. In this paper, we propose a novel approach to learning topic representation for parallel data using a neural network architecture, where abundant topical contexts are embedded via topic relevant monolingual data. By associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Pascal Lamblin</author>
<author>Dan Popovici</author>
<author>Hugo Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>153--160</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7299" citStr="Bengio et al., 2006" startWordPosition="1071" endWordPosition="1074"> significantly improves translation accuracy over a state-of-the-art baseline. 2 Background: Deep Learning Deep learning is an active topic in recent years which has triumphed in many machine learning research areas. This technique began raising public awareness in the mid-2000s after researchers showed how a multi-layer feed-forward neural network can be effectively trained. The training procedure often involves two phases: a layerwise unsupervised pre-training phase and a supervised fine-tuning phase. For pre-training, Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoding (Bengio et al., 2006) and sparse coding (Lee et al., 2006) are most frequently used. Unsupervised pre-training trains the network one layer at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applie</context>
<context position="10797" citStr="Bengio et al., 2006" startWordPosition="1599" endWordPosition="1602">edure (Section 3.3). 3.1 Pre-training using denoising auto-encoder In the pre-training phase, we leverage neural network structures to transform high-dimensional sparse vectors to low-dimensional dense vectors. The topic similarity is calculated on top of the learned dense vectors. This dense representation should preserve the information from the bag-of1We use f and e to denote the n-of-V vector converted from the retrieved documents. words input, meanwhile alleviate data sparse problem. Therefore, we use a specially designed mechanism called auto-encoder to solve this problem. Auto-encoder (Bengio et al., 2006) is one of the basic building blocks of deep learning. Assuming that the input is a n-of-V binary vector x representing the bag-of-words (V is the vocabulary size), an auto-encoder consists of an encoding process g(x) and a decoding process h(g(x)). The objective of the auto-encoder is to minimize the reconstruction error L(h(g(x)), x). Our goal is to learn a low-dimensional vector which can preserve information from the original n-of-V vector. One problem with auto-encoder is that it treats all words in the same way, making no distinguishment between function words and content words. The repr</context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2006</marker>
<rawString>Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2006. Greedy layer-wise training of deep networks. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 153–160. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for ai.</title>
<date>2009</date>
<journal>Found. Trends Mach. Learn.,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7528" citStr="Bengio, 2009" startWordPosition="1113" endWordPosition="1114">n raising public awareness in the mid-2000s after researchers showed how a multi-layer feed-forward neural network can be effectively trained. The training procedure often involves two phases: a layerwise unsupervised pre-training phase and a supervised fine-tuning phase. For pre-training, Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoding (Bengio et al., 2006) and sparse coding (Lee et al., 2006) are most frequently used. Unsupervised pre-training trains the network one layer at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as part-ofspeech tagging, chunking, named entity recognition, semantic role labeling (Collobert et al., 2011), parsing (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), etc. Most </context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for ai. Found. Trends Mach. Learn., 2(1):1–127, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="3611" citStr="Blei et al., 2003" startWordPosition="519" endWordPosition="522"> synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for C</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Word sense disambiguation vs. statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>387--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1589" citStr="Carpuat and Wu, 2005" startWordPosition="204" endWordPosition="207">ation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗This work was done while the first and fourth authors were visiting Microsoft Research. sentence contexts which are insufficient in exploring broader information. For example, the word driver often means</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Marine Carpuat and Dekai Wu. 2005. Word sense disambiguation vs. statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 387–394, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Contextdependent phrasal translation lexicons for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of Machine Translation Summit XI,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="1612" citStr="Carpuat and Wu, 2007" startWordPosition="208" endWordPosition="211">rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗This work was done while the first and fourth authors were visiting Microsoft Research. sentence contexts which are insufficient in exploring broader information. For example, the word driver often means “the operator of a mot</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Contextdependent phrasal translation lexicons for statistical machine translation. Proceedings of Machine Translation Summit XI, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Vector space model for adaptation in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1285--1293</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32290" citStr="Chen et al. (2013)" startWordPosition="5110" endWordPosition="5113">A to learn latent topic distributions across different languages and enforce one-to-one topic correspondence during model training. They incorporated the bilingual topic information into language model adaptation and lexicon translation model adaptation, achieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Generally, most previous research has leveraged conventional topic modeling techniques such as LDA or HTMM. In our work, a novel neural network based approach is proposed to infer topic representations for parallel data. The advantage of 140 Figure 4: An example from the NIST 2005 dataset. We illustrate the normalized topic representations </context>
</contexts>
<marker>Chen, Kuhn, Foster, 2013</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2013. Vector space model for adaptation in statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1285– 1293, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="18139" citStr="Chiang, 2007" startWordPosition="2830" endWordPosition="2831">ne-tune the neural network parameters W and b in Equation (1). The learned neural networks are used to obtain sentence topic representations, which will be further leveraged to infer topic representations of bilingual translation rules. 3.3 Integration into SMT decoding We incorporate the learned topic similarity scores into the standard log-linear framework for SMT. When a synchronous rule (α, γ) is extracted from a sentence pair (f, e), a triple instance Z = ((α, γ), (f, e), c) is collected for inferring the topic representation of (α, γ), where c is the count of rule occurrence. Following (Chiang, 2007), we give a count of one for each phrase pair occurrence and a fractional count for each hierarchical phrase pair. The topic representation of (α, γ) is then calculated as the weighted average: P ((α,γ),(f,e),c)ET Ic X zel P ((α,γ),(f,e),c)ET W where T denotes all instances for the rule (α, γ), zα and zγ are the source-side and target-side topic vectors respectively. By measuring the similarity between the source texts and bilingual translation rules, the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates. Therefore, it helps to trai</context>
<context position="22583" citStr="Chiang (2007)" startWordPosition="3569" endWordPosition="3570">g phase, all parallel data is fed into two neural networks respectively for DAE training, where network parameters W and b are randomly initialized. In the fine-tuning phase, for each parallel sentence pair, we randomly select other ten sentence pairs which satisfy the criterion as negative instances. These training instances are leveraged to optimize the similarity of two vectors. In SMT training, an in-house hierarchical phrase-based SMT decoder is implemented for our experiments. The CKY decoding algorithm is used and cube pruning is performed with the same default parameter settings as in Chiang (2007). The parallel data we use is released by LDC3. In total, the datasets contain nearly 1.1 million sentence pairs. Translation models are trained over the parallel data that is automatically word-aligned 2http://lucene.apache.org/ 3LDC2003E14, LDC2002E18, LDC2003E07, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09 using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8041" citStr="Collobert et al., 2011" startWordPosition="1189" endWordPosition="1192">at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as part-ofspeech tagging, chunking, named entity recognition, semantic role labeling (Collobert et al., 2011), parsing (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), etc. Most NLP research converts a high-dimensional and sparse binary representation into a low-dimensional and real-valued representation. This low-dimensional representation is usually learned from huge amount of monolingual texts in the pre-training phase, and then fine-tuned towards task-specific criterion. Inspired by previous successful research, we first learn sentence representations using topic-related monolingual texts in the pretraining phase, and then optimize the bilingual similarity by leveraging sentence</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Xilun Chen</author>
<author>Dongdong Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Multi-domain adaptation for SMT using multi-task learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1055--1065</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="32546" citStr="Cui et al., 2013" startWordPosition="5146" endWordPosition="5149">hieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Generally, most previous research has leveraged conventional topic modeling techniques such as LDA or HTMM. In our work, a novel neural network based approach is proposed to infer topic representations for parallel data. The advantage of 140 Figure 4: An example from the NIST 2005 dataset. We illustrate the normalized topic representations of the source sentence and three ambiguous synchronous rules. Details are explained in Section 4.5. our method is that it is applicable to both sentencelevel and document-level SMT, since we do not place any restrictions on the input. In addition, our meth</context>
</contexts>
<marker>Cui, Chen, Zhang, Liu, Li, Zhou, 2013</marker>
<rawString>Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu, Mu Li, and Ming Zhou. 2013. Multi-domain adaptation for SMT using multi-task learning. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1055– 1065, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.</title>
<date>2012</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="7845" citStr="Dahl et al., 2012" startWordPosition="1157" endWordPosition="1160">Machine (RBM) (Hinton et al., 2006), auto-encoding (Bengio et al., 2006) and sparse coding (Lee et al., 2006) are most frequently used. Unsupervised pre-training trains the network one layer at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as part-ofspeech tagging, chunking, named entity recognition, semantic role labeling (Collobert et al., 2011), parsing (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), etc. Most NLP research converts a high-dimensional and sparse binary representation into a low-dimensional and real-valued representation. This low-dimensional representation is usually learned from huge amount of monolingual texts in the pre-training phase, and then fine-tuned towards task-specific criterion. Inspired by pre</context>
</contexts>
<marker>Dahl, Yu, Deng, Acero, 2012</marker>
<rawString>George E. Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech and Language Processing, 20(1):30–42, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="21629" citStr="Duchi et al., 2011" startWordPosition="3414" endWordPosition="3417">arallel sentence pairs. The most relevant N documents are collected, where we experiment with N = {1, 5, 10, 20, 50}. Domain Chinese English Docs Words Docs Words News 5.7M 5.4B 9.9M 25.6B Weblog 2.1M 8B 1.2M 2.9B Total 7.8M 13.4B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. We implement a distributed framework to speed up the training process of neural networks. The network is learned with mini-batch asynchronous gradient descent with the adaptive learning rate procedure called AdaGrad (Duchi et al., 2011). We use 32 model replicas in each iteration during the training. The model parameters are averaged after each iteration and sent to each replica for the next iteration. The vocabulary size for the input layer is 100,000, and we choose different lengths for the hidden layer as L = {100, 300, 600,1000} in the experiments. In the pre-training phase, all parallel data is fed into two neural networks respectively for DAE training, where network parameters W and b are randomly initialized. In the fine-tuning phase, for each parallel sentence pair, we randomly select other ten sentence pairs which s</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="23931" citStr="Foster et al. (2006)" startWordPosition="3773" endWordPosition="3776">lopment data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. 4.3 </context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 53–61, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Deep sparse rectifier networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume,</booktitle>
<volume>15</volume>
<pages>315--323</pages>
<contexts>
<context position="13591" citStr="Glorot et al., 2011" startWordPosition="2070" endWordPosition="2073">put of the non-linear layer, b is a sim(f,e) f e Neural Network Training Data Preprocessing de df cos(Zf, ZJ Ze = g(e) Zf = g(f) English document collection Chinese document collection Parallel sentence e f IR IR 135 h(gXt) g (Xt) Xt L(h g Xt , X) X Figure 2: Denoising auto-encoder with a bag-ofwords input. L-length bias vector. f(·) is a non-linear function, where common choices include sigmoid function, hyperbolic function, “hard” hyperbolic function, rectifier function, etc. In this work, we use the rectifier function as our non-linear function due to its efficiency and better performance (Glorot et al., 2011): � x if x &gt; 0 rec(x) = (2) 0 otherwise The decoding process consists of a linear layer and a non-linear layer with similar network structures, but different parameters. It transforms the L-dimensional vector g(˜x) to a V-dimensional vector h(g(˜x)). To minimize reconstruction error with respect to ˜x, we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input: L(h(g(˜x)),x) = Ih(g(˜x)) − xI2 (3) Multi-layer neural networks are trained with the standard back-propagation algorithm (Rumelhart et al., 1988). The gradient of the loss function</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume, volume 15, pages 315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic markov models. In</title>
<date>2007</date>
<booktitle>In Proceedings of Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="3669" citStr="Gruber et al., 2007" startWordPosition="529" endWordPosition="532">el translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics sentence level, it is quite diffi</context>
</contexts>
<marker>Gruber, Rosen-zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-zvi, and Yair Weiss. 2007. Hidden topic markov models. In In Proceedings of Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>321--328</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="1814" citStr="He et al., 2008" startWordPosition="240" endWordPosition="243">e-to-English translation task compared to a state-of-the-art baseline. 1 Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗This work was done while the first and fourth authors were visiting Microsoft Research. sentence contexts which are insufficient in exploring broader information. For example, the word driver often means “the operator of a motor vehicle” in common texts. But in the sentence “Finally, we write the user response to the buffer, i.e., pass it to our driver”, we understand that driver means “computer program”. In this case, peopl</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 321–328, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<journal>Neural Comput.,</journal>
<volume>18</volume>
<issue>7</issue>
<contexts>
<context position="7262" citStr="Hinton et al., 2006" startWordPosition="1066" endWordPosition="1069">al results demonstrate that our model significantly improves translation accuracy over a state-of-the-art baseline. 2 Background: Deep Learning Deep learning is an active topic in recent years which has triumphed in many machine learning research areas. This technique began raising public awareness in the mid-2000s after researchers showed how a multi-layer feed-forward neural network can be effectively trained. The training procedure often involves two phases: a layerwise unsupervised pre-training phase and a supervised fine-tuning phase. For pre-training, Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoding (Bengio et al., 2006) and sparse coding (Lee et al., 2006) are most frequently used. Unsupervised pre-training trains the network one layer at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learn</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural Comput., 18(7):1527–1554, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="23193" citStr="Kneser and Ney, 1995" startWordPosition="3649" endWordPosition="3652">iang (2007). The parallel data we use is released by LDC3. In total, the datasets contain nearly 1.1 million sentence pairs. Translation models are trained over the parallel data that is automatically word-aligned 2http://lucene.apache.org/ 3LDC2003E14, LDC2002E18, LDC2003E07, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09 using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appea</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="23679" citStr="Koehn, 2004" startWordPosition="3731" endWordPosition="3732">language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our paralle</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoff Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1106--1114</pages>
<editor>In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="7774" citStr="Krizhevsky et al., 2012" startWordPosition="1146" endWordPosition="1149">e and a supervised fine-tuning phase. For pre-training, Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoding (Bengio et al., 2006) and sparse coding (Lee et al., 2006) are most frequently used. Unsupervised pre-training trains the network one layer at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as part-ofspeech tagging, chunking, named entity recognition, semantic role labeling (Collobert et al., 2011), parsing (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), etc. Most NLP research converts a high-dimensional and sparse binary representation into a low-dimensional and real-valued representation. This low-dimensional representation is usually learned from huge amount of monolingual texts in the pre-training phas</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. 2012. Imagenet classification with deep convolutional neural networks. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1106–1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglak Lee</author>
<author>Alexis Battle</author>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
</authors>
<title>Efficient sparse coding algorithms.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>801--808</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7336" citStr="Lee et al., 2006" startWordPosition="1078" endWordPosition="1081">racy over a state-of-the-art baseline. 2 Background: Deep Learning Deep learning is an active topic in recent years which has triumphed in many machine learning research areas. This technique began raising public awareness in the mid-2000s after researchers showed how a multi-layer feed-forward neural network can be effectively trained. The training procedure often involves two phases: a layerwise unsupervised pre-training phase and a supervised fine-tuning phase. For pre-training, Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoding (Bengio et al., 2006) and sparse coding (Lee et al., 2006) are most frequently used. Unsupervised pre-training trains the network one layer at a time and helps guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as p</context>
</contexts>
<marker>Lee, Battle, Raina, Ng, 2006</marker>
<rawString>Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. 2006. Efficient sparse coding algorithms. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 801–808. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based rule selection model for syntax-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>89--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1832" citStr="Liu et al., 2008" startWordPosition="244" endWordPosition="247">slation task compared to a state-of-the-art baseline. 1 Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗This work was done while the first and fourth authors were visiting Microsoft Research. sentence contexts which are insufficient in exploring broader information. For example, the word driver often means “the operator of a motor vehicle” in common texts. But in the sentence “Finally, we write the user response to the buffer, i.e., pass it to our driver”, we understand that driver means “computer program”. In this case, people understand the m</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 2008. Maximum entropy based rule selection model for syntax-based statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Yuzhou Zhang</author>
<author>Edward Y Chang</author>
<author>Maosong Sun</author>
</authors>
<title>Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent</journal>
<booktitle>Systems and Technology, special issue on Large Scale Machine Learning. Software available</booktitle>
<contexts>
<context position="24454" citStr="Liu et al., 2011" startWordPosition="3864" endWordPosition="3867">ed because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. 4.3 Effect of retrieved documents and length of hidden layers We illustrate the relationship among translation accuracy (BLEU), the number of retrieved documents (N) and the length of hidden layers (L) on different testing datasets. The results are shown in Figure 3. The best translation accuracy is achieved when N=10 for most settings. This confirms that enriching the source text with topic-related documents is very useful in determining topic representations, thereby help to guide the synchronous rule selection. However</context>
</contexts>
<marker>Liu, Zhang, Chang, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and Maosong Sun. 2011. Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing. ACM Transactions on Intelligent Systems and Technology, special issue on Large Scale Machine Learning. Software available at http://code. google.com/p/plda.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1857" citStr="Marton and Resnik, 2008" startWordPosition="248" endWordPosition="251">red to a state-of-the-art baseline. 1 Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗This work was done while the first and fourth authors were visiting Microsoft Research. sentence contexts which are insufficient in exploring broader information. For example, the word driver often means “the operator of a motor vehicle” in common texts. But in the sentence “Finally, we write the user response to the buffer, i.e., pass it to our driver”, we understand that driver means “computer program”. In this case, people understand the meaning because of the IT </context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-08: HLT, pages 1003– 1011, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="23581" citStr="Och, 2003" startWordPosition="3716" endWordPosition="3717">tions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="23501" citStr="Papineni et al., 2002" startWordPosition="3700" endWordPosition="3703">05E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09 using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors,</title>
<date>1988</date>
<pages>696--699</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="14156" citStr="Rumelhart et al., 1988" startWordPosition="2159" endWordPosition="2162">ts efficiency and better performance (Glorot et al., 2011): � x if x &gt; 0 rec(x) = (2) 0 otherwise The decoding process consists of a linear layer and a non-linear layer with similar network structures, but different parameters. It transforms the L-dimensional vector g(˜x) to a V-dimensional vector h(g(˜x)). To minimize reconstruction error with respect to ˜x, we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input: L(h(g(˜x)),x) = Ih(g(˜x)) − xI2 (3) Multi-layer neural networks are trained with the standard back-propagation algorithm (Rumelhart et al., 1988). The gradient of the loss function is calculated and back-propagated to the previous layer to update its parameters. Training neural networks involves many factors such as the learning rate and the length of hidden layers. We will discuss the optimization of these parameters in Section 4. 3.2 Fine-tuning with parallel data In the fine-tuning phase, we stack another layer on top of the two low-dimensional vectors to maximize the similarity between source and target languages. The similarity scores are integrated into the standard log-linear model for making translation decisions. Since the vec</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1988. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696–699. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>354--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="16092" citStr="Smith and Eisner, 2005" startWordPosition="2484" endWordPosition="2487">spectively, as zf = g(f) and ze = g(e) in Figure 1. We then take two vectors as the input to calculate their similarity. Consequently, the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data. The similarity score of the representation pair (zf, ze) is defined as the cosine similarity of the two vectors: sim(f, e) = COS(zf, ze) zf · ze (4) IzfIIzeI Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence. Inspired by the contrastive estimation method (Smith and Eisner, 2005), for each parallel sentence pair (f, e) as a positive instance, we select another sentence pair (f&apos;, e&apos;) from the training data and treat (f, e&apos;) as a negative instance. To make the similarity of the positive instance larger than the negative instance by some margin q, we utilize the following pairwise ranking loss: L(f, e) = max{0, q − sim(f, e) + sim(f, e&apos;)} (5) where q =12 − sim(f, f&apos;). The rationale behind this criterion is, the smaller sim(f, f&apos;) is, the more we should penalize negative instances. To effectively train the model in this task, negative instances must be selected carefully.</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 354–362, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="8071" citStr="Socher et al., 2011" startWordPosition="1194" endWordPosition="1197">meters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as part-ofspeech tagging, chunking, named entity recognition, semantic role labeling (Collobert et al., 2011), parsing (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), etc. Most NLP research converts a high-dimensional and sparse binary representation into a low-dimensional and real-valued representation. This low-dimensional representation is usually learned from huge amount of monolingual texts in the pre-training phase, and then fine-tuned towards task-specific criterion. Inspired by previous successful research, we first learn sentence representations using topic-related monolingual texts in the pretraining phase, and then optimize the bilingual similarity by leveraging sentence-level parallel data in the fi</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the 26th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="8071" citStr="Socher et al., 2011" startWordPosition="1194" endWordPosition="1197">meters of the layer towards better regions in parameter space (Bengio, 2009). Followed by finetuning in this parameter region, deep learning is able to achieve state-of-the-art performance in various research areas, including breakthrough results on the ImageNet dataset for objective recognition (Krizhevsky et al., 2012), significant error reduction in speech recognition (Dahl et al., 2012), etc. Deep learning has also been successfully applied in a variety of NLP tasks such as part-ofspeech tagging, chunking, named entity recognition, semantic role labeling (Collobert et al., 2011), parsing (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), etc. Most NLP research converts a high-dimensional and sparse binary representation into a low-dimensional and real-valued representation. This low-dimensional representation is usually learned from huge amount of monolingual texts in the pre-training phase, and then fine-tuned towards task-specific criterion. Inspired by previous successful research, we first learn sentence representations using topic-related monolingual texts in the pretraining phase, and then optimize the bilingual similarity by leveraging sentence-level parallel data in the fi</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Yidong Chen</author>
<author>Xiaodong Shi</author>
<author>Huailin Dong</author>
<author>Qun Liu</author>
</authors>
<title>Translation model adaptation for statistical machine translation with monolingual topic information.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>459--468</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3225" citStr="Su et al., 2012" startWordPosition="460" endWordPosition="463"> to learn smarter translation models and achieve better translation performance. Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of pa</context>
<context position="32010" citStr="Su et al., 2012" startWordPosition="5073" endWordPosition="5076">r approach not only achieved better translation performance but also provided a faster decoding speed compared with previous lexicon-based LDA methods. Another direction of approaches leveraged topic modeling techniques for domain adaptation. Tam et al. (2007) used bilingual LSA to learn latent topic distributions across different languages and enforce one-to-one topic correspondence during model training. They incorporated the bilingual topic information into language model adaptation and lexicon translation model adaptation, achieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Generally, most previous research has leveraged conventional t</context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, and Qun Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 459–468, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual lsa-based adaptation for statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>207</pages>
<contexts>
<context position="3207" citStr="Tam et al., 2007" startWordPosition="456" endWordPosition="459"> topic information to learn smarter translation models and achieve better translation performance. Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is conside</context>
<context position="31654" citStr="Tam et al. (2007)" startWordPosition="5027" endWordPosition="5030">lysis and improved word alignment accuracy as well as translation quality. Following this work, (Xiao et al., 2012) extended topicspecific lexicon translation models to hierarchical phrase-based translation models, where the topic information of synchronous rules was directly inferred with the help of document-level information. Experiments show that their approach not only achieved better translation performance but also provided a faster decoding speed compared with previous lexicon-based LDA methods. Another direction of approaches leveraged topic modeling techniques for domain adaptation. Tam et al. (2007) used bilingual LSA to learn latent topic distributions across different languages and enforce one-to-one topic correspondence during model training. They incorporated the bilingual topic information into language model adaptation and lexicon translation model adaptation, achieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation q</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007. Bilingual lsa-based adaptation for statistical machine translation. Machine Translation, 21(4):187– 207, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>1096--1103</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11553" citStr="Vincent et al. (2008)" startWordPosition="1723" endWordPosition="1726">s (V is the vocabulary size), an auto-encoder consists of an encoding process g(x) and a decoding process h(g(x)). The objective of the auto-encoder is to minimize the reconstruction error L(h(g(x)), x). Our goal is to learn a low-dimensional vector which can preserve information from the original n-of-V vector. One problem with auto-encoder is that it treats all words in the same way, making no distinguishment between function words and content words. The representation learned by auto-encoders tends to be influenced by the function words, thereby it is not robust. To alleviate this problem, Vincent et al. (2008) proposed the Denoising Auto-Encoder (DAE), which aims to reconstruct a clean, “repaired” input from a corrupted, partially destroyed vector. This is done by corrupting the initial input x to get a partially destroyed version ˜x. DAE is capable of capturing the global structure of the input while ignoring the noise. In our task, for each sentence, we treat the retrieved N relevant documents as a single large document and convert it to a bag-of-words vector x in Figure 2. With DAE, the input x is manually corrupted by applying masking noise (randomly mask 1 to 0) and getting ˜x. Denoising train</context>
</contexts>
<marker>Vincent, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 1096–1103, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Isabelle Lajoie</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.</title>
<date>2010</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>11</volume>
<pages>3408</pages>
<contexts>
<context position="12220" citStr="Vincent et al., 2010" startWordPosition="1839" endWordPosition="1842">ich aims to reconstruct a clean, “repaired” input from a corrupted, partially destroyed vector. This is done by corrupting the initial input x to get a partially destroyed version ˜x. DAE is capable of capturing the global structure of the input while ignoring the noise. In our task, for each sentence, we treat the retrieved N relevant documents as a single large document and convert it to a bag-of-words vector x in Figure 2. With DAE, the input x is manually corrupted by applying masking noise (randomly mask 1 to 0) and getting ˜x. Denoising training is considered as “filling in the blanks” (Vincent et al., 2010), which means the masking components can be recovered from the non-corrupted components. For example, in IT related texts, if the word driver is masked, it should be predicted through hidden units in neural networks by active signals such as “buffer”, “user response”, etc. In our case, the encoding process transforms the corrupted input x˜ into g(˜x) with two layers: a linear layer connected with a non-linear layer. Assuming that the dimension of the g(˜x) is L, the linear layer forms a L x V matrix W which projects the n-of-V vector to a L-dimensional hidden layer. After the bag-of-words inpu</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371– 3408, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A topic similarity model for hierarchical phrase-based translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>750--758</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3031" citStr="Xiao et al., 2012" startWordPosition="430" endWordPosition="433">le understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge. Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance. Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Mar</context>
<context position="24145" citStr="Xiao et al. (2012)" startWordPosition="3807" endWordPosition="3810"> are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. 4.3 Effect of retrieved documents and length of hidden layers We illustrate the relationship among translation accuracy (BLEU), the number of retrieved documents (N) and the length of hidden layers (L) on different tes</context>
<context position="27053" citStr="Xiao et al., 2012" startWordPosition="4307" endWordPosition="4310">ettings. The main reason is that parameters in the neural networks are too many to be effectively trained. As we know when L=1000, there are a total of 100, 000 × 1, 000 parameters between the linear and non-linear layers in the network. Limited training data prevents the model from getting close to the global optimum. Therefore, the model is likely to fall in local optima and lead to unacceptable representations. 4.4 Effect of topic related features We evaluate the performance of adding new topicrelated features to the log-linear model and compare the translation accuracy with the method in (Xiao et al., 2012). To make different methods comparable, we set the dimension of topic representation as 100 for all settings. This takes 10 hours in pre-training phase and 22 hours in finetuning phase. Table 2 shows how the accuracy is improved with more features added. The results confirm that topic information is indispensable for SMT since both (Xiao et al., 2012) and our neural network based method significantly outperforms the baseline system. Our method improves 0.86 BLEU points at most and 0.76 BLEU points on average over the baseline. We observe that sourceside similarity is more effective than target</context>
<context position="28358" citStr="Xiao et al., 2012" startWordPosition="4511" endWordPosition="4514">induced topic representation with neural network helps the SMT system disambiguate translation candidates. Furthermore, rule sensitivity features improve SMT performance compared with only using similarity features. Because topic-specific rules usually have a larger sensitivity score, they can beat general rules when they obtain the same similarity score against the input sentence. Finally, when all new features are integrated, the performance is the best, preforming substantially better than (Xiao et al., 2012) with 0.39 BLEU points on average. It is worth mentioning that the performance of (Xiao et al., 2012) is similar to the settings with N=1 and L=100 in Figure 3. This is not simply coincidence since we can interpret their approach as a special case in our neural network method: when a parallel sentence pair has 139 Settings NIST 2004 NIST 2005 NIST 2006 NIST 2008 Average Baseline 42.25 41.21 38.05 31.16 38.17 (Xiao et al., 2012) 42.58 41.61 38.39 31.58 38.54 Sim(Src) 42.51 41.55 38.53 31.57 38.54 Sim(Trg) 42.43 41.48 38.4 31.49 38.45 Sim(Src+Trg) 42.7 41.66 38.66 31.66 38.67 Sim(Src+Trg)+Sen(Src) 42.77 41.81 38.85 31.73 38.79 Sim(Src+Trg)+Sen(Trg) 42.85 41.79 38.76 31.7 38.78 Sim(Src+Trg)+Sen(</context>
<context position="31152" citStr="Xiao et al., 2012" startWordPosition="4959" endWordPosition="4962">liver X” and “distribute X” are more appropriate to translate the sentence. Therefore, adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy. 5 Related Work Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topicspecific model. They reported extensive empirical analysis and improved word alignment accuracy as well as translation quality. Following this work, (Xiao et al., 2012) extended topicspecific lexicon translation models to hierarchical phrase-based translation models, where the topic information of synchronous rules was directly inferred with the help of document-level information. Experiments show that their approach not only achieved better translation performance but also provided a faster decoding speed compared with previous lexicon-based LDA methods. Another direction of approaches leveraged topic modeling techniques for domain adaptation. Tam et al. (2007) used bilingual LSA to learn latent topic distributions across different languages and enforce one</context>
<context position="33383" citStr="Xiao et al., 2012" startWordPosition="5280" endWordPosition="5283">ta. The advantage of 140 Figure 4: An example from the NIST 2005 dataset. We illustrate the normalized topic representations of the source sentence and three ambiguous synchronous rules. Details are explained in Section 4.5. our method is that it is applicable to both sentencelevel and document-level SMT, since we do not place any restrictions on the input. In addition, our method directly maximizes the similarity between parallel sentence pairs, which is ideal for SMT decoding. Compared to document-level topic modeling which uses the topic of a document for all sentences within the document (Xiao et al., 2012), our contributions are: • We proposed a more general approach to leveraging topic information for SMT by using IR methods to get a collection of related documents, regardless of whether or not document boundaries are explicitly given. • We used neural networks to learn topic representations more accurately, with more practicable and scalable modeling techniques. • We directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel data, so that the learned representation could be easily used in SMT decoding procedure. 6 Conclusion and Futur</context>
</contexts>
<marker>Xiao, Xiong, Zhang, Liu, Lin, 2012</marker>
<rawString>Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and Shouxun Lin. 2012. A topic similarity model for hierarchical phrase-based translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 750–758, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
</authors>
<title>A topic-based coherence model for statistical machine translation.</title>
<date>2013</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="3108" citStr="Xiong and Zhang, 2013" startWordPosition="440" endWordPosition="443">yond sentence-level analysis and requires more relevant knowledge. Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance. Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the inp</context>
</contexts>
<marker>Xiong, Zhang, 2013</marker>
<rawString>Deyi Xiong and Min Zhang. 2013. A topic-based coherence model for statistical machine translation. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A syntax-driven bracketing model for phrasebased translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>315--323</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="1878" citStr="Xiong et al., 2009" startWordPosition="252" endWordPosition="255"> baseline. 1 Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗This work was done while the first and fourth authors were visiting Microsoft Research. sentence contexts which are insufficient in exploring broader information. For example, the word driver often means “the operator of a motor vehicle” in common texts. But in the sentence “Finally, we write the user response to the buffer, i.e., pass it to our driver”, we understand that driver means “computer program”. In this case, people understand the meaning because of the IT topical context which</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2009</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A syntax-driven bracketing model for phrasebased translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 315– 323, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Bitam: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>969--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2942" citStr="Zhao and Xing, 2006" startWordPosition="416" endWordPosition="419"> it to our driver”, we understand that driver means “computer program”. In this case, people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge. Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance. Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling t</context>
<context position="30841" citStr="Zhao and Xing, 2006" startWordPosition="4910" endWordPosition="4913">ple, (&amp;送邮件, send emails), (&amp;送信息, send messages) and (&amp;送数据, send data). In contrast, with our neural network based approach, the learned topic distributions of “deliver X” or “distribute X” are more similar with the input sentence than “send X”, which is shown in Figure 4. The similarity scores indicate that “deliver X” and “distribute X” are more appropriate to translate the sentence. Therefore, adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy. 5 Related Work Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topicspecific model. They reported extensive empirical analysis and improved word alignment accuracy as well as translation quality. Following this work, (Xiao et al., 2012) extended topicspecific lexicon translation models to hierarchical phrase-based translation models, where the topic information of synchronous rules was directly inferred with the help of document-level information. Experiments show that their approach not only achieved better translation</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual topic admixture models for word alignment. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Hm-bitam: Bilingual topic exploration, word alignment, and translation.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1689--1696</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2964" citStr="Zhao and Xing, 2007" startWordPosition="420" endWordPosition="423">e understand that driver means “computer program”. In this case, people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge. Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance. Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent</context>
<context position="30863" citStr="Zhao and Xing, 2007" startWordPosition="4914" endWordPosition="4917">ls), (&amp;送信息, send messages) and (&amp;送数据, send data). In contrast, with our neural network based approach, the learned topic distributions of “deliver X” or “distribute X” are more similar with the input sentence than “send X”, which is shown in Figure 4. The similarity scores indicate that “deliver X” and “distribute X” are more appropriate to translate the sentence. Therefore, adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy. 5 Related Work Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topicspecific model. They reported extensive empirical analysis and improved word alignment accuracy as well as translation quality. Following this work, (Xiao et al., 2012) extended topicspecific lexicon translation models to hierarchical phrase-based translation models, where the topic information of synchronous rules was directly inferred with the help of document-level information. Experiments show that their approach not only achieved better translation performance but also </context>
</contexts>
<marker>Zhao, Xing, 2007</marker>
<rawString>Bing Zhao and Eric P. Xing. 2007. Hm-bitam: Bilingual topic exploration, word alignment, and translation. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1689–1696. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>