<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994092">
Detecting Deceptive Groups Using Conversations and Network Analysis
</title>
<author confidence="0.999693">
Dian Yu1, Yulia Tyshchuk2, Heng Ji1, William Wallace2
</author>
<affiliation confidence="0.9979715">
1Computer Science Department, Rensselaer Polytechnic Institute
2Department of Industrial and Systems Engineering, Rensselaer Polytechnic Institute
</affiliation>
<email confidence="0.995274">
1,2lyud2,tyshcy,jih,wallawl@rpi.edu
</email>
<sectionHeader confidence="0.997341" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866590909091">
Deception detection has been formulated
as a supervised binary classification prob-
lem on single documents. However, in
daily life, millions of fraud cases involve
detailed conversations between deceivers
and victims. Deceivers may dynamically
adjust their deceptive statements accord-
ing to the reactions of victims. In addition,
people may form groups and collaborate
to deceive others. In this paper, we seek to
identify deceptive groups from their con-
versations. We propose a novel subgroup
detection method that combines linguis-
tic signals and signed network analysis for
dynamic clustering. A social-elimination
game called Killer Game is introduced as a
case study1. Experimental results demon-
strate that our approach significantly out-
performs human voting and state-of-the-
art subgroup detection methods at dynam-
ically differentiating the deceptive groups
from truth-tellers.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999742">
Deception generally entails messages and infor-
mation intentionally transmitted to create a false
conclusion (Buller et al., 1994). Deception detec-
tion is an important task for a wide range of ap-
plications including law enforcement, intelligence
gathering, and financial fraud. Most of the previ-
ous work (e.g., (Ott et al., 2011; Feng et al., 2012))
focused on content analysis of a single document
in isolation (e.g., a product review). The promot-
ers of a product may post fake complimentary re-
views, while their competitors may hire people to
write fake negative reviews (Ott et al., 2011).
</bodyText>
<footnote confidence="0.995298">
1The data set is publicly available for research purposes
at: http://nlp.cs.rpi.edu/data/killer.zip
</footnote>
<bodyText confidence="0.837629725">
However, when we want to detect deception
from text or voice conversations, the deception be-
havior may be affected by the following factors be-
yond textual statements.
1. Dynamic. Recent research in social science
suggests that deception communication is dy-
namic and involves interactions among peo-
ple (e.g., (Buller and Burgoon, 1996)). Addi-
tionally, the research postulates that human’s
capacity to learn by observation enables him
to acquire large, integrated units of behav-
ior by example (Bandura, 1971). Therefore,
a person’s behavior concerning deception or
truth-telling can change constantly, while he
learns from others’ statements during conver-
sations.
2. Global. People may form groups for purpose
of deception. Research in social psychology
has shown that an individual’s object-related
behavior may be affected by the attitudes of
other people due to group dynamics (Fried-
kin, 2010).
Recent studies typically have been conducted
over “static” written or oral deceptive statements.
There is no obligatory requirement for communi-
cation between the author and the readers of these
statements (Yancheva and Rudzicz, 2013). As a
result, a victim of deception tends to trust the sto-
ry mainly based on the statement he reads (Ott et
al., 2011). However, in daily life, millions of fraud
cases involve detailed conversations between de-
ceivers and victims. A deceiver may make a state-
ment, which is partially true in order to deceive
or mislead victims and adjust his deceptive strate-
gies based on the reactions of victims (Zhou et al.,
2004). Therefore, it is more challenging to identity
a deceiver in an interactive process of deception.
Most deception detection research addressed in-
dividual deceivers, but deceivers often act in pairs
or larger groups (Vrij et al., 2010). The interac-
</bodyText>
<page confidence="0.96767">
857
</page>
<note confidence="0.812992285714286">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 857–866,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Identify a player’s
attitude toward other
players based on
his statement during
</note>
<figure confidence="0.998511804347826">
each round
①
②
③
① ② ③
①
①
②
③
−1
1
1
−1
−1
1
−1
1
1
Clustering
③
②
Player Attitude Profile
(each round)
Subgroups
①
①
1
-1
③
1
-1
-1
-1
②
Partition
③
②
Signed Network
(each round)
Subgroups
Cluster
Ensembles
①
Subgroups
③
②
</figure>
<figureCaption confidence="0.99998">
Figure 1: Deceptive group detection for a single round.
</figureCaption>
<bodyText confidence="0.999905894736842">
tions within a deceptive group have been ignored.
For example, a product review from a deceiver
may be supported by his teammates so that his
deceptive comments can be read by more poten-
tial buyers. In this case, we can identify a decep-
tive group based on their collaborations and com-
mon characteristics, which is more promising than
the typical methods of classifying individual state-
ments as deceptive or trustworthy.
In order to identify deceptive groups by analyz-
ing the evolution of a person’s deception strategy
during his interactions with victims and the inter-
actions within the deceptive group from conver-
sations, we use a social-elimination game called
Killer Game which contains the ground-truth of
subgroups.
The killer game has many variants that involve
different roles and skills. We choose a classical
version played by three roles/teams: detectives, c-
itizens, and killers. The role of each player (game
participant) is randomly assigned by a third-party
game judge. Every killer/detective is given the i-
dentities of his teammates. There are two alter-
nating phases of the game: “night”, when killer-
s may covertly “murder” a player and detectives
may learn one player’s role; and “day”, when sur-
viving players are informed of who was killed last
“night” and then asked to speculate about the roles
of other surviving players. Before a “day” ends,
every surviving player should vote for a suspect.
The candidate with the most votes is eliminated. A
player’s identity is not exposed after his “death”.
The game continues until all killers have been e-
liminated or all detectives have been killed. The
killers are treated as deceivers, and citizens and
detectives as truth-tellers.
In this paper, we present an unsupervised ap-
proach for differentiating the deceptive groups
from truth-tellers in a game. During each round,
we use Natural Language Processing (NLP) tech-
niques to identify a player’s attitude toward other
players (Section 2), which are used to construc-
t a vector of attitudes for each surviving player
(Section 3.1) and a signed social network repre-
sentation (Section 3.2) for the discussions. Then
we use a clustering algorithm to cluster the atti-
tude vector space and obtain results for each round
(Section 3.1). We also implement a greedy op-
timization algorithm to partition the singed net-
work based on the attitude clustering result (Sec-
tion 3.2). Finally, we apply a pairwise-similarity
approach that makes use of the predicted co-
occurrence relations between players to combine
all results from each round (Section 3.3). Figure 1
provides an overview of our system pipeline.
The major novel contributions of this paper are
as follows.
</bodyText>
<listItem confidence="0.976638888888889">
• This is the first study to investigate conversa-
tions and deceptive groups for computerized
deception detection.
• The proposed clustering technique is shown
to be successful in separating deceptive
groups from truth-tellers.
• The method can be applied to dynamically
detect subgroups in a network with discus-
sants who tend to change their opinions.
</listItem>
<sectionHeader confidence="0.962606" genericHeader="method">
2 Attitude Identification
</sectionHeader>
<bodyText confidence="0.998531571428571">
In this section, we describe how we take a player’s
statement in a single round as input to extract his
attitudes toward other players and represent them
by an attitude 3-tuple (speaker, target, polarity)
list. For this work, the polarity of attitudes (Bal-
ahur et al., 2009) can be positive (1), negative (-1)
or neutral (0). A game log from a single round
</bodyText>
<page confidence="0.998002">
858
</page>
<figureCaption confidence="0.984106">
will be used as our illustrative example, as shown
Figure 2: Killer game sampe log (1st round)
</figureCaption>
<table confidence="0.7394152">
in Figure 2.
C: CITIZEN; D: DETECTIVE; K: KILLER
System: First Round.
System: 15 was killed last night. 15, please leave your last words.
15(C): I’m a citizen. Over.
</table>
<tableCaption confidence="0.819915">
16(K): I’m a good person. 11 and 2 are suspicious.
1(K): I’m a good person. It has been a long time since I played as a
killer. I’m a citizen. 11 is suspicious and I don’t want to comment
on 16’s statement.
2(C): I’m a detective. 6 was proved as a killer last night. Over.
3(C): I don’t know 2’s identity. It’s hard to judge 16’s statement. 1
seems to be a good person. I’m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. I’m 2’s teammate.
5(D): I’m a detective. I verify 2’s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I don’t understand. 14 is
suspicious.
7(K): It’s hard to define 6’s identity. 4 may be a citizen. I will vote
for 2. 6 sounds very weird and I found 6 very suspicious. I will
follow the detective 5 to vote for 2.
</tableCaption>
<footnote confidence="0.801925833333333">
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I don’t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
</footnote>
<construct confidence="0.923128714285714">
12(K): 11 sounds like a killer. 2 is a killer. I’m a citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
don’t think 2 is a killer. I doubt 7’s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I don’t think 7 must be a killer. 2 is
obviously bad. I’m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 · · · 10, 13, 5, 2 vote for
7 · · · 9, 6 vote for 11 · · · 2 is out.
</construct>
<figureCaption confidence="0.99082">
Figure 2: Killer game sample log (the 1st round).
</figureCaption>
<subsectionHeader confidence="0.614436">
21 Target and Attitude Word Identification
2.1 Target and Attitude Word Identification
</subsectionHeader>
<bodyText confidence="0.998906470588235">
fro conversations. In the killer game, a target
We start by identifying targets and attitude word-
s from conversations. In the killer game, a target
is represented by his unique ID2 and game terms
are regarded as attitude words. We collected 41
terms in total from the game’s website 3 and re-
lated discussion forum posts. ICTCLAS (Zhang
et al., 2003) is used for word segmentation and
part-of-speech (POS) tagging. There are two kind-
s of game terms: positive and negative. Posi-
tive terms include “citizen”, “good person”, “good
person certified by the detectives” and “detective”.
Negative terms include “killer”, “killer verified by
the detectives” and “a killer who claimed him-
self/herself to be a detective”. We assign the po-
larity score +1, -1 to positive and negative terms
respectively.
</bodyText>
<footnote confidence="0.999018333333333">
2Each player has a game ID, assigned by the online game
system based on when he entered the game room.
3e.g., http://www.3j3f.com/how/
</footnote>
<subsectionHeader confidence="0.996562">
2.2 Attitude-Trget Piring
</subsectionHeader>
<bodyText confidence="0.971309611111111">
ten, we associate all attitude
Then we associate each attitude word with it-
tence with it. Othewise, f I is he ony subec
scorrspondig targt. We remove interrogative
or there are no subjecs at all, we associate atti
an xclamatory sentences and only keep th sn-
tude words with the ID of the speaker. We reverse
tences that include least one atttude word from
the polarity of an attitude word if it appears in a
a player’s statement during each round.
negation context
We develop a rule-based approach for attitude-
Prioumethdpiratgtandatt
target pairing: if there is at least one ID in the sen-
tence, we associate all attitude words in that sen-
tude word if they satify at least one dependency
tence with it. Otherwise, if “I” is the only subject
rules (e.g.(Somasundarn and Wiebe, 2009)). We
orthere are no subjects at all, we associate atti-
check the POS tag sequence between them For
tude words wih the ID of the spaker. We reverse
each attitude-target pair if there exist an attitude
the polrity o an atitude word if it appears in a
word a beleforiented verb such as “think” “be
negation context.
lieve” “feel” or
Previous methods pair a target and an attitude
qnce w ill diad thi pai Th aptio
word if they satisfy at least one dependency rule
(e.g., (Somasundaran and Wiebe, 2009)). We
is tht POS tag sequeces can be ed to smma
check the POS tag squence between hem. For
rize dependency rules when statements are rela
each attitude-target pair, if there exists an attitude
tively short.
word, a belief-oriented verb such as “think”, “be-
Fo hose targets the speaker didn’t metion
lieve”, “feel”, or more than two verbs in the se-
or thee is no positive/negatve atitude word used
quence, we will discard this pair. The assumption
whn they a tid he itde plrity
is that POS tag sequences can be used to summa-
rize dependency rules when statements are rela-
cor is set to 0. For instance given Player 6s
tively short.
tatement in
6, +), (16 11, -1) (16 2, -1), (16, 1 0), (16, 3
For those targets, the speaker didn’t mention
or there is no positive/negative attitude word used
0), . . . , (16, 15, 0)]
when they are mentioned, the attitude polarity s-
core is set to 0. For instance, given Player 16’s s-
1Eh ply h g ID sigd by the i g
tatement in Figure 2, its attitude tuple list is: [(16,
</bodyText>
<equation confidence="0.57888975">
system based on when s/he entered the game room
16,+1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,
2eg, http:/www3j3comhow/
0), ..., (16, 15, 0)].
</equation>
<sectionHeader confidence="0.993059" genericHeader="method">
3 Clustering
</sectionHeader>
<bodyText confidence="0.9986">
Since the statements in conversations are relatively
short and concise, it is difficult to identify which
one is deceptive, even using deep linguistic fea-
tures such as the language style.
In this section, we introduce a method to con-
struct an attitude profile for each player and a
signed network based on the attitude tuple list in
Section 2, and combine them to analyze a dynam-
ic network with discussants telling lies and truths.
</bodyText>
<subsectionHeader confidence="0.999856">
3.1 Clustering based on Attitude Profile
</subsectionHeader>
<bodyText confidence="0.999884833333333">
We use a vector containing numerical values to
represent each player’s attitude toward identified
targets in each round. The values correspond to
the polarity scores in a player’s attitude tuple list.
For example, the polarity score of player 16’s atti-
tude toward target 11 is −1 as shown in Figure 2.
</bodyText>
<page confidence="0.993538">
859
</page>
<bodyText confidence="0.999672473684211">
We call this vector as the discussant attitude pro-
file (DAP) following (Abu-Jbara et al., 2012a).
Suppose there are n players who participate in
a single game. Since a player’s identity is not ex-
posed to the public after his death4, people can still
analyze the identity of a “dead” player. Therefore,
the number of possibly mentioned targets in each
round equals to n. Given all the statements from
m surviving players in a single round, each play-
er’s DAP has n + 1 dimensions including his vote
and thus we can have a m x (n + 1) attitude ma-
trix A where Aij represents the attitude polarity of
i toward j we got from Section 2. Ai(n+1) repre-
sents i’s vote.
In a certain round, given a set of m surviving
players X = {x1, x2, · · · , xm1 to be clustered
and their respective DAPs, we can modify the Eu-
clidean metric to compute the differences in atti-
tudes and get an m x m distance matrix M:
</bodyText>
<equation confidence="0.892057555555556">
n
tuuv
Mij = X (Aik − Ajk)2 + (2 − 2δAi(n+1),Aj(n+1))2
k=1
(1)
The Kronecker delta function δ is:
~ 1 i = j
δij = 2
0 i #j ()
</equation>
<bodyText confidence="0.999484052631579">
We use this function to compare the votes of t-
wo players separately because a player’s vote can
be inconsistent with his previous statements. We
assume that there is a larger distance between two
players when they vote for different suspects.
A common assumption in previous research was
that a member is more likely to show a positive
attitude toward other members in the same group,
and a negative attitude toward the opposing group-
s (Abu-Jbara et al., 2012a). However, a deceiver
may pretend to be innocent by supporting those
truth-tellers and attacking his teammates, whose i-
dentities have already been exposed. Therefore,
it is not enough to judge the relationship between
two players by simply measuring the distance be-
tween their DAPs.
In addition to comparing DAPs between player-
s i and j, we also consider the attitudes of other
players toward i and j, as well as their attitudes
</bodyText>
<footnote confidence="0.9716225">
4Each round, the player killed by killers and the player
with the most votes are out.
</footnote>
<equation confidence="0.988365">
toward each other. We modify Mij as follows and
show it in Figure 3:
(Aki − Akj)2 + (h(Aij) + h(Aji))2
(3)
</equation>
<bodyText confidence="0.998711909090909">
where the function h detects the negative atti-
tudes. h(x) = 0 if x &gt; 0 and h(x) = −1 other-
wise.
We perform hierarchical clustering on the con-
densed distance matrix of M and use the complete
linkage method to compute the distance between
two clusters (Voorhees, 1986). We set the num-
ber of clusters as 3 since there are three natural
groups in the game. We focus on separating de-
ceivers (killers) from truth-tellers (citizens and de-
tectives).
</bodyText>
<figure confidence="0.843994">
𝑖 𝑗
compare 𝑖
and 𝑗′s DAPs
</figure>
<figureCaption confidence="0.9907265">
Figure 3: Computation of the distance between
player i and j based on the attitude matrix.
</figureCaption>
<subsectionHeader confidence="0.991854">
3.2 Signed Network Partition
</subsectionHeader>
<bodyText confidence="0.981666">
When we computed the distance between two
players in Section 3.1, we did not consider the net-
work structure among all the players. For exam-
ple, if A supports C, B supports D and C and D
dislike each other, A and B may belong to differ-
ent groups. Thus, we propose to capture the in-
teractions in the social network to further improve
the attitude-profile-based clustering result.
We can easily convert the attitude matrix A into
a signed network by adding a directed edge i —* j
between i and j if Aij =� 0. We denote a directed
graph corresponding to a signed network as G =
(V, 5, N, W), where V is the set of nodes, 5 is the
set of positive edges, N is the set of negative edges
and W : (V x V ) —* {−1, 11 is a function that
maps every directed edge to a value, W(i, j) =
Aij.
We use a greedy optimization algorithm (Dor-
eian and Mrvar, 1996) to find partitions. A criteri-
on function for an optimal partitioning procedure
</bodyText>
<figure confidence="0.993756833333333">
0
Mij = Mij +
v u u Xm
tk=1
𝑗
𝑖
</figure>
<page confidence="0.978762">
860
</page>
<bodyText confidence="0.9984335">
is constructed such that positive links are dense
within groups and negative links are dense be-
tween groups. For any potential partition C, we
seek to minimize the following error function:
</bodyText>
<equation confidence="0.715808">
(4)
</equation>
<bodyText confidence="0.999981176470588">
where -y E [0, 1] controls the balance of the
penalty difference between putting a positive edge
across and a negative edge within a group. We re-
gard these two types of errors as equally important
and set -y = 0.5 for our experiments.
Initially, we use the clustering result in Sec-
tion 3.1 to partition nodes into three differen-
t groups and an error function, E, is evaluated for
that cluster. Every cluster has a set of neighbor
clusters in the cluster space. A neighbor cluster
is obtained by moving a node from one group to
another, or exchanging two nodes in two different
groups. E is evaluated for all the neighbor clusters
of the current cluster and the one with the lowest
value is set as the new cluster. The algorithm is
repeated until it finds a minimal solution5. We set
the upper limit for the number of subgroups to 3.
</bodyText>
<subsectionHeader confidence="0.998426">
3.3 Cluster Ensembles
</subsectionHeader>
<bodyText confidence="0.999324833333333">
The relationships between players are dynamic
throughout the game. For example, a killer tends
to hide his identity and pretends to be friendly to
others at later stages in order to survive. Thus, it
is insufficient to rely on a single round’s discus-
sion to cluster players. In addition, for each single
round, we also need to combine the clustering re-
sults from the attitude profiles of the players and
the signed network.
In a game with information gathered from up
to r rounds, let P = {P1, P2, · · · , Pr} be the set
of r clusterings (partitionings) based on attitude
profiles and P0 = {P01, P02,··· , P0r} be the set of
r clusterings based on the signed network.
Using the co-occurrence relations between
players, we can generate a n x n pairwise simi-
larity matrix T based on the information of all r
rounds:
</bodyText>
<figure confidence="0.344935">
A · voteij + (1 − A) · vote0ij
rij
</figure>
<footnote confidence="0.8259075">
5Since our graphs are small, we search through all parti-
tions. We repeated 1000 times in our experiment.
</footnote>
<equation confidence="0.501481">
0
</equation>
<bodyText confidence="0.998835818181818">
where voteij, voteij are the number of times
that player i and j are assigned to the same cluster
in P and P0 respectively. rij denotes the number
of rounds when both of them survived (rij G r).
Tirj E [0, 1]. We assign a higher weight to the re-
sult of P1 and set A = 2/3 in our experiments.
Given the input in Figure 2, x3 and x4 are as-
signed to the same cluster in P1 (vote34 = 1) and
inP10 (vote034 = 1) respectively as shown in Fig-
ure 4. x3 and x4 co-occurred in the first round
(r34 = 1). T134 = (2/3 x 1 + 1/3 x 1)/1 = 1.
</bodyText>
<figureCaption confidence="0.8502175">
Figure 4: Example of cluster ensemble for a single
round.
</figureCaption>
<bodyText confidence="0.999712666666667">
We apply hierarchical clustering (Voorhees,
1986) to the similarity matrix above to obtain the
final global clustering results.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998079">
4.1 Dataset Construction
</subsectionHeader>
<bodyText confidence="0.999923875">
We recorded 10 games from 3J3F6, one of the
most popular Chinese online killer game web-
sites 7. A screenshot of the game system inter-
face is shown in Figure 5. There are 16 partic-
ipating players per game: 4 detectives, 4 killer-
s and 8 citizens. Each player occupies a posi-
tion in 1 . All the surviving players can express
their attitudes via a voice channel using 2 , while
detectives and killers can also communicate with
teammates in their respective private team chan-
nels 3 via texts. The system provides real-time
updates on the game progress, voting results, and
so on using the public channel 4 . We manually
transcribed speech and stored the text information
in the public channel, which contains the voting
and death information. The average game length
</bodyText>
<footnote confidence="0.994388">
6http://www.3j3f.com
7All data sets and resources will be made available for
research purposes upon the acceptance of the paper.
</footnote>
<figure confidence="0.958126341463414">
Round 1
CITIZEN OR DETECTIVE KILLER
𝑷𝟏
𝑥11
𝑥2
𝑥4
𝑥3
𝑥5
𝑥15
𝑥6
𝑥13
𝑥10
𝑥7
𝑥12
𝑥1
𝑥14
𝑥16
𝑥8
𝑥9
𝑷′𝟏
𝑥3
𝑥6
𝑥13
𝑥1
𝑥11
𝑥2
𝑥4
𝑥5
𝑥12
𝑥8
𝑥15
𝑥9
𝑥10
𝑥7
𝑥14
𝑥16
E(C) = � [(1 − γ) � W(i, j)Si,j − γ � W(i, j)Ni,j]
C∈C i∈C i,j∈C
j /∈C
Tijr =
(5)
</figure>
<page confidence="0.982746">
861
</page>
<table confidence="0.997543038461538">
Game D N Purity (%) eD + N
# H eD
1 68.8 75.0 75.0 68.8 75.0
2 75.0 68.8 68.8 43.8 81.3
3 43.8 81.3 56.3 75.0 75.0
4 75.0 62.5 75.0 93.8 93.8
5 62.5 75.0 81.3 75.0 75.0
6 81.3 81.3 75.0 81.3 81.3
7 81.3 75.0 81.3 81.3 87.5
8 87.5 75.0 75.0 93.8 93.8
9 75.0 43.8 75.0 81.3 87.5
10 62.5 75.0 87.5 81.3 81.3
Average 71.3 71.3 75.0 77.5 83.2
Entropy
D N H eD eD + N
0.48 0.50 0.78 0.63 0.50
0.71 0.69 0.81 0.73 0.43
0.77 0.67 0.81 0.72 0.72
0.78 0.68 0.74 0.28 0.28
0.61 0.50 0.61 0.72 0.72
0.64 0.38 0.74 0.60 0.60
0.65 0.70 0.68 0.51 0.51
0.41 0.73 0.78 0.23 0.23
0.76 0.80 0.78 0.67 0.49
0.78 0.60 0.51 0.61 0.67
0.66 0.62 0.72 0.57 0.51
</table>
<tableCaption confidence="0.994818">
Table 1: Results on subgroup detection. D refers to DAPC, N refers to Network, H refers to Human Voting, and eD
refers to extended DAPC.
</tableCaption>
<bodyText confidence="0.874544">
is about 76.3 minutes and there are on average 5
rounds and 411 sentences per game. Note that our
method is language-independent and could easily
be adapted to other languages.
</bodyText>
<figureCaption confidence="0.9656035">
Figure 5: Screenshot of the online killer game in-
terface.
</figureCaption>
<subsectionHeader confidence="0.966676">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999203125">
We use two metrics to evaluate the clustering ac-
curacy: Purity and Entropy. Purity (Manning et
al., 2008) is a metric in which each cluster is as-
signed to the class with the majority vote in the
cluster, and then the accuracy of this assignmen-
t is measured by dividing the number of correctly
assigned instances by the total number of instances
N. More formally:
</bodyText>
<equation confidence="0.988407333333333">
1 � maxj|wk n cj |(6)
purity(Q, C) =
N k
</equation>
<bodyText confidence="0.998048666666666">
where Q = {w1, w2, · · · , wk} is the set of clusters
and C = {c1, c2, · · · , cj} is the set of classes. wk
is interpreted as the set of instances in wk and cj
is the set of instances in cj. The purity increases
as the quality of clustering improves.
Entropy (Steinbach et al., 2000) measures the
uniformity of a cluster. The entropy for all clusters
is defined by the weighted sum of the entropy of
each cluster:
</bodyText>
<equation confidence="0.996848333333333">
i
E P(i, j) x log2P(i, j)
(7)
</equation>
<bodyText confidence="0.999968">
where P(i, j) is the probability of finding an el-
ement from the category i in the cluster j, nj is
the number of items in cluster j and n is the total
number of items in the distribution. The entropy
decreases as the quality of clustering improves.
</bodyText>
<subsectionHeader confidence="0.999661">
4.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999438">
We compare our approach with two state-of-the-
art subgroup detection methods and human perfor-
mance as follows:
</bodyText>
<listItem confidence="0.965256">
1. DAPC: In Section 3.1, we introduced our im-
</listItem>
<bodyText confidence="0.8749367">
plementation of the discussant attitude profile
clustering (DAPC) method proposed in (Abu-
Jbara et al., 2012a). In the original DAPC
method, for each opinion target, there are 3
dimensions in the feature vector, correspond-
ing to (1) the number of positive expression-
s, (2) negative expressions toward the tar-
get from the online posts and (3) the num-
ber of times the discussant mentioned the tar-
get. For our experiment, we only keep one
dimension representing the discussant’s atti-
tude (positive, negative, neutral) toward the
target since a discussant attitude remains the
same in his statement within a single round.
2. Network: We also implemented the signed
network partition method for subgroup detec-
tion proposed by (Hassan et al., 2012). To
determine the number of subgroups t, we set
an upper limit of t = 3 in order to minimize
the optimization function.
</bodyText>
<figure confidence="0.995655583333333">
START END
2
1
TEAM CHANNEL
3
OUT
Current Speaker: 14
PUBLIC CHANNEL
4
Entropy = − �j
nj
n
</figure>
<page confidence="0.977449">
862
</page>
<listItem confidence="0.777385">
3. Human Voting: We also compare our meth-
</listItem>
<bodyText confidence="0.993114357142857">
ods with human voting results. There are two
subgroups based on the voting results. The
players with the highest votes each round be-
long to one subgroup and the rest of the play-
ers are in the other subgroup.
Table 1 shows the overall performance of vari-
ous methods on subgroup detection and Figure 6
depicts the average performance. We can see that
our method significantly outperforms two baseline
methods and human voting. The human perfor-
mance is not satisfying, which indicates it’s very
challenging even for a human to identify a deceiv-
er whose deceptive statement is mixed with plenty
of truthful opinions (Xu and Zhao, 2012).
</bodyText>
<figureCaption confidence="0.9888415">
Figure 6: An overview of the average performance
of all the methods.
</figureCaption>
<subsectionHeader confidence="0.995843">
4.4 Dynamic Subgroup Detection
</subsectionHeader>
<bodyText confidence="0.99946575">
As shown in Figure 7, the performance of our
approach improves as the game proceeds. Play-
ers seldom maintain their opinions throughout a
game. Figure 2 shows that most killers (16,1,12)
insisted that citizen 11 should be a killer except 7.
As a response to the group pressure (Asch, 1951),
7 changed his opinion and stated that 11 could be
a killer in the following round.
In reality, a discussant who participates in an
online discussion tends to change his opinion-
s about a target as he learns more information,
which shows both the necessity and importance of
the dynamic detection of subgroups. Our method
can be applied to detect subgroups dynamically by
grouping posts into multiple discussion “rounds”
based on their timestamps.
</bodyText>
<figure confidence="0.448497">
Purity Entropy
</figure>
<figureCaption confidence="0.9896665">
Figure 7: Average performance based on different
rounds.
</figureCaption>
<figure confidence="0.9939639">
70
%
65
85
80
75
Purity
Entropy
60
55
50
Method
%
80
70
60
50
1st round
1st + 2nd rounds
all rounds
</figure>
<bodyText confidence="0.999943166666666">
By extending the DAPC method (EDPAC), we
can estimate the distance between two players
more accurately by considering the attitudes of
other players toward them and their attitudes to-
ward each other. Given the log in Figure 2 as in-
put, players 5 (detective) and 7 (killer) are clus-
tered into one group when DAPC is applied s-
ince they don’t have conflicting views on the i-
dentities of other players. However, 5 voted for
7 and is supported by more players compared with
7, which indicates that they are less likely to be
teammates. We can successfully separate them af-
ter re-computing the distance between them.
Adding network information provided 5.7%
further gain in Purity. In some cases, the perfor-
mance remains the same when EDAPC clustering
result is already optimal with the minimum value
of the criterion function.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.997372">
5.1 Opinion Analysis
</subsectionHeader>
<bodyText confidence="0.999889571428571">
Our work on mining a player’s attitude toward oth-
er players is related to opinion mining. Attitudes
and opinions are related and can be regarded as
the same in our task. Compared with the previ-
ous work (e.g.,(Qiu et al., 2011; Kim and Hovy,
2006)), the opinion words and targets in our task
are relatively easier to recognize due to the sim-
plicity of statements. Some recent work (e.g., (So-
masundaran and Wiebe, 2009; Abu-Jbara et al.,
2012a)) developed syntactic rules to pair an opin-
ion word and a target if they satisfy at least one
specific dependency rule. We use POS tag se-
quences to efficiently help us filter out irrelevant
pairs.
</bodyText>
<page confidence="0.994985">
863
</page>
<subsectionHeader confidence="0.992414">
5.2 Deception Detection
</subsectionHeader>
<bodyText confidence="0.9999949375">
Most of the previous computational work for
deception detection used supervised/semi-
supervised classification methods (Li et al.,
2013b). Besides lexical and syntactical fea-
tures (Ott et al., 2011; Feng et al., 2012; Yancheva
and Rudzicz, 2013), Feng and Hirst (2013) pro-
posed using profile compatibility to distinguish
fake and genuine reviews. Xu and Zhao (2012)
used deep linguistic features such as text genre
to detect deceptive opinion spams. Banerjee et
al. (2014) used extended linguistic signals such
as keystroke patterns. Li et al. (2013a) used topic
models to detect the difference between deceptive
and truthful topic-word distribution. Researchers
have began to realize the importance of analyzing
computer-mediated communication in deception
detection. Zhou and Sung (2008) conducted
an empirical study on deception cues using the
killer game as a task scenario and obtained many
interesting findings (e.g., deceivers send fewer
messages than truth-tellers).
Our work is most related to the work of Chit-
taranjan and Hung (2010) on detecting deceptive
roles in the Werewolf Game which is another vari-
ant of the killer game. They created a Werewolf
data set by audio-visual recording 8 games played
by 2 groups of people face-to-face and extract-
ed audio features and interaction features for their
experiments. However, we should note that non
face-to-face deception detection emphasizes ver-
bal and linguistic cues over less controllable non-
verbal communication cues (Walther, 1996).
</bodyText>
<subsectionHeader confidence="0.994623">
5.3 Subgroup Detection
</subsectionHeader>
<bodyText confidence="0.999971666666667">
In online discussions, people usually split into
subgroups based on various topics. The member
of a subgroup is more likely to show positive at-
titude to the members of the same subgroup, and
negative attitude to the members of opposing sub-
groups (Abu-Jbara et al., 2012a). Previous work
also studied subgroup detection in social media
sites. Abu-Jbara et al. (2012a) constructed a dis-
cussant attitude profile (DAP) for each discussant
and then used clustering techniques to cluster their
attitudes. Hassan et al. (2012; 2012b; 2013) pro-
posed various methods to automatically construct
a signed social network representation of discus-
sions and then identify subgroups by partitioning
their signed networks. Qiu et al. (2013) applied
collaborative filtering through Probabilistic Matrix
Factorization (PMF) to generalize and improve ex-
tracted opinion matrices.
An underlying assumption of the previous work
was that a participant will not tell lies nor hide his
own stance. Moreover, their work did not take in-
to account that a person’s attitude or stance will
change as he learns more by reading the com-
ments from others and acquiring more background
knowledge (Bandura, 1971). Our contribution is
that we extend the DAP method and combine it
with the signed network partition in order to clus-
ter the hidden group members. We also develop a
novel cluster ensemble approach in order to ana-
lyze the dynamic network.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979371428571">
Using the killer game as a case study, we present
an effective clustering method to detect subgroups
from dynamic conversations with lies and truth-
s. This is the first work to utilize the dynam-
ics of group conversations for deception detec-
tion. Experiments demonstrated that truth-tellers
and deceptive groups are separable and the pro-
posed method significantly outperforms baseline
approaches and human voting.
Our work builds a pathway to future work in
deception detection in content-rich dynamic envi-
ronments such as electronic commerce and repeat-
ed interrogation which will require sophisticated
content and network analysis. In real-life suspects
may be interrogated about particular events on nu-
merous occasions. Our method can potentially be
modified to find criminals who act in groups based
on their statements. Other applications of this re-
search include law enforcement, financial fraud,
fraudulent ad campaigns and social engineering.
This study focuses on analyzing the verbal con-
tent in conversations. It will be interesting to study
non-verbal features such as blink rate, gaze aver-
sion and pauses (Granhag and Str¨omwall, 2002)
when people play this game face-to-face and com-
bine the non-verbal and verbal features for decep-
tion detection. In addition, it is worth exploring
the impact of cross-cultural analysis in detecting
deception. When attempting to detect deceit in
people of other ethnic origin than themselves, peo-
ple perform even worse in terms of lie detection
accuracy than when judging people of their own
ethnic origin (Vrij, 2000). For the future work,
we aim to use automatic prediction of deceivers to
help truth-tellers win games more easily.
</bodyText>
<page confidence="0.99742">
864
</page>
<sectionHeader confidence="0.985654" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999757">
This work was supported by the U.S. DARPA
DEFT Program No. FA8750-13-2-0041, ARL
NS-CTA No. W911NF-09-2-0053, NSF Award-
s IIS-0953149 and IIS-1523198, AFRL DREAM
project, gift awards from IBM, Google, Disney
and Bosch. The views and conclusions contained
in this document are those of the authors and
should not be interpreted as representing the of-
ficial policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999678597701149">
A. Abu-Jbara, M. Diab, P. Dasigi, and D. Radev.
2012a. Subgroup detection in ideological discus-
sions. In Proc. Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
A. Abu-Jbara, A. Hassan, and D. Radev. 2012b. Atti-
tudeminer: mining attitude from online discussions.
In Proc. North American Chapter of the Association
for Computational Linguistics - Human Language
Technologies (NAACL HLT 2012).
A. Abu-Jbara, B. King, M. Diab, and D. Radev. 2013.
Identifying opinion subgroups in arabic online dis-
cussions. In Proc. Association for Computational
Linguistics (ACL 2013).
S. Asch. 1951. Effects of group pressure upon the
modification and distortion of judgments. Groups,
leadership, and men. S.
A. Balahur, R. Steinberger, E. Goot, B. Pouliquen, and
M. Kabadjov. 2009. Opinion mining on newspaper
quotations. In IEEE/WIC/ACM International Joint
Conferences on Web Intelligence and Intelligent A-
gent Technologies (WI-IAT 2009).
A. Bandura. 1971. Social Learning Theory. General
Learning Corporation.
R. Banerjee, S. Feng, J. Kang, and Y. Choi. 2014.
Keystroke patterns as prosody in digital writings: A
case study with deceptive reviews and essays. In
Proc. Empirical Methods on Natural Language Pro-
cessing (EMNLP 2014).
D. Buller and J. Burgoon. 1996. Interpersonal decep-
tion theory. Communication theory.
David B Buller, Judee K Burgoon, JA Daly, and
JM Wiemann. 1994. Deception: Strategic and
nonstrategic communication. Strategic interperson-
al communication.
G. Chittaranjan and H. Hung. 2010. Are you awere-
wolf? detecting deceptive roles and outcomes in a
conversational role-playing game. In IEEE Interna-
tional Conference on Acoustics Speech and Signal
Processing (ICASSP 2010).
P. Doreian and A. Mrvar. 1996. A partitioning ap-
proach to structural balance. Social networks.
V. Feng and G. Hirst. 2013. Detecting deceptive opin-
ions with profile compatibility. In Proc. Internation-
al Joint Conference on Natural Language Process-
ing (IJCNLP 2013).
S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic
stylometry for deception detection. In Proc. Associ-
ation for Computational Linguistics (ACL 2012).
N. E. Friedkin. 2010. The attitude-behavior linkage in
behavioral cascades. Social Psychology Quarterly.
P. Granhag and L. Str¨omwall. 2002. Repeated inter-
rogations: verbal and non-verbal cues to deception.
Applied Cognitive Psychology.
A. Hassan, A. Abu-Jbara, and D. Radev. 2012. De-
tecting subgroups in online discussions by model-
ing positive and negative relations among partici-
pants. In Proc. Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012).
S. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news me-
dia text. In Proc. ACL-COLING 2006 Workshop on
Sentiment and Subjectivity in Text.
J. Li, C. Cardie, and S. Li. 2013a. Topicspam: a
topic-model based approach for spam detection. In
Proc. Association for Computational Linguistics (A-
CL 2013).
J. Li, M. Ott, and C. Cardie. 2013b. Identifying ma-
nipulated offerings on review portals. In Proc. Em-
pirical Methods on Natural Language Processing
(EMNLP 2013).
C. Manning, P. Raghavan, and H. Sch¨utze. 2008. In-
troduction to information retrieval. Cambridge uni-
versity press Cambridge.
M. Ott, Y. Choi, C. Cardie, and J. Hancock. 2011.
Finding deceptive opinion spam by any stretch of the
imagination. In Proc. Association for Computation-
al Linguistics (ACL 2011).
G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion
word expansion and target extraction through double
propagation. Computational linguistics.
M. Qiu, L. Yang, and J. Jiang. 2013. Mining us-
er relations from online discussions using sentimen-
t analysis and probabilistic matrix factorization. In
Proc. North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL HLT 2013).
</reference>
<page confidence="0.985646">
865
</page>
<reference confidence="0.99944275">
S. Somasundaran and J. Wiebe. 2009. Recognizing s-
tances in online debates. In Proc. Joint Conference
of the Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP.
M. Steinbach, G. Karypis, V. Kumar, et al. 2000. A
comparison of document clustering techniques. In
Proc. KDD 2000 workshop on text mining.
E. Voorhees. 1986. Implementing agglomerative hi-
erarchic clustering algorithms for use in document
retrieval. Information Processing &amp; Management.
A. Vrij, P. Granhag, and S. Porter. 2010. Pitfalls and
opportunities in nonverbal and verbal lie detection.
Psychological Science in the Public Interest.
A. Vrij. 2000. Detecting lies and deceit: The psychol-
ogy of lying and implications for professional prac-
tice. Wiley.
J. Walther. 1996. Computer-mediated communication
impersonal, interpersonal, and hyperpersonal inter-
action. Communication research.
Q. Xu and H. Zhao. 2012. Using deep linguistic
features for finding deceptive opinion spam. In
Proc. International Conference on Computational
Linguistics (COLING 2012).
M. Yancheva and F. Rudzicz. 2013. Automatic de-
tection of deception in child-produced speech using
syntactic complexity features. In Proc. Association
for Computational Linguistics (ACL 2013).
H. Zhang, H. Yu, D. Xiong, and Q. Liu. 2003. Hhmm-
based chinese lexical analyzer ictclas. In Proc.
SIGHAN 2003 workshop on Chinese language pro-
cessing.
L. Zhou and Y. Sung. 2008. Cues to deception in on-
line chinese groups. In Proc. Hawaii International
Conference on System Sciences (HICSS 2008).
L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.
2004. Automating linguistics-based cues for detect-
ing deception in text-based asynchronous computer-
mediated communications. Group decision and ne-
gotiation.
</reference>
<page confidence="0.998841">
866
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.227350">
<title confidence="0.999907">Detecting Deceptive Groups Using Conversations and Network Analysis</title>
<author confidence="0.993139">Yulia Heng William</author>
<affiliation confidence="0.4161485">Science Department, Rensselaer Polytechnic of Industrial and Systems Engineering, Rensselaer Polytechnic</affiliation>
<abstract confidence="0.998587043478261">Deception detection has been formulated as a supervised binary classification problem on single documents. However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. Deceivers may dynamically adjust their deceptive statements according to the reactions of victims. In addition, people may form groups and collaborate to deceive others. In this paper, we seek to identify deceptive groups from their conversations. We propose a novel subgroup detection method that combines linguistic signals and signed network analysis for dynamic clustering. A social-elimination called Game introduced as a Experimental results demonstrate that our approach significantly outperforms human voting and state-of-theart subgroup detection methods at dynamically differentiating the deceptive groups from truth-tellers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abu-Jbara</author>
<author>M Diab</author>
<author>P Dasigi</author>
<author>D Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="14044" citStr="Abu-Jbara et al., 2012" startWordPosition="2387" endWordPosition="2390"> profile for each player and a signed network based on the attitude tuple list in Section 2, and combine them to analyze a dynamic network with discussants telling lies and truths. 3.1 Clustering based on Attitude Profile We use a vector containing numerical values to represent each player’s attitude toward identified targets in each round. The values correspond to the polarity scores in a player’s attitude tuple list. For example, the polarity score of player 16’s attitude toward target 11 is −1 as shown in Figure 2. 859 We call this vector as the discussant attitude profile (DAP) following (Abu-Jbara et al., 2012a). Suppose there are n players who participate in a single game. Since a player’s identity is not exposed to the public after his death4, people can still analyze the identity of a “dead” player. Therefore, the number of possibly mentioned targets in each round equals to n. Given all the statements from m surviving players in a single round, each player’s DAP has n + 1 dimensions including his vote and thus we can have a m x (n + 1) attitude matrix A where Aij represents the attitude polarity of i toward j we got from Section 2. Ai(n+1) represents i’s vote. In a certain round, given a set of </context>
<context position="15428" citStr="Abu-Jbara et al., 2012" startWordPosition="2655" endWordPosition="2658"> and get an m x m distance matrix M: n tuuv Mij = X (Aik − Ajk)2 + (2 − 2δAi(n+1),Aj(n+1))2 k=1 (1) The Kronecker delta function δ is: ~ 1 i = j δij = 2 0 i #j () We use this function to compare the votes of two players separately because a player’s vote can be inconsistent with his previous statements. We assume that there is a larger distance between two players when they vote for different suspects. A common assumption in previous research was that a member is more likely to show a positive attitude toward other members in the same group, and a negative attitude toward the opposing groups (Abu-Jbara et al., 2012a). However, a deceiver may pretend to be innocent by supporting those truth-tellers and attacking his teammates, whose identities have already been exposed. Therefore, it is not enough to judge the relationship between two players by simply measuring the distance between their DAPs. In addition to comparing DAPs between players i and j, we also consider the attitudes of other players toward i and j, as well as their attitudes 4Each round, the player killed by killers and the player with the most votes are out. toward each other. We modify Mij as follows and show it in Figure 3: (Aki − Akj)2 +</context>
<context position="27733" citStr="Abu-Jbara et al., 2012" startWordPosition="4931" endWordPosition="4934">In some cases, the performance remains the same when EDAPC clustering result is already optimal with the minimum value of the criterion function. 5 Related Work 5.1 Opinion Analysis Our work on mining a player’s attitude toward other players is related to opinion mining. Attitudes and opinions are related and can be regarded as the same in our task. Compared with the previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used dee</context>
<context position="29749" citStr="Abu-Jbara et al., 2012" startWordPosition="5245" endWordPosition="5248">set by audio-visual recording 8 games played by 2 groups of people face-to-face and extracted audio features and interaction features for their experiments. However, we should note that non face-to-face deception detection emphasizes verbal and linguistic cues over less controllable nonverbal communication cues (Walther, 1996). 5.3 Subgroup Detection In online discussions, people usually split into subgroups based on various topics. The member of a subgroup is more likely to show positive attitude to the members of the same subgroup, and negative attitude to the members of opposing subgroups (Abu-Jbara et al., 2012a). Previous work also studied subgroup detection in social media sites. Abu-Jbara et al. (2012a) constructed a discussant attitude profile (DAP) for each discussant and then used clustering techniques to cluster their attitudes. Hassan et al. (2012; 2012b; 2013) proposed various methods to automatically construct a signed social network representation of discussions and then identify subgroups by partitioning their signed networks. Qiu et al. (2013) applied collaborative filtering through Probabilistic Matrix Factorization (PMF) to generalize and improve extracted opinion matrices. An underly</context>
</contexts>
<marker>Abu-Jbara, Diab, Dasigi, Radev, 2012</marker>
<rawString>A. Abu-Jbara, M. Diab, P. Dasigi, and D. Radev. 2012a. Subgroup detection in ideological discussions. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abu-Jbara</author>
<author>A Hassan</author>
<author>D Radev</author>
</authors>
<title>Attitudeminer: mining attitude from online discussions.</title>
<date>2012</date>
<booktitle>In Proc. North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT</booktitle>
<contexts>
<context position="14044" citStr="Abu-Jbara et al., 2012" startWordPosition="2387" endWordPosition="2390"> profile for each player and a signed network based on the attitude tuple list in Section 2, and combine them to analyze a dynamic network with discussants telling lies and truths. 3.1 Clustering based on Attitude Profile We use a vector containing numerical values to represent each player’s attitude toward identified targets in each round. The values correspond to the polarity scores in a player’s attitude tuple list. For example, the polarity score of player 16’s attitude toward target 11 is −1 as shown in Figure 2. 859 We call this vector as the discussant attitude profile (DAP) following (Abu-Jbara et al., 2012a). Suppose there are n players who participate in a single game. Since a player’s identity is not exposed to the public after his death4, people can still analyze the identity of a “dead” player. Therefore, the number of possibly mentioned targets in each round equals to n. Given all the statements from m surviving players in a single round, each player’s DAP has n + 1 dimensions including his vote and thus we can have a m x (n + 1) attitude matrix A where Aij represents the attitude polarity of i toward j we got from Section 2. Ai(n+1) represents i’s vote. In a certain round, given a set of </context>
<context position="15428" citStr="Abu-Jbara et al., 2012" startWordPosition="2655" endWordPosition="2658"> and get an m x m distance matrix M: n tuuv Mij = X (Aik − Ajk)2 + (2 − 2δAi(n+1),Aj(n+1))2 k=1 (1) The Kronecker delta function δ is: ~ 1 i = j δij = 2 0 i #j () We use this function to compare the votes of two players separately because a player’s vote can be inconsistent with his previous statements. We assume that there is a larger distance between two players when they vote for different suspects. A common assumption in previous research was that a member is more likely to show a positive attitude toward other members in the same group, and a negative attitude toward the opposing groups (Abu-Jbara et al., 2012a). However, a deceiver may pretend to be innocent by supporting those truth-tellers and attacking his teammates, whose identities have already been exposed. Therefore, it is not enough to judge the relationship between two players by simply measuring the distance between their DAPs. In addition to comparing DAPs between players i and j, we also consider the attitudes of other players toward i and j, as well as their attitudes 4Each round, the player killed by killers and the player with the most votes are out. toward each other. We modify Mij as follows and show it in Figure 3: (Aki − Akj)2 +</context>
<context position="27733" citStr="Abu-Jbara et al., 2012" startWordPosition="4931" endWordPosition="4934">In some cases, the performance remains the same when EDAPC clustering result is already optimal with the minimum value of the criterion function. 5 Related Work 5.1 Opinion Analysis Our work on mining a player’s attitude toward other players is related to opinion mining. Attitudes and opinions are related and can be regarded as the same in our task. Compared with the previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used dee</context>
<context position="29749" citStr="Abu-Jbara et al., 2012" startWordPosition="5245" endWordPosition="5248">set by audio-visual recording 8 games played by 2 groups of people face-to-face and extracted audio features and interaction features for their experiments. However, we should note that non face-to-face deception detection emphasizes verbal and linguistic cues over less controllable nonverbal communication cues (Walther, 1996). 5.3 Subgroup Detection In online discussions, people usually split into subgroups based on various topics. The member of a subgroup is more likely to show positive attitude to the members of the same subgroup, and negative attitude to the members of opposing subgroups (Abu-Jbara et al., 2012a). Previous work also studied subgroup detection in social media sites. Abu-Jbara et al. (2012a) constructed a discussant attitude profile (DAP) for each discussant and then used clustering techniques to cluster their attitudes. Hassan et al. (2012; 2012b; 2013) proposed various methods to automatically construct a signed social network representation of discussions and then identify subgroups by partitioning their signed networks. Qiu et al. (2013) applied collaborative filtering through Probabilistic Matrix Factorization (PMF) to generalize and improve extracted opinion matrices. An underly</context>
</contexts>
<marker>Abu-Jbara, Hassan, Radev, 2012</marker>
<rawString>A. Abu-Jbara, A. Hassan, and D. Radev. 2012b. Attitudeminer: mining attitude from online discussions. In Proc. North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abu-Jbara</author>
<author>B King</author>
<author>M Diab</author>
<author>D Radev</author>
</authors>
<title>Identifying opinion subgroups in arabic online discussions.</title>
<date>2013</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL</booktitle>
<marker>Abu-Jbara, King, Diab, Radev, 2013</marker>
<rawString>A. Abu-Jbara, B. King, M. Diab, and D. Radev. 2013. Identifying opinion subgroups in arabic online discussions. In Proc. Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Asch</author>
</authors>
<title>Effects of group pressure upon the modification and distortion of judgments. Groups, leadership, and men.</title>
<date>1951</date>
<publisher>S.</publisher>
<contexts>
<context position="25815" citStr="Asch, 1951" startWordPosition="4601" endWordPosition="4602"> human voting. The human performance is not satisfying, which indicates it’s very challenging even for a human to identify a deceiver whose deceptive statement is mixed with plenty of truthful opinions (Xu and Zhao, 2012). Figure 6: An overview of the average performance of all the methods. 4.4 Dynamic Subgroup Detection As shown in Figure 7, the performance of our approach improves as the game proceeds. Players seldom maintain their opinions throughout a game. Figure 2 shows that most killers (16,1,12) insisted that citizen 11 should be a killer except 7. As a response to the group pressure (Asch, 1951), 7 changed his opinion and stated that 11 could be a killer in the following round. In reality, a discussant who participates in an online discussion tends to change his opinions about a target as he learns more information, which shows both the necessity and importance of the dynamic detection of subgroups. Our method can be applied to detect subgroups dynamically by grouping posts into multiple discussion “rounds” based on their timestamps. Purity Entropy Figure 7: Average performance based on different rounds. 70 % 65 85 80 75 Purity Entropy 60 55 50 Method % 80 70 60 50 1st round 1st + 2n</context>
</contexts>
<marker>Asch, 1951</marker>
<rawString>S. Asch. 1951. Effects of group pressure upon the modification and distortion of judgments. Groups, leadership, and men. S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>R Steinberger</author>
<author>E Goot</author>
<author>B Pouliquen</author>
<author>M Kabadjov</author>
</authors>
<title>Opinion mining on newspaper quotations.</title>
<date>2009</date>
<booktitle>In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT</booktitle>
<contexts>
<context position="7638" citStr="Balahur et al., 2009" startWordPosition="1191" endWordPosition="1195">vestigate conversations and deceptive groups for computerized deception detection. • The proposed clustering technique is shown to be successful in separating deceptive groups from truth-tellers. • The method can be applied to dynamically detect subgroups in a network with discussants who tend to change their opinions. 2 Attitude Identification In this section, we describe how we take a player’s statement in a single round as input to extract his attitudes toward other players and represent them by an attitude 3-tuple (speaker, target, polarity) list. For this work, the polarity of attitudes (Balahur et al., 2009) can be positive (1), negative (-1) or neutral (0). A game log from a single round 858 will be used as our illustrative example, as shown Figure 2: Killer game sampe log (1st round) in Figure 2. C: CITIZEN; D: DETECTIVE; K: KILLER System: First Round. System: 15 was killed last night. 15, please leave your last words. 15(C): I’m a citizen. Over. 16(K): I’m a good person. 11 and 2 are suspicious. 1(K): I’m a good person. It has been a long time since I played as a killer. I’m a citizen. 11 is suspicious and I don’t want to comment on 16’s statement. 2(C): I’m a detective. 6 was proved as a kill</context>
</contexts>
<marker>Balahur, Steinberger, Goot, Pouliquen, Kabadjov, 2009</marker>
<rawString>A. Balahur, R. Steinberger, E. Goot, B. Pouliquen, and M. Kabadjov. 2009. Opinion mining on newspaper quotations. In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bandura</author>
</authors>
<title>Social Learning Theory. General Learning Corporation.</title>
<date>1971</date>
<contexts>
<context position="2408" citStr="Bandura, 1971" startWordPosition="344" endWordPosition="345"> 2011). 1The data set is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/killer.zip However, when we want to detect deception from text or voice conversations, the deception behavior may be affected by the following factors beyond textual statements. 1. Dynamic. Recent research in social science suggests that deception communication is dynamic and involves interactions among people (e.g., (Buller and Burgoon, 1996)). Additionally, the research postulates that human’s capacity to learn by observation enables him to acquire large, integrated units of behavior by example (Bandura, 1971). Therefore, a person’s behavior concerning deception or truth-telling can change constantly, while he learns from others’ statements during conversations. 2. Global. People may form groups for purpose of deception. Research in social psychology has shown that an individual’s object-related behavior may be affected by the attitudes of other people due to group dynamics (Friedkin, 2010). Recent studies typically have been conducted over “static” written or oral deceptive statements. There is no obligatory requirement for communication between the author and the readers of these statements (Yanc</context>
<context position="30655" citStr="Bandura, 1971" startWordPosition="5385" endWordPosition="5386">automatically construct a signed social network representation of discussions and then identify subgroups by partitioning their signed networks. Qiu et al. (2013) applied collaborative filtering through Probabilistic Matrix Factorization (PMF) to generalize and improve extracted opinion matrices. An underlying assumption of the previous work was that a participant will not tell lies nor hide his own stance. Moreover, their work did not take into account that a person’s attitude or stance will change as he learns more by reading the comments from others and acquiring more background knowledge (Bandura, 1971). Our contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. 6 Conclusions and Future Work Using the killer game as a case study, we present an effective clustering method to detect subgroups from dynamic conversations with lies and truths. This is the first work to utilize the dynamics of group conversations for deception detection. Experiments demonstrated that truth-tellers and deceptive groups are separable and the </context>
</contexts>
<marker>Bandura, 1971</marker>
<rawString>A. Bandura. 1971. Social Learning Theory. General Learning Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Banerjee</author>
<author>S Feng</author>
<author>J Kang</author>
<author>Y Choi</author>
</authors>
<title>Keystroke patterns as prosody in digital writings: A case study with deceptive reviews and essays.</title>
<date>2014</date>
<booktitle>In Proc. Empirical Methods on Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="28431" citStr="Banerjee et al. (2014)" startWordPosition="5042" endWordPosition="5045">atisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task scenario and obtained many interesting findings (e.g., deceivers send fewer messages than truth-tellers). Our work is most related to the work of Chittaranjan and Hung (2010) on detecting deceptive roles</context>
</contexts>
<marker>Banerjee, Feng, Kang, Choi, 2014</marker>
<rawString>R. Banerjee, S. Feng, J. Kang, and Y. Choi. 2014. Keystroke patterns as prosody in digital writings: A case study with deceptive reviews and essays. In Proc. Empirical Methods on Natural Language Processing (EMNLP 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Buller</author>
<author>J Burgoon</author>
</authors>
<title>Interpersonal deception theory. Communication theory.</title>
<date>1996</date>
<contexts>
<context position="2236" citStr="Buller and Burgoon, 1996" startWordPosition="316" endWordPosition="319">isolation (e.g., a product review). The promoters of a product may post fake complimentary reviews, while their competitors may hire people to write fake negative reviews (Ott et al., 2011). 1The data set is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/killer.zip However, when we want to detect deception from text or voice conversations, the deception behavior may be affected by the following factors beyond textual statements. 1. Dynamic. Recent research in social science suggests that deception communication is dynamic and involves interactions among people (e.g., (Buller and Burgoon, 1996)). Additionally, the research postulates that human’s capacity to learn by observation enables him to acquire large, integrated units of behavior by example (Bandura, 1971). Therefore, a person’s behavior concerning deception or truth-telling can change constantly, while he learns from others’ statements during conversations. 2. Global. People may form groups for purpose of deception. Research in social psychology has shown that an individual’s object-related behavior may be affected by the attitudes of other people due to group dynamics (Friedkin, 2010). Recent studies typically have been con</context>
</contexts>
<marker>Buller, Burgoon, 1996</marker>
<rawString>D. Buller and J. Burgoon. 1996. Interpersonal deception theory. Communication theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David B Buller</author>
<author>Judee K Burgoon</author>
<author>JA Daly</author>
<author>JM Wiemann</author>
</authors>
<title>Deception: Strategic and nonstrategic communication. Strategic interpersonal communication.</title>
<date>1994</date>
<contexts>
<context position="1339" citStr="Buller et al., 1994" startWordPosition="175" endWordPosition="178">seek to identify deceptive groups from their conversations. We propose a novel subgroup detection method that combines linguistic signals and signed network analysis for dynamic clustering. A social-elimination game called Killer Game is introduced as a case study1. Experimental results demonstrate that our approach significantly outperforms human voting and state-of-theart subgroup detection methods at dynamically differentiating the deceptive groups from truth-tellers. 1 Introduction Deception generally entails messages and information intentionally transmitted to create a false conclusion (Buller et al., 1994). Deception detection is an important task for a wide range of applications including law enforcement, intelligence gathering, and financial fraud. Most of the previous work (e.g., (Ott et al., 2011; Feng et al., 2012)) focused on content analysis of a single document in isolation (e.g., a product review). The promoters of a product may post fake complimentary reviews, while their competitors may hire people to write fake negative reviews (Ott et al., 2011). 1The data set is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/killer.zip However, when we want to detect decep</context>
</contexts>
<marker>Buller, Burgoon, Daly, Wiemann, 1994</marker>
<rawString>David B Buller, Judee K Burgoon, JA Daly, and JM Wiemann. 1994. Deception: Strategic and nonstrategic communication. Strategic interpersonal communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chittaranjan</author>
<author>H Hung</author>
</authors>
<title>Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game.</title>
<date>2010</date>
<booktitle>In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP</booktitle>
<contexts>
<context position="29002" citStr="Chittaranjan and Hung (2010)" startWordPosition="5125" endWordPosition="5129">to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task scenario and obtained many interesting findings (e.g., deceivers send fewer messages than truth-tellers). Our work is most related to the work of Chittaranjan and Hung (2010) on detecting deceptive roles in the Werewolf Game which is another variant of the killer game. They created a Werewolf data set by audio-visual recording 8 games played by 2 groups of people face-to-face and extracted audio features and interaction features for their experiments. However, we should note that non face-to-face deception detection emphasizes verbal and linguistic cues over less controllable nonverbal communication cues (Walther, 1996). 5.3 Subgroup Detection In online discussions, people usually split into subgroups based on various topics. The member of a subgroup is more likel</context>
</contexts>
<marker>Chittaranjan, Hung, 2010</marker>
<rawString>G. Chittaranjan and H. Hung. 2010. Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game. In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Doreian</author>
<author>A Mrvar</author>
</authors>
<title>A partitioning approach to structural balance. Social networks.</title>
<date>1996</date>
<contexts>
<context position="17484" citStr="Doreian and Mrvar, 1996" startWordPosition="3041" endWordPosition="3045">elong to different groups. Thus, we propose to capture the interactions in the social network to further improve the attitude-profile-based clustering result. We can easily convert the attitude matrix A into a signed network by adding a directed edge i —* j between i and j if Aij =� 0. We denote a directed graph corresponding to a signed network as G = (V, 5, N, W), where V is the set of nodes, 5 is the set of positive edges, N is the set of negative edges and W : (V x V ) —* {−1, 11 is a function that maps every directed edge to a value, W(i, j) = Aij. We use a greedy optimization algorithm (Doreian and Mrvar, 1996) to find partitions. A criterion function for an optimal partitioning procedure 0 Mij = Mij + v u u Xm tk=1 𝑗 𝑖 860 is constructed such that positive links are dense within groups and negative links are dense between groups. For any potential partition C, we seek to minimize the following error function: (4) where -y E [0, 1] controls the balance of the penalty difference between putting a positive edge across and a negative edge within a group. We regard these two types of errors as equally important and set -y = 0.5 for our experiments. Initially, we use the clustering result in Section 3.1 </context>
</contexts>
<marker>Doreian, Mrvar, 1996</marker>
<rawString>P. Doreian and A. Mrvar. 1996. A partitioning approach to structural balance. Social networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Feng</author>
<author>G Hirst</author>
</authors>
<title>Detecting deceptive opinions with profile compatibility.</title>
<date>2013</date>
<booktitle>In Proc. International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="28227" citStr="Feng and Hirst (2013)" startWordPosition="5010" endWordPosition="5013">to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task</context>
</contexts>
<marker>Feng, Hirst, 2013</marker>
<rawString>V. Feng and G. Hirst. 2013. Detecting deceptive opinions with profile compatibility. In Proc. International Joint Conference on Natural Language Processing (IJCNLP 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feng</author>
<author>R Banerjee</author>
<author>Y Choi</author>
</authors>
<title>Syntactic stylometry for deception detection.</title>
<date>2012</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1557" citStr="Feng et al., 2012" startWordPosition="212" endWordPosition="215">ed Killer Game is introduced as a case study1. Experimental results demonstrate that our approach significantly outperforms human voting and state-of-theart subgroup detection methods at dynamically differentiating the deceptive groups from truth-tellers. 1 Introduction Deception generally entails messages and information intentionally transmitted to create a false conclusion (Buller et al., 1994). Deception detection is an important task for a wide range of applications including law enforcement, intelligence gathering, and financial fraud. Most of the previous work (e.g., (Ott et al., 2011; Feng et al., 2012)) focused on content analysis of a single document in isolation (e.g., a product review). The promoters of a product may post fake complimentary reviews, while their competitors may hire people to write fake negative reviews (Ott et al., 2011). 1The data set is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/killer.zip However, when we want to detect deception from text or voice conversations, the deception behavior may be affected by the following factors beyond textual statements. 1. Dynamic. Recent research in social science suggests that deception communication is d</context>
<context position="28175" citStr="Feng et al., 2012" startWordPosition="5002" endWordPosition="5005">s and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical stu</context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic stylometry for deception detection. In Proc. Association for Computational Linguistics (ACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N E Friedkin</author>
</authors>
<title>The attitude-behavior linkage in behavioral cascades. Social Psychology Quarterly.</title>
<date>2010</date>
<contexts>
<context position="2796" citStr="Friedkin, 2010" startWordPosition="400" endWordPosition="402">ractions among people (e.g., (Buller and Burgoon, 1996)). Additionally, the research postulates that human’s capacity to learn by observation enables him to acquire large, integrated units of behavior by example (Bandura, 1971). Therefore, a person’s behavior concerning deception or truth-telling can change constantly, while he learns from others’ statements during conversations. 2. Global. People may form groups for purpose of deception. Research in social psychology has shown that an individual’s object-related behavior may be affected by the attitudes of other people due to group dynamics (Friedkin, 2010). Recent studies typically have been conducted over “static” written or oral deceptive statements. There is no obligatory requirement for communication between the author and the readers of these statements (Yancheva and Rudzicz, 2013). As a result, a victim of deception tends to trust the story mainly based on the statement he reads (Ott et al., 2011). However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. A deceiver may make a statement, which is partially true in order to deceive or mislead victims and adjust his deceptive strategies ba</context>
</contexts>
<marker>Friedkin, 2010</marker>
<rawString>N. E. Friedkin. 2010. The attitude-behavior linkage in behavioral cascades. Social Psychology Quarterly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Granhag</author>
<author>L Str¨omwall</author>
</authors>
<title>Repeated interrogations: verbal and non-verbal cues to deception. Applied Cognitive Psychology.</title>
<date>2002</date>
<marker>Granhag, Str¨omwall, 2002</marker>
<rawString>P. Granhag and L. Str¨omwall. 2002. Repeated interrogations: verbal and non-verbal cues to deception. Applied Cognitive Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hassan</author>
<author>A Abu-Jbara</author>
<author>D Radev</author>
</authors>
<title>Detecting subgroups in online discussions by modeling positive and negative relations among participants.</title>
<date>2012</date>
<booktitle>In Proc. Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<contexts>
<context position="24538" citStr="Hassan et al., 2012" startWordPosition="4373" endWordPosition="4376">ginal DAPC method, for each opinion target, there are 3 dimensions in the feature vector, corresponding to (1) the number of positive expressions, (2) negative expressions toward the target from the online posts and (3) the number of times the discussant mentioned the target. For our experiment, we only keep one dimension representing the discussant’s attitude (positive, negative, neutral) toward the target since a discussant attitude remains the same in his statement within a single round. 2. Network: We also implemented the signed network partition method for subgroup detection proposed by (Hassan et al., 2012). To determine the number of subgroups t, we set an upper limit of t = 3 in order to minimize the optimization function. START END 2 1 TEAM CHANNEL 3 OUT Current Speaker: 14 PUBLIC CHANNEL 4 Entropy = − �j nj n 862 3. Human Voting: We also compare our methods with human voting results. There are two subgroups based on the voting results. The players with the highest votes each round belong to one subgroup and the rest of the players are in the other subgroup. Table 1 shows the overall performance of various methods on subgroup detection and Figure 6 depicts the average performance. We can see </context>
<context position="29998" citStr="Hassan et al. (2012" startWordPosition="5282" endWordPosition="5285">tic cues over less controllable nonverbal communication cues (Walther, 1996). 5.3 Subgroup Detection In online discussions, people usually split into subgroups based on various topics. The member of a subgroup is more likely to show positive attitude to the members of the same subgroup, and negative attitude to the members of opposing subgroups (Abu-Jbara et al., 2012a). Previous work also studied subgroup detection in social media sites. Abu-Jbara et al. (2012a) constructed a discussant attitude profile (DAP) for each discussant and then used clustering techniques to cluster their attitudes. Hassan et al. (2012; 2012b; 2013) proposed various methods to automatically construct a signed social network representation of discussions and then identify subgroups by partitioning their signed networks. Qiu et al. (2013) applied collaborative filtering through Probabilistic Matrix Factorization (PMF) to generalize and improve extracted opinion matrices. An underlying assumption of the previous work was that a participant will not tell lies nor hide his own stance. Moreover, their work did not take into account that a person’s attitude or stance will change as he learns more by reading the comments from other</context>
</contexts>
<marker>Hassan, Abu-Jbara, Radev, 2012</marker>
<rawString>A. Hassan, A. Abu-Jbara, and D. Radev. 2012. Detecting subgroups in online discussions by modeling positive and negative relations among participants. In Proc. Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proc. ACL-COLING 2006 Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="27539" citStr="Kim and Hovy, 2006" startWordPosition="4898" endWordPosition="4901">es that they are less likely to be teammates. We can successfully separate them after re-computing the distance between them. Adding network information provided 5.7% further gain in Purity. In some cases, the performance remains the same when EDAPC clustering result is already optimal with the minimum value of the criterion function. 5 Related Work 5.1 Opinion Analysis Our work on mining a player’s attitude toward other players is related to opinion mining. Attitudes and opinions are related and can be regarded as the same in our task. Compared with the previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features </context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>S. Kim and E. Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proc. ACL-COLING 2006 Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>C Cardie</author>
<author>S Li</author>
</authors>
<title>Topicspam: a topic-model based approach for spam detection.</title>
<date>2013</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="28094" citStr="Li et al., 2013" startWordPosition="4988" endWordPosition="4991">e previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated comm</context>
</contexts>
<marker>Li, Cardie, Li, 2013</marker>
<rawString>J. Li, C. Cardie, and S. Li. 2013a. Topicspam: a topic-model based approach for spam detection. In Proc. Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>M Ott</author>
<author>C Cardie</author>
</authors>
<title>Identifying manipulated offerings on review portals.</title>
<date>2013</date>
<booktitle>In Proc. Empirical Methods on Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="28094" citStr="Li et al., 2013" startWordPosition="4988" endWordPosition="4991">e previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated comm</context>
</contexts>
<marker>Li, Ott, Cardie, 2013</marker>
<rawString>J. Li, M. Ott, and C. Cardie. 2013b. Identifying manipulated offerings on review portals. In Proc. Empirical Methods on Natural Language Processing (EMNLP 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to information retrieval. Cambridge university press Cambridge.</title>
<date>2008</date>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>C. Manning, P. Raghavan, and H. Sch¨utze. 2008. Introduction to information retrieval. Cambridge university press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ott</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>J Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1537" citStr="Ott et al., 2011" startWordPosition="208" endWordPosition="211">mination game called Killer Game is introduced as a case study1. Experimental results demonstrate that our approach significantly outperforms human voting and state-of-theart subgroup detection methods at dynamically differentiating the deceptive groups from truth-tellers. 1 Introduction Deception generally entails messages and information intentionally transmitted to create a false conclusion (Buller et al., 1994). Deception detection is an important task for a wide range of applications including law enforcement, intelligence gathering, and financial fraud. Most of the previous work (e.g., (Ott et al., 2011; Feng et al., 2012)) focused on content analysis of a single document in isolation (e.g., a product review). The promoters of a product may post fake complimentary reviews, while their competitors may hire people to write fake negative reviews (Ott et al., 2011). 1The data set is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/killer.zip However, when we want to detect deception from text or voice conversations, the deception behavior may be affected by the following factors beyond textual statements. 1. Dynamic. Recent research in social science suggests that deceptio</context>
<context position="3150" citStr="Ott et al., 2011" startWordPosition="457" endWordPosition="460">atements during conversations. 2. Global. People may form groups for purpose of deception. Research in social psychology has shown that an individual’s object-related behavior may be affected by the attitudes of other people due to group dynamics (Friedkin, 2010). Recent studies typically have been conducted over “static” written or oral deceptive statements. There is no obligatory requirement for communication between the author and the readers of these statements (Yancheva and Rudzicz, 2013). As a result, a victim of deception tends to trust the story mainly based on the statement he reads (Ott et al., 2011). However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. A deceiver may make a statement, which is partially true in order to deceive or mislead victims and adjust his deceptive strategies based on the reactions of victims (Zhou et al., 2004). Therefore, it is more challenging to identity a deceiver in an interactive process of deception. Most deception detection research addressed individual deceivers, but deceivers often act in pairs or larger groups (Vrij et al., 2010). The interac857 Proceedings of the 53rd Annual Meeting of the Associ</context>
<context position="28156" citStr="Ott et al., 2011" startWordPosition="4998" endWordPosition="5001">, the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conduct</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>M. Ott, Y. Choi, C. Cardie, and J. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proc. Association for Computational Linguistics (ACL 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Qiu</author>
<author>B Liu</author>
<author>J Bu</author>
<author>C Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation. Computational linguistics.</title>
<date>2011</date>
<contexts>
<context position="27518" citStr="Qiu et al., 2011" startWordPosition="4894" endWordPosition="4897">h 7, which indicates that they are less likely to be teammates. We can successfully separate them after re-computing the distance between them. Adding network information provided 5.7% further gain in Purity. In some cases, the performance remains the same when EDAPC clustering result is already optimal with the minimum value of the criterion function. 5 Related Work 5.1 Opinion Analysis Our work on mining a player’s attitude toward other players is related to opinion mining. Attitudes and opinions are related and can be regarded as the same in our task. Compared with the previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and </context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Qiu</author>
<author>L Yang</author>
<author>J Jiang</author>
</authors>
<title>Mining user relations from online discussions using sentiment analysis and probabilistic matrix factorization.</title>
<date>2013</date>
<booktitle>In Proc. North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT</booktitle>
<contexts>
<context position="30203" citStr="Qiu et al. (2013)" startWordPosition="5312" endWordPosition="5315">up is more likely to show positive attitude to the members of the same subgroup, and negative attitude to the members of opposing subgroups (Abu-Jbara et al., 2012a). Previous work also studied subgroup detection in social media sites. Abu-Jbara et al. (2012a) constructed a discussant attitude profile (DAP) for each discussant and then used clustering techniques to cluster their attitudes. Hassan et al. (2012; 2012b; 2013) proposed various methods to automatically construct a signed social network representation of discussions and then identify subgroups by partitioning their signed networks. Qiu et al. (2013) applied collaborative filtering through Probabilistic Matrix Factorization (PMF) to generalize and improve extracted opinion matrices. An underlying assumption of the previous work was that a participant will not tell lies nor hide his own stance. Moreover, their work did not take into account that a person’s attitude or stance will change as he learns more by reading the comments from others and acquiring more background knowledge (Bandura, 1971). Our contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. W</context>
</contexts>
<marker>Qiu, Yang, Jiang, 2013</marker>
<rawString>M. Qiu, L. Yang, and J. Jiang. 2013. Mining user relations from online discussions using sentiment analysis and probabilistic matrix factorization. In Proc. North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proc. Joint Conference of the Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="12010" citStr="Somasundaran and Wiebe, 2009" startWordPosition="2019" endWordPosition="2022">if they satify at least one dependency tence with it. Otherwise, if “I” is the only subject rules (e.g.(Somasundarn and Wiebe, 2009)). We orthere are no subjects at all, we associate atticheck the POS tag sequence between them For tude words wih the ID of the spaker. We reverse each attitude-target pair if there exist an attitude the polrity o an atitude word if it appears in a word a beleforiented verb such as “think” “be negation context. lieve” “feel” or Previous methods pair a target and an attitude qnce w ill diad thi pai Th aptio word if they satisfy at least one dependency rule (e.g., (Somasundaran and Wiebe, 2009)). We is tht POS tag sequeces can be ed to smma check the POS tag squence between hem. For rize dependency rules when statements are rela each attitude-target pair, if there exists an attitude tively short. word, a belief-oriented verb such as “think”, “beFo hose targets the speaker didn’t metion lieve”, “feel”, or more than two verbs in the seor thee is no positive/negatve atitude word used quence, we will discard this pair. The assumption whn they a tid he itde plrity is that POS tag sequences can be used to summarize dependency rules when statements are relacor is set to 0. For instance giv</context>
<context position="27709" citStr="Somasundaran and Wiebe, 2009" startWordPosition="4926" endWordPosition="4930"> 5.7% further gain in Purity. In some cases, the performance remains the same when EDAPC clustering result is already optimal with the minimum value of the criterion function. 5 Related Work 5.1 Opinion Analysis Our work on mining a player’s attitude toward other players is related to opinion mining. Attitudes and opinions are related and can be regarded as the same in our task. Compared with the previous work (e.g.,(Qiu et al., 2011; Kim and Hovy, 2006)), the opinion words and targets in our task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu </context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>S. Somasundaran and J. Wiebe. 2009. Recognizing stances in online debates. In Proc. Joint Conference of the Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steinbach</author>
<author>G Karypis</author>
<author>V Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<booktitle>In Proc. KDD</booktitle>
<contexts>
<context position="23208" citStr="Steinbach et al., 2000" startWordPosition="4145" endWordPosition="4148">d Entropy. Purity (Manning et al., 2008) is a metric in which each cluster is assigned to the class with the majority vote in the cluster, and then the accuracy of this assignment is measured by dividing the number of correctly assigned instances by the total number of instances N. More formally: 1 � maxj|wk n cj |(6) purity(Q, C) = N k where Q = {w1, w2, · · · , wk} is the set of clusters and C = {c1, c2, · · · , cj} is the set of classes. wk is interpreted as the set of instances in wk and cj is the set of instances in cj. The purity increases as the quality of clustering improves. Entropy (Steinbach et al., 2000) measures the uniformity of a cluster. The entropy for all clusters is defined by the weighted sum of the entropy of each cluster: i E P(i, j) x log2P(i, j) (7) where P(i, j) is the probability of finding an element from the category i in the cluster j, nj is the number of items in cluster j and n is the total number of items in the distribution. The entropy decreases as the quality of clustering improves. 4.3 Overall Performance We compare our approach with two state-of-theart subgroup detection methods and human performance as follows: 1. DAPC: In Section 3.1, we introduced our implementatio</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>M. Steinbach, G. Karypis, V. Kumar, et al. 2000. A comparison of document clustering techniques. In Proc. KDD 2000 workshop on text mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Implementing agglomerative hierarchic clustering algorithms for use in document retrieval.</title>
<date>1986</date>
<journal>Information Processing &amp; Management.</journal>
<contexts>
<context position="16318" citStr="Voorhees, 1986" startWordPosition="2816" endWordPosition="2817">eir DAPs. In addition to comparing DAPs between players i and j, we also consider the attitudes of other players toward i and j, as well as their attitudes 4Each round, the player killed by killers and the player with the most votes are out. toward each other. We modify Mij as follows and show it in Figure 3: (Aki − Akj)2 + (h(Aij) + h(Aji))2 (3) where the function h detects the negative attitudes. h(x) = 0 if x &gt; 0 and h(x) = −1 otherwise. We perform hierarchical clustering on the condensed distance matrix of M and use the complete linkage method to compute the distance between two clusters (Voorhees, 1986). We set the number of clusters as 3 since there are three natural groups in the game. We focus on separating deceivers (killers) from truth-tellers (citizens and detectives). 𝑖 𝑗 compare 𝑖 and 𝑗′s DAPs Figure 3: Computation of the distance between player i and j based on the attitude matrix. 3.2 Signed Network Partition When we computed the distance between two players in Section 3.1, we did not consider the network structure among all the players. For example, if A supports C, B supports D and C and D dislike each other, A and B may belong to different groups. Thus, we propose to capture the</context>
<context position="20240" citStr="Voorhees, 1986" startWordPosition="3567" endWordPosition="3568"> are the number of times that player i and j are assigned to the same cluster in P and P0 respectively. rij denotes the number of rounds when both of them survived (rij G r). Tirj E [0, 1]. We assign a higher weight to the result of P1 and set A = 2/3 in our experiments. Given the input in Figure 2, x3 and x4 are assigned to the same cluster in P1 (vote34 = 1) and inP10 (vote034 = 1) respectively as shown in Figure 4. x3 and x4 co-occurred in the first round (r34 = 1). T134 = (2/3 x 1 + 1/3 x 1)/1 = 1. Figure 4: Example of cluster ensemble for a single round. We apply hierarchical clustering (Voorhees, 1986) to the similarity matrix above to obtain the final global clustering results. 4 Experiments 4.1 Dataset Construction We recorded 10 games from 3J3F6, one of the most popular Chinese online killer game websites 7. A screenshot of the game system interface is shown in Figure 5. There are 16 participating players per game: 4 detectives, 4 killers and 8 citizens. Each player occupies a position in 1 . All the surviving players can express their attitudes via a voice channel using 2 , while detectives and killers can also communicate with teammates in their respective private team channels 3 via t</context>
</contexts>
<marker>Voorhees, 1986</marker>
<rawString>E. Voorhees. 1986. Implementing agglomerative hierarchic clustering algorithms for use in document retrieval. Information Processing &amp; Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vrij</author>
<author>P Granhag</author>
<author>S Porter</author>
</authors>
<title>Pitfalls and opportunities in nonverbal and verbal lie detection.</title>
<date>2010</date>
<booktitle>Psychological Science in the Public Interest.</booktitle>
<contexts>
<context position="3681" citStr="Vrij et al., 2010" startWordPosition="543" endWordPosition="546">tion tends to trust the story mainly based on the statement he reads (Ott et al., 2011). However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. A deceiver may make a statement, which is partially true in order to deceive or mislead victims and adjust his deceptive strategies based on the reactions of victims (Zhou et al., 2004). Therefore, it is more challenging to identity a deceiver in an interactive process of deception. Most deception detection research addressed individual deceivers, but deceivers often act in pairs or larger groups (Vrij et al., 2010). The interac857 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 857–866, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Identify a player’s attitude toward other players based on his statement during each round           −1 1 1 −1 −1 1 −1 1 1 Clustering   Player Attitude Profile (each round) Subgroups   1 -1  1 -1 -1 -1  Partition   Signed Network (each round) Subgroups Cluster Ensembles  Subgroups   Figure 1: Decepti</context>
</contexts>
<marker>Vrij, Granhag, Porter, 2010</marker>
<rawString>A. Vrij, P. Granhag, and S. Porter. 2010. Pitfalls and opportunities in nonverbal and verbal lie detection. Psychological Science in the Public Interest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vrij</author>
</authors>
<title>Detecting lies and deceit: The psychology of lying and implications for professional practice.</title>
<date>2000</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="32485" citStr="Vrij, 2000" startWordPosition="5673" endWordPosition="5674">s study focuses on analyzing the verbal content in conversations. It will be interesting to study non-verbal features such as blink rate, gaze aversion and pauses (Granhag and Str¨omwall, 2002) when people play this game face-to-face and combine the non-verbal and verbal features for deception detection. In addition, it is worth exploring the impact of cross-cultural analysis in detecting deception. When attempting to detect deceit in people of other ethnic origin than themselves, people perform even worse in terms of lie detection accuracy than when judging people of their own ethnic origin (Vrij, 2000). For the future work, we aim to use automatic prediction of deceivers to help truth-tellers win games more easily. 864 Acknowledgement This work was supported by the U.S. DARPA DEFT Program No. FA8750-13-2-0041, ARL NS-CTA No. W911NF-09-2-0053, NSF Awards IIS-0953149 and IIS-1523198, AFRL DREAM project, gift awards from IBM, Google, Disney and Bosch. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce</context>
</contexts>
<marker>Vrij, 2000</marker>
<rawString>A. Vrij. 2000. Detecting lies and deceit: The psychology of lying and implications for professional practice. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Walther</author>
</authors>
<title>Computer-mediated communication impersonal, interpersonal, and hyperpersonal interaction. Communication research.</title>
<date>1996</date>
<contexts>
<context position="29455" citStr="Walther, 1996" startWordPosition="5198" endWordPosition="5199">obtained many interesting findings (e.g., deceivers send fewer messages than truth-tellers). Our work is most related to the work of Chittaranjan and Hung (2010) on detecting deceptive roles in the Werewolf Game which is another variant of the killer game. They created a Werewolf data set by audio-visual recording 8 games played by 2 groups of people face-to-face and extracted audio features and interaction features for their experiments. However, we should note that non face-to-face deception detection emphasizes verbal and linguistic cues over less controllable nonverbal communication cues (Walther, 1996). 5.3 Subgroup Detection In online discussions, people usually split into subgroups based on various topics. The member of a subgroup is more likely to show positive attitude to the members of the same subgroup, and negative attitude to the members of opposing subgroups (Abu-Jbara et al., 2012a). Previous work also studied subgroup detection in social media sites. Abu-Jbara et al. (2012a) constructed a discussant attitude profile (DAP) for each discussant and then used clustering techniques to cluster their attitudes. Hassan et al. (2012; 2012b; 2013) proposed various methods to automatically </context>
</contexts>
<marker>Walther, 1996</marker>
<rawString>J. Walther. 1996. Computer-mediated communication impersonal, interpersonal, and hyperpersonal interaction. Communication research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Xu</author>
<author>H Zhao</author>
</authors>
<title>Using deep linguistic features for finding deceptive opinion spam.</title>
<date>2012</date>
<booktitle>In Proc. International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="25425" citStr="Xu and Zhao, 2012" startWordPosition="4533" endWordPosition="4536">n voting results. There are two subgroups based on the voting results. The players with the highest votes each round belong to one subgroup and the rest of the players are in the other subgroup. Table 1 shows the overall performance of various methods on subgroup detection and Figure 6 depicts the average performance. We can see that our method significantly outperforms two baseline methods and human voting. The human performance is not satisfying, which indicates it’s very challenging even for a human to identify a deceiver whose deceptive statement is mixed with plenty of truthful opinions (Xu and Zhao, 2012). Figure 6: An overview of the average performance of all the methods. 4.4 Dynamic Subgroup Detection As shown in Figure 7, the performance of our approach improves as the game proceeds. Players seldom maintain their opinions throughout a game. Figure 2 shows that most killers (16,1,12) insisted that citizen 11 should be a killer except 7. As a response to the group pressure (Asch, 1951), 7 changed his opinion and stated that 11 could be a killer in the following round. In reality, a discussant who participates in an online discussion tends to change his opinions about a target as he learns mo</context>
<context position="28324" citStr="Xu and Zhao (2012)" startWordPosition="5025" endWordPosition="5028">009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task scenario and obtained many interesting findings (e.g., deceivers send fewer messages than truth-</context>
</contexts>
<marker>Xu, Zhao, 2012</marker>
<rawString>Q. Xu and H. Zhao. 2012. Using deep linguistic features for finding deceptive opinion spam. In Proc. International Conference on Computational Linguistics (COLING 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yancheva</author>
<author>F Rudzicz</author>
</authors>
<title>Automatic detection of deception in child-produced speech using syntactic complexity features.</title>
<date>2013</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3031" citStr="Yancheva and Rudzicz, 2013" startWordPosition="433" endWordPosition="436">971). Therefore, a person’s behavior concerning deception or truth-telling can change constantly, while he learns from others’ statements during conversations. 2. Global. People may form groups for purpose of deception. Research in social psychology has shown that an individual’s object-related behavior may be affected by the attitudes of other people due to group dynamics (Friedkin, 2010). Recent studies typically have been conducted over “static” written or oral deceptive statements. There is no obligatory requirement for communication between the author and the readers of these statements (Yancheva and Rudzicz, 2013). As a result, a victim of deception tends to trust the story mainly based on the statement he reads (Ott et al., 2011). However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. A deceiver may make a statement, which is partially true in order to deceive or mislead victims and adjust his deceptive strategies based on the reactions of victims (Zhou et al., 2004). Therefore, it is more challenging to identity a deceiver in an interactive process of deception. Most deception detection research addressed individual deceivers, but deceivers often</context>
<context position="28204" citStr="Yancheva and Rudzicz, 2013" startWordPosition="5006" endWordPosition="5009">r task are relatively easier to recognize due to the simplicity of statements. Some recent work (e.g., (Somasundaran and Wiebe, 2009; Abu-Jbara et al., 2012a)) developed syntactic rules to pair an opinion word and a target if they satisfy at least one specific dependency rule. We use POS tag sequences to efficiently help us filter out irrelevant pairs. 863 5.2 Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using th</context>
</contexts>
<marker>Yancheva, Rudzicz, 2013</marker>
<rawString>M. Yancheva and F. Rudzicz. 2013. Automatic detection of deception in child-produced speech using syntactic complexity features. In Proc. Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>H Yu</author>
<author>D Xiong</author>
<author>Q Liu</author>
</authors>
<title>Hhmmbased chinese lexical analyzer ictclas.</title>
<date>2003</date>
<booktitle>In Proc. SIGHAN 2003 workshop on Chinese language processing.</booktitle>
<contexts>
<context position="10130" citStr="Zhang et al., 2003" startWordPosition="1699" endWordPosition="1702">I’m a citizen. System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 · · · 10, 13, 5, 2 vote for 7 · · · 9, 6 vote for 11 · · · 2 is out. Figure 2: Killer game sample log (the 1st round). 21 Target and Attitude Word Identification 2.1 Target and Attitude Word Identification fro conversations. In the killer game, a target We start by identifying targets and attitude words from conversations. In the killer game, a target is represented by his unique ID2 and game terms are regarded as attitude words. We collected 41 terms in total from the game’s website 3 and related discussion forum posts. ICTCLAS (Zhang et al., 2003) is used for word segmentation and part-of-speech (POS) tagging. There are two kinds of game terms: positive and negative. Positive terms include “citizen”, “good person”, “good person certified by the detectives” and “detective”. Negative terms include “killer”, “killer verified by the detectives” and “a killer who claimed himself/herself to be a detective”. We assign the polarity score +1, -1 to positive and negative terms respectively. 2Each player has a game ID, assigned by the online game system based on when he entered the game room. 3e.g., http://www.3j3f.com/how/ 2.2 Attitude-Trget Pir</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>H. Zhang, H. Yu, D. Xiong, and Q. Liu. 2003. Hhmmbased chinese lexical analyzer ictclas. In Proc. SIGHAN 2003 workshop on Chinese language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>Y Sung</author>
</authors>
<title>Cues to deception in online chinese groups.</title>
<date>2008</date>
<booktitle>In Proc. Hawaii International Conference on System Sciences (HICSS</booktitle>
<contexts>
<context position="28748" citStr="Zhou and Sung (2008)" startWordPosition="5085" endWordPosition="5088">l features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task scenario and obtained many interesting findings (e.g., deceivers send fewer messages than truth-tellers). Our work is most related to the work of Chittaranjan and Hung (2010) on detecting deceptive roles in the Werewolf Game which is another variant of the killer game. They created a Werewolf data set by audio-visual recording 8 games played by 2 groups of people face-to-face and extracted audio features and interaction features for their experiments. However, we should note that non face-to-face deception detectio</context>
</contexts>
<marker>Zhou, Sung, 2008</marker>
<rawString>L. Zhou and Y. Sung. 2008. Cues to deception in online chinese groups. In Proc. Hawaii International Conference on System Sciences (HICSS 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>J Burgoon</author>
<author>J Nunamaker</author>
<author>D Twitchell</author>
</authors>
<title>Automating linguistics-based cues for detecting deception in text-based asynchronous computermediated communications. Group decision and negotiation.</title>
<date>2004</date>
<contexts>
<context position="3447" citStr="Zhou et al., 2004" startWordPosition="507" endWordPosition="510">een conducted over “static” written or oral deceptive statements. There is no obligatory requirement for communication between the author and the readers of these statements (Yancheva and Rudzicz, 2013). As a result, a victim of deception tends to trust the story mainly based on the statement he reads (Ott et al., 2011). However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. A deceiver may make a statement, which is partially true in order to deceive or mislead victims and adjust his deceptive strategies based on the reactions of victims (Zhou et al., 2004). Therefore, it is more challenging to identity a deceiver in an interactive process of deception. Most deception detection research addressed individual deceivers, but deceivers often act in pairs or larger groups (Vrij et al., 2010). The interac857 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 857–866, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Identify a player’s attitude toward other players based on his statement during each rou</context>
</contexts>
<marker>Zhou, Burgoon, Nunamaker, Twitchell, 2004</marker>
<rawString>L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell. 2004. Automating linguistics-based cues for detecting deception in text-based asynchronous computermediated communications. Group decision and negotiation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>