<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001331">
<title confidence="0.993161">
Testing the Efficacy of Part-of-Speech Information in Word Completion
</title>
<author confidence="0.960105">
Afsaneh Fazly and Graeme Hirst
</author>
<affiliation confidence="0.9990705">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.503819">
Toronto, Ontario, Canada M5S 3G4
</address>
<email confidence="0.998565">
{afsaneh, gh}@cs.toronto.edu
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887090909091">
We investigate the effect of incorporat-
ing syntactic information into a word-
completion algorithm. We introduce
two new algorithms that combine part-
of-speech tag trigrams with word hi-
grams, and evaluate them with a test-
bench constructed for the purpose. The
results show a small but statistically sig-
nificant improvement in keystroke sav-
ings for one of our algorithms over base-
lines that use only word n-grams.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867161290322">
A word-completion utility facilitates the typing of
text by a user with physical or cognitive disabil-
ities. As the user enters each keystroke, the pro-
gram displays a list of the most likely completions
of the partially typed word. As the user continues
to enter letters, the program updates the sugges-
tion list accordingly. If the intended word is in the
list, the user can select it with a single keystroke
or mouse-click. For a user with physical disabil-
ities, for whom each keystroke is an effort, this
saves time and energy; for a user with cognitive
disabilities, this can assist in the composition of
well-formed text. A number of word-completion
utilities are available commercially; but their sug-
gestions, which are based on n-gram frequencies,
are often syntactically or semantically implausi-
ble, excluding more-plausible but lower-frequency
possibilities from the list. This can be particu-
larly problematic for users with certain cognitive
disabilities, such as dyslexia, who often are easily
confused by inappropriate suggestions.
In this study, we explore the addition of syn-
tactic information to word completion, developing
new algorithms in which the part-of-speech tags
of words are used in addition to the words them-
selves to improve the accuracy of the suggestions.
We hypothesize that this will reduce the likelihood
of suggesting words that are syntactically inappro-
priate in context and hence will result in greater
savings. Details not presented here are given by
Fazly (2002).
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99989236">
Early word-completion programs, mostly devel-
oped in the 1980s, were based on very sim-
ple language models. They suggest high-
frequency words that match the partially typed
word, and ignore all the previous context;
e.g., Swiffin et al. (1985). To provide more-
appropriate suggestions, some programs look at
a larger context by exploiting word bigram or
trigram language models. WordQ (Nantais et
al., 2001; Shein et al., 2001), developed at the
Bloorview MacMillan Children&apos;s Centre, Toronto,
Profet (Carlberger et al., 1997a, 1997b), and a pro-
gram by Bentrup (1987) are statistical and adap-
tive programs that incorporate n-gram language
models. These programs employ information on
the user&apos;s recency and frequency of use of each
word to adapt to the user&apos;s typing behaviour.
The research closest to ours here is perhaps
that of VanDyke (1991), Wood (1996), and
Garay-Vitoria and Gonzalez-Abascal (1997), who
employed parsers, requiring a considerable
amount of work to reparse the partial input
sentence every time a new word is completed by
the user. Carlberger et al. (1997b) incorporate
part-of-speech tag information about words. They
</bodyText>
<page confidence="0.995516">
9
</page>
<bodyText confidence="0.999970814814815">
first obtain a probability estimate for the tag of
the next word and then suggest words using this
probability estimation and word bigram models.
Copestake (1997) used the part-of-speech tag
bigrams collected from a small and unbalanced
corpus, along with word recency information.
She compared the results with that of a simple
frequency-based algorithm.
FASTY, a multilingual word-prediction system
(Matiasek et al., 2002), and a prediction sys-
tem for Spanish developed by Palazuelos (2001),
both exploit syntactic knowledge in the form of
part-of-speech tag statistics and grammar. FASTY
uses collocation-based statistics to include long-
distance relationships among words; it is mostly
concerned with the prediction of nominal com-
pounds (e.g., in German). Palazuelos&apos;s system is
designed for Spanish and hence direct comparison
with our results below is not possible.
In this work, we introduce several new word-
completion algorithms that exploit part-of-speech
tag information about words, and we employ a
large, balanced corpus for training. And unlike
most previous work that used only one perfor-
mance measure, we introduce a number of mea-
sures to evaluate and compare different algorithms
in different experimental conditions.
</bodyText>
<sectionHeader confidence="0.99402" genericHeader="method">
3 Word-completion algorithms
</sectionHeader>
<bodyText confidence="0.9949715">
Suppose the user is typing a sentence and the fol-
lowing sequence has been entered so far:
</bodyText>
<listItem confidence="0.827968">
• • • Wi-2Wi-1N,„fi,
</listItem>
<bodyText confidence="0.9997189">
where wi_2 and wi_ I are the most recently com-
pleted words, and wi is a partially typed word.
Let W be the set of all words in the lexicon that be-
gin with the prefix wi . A word-completion al-
gorithm attempts to select the n most-appropriate
words from W that are likely to be the user&apos;s in-
tended word, where n is usually between I and 10.
The general approach is to estimate the probabil-
ity of each candidate word wi E W being the user&apos;s
intended word, given the context.
</bodyText>
<page confidence="0.766978">
3.1 Baselines: Unigram, Bigram, and WordQ
</page>
<bodyText confidence="0.998741875">
The Unigram algorithm simply suggests the n
highest frequency words of W. The Bigram al-
gorithm instead uses an estimation of the bigram
probability P(wi wi_i) to select the most likely
words for the desired sentence position. These are
our baseline algorithms. In addition, we compare
our results with the adaptive n-gram algorithm of
WordQ.
</bodyText>
<subsectionHeader confidence="0.999349">
3.2 Syntactic algorithms
</subsectionHeader>
<bodyText confidence="0.998539866666667">
We introduce two algorithms—Tags-and-Words
and Linear Combination—that use part-of-speech
tag information in addition to word n-grams in or-
der to maximize the likelihood of syntactic appro-
priateness of the suggestions. Both algorithms as-
sume the presence of a part-of-speech tagger that
annotates words with their most likely part-of-
speech tags incrementally as the user types them
in. In different ways, each attempts to estimate the
probability ti_1, ti_2), where t1 is the
part-of-speech tag of word w. These algorithms
are described in more detail in the following sec-
tions. (We also tried an algorithm that used only
part-of-speech tags; its performance was inferior
to that of the Bigram algorithm; see Fazly (2002).)
</bodyText>
<subsectionHeader confidence="0.592338">
3.2.1 Tags and Words
</subsectionHeader>
<bodyText confidence="0.9246215">
The Tags-and-Words algorithm combines
tag trigrams and word bigrams in a single
model. This algorithm estimates the probability
ti_1, ti_2) with the following formula:
</bodyText>
<equation confidence="0.990881166666667">
p(vvi wi_l, ti_i, ti_2)
EP(wi, t wi_i, ti_i, ti_2)
EP(wi wi_i, ti, ti_i, ti_2)
xP(ti wi_i, ti_i, ti_7)
EP(vvi wi_i, to x p(ti ti-2)
tiET(vvi)
</equation>
<bodyText confidence="0.999671">
where T(W) is the set of possible part-of-speech
tags for W.
The conditional independence assumptions
among random variables ti_?, ti_i, ti, wi_ 1, and
vv i are depicted in a Bayesian network in Figure 1.
Applying these conditional independence assump-
tions, the desired probability can be estimated by
the formula above.
</bodyText>
<page confidence="0.996621">
10
</page>
<figureCaption confidence="0.9908215">
Figure 1: Bayesian network of conditional inde-
pendence between words and part-of-speech tags.
</figureCaption>
<bodyText confidence="0.5978585">
To estimate P (wi wi_i, ti), we first rewrite it us-
ing Bayes&apos;s rule:
</bodyText>
<equation confidence="0.9999095">
P( wi wi-1 ti)
P(Wi_i ti Wi) x P(w)
P(wi_i,ti)
ti) x P(tilwi) x P(w)
x P(tilwi) x P(wi)
P(wi—i,ti)
</equation>
<bodyText confidence="0.915986">
P(wi_i,ti) and P(wi_i wi) are then rewritten
using Bayes&apos;s rule and P(ti wi_i) is replaced by
P(ti) assuming the conditional independence of ti
and wi_i given :
</bodyText>
<equation confidence="0.999402">
P(wi wi- ti)
x P(wi_i) x P(ti wi) x P(w)
e■I P(w) x P(ti wi_i) x P(wi_i)
P(wilwi-1) x P(tilwi)
P(ti)
</equation>
<bodyText confidence="0.999937">
The likelihood of a candidate word wi thus can
be estimated by the following formula:
</bodyText>
<subsubsectionHeader confidence="0.90098">
3.2.2 Linear Combination
</subsubsectionHeader>
<bodyText confidence="0.999489818181818">
The Linear Combination algorithm combines
two models: tag trigram and word bigram. In the
first model, we attempt to find the most likely part-
of-speech tag for the current position according
to the two previous part-of-speech tags, and then
look for words that have the highest probability of
being in the current position given the most likely
tag for this position and the previous word. The
second model is the simple Bigram model. The
final probability is a weighted combination of the
two models:
</bodyText>
<equation confidence="0.99571475">
P(wi wi—i,ti—i, ti-2)
= a x P (w w i_t) + (1— a)
x max [P(wilti)xP(tilti_i ,
tiET(wi)
</equation>
<bodyText confidence="0.9999824">
where 0 &lt; a &lt; 1 is the coefficient of the linear
combination that weights both factors of the prob-
ability estimation. An important issue in this ap-
proach is finding the optimal value of a, which
must be determined experimentally.
</bodyText>
<sectionHeader confidence="0.997358" genericHeader="method">
4 Experimental methodology
</sectionHeader>
<subsectionHeader confidence="0.95577">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999875909090909">
We used the British National Corpus World Edi-
tion (BNC) (Burnard, 2000), a corpus of English
texts from different eras and different genres that
has been tagged by the Constituent Likelihood Au-
tomatic Word-tagging System (CLAWS) using the
C5 tagset. Our training and test data sets were
disjoint subsets of the BNC, randomly selected
from the written language section; they contained
5,585, 192 and 594,988 words, respectively (ex-
cept in a later experiment described in Section 5.6
below).
</bodyText>
<equation confidence="0.9830975">
P(w ti_,, ti_,)
= E p(wi, ti ti_,, ti_,) 4.2 Method
tieT(wi) We developed a generic test bench to perform var-
E Pc wi wi_i) x p(ti x p(t ti_1, t1_2) ions experiments with different word-completion
P(4) algorithms in varying conditions. The testbench
riET(w i)
contains three major components: a simulated
P(ti wi) t
x P(ti
_i_1,t1_2)
user that &amp;quot;types&amp;quot; i
P(wi wi_i) x n the test text; a completion Epro-
</equation>
<bodyText confidence="0.984249">
11E1(w1) gram that can employ any of the completion algo-
rithms to suggest words to the simulated user; and
a set of language models, derived from the training
</bodyText>
<page confidence="0.984874">
11
</page>
<figure confidence="0.8750435">
Results
Algorithms to be tested
</figure>
<figureCaption confidence="0.9820835">
Figure 2: Architecture of the word-completion
testbench used in the experiments.
</figureCaption>
<bodyText confidence="0.9998752">
corpus, for the algorithms to draw on. The archi-
tecture of the testbench is shown in Figure 2.
The simulated user is a &amp;quot;perfect&amp;quot; user who al-
ways chooses the desired word when it is avail-
able in the suggestion list. (For real users, this
is not always the case, especially for people with
cognitive disabilities or when the suggestion list
is long.) The completion program permits any
completion algorithm to be plugged in for evalua-
tion. The n-gram language models, both for words
and part-of-speech tags, were generated with the
CMU—Cambridge Statistical Language Modeling
Toolkit (Clarkson and Rosenfeld, 1997). Word-
given-tag probabilities, P(w It), were collected by
a Perl program written for the purpose.
</bodyText>
<subsectionHeader confidence="0.993091">
4.3 Experimental conditions
</subsectionHeader>
<bodyText confidence="0.999955161290323">
In addition to the word-completion algorithms
themselves, the parameters that we varied in our
experiments were the following:
Coefficient a: a is the coefficient of the Linear
Combination algorithm that ranges from 0 to 1,
giving more weight to either factor of the proba-
bility estimation. We tried all values from 0 to 1 in
0.1 increments.
Repetition of suggestions in consecutive com-
pletions: If we assume that the user is (close to)
perfect, we may avoid repeating suggestions that
have previously not been accepted by the user for a
particular prefix. We hypothesized that if we avoid
suggesting already-unaccepted words, the likeli-
hood of having the intended word among the new
suggestions will be higher. Thus, we compared the
results in both conditions: avoiding repetition, or
not doing so.
Maximum number of suggestions (n): It is
clear that the higher the number of words in the
suggestion list, the greater the chance of having
the intended word among the suggestions. But,
larger values for n impose a cognitive load on the
user as they make the search time for the desired
word longer, and it is more likely that the user will
overlook the word they are looking for. Different
users of word-completion utilities may prefer dif-
ferent values for this parameter, according to their
level and type of disabilities. We selected the val-
ues 1, 5, and 10 for n to measure how this param-
eter affects the performance.
</bodyText>
<subsectionHeader confidence="0.909061">
4.4 Performance measures
</subsectionHeader>
<bodyText confidence="0.999848235294118">
Hit rate (HR): The percentage of times that the
intended word appears in the suggestion list. A
higher hit rate implies a better performance.
Keystroke savings (KS): The percentage of
keystrokes that the user saves by using the word-
completion utility. A higher value for keystroke
savings implies a better performance.
Keystrokes until completion (KuC): The aver-
age number of keystrokes that the user enters for
each word, before it appears in the suggestion list.
The lower the value of this measure, the better the
algorithm.
Accuracy (Acc): The percentage of words that
have been successfully completed by the program
before the user reached the end of the word. A
good completion program is one that successfully
completes words in the early stages of typing.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.995894">
5.1 Comparison of the algorithms
</subsectionHeader>
<bodyText confidence="0.993568571428571">
The completion algorithms are compared when
n = 5 and the suggestions are allowed to be re-
peated in consecutive completions. The results are
presented in Table 1. The value of a in the Linear
Combination algorithm is 0.6 (see Section 5.2.)
The Linear Combination algorithm has the best
values for all the performance measures. A paired
</bodyText>
<figure confidence="0.917741444444445">
Training
texts
Language
models
Test
texts
Prediction program
Algorithm Simulated
under test typist
</figure>
<page confidence="0.854242">
12
</page>
<table confidence="0.998135666666667">
Algorithm %HR %KS KuC %Acc
Unigram 30.77 45.06 2.02 89.03
WordQ 34.04 49.88 1.76 89.43
Bigram 35.25 51.08 1.69 90.75
Tags+Words 35.26 51.08 1.69 90.78
Linear (a = .6) 36.23 51.98 1.64 91.80
</table>
<tableCaption confidence="0.91568">
Table 1: Performance measures for the algorithms
when n = 5 and suggestions may be repeated.
</tableCaption>
<figure confidence="0.97400975">
52 -
49
00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
alpha
</figure>
<figureCaption confidence="0.995801">
Figure 3: Keystroke savings of the Linear Com-
</figureCaption>
<bodyText confidence="0.959422388888889">
bination algorithm for different values of a when
n = 5 and suggestions may be repeated.
ANOVA test and a Tukey-Kramer post test, with
confidence interval set to 95%, on the values of
keystroke savings for all five algorithms showed
that performance of the Linear Combination algo-
rithm is better than that of the other algorithms
and the difference is statistically significant (p &lt;
0.001). It also showed that differences among
keystroke savings for Unigram, WordQ, Bigram,
and Linear algorithms are statistically significant
(p &lt; 0.001), but that between Bigram and Tags-
and-Words is not (p &gt; 0.05). But two different
completion algorithms may have a similar perfor-
mance according to our measures and yet differ
in the situations in which each succeeds or fails.
Thus, a more-detailed analysis of the results of
these two algorithms will be given in Section 5.7.
</bodyText>
<subsectionHeader confidence="0.999887">
5.2 Effect of coefficient a
</subsectionHeader>
<bodyText confidence="0.997431105263158">
An experiment with the Linear Combination algo-
rithm was conducted to investigate the impact on
performance of changing the value of a. Figure 3
shows the change in keystroke savings as a ranges
from 0 to 1.
Table 2: Effect of maximum suggestion list size
on performance of the Tags-and-Words algorithm
when suggestions may be repeated.
According to the experiments, the best value for
a is 0.6, giving somewhat more weight to word n-
grams than to tags. Thus, we set a to 0.6 in every
other experiment with the Linear Combination al-
gorithm. Assigning a constant value to a may not
be the best approach. For some words, the PoS tag
information may help more than others and thus
the best approach might be to define a as a func-
tion of the word and tag n-gram probability distri-
butions. Discussion of this approach is not within
the scope of this work.
</bodyText>
<subsectionHeader confidence="0.99978">
5.3 Effect of suggestion list size
</subsectionHeader>
<bodyText confidence="0.999951421052632">
We investigated the effect on performance of
changing the maximum suggestion list size n. Ta-
ble 2 shows the results for n = 1, 5, and 10 for the
Tags-and-Words algorithm.
The results show that the algorithm gives more
appropriate suggestions as the number of words in
the suggestion list increases, resulting in higher
hit rate and keystroke savings and fewer num-
ber of keystrokes needed before completion. The
best value for this parameter in real-world word-
completion utilities depends on the type and level
of the users&apos; disabilities. If the users have learning
or cognitive disabilities, suggesting fewer num-
ber of words decreases the cognitive load imposed
by the system. For users with physical disabili-
ties, saving physical effort, i.e., saving as many
keystrokes as possible, is usually most important
and hence suggesting more words (e.g., n = 10) is
desirable.
</bodyText>
<subsectionHeader confidence="0.9602535">
5.4 Effect of repeating previously suggested
words
</subsectionHeader>
<bodyText confidence="0.997642333333333">
We hypothesized that if previously suggested
words are not repeated, the likelihood of suggest-
ing the appropriate word will increase. Thus, we
</bodyText>
<equation confidence="0.8678858">
Suggestion %HR %KS KuC
list size
n = 1 21.30 34.40 2.61
n = 5 35.26 51.08 1.69
n = 10 39.69 55.90 1.43
</equation>
<page confidence="0.982893">
13
</page>
<table confidence="0.995839333333333">
Repeat? %HR %KS KuC
Yes 35.26 51.08 1.69
No 36.19 52.44 1.62
</table>
<tableCaption confidence="0.937353">
Table 3: Effect of prohibiting repeated suggestions
on performance of the Tags-and-Words algorithm
</tableCaption>
<table confidence="0.8808345">
when n = 5.
Tagset %HR %KS KuC
C5 35.26 51.08 1.69
Coarse 34.87 50.77 1.71
</table>
<tableCaption confidence="0.970213666666667">
Table 4: Results for Tags-and-Words algorithm
with fine- and coarse-grained tagsets; n = 5 and
suggestions may be repeated.
</tableCaption>
<bodyText confidence="0.998597">
conducted an experiment to observe the change
in performance if the previously suggested words
that have been rejected by the user are not sug-
gested for the same position. For this experiment
we used the Tags-and-Words algorithm. Results
are shown in Table 3. Although the performance
increase is small, the ANOVA test shows that the
difference is statistically significant (p &lt; .0001).
</bodyText>
<subsectionHeader confidence="0.997333">
5.5 A coarse-grained tagset
</subsectionHeader>
<bodyText confidence="0.999869130434782">
We hypothesized that using a coarser-grained
tagset than the one used to tag the BNC (C5 with
61 tags) might improve the performance. It is be-
cause some distinctions made by C5 tags might be
too fine-grained to help suggest syntactically ap-
propriate words. Thus we designed a new tagset
of 28 tags that conflates many of the finer-grained
distinctions in C5. For example, in the new tagset,
there is no distinction among various inflected
forms of the verb be.
We retagged the training and test data with the
coarse-grained tagset, and compared the results of
the Tags-and-Words algorithm on this data with
those on the original data. Hit rate, keystroke sav-
ings, and keystrokes until completion for both con-
ditions are presented in Table 4. In both experi-
ments, n = 5 and suggestions may be repeated.
The results show that using the coarser-grained
tagset causes a small decrease in performance.
However, according to the results of ANOVA test,
the difference is not statistically significant (p &gt;
0.05). The reason for the decrease might be an in-
appropriate selection of tags in which some of the
</bodyText>
<table confidence="0.998746">
Algorithm Training %HR %KS KuC
data
Bigram 5.6 M 35.25 51.08 1.69
81 M 37.43 52.90 1.55
Tags+words 5.6 M 35.26 51.08 1.69
81 M 37.49 53.30 1.53
</table>
<tableCaption confidence="0.9358255">
Table 5: Effect of training-data size on perfor-
mance.
</tableCaption>
<bodyText confidence="0.902396">
fine distinctions among syntactic categories that
are necessary for the task of word completion are
not considered in the new coarse-grained tagset.
</bodyText>
<subsectionHeader confidence="0.998334">
5.6 A larger training-data set
</subsectionHeader>
<bodyText confidence="0.999985545454545">
To investigate the effect of training-data size on
the performance of the Bigram and Tags-and-
Words algorithms, we trained them on a 14.5-
times larger subset of the BNC, around 81 million
words (instead of around 5 6 million words), and
tested them on a one-million-word subset of the
same corpus (instead of around 600,000 words).
The results are shown and compared with those of
the previous experiments in Table 5. There is only
a small improvement in the performance when the
larger training-data set is used.
</bodyText>
<subsectionHeader confidence="0.999611">
5.7 Comparison of errors
</subsectionHeader>
<bodyText confidence="0.999862631578948">
It is possible that two different algorithms could
have a similar performance by our measures and
yet differ in the situations in which each succeeds
or fails. We analyzed our results to see if this was
the case.
A comparison of the Bigram and Tags-and-
Words algorithms is presented in Table 6. In the
table, each entry au (the entry in row i and col-
umn j) shows the number of words that were
correctly suggested after i keystrokes by the Bi-
gram algorithm and after j keystrokes by Tags-
and-Words. In particular, entries on the diagonal
show the number of occasions in which the be-
havior of the algorithms was identical. For ex-
ample, 198,359 tokens were correctly completed
after one keystroke by both algorithms, whereas
505 tokens were completed by Tags-and-Words
after one keystroke but by Bigram only after two
keystrokes.
</bodyText>
<page confidence="0.995684">
14
</page>
<table confidence="0.9997629375">
1 2 3 4 5 6 7 8 9 10 11 12 13 14
0 206709 395 0 0 0 0 0 0 0 0 o o o o o
1 463 198359 486 14 0 0 0 0 0 0 o o o o o
2 0 505 89581 401 19 2 0 0 7 1 o o o o o
3 0 34 415 81793 385 32 0 3 1 0 0 0 0 0 0
4 0 0 34 358 44056 157 16 4 7 0 1 0 0 0 0
5 0 0 0 25 129 16598 69 10 2 0 1 1 0 0 0
6 0 0 0 9 20 42 8263 11 0 2 2 1 0 0 0
7 0 0 0 0 12 22 12 5201 10 2 0 0 0 0 0
8 0 0 3 0 1 6 6 13 3893 6 0 1 0 0 0
9 0 0 0 0 0 1 5 2 2 2964 0 0 0 0 0
10 0 0 0 0 0 0 0 0 0 2 2153 0 0 0 0
11 0 0 0 0 0 0 0 0 0 0 14 1928 0 0 0
12 0 0 0 0 0 0 0 0 0 0 0 0 1271 0 0
13 0 0 0 0 0 0 0 0 0 0 0 0 0 969 0
14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 568
</table>
<tableCaption confidence="0.995394">
Table 6: Comparison of the performance of the Bigram and Tags-and-Words algorithms. The entry in
</tableCaption>
<bodyText confidence="0.9614320625">
row i and column j shows the number of words that were correctly suggested after i keystrokes by the
Bigram algorithm and after j keystrokes by Tags-and-Words.
As can be seen in the table, the behavior of the
algorithms was almost always identical. Out of
668,490 trials, they correctly completed the word
after the same number of keystrokes 664,306 times
(99.37%). On those occasions on which the algo-
rithms differ, the difference was rarely more than
one keystroke, and the algorithms were almost
equally divided as to which did better: The Bigram
algorithm was better by one keystroke on 1,920
(0.29%) trials and by more than one keystroke on
129 (0.02%) trials; the Tags-and-Words algorithm
was better by one keystroke on 1,955 (0.29%)
trials and by more than one keystroke on 180
(0.03%) trials.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99990752">
We have presented and tested two algorithms that
use part-of-speech information to improve the syn-
tactic appropriateness of the suggestions in word
completion. It should be noted that nothing in the
algorithms relies on the particular keyboard used;
reduced keyboards, such as T9 (Kushler, 1998) or
Kiihn and Garbe&apos;s (2001) six-key set-up, will of
course give greater ambiguity in the input, but our
algorithms may still be applied.
We found that the keystroke savings of the Lin-
ear Combination algorithm was significantly bet-
ter than the baseline algorithms, which used only
word n-grams, and the other syntactic algorithm,
Tags-and-Words. Nonetheless, the improvement,
while statistically significant, was not large, and
might not be considered worth the considerable
extra cost that it requires. For example, the Bigram
algorithm was about 6.5 times faster than the Lin-
ear Combination algorithm on our test data. It is
important, after all, that word-completion appear
instantaneous to the user.
The relatively small improvement is possi-
bly because word-bigram probabilities implic-
itly capture tag-bigram probabilities to a con-
siderable extent as well. Thus, there might
be a high overlap between the information that
is used by the word-based and the tag-based
models. This result is consistent with that of
Garay-Vitoria and Gonzalez-Abascal (1997), who
found that the use of probabilistic grammar rules
in word completion gave only small improve-
ments (which they did not test for statistical sig-
nificance.) The result is also consistent with that
of Banko and Brill (2001), who showed that in
confusion-set disambiguation, having a very large
training corpus is more efficacious than having a
smarter and stronger classifier.
Conditional independence assumptions Both
syntactic algorithms make independence as-
sumptions in order to calculate the probability
ti-1, ti-2) using separate n-gram mod-
els of words and PoS tags. For example, in esti-
mating the above probability, it is assumed that ti
and wi_ I are conditionally independent given the
tag of the previous word, ti_i. Although this is
a reasonable assumption to make, it is not clear
how it may affect the results. Without making this
independence assumption, it would have been nec-
essary to calculate the probabilities P(tilwi_i) and
P(wi_t wi, ti) directly from the corpus.
</bodyText>
<page confidence="0.992453">
15
</page>
<bodyText confidence="0.999506346153846">
Therefore, employing hybrid n-gram models for
words and tags might result in more accurate syn-
tactic algorithms. For example, an algorithm could
estimate the joint probability of having a word wi
and its part-of-speech tag ti in the current posi-
tion, given the previous words and their associated
part-of-speech tags, P(wi, ti_i. wi_2,
directly from the corpus as a hybrid n-gram model.
Errors from the CLAWS tagger Inaccuracy in
tagging might also be a factor. The BNC&apos;s tag-
ging has a 3-4% error rate, and many &amp;quot;ambigu-
ous&amp;quot; tags. This means that although tags add a
new source of information to the completion algo-
rithm, more uncertainty is introduced as well.
Future work So far, we have carried out only
performance measures on our algorithms, and
more evaluation remains to be undertaken in or-
der to better understand their abilities and limi-
tations. For example, we have not yet assessed
the improvement (if any) in syntactic correctness
of suggestions that our algorithms actually afford,
nor have we conducted any user studies. In the
next phase of the work, we will add criteria for se-
mantic coherence to the algorithms, adapting mea-
sures of semantic similarity (Budanitsky, 1999) to
the task.
</bodyText>
<sectionHeader confidence="0.998588" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9987562">
This research was supported by a grant from Communica-
tions and Information Technology Ontario. We are grateful
to Fraser Shein and Tom Nantais of the Bloorview MacMil-
lan Children&apos;s Centre, Toronto, for advice and assistance in
all stages of this research.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998024817073171">
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambiguation.
Proceedings, 39th Annual Meeting, Association for
Computational Linguistics, 26-33, Toulouse.
John A. Bentrup. 1987. Exploiting word frequencies and
their sequential dependencies. Proceedings, 10th Annual
Conf. on Rehabilitation Technology, 121-123. RESNA.
Alexander Budanitsky. 1999. Lexical semantic re-
latedness and its application in natural language
processing. Technical report CSRG-390, Dept
of Computer Science, University of Toronto.
WWW. cs. toronto. edu/compling/Publications/
Lou Barnard, 2000. Reference Guide for the British National
Corpus (World Edition). wwwlicu.ox.ac.uk/BNC, 2nd
edition.
Alice Carlberger, Johan Carlberger, Tina Magnuson, Sharon
Hunnicutt, Sira E. Palazuelos-Cagigas, and Santi-
ago Aguilera Navarro. 1997a. Profet, a new generation of
word prediction: An evaluation study. Proceedings,ACL
Workshop on Natural language processing for communi-
cation aids, 23-28, Madrid.
Alice Carlberger, Tina Magnuson, Johan Carlberger, Henrik
Wachtmeister, and Sheri Hunnicutt. 1997b. Probability-
based word prediction for writing suport in dyslexia.
Proceedings of Fonetik&apos;97 Conference, vol. 4,17-20.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU–Cambridge toolkit. In
Proceedings of European Conference on Speech Cornmu-
nication and Technology (EUROSPEECH), 2707-2710,
Rhodes, Greece.
Ann Copestake. 1997. Augmented and alternative nlp
techniques for augmentative and alternative communica-
tion. In In Proceedings of the ACL workshop on Natural
Language Processing for Communication Aids, 37-42,
Madrid.
Afsaneh Fazly. 2002. Word prediction as a writ-
ing aid for the disabled. Master&apos;s thesis, Dept of
Computer Science, University of Toronto, January.
www.c.s.toronto.edukompling/Publication.s1
Nestor Garay-Vitoria and Julio Gonzalez-Abascal. 1997.
Intelligent word prediction to enhance text input rate (a
syntactic analysis-based word prediction aid for people
with severe motor and speech disability). In Proceedings
of the Annual International Conference on Intelligent
User Interfaces, 241-244.
Michael Kuhn and Jorn Garbe. 2001. Predictive and
highly ambiguous typing for a severely speech and
motion impaired user. In Constantine Stephanidis, editor,
Universal Access in Human–Computer Interaction (Pro-
ceedings of UAHCI-2001). Lawrence Erlbaum Associates.
Cliff Kushler. 1998. AAC using a reduced keyboard.
In Proceedings of CSUN 13th Annual Conference on
Technology for Persons with Disabilities.
Johannes Matiasek, Marco Baroni, and Harald Trost. 2002.
FASTY — A multi-lingual approach to text prediction. In
K. Miesenberger et al (eds.) Computers Helping People
wih Special Needs, Springer-Verlag.
Tom Nantais, Fraser Shein, and Mattias Johansson. 2001.
Efficacy of the word prediction algorithm in WordQ. In
Proceedings of the 24th Annual Conference on Technol-
ogy and Disability. RESNA.
Sira E. Palazuelos Cagigas. 2001. Contribution to Word
Prediction in Spanish and its Integration in Technical
Aids for People with Physical Disabilities. PhD thesis,
Universidad Politecnica de Madrid.
Fraser Shein, Tom Nantais, Rose Nishiyama, Cynthia Tam,
and Paul Marshall. 2001. Word cueing for persons with
writing difficulties: WordQ. In Proceedings of CSUN
16th Annual Conference on Technology for Persons with
Disabilities.
Andrew L. Swiffin, J. Adrian Pickering, John L. Arnott, and
Alan F. Newell. 1985. Pal: An effort efficient portable
communication aid and keyboard emulator. In Proceed-
ings of the 8th Annual Conference on Rehabilitation
Technology, 197-199. RESNA.
Julie A. VanDyke. 1991. A syntactic predictor to enhance
communication for disabled users. Technical Report
92-03, Dept of Computer and Information Sciences,
University of Delaware.
Matthew E.J. Wood. 1996. Syntactic pre-processing in
single-word prediction for disabled people. Ph.D. thesis,
Dept of Computer Science, University of Bristol.
</reference>
<page confidence="0.998758">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904309">
<title confidence="0.99988">Testing the Efficacy of Part-of-Speech Information in Word Completion</title>
<author confidence="0.956745">Fazly</author>
<affiliation confidence="0.99985">Department of Computer University of</affiliation>
<address confidence="0.972039">Ontario, Canada</address>
<email confidence="0.999939">afsaneh@cs.toronto.edu</email>
<email confidence="0.999939">gh@cs.toronto.edu</email>
<abstract confidence="0.997554666666667">We investigate the effect of incorporating syntactic information into a wordcompletion algorithm. We introduce two new algorithms that combine partof-speech tag trigrams with word higrams, and evaluate them with a testbench constructed for the purpose. The results show a small but statistically significant improvement in keystroke savings for one of our algorithms over baselines that use only word n-grams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>Proceedings, 39th Annual Meeting, Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<location>Toulouse.</location>
<contexts>
<context position="23069" citStr="Banko and Brill (2001)" startWordPosition="3949" endWordPosition="3952">mpletion appear instantaneous to the user. The relatively small improvement is possibly because word-bigram probabilities implicitly capture tag-bigram probabilities to a considerable extent as well. Thus, there might be a high overlap between the information that is used by the word-based and the tag-based models. This result is consistent with that of Garay-Vitoria and Gonzalez-Abascal (1997), who found that the use of probabilistic grammar rules in word completion gave only small improvements (which they did not test for statistical significance.) The result is also consistent with that of Banko and Brill (2001), who showed that in confusion-set disambiguation, having a very large training corpus is more efficacious than having a smarter and stronger classifier. Conditional independence assumptions Both syntactic algorithms make independence assumptions in order to calculate the probability ti-1, ti-2) using separate n-gram models of words and PoS tags. For example, in estimating the above probability, it is assumed that ti and wi_ I are conditionally independent given the tag of the previous word, ti_i. Although this is a reasonable assumption to make, it is not clear how it may affect the results. </context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. Proceedings, 39th Annual Meeting, Association for Computational Linguistics, 26-33, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bentrup</author>
</authors>
<title>Exploiting word frequencies and their sequential dependencies.</title>
<date>1987</date>
<booktitle>Proceedings, 10th Annual Conf. on Rehabilitation Technology,</booktitle>
<pages>121--123</pages>
<publisher>RESNA.</publisher>
<contexts>
<context position="2727" citStr="Bentrup (1987)" startWordPosition="425" endWordPosition="426">ted here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information about words. They 9 first obtain a probabil</context>
</contexts>
<marker>Bentrup, 1987</marker>
<rawString>John A. Bentrup. 1987. Exploiting word frequencies and their sequential dependencies. Proceedings, 10th Annual Conf. on Rehabilitation Technology, 121-123. RESNA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
</authors>
<title>Lexical semantic relatedness and its application in natural language processing.</title>
<date>1999</date>
<tech>Technical report CSRG-390,</tech>
<institution>Dept of Computer Science, University of Toronto.</institution>
<marker>Budanitsky, 1999</marker>
<rawString>Alexander Budanitsky. 1999. Lexical semantic relatedness and its application in natural language processing. Technical report CSRG-390, Dept of Computer Science, University of Toronto. WWW. cs. toronto. edu/compling/Publications/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Barnard</author>
</authors>
<title>Reference Guide for the British National Corpus (World Edition). wwwlicu.ox.ac.uk/BNC, 2nd edition.</title>
<date>2000</date>
<marker>Barnard, 2000</marker>
<rawString>Lou Barnard, 2000. Reference Guide for the British National Corpus (World Edition). wwwlicu.ox.ac.uk/BNC, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Carlberger</author>
<author>Johan Carlberger</author>
<author>Tina Magnuson</author>
<author>Sharon Hunnicutt</author>
<author>Sira E Palazuelos-Cagigas</author>
<author>Santiago Aguilera Navarro</author>
</authors>
<title>Profet, a new generation of word prediction: An evaluation study. Proceedings,ACL Workshop on Natural language processing for communication aids,</title>
<date>1997</date>
<pages>23--28</pages>
<location>Madrid.</location>
<contexts>
<context position="2685" citStr="Carlberger et al., 1997" startWordPosition="415" endWordPosition="418"> will result in greater savings. Details not presented here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information a</context>
</contexts>
<marker>Carlberger, Carlberger, Magnuson, Hunnicutt, Palazuelos-Cagigas, Navarro, 1997</marker>
<rawString>Alice Carlberger, Johan Carlberger, Tina Magnuson, Sharon Hunnicutt, Sira E. Palazuelos-Cagigas, and Santiago Aguilera Navarro. 1997a. Profet, a new generation of word prediction: An evaluation study. Proceedings,ACL Workshop on Natural language processing for communication aids, 23-28, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Carlberger</author>
<author>Tina Magnuson</author>
<author>Johan Carlberger</author>
<author>Henrik Wachtmeister</author>
<author>Sheri Hunnicutt</author>
</authors>
<title>Probabilitybased word prediction for writing suport in dyslexia.</title>
<date>1997</date>
<booktitle>Proceedings of Fonetik&apos;97 Conference,</booktitle>
<volume>vol.</volume>
<pages>4--17</pages>
<contexts>
<context position="2685" citStr="Carlberger et al., 1997" startWordPosition="415" endWordPosition="418"> will result in greater savings. Details not presented here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information a</context>
</contexts>
<marker>Carlberger, Magnuson, Carlberger, Wachtmeister, Hunnicutt, 1997</marker>
<rawString>Alice Carlberger, Tina Magnuson, Johan Carlberger, Henrik Wachtmeister, and Sheri Hunnicutt. 1997b. Probabilitybased word prediction for writing suport in dyslexia. Proceedings of Fonetik&apos;97 Conference, vol. 4,17-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU–Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Cornmunication and Technology (EUROSPEECH),</booktitle>
<pages>2707--2710</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="10205" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="1649" endWordPosition="1652">the experiments. corpus, for the algorithms to draw on. The architecture of the testbench is shown in Figure 2. The simulated user is a &amp;quot;perfect&amp;quot; user who always chooses the desired word when it is available in the suggestion list. (For real users, this is not always the case, especially for people with cognitive disabilities or when the suggestion list is long.) The completion program permits any completion algorithm to be plugged in for evaluation. The n-gram language models, both for words and part-of-speech tags, were generated with the CMU—Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). Wordgiven-tag probabilities, P(w It), were collected by a Perl program written for the purpose. 4.3 Experimental conditions In addition to the word-completion algorithms themselves, the parameters that we varied in our experiments were the following: Coefficient a: a is the coefficient of the Linear Combination algorithm that ranges from 0 to 1, giving more weight to either factor of the probability estimation. We tried all values from 0 to 1 in 0.1 increments. Repetition of suggestions in consecutive completions: If we assume that the user is (close to) perfect, we may avoid repeating sugge</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU–Cambridge toolkit. In Proceedings of European Conference on Speech Cornmunication and Technology (EUROSPEECH), 2707-2710, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Augmented and alternative nlp techniques for augmentative and alternative communication. In</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL workshop on Natural Language Processing for Communication Aids,</booktitle>
<pages>37--42</pages>
<location>Madrid.</location>
<contexts>
<context position="3466" citStr="Copestake (1997)" startWordPosition="540" endWordPosition="541">er&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information about words. They 9 first obtain a probability estimate for the tag of the next word and then suggest words using this probability estimation and word bigram models. Copestake (1997) used the part-of-speech tag bigrams collected from a small and unbalanced corpus, along with word recency information. She compared the results with that of a simple frequency-based algorithm. FASTY, a multilingual word-prediction system (Matiasek et al., 2002), and a prediction system for Spanish developed by Palazuelos (2001), both exploit syntactic knowledge in the form of part-of-speech tag statistics and grammar. FASTY uses collocation-based statistics to include longdistance relationships among words; it is mostly concerned with the prediction of nominal compounds (e.g., in German). Pal</context>
</contexts>
<marker>Copestake, 1997</marker>
<rawString>Ann Copestake. 1997. Augmented and alternative nlp techniques for augmentative and alternative communication. In In Proceedings of the ACL workshop on Natural Language Processing for Communication Aids, 37-42, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
</authors>
<title>Word prediction as a writing aid for the disabled. Master&apos;s thesis,</title>
<date>2002</date>
<institution>Dept of Computer Science, University of Toronto,</institution>
<contexts>
<context position="2147" citStr="Fazly (2002)" startWordPosition="333" endWordPosition="334">can be particularly problematic for users with certain cognitive disabilities, such as dyslexia, who often are easily confused by inappropriate suggestions. In this study, we explore the addition of syntactic information to word completion, developing new algorithms in which the part-of-speech tags of words are used in addition to the words themselves to improve the accuracy of the suggestions. We hypothesize that this will reduce the likelihood of suggesting words that are syntactically inappropriate in context and hence will result in greater savings. Details not presented here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and</context>
<context position="6279" citStr="Fazly (2002)" startWordPosition="991" endWordPosition="992">addition to word n-grams in order to maximize the likelihood of syntactic appropriateness of the suggestions. Both algorithms assume the presence of a part-of-speech tagger that annotates words with their most likely part-ofspeech tags incrementally as the user types them in. In different ways, each attempts to estimate the probability ti_1, ti_2), where t1 is the part-of-speech tag of word w. These algorithms are described in more detail in the following sections. (We also tried an algorithm that used only part-of-speech tags; its performance was inferior to that of the Bigram algorithm; see Fazly (2002).) 3.2.1 Tags and Words The Tags-and-Words algorithm combines tag trigrams and word bigrams in a single model. This algorithm estimates the probability ti_1, ti_2) with the following formula: p(vvi wi_l, ti_i, ti_2) EP(wi, t wi_i, ti_i, ti_2) EP(wi wi_i, ti, ti_i, ti_2) xP(ti wi_i, ti_i, ti_7) EP(vvi wi_i, to x p(ti ti-2) tiET(vvi) where T(W) is the set of possible part-of-speech tags for W. The conditional independence assumptions among random variables ti_?, ti_i, ti, wi_ 1, and vv i are depicted in a Bayesian network in Figure 1. Applying these conditional independence assumptions, the desi</context>
</contexts>
<marker>Fazly, 2002</marker>
<rawString>Afsaneh Fazly. 2002. Word prediction as a writing aid for the disabled. Master&apos;s thesis, Dept of Computer Science, University of Toronto, January. www.c.s.toronto.edukompling/Publication.s1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nestor Garay-Vitoria</author>
<author>Julio Gonzalez-Abascal</author>
</authors>
<title>Intelligent word prediction to enhance text input rate (a syntactic analysis-based word prediction aid for people with severe motor and speech disability).</title>
<date>1997</date>
<booktitle>In Proceedings of the Annual International Conference on Intelligent User Interfaces,</booktitle>
<pages>241--244</pages>
<contexts>
<context position="3065" citStr="Garay-Vitoria and Gonzalez-Abascal (1997)" startWordPosition="475" endWordPosition="478">ropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information about words. They 9 first obtain a probability estimate for the tag of the next word and then suggest words using this probability estimation and word bigram models. Copestake (1997) used the part-of-speech tag bigrams collected from a small and unbalanced corpus, along with word recency information. She compared the results with that of a simple frequency-based algorithm. FASTY</context>
<context position="22844" citStr="Garay-Vitoria and Gonzalez-Abascal (1997)" startWordPosition="3911" endWordPosition="3914">large, and might not be considered worth the considerable extra cost that it requires. For example, the Bigram algorithm was about 6.5 times faster than the Linear Combination algorithm on our test data. It is important, after all, that word-completion appear instantaneous to the user. The relatively small improvement is possibly because word-bigram probabilities implicitly capture tag-bigram probabilities to a considerable extent as well. Thus, there might be a high overlap between the information that is used by the word-based and the tag-based models. This result is consistent with that of Garay-Vitoria and Gonzalez-Abascal (1997), who found that the use of probabilistic grammar rules in word completion gave only small improvements (which they did not test for statistical significance.) The result is also consistent with that of Banko and Brill (2001), who showed that in confusion-set disambiguation, having a very large training corpus is more efficacious than having a smarter and stronger classifier. Conditional independence assumptions Both syntactic algorithms make independence assumptions in order to calculate the probability ti-1, ti-2) using separate n-gram models of words and PoS tags. For example, in estimating</context>
</contexts>
<marker>Garay-Vitoria, Gonzalez-Abascal, 1997</marker>
<rawString>Nestor Garay-Vitoria and Julio Gonzalez-Abascal. 1997. Intelligent word prediction to enhance text input rate (a syntactic analysis-based word prediction aid for people with severe motor and speech disability). In Proceedings of the Annual International Conference on Intelligent User Interfaces, 241-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kuhn</author>
<author>Jorn Garbe</author>
</authors>
<title>Predictive and highly ambiguous typing for a severely speech and motion impaired user.</title>
<date>2001</date>
<booktitle>Universal Access in Human–Computer Interaction (Proceedings of UAHCI-2001). Lawrence Erlbaum Associates.</booktitle>
<editor>In Constantine Stephanidis, editor,</editor>
<marker>Kuhn, Garbe, 2001</marker>
<rawString>Michael Kuhn and Jorn Garbe. 2001. Predictive and highly ambiguous typing for a severely speech and motion impaired user. In Constantine Stephanidis, editor, Universal Access in Human–Computer Interaction (Proceedings of UAHCI-2001). Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cliff Kushler</author>
</authors>
<title>AAC using a reduced keyboard.</title>
<date>1998</date>
<booktitle>In Proceedings of CSUN 13th Annual Conference on Technology for Persons with Disabilities.</booktitle>
<contexts>
<context position="21786" citStr="Kushler, 1998" startWordPosition="3752" endWordPosition="3753">ost equally divided as to which did better: The Bigram algorithm was better by one keystroke on 1,920 (0.29%) trials and by more than one keystroke on 129 (0.02%) trials; the Tags-and-Words algorithm was better by one keystroke on 1,955 (0.29%) trials and by more than one keystroke on 180 (0.03%) trials. 6 Conclusion We have presented and tested two algorithms that use part-of-speech information to improve the syntactic appropriateness of the suggestions in word completion. It should be noted that nothing in the algorithms relies on the particular keyboard used; reduced keyboards, such as T9 (Kushler, 1998) or Kiihn and Garbe&apos;s (2001) six-key set-up, will of course give greater ambiguity in the input, but our algorithms may still be applied. We found that the keystroke savings of the Linear Combination algorithm was significantly better than the baseline algorithms, which used only word n-grams, and the other syntactic algorithm, Tags-and-Words. Nonetheless, the improvement, while statistically significant, was not large, and might not be considered worth the considerable extra cost that it requires. For example, the Bigram algorithm was about 6.5 times faster than the Linear Combination algorit</context>
</contexts>
<marker>Kushler, 1998</marker>
<rawString>Cliff Kushler. 1998. AAC using a reduced keyboard. In Proceedings of CSUN 13th Annual Conference on Technology for Persons with Disabilities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Matiasek</author>
<author>Marco Baroni</author>
<author>Harald Trost</author>
</authors>
<title>FASTY — A multi-lingual approach to text prediction.</title>
<date>2002</date>
<booktitle>Computers Helping People wih Special Needs,</booktitle>
<editor>In K. Miesenberger et al (eds.)</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3728" citStr="Matiasek et al., 2002" startWordPosition="575" endWordPosition="578">derable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information about words. They 9 first obtain a probability estimate for the tag of the next word and then suggest words using this probability estimation and word bigram models. Copestake (1997) used the part-of-speech tag bigrams collected from a small and unbalanced corpus, along with word recency information. She compared the results with that of a simple frequency-based algorithm. FASTY, a multilingual word-prediction system (Matiasek et al., 2002), and a prediction system for Spanish developed by Palazuelos (2001), both exploit syntactic knowledge in the form of part-of-speech tag statistics and grammar. FASTY uses collocation-based statistics to include longdistance relationships among words; it is mostly concerned with the prediction of nominal compounds (e.g., in German). Palazuelos&apos;s system is designed for Spanish and hence direct comparison with our results below is not possible. In this work, we introduce several new wordcompletion algorithms that exploit part-of-speech tag information about words, and we employ a large, balanced</context>
</contexts>
<marker>Matiasek, Baroni, Trost, 2002</marker>
<rawString>Johannes Matiasek, Marco Baroni, and Harald Trost. 2002. FASTY — A multi-lingual approach to text prediction. In K. Miesenberger et al (eds.) Computers Helping People wih Special Needs, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Nantais</author>
<author>Fraser Shein</author>
<author>Mattias Johansson</author>
</authors>
<title>Efficacy of the word prediction algorithm in WordQ.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual Conference on Technology and Disability. RESNA.</booktitle>
<contexts>
<context position="2566" citStr="Nantais et al., 2001" startWordPosition="398" endWordPosition="401">e that this will reduce the likelihood of suggesting words that are syntactically inappropriate in context and hence will result in greater savings. Details not presented here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence</context>
</contexts>
<marker>Nantais, Shein, Johansson, 2001</marker>
<rawString>Tom Nantais, Fraser Shein, and Mattias Johansson. 2001. Efficacy of the word prediction algorithm in WordQ. In Proceedings of the 24th Annual Conference on Technology and Disability. RESNA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sira</author>
</authors>
<title>Palazuelos Cagigas.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>Universidad Politecnica de Madrid.</institution>
<marker>Sira, 2001</marker>
<rawString>Sira E. Palazuelos Cagigas. 2001. Contribution to Word Prediction in Spanish and its Integration in Technical Aids for People with Physical Disabilities. PhD thesis, Universidad Politecnica de Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fraser Shein</author>
<author>Tom Nantais</author>
<author>Rose Nishiyama</author>
<author>Cynthia Tam</author>
<author>Paul Marshall</author>
</authors>
<title>Word cueing for persons with writing difficulties: WordQ.</title>
<date>2001</date>
<booktitle>In Proceedings of CSUN 16th Annual Conference on Technology for Persons with Disabilities.</booktitle>
<contexts>
<context position="2587" citStr="Shein et al., 2001" startWordPosition="402" endWordPosition="405">e the likelihood of suggesting words that are syntactically inappropriate in context and hence will result in greater savings. Details not presented here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new wor</context>
</contexts>
<marker>Shein, Nantais, Nishiyama, Tam, Marshall, 2001</marker>
<rawString>Fraser Shein, Tom Nantais, Rose Nishiyama, Cynthia Tam, and Paul Marshall. 2001. Word cueing for persons with writing difficulties: WordQ. In Proceedings of CSUN 16th Annual Conference on Technology for Persons with Disabilities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Swiffin</author>
<author>J Adrian Pickering</author>
<author>John L Arnott</author>
<author>Alan F Newell</author>
</authors>
<title>Pal: An effort efficient portable communication aid and keyboard emulator.</title>
<date>1985</date>
<booktitle>In Proceedings of the 8th Annual Conference on Rehabilitation Technology,</booktitle>
<publisher>RESNA.</publisher>
<contexts>
<context position="2404" citStr="Swiffin et al. (1985)" startWordPosition="373" endWordPosition="376">ing new algorithms in which the part-of-speech tags of words are used in addition to the words themselves to improve the accuracy of the suggestions. We hypothesize that this will reduce the likelihood of suggesting words that are syntactically inappropriate in context and hence will result in greater savings. Details not presented here are given by Fazly (2002). 2 Related work Early word-completion programs, mostly developed in the 1980s, were based on very simple language models. They suggest highfrequency words that match the partially typed word, and ignore all the previous context; e.g., Swiffin et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991</context>
</contexts>
<marker>Swiffin, Pickering, Arnott, Newell, 1985</marker>
<rawString>Andrew L. Swiffin, J. Adrian Pickering, John L. Arnott, and Alan F. Newell. 1985. Pal: An effort efficient portable communication aid and keyboard emulator. In Proceedings of the 8th Annual Conference on Rehabilitation Technology, 197-199. RESNA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie A VanDyke</author>
</authors>
<title>A syntactic predictor to enhance communication for disabled users.</title>
<date>1991</date>
<tech>Technical Report 92-03,</tech>
<institution>Dept of Computer and Information Sciences, University of Delaware.</institution>
<contexts>
<context position="3005" citStr="VanDyke (1991)" startWordPosition="470" endWordPosition="471">et al. (1985). To provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information about words. They 9 first obtain a probability estimate for the tag of the next word and then suggest words using this probability estimation and word bigram models. Copestake (1997) used the part-of-speech tag bigrams collected from a small and unbalanced corpus, along with word recency information. She compared the re</context>
</contexts>
<marker>VanDyke, 1991</marker>
<rawString>Julie A. VanDyke. 1991. A syntactic predictor to enhance communication for disabled users. Technical Report 92-03, Dept of Computer and Information Sciences, University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew E J Wood</author>
</authors>
<title>Syntactic pre-processing in single-word prediction for disabled people.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept of Computer Science, University of Bristol.</institution>
<contexts>
<context position="3018" citStr="Wood (1996)" startWordPosition="472" endWordPosition="473">o provide moreappropriate suggestions, some programs look at a larger context by exploiting word bigram or trigram language models. WordQ (Nantais et al., 2001; Shein et al., 2001), developed at the Bloorview MacMillan Children&apos;s Centre, Toronto, Profet (Carlberger et al., 1997a, 1997b), and a program by Bentrup (1987) are statistical and adaptive programs that incorporate n-gram language models. These programs employ information on the user&apos;s recency and frequency of use of each word to adapt to the user&apos;s typing behaviour. The research closest to ours here is perhaps that of VanDyke (1991), Wood (1996), and Garay-Vitoria and Gonzalez-Abascal (1997), who employed parsers, requiring a considerable amount of work to reparse the partial input sentence every time a new word is completed by the user. Carlberger et al. (1997b) incorporate part-of-speech tag information about words. They 9 first obtain a probability estimate for the tag of the next word and then suggest words using this probability estimation and word bigram models. Copestake (1997) used the part-of-speech tag bigrams collected from a small and unbalanced corpus, along with word recency information. She compared the results with th</context>
</contexts>
<marker>Wood, 1996</marker>
<rawString>Matthew E.J. Wood. 1996. Syntactic pre-processing in single-word prediction for disabled people. Ph.D. thesis, Dept of Computer Science, University of Bristol.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>