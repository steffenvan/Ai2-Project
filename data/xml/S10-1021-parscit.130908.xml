<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.143273">
<title confidence="0.964844">
BART: A Multilingual Anaphora Resolution System
</title>
<author confidence="0.9936695">
Samuel Broscheit∗, Massimo Poesio‡, Simone Paolo Ponzetto∗, Kepa Joseba Rodriguez‡,
Lorenza Romano†, Olga Uryupina‡, Yannick Versley⋄, Roberto Zanoli†
</author>
<affiliation confidence="0.92569725">
∗Seminar f¨ur Computerlinguistik, University of Heidelberg
‡CiMeC, University of Trento
†Fondazione Bruno Kessler
⋄SFB 833, University of T¨ubingen
</affiliation>
<email confidence="0.98307225">
broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,
ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,
romano@fbk.eu, uryupina@gmail.com,
versley@sfs.uni-tuebingen.de, zanoli@fbk.eu
</email>
<sectionHeader confidence="0.993624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997571545454546">
BART (Versley et al., 2008) is a highly mod-
ular toolkit for coreference resolution that
supports state-of-the-art statistical approaches
and enables efficient feature engineering. For
the SemEval task 1 on Coreference Resolu-
tion, BART runs have been submitted for Ger-
man, English, and Italian.
BART relies on a maximum entropy-based
classifier for pairs of mentions. A novel entity-
mention approach based on Semantic Trees is
at the moment only supported for English.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971238095238">
This paper presents a multilingual coreference reso-
lution system based on BART (Versley et al., 2008).
BART is a modular toolkit for coreference resolution
that supports state-of-the-art statistical approaches
to the task and enables efficient feature engineer-
ing. BART has originally been created and tested
for English, but its flexible modular architecture en-
sures its portability to other languages and domains.
In SemEval-2010 task 1 on Coreference Resolution,
BART has shown reliable performance for English,
German and Italian.
In our SemEval experiments, we mainly focus on
extending BART to cover multiple languages. Given
a corpus in a new language, one can re-train BART
to obtain baseline results. Such a language-agnostic
system, however, is only used as a starting point:
substantial improvements can be achieved by incor-
porating language-specific information with the help
of the Language Plugin. This design provides ef-
fective separation between linguistic and machine
learning aspects of the problem.
</bodyText>
<sectionHeader confidence="0.980809" genericHeader="method">
2 BART Architecture
</sectionHeader>
<bodyText confidence="0.999411272727273">
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component. The architecture is shown on Figure
1. Each module can be accessed independently and
thus adjusted to leverage the system’s performance
on a particular language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of lingustic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{Mi, Mj}, i &lt; j as a set of features.
The decoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the encoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
</bodyText>
<sectionHeader confidence="0.997812" genericHeader="method">
3 Language-specific issues
</sectionHeader>
<bodyText confidence="0.9993265">
Below we briefly describe our language-specific ex-
tensions to BART. These issues are addressed in
more details in our recent papers (Broscheit et al.,
2010; Poesio et al., 2010).
</bodyText>
<subsectionHeader confidence="0.999509">
3.1 Mention Detection
</subsectionHeader>
<bodyText confidence="0.999955333333333">
Robust mention detection is an essential component
of any coreference resolution system. BART sup-
ports different pipelines for mention detection. The
</bodyText>
<page confidence="0.985543">
104
</page>
<note confidence="0.5595865">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.999677692307692">
Basic features
Syntactic features
Knowledge-based
features
Parser
Dep-to-Const
Converter
Morphology
Preprocessing
Mention
(with basic
properties):
- Number
- Gender
- Mention Type
- Modifiers
Mention
Factory
Decoder
Coreference
Chains
LanguagePlugin
Unannotated
Text
MaxEnt
Classifier
</figure>
<figureCaption confidence="0.999993">
Figure 1: BART architecture
</figureCaption>
<bodyText confidence="0.996183694444445">
choice of a pipeline depends crucially on the avail-
ability of linguistic resources for a given language.
For English and German, we use the Parsing
Pipeline and Mention Factory to extract mentions.
The parse trees are used to identify minimal and
maximal noun projections, as well as additional fea-
tures such as number, gender, and semantic class.
For English, we use parses from a state-of-the-art
constituent parser (Petrov et al., 2006) and extract
all base noun phrases as mentions. For German,
the SemEval dependency tree is transformed to a
constituent representation and minimal and maxi-
mal phrases are extracted for all nominal elements
(pronouns, common nouns, names), except when the
noun phrase is in a non-referring syntactic position
(for example, expletive “es”, predicates in copula
constructions).
For Italian, we use the EMD Pipeline and Men-
tion Factory. The Typhoon (Zanoli et al., 2009)
and DEMention (Biggio et al., 2009) systems were
used to recognize mentions in the test set. For each
mention, its head and extension were considered.
The extension was learned by using the mention an-
notation provided in the training set (13th column)
whereas the head annotation was learned by exploit-
ing the information produced by MaltParser (Nivre
et al., 2007). In addition to the features extracted
from the training set, such as prefixes and suffixes
(1-4 characters) and orthographic information (capi-
talization and hyphenation), a number of features ex-
tracted by using external resources were used: men-
tions recognized by TextPro (http://textpro.fbk.eu),
gazetteers of generic proper nouns extracted from
the Italian phone-book and Wikipedia, and other fea-
tures derived from WordNet. Each of these features
was extracted in a local context of f2 words.
</bodyText>
<subsectionHeader confidence="0.81559">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999764875">
We view coreference resolution as a binary classifi-
cation problem. Each classification instance consists
of two markables, i.e. an anaphor and potential an-
tecedent. Instances are modeled as feature vectors
(cf. Table 1) and are handed over to a binary clas-
sifier that decides, given the features, whether the
anaphor and the candidate are coreferent or not. All
the feature values are computed automatically, with-
out any manual intervention.
Basic feature set. We use the same set of rela-
tively language-independent features as a backbone
of our system, extending it with a few language-
specific features for each subtask. Most of them are
used by virtually all the state-of-the-art coreference
resolution systems. A detailed description can be
found, for example, in (Soon et al., 2001).
</bodyText>
<listItem confidence="0.9403745">
English. Our English system is based on a novel
model of coreference. The key concept of our model
is a Semantic Tree – a filecard associated with each
discourse entity containing the following fields:
• Types: the list of types for mentions of a given
entity. For example, if an entity contains the
mention “software from India”, the shallow
predicate “software” is added to the types.
• Attributes: this field collects the premodifiers.
For instance, if one of the mentions is “the ex-
pensive software” the shallow attribute “expen-
sive” is added to the list of attributes.
• Relations: this field collects the prepositional
postmodifiers. If an entity contains the men-
tion “software from India”, the shallow relation
“from(India)” is added to the list of relations.
</listItem>
<page confidence="0.996577">
105
</page>
<bodyText confidence="0.998305191489362">
For each mention BART creates such a filecard
using syntactic information. If the classifier decides
that both mentions are corefering, the filecard of
the anaphora is merged into the filecard of the an-
tecedent (cf. Section 3.3 below).
The SemanticTreeCompatibility feature
extractor checks whether individual slots of the
anaphor’s filecard are compatible with those of the
antecedent’s.
The StrudelRelatedness feature relies on
Strudel – a distributional semantic model (Baroni et
al., 2010). We compute Strudel vectors for the sets
of types of the anaphor and the antecedent. The re-
latedness value is determined as the cosine between
the two.
German. We have tested extra features for Ger-
man in our previous study (Broscheit et al., 2010).
The NodeDistance feature measures the num-
ber of clause nodes (SIMPX, R-SIMPX) and preposi-
tional phrase nodes (PX) along the path between Mj
and Mi in the parse tree.
The PartialMorphMatch feature is a sub-
string match with a morphological extension for
common nouns. In German the frequent use of
noun composition makes a simple string match for
common nouns unfeasible. The feature checks for
a match between the noun stems of Mi and Mj.
We extract the morphology with SMOR/Morphisto
(Schmid et al., 2004).
The GermanetRelatedness feature uses the
Pathfinder library for GermaNet (Finthammer and
Cramer, 2008) that computes and discretizes raw
scores into three categories of semantic relatedness.
In our experiments we use the measure from Wu and
Palmer (1994), which has been found to be the best
performing on our development data.
Italian. We have designed a feature to cover Ital-
ian aliasing patterns. A list of company/person des-
ignators (e.g., “S.p.a” or “D.ssa”) has been manually
crafted. We have collected patterns of name variants
for locations. Finally, we have relaxed abbreviation
constraints, allowing for lower-case characters in the
abbreviations. Our pilot experiments suggest that,
although a universal aliasing algorithm is able to re-
solve some coreference links between NEs, creating
a language-specific module boosts the system’s per-
formance for Italian substantially.
</bodyText>
<table confidence="0.996694">
Basic feature set
MentionType(Mi),MentionType(Mj)
SemanticClass(Mi), SemanticClass(Mj)
GenderAgreement(Mi,Mj)
NumberAgreement(Mi,Mj)
AnimacyAgreement(Mi,Mj)
StringMatch(Mi,Mj)
Distance(Mi,Mj)
Basic features used for English and Italian
Alias(Mi,Mj)
Apposition(Mi,Mj)
FirstMention(Mi)
English
IsSubject(Mi)
SemanticTreeCompatibility(Mi,Mj)
StrudelRelatedness(Mi,Mj)
German
InQuotedSpeech(Mi), InQuotedSpeech(Mj)
NodeDistance(Mi,Mj)
PartialMorphMatch(Mi,Mj)
GermanetRelatedness(Mi,Mj)
Italian
AliasItalian(Mi,Mj)
</table>
<tableCaption confidence="0.978998666666667">
Table 1: Features used by BART: each feature describes
a pair of mentions {Mi, Mj}, i &lt; j, where Mi is a can-
didate antecedent and Mj is a candidate anaphor
</tableCaption>
<subsectionHeader confidence="0.998432">
3.3 Resolution Algorithm
</subsectionHeader>
<bodyText confidence="0.999903666666667">
The BART toolkit supports several models of coref-
erence (pairwise modeling, rankers, semantic trees),
as well as different machine learning algorithms.
Our final setting relies on a pairwise maximum en-
tropy classifier for Italian and German.
Our English system is based on an entity-mention
model of coreference. The key concept of our model
is a Semantic Tree - a filecard associated to each dis-
course entity (cf. Section 3.2). Semantic trees are
used for both computing feature values and guiding
the resolution process.
We start by creating a Semantic Tree for each
mention. We process the document from left to
right, trying to find an antecedent for each men-
tion (candidate anaphor). When the antecedent is
found, we extend its Semantic Tree with the types,
attributes and relations of the anaphor, provided
they are mutually compatible. Consider, for ex-
</bodyText>
<page confidence="0.996454">
106
</page>
<bodyText confidence="0.999358666666667">
ample, a list of mentions, containing, among oth-
ers, “software from India”, “the software” and “soft-
ware from China”. Initially, BART creates the fol-
lowing semantic trees: “(type: software) (relation:
from(India))”, “(type: software)” and “(type: soft-
ware) (relation: from(China))”. When the second
mention gets resolved to the first one, their seman-
tic trees are merged to “(type: software) (relation:
from(India)”. Therefore, when we attempt to resolve
the third mention, both candidate antecedents are re-
jected, as their relation attributes are incompatible
with “from(China)”. This approach helps us avoid
erroneous links (such as the link between the second
and the third mentions in our example) by leveraging
entity-level information.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999964166666667">
The system was evaluated on the SemEval task 1
corpus by using the SemEval scorer.
First, we have evaluated our mention detection
modules: the system’s ability to recognize both the
mention extensions and the heads in the regular set-
ting. BART has achieved the best score for men-
tion detection in German and has shown reliable
figures for English. For Italian, the moderate per-
formance level is due to the different algorithms
for identifying the heads: the MaltParser (trained
on TUT: http://www.di.unito.it/˜tutreeb) produces a
more semantic representation, while the SemEval
scorer seems to adopt a more syntactic approach.
Second, we have evaluated the quality of our
coreference resolution modules. For German, BART
has shown better performance than all the other sys-
tems on the regular track.
For English, the only language targeted by all sys-
tems, BART shows good performance over all met-
rics in the regular setting, usually only outperformed
by systems that were tuned to a particular metric.
Finally, the Italian version of BART shows re-
liable figures for coreference resolution, given the
mention alignment problem discussed above.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999774">
We have presented BART – a multilingual toolkit
for coreference resolution. Due to its highly modu-
lar architecture, BART allows for efficient language-
specific feature engineering. Our effort represents
the first steps towards building a freely available
coreference resolution system for many languages.
</bodyText>
<sectionHeader confidence="0.990146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999730104166666">
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222–254.
Silvana Marianela Bernaola Biggio, Claudio Giuliano,
Massimo Poesio, Yannick Versley, Olga Uryupina, and
Roberto Zanoli. 2009. Local entity detection and
recognition task. In Proc. of Evalita-09.
Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-
sley, and Massimo Poesio. 2010. Extending BART to
provide a coreference resolution system for German.
In Proc. ofLREC ’10.
Marc Finthammer and Irene Cramer. 2008. Explor-
ing and navigating: Tools for GermaNet. In Proc. of
LREC ’08.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95–135.
Slav Petrov, Leon Barett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL-06.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
Italian. In Proc. ofLREC ’10.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Proc. of
LREC ’04.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521–544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the Linguistic Coreference Work-
shop at the International Conference on Language Re-
sources and Evaluation (LREC-2008).
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proc. ofACL-94, pages 133–
138.
Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.
2009. Named entity recognition through redundancy
driven classifier. In Proc. of Evalita-09.
</reference>
<page confidence="0.998704">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.164618">
<title confidence="0.999762">BART: A Multilingual Anaphora Resolution System</title>
<author confidence="0.94295">Massimo Simone Paolo Kepa Joseba Olga Yannick Roberto</author>
<affiliation confidence="0.996471">f¨ur Computerlinguistik, University of Heidelberg University of Trento</affiliation>
<author confidence="0.97686">Bruno Kessler</author>
<affiliation confidence="0.784559">833, University of T¨ubingen</affiliation>
<keyword confidence="0.626796">broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it, ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it, romano@fbk.eu, uryupina@gmail.com, versley@sfs.uni-tuebingen.de, zanoli@fbk.eu</keyword>
<abstract confidence="0.991432333333333">BART (Versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering. For the SemEval task 1 on Coreference Resolution, BART runs have been submitted for German, English, and Italian. BART relies on a maximum entropy-based classifier for pairs of mentions. A novel entitymention approach based on Semantic Trees is at the moment only supported for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Brian Murphy</author>
<author>Eduard Barbu</author>
<author>Massimo Poesio</author>
</authors>
<title>Strudel: A corpus-based semantic model based on properties and types.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="7835" citStr="Baroni et al., 2010" startWordPosition="1157" endWordPosition="1160">n entity contains the mention “software from India”, the shallow relation “from(India)” is added to the list of relations. 105 For each mention BART creates such a filecard using syntactic information. If the classifier decides that both mentions are corefering, the filecard of the anaphora is merged into the filecard of the antecedent (cf. Section 3.3 below). The SemanticTreeCompatibility feature extractor checks whether individual slots of the anaphor’s filecard are compatible with those of the antecedent’s. The StrudelRelatedness feature relies on Strudel – a distributional semantic model (Baroni et al., 2010). We compute Strudel vectors for the sets of types of the anaphor and the antecedent. The relatedness value is determined as the cosine between the two. German. We have tested extra features for German in our previous study (Broscheit et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R-SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for commo</context>
</contexts>
<marker>Baroni, Murphy, Barbu, Poesio, 2010</marker>
<rawString>Marco Baroni, Brian Murphy, Eduard Barbu, and Massimo Poesio. 2010. Strudel: A corpus-based semantic model based on properties and types. Cognitive Science, 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvana Marianela Bernaola Biggio</author>
<author>Claudio Giuliano</author>
<author>Massimo Poesio</author>
<author>Yannick Versley</author>
<author>Olga Uryupina</author>
<author>Roberto Zanoli</author>
</authors>
<title>Local entity detection and recognition task.</title>
<date>2009</date>
<booktitle>In Proc. of Evalita-09.</booktitle>
<contexts>
<context position="4943" citStr="Biggio et al., 2009" startWordPosition="703" endWordPosition="706">ender, and semantic class. For English, we use parses from a state-of-the-art constituent parser (Petrov et al., 2006) and extract all base noun phrases as mentions. For German, the SemEval dependency tree is transformed to a constituent representation and minimal and maximal phrases are extracted for all nominal elements (pronouns, common nouns, names), except when the noun phrase is in a non-referring syntactic position (for example, expletive “es”, predicates in copula constructions). For Italian, we use the EMD Pipeline and Mention Factory. The Typhoon (Zanoli et al., 2009) and DEMention (Biggio et al., 2009) systems were used to recognize mentions in the test set. For each mention, its head and extension were considered. The extension was learned by using the mention annotation provided in the training set (13th column) whereas the head annotation was learned by exploiting the information produced by MaltParser (Nivre et al., 2007). In addition to the features extracted from the training set, such as prefixes and suffixes (1-4 characters) and orthographic information (capitalization and hyphenation), a number of features extracted by using external resources were used: mentions recognized by Text</context>
</contexts>
<marker>Biggio, Giuliano, Poesio, Versley, Uryupina, Zanoli, 2009</marker>
<rawString>Silvana Marianela Bernaola Biggio, Claudio Giuliano, Massimo Poesio, Yannick Versley, Olga Uryupina, and Roberto Zanoli. 2009. Local entity detection and recognition task. In Proc. of Evalita-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Broscheit</author>
<author>Simone Paolo Ponzetto</author>
<author>Yannick Versley</author>
<author>Massimo Poesio</author>
</authors>
<title>Extending BART to provide a coreference resolution system for German.</title>
<date>2010</date>
<booktitle>In Proc. ofLREC ’10.</booktitle>
<contexts>
<context position="3311" citStr="Broscheit et al., 2010" startWordPosition="464" endWordPosition="467">assign their basic properties (number, gender etc). The feature extraction module describes pairs of mentions {Mi, Mj}, i &lt; j as a set of features. The decoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the encoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 3 Language-specific issues Below we briefly describe our language-specific extensions to BART. These issues are addressed in more details in our recent papers (Broscheit et al., 2010; Poesio et al., 2010). 3.1 Mention Detection Robust mention detection is an essential component of any coreference resolution system. BART supports different pipelines for mention detection. The 104 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Basic features Syntactic features Knowledge-based features Parser Dep-to-Const Converter Morphology Preprocessing Mention (with basic properties): - Number - Gender - Mention Type - Modifiers Mention Factory Decoder Corefe</context>
<context position="8083" citStr="Broscheit et al., 2010" startWordPosition="1201" endWordPosition="1204">s are corefering, the filecard of the anaphora is merged into the filecard of the antecedent (cf. Section 3.3 below). The SemanticTreeCompatibility feature extractor checks whether individual slots of the anaphor’s filecard are compatible with those of the antecedent’s. The StrudelRelatedness feature relies on Strudel – a distributional semantic model (Baroni et al., 2010). We compute Strudel vectors for the sets of types of the anaphor and the antecedent. The relatedness value is determined as the cosine between the two. German. We have tested extra features for German in our previous study (Broscheit et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R-SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj. We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cr</context>
</contexts>
<marker>Broscheit, Ponzetto, Versley, Poesio, 2010</marker>
<rawString>Samuel Broscheit, Simone Paolo Ponzetto, Yannick Versley, and Massimo Poesio. 2010. Extending BART to provide a coreference resolution system for German. In Proc. ofLREC ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Finthammer</author>
<author>Irene Cramer</author>
</authors>
<title>Exploring and navigating: Tools for GermaNet.</title>
<date>2008</date>
<booktitle>In Proc. of LREC ’08.</booktitle>
<contexts>
<context position="8694" citStr="Finthammer and Cramer, 2008" startWordPosition="1299" endWordPosition="1302">eit et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R-SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj. We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraints, allowing for lower-case characters in the abbreviations. Our pilot experiments suggest that, although</context>
</contexts>
<marker>Finthammer, Cramer, 2008</marker>
<rawString>Marc Finthammer and Irene Cramer. 2008. Exploring and navigating: Tools for GermaNet. In Proc. of LREC ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Gulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="5273" citStr="Nivre et al., 2007" startWordPosition="757" endWordPosition="760">ns, common nouns, names), except when the noun phrase is in a non-referring syntactic position (for example, expletive “es”, predicates in copula constructions). For Italian, we use the EMD Pipeline and Mention Factory. The Typhoon (Zanoli et al., 2009) and DEMention (Biggio et al., 2009) systems were used to recognize mentions in the test set. For each mention, its head and extension were considered. The extension was learned by using the mention annotation provided in the training set (13th column) whereas the head annotation was learned by exploiting the information produced by MaltParser (Nivre et al., 2007). In addition to the features extracted from the training set, such as prefixes and suffixes (1-4 characters) and orthographic information (capitalization and hyphenation), a number of features extracted by using external resources were used: mentions recognized by TextPro (http://textpro.fbk.eu), gazetteers of generic proper nouns extracted from the Italian phone-book and Wikipedia, and other features derived from WordNet. Each of these features was extracted in a local context of f2 words. 3.2 Features We view coreference resolution as a binary classification problem. Each classification ins</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Gulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL-06.</booktitle>
<contexts>
<context position="4441" citStr="Petrov et al., 2006" startWordPosition="625" endWordPosition="628">c properties): - Number - Gender - Mention Type - Modifiers Mention Factory Decoder Coreference Chains LanguagePlugin Unannotated Text MaxEnt Classifier Figure 1: BART architecture choice of a pipeline depends crucially on the availability of linguistic resources for a given language. For English and German, we use the Parsing Pipeline and Mention Factory to extract mentions. The parse trees are used to identify minimal and maximal noun projections, as well as additional features such as number, gender, and semantic class. For English, we use parses from a state-of-the-art constituent parser (Petrov et al., 2006) and extract all base noun phrases as mentions. For German, the SemEval dependency tree is transformed to a constituent representation and minimal and maximal phrases are extracted for all nominal elements (pronouns, common nouns, names), except when the noun phrase is in a non-referring syntactic position (for example, expletive “es”, predicates in copula constructions). For Italian, we use the EMD Pipeline and Mention Factory. The Typhoon (Zanoli et al., 2009) and DEMention (Biggio et al., 2009) systems were used to recognize mentions in the test set. For each mention, its head and extension</context>
</contexts>
<marker>Petrov, Barett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLING-ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Olga Uryupina</author>
<author>Yannick Versley</author>
</authors>
<title>Creating a coreference resolution system for Italian.</title>
<date>2010</date>
<booktitle>In Proc. ofLREC ’10.</booktitle>
<contexts>
<context position="3333" citStr="Poesio et al., 2010" startWordPosition="468" endWordPosition="471">rties (number, gender etc). The feature extraction module describes pairs of mentions {Mi, Mj}, i &lt; j as a set of features. The decoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the encoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 3 Language-specific issues Below we briefly describe our language-specific extensions to BART. These issues are addressed in more details in our recent papers (Broscheit et al., 2010; Poesio et al., 2010). 3.1 Mention Detection Robust mention detection is an essential component of any coreference resolution system. BART supports different pipelines for mention detection. The 104 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Basic features Syntactic features Knowledge-based features Parser Dep-to-Const Converter Morphology Preprocessing Mention (with basic properties): - Number - Gender - Mention Type - Modifiers Mention Factory Decoder Coreference Chains LanguageP</context>
</contexts>
<marker>Poesio, Uryupina, Versley, 2010</marker>
<rawString>Massimo Poesio, Olga Uryupina, and Yannick Versley. 2010. Creating a coreference resolution system for Italian. In Proc. ofLREC ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>SMOR: A German computational morphology covering derivation, composition and inflection.</title>
<date>2004</date>
<booktitle>In Proc. of LREC ’04.</booktitle>
<contexts>
<context position="8590" citStr="Schmid et al., 2004" startWordPosition="1286" endWordPosition="1289"> between the two. German. We have tested extra features for German in our previous study (Broscheit et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R-SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj. We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraint</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. SMOR: A German computational morphology covering derivation, composition and inflection. In Proc. of LREC ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics (Special Issue on Computational Anaphora Resolution),</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="6569" citStr="Soon et al., 2001" startWordPosition="958" endWordPosition="961">tances are modeled as feature vectors (cf. Table 1) and are handed over to a binary classifier that decides, given the features, whether the anaphor and the candidate are coreferent or not. All the feature values are computed automatically, without any manual intervention. Basic feature set. We use the same set of relatively language-independent features as a backbone of our system, extending it with a few languagespecific features for each subtask. Most of them are used by virtually all the state-of-the-art coreference resolution systems. A detailed description can be found, for example, in (Soon et al., 2001). English. Our English system is based on a novel model of coreference. The key concept of our model is a Semantic Tree – a filecard associated with each discourse entity containing the following fields: • Types: the list of types for mentions of a given entity. For example, if an entity contains the mention “software from India”, the shallow predicate “software” is added to the types. • Attributes: this field collects the premodifiers. For instance, if one of the mentions is “the expensive software” the shallow attribute “expensive” is added to the list of attributes. • Relations: this field </context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics (Special Issue on Computational Anaphora Resolution), 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
<author>Xiaofeng Yang</author>
<author>Alessandro Moschitti</author>
</authors>
<title>BART: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at the International Conference on Language Resources and Evaluation (LREC-2008).</booktitle>
<contexts>
<context position="1131" citStr="Versley et al., 2008" startWordPosition="137" endWordPosition="140">-tuebingen.de, zanoli@fbk.eu Abstract BART (Versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering. For the SemEval task 1 on Coreference Resolution, BART runs have been submitted for German, English, and Italian. BART relies on a maximum entropy-based classifier for pairs of mentions. A novel entitymention approach based on Semantic Trees is at the moment only supported for English. 1 Introduction This paper presents a multilingual coreference resolution system based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and domains. In SemEval-2010 task 1 on Coreference Resolution, BART has shown reliable performance for English, German and Italian. In our SemEval experiments, we mainly focus on extending BART to cover multiple languages. Given a corpus in a new language, one can re-train BART to obtain baseline </context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang, and Alessandro Moschitti. 2008. BART: A modular toolkit for coreference resolution. In Proceedings of the Linguistic Coreference Workshop at the International Conference on Language Resources and Evaluation (LREC-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proc. ofACL-94,</booktitle>
<pages>133--138</pages>
<contexts>
<context position="8846" citStr="Wu and Palmer (1994)" startWordPosition="1323" endWordPosition="1326">nd Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj. We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraints, allowing for lower-case characters in the abbreviations. Our pilot experiments suggest that, although a universal aliasing algorithm is able to resolve some coreference links between NEs, creating a language-specific module boosts the system’s performan</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proc. ofACL-94, pages 133– 138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Zanoli</author>
<author>Emiliano Pianta</author>
<author>Claudio Giuliano</author>
</authors>
<title>Named entity recognition through redundancy driven classifier.</title>
<date>2009</date>
<booktitle>In Proc. of Evalita-09.</booktitle>
<contexts>
<context position="4907" citStr="Zanoli et al., 2009" startWordPosition="697" endWordPosition="700">dditional features such as number, gender, and semantic class. For English, we use parses from a state-of-the-art constituent parser (Petrov et al., 2006) and extract all base noun phrases as mentions. For German, the SemEval dependency tree is transformed to a constituent representation and minimal and maximal phrases are extracted for all nominal elements (pronouns, common nouns, names), except when the noun phrase is in a non-referring syntactic position (for example, expletive “es”, predicates in copula constructions). For Italian, we use the EMD Pipeline and Mention Factory. The Typhoon (Zanoli et al., 2009) and DEMention (Biggio et al., 2009) systems were used to recognize mentions in the test set. For each mention, its head and extension were considered. The extension was learned by using the mention annotation provided in the training set (13th column) whereas the head annotation was learned by exploiting the information produced by MaltParser (Nivre et al., 2007). In addition to the features extracted from the training set, such as prefixes and suffixes (1-4 characters) and orthographic information (capitalization and hyphenation), a number of features extracted by using external resources we</context>
</contexts>
<marker>Zanoli, Pianta, Giuliano, 2009</marker>
<rawString>Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano. 2009. Named entity recognition through redundancy driven classifier. In Proc. of Evalita-09.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>