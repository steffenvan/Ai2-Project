<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011209">
<title confidence="0.997652">
Graph-based Clustering for Computational Linguistics: A Survey
</title>
<author confidence="0.996667">
Zheng Chen Heng Ji
</author>
<affiliation confidence="0.985065">
The Graduate Center Queens College and The Graduate Center
The City University of New York The City University of New York
</affiliation>
<email confidence="0.998757">
zchen1@gc.cuny.edu hengji@cs.qc.cuny.edu
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822916666666">
In this survey we overview graph-based clus-
tering and its applications in computational
linguistics. We summarize graph-based clus-
tering as a five-part story: hypothesis, model-
ing, measure, algorithm and evaluation. We
then survey three typical NLP problems in
which graph-based clustering approaches
have been successfully applied. Finally, we
comment on the strengths and weaknesses of
graph-based clustering and envision that
graph-based clustering is a promising solu-
tion for some emerging NLP problems.
</bodyText>
<sectionHeader confidence="0.99849" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999780538461539">
In the passing years, there has been a tremend-
ous body of work on graph-based clustering,
either done by theoreticians or practitioners.
Theoreticians have been extensively investigat-
ing cluster properties, quality measures and var-
ious clustering algorithms by taking advantage
of elegant mathematical structures built in graph
theory. Practitioners have been investigating the
graph clustering algorithms for specific applica-
tions and claiming their effectiveness by taking
advantage of the underlying structure or other
known characteristics of the data. Although
graph-based clustering has gained increasing
attentions from Computational Linguistic (CL)
community (especially through the series of
TextGraphs workshops), it is studied case by
case and as far as we know, we have not seen
much work on comparative study of various
graph-based clustering algorithms for certain
NLP problems. The major goal of this survey is
to “bridge” the gap between theoretical aspect
and practical aspect in graph-based clustering,
especially for computational linguistics.
From the theoretical aspect, we state that the
following five-part story describes the general
methodology of graph-based clustering:
</bodyText>
<listItem confidence="0.929847">
(1) Hypothesis. The hypothesis is that a graph
can be partitioned into densely connected sub-
graphs that are sparsely connected to each other.
(2) Modeling. It deals with the problem of trans-
forming data into a graph or modeling the real
application as a graph.
(3) Measure. A quality measure is an objective
function that rates the quality of a clustering.
(4) Algorithm. An algorithm is to exactly or
approximately optimize the quality measure.
(5) Evaluation. Various metrics can be used to
evaluate the performance of clustering by com-
paring with a “ground truth” clustering.
</listItem>
<bodyText confidence="0.983140875">
From the practical aspect, we focus on three
typical NLP applications, including coreference
resolution, word clustering and word sense dis-
ambiguation, in which graph-based clustering
approaches have been successfully applied and
achieved competitive performance.
2 Graph-based Clustering Methodology
We start with the basic clustering problem. Let
</bodyText>
<equation confidence="0.499024333333333">
X = {x1, . .., xN} be a set of data points, S =
(sitj =1,...,N be the similarity matrix in which
i
</equation>
<bodyText confidence="0.999717230769231">
each element indicates the similarity sit &gt;_ 0 be-
tween two data points xi and xt. A nice way to
represent the data is to construct a graph on
which each vertex represents a data point and
the edge weight carries the similarity of two
vertices. The clustering problem in graph pers-
pective is then formulated as partitioning the
graph into subgraphs such that the edges in the
same subgraph have high weights and the edges
between different subgraphs have low weights.
In the next section, we define essential graph
notation to facilitate discussions in the rest of
this survey.
</bodyText>
<page confidence="0.822078">
1
</page>
<note confidence="0.9879565">
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 1–9,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.838849">
2.1 Graph Notation
</subsectionHeader>
<bodyText confidence="0.9775803">
A graph is a triple G=(V,E,W) where V =
{v1, ... ,vN} is a set of vertices, ESVXV is a set
of edges, and W = (wij)i ,j=1,...,N is called adja-
cency matrix in which each element indicates a
non-negative weight ( wij &gt;_ 0) between two
vertices vi and vj .
In this survey we target at hard clustering
problem which means we partition vertices of
the graph into non-overlapping clusters, i.e., let
C = (C1,..., CK) be a partition of V such that
</bodyText>
<listItem confidence="0.995538">
(1) Ci # 0 for i E {1, ..., K}.
(2) Ci n Cj = 0 for i, j E {1, ..., K} and i # j
(3) C1 U ... U CK= V
</listItem>
<subsectionHeader confidence="0.98101">
2.2 Hypothesis
</subsectionHeader>
<bodyText confidence="0.999157">
The hypothesis behind graph-based clustering
can be stated in the following ways:
</bodyText>
<listItem confidence="0.996903916666667">
(1) The graph consists of dense subgraphs such
that a dense subgraph contains more well-
connected internal edges connecting the
vertices in the subgraph than cutting edges
connecting the vertices across subgraphs.
(2) A random walk that visits a subgraph will
likely stay in the subgraph until many of its
vertices have been visited (Dongen, 2000).
(3) Among all shortest paths between all pairs
of vertices, links between different dense
subgraphs are likely to be in many shortest
paths (Dongen, 2000).
</listItem>
<subsectionHeader confidence="0.999024">
2.3 Modeling
</subsectionHeader>
<bodyText confidence="0.999957571428571">
Modeling addresses the problem of transform-
ing the problem into graph structure, specifical-
ly, designating the meaning of vertices and
edges in the graph, computing the edge weights
for weighted graph, and constructing the graph.
Luxburg (2006) stated three most common me-
thods to construct a graph: s -neighborhood
graph, k-nearest neighbor graph, and fully con-
nected graph. Luxburg analyzed different beha-
viors of the three graph construction methods,
and stated that some graph-cluster algorithms
(e.g., spectral clustering) can be quite sensitive
to the choice of graphs and parameters (s and k).
As a general recommendation, Luxburg sug-
gested exploiting k-nearest neighbor graph as
the first choice, which is less vulnerable to the
choices of parameters than other graphs. Unfor-
tunately, theoretical justifications on the choices
of graphs and parameters do not exist and as a
result, the problem has been ignored by practi-
tioners.
</bodyText>
<subsectionHeader confidence="0.99276">
2.4 Measure
</subsectionHeader>
<bodyText confidence="0.999929269230769">
A measure is an objective function that rates the
quality of a clustering, thus called quality meas-
ure. By optimizing the quality measure, we can
obtain the “optimal” clustering.
It is worth noting that quality measure should
not be confused with vertex similarity measure
where it is used to compute edge weights. Fur-
thermore, we should distinguish quality meas-
ure from evaluation measure which will be dis-
cussed in section 2.6. The main difference is
that cluster quality measure directly identifies a
clustering that fulfills a desirable property while
evaluation measure rates the quality of a cluster-
ing by comparing with a ground-truth clustering.
We summarize various quality measures in
Table 1, from the basic density measures (intra-
cluster and inter-cluster), to cut-based measures
(ratio cut, ncut, performance, expansion, con-
ductance, bicriteria), then to the latest proposed
measure modularity. Each of the measures has
strengths and weaknesses as commented in Ta-
ble 1. Optimizing each of the measures is NP-
hard. As a result, many efficient algorithms,
which have been claimed to solve the optimal
problem with polynomial-time complexity,
yield sub-optimal clustering.
</bodyText>
<subsectionHeader confidence="0.975993">
2.5 Algorithm
</subsectionHeader>
<bodyText confidence="0.999977333333333">
We categorize graph clustering algorithms into
two major classes: divisive and agglomerative
(Table 2). In the divisive clustering class, we
categorize algorithms into several subclasses,
namely, cut-based, spectral clustering, multile-
vel, random walks, shortest path. Divisive clus-
tering follows top-down style and recursively
splits a graph into subgraphs. In contrast, ag-
glomerative clustering works bottom-up and
iteratively merges singleton sets of vertices into
subgraphs. The divisive and agglomerative al-
gorithms are also called hierarchical since they
produce multi-level clusterings, i.e., one cluster-
ing follows the other by refining (divisive) or
coarsening (agglomerative). Most graph cluster-
ing algorithms ever proposed are divisive. We
list the quality measure and the running com-
plexity for each algorithm in Table 2.
</bodyText>
<page confidence="0.996404">
2
</page>
<table confidence="0.972001173913043">
Measures Comments
intra-cluster density ― Maximizing intra-cluster density is equivalent to minimizing inter-cluster
inter-cluster density ― density and vice versa
Drawback: both favor cutting small sets of isolated vertices in the graph
(Shi and Malik, 2000)
ratio cut (Hagan and Kahng, ― Ratio cut is suitable for unweighted graph, and ncut is a better choice for
1992) ― weighted graph
ncut (Shi and Malik, 2000) ― Overcome the drawback of intra-cluster density or inter-cluster density
Drawback: both favor clusters with equal size
performance (Dongen, 2000; ― Performance takes both intra-cluster density and inter-cluster density
Brandes et al., 2003) into considerations simultaneously
expansion, conductance, ― Expansion is suitable for unweighted graph, and conductance is a better
bicriteria ― choice for weighted graph
(Kannan et al., 2000) Both expansion and conductance impose quality within clusters, but not
inter-cluster quality; bicriteria takes both into considerations
modularity (Newman and ― Evaluates the quality of clustering with respect to a randomized graph
Girvan,2004) ― Drawbacks: (1) It requires global knowledge of the graph’s topology,
i.e., the number of edges. Clauset (2005) proposed an improved measure
Local Modularity. (2) Resolution limit problem: it fails to identify clus-
ters smaller than a certain scale. Ruan and Zhang (2008) proposed an
improved measure HQcut. (3) It fails to distinguish good from bad clus-
tering between different graphs with the same modularity value. Chen et
al. (2009) proposed an improved measure Max-Min Modularity
</table>
<tableCaption confidence="0.999475">
Table 1. Summary of Quality Measures
</tableCaption>
<table confidence="0.9994488">
Category Algorithms optimized running
measure complexity
divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3)
(Kernighan and Lin, 1970)
cut-clustering algorithm bicriteria 0 (|V|)
(Flake et al., 2003)
spectral unnormalized spectral clustering ratiocut 0 (|V||E|)
(Luxburg, 2006)
normalized spectral clustering I ncut 0 (|V||E|)
(Luxburg, 2006; Shi and Malik, 2000)
normalized spectral clustering II ncut 0 (|V ||E |)
(Luxburg, 2006; Ng, 2002)
iterative conductance cutting (ICC) conductance 0(|V ||E|)
(Kannan et al.,2000)
geometric MST clustering (GMC) pluggable(any 0(|V ||E|)
(Brandes et al., 2007) quality measure)
modularity oriented modularity 0 (|V||E|)
(White and Smyth,2005)
multilevel multilevel recursive bisection intercluster 0(|V|logK)
(Karypis and Kumar, 1999)
multilevel K-way partitioning intercluster 0 (|V|
(Karypis and Kumar, 1999) + KlogK)
random Markov Clustering Algorithm (MCL) performance 0(m2|V|)
(Dongen, 2000)
shortest betweenness modularity 0 (|V ||E |2)
path (Girvan and Newman, 2003)
information centrality modularity 0(|V||E |3)
(Fortunato et al., 2004)
agglomerative modularity oriented modularity 0 (|V ||E |)
(Newman, 2004)
</table>
<tableCaption confidence="0.9667635">
Table 2. Summary of Graph-based Clustering Algorithms (|V |: the number of vertices, |E |: the
number of edges, K: the number of clusters, m: the number of resources allocated for each vertex)
</tableCaption>
<page confidence="0.997632">
3
</page>
<bodyText confidence="0.999991577981651">
The first set of algorithms (cut-based) is asso-
ciated with max-flow min-cut theorem (Ford and
Fulkerson, 1956) which states that “the value of
the maximum flow is equal to the cost of the
minimum cut”. One of the earliest algorithm,
Kernighan-Lin algorithm (Kernighan and Lin,
1970) splits the graph by performing recursive
bisection (split into two parts at a time), aiming
to minimize inter-cluster density (cut size). The
high complexity of the algorithm ( 0 (|V |3)
makes it less competitive in real applications.
Flake et al. (2003) proposed a cut-clustering al-
gorithm which optimizes the bicriterion measure
and the complexity is proportional to the number
of clusters K using a heuristic, thus the algorithm
is competitive in practice.
The second set of algorithms is based on spec-
tral graph theory with Laplacian matrix as the
mathematical tool. The connection between clus-
tering and spectrum of Laplacian matrix (L) bas-
ically lies in the following important proposition:
the multiplicity k of the eigenvalue 0 of L equals
to the number of connected components in the
graph. Luxburg (2006) and Abney (2007) pre-
sented a comprehensive tutorial on spectral clus-
tering. Luxburg (2006) discussed three forms of
Laplacian matrices (one unnormalized form and
two normalized forms) and their three corres-
ponding spectral clustering algorithms (unnorma-
lized, normalized I and normalized II). Unnorma-
lized clustering aims to optimize ratiocut meas-
ure while normalized clustering aims to optimize
ncut measure (Shi and Malik, 2000), thus spec-
tral clustering actually relates with cut-based
clustering. The success of spectral clustering is
mainly based on the fact that it does not make
strong assumptions on the form of the clusters
and can solve very general problems like intert-
wined spirals which k-means clustering handles
much worse. Unfortunately, spectral clustering
could be unstable under different choices of
graphs and parameters as mentioned in section
2.3. Luxburg et al. (2005) compared unnorma-
lized clustering with normalized version and
proved that normalized version always converges
to a sensible limit clustering while for unnorma-
lized case the same only holds under strong addi-
tional assumptions which are not always satisfied.
The running complexity of spectral clustering
equals to the complexity of computing the eigen-
vectors of Laplacian matrix which is 0 (|V |3).
However, when the graph is sparse, the complex-
ity is reduced to 0 (|V ||E |) by applying efficient
Lanczos algorithm.
The third set of algorithms is based on multi-
level graph partitioning paradigm (Karypis and
Kumar, 1999) which consists of three phases:
coarsening phase, initial partitioning phase and
refinement phase. Two approaches have been
developed in this category, one is multilevel re-
cursive bisection which recursively splits into
two parts by performing multilevel paradigm
with complexity of 0(|V|logK); the other is
multilevel K -way partitioning which performs
coarsening and refinement only once and directly
partitions the graph into K clusters with com-
plexity of 0 (|V  |+ KlogK). The latter approach
is superior to the former one for less running
complexity and comparable (sometimes better)
clustering quality.
The fourth set of algorithms is based on the
second interpretation of the hypothesis in section
2.2, i.e., a random walk is likely to visit many
vertices in a cluster before moving to the other
cluster. An outstanding approach in this category
is presented in Dogen (2000), named Markov
clustering algorithm (MCL). The algorithm itera-
tively applies two operators (expansion and infla-
tion) by matrix computation until convergence.
Expansion operator simulates spreading of ran-
dom walks and inflation models demotion of in-
ter-cluster walks; the sequence matrix computa-
tion results in eliminating inter-cluster interac-
tions and leaving only intra-cluster components.
The complexity of MCL is 0(m2|V |) where m is
the number of resources allocated for each vertex.
A key point of random walk is that it is actually
linked to spectral clustering (Luxburg, 2006),
e.g., ncut can be expressed in terms of transition
probabilities and optimizing ncut can be
achieved by computing the stationary distribu-
tion of a random walk in the graph.
The final set of algorithms in divisive category
is based on the third interpretation of the hypo-
thesis in section 2.2, i.e., the links between clus-
ters are likely to be in the shortest paths. Girvan
and Newman (2003) proposed the concept of
edge betweenness which is the number of short-
est paths connecting any pair of vertices that pass
through the edge. Their algorithm iteratively re-
moves one of the edges with the highest bet-
weenness. The complexity of the algorithm is
0 (|V ||E |2). Instead of betweenness, Fortunato et
al. (2004) used information centrality for each
edge and stated that it performs better than bet-
weenness but with a higher complexity of
0(|V||E|3).
The agglomerative category contains much
fewer algorithms. Newman (2004) proposed an
</bodyText>
<page confidence="0.987945">
4
</page>
<bodyText confidence="0.999982103448276">
algorithm that starts each vertex as singletons,
and then iteratively merges clusters together in
pairs, choosing the join that results in the greatest
increase (or smallest decrease) in modularity
score. The algorithm converges if there is only
cluster left in the graph, then from the clustering
hierarchy, we choose the clustering with maxi-
mum modularity. The complexity of the algo-
rithm is 𝑂𝑂(J𝑉𝑉JJ𝐸𝐸J).
The algorithms we surveyed in this section are
by no means comprehensive as the field is long-
standing and still evolving rapidly. We also refer
readers to other informative references, e.g.,
Schaeffer (2007), Brandes et al. (2007) and
Newman (2004).
A natural question arises: “which algorithm
should we choose?” A general answer to this
question is that no algorithm is a panacea. First,
as we mentioned earlier, a clustering algorithm is
usually proposed to optimize some quality meas-
ure, therefore, it is not fair to compare an algo-
rithm that favors one measure with the other one
that favors some other measure. Second, there is
not a perfect measure that captures the full cha-
racteristics of cluster structures; therefore a per-
fect algorithm does not exist. Third, there is no
definition for so called “best clustering”. The
“best” depends on applications, data characteris-
tics, and granularity.
</bodyText>
<subsectionHeader confidence="0.886554">
2.6 Evaluation
</subsectionHeader>
<bodyText confidence="0.970524872340425">
We discussed various quality measures in section
2.4, however, a clustering optimizing some
quality measure does not necessarily translate
into effectiveness in real applications with re-
spect to the ground truth clustering and thus an
evaluation measure plays the role of evaluating
how well the clustering matches the gold stan-
dard. Two questions arise: (1) what constraints
(properties, criteria) should an ideal evaluation
measure satisfy? (2) Do the evaluation measures
ever proposed satisfy the constraints?
For the first question, there have been several
attempts on it: Dom (2001) developed a parame-
tric technique for describing the quality of a clus-
tering and proposed five “desirable properties”
based on the parameters; Meila (2003) listed 12
properties associated with the proposed entropy
measure; Amigo et al. (2008) proposed four con-
straints including homogeneity, completeness,
rag bag, and cluster size vs. quantity. A parallel
comparison shows that the four constraints pro-
posed by Amigo et al. (2008) have advantages
over the constraints proposed in the other two
papers, for one reason, the four constraints can
describe all the important constraints in Dom
(2001) and Meila (2003), but the reverse does
not hold; for the other reason, the four con-
straints can be formally verified for each evalua-
tion measure, but it is not true for the constraints
in Dom (2001).
Table 3 lists the evaluation measures ever pro-
posed (including those discussed in Amigo et al.,
2008 and some other measures known for corefe-
rence resolution). To answer the second question
proposed in this section, we conclude the find-
ings in Amigo et al. (2008) plus our new findings
about MUC and CEAF as follows: (1) all the
measures except B-Cubed fail the rag bag con-
straint and only B-Cubed measure can satisfy all
the four constraints; (2) two entropy based meas-
ures (VI and V) and MUC only fail the rag bag
constraint; (3) all the measures in set mapping
category fail completeness constraint (4) all the
measures in pair counting category fail cluster
size vs. quantity constraint; (5) CEAF, unfortu-
nately, fails homogeneity, completeness, rag bag
constraints.
</bodyText>
<table confidence="0.997549727272727">
Category Evaluation Measures
set mapping purity, inverse purity, F-measure
pair counting rand index, Jaccard Coefficient,
Folks and Mallows FM
entropy entropy, mutual information, VI,
V
editing editing distance
distance
coreference MUC (Vilain et al.,1995),
resolution B-Cubed (Bagga and Baldwin,
1998), CEAF (Luo, 2005)
</table>
<tableCaption confidence="0.999565">
Table 3. Summary of Evaluation Measures
</tableCaption>
<sectionHeader confidence="0.934175" genericHeader="introduction">
3 Applying Graph Clustering to NLP
</sectionHeader>
<bodyText confidence="0.9999533125">
A variety of structures in NLP can be naturally
represented as graphs, e.g., co-occurrence graphs,
coreference graphs, word/sentence/ document
graphs. In recent years, there have been an in-
creasing amount of interests in applying graph-
based clustering to some NLP problems, e.g.,
document clustering (Zhong and Ghosh, 2004),
summarization (Zha, 2002), coreference resolu-
tion (Nicolae and Nicolae, 2006), word sense
disambiguation (Dorow and Widdows, 2003;
Véronis, 2004; Agirre et al., 2007), word cluster-
ing (Matsuo et al., 2006; Biemann, 2006). Many
authors chose one or two their favorite graph
clustering algorithms and claimed the effective-
ness by comparing with supervised algorithms
(which need expensive annotations) or other non-
</bodyText>
<page confidence="0.98499">
5
</page>
<bodyText confidence="0.999900764705882">
graph clustering algorithms. As far as we know,
there is not much work on the comparative study
of various graph-based clustering algorithms for
certain NLP problems. As mentioned at the end
of section 2.5, there is not a graph clustering al-
gorithm that is effective for all applications.
However, it is interesting to find out, for a spe-
cific NLP problem, if graph clustering methods
can be applied, (1) how the parameters in the
graph model affects the performance? (2) Does
the NLP problem favor some quality measure
and some graph clustering algorithm rather than
the others? Unfortunately, this survey neither
provides answers for these questions; instead, we
overview a few NLP case studies in which some
graph-based clustering methods have been suc-
cessfully applied.
</bodyText>
<subsectionHeader confidence="0.997976">
3.1 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999991689189189">
Coreference resolution is typically defined as the
problem of partitioning a set of mentions into
entities. An entity is an object or a set of objects
in the real world such as person, organization,
facility, while a mention is a textual reference to
an entity. The approaches to solving coreference
resolution have shifted from earlier linguistics-
based (rely on domain knowledge and hand-
crafted rules) to machine-learning based ap-
proaches. Elango (2005) and Chen (2010) pre-
sented a comprehensive survey on this topic. One
of the most prevalent approaches for coreference
resolution is to follow a two-step procedure: (1) a
classification step that computes how likely one
mention corefers with the other and (2) a
clustering step that groups the mentions into
clusters such that all mentions in a cluster refer
to the same entity. In the past years, NLP
researchers have explored and enriched this
methodogy from various directions (either in
classification or clustering step). Unfortunately,
most of the proposed clustering algorithms, e.g.,
closest-first clustering (Soon et al., 2001), best-
first clustering (Ng and Cardie, 2002), suffer
from a drawback: an instant decision is made (in
greedy style) when considering two mentions are
coreferent or not, therefore, the algorithm makes
no attempt to search through the space of all
possible clusterings, which results in a sub-
optimal clustering (Luo et al., 2004). Various
approaches have been proposed to alleviate this
problem, of which graph clustering methodology
is one of the most promising solutions.
The problem of coreference resolution can be
modeled as a graph such that the vertex
represents a mention, and the edge weight carries
the coreference likelihood between two mentions.
Nicolae and Nicolae (2006) proposed a new
quality measure named BESTCUT which is to
optimize the sum of “correctly” placed vertices
in the graph. The BESTCUT algorithm works by
performing recursive bisection (similar to Ker-
nighan-Lin algorithm) and in each iteration, it
searches the best cut that leads to partition into
halves. They compared BESTCUT algorithm
with (Luo et al., 2004)’s Belltree and (Ng and
Cardie, 2002)’s Link-Best algorithm and showed
that using ground-truth entities, BESTCUT out-
performs the other two with statistical signific-
ance (4.8% improvement over Belltree and Link-
Best algorithm in ECM F-measure). Nevertheless,
we believe that the BESTCUT algorithm is not
the only choice and the running complexity of
BESTCUT,O(|V||E |+ |V|2log|V|), is not com-
petitive, thus could be improved by other graph
clustering algorithms.
Chen and Ji (2009a) applied normalized spec-
tral algorithm to conduct event coreference reso-
lution: partitioning a set of mentions into events.
An event is a specific occurrence involving par-
ticipants. An event mention is a textual reference
to an event which includes a distinguished trig-
ger (the word that most clearly expresses an
event occurs) and involving arguments (enti-
ties/temporal expressions that play certain roles
in the event). A graph is similarly constructed as
in entity coreference resolution except that it in-
volves quite different feature engineering (most
features are related with event trigger and argu-
ments). The graph clustering approach yields
competitive results by comparing with an agglo-
merative clustering algorithm proposed in (Chen
et al., 2009b), unfortunately, a scientific compar-
ison among the algorithms remains unexplored.
</bodyText>
<subsectionHeader confidence="0.999879">
3.2 Word Clustering
</subsectionHeader>
<bodyText confidence="0.989829666666667">
Word clustering is a problem defined as cluster-
ing a set of words (e.g., nouns, verbs) into groups
so that similar words are in the same cluster.
Word clustering is a major technique that can
benefit many NLP tasks, e.g., thesaurus construc-
tion, text classification, and word sense disam-
biguation. Word clustering can be solved by fol-
lowing a two-step procedure: (1) classification
step by representing each word as a feature vec-
tor and computing the similarity of two words; (2)
clustering step which applies some clustering
algorithm, e.g., single-link clustering, complete-
link clustering, average-link clustering, such that
similar words are grouped together.
Matsuo et al. (2006) presented a graph cluster-
</bodyText>
<page confidence="0.998636">
6
</page>
<bodyText confidence="0.999955103448276">
ing algorithm for word clustering based on word
similarity measures by web counts. A word co-
occurrence graph is constructed in which the ver-
tex represents a word, and the edge weight is
computed by applying some similarity measure
(e.g., PMI, χ2) on a co-occurrence matrix, which
is the result of querying a pair of words to a
search engine. Then an agglomerative graph
clustering algorithm (Newman, 2004), which is
surveyed in section 2.5, is applied. They showed
that the similarity measure χ2 performs better
than PMI, for one reason, PMI performs worse
when a word group contains rare or frequent
words, for the other reason, PMI is sensitive to
web output inconsistency, e.g., the web count of
w1is below the web count of w1ANDw2 in ex-
treme case. They also showed that their graph
clustering algorithm outperforms average-link
agglomerative clustering by almost 32% using χ2
similarity measure. The concern of their ap-
proach is the running complexity for constructing
co-occurrence matrix, i.e., for n words, 0(n 2)
queries are required which is intractable for a
large graph.
Ichioka and Fukumoto (2008) applied similar
approach as Matsuo et al. (2006) for Japanese
Onomatopoetic word clustering, and showed that
the approach outperforms k-means clustering by
16.2%.
</bodyText>
<subsectionHeader confidence="0.999663">
3.3 Word Sense Disambiguation (WSD)
</subsectionHeader>
<bodyText confidence="0.999987041666667">
Word sense disambiguation is the problem of
identifying which sense of a word (meaning) is
conveyed in the context of a sentence, when the
word is polysemic. In contrast to supervised
WSD which relies on pre-defined list of senses
from dictionaries, unsupervised WSD induces
word senses directly from the corpus. Among
those unsupervised WSD algorithms, graph-
based clustering algorithms have been found
competitive with supervised methods, and in
many cases outperform most vector-based clus-
tering methods.
Dorow and Widdows (2003) built a co-
occurrence graph in which each node represents
a noun and two nodes have an edge between
them if they co-occur more than a given thre-
shold. They then applied Markov Clustering al-
gorithm (MCL) which is surveyed in section 2.5,
but cleverly circumvent the problem of choosing
the right parameters. Their algorithm not only
recognizes senses of polysemic words, but also
provides high-level readable cluster name for
each sense. Unfortunately, they neither discussed
further how to identify the sense of a word in a
given context, nor compared their algorithm with
other algorithms by conducting experiments.
Véronis (2004) proposed a graph based model
named HyperLex based on the small-world prop-
erties of co-occurrence graphs. Detecting the dif-
ferent senses (uses) of a word reduces to isolat-
ing the high-density components (hubs) in the
co-occurrence graph. Those hubs are then used to
perform WSD. To obtain the hubs, HyperLex
finds the vertex with highest relative frequency
in the graph at each iteration and if it meets some
criteria, it is selected as a hub. Agirre (2007)
proposed another method based on PageRank for
finding hubs. HyperLex can detect low-frequency
senses (as low as 1%) and most importantly, it
offers an excellent precision (97% compared to
73% for baseline). Agirre (2007) further con-
ducted extensive experiments by comparing the
two graph based models (HyperLex and Page-
Rank) with other supervised and non-supervised
graph methods and concluded that graph based
methods perform close to supervised systems in
the lexical sample task and yield the second-best
WSD systems for the Senseval-3 all-words task.
</bodyText>
<sectionHeader confidence="0.997438" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999752647058824">
In this survey, we organize the sparse related
literature of graph clustering into a structured
presentation and summarize the topic as a five
part story, namely, hypothesis, modeling, meas-
ure, algorithm, and evaluation. The hypothesis
serves as a basis for the whole graph clustering
methodology, quality measures and graph clus-
tering algorithms construct the backbone of the
methodology, modeling acts as the interface be-
tween the real application and the methodology,
and evaluation deals with utility. We also survey
several typical NLP problems, in which graph-
based clustering approaches have been success-
fully applied.
We have the following final comments on the
strengths and weaknesses of graph clustering
approaches:
</bodyText>
<listItem confidence="0.7173488">
(1) Graph is an elegant data structure that can
model many real applications with solid ma-
thematical foundations including spectral
theory, Markov stochastic process.
(2) Unlike many other clustering algorithms
</listItem>
<bodyText confidence="0.872162666666667">
which act greedily towards the final clustering
and thus may miss the optimal clustering,
graph clustering transforms the clustering
problem into optimizing some quality meas-
ure. Unfortunately, those optimization prob-
lems are NP-Hard, thus, all proposed graph
</bodyText>
<page confidence="0.997743">
7
</page>
<bodyText confidence="0.998100515151515">
clustering algorithms only approximately
yield “optimal” clustering.
(3) Graph clustering algorithms have been criti-
cized for low speed when working on large
scale graph (with millions of vertices). This
may not be true since new graph clustering
algorithms have been proposed, e.g., the mul-
tilevel graph clustering algorithm (Karypis
and Kumar, 1999) can partition a graph with
one million vertices into 256 clusters in a few
seconds on current generation workstations
and PCs. Nevertheless, scalability problem of
graph clustering algorithm still needs to be
explored which is becoming more important
in social network study.
We envision that graph clustering methods can
lead to promising solutions in the following
emerging NLP problems:
(1) Detection of new entity types, relation types
and event types (IE area). For example, the
eight event types defined in the ACE1 pro-
gram may not be enough for wider usage and
more event types can be induced by graph
clustering on verbs.
(2) Web people search (IR area). The main issue
in web people search is the ambiguity of the
person name. Thus by extracting attributes
(e.g., attended schools, spouse, children,
friends) from returned web pages, construct-
ing person graphs (involving those attributes)
and applying graph clustering, we are opti-
mistic to achieve a better person search en-
gine.
</bodyText>
<sectionHeader confidence="0.997303" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9989018125">
This work was supported by the U.S. National
Science Foundation Faculty Early Career Devel-
opment (CAREER) Award under Grant IIS-
0953149, the U.S. Army Research Laboratory
under Cooperative Agreement Number
W911NF-09-2-0053, Google, Inc., CUNY Re-
search Enhancement Program, Faculty Publica-
tion Program and GRTI Program. The views and
conclusions contained in this document are those
of the authors and should not be interpreted as
representing the official policies, either ex-
pressed or implied, of the Army Research Labor-
atory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstand-
ing any copyright notation here on.
</bodyText>
<footnote confidence="0.928336">
1 http://www.nist.gov/speech/tests/ace/
</footnote>
<sectionHeader confidence="0.89974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999310235294118">
A. Bagga and B. Baldwin.1998. Algorithms for scor-
ing coreference chains. Proc. The First Interna-
tional Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference.
A. Clauset. 2005. Finding local community structure
in networks. Physical Review E, 72:026132.
B. Dom. 2001. An information-theoretic external
cluster-validity measure. IBM Research Report.
B. Dorow, D. Widdows. 2003. Discovering corpus-
specific word-senses. In Proc. EACL.
B. W. Kernighan and S. Lin.1970. An efficient heuris-
tic procedur for partitioning graphs. Bell Syst.
Techn. J.,Vol. 49, No. 2, pp. 291–307.
C. Biemann. 2006. Chinese Whispers - an Efficient-
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proc. of
the HLT-NAACL-06 Workshop on Textgraphs-06.
C. Nicolae and G. Nicolae. 2006. Bestcut: A graph
algorithm for coreference resolution. In EMNLP,
pages 275–283, Sydney, Australia.
E. Agirre, D. Martinez, O.L. de Lacalle and A.Soroa.
2007. Two graph-based algorithms for state-of-the-
art WSD. In Proc. EMNLP.
E. Amigo, J. Gonzalo, J. Artiles and F. Verdejo.
2008. A comparison of extrinsic clustering evalua-
tion metrics based on formal constraints. Informa-
tion Retrieval.
E. Terra and C. L. A. Clarke. Frequency Estimates for
Statistical Word Similarity Measures. In Proc.
HLT/NAACL 2003.
G. Karypis and V. Kumar. 1999. Multilevel algo-
rithms for multiconstraint graph partitioning. in
Proceedings of the 36th ACM/IEEE conference on
Design automation conference, (New Orleans,
Louisiana), pp. 343 – 348.
G. W. Flake, R. E. Tarjan and K. Tsioutsiouliklis.
2003. Graph clustering and minimum cut trees. In-
ternet Mathematics, 1(4):385–408.
H. Zha.2002. Generic summarization and keyphrase
extraction using mutual reinforcement principle
and sentence clustering. In Proc. of SIGIR2002, pp.
113-120.
J. Chen, O. R. Zaïane, R. Goebel. 2009. Detecting
Communities in Social Networks Using Max-Min
Modularity. SDM 2009: 978-989.
J. Ruan and W. Zhang.2008. Identifying network
communities with a high resolution. Physical Re-
view E, 77:016104.
J. Shi and J. Malik. 2000. Normalized Cuts and Image
Segmentation. IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 22, no. 8, pp. 888-905.
</reference>
<page confidence="0.976288">
8
</page>
<reference confidence="0.999884684782609">
J. Véronis. 2004. HyperLex: Lexical Cartography for
Information Retrieval. Computer Speech &amp; Lan-
guage 18(3).
K. Ichioka and F. Fukumoto. 2008. Graph-based
clustering for semantic classification of onomato-
poetic words. In Proc. of the 3rd Textgraphs Work-
shop on Graph-based Algorithms for Natural Lan-
guage Processing.
L. Hagen and A. B. Kahng. 1992. New spectral me-
thods for ratio cut partitioning and clustering. IEEE
Transactions Computer-Aided Design, Santa Clara
CA, 422-427.
L. R. Ford, D. R. Fulkerson. 1956. Maximal flow
through a network. Canadian Journal of Mathe-
matics 8: 399–404.
M. E. J. Newman. 2004. Detecting community struc-
ture in networks. Eur. Phys. J. B, 38, 321–330.
M. E. J. Newman. 2004. Fast algorithm for detecting
community structure in networks. Phys Rev E. 69,
2004.
M. E. J. Newman and M. Girvan. 2004. Finding and
evaluating community structure in networks. Phys.
Rev. E 69,026113.
M. Girvan and M. E. J. Newman. 2002. Community
structure in social and biological networks. Proc.
Natl. Acad. Sci. USA 99, 7821-7826.
M. Meila. 2003. Comparing clusterings. In Proceed-
ings of COLT03.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L.
Hirschman.1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6).
P. Elango. 2005. Coreference Resolution: A Survey.
Technical Report, University of Wisconsin Madi-
son.
R. Kannan, S. Vempala, and A. Vetta. 2000. On clus-
terings:good, bad and spectral. In Proceedings of
the 41st Annual Symposium on Foundations of
Computer Science.
S. Abney. 2007. Semi-supervised Learning for Com-
putational Linguistics, Chapman and Hall.
S. E. Schaeffer. 2007. Graph clustering. Computer
Science Review, 1(1):27–64.
S. Fortunato, V. Latora, and M. Marchiori. 2004. A
Method to Find Community Structures Based on
Information Centrality. Phys Rev E.70, 056104.
S. van Dongen. 2000. Graph Clustering by Flow Si-
mulation. PhD thesis, University of Utrecht.
S. White and P. Smyth. 2005. A spectral clustering
approach to finding communities in graphs. In
SIAM International Conference on Data Mining.
U. Brandes, M. Gaertler, and D. Wagner. 2003. Expe-
riments on graph clustering algorithms. Proc. 11th
European Symp. Algorithms, LNCS 2832:568-579.
U. Brandes, M. Gaertler, and D.Wagner. 2007. Engi-
neering graph clustering: Models and experimental
evaluation. J. Exp. Algorithmics, 12:1.1.
U. Luxburg, O. Bousquet, M. Belkin. 2005. Limits of
spectral clustering. In L. K. Saul, Y. Weiss and L.
Bottou (Eds.), Advances in neural information
processing systems 17. Cambridge, MA: MIT Press.
U. Luxburg.2006. A tutorial on spectral clustering.
Technical Report 149, Max Plank Institute for Bio-
logical Cybernetics.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc.
Of the ACL, pages 104–111.
W. M. Soon, H. T. Ng and D. Lim.2001. A machine
learning approach to coreference resolution of
noun phrases. Computational Linguistics,
27(4):521–544.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla and S.
Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the Bell Tree. In
Proc. of ACL-04, pp.136–143.
Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006. Graph-based word clustering using web
search engine. In Proc. of EMNLP 2006.
Z. Chen and H. Ji. 2009a. Graph-based Event Corefe-
rence Resolution. In Proc. ACL-IJCNLP 2009
workshop on TextGraphs-4: Graph-based Methods
for Natural Language Processing.
Z. Chen, H. Ji, R. Haralick. 2009b. A Pairwise Core-
ference Model, Feature Impact and Evaluation for
Event Coreference Resolution. In Proc. RANLP
2009 workshop on Events in Emerging Text Types.
Z. Chen. 2010. Graph-based Clustering and its Appli-
cation in Coreference Resolution. Technical Report,
the Graduate Center, the City University of New
York.
</reference>
<page confidence="0.997082">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806023">
<title confidence="0.975079">Graph-based Clustering for Computational Linguistics: A Survey</title>
<author confidence="0.999838">Zheng Chen Heng Ji</author>
<affiliation confidence="0.998475">The Graduate Center Queens College and The Graduate Center</affiliation>
<address confidence="0.893024">The City University of New York The City University of New York</address>
<abstract confidence="0.994299846153846">In this survey we overview graph-based clustering and its applications in computational linguistics. We summarize graph-based clusas a five-part story: modelmeasure, algorithm We then survey three typical NLP problems in which graph-based clustering approaches have been successfully applied. Finally, we comment on the strengths and weaknesses of graph-based clustering and envision that graph-based clustering is a promising solution for some emerging NLP problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>A Bagga</author>
<author>B Baldwin 1998</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<booktitle>Proc. The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference.</booktitle>
<marker>Bagga, 1998, </marker>
<rawString>A. Bagga and B. Baldwin.1998. Algorithms for scoring coreference chains. Proc. The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clauset</author>
</authors>
<title>Finding local community structure in networks. Physical Review E,</title>
<date>2005</date>
<pages>72--026132</pages>
<contexts>
<context position="9136" citStr="Clauset (2005)" startWordPosition="1411" endWordPosition="1412">luster density and inter-cluster density Brandes et al., 2003) into considerations simultaneously expansion, conductance, ― Expansion is suitable for unweighted graph, and conductance is a better bicriteria ― choice for weighted graph (Kannan et al., 2000) Both expansion and conductance impose quality within clusters, but not inter-cluster quality; bicriteria takes both into considerations modularity (Newman and ― Evaluates the quality of clustering with respect to a randomized graph Girvan,2004) ― Drawbacks: (1) It requires global knowledge of the graph’s topology, i.e., the number of edges. Clauset (2005) proposed an improved measure Local Modularity. (2) Resolution limit problem: it fails to identify clusters smaller than a certain scale. Ruan and Zhang (2008) proposed an improved measure HQcut. (3) It fails to distinguish good from bad clustering between different graphs with the same modularity value. Chen et al. (2009) proposed an improved measure Max-Min Modularity Table 1. Summary of Quality Measures Category Algorithms optimized running measure complexity divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3) (Kernighan and Lin, 1970) cut-clustering algorithm bicriteria 0 (|V|</context>
</contexts>
<marker>Clauset, 2005</marker>
<rawString>A. Clauset. 2005. Finding local community structure in networks. Physical Review E, 72:026132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dom</author>
</authors>
<title>An information-theoretic external cluster-validity measure.</title>
<date>2001</date>
<journal>IBM Research Report.</journal>
<contexts>
<context position="17835" citStr="Dom (2001)" startWordPosition="2755" endWordPosition="2756">larity. 2.6 Evaluation We discussed various quality measures in section 2.4, however, a clustering optimizing some quality measure does not necessarily translate into effectiveness in real applications with respect to the ground truth clustering and thus an evaluation measure plays the role of evaluating how well the clustering matches the gold standard. Two questions arise: (1) what constraints (properties, criteria) should an ideal evaluation measure satisfy? (2) Do the evaluation measures ever proposed satisfy the constraints? For the first question, there have been several attempts on it: Dom (2001) developed a parametric technique for describing the quality of a clustering and proposed five “desirable properties” based on the parameters; Meila (2003) listed 12 properties associated with the proposed entropy measure; Amigo et al. (2008) proposed four constraints including homogeneity, completeness, rag bag, and cluster size vs. quantity. A parallel comparison shows that the four constraints proposed by Amigo et al. (2008) have advantages over the constraints proposed in the other two papers, for one reason, the four constraints can describe all the important constraints in Dom (2001) and</context>
</contexts>
<marker>Dom, 2001</marker>
<rawString>B. Dom. 2001. An information-theoretic external cluster-validity measure. IBM Research Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorow</author>
<author>D Widdows</author>
</authors>
<title>Discovering corpusspecific word-senses.</title>
<date>2003</date>
<booktitle>In Proc. EACL.</booktitle>
<contexts>
<context position="20253" citStr="Dorow and Widdows, 2003" startWordPosition="3131" endWordPosition="3134">ence MUC (Vilain et al.,1995), resolution B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 graph clustering algorithms. As far as we know, there is not much work on the comparative study of various graph-based clustering algorithms for certain NLP problems. As mentioned at the end of section 2.5, there is not a graph clustering algorithm that is effective for all applications. However, it is interesting</context>
<context position="27360" citStr="Dorow and Widdows (2003)" startWordPosition="4239" endWordPosition="4242">at the approach outperforms k-means clustering by 16.2%. 3.3 Word Sense Disambiguation (WSD) Word sense disambiguation is the problem of identifying which sense of a word (meaning) is conveyed in the context of a sentence, when the word is polysemic. In contrast to supervised WSD which relies on pre-defined list of senses from dictionaries, unsupervised WSD induces word senses directly from the corpus. Among those unsupervised WSD algorithms, graphbased clustering algorithms have been found competitive with supervised methods, and in many cases outperform most vector-based clustering methods. Dorow and Widdows (2003) built a cooccurrence graph in which each node represents a noun and two nodes have an edge between them if they co-occur more than a given threshold. They then applied Markov Clustering algorithm (MCL) which is surveyed in section 2.5, but cleverly circumvent the problem of choosing the right parameters. Their algorithm not only recognizes senses of polysemic words, but also provides high-level readable cluster name for each sense. Unfortunately, they neither discussed further how to identify the sense of a word in a given context, nor compared their algorithm with other algorithms by conduct</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>B. Dorow, D. Widdows. 2003. Discovering corpusspecific word-senses. In Proc. EACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B W Kernighan</author>
<author>S Lin 1970</author>
</authors>
<title>An efficient heuristic procedur for partitioning graphs.</title>
<journal>Bell Syst. Techn.</journal>
<volume>49</volume>
<pages>291--307</pages>
<marker>Kernighan, 1970, </marker>
<rawString>B. W. Kernighan and S. Lin.1970. An efficient heuristic procedur for partitioning graphs. Bell Syst. Techn. J.,Vol. 49, No. 2, pp. 291–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Chinese Whispers - an EfficientGraph Clustering Algorithm and its Application to Natural Language Processing Problems.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT-NAACL-06 Workshop on Textgraphs-06.</booktitle>
<contexts>
<context position="20344" citStr="Biemann, 2006" startWordPosition="3148" endWordPosition="3149">3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 graph clustering algorithms. As far as we know, there is not much work on the comparative study of various graph-based clustering algorithms for certain NLP problems. As mentioned at the end of section 2.5, there is not a graph clustering algorithm that is effective for all applications. However, it is interesting to find out, for a specific NLP problem, if graph clustering methods can be applied, (1) h</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006. Chinese Whispers - an EfficientGraph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proc. of the HLT-NAACL-06 Workshop on Textgraphs-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nicolae</author>
<author>G Nicolae</author>
</authors>
<title>Bestcut: A graph algorithm for coreference resolution. In</title>
<date>2006</date>
<booktitle>EMNLP,</booktitle>
<pages>275--283</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="20201" citStr="Nicolae and Nicolae, 2006" startWordPosition="3124" endWordPosition="3127">mation, VI, V editing editing distance distance coreference MUC (Vilain et al.,1995), resolution B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 graph clustering algorithms. As far as we know, there is not much work on the comparative study of various graph-based clustering algorithms for certain NLP problems. As mentioned at the end of section 2.5, there is not a graph clustering algorithm that is effect</context>
<context position="23110" citStr="Nicolae and Nicolae (2006)" startWordPosition="3580" endWordPosition="3583">m a drawback: an instant decision is made (in greedy style) when considering two mentions are coreferent or not, therefore, the algorithm makes no attempt to search through the space of all possible clusterings, which results in a suboptimal clustering (Luo et al., 2004). Various approaches have been proposed to alleviate this problem, of which graph clustering methodology is one of the most promising solutions. The problem of coreference resolution can be modeled as a graph such that the vertex represents a mention, and the edge weight carries the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (similar to Kernighan-Lin algorithm) and in each iteration, it searches the best cut that leads to partition into halves. They compared BESTCUT algorithm with (Luo et al., 2004)’s Belltree and (Ng and Cardie, 2002)’s Link-Best algorithm and showed that using ground-truth entities, BESTCUT outperforms the other two with statistical significance (4.8% improvement over Belltree and LinkBest algorithm in ECM F-measure)</context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>C. Nicolae and G. Nicolae. 2006. Bestcut: A graph algorithm for coreference resolution. In EMNLP, pages 275–283, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
<author>O L</author>
</authors>
<title>de Lacalle and A.Soroa.</title>
<date>2007</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="20290" citStr="Agirre et al., 2007" startWordPosition="3137" endWordPosition="3140">n B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 graph clustering algorithms. As far as we know, there is not much work on the comparative study of various graph-based clustering algorithms for certain NLP problems. As mentioned at the end of section 2.5, there is not a graph clustering algorithm that is effective for all applications. However, it is interesting to find out, for a specific NLP prob</context>
</contexts>
<marker>Agirre, Martinez, L, 2007</marker>
<rawString>E. Agirre, D. Martinez, O.L. de Lacalle and A.Soroa. 2007. Two graph-based algorithms for state-of-theart WSD. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Amigo</author>
<author>J Gonzalo</author>
<author>J Artiles</author>
<author>F Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval.</title>
<date>2008</date>
<contexts>
<context position="18077" citStr="Amigo et al. (2008)" startWordPosition="2790" endWordPosition="2793">h clustering and thus an evaluation measure plays the role of evaluating how well the clustering matches the gold standard. Two questions arise: (1) what constraints (properties, criteria) should an ideal evaluation measure satisfy? (2) Do the evaluation measures ever proposed satisfy the constraints? For the first question, there have been several attempts on it: Dom (2001) developed a parametric technique for describing the quality of a clustering and proposed five “desirable properties” based on the parameters; Meila (2003) listed 12 properties associated with the proposed entropy measure; Amigo et al. (2008) proposed four constraints including homogeneity, completeness, rag bag, and cluster size vs. quantity. A parallel comparison shows that the four constraints proposed by Amigo et al. (2008) have advantages over the constraints proposed in the other two papers, for one reason, the four constraints can describe all the important constraints in Dom (2001) and Meila (2003), but the reverse does not hold; for the other reason, the four constraints can be formally verified for each evaluation measure, but it is not true for the constraints in Dom (2001). Table 3 lists the evaluation measures ever pr</context>
</contexts>
<marker>Amigo, Gonzalo, Artiles, Verdejo, 2008</marker>
<rawString>E. Amigo, J. Gonzalo, J. Artiles and F. Verdejo. 2008. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Terra</author>
<author>C L A Clarke</author>
</authors>
<title>Frequency Estimates for Statistical Word Similarity Measures.</title>
<date>2003</date>
<booktitle>In Proc. HLT/NAACL</booktitle>
<marker>Terra, Clarke, 2003</marker>
<rawString>E. Terra and C. L. A. Clarke. Frequency Estimates for Statistical Word Similarity Measures. In Proc. HLT/NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Karypis</author>
<author>V Kumar</author>
</authors>
<title>Multilevel algorithms for multiconstraint graph partitioning.</title>
<date>1999</date>
<booktitle>in Proceedings of the 36th ACM/IEEE conference on Design automation conference,</booktitle>
<pages>343--348</pages>
<location>(New Orleans, Louisiana),</location>
<contexts>
<context position="10333" citStr="Karypis and Kumar, 1999" startWordPosition="1575" endWordPosition="1578">algorithm bicriteria 0 (|V|) (Flake et al., 2003) spectral unnormalized spectral clustering ratiocut 0 (|V||E|) (Luxburg, 2006) normalized spectral clustering I ncut 0 (|V||E|) (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II ncut 0 (|V ||E |) (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) conductance 0(|V ||E|) (Kannan et al.,2000) geometric MST clustering (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partitioning intercluster 0 (|V| (Karypis and Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modularity 0 (|V ||E |2) path (Girvan and Newman, 2003) information centrality modularity 0(|V||E |3) (Fortunato et al., 2004) agglomerative modularity oriented modularity 0 (|V ||E |) (Newman, 2004) Table 2. Summary of Graph-based Clustering Algorithms (|V |: the number of vertices, |E |: the number of edges, K: the number of clusters, m: the number of resources allocated for each vertex) 3 The first set of algo</context>
<context position="13521" citStr="Karypis and Kumar, 1999" startWordPosition="2072" endWordPosition="2075">005) compared unnormalized clustering with normalized version and proved that normalized version always converges to a sensible limit clustering while for unnormalized case the same only holds under strong additional assumptions which are not always satisfied. The running complexity of spectral clustering equals to the complexity of computing the eigenvectors of Laplacian matrix which is 0 (|V |3). However, when the graph is sparse, the complexity is reduced to 0 (|V ||E |) by applying efficient Lanczos algorithm. The third set of algorithms is based on multilevel graph partitioning paradigm (Karypis and Kumar, 1999) which consists of three phases: coarsening phase, initial partitioning phase and refinement phase. Two approaches have been developed in this category, one is multilevel recursive bisection which recursively splits into two parts by performing multilevel paradigm with complexity of 0(|V|logK); the other is multilevel K -way partitioning which performs coarsening and refinement only once and directly partitions the graph into K clusters with complexity of 0 (|V |+ KlogK). The latter approach is superior to the former one for less running complexity and comparable (sometimes better) clustering </context>
<context position="30568" citStr="Karypis and Kumar, 1999" startWordPosition="4732" endWordPosition="4735">algorithms which act greedily towards the final clustering and thus may miss the optimal clustering, graph clustering transforms the clustering problem into optimizing some quality measure. Unfortunately, those optimization problems are NP-Hard, thus, all proposed graph 7 clustering algorithms only approximately yield “optimal” clustering. (3) Graph clustering algorithms have been criticized for low speed when working on large scale graph (with millions of vertices). This may not be true since new graph clustering algorithms have been proposed, e.g., the multilevel graph clustering algorithm (Karypis and Kumar, 1999) can partition a graph with one million vertices into 256 clusters in a few seconds on current generation workstations and PCs. Nevertheless, scalability problem of graph clustering algorithm still needs to be explored which is becoming more important in social network study. We envision that graph clustering methods can lead to promising solutions in the following emerging NLP problems: (1) Detection of new entity types, relation types and event types (IE area). For example, the eight event types defined in the ACE1 program may not be enough for wider usage and more event types can be induced</context>
</contexts>
<marker>Karypis, Kumar, 1999</marker>
<rawString>G. Karypis and V. Kumar. 1999. Multilevel algorithms for multiconstraint graph partitioning. in Proceedings of the 36th ACM/IEEE conference on Design automation conference, (New Orleans, Louisiana), pp. 343 – 348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Flake</author>
<author>R E Tarjan</author>
<author>K Tsioutsiouliklis</author>
</authors>
<title>Graph clustering and minimum cut trees.</title>
<date>2003</date>
<journal>Internet Mathematics,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="9758" citStr="Flake et al., 2003" startWordPosition="1499" endWordPosition="1502">oposed an improved measure Local Modularity. (2) Resolution limit problem: it fails to identify clusters smaller than a certain scale. Ruan and Zhang (2008) proposed an improved measure HQcut. (3) It fails to distinguish good from bad clustering between different graphs with the same modularity value. Chen et al. (2009) proposed an improved measure Max-Min Modularity Table 1. Summary of Quality Measures Category Algorithms optimized running measure complexity divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3) (Kernighan and Lin, 1970) cut-clustering algorithm bicriteria 0 (|V|) (Flake et al., 2003) spectral unnormalized spectral clustering ratiocut 0 (|V||E|) (Luxburg, 2006) normalized spectral clustering I ncut 0 (|V||E|) (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II ncut 0 (|V ||E |) (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) conductance 0(|V ||E|) (Kannan et al.,2000) geometric MST clustering (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partiti</context>
<context position="11449" citStr="Flake et al. (2003)" startWordPosition="1749" endWordPosition="1752">: the number of clusters, m: the number of resources allocated for each vertex) 3 The first set of algorithms (cut-based) is associated with max-flow min-cut theorem (Ford and Fulkerson, 1956) which states that “the value of the maximum flow is equal to the cost of the minimum cut”. One of the earliest algorithm, Kernighan-Lin algorithm (Kernighan and Lin, 1970) splits the graph by performing recursive bisection (split into two parts at a time), aiming to minimize inter-cluster density (cut size). The high complexity of the algorithm ( 0 (|V |3) makes it less competitive in real applications. Flake et al. (2003) proposed a cut-clustering algorithm which optimizes the bicriterion measure and the complexity is proportional to the number of clusters K using a heuristic, thus the algorithm is competitive in practice. The second set of algorithms is based on spectral graph theory with Laplacian matrix as the mathematical tool. The connection between clustering and spectrum of Laplacian matrix (L) basically lies in the following important proposition: the multiplicity k of the eigenvalue 0 of L equals to the number of connected components in the graph. Luxburg (2006) and Abney (2007) presented a comprehens</context>
</contexts>
<marker>Flake, Tarjan, Tsioutsiouliklis, 2003</marker>
<rawString>G. W. Flake, R. E. Tarjan and K. Tsioutsiouliklis. 2003. Graph clustering and minimum cut trees. Internet Mathematics, 1(4):385–408.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Zha 2002</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<booktitle>In Proc. of SIGIR2002,</booktitle>
<pages>113--120</pages>
<marker>2002, </marker>
<rawString>H. Zha.2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proc. of SIGIR2002, pp. 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>O R Zaïane</author>
<author>R Goebel</author>
</authors>
<title>Detecting Communities in Social Networks Using Max-Min Modularity. SDM</title>
<date>2009</date>
<pages>978--989</pages>
<contexts>
<context position="9460" citStr="Chen et al. (2009)" startWordPosition="1461" endWordPosition="1464">but not inter-cluster quality; bicriteria takes both into considerations modularity (Newman and ― Evaluates the quality of clustering with respect to a randomized graph Girvan,2004) ― Drawbacks: (1) It requires global knowledge of the graph’s topology, i.e., the number of edges. Clauset (2005) proposed an improved measure Local Modularity. (2) Resolution limit problem: it fails to identify clusters smaller than a certain scale. Ruan and Zhang (2008) proposed an improved measure HQcut. (3) It fails to distinguish good from bad clustering between different graphs with the same modularity value. Chen et al. (2009) proposed an improved measure Max-Min Modularity Table 1. Summary of Quality Measures Category Algorithms optimized running measure complexity divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3) (Kernighan and Lin, 1970) cut-clustering algorithm bicriteria 0 (|V|) (Flake et al., 2003) spectral unnormalized spectral clustering ratiocut 0 (|V||E|) (Luxburg, 2006) normalized spectral clustering I ncut 0 (|V||E|) (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II ncut 0 (|V ||E |) (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) conductance 0(|V ||E|) </context>
<context position="24705" citStr="Chen et al., 2009" startWordPosition="3824" endWordPosition="3827">pecific occurrence involving participants. An event mention is a textual reference to an event which includes a distinguished trigger (the word that most clearly expresses an event occurs) and involving arguments (entities/temporal expressions that play certain roles in the event). A graph is similarly constructed as in entity coreference resolution except that it involves quite different feature engineering (most features are related with event trigger and arguments). The graph clustering approach yields competitive results by comparing with an agglomerative clustering algorithm proposed in (Chen et al., 2009b), unfortunately, a scientific comparison among the algorithms remains unexplored. 3.2 Word Clustering Word clustering is a problem defined as clustering a set of words (e.g., nouns, verbs) into groups so that similar words are in the same cluster. Word clustering is a major technique that can benefit many NLP tasks, e.g., thesaurus construction, text classification, and word sense disambiguation. Word clustering can be solved by following a two-step procedure: (1) classification step by representing each word as a feature vector and computing the similarity of two words; (2) clustering step </context>
</contexts>
<marker>Chen, Zaïane, Goebel, 2009</marker>
<rawString>J. Chen, O. R. Zaïane, R. Goebel. 2009. Detecting Communities in Social Networks Using Max-Min Modularity. SDM 2009: 978-989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Ruan</author>
<author>W Zhang 2008</author>
</authors>
<title>Identifying network communities with a high resolution. Physical Review E,</title>
<pages>77--016104</pages>
<marker>Ruan, 2008, </marker>
<rawString>J. Ruan and W. Zhang.2008. Identifying network communities with a high resolution. Physical Review E, 77:016104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shi</author>
<author>J Malik</author>
</authors>
<title>Normalized Cuts and Image Segmentation.</title>
<date>2000</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<pages>888--905</pages>
<contexts>
<context position="8186" citStr="Shi and Malik, 2000" startWordPosition="1269" endWordPosition="1272">hs. The divisive and agglomerative algorithms are also called hierarchical since they produce multi-level clusterings, i.e., one clustering follows the other by refining (divisive) or coarsening (agglomerative). Most graph clustering algorithms ever proposed are divisive. We list the quality measure and the running complexity for each algorithm in Table 2. 2 Measures Comments intra-cluster density ― Maximizing intra-cluster density is equivalent to minimizing inter-cluster inter-cluster density ― density and vice versa Drawback: both favor cutting small sets of isolated vertices in the graph (Shi and Malik, 2000) ratio cut (Hagan and Kahng, ― Ratio cut is suitable for unweighted graph, and ncut is a better choice for 1992) ― weighted graph ncut (Shi and Malik, 2000) ― Overcome the drawback of intra-cluster density or inter-cluster density Drawback: both favor clusters with equal size performance (Dongen, 2000; ― Performance takes both intra-cluster density and inter-cluster density Brandes et al., 2003) into considerations simultaneously expansion, conductance, ― Expansion is suitable for unweighted graph, and conductance is a better bicriteria ― choice for weighted graph (Kannan et al., 2000) Both ex</context>
<context position="9922" citStr="Shi and Malik, 2000" startWordPosition="1521" endWordPosition="1524">ed an improved measure HQcut. (3) It fails to distinguish good from bad clustering between different graphs with the same modularity value. Chen et al. (2009) proposed an improved measure Max-Min Modularity Table 1. Summary of Quality Measures Category Algorithms optimized running measure complexity divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3) (Kernighan and Lin, 1970) cut-clustering algorithm bicriteria 0 (|V|) (Flake et al., 2003) spectral unnormalized spectral clustering ratiocut 0 (|V||E|) (Luxburg, 2006) normalized spectral clustering I ncut 0 (|V||E|) (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II ncut 0 (|V ||E |) (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) conductance 0(|V ||E|) (Kannan et al.,2000) geometric MST clustering (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partitioning intercluster 0 (|V| (Karypis and Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modul</context>
<context position="12440" citStr="Shi and Malik, 2000" startWordPosition="1902" endWordPosition="1905">acian matrix (L) basically lies in the following important proposition: the multiplicity k of the eigenvalue 0 of L equals to the number of connected components in the graph. Luxburg (2006) and Abney (2007) presented a comprehensive tutorial on spectral clustering. Luxburg (2006) discussed three forms of Laplacian matrices (one unnormalized form and two normalized forms) and their three corresponding spectral clustering algorithms (unnormalized, normalized I and normalized II). Unnormalized clustering aims to optimize ratiocut measure while normalized clustering aims to optimize ncut measure (Shi and Malik, 2000), thus spectral clustering actually relates with cut-based clustering. The success of spectral clustering is mainly based on the fact that it does not make strong assumptions on the form of the clusters and can solve very general problems like intertwined spirals which k-means clustering handles much worse. Unfortunately, spectral clustering could be unstable under different choices of graphs and parameters as mentioned in section 2.3. Luxburg et al. (2005) compared unnormalized clustering with normalized version and proved that normalized version always converges to a sensible limit clusterin</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>J. Shi and J. Malik. 2000. Normalized Cuts and Image Segmentation. IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888-905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Véronis</author>
</authors>
<title>HyperLex: Lexical Cartography for Information Retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="20268" citStr="Véronis, 2004" startWordPosition="3135" endWordPosition="3136">995), resolution B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 graph clustering algorithms. As far as we know, there is not much work on the comparative study of various graph-based clustering algorithms for certain NLP problems. As mentioned at the end of section 2.5, there is not a graph clustering algorithm that is effective for all applications. However, it is interesting to find out, f</context>
<context position="27991" citStr="Véronis (2004)" startWordPosition="4342" endWordPosition="4343">ence graph in which each node represents a noun and two nodes have an edge between them if they co-occur more than a given threshold. They then applied Markov Clustering algorithm (MCL) which is surveyed in section 2.5, but cleverly circumvent the problem of choosing the right parameters. Their algorithm not only recognizes senses of polysemic words, but also provides high-level readable cluster name for each sense. Unfortunately, they neither discussed further how to identify the sense of a word in a given context, nor compared their algorithm with other algorithms by conducting experiments. Véronis (2004) proposed a graph based model named HyperLex based on the small-world properties of co-occurrence graphs. Detecting the different senses (uses) of a word reduces to isolating the high-density components (hubs) in the co-occurrence graph. Those hubs are then used to perform WSD. To obtain the hubs, HyperLex finds the vertex with highest relative frequency in the graph at each iteration and if it meets some criteria, it is selected as a hub. Agirre (2007) proposed another method based on PageRank for finding hubs. HyperLex can detect low-frequency senses (as low as 1%) and most importantly, it o</context>
</contexts>
<marker>Véronis, 2004</marker>
<rawString>J. Véronis. 2004. HyperLex: Lexical Cartography for Information Retrieval. Computer Speech &amp; Language 18(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ichioka</author>
<author>F Fukumoto</author>
</authors>
<title>Graph-based clustering for semantic classification of onomatopoetic words.</title>
<date>2008</date>
<booktitle>In Proc. of the 3rd Textgraphs Workshop on Graph-based Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="26629" citStr="Ichioka and Fukumoto (2008)" startWordPosition="4131" endWordPosition="4134">ty measure χ2 performs better than PMI, for one reason, PMI performs worse when a word group contains rare or frequent words, for the other reason, PMI is sensitive to web output inconsistency, e.g., the web count of w1is below the web count of w1ANDw2 in extreme case. They also showed that their graph clustering algorithm outperforms average-link agglomerative clustering by almost 32% using χ2 similarity measure. The concern of their approach is the running complexity for constructing co-occurrence matrix, i.e., for n words, 0(n 2) queries are required which is intractable for a large graph. Ichioka and Fukumoto (2008) applied similar approach as Matsuo et al. (2006) for Japanese Onomatopoetic word clustering, and showed that the approach outperforms k-means clustering by 16.2%. 3.3 Word Sense Disambiguation (WSD) Word sense disambiguation is the problem of identifying which sense of a word (meaning) is conveyed in the context of a sentence, when the word is polysemic. In contrast to supervised WSD which relies on pre-defined list of senses from dictionaries, unsupervised WSD induces word senses directly from the corpus. Among those unsupervised WSD algorithms, graphbased clustering algorithms have been fou</context>
</contexts>
<marker>Ichioka, Fukumoto, 2008</marker>
<rawString>K. Ichioka and F. Fukumoto. 2008. Graph-based clustering for semantic classification of onomatopoetic words. In Proc. of the 3rd Textgraphs Workshop on Graph-based Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hagen</author>
<author>A B Kahng</author>
</authors>
<title>New spectral methods for ratio cut partitioning and clustering.</title>
<date>1992</date>
<journal>IEEE Transactions Computer-Aided Design,</journal>
<pages>422--427</pages>
<location>Santa Clara CA,</location>
<marker>Hagen, Kahng, 1992</marker>
<rawString>L. Hagen and A. B. Kahng. 1992. New spectral methods for ratio cut partitioning and clustering. IEEE Transactions Computer-Aided Design, Santa Clara CA, 422-427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Ford</author>
<author>D R Fulkerson</author>
</authors>
<title>Maximal flow through a network.</title>
<date>1956</date>
<journal>Canadian Journal of Mathematics</journal>
<volume>8</volume>
<pages>399--404</pages>
<contexts>
<context position="11022" citStr="Ford and Fulkerson, 1956" startWordPosition="1678" endWordPosition="1681">Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modularity 0 (|V ||E |2) path (Girvan and Newman, 2003) information centrality modularity 0(|V||E |3) (Fortunato et al., 2004) agglomerative modularity oriented modularity 0 (|V ||E |) (Newman, 2004) Table 2. Summary of Graph-based Clustering Algorithms (|V |: the number of vertices, |E |: the number of edges, K: the number of clusters, m: the number of resources allocated for each vertex) 3 The first set of algorithms (cut-based) is associated with max-flow min-cut theorem (Ford and Fulkerson, 1956) which states that “the value of the maximum flow is equal to the cost of the minimum cut”. One of the earliest algorithm, Kernighan-Lin algorithm (Kernighan and Lin, 1970) splits the graph by performing recursive bisection (split into two parts at a time), aiming to minimize inter-cluster density (cut size). The high complexity of the algorithm ( 0 (|V |3) makes it less competitive in real applications. Flake et al. (2003) proposed a cut-clustering algorithm which optimizes the bicriterion measure and the complexity is proportional to the number of clusters K using a heuristic, thus the algor</context>
</contexts>
<marker>Ford, Fulkerson, 1956</marker>
<rawString>L. R. Ford, D. R. Fulkerson. 1956. Maximal flow through a network. Canadian Journal of Mathematics 8: 399–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>Detecting community structure in networks.</title>
<date>2004</date>
<journal>Eur. Phys. J. B,</journal>
<volume>38</volume>
<pages>321--330</pages>
<contexts>
<context position="10716" citStr="Newman, 2004" startWordPosition="1629" endWordPosition="1630">ng (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partitioning intercluster 0 (|V| (Karypis and Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modularity 0 (|V ||E |2) path (Girvan and Newman, 2003) information centrality modularity 0(|V||E |3) (Fortunato et al., 2004) agglomerative modularity oriented modularity 0 (|V ||E |) (Newman, 2004) Table 2. Summary of Graph-based Clustering Algorithms (|V |: the number of vertices, |E |: the number of edges, K: the number of clusters, m: the number of resources allocated for each vertex) 3 The first set of algorithms (cut-based) is associated with max-flow min-cut theorem (Ford and Fulkerson, 1956) which states that “the value of the maximum flow is equal to the cost of the minimum cut”. One of the earliest algorithm, Kernighan-Lin algorithm (Kernighan and Lin, 1970) splits the graph by performing recursive bisection (split into two parts at a time), aiming to minimize inter-cluster den</context>
<context position="15909" citStr="Newman (2004)" startWordPosition="2453" endWordPosition="2454"> between clusters are likely to be in the shortest paths. Girvan and Newman (2003) proposed the concept of edge betweenness which is the number of shortest paths connecting any pair of vertices that pass through the edge. Their algorithm iteratively removes one of the edges with the highest betweenness. The complexity of the algorithm is 0 (|V ||E |2). Instead of betweenness, Fortunato et al. (2004) used information centrality for each edge and stated that it performs better than betweenness but with a higher complexity of 0(|V||E|3). The agglomerative category contains much fewer algorithms. Newman (2004) proposed an 4 algorithm that starts each vertex as singletons, and then iteratively merges clusters together in pairs, choosing the join that results in the greatest increase (or smallest decrease) in modularity score. The algorithm converges if there is only cluster left in the graph, then from the clustering hierarchy, we choose the clustering with maximum modularity. The complexity of the algorithm is 𝑂𝑂(J𝑉𝑉JJ𝐸𝐸J). The algorithms we surveyed in this section are by no means comprehensive as the field is longstanding and still evolving rapidly. We also refer readers to other informative refe</context>
<context position="25925" citStr="Newman, 2004" startWordPosition="4019" endWordPosition="4020">pplies some clustering algorithm, e.g., single-link clustering, completelink clustering, average-link clustering, such that similar words are grouped together. Matsuo et al. (2006) presented a graph cluster6 ing algorithm for word clustering based on word similarity measures by web counts. A word cooccurrence graph is constructed in which the vertex represents a word, and the edge weight is computed by applying some similarity measure (e.g., PMI, χ2) on a co-occurrence matrix, which is the result of querying a pair of words to a search engine. Then an agglomerative graph clustering algorithm (Newman, 2004), which is surveyed in section 2.5, is applied. They showed that the similarity measure χ2 performs better than PMI, for one reason, PMI performs worse when a word group contains rare or frequent words, for the other reason, PMI is sensitive to web output inconsistency, e.g., the web count of w1is below the web count of w1ANDw2 in extreme case. They also showed that their graph clustering algorithm outperforms average-link agglomerative clustering by almost 32% using χ2 similarity measure. The concern of their approach is the running complexity for constructing co-occurrence matrix, i.e., for </context>
</contexts>
<marker>Newman, 2004</marker>
<rawString>M. E. J. Newman. 2004. Detecting community structure in networks. Eur. Phys. J. B, 38, 321–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>Fast algorithm for detecting community structure in networks.</title>
<date>2004</date>
<journal>Phys Rev E.</journal>
<volume>69</volume>
<contexts>
<context position="10716" citStr="Newman, 2004" startWordPosition="1629" endWordPosition="1630">ng (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partitioning intercluster 0 (|V| (Karypis and Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modularity 0 (|V ||E |2) path (Girvan and Newman, 2003) information centrality modularity 0(|V||E |3) (Fortunato et al., 2004) agglomerative modularity oriented modularity 0 (|V ||E |) (Newman, 2004) Table 2. Summary of Graph-based Clustering Algorithms (|V |: the number of vertices, |E |: the number of edges, K: the number of clusters, m: the number of resources allocated for each vertex) 3 The first set of algorithms (cut-based) is associated with max-flow min-cut theorem (Ford and Fulkerson, 1956) which states that “the value of the maximum flow is equal to the cost of the minimum cut”. One of the earliest algorithm, Kernighan-Lin algorithm (Kernighan and Lin, 1970) splits the graph by performing recursive bisection (split into two parts at a time), aiming to minimize inter-cluster den</context>
<context position="15909" citStr="Newman (2004)" startWordPosition="2453" endWordPosition="2454"> between clusters are likely to be in the shortest paths. Girvan and Newman (2003) proposed the concept of edge betweenness which is the number of shortest paths connecting any pair of vertices that pass through the edge. Their algorithm iteratively removes one of the edges with the highest betweenness. The complexity of the algorithm is 0 (|V ||E |2). Instead of betweenness, Fortunato et al. (2004) used information centrality for each edge and stated that it performs better than betweenness but with a higher complexity of 0(|V||E|3). The agglomerative category contains much fewer algorithms. Newman (2004) proposed an 4 algorithm that starts each vertex as singletons, and then iteratively merges clusters together in pairs, choosing the join that results in the greatest increase (or smallest decrease) in modularity score. The algorithm converges if there is only cluster left in the graph, then from the clustering hierarchy, we choose the clustering with maximum modularity. The complexity of the algorithm is 𝑂𝑂(J𝑉𝑉JJ𝐸𝐸J). The algorithms we surveyed in this section are by no means comprehensive as the field is longstanding and still evolving rapidly. We also refer readers to other informative refe</context>
<context position="25925" citStr="Newman, 2004" startWordPosition="4019" endWordPosition="4020">pplies some clustering algorithm, e.g., single-link clustering, completelink clustering, average-link clustering, such that similar words are grouped together. Matsuo et al. (2006) presented a graph cluster6 ing algorithm for word clustering based on word similarity measures by web counts. A word cooccurrence graph is constructed in which the vertex represents a word, and the edge weight is computed by applying some similarity measure (e.g., PMI, χ2) on a co-occurrence matrix, which is the result of querying a pair of words to a search engine. Then an agglomerative graph clustering algorithm (Newman, 2004), which is surveyed in section 2.5, is applied. They showed that the similarity measure χ2 performs better than PMI, for one reason, PMI performs worse when a word group contains rare or frequent words, for the other reason, PMI is sensitive to web output inconsistency, e.g., the web count of w1is below the web count of w1ANDw2 in extreme case. They also showed that their graph clustering algorithm outperforms average-link agglomerative clustering by almost 32% using χ2 similarity measure. The concern of their approach is the running complexity for constructing co-occurrence matrix, i.e., for </context>
</contexts>
<marker>Newman, 2004</marker>
<rawString>M. E. J. Newman. 2004. Fast algorithm for detecting community structure in networks. Phys Rev E. 69, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
<author>M Girvan</author>
</authors>
<title>Finding and evaluating community structure in networks.</title>
<date>2004</date>
<journal>Phys. Rev. E</journal>
<pages>69--026113</pages>
<marker>Newman, Girvan, 2004</marker>
<rawString>M. E. J. Newman and M. Girvan. 2004. Finding and evaluating community structure in networks. Phys. Rev. E 69,026113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Girvan</author>
<author>M E J Newman</author>
</authors>
<title>Community structure in social and biological networks.</title>
<date>2002</date>
<journal>Proc. Natl. Acad. Sci. USA</journal>
<volume>99</volume>
<pages>7821--7826</pages>
<marker>Girvan, Newman, 2002</marker>
<rawString>M. Girvan and M. E. J. Newman. 2002. Community structure in social and biological networks. Proc. Natl. Acad. Sci. USA 99, 7821-7826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meila</author>
</authors>
<title>Comparing clusterings.</title>
<date>2003</date>
<booktitle>In Proceedings of COLT03.</booktitle>
<contexts>
<context position="17990" citStr="Meila (2003)" startWordPosition="2779" endWordPosition="2780">ranslate into effectiveness in real applications with respect to the ground truth clustering and thus an evaluation measure plays the role of evaluating how well the clustering matches the gold standard. Two questions arise: (1) what constraints (properties, criteria) should an ideal evaluation measure satisfy? (2) Do the evaluation measures ever proposed satisfy the constraints? For the first question, there have been several attempts on it: Dom (2001) developed a parametric technique for describing the quality of a clustering and proposed five “desirable properties” based on the parameters; Meila (2003) listed 12 properties associated with the proposed entropy measure; Amigo et al. (2008) proposed four constraints including homogeneity, completeness, rag bag, and cluster size vs. quantity. A parallel comparison shows that the four constraints proposed by Amigo et al. (2008) have advantages over the constraints proposed in the other two papers, for one reason, the four constraints can describe all the important constraints in Dom (2001) and Meila (2003), but the reverse does not hold; for the other reason, the four constraints can be formally verified for each evaluation measure, but it is no</context>
</contexts>
<marker>Meila, 2003</marker>
<rawString>M. Meila. 2003. Comparing clusterings. In Proceedings of COLT03.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman 1995</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<marker>Vilain, Burger, Aberdeen, Connolly, 1995, </marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. Hirschman.1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Elango</author>
</authors>
<title>Coreference Resolution: A Survey.</title>
<date>2005</date>
<tech>Technical Report,</tech>
<institution>University of Wisconsin Madison.</institution>
<contexts>
<context position="21793" citStr="Elango (2005)" startWordPosition="3377" endWordPosition="3378">questions; instead, we overview a few NLP case studies in which some graph-based clustering methods have been successfully applied. 3.1 Coreference Resolution Coreference resolution is typically defined as the problem of partitioning a set of mentions into entities. An entity is an object or a set of objects in the real world such as person, organization, facility, while a mention is a textual reference to an entity. The approaches to solving coreference resolution have shifted from earlier linguisticsbased (rely on domain knowledge and handcrafted rules) to machine-learning based approaches. Elango (2005) and Chen (2010) presented a comprehensive survey on this topic. One of the most prevalent approaches for coreference resolution is to follow a two-step procedure: (1) a classification step that computes how likely one mention corefers with the other and (2) a clustering step that groups the mentions into clusters such that all mentions in a cluster refer to the same entity. In the past years, NLP researchers have explored and enriched this methodogy from various directions (either in classification or clustering step). Unfortunately, most of the proposed clustering algorithms, e.g., closest-f</context>
</contexts>
<marker>Elango, 2005</marker>
<rawString>P. Elango. 2005. Coreference Resolution: A Survey. Technical Report, University of Wisconsin Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kannan</author>
<author>S Vempala</author>
<author>A Vetta</author>
</authors>
<title>On clusterings:good, bad and spectral.</title>
<date>2000</date>
<booktitle>In Proceedings of the 41st Annual Symposium on Foundations of Computer Science.</booktitle>
<contexts>
<context position="8778" citStr="Kannan et al., 2000" startWordPosition="1358" endWordPosition="1361"> graph (Shi and Malik, 2000) ratio cut (Hagan and Kahng, ― Ratio cut is suitable for unweighted graph, and ncut is a better choice for 1992) ― weighted graph ncut (Shi and Malik, 2000) ― Overcome the drawback of intra-cluster density or inter-cluster density Drawback: both favor clusters with equal size performance (Dongen, 2000; ― Performance takes both intra-cluster density and inter-cluster density Brandes et al., 2003) into considerations simultaneously expansion, conductance, ― Expansion is suitable for unweighted graph, and conductance is a better bicriteria ― choice for weighted graph (Kannan et al., 2000) Both expansion and conductance impose quality within clusters, but not inter-cluster quality; bicriteria takes both into considerations modularity (Newman and ― Evaluates the quality of clustering with respect to a randomized graph Girvan,2004) ― Drawbacks: (1) It requires global knowledge of the graph’s topology, i.e., the number of edges. Clauset (2005) proposed an improved measure Local Modularity. (2) Resolution limit problem: it fails to identify clusters smaller than a certain scale. Ruan and Zhang (2008) proposed an improved measure HQcut. (3) It fails to distinguish good from bad clus</context>
</contexts>
<marker>Kannan, Vempala, Vetta, 2000</marker>
<rawString>R. Kannan, S. Vempala, and A. Vetta. 2000. On clusterings:good, bad and spectral. In Proceedings of the 41st Annual Symposium on Foundations of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Semi-supervised Learning for Computational Linguistics, Chapman and Hall.</title>
<date>2007</date>
<contexts>
<context position="12026" citStr="Abney (2007)" startWordPosition="1844" endWordPosition="1845">l applications. Flake et al. (2003) proposed a cut-clustering algorithm which optimizes the bicriterion measure and the complexity is proportional to the number of clusters K using a heuristic, thus the algorithm is competitive in practice. The second set of algorithms is based on spectral graph theory with Laplacian matrix as the mathematical tool. The connection between clustering and spectrum of Laplacian matrix (L) basically lies in the following important proposition: the multiplicity k of the eigenvalue 0 of L equals to the number of connected components in the graph. Luxburg (2006) and Abney (2007) presented a comprehensive tutorial on spectral clustering. Luxburg (2006) discussed three forms of Laplacian matrices (one unnormalized form and two normalized forms) and their three corresponding spectral clustering algorithms (unnormalized, normalized I and normalized II). Unnormalized clustering aims to optimize ratiocut measure while normalized clustering aims to optimize ncut measure (Shi and Malik, 2000), thus spectral clustering actually relates with cut-based clustering. The success of spectral clustering is mainly based on the fact that it does not make strong assumptions on the form</context>
</contexts>
<marker>Abney, 2007</marker>
<rawString>S. Abney. 2007. Semi-supervised Learning for Computational Linguistics, Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Schaeffer</author>
</authors>
<title>Graph clustering.</title>
<date>2007</date>
<journal>Computer Science Review,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="16539" citStr="Schaeffer (2007)" startWordPosition="2552" endWordPosition="2553"> algorithm that starts each vertex as singletons, and then iteratively merges clusters together in pairs, choosing the join that results in the greatest increase (or smallest decrease) in modularity score. The algorithm converges if there is only cluster left in the graph, then from the clustering hierarchy, we choose the clustering with maximum modularity. The complexity of the algorithm is 𝑂𝑂(J𝑉𝑉JJ𝐸𝐸J). The algorithms we surveyed in this section are by no means comprehensive as the field is longstanding and still evolving rapidly. We also refer readers to other informative references, e.g., Schaeffer (2007), Brandes et al. (2007) and Newman (2004). A natural question arises: “which algorithm should we choose?” A general answer to this question is that no algorithm is a panacea. First, as we mentioned earlier, a clustering algorithm is usually proposed to optimize some quality measure, therefore, it is not fair to compare an algorithm that favors one measure with the other one that favors some other measure. Second, there is not a perfect measure that captures the full characteristics of cluster structures; therefore a perfect algorithm does not exist. Third, there is no definition for so called </context>
</contexts>
<marker>Schaeffer, 2007</marker>
<rawString>S. E. Schaeffer. 2007. Graph clustering. Computer Science Review, 1(1):27–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fortunato</author>
<author>V Latora</author>
<author>M Marchiori</author>
</authors>
<date>2004</date>
<booktitle>A Method to Find Community Structures Based on Information Centrality. Phys Rev E.70,</booktitle>
<pages>056104</pages>
<contexts>
<context position="10643" citStr="Fortunato et al., 2004" startWordPosition="1617" endWordPosition="1620">ce cutting (ICC) conductance 0(|V ||E|) (Kannan et al.,2000) geometric MST clustering (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partitioning intercluster 0 (|V| (Karypis and Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modularity 0 (|V ||E |2) path (Girvan and Newman, 2003) information centrality modularity 0(|V||E |3) (Fortunato et al., 2004) agglomerative modularity oriented modularity 0 (|V ||E |) (Newman, 2004) Table 2. Summary of Graph-based Clustering Algorithms (|V |: the number of vertices, |E |: the number of edges, K: the number of clusters, m: the number of resources allocated for each vertex) 3 The first set of algorithms (cut-based) is associated with max-flow min-cut theorem (Ford and Fulkerson, 1956) which states that “the value of the maximum flow is equal to the cost of the minimum cut”. One of the earliest algorithm, Kernighan-Lin algorithm (Kernighan and Lin, 1970) splits the graph by performing recursive bisecti</context>
<context position="15698" citStr="Fortunato et al. (2004)" startWordPosition="2420" endWordPosition="2423">n be achieved by computing the stationary distribution of a random walk in the graph. The final set of algorithms in divisive category is based on the third interpretation of the hypothesis in section 2.2, i.e., the links between clusters are likely to be in the shortest paths. Girvan and Newman (2003) proposed the concept of edge betweenness which is the number of shortest paths connecting any pair of vertices that pass through the edge. Their algorithm iteratively removes one of the edges with the highest betweenness. The complexity of the algorithm is 0 (|V ||E |2). Instead of betweenness, Fortunato et al. (2004) used information centrality for each edge and stated that it performs better than betweenness but with a higher complexity of 0(|V||E|3). The agglomerative category contains much fewer algorithms. Newman (2004) proposed an 4 algorithm that starts each vertex as singletons, and then iteratively merges clusters together in pairs, choosing the join that results in the greatest increase (or smallest decrease) in modularity score. The algorithm converges if there is only cluster left in the graph, then from the clustering hierarchy, we choose the clustering with maximum modularity. The complexity </context>
</contexts>
<marker>Fortunato, Latora, Marchiori, 2004</marker>
<rawString>S. Fortunato, V. Latora, and M. Marchiori. 2004. A Method to Find Community Structures Based on Information Centrality. Phys Rev E.70, 056104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S van Dongen</author>
</authors>
<title>Graph Clustering by Flow Simulation.</title>
<date>2000</date>
<tech>PhD thesis,</tech>
<institution>University of Utrecht.</institution>
<marker>van Dongen, 2000</marker>
<rawString>S. van Dongen. 2000. Graph Clustering by Flow Simulation. PhD thesis, University of Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S White</author>
<author>P Smyth</author>
</authors>
<title>A spectral clustering approach to finding communities in graphs.</title>
<date>2005</date>
<booktitle>In SIAM International Conference on Data Mining.</booktitle>
<marker>White, Smyth, 2005</marker>
<rawString>S. White and P. Smyth. 2005. A spectral clustering approach to finding communities in graphs. In SIAM International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Brandes</author>
<author>M Gaertler</author>
<author>D Wagner</author>
</authors>
<title>Experiments on graph clustering algorithms.</title>
<date>2003</date>
<booktitle>Proc. 11th European Symp. Algorithms, LNCS</booktitle>
<pages>2832--568</pages>
<contexts>
<context position="8584" citStr="Brandes et al., 2003" startWordPosition="1331" endWordPosition="1334">ity ― Maximizing intra-cluster density is equivalent to minimizing inter-cluster inter-cluster density ― density and vice versa Drawback: both favor cutting small sets of isolated vertices in the graph (Shi and Malik, 2000) ratio cut (Hagan and Kahng, ― Ratio cut is suitable for unweighted graph, and ncut is a better choice for 1992) ― weighted graph ncut (Shi and Malik, 2000) ― Overcome the drawback of intra-cluster density or inter-cluster density Drawback: both favor clusters with equal size performance (Dongen, 2000; ― Performance takes both intra-cluster density and inter-cluster density Brandes et al., 2003) into considerations simultaneously expansion, conductance, ― Expansion is suitable for unweighted graph, and conductance is a better bicriteria ― choice for weighted graph (Kannan et al., 2000) Both expansion and conductance impose quality within clusters, but not inter-cluster quality; bicriteria takes both into considerations modularity (Newman and ― Evaluates the quality of clustering with respect to a randomized graph Girvan,2004) ― Drawbacks: (1) It requires global knowledge of the graph’s topology, i.e., the number of edges. Clauset (2005) proposed an improved measure Local Modularity. </context>
</contexts>
<marker>Brandes, Gaertler, Wagner, 2003</marker>
<rawString>U. Brandes, M. Gaertler, and D. Wagner. 2003. Experiments on graph clustering algorithms. Proc. 11th European Symp. Algorithms, LNCS 2832:568-579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Brandes</author>
<author>M Gaertler</author>
<author>D Wagner</author>
</authors>
<title>Engineering graph clustering: Models and experimental evaluation.</title>
<date>2007</date>
<journal>J. Exp. Algorithmics,</journal>
<pages>12--1</pages>
<contexts>
<context position="10159" citStr="Brandes et al., 2007" startWordPosition="1555" endWordPosition="1558">easures Category Algorithms optimized running measure complexity divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3) (Kernighan and Lin, 1970) cut-clustering algorithm bicriteria 0 (|V|) (Flake et al., 2003) spectral unnormalized spectral clustering ratiocut 0 (|V||E|) (Luxburg, 2006) normalized spectral clustering I ncut 0 (|V||E|) (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II ncut 0 (|V ||E |) (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) conductance 0(|V ||E|) (Kannan et al.,2000) geometric MST clustering (GMC) pluggable(any 0(|V ||E|) (Brandes et al., 2007) quality measure) modularity oriented modularity 0 (|V||E|) (White and Smyth,2005) multilevel multilevel recursive bisection intercluster 0(|V|logK) (Karypis and Kumar, 1999) multilevel K-way partitioning intercluster 0 (|V| (Karypis and Kumar, 1999) + KlogK) random Markov Clustering Algorithm (MCL) performance 0(m2|V|) (Dongen, 2000) shortest betweenness modularity 0 (|V ||E |2) path (Girvan and Newman, 2003) information centrality modularity 0(|V||E |3) (Fortunato et al., 2004) agglomerative modularity oriented modularity 0 (|V ||E |) (Newman, 2004) Table 2. Summary of Graph-based Clustering</context>
<context position="16562" citStr="Brandes et al. (2007)" startWordPosition="2554" endWordPosition="2557">arts each vertex as singletons, and then iteratively merges clusters together in pairs, choosing the join that results in the greatest increase (or smallest decrease) in modularity score. The algorithm converges if there is only cluster left in the graph, then from the clustering hierarchy, we choose the clustering with maximum modularity. The complexity of the algorithm is 𝑂𝑂(J𝑉𝑉JJ𝐸𝐸J). The algorithms we surveyed in this section are by no means comprehensive as the field is longstanding and still evolving rapidly. We also refer readers to other informative references, e.g., Schaeffer (2007), Brandes et al. (2007) and Newman (2004). A natural question arises: “which algorithm should we choose?” A general answer to this question is that no algorithm is a panacea. First, as we mentioned earlier, a clustering algorithm is usually proposed to optimize some quality measure, therefore, it is not fair to compare an algorithm that favors one measure with the other one that favors some other measure. Second, there is not a perfect measure that captures the full characteristics of cluster structures; therefore a perfect algorithm does not exist. Third, there is no definition for so called “best clustering”. The </context>
</contexts>
<marker>Brandes, Gaertler, Wagner, 2007</marker>
<rawString>U. Brandes, M. Gaertler, and D.Wagner. 2007. Engineering graph clustering: Models and experimental evaluation. J. Exp. Algorithmics, 12:1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Luxburg</author>
<author>O Bousquet</author>
<author>M Belkin</author>
</authors>
<title>Limits of spectral clustering. In</title>
<date>2005</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="12901" citStr="Luxburg et al. (2005)" startWordPosition="1973" endWordPosition="1976"> I and normalized II). Unnormalized clustering aims to optimize ratiocut measure while normalized clustering aims to optimize ncut measure (Shi and Malik, 2000), thus spectral clustering actually relates with cut-based clustering. The success of spectral clustering is mainly based on the fact that it does not make strong assumptions on the form of the clusters and can solve very general problems like intertwined spirals which k-means clustering handles much worse. Unfortunately, spectral clustering could be unstable under different choices of graphs and parameters as mentioned in section 2.3. Luxburg et al. (2005) compared unnormalized clustering with normalized version and proved that normalized version always converges to a sensible limit clustering while for unnormalized case the same only holds under strong additional assumptions which are not always satisfied. The running complexity of spectral clustering equals to the complexity of computing the eigenvectors of Laplacian matrix which is 0 (|V |3). However, when the graph is sparse, the complexity is reduced to 0 (|V ||E |) by applying efficient Lanczos algorithm. The third set of algorithms is based on multilevel graph partitioning paradigm (Kary</context>
</contexts>
<marker>Luxburg, Bousquet, Belkin, 2005</marker>
<rawString>U. Luxburg, O. Bousquet, M. Belkin. 2005. Limits of spectral clustering. In L. K. Saul, Y. Weiss and L. Bottou (Eds.), Advances in neural information processing systems 17. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>U Luxburg 2006</author>
</authors>
<title>A tutorial on spectral clustering.</title>
<tech>Technical Report 149,</tech>
<institution>Max Plank Institute for Biological Cybernetics.</institution>
<contexts>
<context position="5190" citStr="(2006)" startWordPosition="824" endWordPosition="824">n cutting edges connecting the vertices across subgraphs. (2) A random walk that visits a subgraph will likely stay in the subgraph until many of its vertices have been visited (Dongen, 2000). (3) Among all shortest paths between all pairs of vertices, links between different dense subgraphs are likely to be in many shortest paths (Dongen, 2000). 2.3 Modeling Modeling addresses the problem of transforming the problem into graph structure, specifically, designating the meaning of vertices and edges in the graph, computing the edge weights for weighted graph, and constructing the graph. Luxburg (2006) stated three most common methods to construct a graph: s -neighborhood graph, k-nearest neighbor graph, and fully connected graph. Luxburg analyzed different behaviors of the three graph construction methods, and stated that some graph-cluster algorithms (e.g., spectral clustering) can be quite sensitive to the choice of graphs and parameters (s and k). As a general recommendation, Luxburg suggested exploiting k-nearest neighbor graph as the first choice, which is less vulnerable to the choices of parameters than other graphs. Unfortunately, theoretical justifications on the choices of graphs</context>
<context position="12009" citStr="(2006)" startWordPosition="1842" endWordPosition="1842">tive in real applications. Flake et al. (2003) proposed a cut-clustering algorithm which optimizes the bicriterion measure and the complexity is proportional to the number of clusters K using a heuristic, thus the algorithm is competitive in practice. The second set of algorithms is based on spectral graph theory with Laplacian matrix as the mathematical tool. The connection between clustering and spectrum of Laplacian matrix (L) basically lies in the following important proposition: the multiplicity k of the eigenvalue 0 of L equals to the number of connected components in the graph. Luxburg (2006) and Abney (2007) presented a comprehensive tutorial on spectral clustering. Luxburg (2006) discussed three forms of Laplacian matrices (one unnormalized form and two normalized forms) and their three corresponding spectral clustering algorithms (unnormalized, normalized I and normalized II). Unnormalized clustering aims to optimize ratiocut measure while normalized clustering aims to optimize ncut measure (Shi and Malik, 2000), thus spectral clustering actually relates with cut-based clustering. The success of spectral clustering is mainly based on the fact that it does not make strong assump</context>
<context position="23110" citStr="(2006)" startWordPosition="3583" endWordPosition="3583">tant decision is made (in greedy style) when considering two mentions are coreferent or not, therefore, the algorithm makes no attempt to search through the space of all possible clusterings, which results in a suboptimal clustering (Luo et al., 2004). Various approaches have been proposed to alleviate this problem, of which graph clustering methodology is one of the most promising solutions. The problem of coreference resolution can be modeled as a graph such that the vertex represents a mention, and the edge weight carries the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (similar to Kernighan-Lin algorithm) and in each iteration, it searches the best cut that leads to partition into halves. They compared BESTCUT algorithm with (Luo et al., 2004)’s Belltree and (Ng and Cardie, 2002)’s Link-Best algorithm and showed that using ground-truth entities, BESTCUT outperforms the other two with statistical significance (4.8% improvement over Belltree and LinkBest algorithm in ECM F-measure)</context>
<context position="25492" citStr="(2006)" startWordPosition="3947" endWordPosition="3947">s, verbs) into groups so that similar words are in the same cluster. Word clustering is a major technique that can benefit many NLP tasks, e.g., thesaurus construction, text classification, and word sense disambiguation. Word clustering can be solved by following a two-step procedure: (1) classification step by representing each word as a feature vector and computing the similarity of two words; (2) clustering step which applies some clustering algorithm, e.g., single-link clustering, completelink clustering, average-link clustering, such that similar words are grouped together. Matsuo et al. (2006) presented a graph cluster6 ing algorithm for word clustering based on word similarity measures by web counts. A word cooccurrence graph is constructed in which the vertex represents a word, and the edge weight is computed by applying some similarity measure (e.g., PMI, χ2) on a co-occurrence matrix, which is the result of querying a pair of words to a search engine. Then an agglomerative graph clustering algorithm (Newman, 2004), which is surveyed in section 2.5, is applied. They showed that the similarity measure χ2 performs better than PMI, for one reason, PMI performs worse when a word gro</context>
</contexts>
<marker>2006, </marker>
<rawString>U. Luxburg.2006. A tutorial on spectral clustering. Technical Report 149, Max Plank Institute for Biological Cybernetics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. Of the ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="22472" citStr="Ng and Cardie, 2002" startWordPosition="3480" endWordPosition="3483">opic. One of the most prevalent approaches for coreference resolution is to follow a two-step procedure: (1) a classification step that computes how likely one mention corefers with the other and (2) a clustering step that groups the mentions into clusters such that all mentions in a cluster refer to the same entity. In the past years, NLP researchers have explored and enriched this methodogy from various directions (either in classification or clustering step). Unfortunately, most of the proposed clustering algorithms, e.g., closest-first clustering (Soon et al., 2001), bestfirst clustering (Ng and Cardie, 2002), suffer from a drawback: an instant decision is made (in greedy style) when considering two mentions are coreferent or not, therefore, the algorithm makes no attempt to search through the space of all possible clusterings, which results in a suboptimal clustering (Luo et al., 2004). Various approaches have been proposed to alleviate this problem, of which graph clustering methodology is one of the most promising solutions. The problem of coreference resolution can be modeled as a graph such that the vertex represents a mention, and the edge weight carries the coreference likelihood between tw</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. Of the ACL, pages 104–111.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D Lim 2001</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Soon, Ng, 2001, </marker>
<rawString>W. M. Soon, H. T. Ng and D. Lim.2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="19723" citStr="Luo, 2005" startWordPosition="3056" endWordPosition="3057">d V) and MUC only fail the rag bag constraint; (3) all the measures in set mapping category fail completeness constraint (4) all the measures in pair counting category fail cluster size vs. quantity constraint; (5) CEAF, unfortunately, fails homogeneity, completeness, rag bag constraints. Category Evaluation Measures set mapping purity, inverse purity, F-measure pair counting rand index, Jaccard Coefficient, Folks and Mallows FM entropy entropy, mutual information, VI, V editing editing distance distance coreference MUC (Vilain et al.,1995), resolution B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al.,</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell Tree.</title>
<date>2004</date>
<booktitle>In Proc. of ACL-04,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="22755" citStr="Luo et al., 2004" startWordPosition="3526" endWordPosition="3529">luster refer to the same entity. In the past years, NLP researchers have explored and enriched this methodogy from various directions (either in classification or clustering step). Unfortunately, most of the proposed clustering algorithms, e.g., closest-first clustering (Soon et al., 2001), bestfirst clustering (Ng and Cardie, 2002), suffer from a drawback: an instant decision is made (in greedy style) when considering two mentions are coreferent or not, therefore, the algorithm makes no attempt to search through the space of all possible clusterings, which results in a suboptimal clustering (Luo et al., 2004). Various approaches have been proposed to alleviate this problem, of which graph clustering methodology is one of the most promising solutions. The problem of coreference resolution can be modeled as a graph such that the vertex represents a mention, and the edge weight carries the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (similar to Kernighan-Lin algorithm) and in each iteration, it </context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell Tree. In Proc. of ACL-04, pp.136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>T Sakaki</author>
<author>K Uchiyama</author>
<author>M Ishizuka</author>
</authors>
<title>Graph-based word clustering using web search engine.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="20328" citStr="Matsuo et al., 2006" startWordPosition="3144" endWordPosition="3147">AF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 graph clustering algorithms. As far as we know, there is not much work on the comparative study of various graph-based clustering algorithms for certain NLP problems. As mentioned at the end of section 2.5, there is not a graph clustering algorithm that is effective for all applications. However, it is interesting to find out, for a specific NLP problem, if graph clustering methods can b</context>
<context position="25492" citStr="Matsuo et al. (2006)" startWordPosition="3944" endWordPosition="3947">ds (e.g., nouns, verbs) into groups so that similar words are in the same cluster. Word clustering is a major technique that can benefit many NLP tasks, e.g., thesaurus construction, text classification, and word sense disambiguation. Word clustering can be solved by following a two-step procedure: (1) classification step by representing each word as a feature vector and computing the similarity of two words; (2) clustering step which applies some clustering algorithm, e.g., single-link clustering, completelink clustering, average-link clustering, such that similar words are grouped together. Matsuo et al. (2006) presented a graph cluster6 ing algorithm for word clustering based on word similarity measures by web counts. A word cooccurrence graph is constructed in which the vertex represents a word, and the edge weight is computed by applying some similarity measure (e.g., PMI, χ2) on a co-occurrence matrix, which is the result of querying a pair of words to a search engine. Then an agglomerative graph clustering algorithm (Newman, 2004), which is surveyed in section 2.5, is applied. They showed that the similarity measure χ2 performs better than PMI, for one reason, PMI performs worse when a word gro</context>
</contexts>
<marker>Matsuo, Sakaki, Uchiyama, Ishizuka, 2006</marker>
<rawString>Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka. 2006. Graph-based word clustering using web search engine. In Proc. of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chen</author>
<author>H Ji</author>
</authors>
<title>Graph-based Event Coreference Resolution. In</title>
<date>2009</date>
<booktitle>Proc. ACL-IJCNLP 2009 workshop on TextGraphs-4: Graph-based Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="23947" citStr="Chen and Ji (2009" startWordPosition="3709" endWordPosition="3712">nd in each iteration, it searches the best cut that leads to partition into halves. They compared BESTCUT algorithm with (Luo et al., 2004)’s Belltree and (Ng and Cardie, 2002)’s Link-Best algorithm and showed that using ground-truth entities, BESTCUT outperforms the other two with statistical significance (4.8% improvement over Belltree and LinkBest algorithm in ECM F-measure). Nevertheless, we believe that the BESTCUT algorithm is not the only choice and the running complexity of BESTCUT,O(|V||E |+ |V|2log|V|), is not competitive, thus could be improved by other graph clustering algorithms. Chen and Ji (2009a) applied normalized spectral algorithm to conduct event coreference resolution: partitioning a set of mentions into events. An event is a specific occurrence involving participants. An event mention is a textual reference to an event which includes a distinguished trigger (the word that most clearly expresses an event occurs) and involving arguments (entities/temporal expressions that play certain roles in the event). A graph is similarly constructed as in entity coreference resolution except that it involves quite different feature engineering (most features are related with event trigger a</context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Z. Chen and H. Ji. 2009a. Graph-based Event Coreference Resolution. In Proc. ACL-IJCNLP 2009 workshop on TextGraphs-4: Graph-based Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chen</author>
<author>H Ji</author>
<author>R Haralick</author>
</authors>
<title>A Pairwise Coreference Model, Feature Impact and Evaluation for Event Coreference Resolution.</title>
<date>2009</date>
<booktitle>In Proc. RANLP</booktitle>
<contexts>
<context position="9460" citStr="Chen et al. (2009)" startWordPosition="1461" endWordPosition="1464">but not inter-cluster quality; bicriteria takes both into considerations modularity (Newman and ― Evaluates the quality of clustering with respect to a randomized graph Girvan,2004) ― Drawbacks: (1) It requires global knowledge of the graph’s topology, i.e., the number of edges. Clauset (2005) proposed an improved measure Local Modularity. (2) Resolution limit problem: it fails to identify clusters smaller than a certain scale. Ruan and Zhang (2008) proposed an improved measure HQcut. (3) It fails to distinguish good from bad clustering between different graphs with the same modularity value. Chen et al. (2009) proposed an improved measure Max-Min Modularity Table 1. Summary of Quality Measures Category Algorithms optimized running measure complexity divisive cut-based Kernighan-Lin algorithm intercluster 0 (|V|3) (Kernighan and Lin, 1970) cut-clustering algorithm bicriteria 0 (|V|) (Flake et al., 2003) spectral unnormalized spectral clustering ratiocut 0 (|V||E|) (Luxburg, 2006) normalized spectral clustering I ncut 0 (|V||E|) (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II ncut 0 (|V ||E |) (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) conductance 0(|V ||E|) </context>
<context position="24705" citStr="Chen et al., 2009" startWordPosition="3824" endWordPosition="3827">pecific occurrence involving participants. An event mention is a textual reference to an event which includes a distinguished trigger (the word that most clearly expresses an event occurs) and involving arguments (entities/temporal expressions that play certain roles in the event). A graph is similarly constructed as in entity coreference resolution except that it involves quite different feature engineering (most features are related with event trigger and arguments). The graph clustering approach yields competitive results by comparing with an agglomerative clustering algorithm proposed in (Chen et al., 2009b), unfortunately, a scientific comparison among the algorithms remains unexplored. 3.2 Word Clustering Word clustering is a problem defined as clustering a set of words (e.g., nouns, verbs) into groups so that similar words are in the same cluster. Word clustering is a major technique that can benefit many NLP tasks, e.g., thesaurus construction, text classification, and word sense disambiguation. Word clustering can be solved by following a two-step procedure: (1) classification step by representing each word as a feature vector and computing the similarity of two words; (2) clustering step </context>
</contexts>
<marker>Chen, Ji, Haralick, 2009</marker>
<rawString>Z. Chen, H. Ji, R. Haralick. 2009b. A Pairwise Coreference Model, Feature Impact and Evaluation for Event Coreference Resolution. In Proc. RANLP 2009 workshop on Events in Emerging Text Types.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chen</author>
</authors>
<title>Graph-based Clustering and its Application in Coreference Resolution.</title>
<date>2010</date>
<tech>Technical Report, the</tech>
<institution>Graduate Center, the City University of New York.</institution>
<contexts>
<context position="21809" citStr="Chen (2010)" startWordPosition="3380" endWordPosition="3381">, we overview a few NLP case studies in which some graph-based clustering methods have been successfully applied. 3.1 Coreference Resolution Coreference resolution is typically defined as the problem of partitioning a set of mentions into entities. An entity is an object or a set of objects in the real world such as person, organization, facility, while a mention is a textual reference to an entity. The approaches to solving coreference resolution have shifted from earlier linguisticsbased (rely on domain knowledge and handcrafted rules) to machine-learning based approaches. Elango (2005) and Chen (2010) presented a comprehensive survey on this topic. One of the most prevalent approaches for coreference resolution is to follow a two-step procedure: (1) a classification step that computes how likely one mention corefers with the other and (2) a clustering step that groups the mentions into clusters such that all mentions in a cluster refer to the same entity. In the past years, NLP researchers have explored and enriched this methodogy from various directions (either in classification or clustering step). Unfortunately, most of the proposed clustering algorithms, e.g., closest-first clustering </context>
</contexts>
<marker>Chen, 2010</marker>
<rawString>Z. Chen. 2010. Graph-based Clustering and its Application in Coreference Resolution. Technical Report, the Graduate Center, the City University of New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>