<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002109">
<title confidence="0.9960095">
The Third International Chinese Language Processing Bakeoff:
Word Segmentation and Named Entity Recognition
</title>
<author confidence="0.988249">
Gina-Anne Levow
</author>
<affiliation confidence="0.995343">
University of Chicago
</affiliation>
<address confidence="0.7988025">
1100 E. 58th St.
Chicago, IL 60637 USA
</address>
<email confidence="0.996195">
levow@cs.uchicago.edu
</email>
<sectionHeader confidence="0.99377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964555555556">
The Third International Chinese Language
Processing Bakeoff was held in Spring
2006 to assess the state of the art in two
important tasks: word segmentation and
named entity recognition. Twenty-nine
groups submitted result sets in the two
tasks across two tracks and a total of five
corpora. We found strong results in both
tasks as well as continuing challenges.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869025">
Many important natural language processing tasks
ranging from part of speech tagging to parsing
to reference resolution and machine translation
assume the ready availability of a tokenization
into words. While such tokenization is relatively
straight-forward in languages which use whites-
pace to delimit words, Chinese presents a signif-
icant challenge since it is typically written with-
out such separation. Word segmentation has thus
long been the focus of significant research because
of its role as a necessary pre-processing phase for
the tasks above. However, word segmentation re-
mains a significant challenge both for the difficulty
of the task itself and because standards for seg-
mentation vary and human segmenters may often
disagree.
SIGHAN, the Special Interest Group for Chi-
nese Language Processing of the Association
for Computational Linguistics, conducted two
prior word segmentation bakeoffs, in 2003 and
2005(Emerson, 2005), which established bench-
marks for word segmentation against which other
systems are judged. The bakeoff presentations at
SIGHAN workshops highlighted new approaches
in the field as well as the crucial importance of
handling out-of-vocabulary (OOV) words.
A significant class of OOV words is Named En-
tities, such as person, location, and organization
names. These terms are frequently poorly covered
in lexical resources and change over time as new
individuals, institutions, or products appear. These
terms also play a particularly crucial role in infor-
mation retrieval, reference resolution, and ques-
tion answering. As a result of this importance, and
interest in expanding the scope of the bakeoff ex-
pressed at the Fourth SIGHAN Workshop, in the
Winter of 2005 it was decided to hold a new bake-
off to evaluate both continued progress in Word
Segmentation (WS) and the state of the art in Chi-
nese Named Entity Recognition (NER).
</bodyText>
<sectionHeader confidence="0.80821" genericHeader="method">
2 Details of the Evaluation
</sectionHeader>
<subsectionHeader confidence="0.712798">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.9956604">
Five corpora were provided for the evaluation:
three in Simplified characters and two in tradi-
tional characters. The Simplified character cor-
pora were provided by Microsoft Research Asia
(MSRA) for WS and NER, by University of Penn-
sylvania/University of Colorado (UPUC) for WS,
and by the Linguistic Data Consortium (LDC) for
NER. The Traditional character corpora were pro-
vided by City University of Hong Kong (CITYU)
for WS and NER and by the Chinese Knowl-
edge Information Processing Laboratory (CKIP)
of the Academia Sinica, Taiwan for WS. Each data
provider offered separate training and test corpora.
General information for each corpus appears in
Table 1.
All data providers were requested to supply
the training and test corpora in both the stan-
dard local encoding and in Unicode (UTF-8) in
a standard XML format with sentence and word
tags, and named entity tags if appropriate. For
</bodyText>
<page confidence="0.98202">
108
</page>
<note confidence="0.679595">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108–117,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.999270166666667">
Source Encodings Training (Wds/Types) Test (Wds/Types)
CITYU BIG5HKSCS/Unicode 1.6M/76K 220K/23K
CKIP BIG5/Unicode 5.5M/146K 91K/15K
LDC Unicode 632K (est. wds) 61K (est. wds)
MSRA GB18030/Unicode 1.3M/63K 100K/13K
UPUC GB/Unicode 509K/37K 155K/17K
</table>
<tableCaption confidence="0.999904">
Table 1: Overall corpus statistics
</tableCaption>
<bodyText confidence="0.99987885">
all providers except the LDC, missing encodings
were transcoded by the organizers using the ap-
propriate Python CJK codecs.
Primary training and truth data for word seg-
mentation were generated by the organizers via a
Python script by replacing sentence end tags with
newlines and word end tags with a single whites-
pace character, deleting all other tags and associ-
ated newlines. For test data, end of sentence tags
were replaced with newlines and all other tags re-
moved. Since the UPUC truth corpus was only
provided in white-space separated form, test data
was created by automatically deleting line-internal
whitespace.
Primary training and truth data for named
entity recognition were converted from the
provided XML format to a two-column format
similar to that used in the CoNLL 2002 NER
task(Sang, 2002) adapted for Chinese, where
the first column is the current character and
the second column the corresponding tag. For-
mat details may be found at the bakeoff website
(http://www.sighan.org/bakeoff2006/).
For consistency, we tagged only ”&lt;NAMEX&gt;”
mentions, of either (PER)SON, (LOC)ATION,
(ORG)ANIZATION, or (G)EO-(P)OLITICAL
(E)NTITY as annotated in the corpora.1 Test was
generated as above.
The LDC required sites to download training
data from their website directly in the ACE2 eval-
uation format, restricted to ”NAM” mentions. The
organizers provided the sites with a Python script
to convert the LDC data to the CoNLL format
above, and the same script was used to create the
truth data. Test data was created by splitting on
newlines or Chinese period characters.
Comparable XML format data was also pro-
vided for all corpora and both tasks.
The segmentation and NER annotation stan-
dard, as appropriate, for each corpus was made
</bodyText>
<footnote confidence="0.9997">
1Only the LDC provided GPE tags.
2http://www.ldc.upenn.edu/projects/ACE
</footnote>
<bodyText confidence="0.999257333333333">
available on the bakeoff website. As observed
in previous evaluations, these documents varied
widely in length, detail, and presentation lan-
guage.
Except as noted above, no additional changes
were made to the data furnished by the providers.
</bodyText>
<subsectionHeader confidence="0.998947">
2.2 Rules and Procedures
</subsectionHeader>
<bodyText confidence="0.999549428571429">
The Third Bakeoff followed the structure of the
first two word segmentation bakeoffs. Participat-
ing groups (”sites”) registered by email form; only
the primary contact was required to register, iden-
tifying the corpora and tasks of interest. Train-
ing data was released for download from the web-
sites (both SIGHAN and LDC) on April 17, 2006.
Test data was released on May 15, 2006 and re-
sults were due 14:00 GMT on May 17. Scores for
all submitted runs were emailed to the individual
groups by May 19, and were made available to all
groups on a web page a few days later.
Groups could participate in either or both of two
tracks for each task and corpus:
</bodyText>
<listItem confidence="0.954237">
• In the open track, participants could use any
external data they chose in addition to the
provided training data. Such data could in-
clude external lexica, name lists, gazetteers,
part-of-speech taggers, etc. Groups were re-
quired to specify this information in their sys-
tem descriptions.
• In the closed track, participants could only
use information found in the provided train-
ing data. Information such as externally ob-
</listItem>
<bodyText confidence="0.90931475">
tained word counts, part of speech informa-
tion, or name lists was excluded.
Groups were required to submit fully automatic
runs and were prohibited from testing on corpora
which they had previously used.
Scoring was performed automatically using a
combination of Python and Perl scripts, facilitated
by stringent file naming conventions. In cases
</bodyText>
<page confidence="0.99578">
109
</page>
<bodyText confidence="0.999980666666667">
where naming errors or minor divergences from
required file formats arose, a mix of manual inter-
vention and automatic conversion was employed
to enable scoring. The primary scoring scripts
were made available to participants for followup
experiments.
</bodyText>
<sectionHeader confidence="0.96491" genericHeader="method">
3 Participating Sites
</sectionHeader>
<bodyText confidence="0.9999935">
A total of 36 sites registered, and 29 submitted re-
sults for scoring. The greatest number of partici-
pants came from the People’s Republic of China
(11), followed by Taiwan (7), the United States
(5), Japan (2), with one team each from Singapore,
Korea, Hong Kong, and Canada. A summary of
participating groups with task and track informa-
tion appears in Table 2. A total of 144 official runs
were scored: 101 for word segmentation and 43
for named entity recognition.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="evaluation">
4 Results &amp; Discussion
</sectionHeader>
<bodyText confidence="0.9999495">
We report results below first for word segmenta-
tion and second for named entity recognition.
</bodyText>
<subsectionHeader confidence="0.997655">
4.1 Word Segmentation Results
</subsectionHeader>
<bodyText confidence="0.999954515151515">
To provide a basis for comparison, we computed
baseline and possible topline scores for each of
the corpora. The baseline was constructed by left-
to-right maximal match implemented by Python
script, using the training corpus vocabulary. The
topline employed the same procedure, but instead
used the test vocabulary. These results are shown
in Tables 3 and 4.
For the WS task, we computed the following
measures using the score(Sproat and Emerson,
2003) program developed for the previous bake-
offs: recall (R), precision (P), equally weighted
F-measure (F = 2� �
(� +�)), the rate of out-of-
vocabulary words (OOV rate) in the test cor-
pus, the recall on OOV (Roo„), and recall on in-
vocabulary words (RZ„). In and out of vocabu-
lary status are defined relative to the training cor-
pus. Following previous bakeoffs, we employ the
Central Limit Theorem for Bernoulli trials (Grin-
stead and Snell, 1997) to compute 95% confidence
interval as ±2�(p(n p) ), assuming the binomial
distribution is appropriate. For recall, Cr, we as-
sume that recall represents the probability of cor-
rect word identification. Symmetrically, for pre-
cision, we compute Cp, setting p to the precision
value. One can determine if two systems may then
be viewed as significantly different at a 95% con-
fidence level by computing whether their confi-
dence intervals overlap.
Word segmentation results for all runs grouped
by corpus and track appear in Tables 5-12; all ta-
bles are sorted by F-score.
</bodyText>
<subsectionHeader confidence="0.996596">
4.2 Word Segmentation Discussion
</subsectionHeader>
<bodyText confidence="0.999972860465117">
Across all corpora, the best F-score was achieved
in the MSRA Open Track at 0.979. Overall, as
would be expected, the best results on Open track
runs had higher F-scores than the best results for
Closed Track runs on the same corpora. Likewise,
the OOV recall rates for the best Open Track sys-
tems exceed those of the best Closed Track runs
on comparable corpora by exploiting outside in-
formation. Unfortunately, few sites submitted runs
in both conditions making strong direct compar-
isons difficult.
Many systems strongly outperformed the base-
line runs, though none achieved the topline. The
closest approach to the topline score was on the
CITYU corpus, with the best performing runs
achieving 99% of the topline F-score.
It is also informative to observe the rather wide
variation in scores across the test corpora. The
maximum scores were achieved on the MSRA cor-
pus closely followed by the CITYU corpus. The
best score achieved on the UPUC Open track con-
dition, however, was lower than all scores but one
on the MSRA Open track. However, a comparison
of the baseline, topline, and especially the OOV
rates may shed some light on this disparity. The
UPUC training corpus was only about one-third
the size of the MSRA corpus, and the OOV rate
for UPUC was more than double that of any of
the other corpora, yielding a challenging task, es-
pecially in the Closed track. This high OOV rate
may also be attributed to a change in register, since
the training data for UPUC had been drawn ex-
clusively from the Chinese Treebank and the test
data also included data from other newswire and
broadcast news sources. In contrast, the MSRA
corpus had both the highest baseline and highest
topline scores, possibly indicating an easier cor-
pus in some sense. The differences in topline also
suggest a greater degree of variance in the UPUC,
and in fact all other corpora, relative the MSRA
corpus. These differences highlight the continuing
challenges of handling out-of-vocabulary words
and performing segmentation across different reg-
</bodyText>
<page confidence="0.990377">
110
</page>
<table confidence="0.999756755102041">
Site Name Site Country CITYU CKIP MSRA UPUC CITYU LDC MSRA
ID WS WS WS WS NER NER NER
Natural Language Processing Lab, 1 ZH C C C C
Northeastern University of China
Language Technologies Institute, 2 US O O O O
Carnegie Mellon University
National Institute of Information 3 JP C C C
and Communications Technology, Japan
Basis Technology Corp. 4 US C C C C
Pattern Recognition and Intelligent 5 ZH C C
System Laboratory,Beijing University
of Posts and Telecommunications
HKUST, Human Language 6 HK O O O
Technology Center
The University of Tokyo 7 JP O O O O
Institute of Software, 8 ZH C C C C C C OC
Chinese Academy of Sciences
Alias-i, Inc. 9 US C C C C C C
Beijing University of Posts 10 ZH O O O
and Telecommunications
France Telecom R&amp;D Beijing 11 ZH C OC O
NETEASE Information 12 ZH O O
Technology (Beijing) Co., Ltd.
AI Lab., Dept of Information 13 TW OC OC
Management, Huafan University,
Taiwan.
Nanjing University, China 14 ZH O OC
Intelligent Agent Systems Lab (IASL), 15 TW C C C
Academia Sinica
Simon Fraser University 16 CA C C C
Tung Nan Institute of Technology 18 TW C
Institute of Information Science, 19 TW C C
Taiwan
Microsoft Research Asia 20 ZH OC OC OC
Yahoo! 21 US C C
CKIP, Academia Sinica, Taiwan 22 TW O
Kookmin University 23 KO C C C C
Shenyang Institute of 24 ZH OC OC
Aeronautical Engineering
Institute for Infocomm Research, 26 SG C C C C C C
Singapore
National Taiwan University 29 TW C
ITNLP, Harbin Institute of 30 ZH OC O
Technology, China
National Central University 31 TW C C
at Taiwan
National Laboratory on Machine 32 ZH OC OC OC OC O
Perception, Peking University, China
University of Texas at Austin 34 US O O O O
</table>
<tableCaption confidence="0.949775">
Table 2: Participating Sites by Corpus, Task, and Track
</tableCaption>
<table confidence="0.9996878">
Source Recall Precision F-measure OOV Rate Roo„ RZ„
CITYU 0.930 0.882 0.906 0.040 0.009 0.969
CKIP 0.915 0.870 0.892 0.042 0.030 0.954
MSRA 0.949 0.900 0.924 0.034 0.022 0.981
UPUC 0.869 0.790 0.828 0.088 0.011 0.951
</table>
<tableCaption confidence="0.999676">
Table 3: Baselines: WS: Maximum match with training vocabulary
</tableCaption>
<page confidence="0.96159">
111
</page>
<table confidence="0.9992594">
Source Recall Precision F-measure OOV Rate Roo„ RZ„
CITYU 0.982 0.985 0.984 0.040 0.993 0.981
CKIP 0.980 0.987 0.983 0.042 0.997 0.979
MSRA 0.991 0.993 0.992 0.034 0.999 0.991
UPUC 0.961 0.976 0.968 0.088 0.989 0.958
</table>
<tableCaption confidence="0.997517">
Table 4: Toplines: WS: Maximum match with testing vocabulary
</tableCaption>
<table confidence="0.999364555555555">
Site RunID R Cr P CV F Roov Riv
15 d 0.973 ±0.000691 0.972 ±0.000703 0.972 0.787 0.981
15 b 0.973 ±0.000691 0.972 ±0.000703 0.972 0.787 0.981
20 0.972 ±0.000703 0.971 ±0.000715 0.971 0.792 0.979
32 0.969 ±0.000739 0.970 ±0.000727 0.970 0.773 0.978
1 a 0.971 ±0.000715 0.965 ±0.000783 0.968 0.719 0.981
15 c 0.965 ±0.000783 0.967 ±0.000761 0.966 0.792 0.972
15 a 0.966 ±0.000772 0.967 ±0.000761 0.966 0.786 0.973
26 0.968 ±0.000750 0.961 ±0.000825 0.965 0.633 0.983
11 0.962 ±0.000815 0.962 ±0.000815 0.962 0.722 0.972
16 0.963 ±0.000805 0.958 ±0.000855 0.961 0.689 0.974
9 0.966 ±0.000772 0.957 ±0.000865 0.961 0.555 0.983
1 b 0.958 ±0.000855 0.963 ±0.000805 0.960 0.714 0.968
8 0.952 ±0.000911 0.954 ±0.000893 0.953 0.747 0.960
23 0.950 ±0.000929 0.949 ±0.000938 0.949 0.638 0.963
4 b 0.845 ±0.001543 0.844 ±0.001547 0.844 0.632 0.854
4 a 0.841 ±0.001559 0.844 ±0.001547 0.843 0.506 0.855
13 1 0.589 ±0.002097 0.589 ±0.002097 0.589 0.022 0.613
</table>
<tableCaption confidence="0.976385">
Table 5: CITYU: Word Segmentation: Closed Track
</tableCaption>
<table confidence="0.999851888888889">
Site RunID R Cr P CV F Roov Riv
20 0.978 ±0.000625 0.977 ±0.000639 0.977 0.840 0.984
32 0.979 ±0.000611 0.976 ±0.000652 0.977 0.813 0.985
34 0.971 ±0.000715 0.967 ±0.000761 0.969 0.795 0.978
22 0.970 ±0.000727 0.965 ±0.000783 0.967 0.761 0.979
2 0.964 ±0.000794 0.964 ±0.000794 0.964 0.787 0.971
13 2 0.544 ±0.002123 0.549 ±0.002121 0.547 0.194 0.559
13 3 0.524 ±0.002129 0.503 ±0.002131 0.513 0.195 0.538
13 1 0.497 ±0.002131 0.467 ±0.002127 0.481 0.057 0.516
</table>
<tableCaption confidence="0.945127">
Table 6: CITYU: Word Segmentation: Open Track
</tableCaption>
<table confidence="0.9994325">
Site RunID R Cr P CV F Roov Riv
20 0.961 ±0.001280 0.955 ±0.001371 0.958 0.702 0.972
15 a 0.961 ±0.001280 0.953 ±0.001400 0.957 0.658 0.974
15 b 0.961 ±0.001280 0.952 ±0.001414 0.957 0.656 0.974
32 0.958 ±0.001327 0.948 ±0.001468 0.953 0.646 0.972
26 0.958 ±0.001327 0.941 ±0.001558 0.949 0.554 0.976
1 b 0.947 ±0.001482 0.943 ±0.001533 0.945 0.601 0.962
1 a 0.949 ±0.001455 0.940 ±0.001571 0.944 0.694 0.960
9 0.951 ±0.001428 0.935 ±0.001630 0.943 0.389 0.976
23 0.937 ±0.001607 0.933 ±0.001654 0.935 0.547 0.954
8 0.939 ±0.001583 0.929 ±0.001699 0.934 0.606 0.954
4 a 0.836 ±0.002449 0.834 ±0.002461 0.835 0.521 0.849
4 b 0.836 ±0.002449 0.828 ±0.002496 0.832 0.590 0.847
13 1 0.747 ±0.002875 0.677 ±0.003093 0.710 0.036 0.778
</table>
<tableCaption confidence="0.998814">
Table 7: CKIP: Word Segmentation: Closed Track
</tableCaption>
<page confidence="0.951197">
112
</page>
<table confidence="0.999811666666667">
Site RunID R Cr P CV F Roov Riv
20 0.964 ±0.001232 0.955 ±0.001371 0.959 0.704 0.975
34 0.959 ±0.001311 0.949 ±0.001455 0.954 0.672 0.972
32 0.958 ±0.001327 0.948 ±0.001468 0.953 0.647 0.972
2 a 0.953 ±0.001400 0.946 ±0.001495 0.949 0.679 0.965
2 b 0.951 ±0.001428 0.944 ±0.001521 0.948 0.676 0.964
13 2 0.724 ±0.002956 0.668 ±0.003115 0.695 0.161 0.749
13 3 0.736 ±0.002915 0.653 ±0.003148 0.692 0.160 0.761
13 1 0.654 ±0.003146 0.573 ±0.003271 0.611 0.057 0.680
</table>
<tableCaption confidence="0.982471">
Table 8: CKIP: Word Segmentation: Open Track
</tableCaption>
<table confidence="0.988899304347826">
Site RunID R Cr P CV F Roov Riv
32 0.964 ±0.001176 0.961 ±0.001222 0.963 0.612 0.976
26 0.961 ±0.001222 0.953 ±0.001336 0.957 0.499 0.977
9 0.959 ±0.001252 0.955 ±0.001309 0.957 0.494 0.975
1 a 0.955 ±0.001309 0.956 ±0.001295 0.956 0.650 0.966
15 d 0.953 ±0.001336 0.956 ±0.001295 0.955 0.574 0.966
11 a 0.955 ±0.001309 0.953 ±0.001336 0.954 0.575 0.969
15 b 0.952 ±0.001350 0.956 ±0.001295 0.954 0.575 0.966
15 c 0.949 ±0.001389 0.957 ±0.001281 0.953 0.673 0.959
15 a 0.949 ±0.001389 0.958 ±0.001266 0.953 0.672 0.959
16 0.952 ±0.001350 0.954 ±0.001323 0.953 0.604 0.964
11 b 0.950 ±0.001376 0.954 ±0.001323 0.952 0.602 0.962
5 0.956 ±0.001295 0.947 ±0.001414 0.951 0.493 0.972
1 b 0.946 ±0.001427 0.952 ±0.001350 0.949 0.568 0.959
18 c 0.950 ±0.001376 0.930 ±0.001611 0.940 0.272 0.974
30 a 0.963 ±0.001192 0.918 ±0.001732 0.940 0.175 0.991
18 b 0.954 ±0.001323 0.921 ±0.001703 0.937 0.163 0.981
8 0.933 ±0.001578 0.942 ±0.001476 0.937 0.640 0.943
23 0.933 ±0.001578 0.939 ±0.001511 0.936 0.526 0.948
24 0.923 ±0.001683 0.929 ±0.001621 0.926 0.554 0.936
18 a 0.949 ±0.001389 0.897 ±0.001919 0.922 0.022 0.982
4 a 0.830 ±0.002371 0.832 ±0.002360 0.831 0.473 0.842
4 b 0.817 ±0.002441 0.821 ±0.002420 0.819 0.491 0.829
</table>
<tableCaption confidence="0.982131">
Table 9: MSRA: Word Segmentation: Closed Track
</tableCaption>
<table confidence="0.999875">
Site RunID R Cr P CV F Roov Riv
11 a 0.980 ±0.000884 0.978 ±0.000926 0.979 0.839 0.985
11 b 0.977 ±0.000946 0.976 ±0.000966 0.977 0.840 0.982
14 0.975 ±0.000986 0.976 ±0.000966 0.975 0.811 0.981
32 0.977 ±0.000946 0.971 ±0.001059 0.974 0.675 0.988
10 0.970 ±0.001077 0.970 ±0.001077 0.970 0.804 0.976
30 a 0.977 ±0.000946 0.960 ±0.001237 0.968 0.624 0.989
34 0.959 ±0.001252 0.961 ±0.001222 0.960 0.711 0.968
2 0.949 ±0.001389 0.954 ±0.001323 0.952 0.692 0.958
7 0.953 ±0.001336 0.940 ±0.001499 0.947 0.503 0.969
24 0.938 ±0.001522 0.946 ±0.001427 0.942 0.706 0.946
</table>
<tableCaption confidence="0.998197">
Table 10: MSRA: Word Segmentation: Open Track
</tableCaption>
<page confidence="0.96874">
113
</page>
<table confidence="0.99990825">
Site RunID R Cr P CV F Roov Riv
20 0.940 ±0.001207 0.926 ±0.001330 0.933 0.707 0.963
32 0.936 ±0.001244 0.923 ±0.001355 0.930 0.683 0.961
1 a 0.940 ±0.001207 0.914 ±0.001425 0.927 0.634 0.969
26 a 0.936 ±0.001244 0.917 ±0.001402 0.926 0.617 0.966
26 b 0.932 ±0.001279 0.910 ±0.001454 0.921 0.577 0.966
16 0.929 ±0.001305 0.909 ±0.001462 0.919 0.628 0.958
5 0.932 ±0.001279 0.904 ±0.001497 0.918 0.546 0.969
1 b 0.922 ±0.001363 0.914 ±0.001425 0.918 0.637 0.949
8 0.922 ±0.001363 0.912 ±0.001440 0.917 0.680 0.945
31 1 0.917 ±0.001402 0.904 ±0.001497 0.910 0.676 0.940
9 0.919 ±0.001387 0.895 ±0.001558 0.907 0.459 0.964
23 0.915 ±0.001417 0.896 ±0.001551 0.905 0.565 0.949
24 0.902 ±0.001511 0.887 ±0.001609 0.895 0.568 0.934
4 a 0.831 ±0.001905 0.819 ±0.001957 0.825 0.487 0.864
4 b 0.809 ±0.001998 0.827 ±0.001922 0.818 0.637 0.825
</table>
<tableCaption confidence="0.97701">
Table 11: UPUC: Word Segmentation: Closed Track
</tableCaption>
<table confidence="0.999950888888889">
Site RunID R Cr P CV F Roov Riv
34 0.949 ±0.001118 0.939 ±0.001216 0.944 0.768 0.966
2 0.942 ±0.001188 0.928 ±0.001314 0.935 0.711 0.964
20 0.940 ±0.001207 0.927 ±0.001322 0.933 0.741 0.959
7 0.944 ±0.001169 0.922 ±0.001363 0.933 0.680 0.970
12 0.933 ±0.001271 0.916 ±0.001410 0.924 0.656 0.959
32 0.940 ±0.001207 0.907 ±0.001476 0.923 0.561 0.976
24 0.928 ±0.001314 0.906 ±0.001483 0.917 0.660 0.954
10 0.925 ±0.001339 0.897 ±0.001545 0.911 0.593 0.957
</table>
<tableCaption confidence="0.7831625">
Table 12: UPUC: Word Segmentation: Open Track
isters and writing styles.
</tableCaption>
<subsectionHeader confidence="0.997564">
4.3 Named Entity Results
</subsectionHeader>
<bodyText confidence="0.999971466666667">
We employed a slightly modified version of the
CoNLL 2002 scoring script to evaluate NER task
submissions. For each submission, we compute
overall phrase precision (P), recall(R), and bal-
anced F-measure (F), as well as F-measure for
each entity type (PER-F,ORG-F,LOC-F,GPE-F).
For each corpus, we compute a baseline per-
formance level as follows. Based on the training
data, using a left-to-right pass over the test data,
we assign a named entity tag to a span of charac-
ters if it was tagged with a single unique NE tag
(PER/LOC/ORG/GPE) in the training data.3 All
In the case of overlapping spans, we tag the max-
imal span. These scores for all NER corpora are
found in Table 13.
</bodyText>
<subsectionHeader confidence="0.996608">
4.4 Named Entity Discussion
</subsectionHeader>
<bodyText confidence="0.999077">
Though fewer sites participated in the NER task,
performances overall were very strong, with only
</bodyText>
<footnote confidence="0.80379675">
3If the span was a single character and appeared UN-
tagged in the corpus, we exclude it. Longer spans are re-
tained for tagging even if they might appear both tagged and
untagged in the training corpus.
</footnote>
<bodyText confidence="0.97648344">
two runs performing below baseline. The best F-
score overall on the MSRA Open Track reached
0.912, with ten other scores for MSRA and
CITYU Open Track above 0.85. Only two sites
submitted runs in both Open and Closed Track
conditions, and few Open Track runs were sub-
mitted at all, again limiting comparability. For
the only corpus with substantial numbers of both
Open and Closed Track runs, MSRA, the top three
runs outperformed all Closed Track runs.
System scores and baselines were much higher
for the CITYU and MSRA corpora than for the
LDC corpus. This disparity can, in part, also be at-
tributed to a substantially smaller training corpus
for the LDC than the other two collections. The
presence of an additional category, Geo-political
entity, which is potentially confused for either
location or organization also enhances the diffi-
culty of this corpus. Training requirements, vari-
ation across corpora, and most extensive tag sets
will continue to raise challenges for named entity
recognition.
Named entity recognition results for all runs
grouped by corpus and track appear in Tables 14-
19; all tables are sorted by F-score.
</bodyText>
<footnote confidence="0.946284">
4This result indicates a rescoring of the run below with all
</footnote>
<page confidence="0.989401">
114
</page>
<table confidence="0.99981025">
Source P R F PER-F ORG-F LOC-F GPE-F
CITY 0.611 0.467 0.529 0.587 0.516 0.503 N/A
LDC 0.493 0.378 0.428 0.395 0.29 0.259 0.539
MSRA 0.59 0.488 0.534 0.614 0.469 0.531 N/A
</table>
<tableCaption confidence="0.971295">
Table 13: Baselines: NER: Maximal match with unique tag in training corpus
</tableCaption>
<table confidence="0.999912">
Site RunID P R F ORG-F LOC-F PER-F
3 0.9143 0.8676 0.8903 0.8046 0.9211 0.9087
19 ccrf 0.9201 0.8545 0.8861 0.8054 0.9251 0.8872
21 a 0.9266 0.8475 0.8853 0.7973 0.9232 0.8937
21 b 0.9242 0.8491 0.8850 0.7976 0.9236 0.8920
19 avdic 0.9079 0.8626 0.8847 0.7984 0.9233 0.8914
8 0.9276 0.8181 0.8694 0.7707 0.9114 0.8769
21 f 0.9188 0.8231 0.8683 0.7852 0.9105 0.8652
21 g 0.9164 0.8246 0.8681 0.7842 0.9114 0.8636
9 0.8690 0.8417 0.8551 0.7541 0.8861 0.8845
19 bme 0.8742 0.8307 0.8519 0.7667 0.9015 0.8395
26 0.8466 0.8061 0.8259 0.7467 0.8863 0.7927
31 1 0.9035 0.6973 0.7871 0.7703 0.8905 0.5974
29 0.7703 0.6447 0.7019 0.4948 0.7613 0.7531
</table>
<tableCaption confidence="0.990569">
Table 14: CITYU: Named Entity Recognition: Closed Track
</tableCaption>
<table confidence="0.853519">
Site RunID P R F ORG-F LOC-F PER-F
6 0.8692 0.7498 0.8051 0.6801 0.8604 0.8098
</table>
<tableCaption confidence="0.984941">
Table 15: CITYU: Named Entity Recognition: Open Track
</tableCaption>
<table confidence="0.998005666666667">
Site RunID P R F ORG-F LOC-F PER-F GPE-F
3 0.8026 0.7265 0.7627 0.6585 0.3046 0.7884 0.8204
8 0.8143 0.5953 0.6878 0.5855 0.1705 0.6571 0.7727
</table>
<tableCaption confidence="0.994038">
Table 16: LDC: Named Entity Recognition: Closed Track
</tableCaption>
<table confidence="0.49995175">
Site RunID P R F ORG-F LOC-F PER-F GPE-F
7 0.7616 0.6621 0.7084 0.5209 0.2857 0.7422 0.7930
6 GPE-LOC4 0.6720 0.6554 0.6636 0.4553 0.7078 0.7420
6 0.3058 0.2982 0.3019 0.4553 0.0370 0.7420 0.0
</table>
<tableCaption confidence="0.991916">
Table 17: LDC: Named Entity Recognition: Open Track
</tableCaption>
<table confidence="0.999878066666667">
Site RunID P R F ORG-F LOC-F PER-F
14 0.8894 0.8420 0.8651 0.8310 0.8545 0.9009
21 a 0.9122 0.8171 0.8620 0.8196 0.9053 0.8257
21 b 0.8843 0.8288 0.8556 0.7698 0.9013 0.8495
3 0.8814 0.8234 0.8514 0.8150 0.9062 0.7938
21 f 0.8845 0.7931 0.8363 0.8071 0.9003 0.7568
21 g 0.8661 0.8032 0.8335 0.7742 0.8991 0.7753
19 avdic 0.8637 0.7767 0.8179 0.8138 0.8233 0.8126
19 dcrf 0.8623 0.7740 0.8158 0.8141 0.8207 0.8093
9 0.8188 0.8097 0.8142 0.7295 0.8529 0.8196
19 cnoword 0.8670 0.7554 0.8074 0.8100 0.8257 0.7764
19 bvoting 0.8583 0.7606 0.8065 0.8145 0.8133 0.7899
26 0.8105 0.7882 0.7992 0.7491 0.8385 0.7699
21 r 0.8748 0.7168 0.7880 0.7288 0.8604 0.7107
8 0.8164 0.3124 0.4519 0.4591 0.5084 0.3521
</table>
<tableCaption confidence="0.999055">
Table 18: MSRA: Named Entity Recognition: Closed Track
</tableCaption>
<page confidence="0.948042">
115
</page>
<table confidence="0.999703416666667">
Site RunID P R F ORG-F LOC-F PER-F
10 0.9220 0.9018 0.9118 0.8590 0.9034 0.9604
14 0.9076 0.8922 0.8999 0.8397 0.9099 0.9261
11 b 0.8767 0.8753 0.8760 0.7611 0.8976 0.9225
11 a 0.8645 0.8399 0.8520 0.6945 0.8745 0.9199
32 0.8397 0.8184 0.8289 0.7261 0.8804 0.8207
7 0.8468 0.7822 0.8132 0.6958 0.8552 0.8280
6 0.8195 0.6926 0.7507 0.6443 0.8291 0.6955
30 a 0.8697 0.6556 0.7476 0.5841 0.7029 0.8987
8 0.8320 0.6703 0.7424 0.5651 0.8000 0.7565
12 b 0.7083 0.5464 0.6169 0.4168 0.6154 0.7171
12 a 0.7395 0.5186 0.6096 0.4168 0.6154 0.7074
</table>
<tableCaption confidence="0.999069">
Table 19: MSRA: Named Entity Recognition: Open Track
</tableCaption>
<sectionHeader confidence="0.996218" genericHeader="conclusions">
5 Conclusions &amp; Future Directions
</sectionHeader>
<bodyText confidence="0.999978676470588">
The Third SIGHAN Chinese Language Process-
ing Bakeoff successfully brought together a col-
lection of 29 strong research groups to assess the
progress of research in two important tasks, word
segmentation and named entity recognition, that in
turn enable other important language processing
technologies. The individual group presentations
at the SIGHAN workshop detail the approaches
that yielded strong performance for both tasks. Is-
sues of out-of-vocabulary word handling, anno-
tation consistency, character encoding and code
mixing of Chinese and non-Chinese text all con-
tinue to challenge system designers and bakeoff
organizers alike.
In future analyses, we hope to develop addi-
tional analysis tools to better assess progress in
these fundamental tasks, in a more corpus inde-
pendent fashion. Microsoft Research Asia has
been pursuing work along these lines focusing on
improvements in F-score and OOV F-score rela-
tive to more intrinsic corpus measures, such as
baselines and toplines.5 Such developments will
guide the planning of future evaluations.
Finally, while word segmentation and named
entity recognition are important in themselves, it
is also important to assess the impact of improve-
ments in these enabling technologies on broader
downstream applications. More tightly coupled
experiments that involve joint word segmentation
and named entity recognition could provide in-
sight. Integration of WS and NER with a higher
level task such as parsing, reference resolution, or
machine translation could allow the development
of more refined, task-oriented metrics to evalu-
</bodyText>
<footnote confidence="0.733162">
GPE tags in the truth data mapped to LOC, since no GPE tags
were present in the results.
5Personal communication, Mu Li, Microsoft Research
Asia.
</footnote>
<bodyText confidence="0.993940333333333">
ate WS and NER and focus attention on improve-
ments to the fundamental techniques which en-
hance performance on higher level tasks.
</bodyText>
<sectionHeader confidence="0.997596" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9988134">
We gratefully acknowledge the generous assis-
tance of the organizations and individuals listed
below who provided the data for this bakeoff;
without their support, it could not have taken
place:
</bodyText>
<listItem confidence="0.960138818181818">
• Chinese Knowledge Information Processing
Group, Academia Sinica, Taiwan: Keh-Jiann
Chen, Henning Chiu
• City University of Hong Kong: Benjamin K.
Tsou, Olivia Oi Yee Kwong
• Linguistic Data Consortium: Stephanie
Strassel
• Microsoft Research Asia: Mu Li
• University of Pennsylvania/University of
Colorado, USA: Martha Palmer, Nianwen
Xue
</listItem>
<bodyText confidence="0.99991675">
We also thank Hwee Tou Ng and Olivia Oi Yee
Kwong, the co-organizers of the fifth SIGHAN
workshop, in conjunction with which this bakeoff
takes place. Olivia Kwong merits special thanks
both for her help in co-organizing this bakeoff and
in coordinating publications. Finally, we thank all
the participating sites who enabled the success of
this bakeoff.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991412">
Thomas Emerson. 2005. The Second International
Chinese Word Segmentation Bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chinese
Language Processing, Jeju Island, Republic of Ko-
rea.
</reference>
<page confidence="0.989682">
116
</page>
<reference confidence="0.997487666666666">
Charles M. Grinstead and J. Laurie Snell. 1997. In-
troduction to Probability. American Mathematical
Society, Providence, RI.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In Proceedings of the
6th Conference on Natural Language Learning 2002
(CoNLL-2002).
Richard Sproat and Thomas Emerson. 2003. The First
International Chinese Word Segmentation Bakeoff.
In Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan.
</reference>
<page confidence="0.998143">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854166">
<title confidence="0.9942605">The Third International Chinese Language Processing Word Segmentation and Named Entity Recognition</title>
<author confidence="0.934692">Gina-Anne</author>
<affiliation confidence="0.998802">University of</affiliation>
<address confidence="0.9744325">1100 E. 58th Chicago, IL 60637</address>
<email confidence="0.999615">levow@cs.uchicago.edu</email>
<abstract confidence="0.9935325">The Third International Chinese Language Processing Bakeoff was held in Spring 2006 to assess the state of the art in two important tasks: word segmentation and named entity recognition. Twenty-nine groups submitted result sets in the two tasks across two tracks and a total of five corpora. We found strong results in both tasks as well as continuing challenges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The Second International Chinese Word Segmentation Bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Jeju Island,</booktitle>
<location>Republic of</location>
<contexts>
<context position="1533" citStr="Emerson, 2005" startWordPosition="226" endWordPosition="227">nts a significant challenge since it is typically written without such separation. Word segmentation has thus long been the focus of significant research because of its role as a necessary pre-processing phase for the tasks above. However, word segmentation remains a significant challenge both for the difficulty of the task itself and because standards for segmentation vary and human segmenters may often disagree. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, conducted two prior word segmentation bakeoffs, in 2003 and 2005(Emerson, 2005), which established benchmarks for word segmentation against which other systems are judged. The bakeoff presentations at SIGHAN workshops highlighted new approaches in the field as well as the crucial importance of handling out-of-vocabulary (OOV) words. A significant class of OOV words is Named Entities, such as person, location, and organization names. These terms are frequently poorly covered in lexical resources and change over time as new individuals, institutions, or products appear. These terms also play a particularly crucial role in information retrieval, reference resolution, and qu</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The Second International Chinese Word Segmentation Bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles M Grinstead</author>
<author>J Laurie Snell</author>
</authors>
<title>Introduction to Probability.</title>
<date>1997</date>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="9123" citStr="Grinstead and Snell, 1997" startWordPosition="1438" endWordPosition="1442">procedure, but instead used the test vocabulary. These results are shown in Tables 3 and 4. For the WS task, we computed the following measures using the score(Sproat and Emerson, 2003) program developed for the previous bakeoffs: recall (R), precision (P), equally weighted F-measure (F = 2� � (� +�)), the rate of out-ofvocabulary words (OOV rate) in the test corpus, the recall on OOV (Roo„), and recall on invocabulary words (RZ„). In and out of vocabulary status are defined relative to the training corpus. Following previous bakeoffs, we employ the Central Limit Theorem for Bernoulli trials (Grinstead and Snell, 1997) to compute 95% confidence interval as ±2�(p(n p) ), assuming the binomial distribution is appropriate. For recall, Cr, we assume that recall represents the probability of correct word identification. Symmetrically, for precision, we compute Cp, setting p to the precision value. One can determine if two systems may then be viewed as significantly different at a 95% confidence level by computing whether their confidence intervals overlap. Word segmentation results for all runs grouped by corpus and track appear in Tables 5-12; all tables are sorted by F-score. 4.2 Word Segmentation Discussion A</context>
</contexts>
<marker>Grinstead, Snell, 1997</marker>
<rawString>Charles M. Grinstead and J. Laurie Snell. 1997. Introduction to Probability. American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning</booktitle>
<contexts>
<context position="4647" citStr="Sang, 2002" startWordPosition="714" endWordPosition="715">e organizers via a Python script by replacing sentence end tags with newlines and word end tags with a single whitespace character, deleting all other tags and associated newlines. For test data, end of sentence tags were replaced with newlines and all other tags removed. Since the UPUC truth corpus was only provided in white-space separated form, test data was created by automatically deleting line-internal whitespace. Primary training and truth data for named entity recognition were converted from the provided XML format to a two-column format similar to that used in the CoNLL 2002 NER task(Sang, 2002) adapted for Chinese, where the first column is the current character and the second column the corresponding tag. Format details may be found at the bakeoff website (http://www.sighan.org/bakeoff2006/). For consistency, we tagged only ”&lt;NAMEX&gt;” mentions, of either (PER)SON, (LOC)ATION, (ORG)ANIZATION, or (G)EO-(P)OLITICAL (E)NTITY as annotated in the corpora.1 Test was generated as above. The LDC required sites to download training data from their website directly in the ACE2 evaluation format, restricted to ”NAM” mentions. The organizers provided the sites with a Python script to convert the</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of the 6th Conference on Natural Language Learning 2002 (CoNLL-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The First International Chinese Word Segmentation Bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8682" citStr="Sproat and Emerson, 2003" startWordPosition="1362" endWordPosition="1365">r named entity recognition. 4 Results &amp; Discussion We report results below first for word segmentation and second for named entity recognition. 4.1 Word Segmentation Results To provide a basis for comparison, we computed baseline and possible topline scores for each of the corpora. The baseline was constructed by leftto-right maximal match implemented by Python script, using the training corpus vocabulary. The topline employed the same procedure, but instead used the test vocabulary. These results are shown in Tables 3 and 4. For the WS task, we computed the following measures using the score(Sproat and Emerson, 2003) program developed for the previous bakeoffs: recall (R), precision (P), equally weighted F-measure (F = 2� � (� +�)), the rate of out-ofvocabulary words (OOV rate) in the test corpus, the recall on OOV (Roo„), and recall on invocabulary words (RZ„). In and out of vocabulary status are defined relative to the training corpus. Following previous bakeoffs, we employ the Central Limit Theorem for Bernoulli trials (Grinstead and Snell, 1997) to compute 95% confidence interval as ±2�(p(n p) ), assuming the binomial distribution is appropriate. For recall, Cr, we assume that recall represents the pr</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The First International Chinese Word Segmentation Bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>