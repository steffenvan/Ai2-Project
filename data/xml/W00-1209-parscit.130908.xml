<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996557">
The Research of Word Sense Disambiguation Method Based on
Co-occurrence Frequency of Hownett
</title>
<author confidence="0.999392">
Erhong Yang, Guoqing Zhang, and Yongkui Zhang
</author>
<affiliation confidence="0.7984315">
Dept of Computer Science, Shanxi University,
TaiYuan 030006, P. R. China
</affiliation>
<email confidence="0.997712">
Email: zyk@sxu.edu.cn
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999698777777778">
Word sense disambiguation (WSD) is a difficult
problem in natural language processing. In this
paper, a sememe co-occurrence frequency
based WSD method was introduced. In this
method, Hownet was used as our information
source , and a co-occurrence frequency
database of sememes was constructed and then
used for WSD. The experimental result showed
that this method is successful.
</bodyText>
<sectionHeader confidence="0.987372" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.5855875">
word sense disambiguation, Hownet, sememe,
co-occurrence
</keyword>
<sectionHeader confidence="0.998937" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999896129032258">
Word sense disambiguation (WSD) is one of
the most difficult problems in NLP. It is helpful
and in some instances required for such
applications as machine translation, information
retrieval, content and thematic analysis,
hypertext navigation and so on. The problem of
WSD was first put forward in 1949. And then
in the following decades researchers adopted
many methods to solve the problem of
automatic word sense disambiguation,
including:1) Al-based method, 2) knowledge-
based method and 3) corpus-based method.&apos;1
Although some useful results have been got, the
problem of word sense disambiguation is far
from being solved.
The difficult of WSD is as follow: 1)
Evaluation of word sense disambiguation
systems is not yet standardized. 2) The potential
for WSD varies by task. 3) Adequately large
sense-tagged data sets are difficult to obtain. 4)
The field has narrowed down approaches, but
only a little. 12]
In this paper, we use a statistical based method
to solve the problem of automatic word sense
disambiguationP3 In this method, a new
knowledge base HownetE4&apos;51 was use as
knowledge resources. And instead of words, the
sememes which are defined in Hownet were
used to get the statistical figure. By doing this,
the problem of data sparseness was solved to a
large degree.
</bodyText>
<sectionHeader confidence="0.915927" genericHeader="method">
2. A Brief Introduction Of Hownet
</sectionHeader>
<bodyText confidence="0.979077857142857">
Hownet is a knowledge base which was
released recently on Internet. In Hownet, the
concept which were represented by Chinese or
English words were described and the relations
between concepts and the attributes of concepts
were revealed. In this paper, we use Chinese
knowledge base, which is an important part of
Hownet, as the resource of our disambiguation.
The format of this file is as follow:
W_X =word
E_X = some examples of this word
G_X= the pos of this word
DEF= the definition of this word
This research project is supported by a gam from Shanxi Natural Science Foundation of China
</bodyText>
<page confidence="0.995857">
60
</page>
<bodyText confidence="0.999685833333333">
A important concept used in Hownet that
we must introduce is sememe. In Hownet,
sememes refer to some basic unit of senses.
They are used to describe all the entries in
Hownet and there are more than 1,500 sememe
all together.
</bodyText>
<sectionHeader confidence="0.9018495" genericHeader="method">
3. Sense Co-occurrence Frequency
Database
</sectionHeader>
<bodyText confidence="0.999961352941176">
It is well known that some words tend to
co-occur frequently with some words than with
others[6]. Similarly, some meaning of words
tend to co-occur more often with some meaning
of words than with others. If we can got the
relations of word meanings quantitatively, it
would have some help on word sense
disambiguation. In Hownet, all words are
defined with limited sememes and the
combination of sememes is fixed. If we make
statistic on the co-occurrence frequency of
sememe so as to reflect the co-occurrence of
words, the problem of data sparseness would be
solved to a large degree. Based on the above
thought, we built a sense co-occurrence
frequency database to disambiguate word
senses.
</bodyText>
<subsectionHeader confidence="0.99934">
3.1 The Preprocessing Of Hownet
</subsectionHeader>
<bodyText confidence="0.999927933333333">
The Hownet we downloaded from Internet is in
the form of plain text. It is not convenient for
computer to use and it must been converted into
a database. In the database, each lexical entry is
converted into a record. The formalization
description of the records is as follow:
&lt;lexical entry&gt; ::= &lt;NO.&gt;&lt;morphology&gt;
&lt;part-of-speech&gt;&lt;definition&gt;
Where NO. is the corresponding number of
this lexical entry in Hownet. And the definition
is composed of several sememes (short for SU)
which were divided by comma. In addition, we
have deleted the English sememes in order to
saving space and speeding up the processing.
Here are some examples after preprocessing:
</bodyText>
<table confidence="0.992758666666667">
NO. Morphology Part-of-speech definition
21424 *Ms ADJ ih
18888 ADJ atts,afT,±T,*
18889 V IA*
18887 V
18890
</table>
<subsectionHeader confidence="0.9993105">
3.2 The Creation Of Sememe Co-occurrence
Frequency Database
</subsectionHeader>
<bodyText confidence="0.987982769230769">
The sememe co-occurrence frequency database
is the basic of sense disambiguation. Now we
will introduce it briefly.
The sememe co-occurrence frequency
database is a table of two dimension. Each item
corresponding to the co-occurrence frequency
of a pair of sememes.
Before introducing the sememe
co-occurrence frequency database, we gave the
following definition:
Definiton: suppose word W has m sense
items in ho-wnet, and the corresponding
definition of each sense item is: yll, y12, yAnt),&apos;
</bodyText>
<equation confidence="0.851433">
Y21, Y22, • • -0 Y2(n2); • • •; YmbYm2, • • • Ym(nrn)
</equation>
<bodyText confidence="0.809521">
respectively. We call fyibyi2, youlla
sememe set of W(short for SS), and call (1 yll,
</bodyText>
<construct confidence="0.649934666666667">
y12, • • • Y 1(n1)1,( Y21&gt; Y22, • • • Y2(n2)1,
yniby,nz y,nound jthe sememe expansion of
W (short for SE).
</construct>
<bodyText confidence="0.997497666666667">
For example, in the above mentioned
example, the word &amp;quot;liztF&amp;quot; has only one sense
item. The corresponding sememe set of this
</bodyText>
<page confidence="0.978019">
61
</page>
<bodyText confidence="0.876548888888889">
sense item is { ) and the
sememe expansion of &amp;quot;IMI&amp;quot; is {{altl,
. The word &amp;quot;4&amp;quot; has four sense
items, and the corresponding sememe set of
each item is {MitiA,P-14,4,*}, {NW}, {*1;
N} and {&amp;quot;.A,4;.%) respectively. The sememe
expansion of word &amp;quot;4&amp;quot; is
4,*}, {41/4},
When building the sememe co-occurrence
frequency database, the corpus is segmented
first and each word is tagged with its sememe
expansion in Hownet. Then for each unique
pair of words co-occurred in a sentence (here a
sentence is a string of characters delimited by
punctuations.), the co-occurrence data of
sememes which belong to the definition of each
words respectively were collect. When
collecting co0occurrence data, we adopt a
principle that every pair of word which
co-occurred in a sentence should have equal
contribution to the sememe co-occurrence data
regardless of the number of sense items of this
word and the length of the definition. Moreover,
the contribution of a word should be evenly
distributed between all the senses of a word and
the contribution of a sense should been evenly
distributed between all the sememe in a sense.
The algorithm is as follow:
1.Initial each cell in the sememe
co-occurrence frequency database(short for
SC-1-D) with 0.
2.For each sentence S in training corpus, do
3-7.
3.For each word in sentense S, tag the
sememe expansion to it.
4.For each unique pair of sememe
expansion (SE;, SEJ), do 5-7.
5.For each sememe SU;inp in each sememe
set SSim in SE4, do 6-7.
6.For each sememe SUJ„ci in each sememe
set SS in in SE, do 7.
7.Increase the value of cell SCFD(SU;mp,
SUJ,,q) and SCFD(SUing,SU;) by the product
of w(SU;rnp) and w(SUA). Where w(SU) is
weight of SU givengiven by
</bodyText>
<equation confidence="0.727148">
W(SU,„: ) - „
ISE„IxI1
</equation>
<bodyText confidence="0.9977202">
It can be concluded from the above
algorithm that the SCFD are symmetrical. In
order to saving space and speeding up the
processing, we only save those cells (SUJ,SUp
that satisfying SU;
</bodyText>
<sectionHeader confidence="0.7313965" genericHeader="method">
33 The Sememe Co-occurrence Frequency
Database Based Disambiguation Method
</sectionHeader>
<subsectionHeader confidence="0.689464">
33.1 The Sememe Co-occurrence Frequency
Based Scoring Method
</subsectionHeader>
<bodyText confidence="0.999893833333333">
When disambiguate a polysemous word, we
given the following equation as the score of a
sense item of the polysemous word and the
context containing this polysemous word. The
context of the word is the sentence containing
this word.
</bodyText>
<equation confidence="0.973110666666667">
score(S,C)
(1)
= score(SS,C&apos;)— score(SS,GlobalSS)
</equation>
<bodyText confidence="0.959233833333333">
Where S is a sense item of polysemouse
word W, C is the context containing W, SS is
the corresponding sememe set of S, C&apos; is the set
of sememe expansion of words in C and
GlobaISS is the sememe set that containing all
of the sememe defined in Hownet.
</bodyText>
<equation confidence="0.965188">
score(SS,C&apos;)= Escore(SS,SE&apos;)IICI (2)
VSEee
</equation>
<bodyText confidence="0.983977">
for any sememe set SS and sememe
expansion set C&apos;.
</bodyText>
<equation confidence="0.7511485">
score(SS,SE&apos;)= max score(SS,SS&apos;) (3)
SS&apos; eSE&apos;
</equation>
<bodyText confidence="0.8735355">
for any sememe set SS and sememe
expansion SE&apos;.
</bodyText>
<equation confidence="0.743236666666667">
score(SS,SS&apos;)=Escore(SS,SUVISS1
vsutss.
(4)
</equation>
<bodyText confidence="0.755219">
for any sememe set SS and SS&apos;.
</bodyText>
<page confidence="0.853097">
62
</page>
<figure confidence="0.432884285714286">
score(SS , SW)= Escore(SU,SUVISSI
VSUESS
(5)
for any sememe set SS and sememe SU&apos;.
score(SU , SU&apos;) = I (SU , SU&apos;) (6)
for any sememe SU and SU&apos;.
I (SU , SU&apos;) = log2 (f SU ,SU&apos;)• N2
</figure>
<bodyText confidence="0.871739">
SU&apos;) in SCFD. And for g(SU) and N, we have
the following equation:
</bodyText>
<equation confidence="0.98098075">
g(SU)= Ef(SU,SU.) (8)
vsu&apos;
N =Ef(su,sue)/2 (9)
vsu,vsu
</equation>
<bodyText confidence="0.946487933333333">
In equation (7), the mutual-information-
like measure deviated from the stardard
mutual-information measure by multiple a extra
multiplicative factor N, this is because that the
scale of the corpus is not large enough that the
mutual-information of some sememes pairs
would be negtive if it was not normalized by a
extra multiplicative factor N. In equation (9),
the sum of f(SU,SU) was divided by 2, this is
because for each pair of sememes,
Ef(SU,SU&apos;) is increase by 2.
vsu,vsu&apos;
When disambiguation, we tag the sememe
T that satisfying the following equation to
polysemous word W.
</bodyText>
<equation confidence="0.985392">
T = arg max score(S,C) (10)
</equation>
<sectionHeader confidence="0.3097115" genericHeader="method">
33.2 The Creation Of Mutual Information
Database
</sectionHeader>
<bodyText confidence="0.998433333333333">
We have created a mutual information database
according to (7),(8) and(9) Here is some
examples:
The examples in table 1 have a high mutual
information. The sememe pairs in this table
have certain semantic relations. While the
examples in table 2 have a low mutual
information. And the sememe pairs in this table
have no patency semantic relations.
</bodyText>
<figure confidence="0.61115325">
g(SU)• g(SU&apos;)
Where f(SU,SU&apos;) is the co-occurrence
frequency corresponding to sememe pair ( SU,
(7)
</figure>
<tableCaption confidence="0.997541">
Table 1: example of sememe pairs which have a high mutual information
</tableCaption>
<table confidence="0.999453285714286">
Sememe 1 Sememe Mutual—Inform Sememe 1 Sememe 2 Mutual —Informa
ation tion
9-* 4 -4X1K 33.811057 ti), MtSg 27.418417
Arl3z Vt 29.441937 DA AR 27.234630
YOME A 28.024560 11*0: t 27.093292
mg t---LPA 28.023521 t1474 M 26.984521
ttit ll&apos;a 27.571478 fm a,* 26.710478
</table>
<tableCaption confidence="0.982083">
Table 2: example of sememe pairs which have a low mutual information
</tableCaption>
<table confidence="0.937546333333333">
Sememe 1 Sememe Mutual—Inform Sememe 2 Mutual—Informa
Sememe 1 tion
ation
*A Ak 8.693242 irrg * 9.171023
5Ett E 8.754611 10.1 A 9.357734
$ 09 8.793914 -NI- 1Hk 9.448947
-ftfP 1XM 9.121846 urt 3&apos;EA 9.528801
VIA WM. 9.150412 4 JM 9.599495
It can been concluded from table 1 and table 2 that the mutual information can reflect
</table>
<page confidence="0.998236">
63
</page>
<bodyText confidence="0.810509">
the tightness of semantic relations.
</bodyText>
<sectionHeader confidence="0.969176" genericHeader="method">
4. Experiment And Analysis
</sectionHeader>
<bodyText confidence="0.999614928571428">
We did the experiment on a corpus of 10,000
characters from People&apos;s Dialy.
Firstly, the corpus is segmented, and then
the sememe co-occurrence frequecny database
and mutual information database is created. In
the mutual-information database, there is
709,496 data items corresponding to different
sememes pairs. In order to speeding up the
processing, the mutual-information database
was sorted and indexed according to the first
two bytes of each sememe pair. At last the
experiment of disambiguation of some
polysemous words was dorm Here is two
examples:
</bodyText>
<note confidence="0.4594915">
Example 1: 1-1T1g)YVff.1 !AI:MI-TS
IJWIM.1:114MOKIHH)YA-141.
Example 2: 441*-A I I 4t4k I M I
X1--1511:1*MOYAIJMIMIV414g.
</note>
<bodyText confidence="0.834557">
We use the following eugation to access the
accuracy ratio of disambiguation:
</bodyText>
<figure confidence="0.9447206">
the numberof correctlytaggedexample.
accuracy ratio=
the total numberof examplesin testing se
(11)
the experimental result is shown in table 4.
</figure>
<tableCaption confidence="0.994434">
Table 3: Two examples that disambiguate using sememe co-occurrence frequency database
</tableCaption>
<table confidence="0.8653718">
The definition of The score of sense items and The score of sense items and
word &amp;quot;-g&amp;quot; the context of word &amp;quot;AV in the context ofword &amp;quot;AV in
example 1 example 2
5C1: 14.459068 8.659968
WA 9.817648 10.817648
WI* * 7.415986 12.415986
*A RN -0.134779 -0.134779
WI 5C tkE1 ffi Ail r4-1,1e•. -0.818518 -0.818518
At MR* 14.459068 12.415986
411-1e3A 5C4 VI, V.
</table>
<tableCaption confidence="0.999743">
Table 4: the experiment result
</tableCaption>
<bodyText confidence="0.897384">
Totalnumberoftesting Tlienurdberofcorrectly Accurracy
examples tagged examples ratio
Close test 100 75 75%
Open test 100 71 71%
The disambiguation method introduced
above have the following charatristics:
</bodyText>
<listItem confidence="0.948167166666667">
(1) The problem of data spraseness is
solved in a large degree.
(2) This disambiguation method avoids
the laborious hand tagging of training corpus.
(3) This method can been easily applied
to other kind of corpus.
</listItem>
<sectionHeader confidence="0.985167" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.997563375">
[1]. Nancy Ide, Jean Veronis, Introduction to
the Special Issue on Word Sense
Disambiguation: The State of the Art,
Computational Linguistics, 1998, Volume
24, number 1, pp 1-40
[2]. Philip Resnik, David Yarowsky, A
Perspective on Word Sense
Disambiguation Methods and their
</reference>
<page confidence="0.991804">
64
</page>
<reference confidence="0.943657222222222">
Evaluation,
http://wvvw.cs.jhu.edu/—yarowsky/pubs.ht
ml
[3]. Alpha K. Luk, Statistical Sense
Disambiguation with Relatively Small
Corpus Using Dictionary Definitions, 33rd
Annual Meeting of the Association for
Computational Linguistics,26-30 June,
1995, Massachusetts Institute of
Technology, Cambridge, Massachusetts.
USA, pp.181-188
ifig3C4--Mil, 1998 *M 3 ifi„glA 27 SA,
pp.76-82
[5]. M4g, http://www.how-netcom.
[6]. Kenneth Ward Church, Word Association
Norms, Mutual Information, and
Lexicography, Computational Linguistics,
1990,Volume 16, Number 1, pp.22-29
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.690215">
<title confidence="0.993726">The Research of Word Sense Disambiguation Method Based Frequency of</title>
<author confidence="0.947296">Erhong Yang</author>
<author confidence="0.947296">Guoqing Zhang</author>
<author confidence="0.947296">Yongkui</author>
<affiliation confidence="0.999653">Dept of Computer Science, Shanxi</affiliation>
<address confidence="0.994073">TaiYuan 030006, P. R.</address>
<email confidence="0.997759">zyk@sxu.edu.cn</email>
<abstract confidence="0.9999364">Word sense disambiguation (WSD) is a difficult problem in natural language processing. In this paper, a sememe co-occurrence frequency based WSD method was introduced. In this method, Hownet was used as our information source , and a co-occurrence frequency database of sememes was constructed and then used for WSD. The experimental result showed that this method is successful.</abstract>
<keyword confidence="0.92919">Keywords word sense disambiguation, Hownet, sememe,</keyword>
<intro confidence="0.833934">co-occurrence</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean Veronis</author>
</authors>
<title>Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art, Computational Linguistics,</title>
<date>1998</date>
<volume>24</volume>
<pages>1--40</pages>
<marker>[1]</marker>
<rawString>. Nancy Ide, Jean Veronis, Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art, Computational Linguistics, 1998, Volume 24, number 1, pp 1-40</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>A Perspective on Word Sense Disambiguation Methods and their Evaluation,</title>
<note>http://wvvw.cs.jhu.edu/—yarowsky/pubs.ht ml</note>
<marker>[2]</marker>
<rawString>. Philip Resnik, David Yarowsky, A Perspective on Word Sense Disambiguation Methods and their Evaluation, http://wvvw.cs.jhu.edu/—yarowsky/pubs.ht ml</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alpha K Luk</author>
</authors>
<title>Statistical Sense Disambiguation with Relatively Small Corpus Using Dictionary Definitions,</title>
<date>1995</date>
<journal>M</journal>
<booktitle>33rd Annual Meeting of the Association for Computational Linguistics,26-30</booktitle>
<volume>3</volume>
<pages>181--188</pages>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, Massachusetts. USA,</location>
<marker>[3]</marker>
<rawString>. Alpha K. Luk, Statistical Sense Disambiguation with Relatively Small Corpus Using Dictionary Definitions, 33rd Annual Meeting of the Association for Computational Linguistics,26-30 June, 1995, Massachusetts Institute of Technology, Cambridge, Massachusetts. USA, pp.181-188 ifig3C4--Mil, 1998 *M 3 ifi„glA 27 SA, pp.76-82</rawString>
</citation>
<citation valid="false">
<note>M4g, http://www.how-netcom.</note>
<marker>[5]</marker>
<rawString>. M4g, http://www.how-netcom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography, Computational Linguistics,</title>
<date>1990</date>
<journal>Number</journal>
<volume>1</volume>
<pages>22--29</pages>
<contexts>
<context position="2986" citStr="[6]" startWordPosition="479" endWordPosition="479">n. The format of this file is as follow: W_X =word E_X = some examples of this word G_X= the pos of this word DEF= the definition of this word This research project is supported by a gam from Shanxi Natural Science Foundation of China 60 A important concept used in Hownet that we must introduce is sememe. In Hownet, sememes refer to some basic unit of senses. They are used to describe all the entries in Hownet and there are more than 1,500 sememe all together. 3. Sense Co-occurrence Frequency Database It is well known that some words tend to co-occur frequently with some words than with others[6]. Similarly, some meaning of words tend to co-occur more often with some meaning of words than with others. If we can got the relations of word meanings quantitatively, it would have some help on word sense disambiguation. In Hownet, all words are defined with limited sememes and the combination of sememes is fixed. If we make statistic on the co-occurrence frequency of sememe so as to reflect the co-occurrence of words, the problem of data sparseness would be solved to a large degree. Based on the above thought, we built a sense co-occurrence frequency database to disambiguate word senses. 3.</context>
</contexts>
<marker>[6]</marker>
<rawString>. Kenneth Ward Church, Word Association Norms, Mutual Information, and Lexicography, Computational Linguistics, 1990,Volume 16, Number 1, pp.22-29</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>