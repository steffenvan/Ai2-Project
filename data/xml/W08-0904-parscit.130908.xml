<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.968924">
Recognizing Noisy Romanized Japanese Words in Learner English
</title>
<author confidence="0.888032">
Ryo Nagata
</author>
<affiliation confidence="0.772829">
Konan University
</affiliation>
<note confidence="0.5865695">
Kobe 658-8501, Japan
rnagata[at]konan-u.ac.jp
</note>
<author confidence="0.880964">
Hiromi Sugimoto
</author>
<affiliation confidence="0.660117">
The Japan Institute for
Educational Measurement, Inc.
</affiliation>
<address confidence="0.451623">
Tokyo 162-0831, Japan
</address>
<email confidence="0.731277">
sugimoto[at]jiem.co.jp
</email>
<note confidence="0.62863975">
Jun-ichi Kakegawa
Hyogo University of Teacher Education
Kato 673-1421, Japan
kakegawa[at]hyogo-u.ac.jp
</note>
<author confidence="0.898956">
Yukiko Yabuta
</author>
<affiliation confidence="0.783595">
The Japan Institute for
Educational Measurement, Inc.
</affiliation>
<address confidence="0.477226">
Tokyo 162-0831, Japan
</address>
<email confidence="0.856338">
yabuta[at]jiem.co.jp
</email>
<sectionHeader confidence="0.993565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999282117647059">
This paper describes a method for recognizing
romanized Japanese words in learner English.
They become noise and problematic in a vari-
ety of tasks including Part-Of-Speech tagging,
spell checking, and error detection because
they are mostly unknown words. A problem
one encounters when recognizing romanized
Japanese words in learner English is that the
spelling rules of romanized Japanese words
are often violated by spelling errors. To ad-
dress the problem, the described method uses
a clustering algorithm reinforced by a small
set of rules. Experiments show that it achieves
an -measure of 0.879 and outperforms other
methods. They also show that it only requires
the target text and a fair size of English word
list.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994389145833333">
Japanese learners of English frequently use roman-
ized Japanese words in English writing, which will
be referred to as Roman words hereafter; examples
of Roman words are: SUKIYAKI&apos;, IPPAI (many),
and GANBARU (work hard). Approximately 20%
of different words are Roman words in a corpus con-
sisting of texts written by Japanese second and third
year junior high students. Part of the reason is that
they are lacking in English vocabulary, which leads
them to using Roman words in English writing.
Roman words become noise in a variety of tasks.
In the field of second language acquisition, re-
searchers often use a Part-Of-Speech (POS) tagger
&apos;For consistency, we print Roman words in all capitals.
to analyze learner corpora (Aarts and Granger, 1998;
Granger, 1998; Granger, 1993; Tono, 2000). Since
Roman words are romanized Japanese words and
thus are unknown to POS taggers, they degrades the
performance of POS taggers. In spell checking, they
are a major source of false positives because they
are unknown words as just mentioned. In error de-
tection, most methods such as Chodorow and Lea-
cock (2000), Izumi et al. (2003), Nagata et al. (2005;
2006), and Han et al. (2004; 2006) use a POS tag-
ger and/or a chunker to detect errors. Again, Roman
words degrades their performances.
When viewed from another perspective, Roman
words play an interesting role in second language ac-
quisition. It would be interesting to see what Roman
words are used in the writing of Japanese learners of
English. A frequency list of Roman words should
be useful in vocabulary learning and teaching. En-
glish words corresponding to frequent Roman words
should be taught because learners do not know the
English words despite the fact that they frequently
use the Roman words.
To the best knowledge, there has been no method
for recognizing Roman words in the writing of learn-
ers of English as Sect. 2 will discuss. Therefore, this
paper explores a novel method for the purpose. At
first sight, it might appear to be trivial to recognize
Roman words in English writing since the spelling
system of Roman words is very different from that
of English words. On the contrary, it is not because
spelling errors occur so frequently that the rules in
both spelling systems are violated in many cases. To
address spelling errors, the described method uses a
clustering algorithm reinforced with a small set of
</bodyText>
<page confidence="0.992232">
27
</page>
<note confidence="0.738563">
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 27–35,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.997574166666667">
rules. One of the features of the described method
is that it only requires the target text and a fair size
of an English word list. In other words, it does not
require sources of knowledge such as manually an-
notated training data that are costly to obtain.
The rest of this paper is structured as follows.
Section 2 discusses related work. Section 3 intro-
duces some knowledge of Roman words which is
needed to understand the rest of this paper. Section 4
discusses our initial idea. Section 5 describes the
method. Section 6 describes experiments conducted
to evaluate the method and discusses the results.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999791424242424">
Basically, no methods for recognizing Roman words
have been proposed in the past. However, there have
been a great deal of work related to Roman words.
Transliteration and back-transliteration often in-
volve romanization from Japanese Katakana words
into their equivalents spelled in Roman alphabets as
in Knight and Graehl (1998) and Brill et al. (2001).
For example, Knight and Graehl (1998) back-
transliterate Japanese Katakana words into English
via Japanese romanized equivalents.
Transliteration and back-transliteration, however,
are different tasks from ours. Transliteration and
back-transliteration are a task where given English
and Japanese Katakana words are put into their cor-
responding Japanese Katakana and English words,
respectively, whereas our task is to recognize Roman
words in English text written by learners of English.
More related to our task is loanword identifica-
tion; our task can be viewed as loanword identifica-
tion where loanwords are Roman words in English
text. Jeong et al. (1999) describe a method for distin-
guishing between foreign and pure Korean words in
Korean text. Nwesri et al.(2006) propose a method
for identifying foreign words in Arabic text. Khal-
tar et al. (2006) extract loanwords from Mongolian
corpora using a Japanese loanword dictionary.
These methods are fundamentally different from
ours in the following two points. First, the target text
in our task is full of spelling errors both in Roman
and English words. Second, the above methods re-
quire annotated training data and/or other sources of
knowledge such as a Japanese loanword dictionary
that are hard to obtain in our task.
</bodyText>
<sectionHeader confidence="0.983831" genericHeader="method">
3 Roman Words
</sectionHeader>
<bodyText confidence="0.995503727272727">
This section briefly introduces the spelling sys-
tem of Roman words which is needed to under-
stand the rest of this paper. For detailed discussion
of Japanese-English romanization, see Knight and
Graehl (1998).
The spelling system has five vowels: a, i, u, e,
o . It has 18 consonants : b, c, d, f, g, h, j, k, l, m,
n, p, r, s, t, w, y, z . Note that some alphabets such
as q and x are not used in Roman words.
Roman words basically satisfy the following two
rules:
</bodyText>
<listItem confidence="0.977989">
1. Roman words end with either a vowel or n
2. A consonant is always followed by a vowel
</listItem>
<bodyText confidence="0.9448475625">
The first rule implies that one can tell that a word
ending with a consonant except n is not a Roman
word without looking at the whole word. There are
two exceptions to the second rule. The first is that
the consonant n sometimes behaves like a vowel and
is followed by other consonants such as nb as in
GANBARU. The second is that some combinations
of two consonants such as ky and tt are used to ex-
press gemination and contracted sounds. However,
the second rule is satisfied if these combinations are
regarded to function as a consonant to express gem-
ination and contracted sounds. An implication from
the second rule is that alternate occurrences of a
consonant-vowel are very common to Roman words
as in SAMURAI2 and SUKIYAKI. Another is that
a sequence of three consonants, such as tch and btl
as in watch and subtle, respectively, never appear in
Roman words excluding the exceptional consecutive
consonants for gemination and contracted sounds.
In the writing of Japanese learners of English,
the two rules are often violated because of spelling
errors. For example, SHSHI, GZUUNOTOU, and
MATHYA appear in corpora used in the experi-
ments where the underline indicates where the vio-
lations of the rules exist; we believe that even na-
tive speakers of the Japanese language have diffi-
culty guessing the right spellings (The answers are
shown in Sect. 6.2).
2Well-known Japanese words such as SAMURAI and
SUKIYAKI are used as examples for illustration purpose. In
the writing of Japanese learners of English, however, a wide
variety of Japanese words appear as exemplified in Sect. 1.
</bodyText>
<page confidence="0.997875">
28
</page>
<bodyText confidence="0.999738727272727">
Also, English words are mis-spelled in the writing
of Japanese learners of English. Mis-spelled English
words often satisfy the two rules. For example, the
word because is mis-spelled with variations in error
such as becaus, becose, becoue, becouese, becuse,
becaes, becase, and becaues where the underlines
indicate words that satisfy the two rules.
In summary, the spelling system of Roman words
is quite different from that of English. However, in
the writing of Japanese learners of English, the two
rules are often violated because of spelling errors.
</bodyText>
<sectionHeader confidence="0.99783" genericHeader="method">
4 Initial (but Failed) Idea
</sectionHeader>
<bodyText confidence="0.999989739130435">
This section discusses our initial idea for the task,
which turned out to be a failure. Nevertheless, this
section discusses it because it will play an important
role later on.
Our initial idea was as follows. As shown in
Sect. 3, Roman words are based on a spelling sys-
tem that is very different from that of English. The
spelling system is so different that a clustering al-
gorithm such as -means clustering (Abney, 2007)
is able to distinguish Roman words from English
words if the differences are represented well in the
feature vector.
A trigram-based feature vector is well-suited for
capturing the differences. Each attribute in the vec-
tor corresponds to a certain trigram such as sam. The
value corresponds to the number of occurrences of
the trigram in a given word. For example, the value
of the attribute corresponding to the trigram sam is
1 in the Roman word SAMURAI. The dummy sym-
bols &amp;quot; and $ are appended to denote the beginning
and end of a word, respectively. All words are con-
verted entirely to lowercase when transformed into
feature vectors. For example, the Roman word:
</bodyText>
<sectionHeader confidence="0.649864" genericHeader="method">
SAMURAI
</sectionHeader>
<bodyText confidence="0.99669446">
would give the trigrams:
&amp;quot;&amp;quot;s &amp;quot;sa sam amu mur ura rai ai$ i$$,
and be transformed into a feature vector where the
values corresponding to the above trigrams are 1,
otherwise 0.
The algorithm for recognizing Roman words
based on this initial idea is as follows:
Input: target corpus and English word list
Output: lists of Roman words and English words
Step 1. make a word list from the target corpus
Step 2. remove all words from the list that are in
the English word list
Step 3. transform each word in the resulting list
into the feature vector
Step 4. run -means clustering on the feature vec-
tors with
Step 5. output the result
In Step 1., the target corpus is turned into a word
list. In Step 2., words that are in the English word
list are recognized as English words and removed
from the word list. Note that at this point, there will
be still English words on the list because an English
word list is never comprehensive. More importantly,
the list includes mis-spelled English words. In Step
3., each word in the resulting list is transformed into
the feature vector as just explained above. In Step
4., -means clustering is used to find two clusters
for the feature vectors; because there are two
classes of words — one for Roman words and one
for English words. In Step 5., each word is outputted
with the result of the clustering. This was our initial
idea. It was unsupervised and easy to implement.
Contrary to our expectation, however, the results
were far from satisfactory as Sect. 6 will show. The
resulting clusters were meaningless in terms of Ro-
man word recognition. For instance, one of the
obtained two clusters was for gerunds and present
participles (namely, words ending with ing) and the
other was for the rest (including Roman words and
other English words). The results reveal that it is
impossible to represent all English words by one
cluster obtained from a centroid that is initially ran-
domly chosen. The algorithm was tested with dif-
ferent settings (different and different numbers of
instances to compute the initial centroids). It some-
times performed slightly better, but it was too ad hoc
to be a reliable method.
This is why we had to take another approach. At
the same time, this initial idea will play an important
role soon as already mentioned.
</bodyText>
<sectionHeader confidence="0.990025" genericHeader="method">
5 Proposed Method
</sectionHeader>
<bodyText confidence="0.9992575">
So far, we have seen that a clustering algorithm does
not work well on the task. However, there is no
</bodyText>
<page confidence="0.991693">
29
</page>
<bodyText confidence="0.981398666666667">
doubt that the spelling system of Roman words is
very different from that of English words. Because
of the differences, the two rules described in Sect. 3
should almost perfectly recognize Roman words if
there were no spelling errors.
To make the task simple, let us assume that there
were no spelling errors in the target corpus for the
time being. Under this assumption, the task is
greatly simplified. As with the initial idea, known
English words can easily be removed from the word
list. Then, all Roman words will be retrieved from
the list with few English words by pattern matching
based on the two rules.
For pattern matching, words are first put into a
Consonant Vowel (CV) pattern. It is simply done
by replacing consonants and vowels as defined in
Sect. 3 with dummy characters denoting consonants
and vowels (C and V in this paper), respectively. For
example, the Roman word:
SAMURAI
would be transformed into the CV pattern:
</bodyText>
<sectionHeader confidence="0.822691" genericHeader="method">
CVCVCVV
</sectionHeader>
<bodyText confidence="0.982389338709677">
while the English word:
fighter
into the CV pattern:
CVCCCVC.
There are some notable differences between the two.
An exception to the transformation is that the conso-
nant n is replaced with C only when it follows one
of the consonants since it sometimes behaves like
a vowel (see Sect.3 for details) and requires a spe-
cial care. Before the transformation, the exceptional
consecutive consonants for gemination and contract
sounds are normalized by the following simple re-
placement rules:
For example, the double consonant tt is replaced
with the single consonant t using the first rule. Then,
a word is recognized as a Roman word if its CV pat-
tern matches:
ˆ[Vn]*(C[Vn]+)*$
where the matcher is written in Perl or Java-like reg-
ular expression. Roughly, words that comprise se-
quences of a consonant-vowel, and end with a vowel
or the consonant n are recognized as Roman words.
This method should work perfectly if we disre-
gard spelling errors. We will refer to this method as
the rule-based method, hereafter. Actually, it works
surprisingly well even with spelling errors as the ex-
periments in Sect.6 will show. However, there is
still room for improvement in handling mis-spelled
words.
Now back to the real world. The sources of false
positives and negatives in the rule-based method are
spelling errors both in Roman and English words.
For instance, the rule-based method recognizes mis-
spelled English words such as becose, becoue, and
becouese, which are correctly the word because, as
Roman words. Likewise, mis-spelled Roman words
are recognized as English words.
Here, the initial idea comes to play an important
role. Like in the initial idea, each word can be trans-
formed into a point in vector space as exemplified
in a somewhat simplified manner in Fig. 1; R and E
in Fig.1 denote words recognized by the rule-based
method as Roman and English words, respectively.
Pale R and E correspond to false positives and nega-
tives, (which of course is unknown to the rule-based
method). Unlike in the initial idea, we now know
plausible centroids for Roman and English words.
We can compute the centroid for Roman words from
the words recognized as Roman words by the rule-
based method. Also, we can compute the centroid
for English words from the words in the English
word dictionary. This situation is shown in Fig. 2
where the centroids are denoted by +. False pos-
itives and negatives are expected to be nearer to
the centroids for their true class, because even with
spelling errors they share a structural similarly with
their correctly-spelled counterparts. Taking this into
account, all predictions obtained by the rule-based
method are overridden by the class of their nearest
centroid as shown in Fig. 3. The procedures for com-
puting the centroids and overriding the predictions
can be repeated until convergence. Then, this part is
</bodyText>
<figure confidence="0.990398533333333">
double consonants single consonant
(e.g, tt t),
([bdfghjklmnstprz])y([auo]) $1$2
(e.g., bya ba),
([sc])h([aiueo]) $1$2
(e.g., sha sa),
tsu tu
30
the same as the initial idea based on -means clus-
tering.
R
R
R R
E E
E
R
E E
E
Decision boundary
E
E
R E
R
R
R
+
E
+
E
R R
</figure>
<figureCaption confidence="0.999302">
Figure 3: Overridden false positives and negatives
Figure 1: Roman and English words in vector space
Figure 2: Plausible centroids
</figureCaption>
<bodyText confidence="0.99934464516129">
The algorithm of the proposed method is:
Input: target corpus and English word list
Output: list of Roman words
Step A. make a word list from the target corpus
Step B. remove all words from the list that are in
the English word list
Step C. transform each word in the resulting list
into the feature vector
Step D. obtain a tentative list of Roman words using
the rule-based method
Step E. compute centroids for Roman and English
words from the tentative list and the English
word list, respectively
Step F. override the previous class of each word by
the class of its nearest centroid
Step G. repeat Step E and F until convergence
Step H. output the result
Steps A to C are the same as in the algorithm of the
initial idea. Step D then uses the rule-based method
to obtain a tentative list of Roman words. Step E
computes centroids for Roman and English words
by taking averages of each value of the feature vec-
tors. Step F overrides previous classes obtained by
the rule-based method or previous iteration. The
distances between each feature vector and the cen-
troids are measured by the Euclidean distance. Step
G computes centroids and overrides previous predic-
tions until convergence. This step may be omitted
to give a variation of the proposed method. Step H
outputs words belonging to the centroid for Roman
words.
</bodyText>
<sectionHeader confidence="0.999897" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.9987">
6.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999816888888889">
Three sets of corpora were used for evaluation. The
first consisted of essays on the topic winter holiday
written by second year junior high students. It was
used to develop the rule-based method. The second
consisted of essays on the topic school trip written
by third year junior high students. The third was
the combination of the two. Table 1 shows the tar-
get corpora statistics3. Evaluation was done on only
unknown words in the target corpora since known
</bodyText>
<figure confidence="0.974989692307692">
Decision boundary
R R
E
E
+
R
R
E
E
E
E
R
+
</figure>
<page confidence="0.999917">
31
</page>
<tableCaption confidence="0.999721">
Table 1: Target corpora statistics
</tableCaption>
<table confidence="0.9607625">
Corpus # sentences # words # diff. words # diff. unknown words # diff. Roman words
Jr. high 2 9928 56724 1675 1040 275
Jr. high 3 10441 60546 2163 1334 500
Jr. high 2&amp;3 20369 117270 3299 2237 727
</table>
<bodyText confidence="0.969475">
words can be easily recognized as English words by
referring to an English word list.
As an English word list, the 7,726 words (Leech
et al., 2001) that occur at least 10 times per mil-
lion words in the British National Corpus (Burnard,
1995) were combined with the English word list in
Ispell, the spell checker. The whole list consisted of
19816 words.
As already mentioned in Sect. 2, there has been no
method for recognizing Roman words. Therefore,
we set three baselines for comparison. In the first,
all words that were not listed in the English word list
were recognized as Roman words. In the second,
-means clustering was used to recognize Roman
words in the target corpora as described in Sect.4
(i.e., the initial idea). The -means clustering-based
method was tested on each target corpora five times
and the results were averaged to calculate the overall
performances. Five instances were randomly chosen
to compute the initial centroids for each class. In the
third, the rule-based method described in Sect. 5 was
used as a baseline.
The performance was evaluated by recall, preci-
sion, and -measure. Recall and precision were de-
fined by
# Roman words correctly recognized (1)
# diff. Roman words
and
# Roman words correctly recognized (2)
# words recognized as Roman words
respectively. -measure was defined by
(3)
3From the Jr. high 2&amp;3 corpus, we randomly took 200 sen-
tences (1645 words) to estimate the spelling error rate. It was an
error rate of 2.8% (46/1645). We also investigated if there was
ambiguity between Roman and English words in the target cor-
pora (for example, the word sake can be a Roman word (a kind
of alcohol) and an English word (as in God’s sake). It turned
out that there were no such cases in the target corpora.
</bodyText>
<subsectionHeader confidence="0.999882">
6.2 Experimental Results and Discussion
</subsectionHeader>
<bodyText confidence="0.719853285714286">
Table 2, Table 3, and Table 4 show the experimen-
tal results for the target corpora. In the tables, List-
based, K-means, and Rule-based denote the English
word list-based, -means clustering-based, and rule-
based baselines, respectively. Also, Proposed (itera-
tion) and Proposed denote the proposed method with
and without iteration, respectively.
</bodyText>
<tableCaption confidence="0.997538">
Table 2: Experimental results for Jr. high 2
</tableCaption>
<table confidence="0.985037333333333">
Method
1.00 0.268 0.423
0.737 0.298 0.419
0.898 0.737 0.810
0.855 0.799 0.826
0.938 0.761 0.840
</table>
<tableCaption confidence="0.995575">
Table 3: Experimental results for Jr. high 3
</tableCaption>
<table confidence="0.983087666666667">
Method
1.00 0.382 0.553
0.736 0.368 0.490
0.824 0.831 0.827
0.852 0.916 0.883
0.914 0.882 0.898
</table>
<tableCaption confidence="0.999537">
Table 4: Experimental results for Jr. high 2&amp;3
</tableCaption>
<subsectionHeader confidence="0.727747">
Method
</subsectionHeader>
<bodyText confidence="0.994799">
The results show that the English word list-based
baseline does not work well. The reason is that mis-
</bodyText>
<figure confidence="0.94574925">
List-based
-means
Rule-based
Proposed (iteration)
Proposed
List-based
-means
Rule-based
Proposed (iteration)
Proposed
List-based
-means
Rule-based
Proposed (iteration)
Proposed
1.00 0.331 0.497
0.653 0.491 0.500
0.849 0.794 0.820
0.851 0.867 0.859
0.922 0.840 0.879
</figure>
<page confidence="0.9972">
32
</page>
<bodyText confidence="0.999914830508475">
spelled words occur so frequently in the writing of
Japanese learners of English that simply recogniz-
ing unknown words as Roman words causes a lot of
false positives.
The -means clustering-based baseline performs
similarly or even worse in terms of -measure. Sec-
tion 4 has already discussed the reason. Namely, it
is impossible to represent all English words by one
cluster obtained by simple -means clustering.
Unlike the other two, the rule-based baseline per-
forms surprisingly well considering the fact that it is
based on a simple (pattern matching ) rule. This in-
dicates that the spelling system of Roman words is
quite different from that of English words. Thus, it
would almost perfectly perform for English writing
without spelling errors.
The proposed methods further improve the per-
formance of the rule-based method in all target cor-
pora. Especially, the proposed method without it-
eration performs well. Indeed, it performs signif-
icantly better than the rule-based method does in
both recall (99% confidence level, difference of pro-
portion test) and precision (95% confidence level,
difference of proportion test) in the whole corpus.
They reinforce the rule-based method by overriding
false positives and negatives via centroid identifica-
tion as initially estimated from the results of the rule-
based method as Fig. 1, Fig.2, and Fig.3 illustrate
in Sect. 5. This implies that the estimated centroids
represent Roman and English words well. Because
of this property, the proposed methods can distin-
guish mis-spelled Roman words from (often mis-
spelled) English words. Interestingly, the proposed
methods recognized mis-spelled Roman words that
we would prove are difficult for even native speakers
of the Japanese language to recognize as words; e.g.,
SHSHI, GZUUNOTOU, and MATHYA; correctly,
SUSHI, GOZYUNOTOU (five-story pagoda), and
MATTYA (strong green tea).
To see the property, we extracted characteristic
trigrams of the Roman and English centroids. We
sorted each trigram in descending and ascending or-
ders by where and denote the feature
values corresponding to the -th trigram in the Ro-
man and English centroids, respectively, and is
a parameter to assure that the value can always be
calculated. Table 5 shows the top 20 characteristic
trigrams that are extracted from the centroids of the
proposed method without iteration; the whole target
corpus was used and was set to 0.001. It shows
that trigrams such as i$$, associated with words end-
ing with a vowel are characteristic of the Roman
centroid. This is consistent with the first rule of
the spelling system of Roman words. By contrast,
it shows that trigrams associated with words ending
with a consonant are characteristic of the English
centroid. Indeed, some of these are morphological
suffixes such as ed$ and ly$. Others are associated
with English syllables such as ble and tion.
</bodyText>
<tableCaption confidence="0.997384">
Table 5: Characteristic trigram of centroids
</tableCaption>
<figure confidence="0.942359916666667">
Roman centroid English centroid
i$$ y$$
u$$ s$$
ji$ d$$
aku t$$
hi$ ed$
uji r$$
ko g$$
✰ka l$$
ku$ ng$
ki$ co
ou$ er$
</figure>
<bodyText confidence="0.957756047619048">
kak tio
nka ati
zi$ ly$
uku al$
ryu nt$
dai ble
ya$ abl
ika es$
ri$ ty$
To our surprise, the proposed method without iter-
ation outperforms the one with iteration in terms of
-measure. This implies that the proposed method
performs better when each word is compared to an
exemplar (centroid) based on the idealized Roman
words, rather than one based on the Roman words
actually observed. Like before, we extracted charac-
teristic trigrams from the centroids of the proposed
method with iteration. As a result, we found that
trigrams such as mpl and ✰kn that violate the two
rules of Roman words were ranked much higher.
Similarly, trigrams that associate with Roman words
</bodyText>
<page confidence="0.997015">
33
</page>
<bodyText confidence="0.9999816875">
were extracted as characteristic trigrams of the En-
glish centroid. This explains why the proposed
method without iteration performs better.
Although the proposed methods perform well,
there are still false positives and negatives. A ma-
jor cause of false positives is mis-spelled English
words, which suggests that spelling errors are prob-
lematic even in the proposed methods. It accounts
for 94% of all false positives. The rest are foreign
(excluding Japanese) words such as pizza that were
not in the English word list and flow the two rules
of Roman words. False negatives are mainly Roman
words that partly consist of English syllables and/or
English words. For example, OMIYAGE (souvenir)
contains the English syllable om as in omnipotent as
well as the English word age.
</bodyText>
<sectionHeader confidence="0.999573" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99992675">
This paper described methods for recognizing Ro-
man words in learner English. Experiments show
that the described methods are effective in rec-
ognizing Roman words even in texts containing
spelling errors which is often the case in learner
English. One of the advantages of the described
methods is that they only require the target text
and an English word list that is easy to obtain. A
tool based on the described methods is available at
http://www.ai.info.mie-u.ac.jp/˜nagata/tools/
For future work, we will investigate how to tag
Roman words with POS tags; note that Roman
words vary in POS as exemplified in Sect. 1. Also,
we will explore to apply the described method to
other languages, which will make it more useful in a
variety of applications.
</bodyText>
<sectionHeader confidence="0.998367" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993182333333333">
This research was partially supported by the Min-
istry of Education, Science, Sports and Culture,
Grant-in-Aid for Young Scientists (B), 19700637.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998836983606557">
Jan Aarts and Sylviane Granger. 1998. Tag sequences in
learner corpora: a key to interlanguage grammar and
discourse. Longman Pub Group.
Steven Abney. 2007. Semisupervised Learning for Com-
putational Linguistics. Chapman &amp; Hall/CRC.
Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting Katakana-English term pairs
from search engine query logs. In Proc. of 6th Natural
Language Processing Pacific Rim Symposium, pages
393–399.
Lou Burnard. 1995. Users Reference Guide for the
British National Corpus. version 1.0. Oxford Univer-
sity Computing Services, Oxford.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proc. of 1st Meeting of the North America Chapter of
ACL, pages 140–147.
Sylviane Granger. 1993. The international corpus of
learner English. In English language corpora: De-
sign, analysis and exploitation, pages 57–69. Rodopi.
Sylviane Granger. 1998. Prefabricated patterns in ad-
vanced EFL writing: collocations and formulae. In
A. P. Cowie, editor, Phraseology: theory, analysis, and
application, pages 145–160. Clarendon Press.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting errors in English article usage with
a maximum entropy classifier trained on a large, di-
verse corpus. In Proc. of 4th International Conference
on Language Resources and Evaluation, pages 1625–
1628.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115–129.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners’ English spoken
data. In Proc. of 41st Annual Meeting of ACL, pages
145–148.
Kil S. Jeong, Sung H. Myaeng, Jae S. Lee, and Key-
Sun Choi. 1999. Automatic identification and back-
transliteration of foreign words for information re-
trieval. Information Processing and Management,
35:523–540.
Badam-Osor Khaltar, Atsushi Fujii, and Tetsuya
Ishikawa. 2006. Extracting loanwords from Mon-
golian corpora and producing a Japanese-Mongolian
bilingual dictionary. In Proc. of the 44th Annual Meet-
ing ofACL, pages 657–664.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599–
612.
Geoffrey Leech, Paul Rayson, and Andrew Wilson.
2001. Word Frequencies in Written and Spoken En-
glish: based on the British National Corpus. Long-
man.
Ryo Nagata, Takahiro Wakana, Fumito Masui, Atsuo
Kawai, and Naoki Isu. 2005. Detecting article errors
based on the mass count distinction. In Proc. of 2nd
International Joint Conference on Natural Language
Processing, pages 815–826.
</reference>
<page confidence="0.990578">
34
</page>
<reference confidence="0.999014833333333">
Ryo Nagata, Astuo Kawai, Koichiro Morihiro, and Naoki
Isu. 2006. A feedback-augmented method for detect-
ing errors in the writing of learners of English. In
Proc. of 44th Annual Meeting ofACL, pages 241–248.
Abdusalam F.A. Nwesri, Seyed M.M. Tahaghoghi, and
Falk Scholer. 2006. Capturing out-of-vocabulary
words in Arabic text. In Proc. of 2006 Conference on
EMNLP, pages 258–266.
Yukio Tono. 2000. A corpus-based analysis of inter-
language development: analysing POS tag sequences
of EFL learner corpora. In Practical Applications in
Language Corpora, pages 123–132.
</reference>
<page confidence="0.999326">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048457">
<title confidence="0.840682">Recognizing Noisy Romanized Japanese Words in Learner English Ryo</title>
<note confidence="0.4811725">Konan Kobe 658-8501,</note>
<email confidence="0.881697">rnagata[at]konan-u.ac.jp</email>
<author confidence="0.996249">Hiromi Sugimoto</author>
<affiliation confidence="0.995355">The Japan Institute Educational Measurement, Inc.</affiliation>
<address confidence="0.991735">Tokyo 162-0831,</address>
<email confidence="0.968971">sugimoto[at]jiem.co.jp</email>
<author confidence="0.482617">Jun-ichi</author>
<affiliation confidence="0.98531">Hyogo University of Teacher</affiliation>
<address confidence="0.94502">Kato 673-1421,</address>
<email confidence="0.978439">kakegawa[at]hyogo-u.ac.jp</email>
<author confidence="0.81817">Yukiko</author>
<affiliation confidence="0.9940205">The Japan Institute Educational Measurement,</affiliation>
<address confidence="0.993383">Tokyo 162-0831,</address>
<email confidence="0.983518">yabuta[at]jiem.co.jp</email>
<abstract confidence="0.997289555555556">This paper describes a method for recognizing romanized Japanese words in learner English. They become noise and problematic in a variety of tasks including Part-Of-Speech tagging, spell checking, and error detection because they are mostly unknown words. A problem one encounters when recognizing romanized Japanese words in learner English is that the spelling rules of romanized Japanese words are often violated by spelling errors. To address the problem, the described method uses a clustering algorithm reinforced by a small set of rules. Experiments show that it achieves an -measure of 0.879 and outperforms other methods. They also show that it only requires the target text and a fair size of English word list.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Aarts</author>
<author>Sylviane Granger</author>
</authors>
<title>Tag sequences in learner corpora: a key to interlanguage grammar and discourse. Longman Pub Group.</title>
<date>1998</date>
<contexts>
<context position="1957" citStr="Aarts and Granger, 1998" startWordPosition="289" endWordPosition="292"> words hereafter; examples of Roman words are: SUKIYAKI&apos;, IPPAI (many), and GANBARU (work hard). Approximately 20% of different words are Roman words in a corpus consisting of texts written by Japanese second and third year junior high students. Part of the reason is that they are lacking in English vocabulary, which leads them to using Roman words in English writing. Roman words become noise in a variety of tasks. In the field of second language acquisition, researchers often use a Part-Of-Speech (POS) tagger &apos;For consistency, we print Roman words in all capitals. to analyze learner corpora (Aarts and Granger, 1998; Granger, 1998; Granger, 1993; Tono, 2000). Since Roman words are romanized Japanese words and thus are unknown to POS taggers, they degrades the performance of POS taggers. In spell checking, they are a major source of false positives because they are unknown words as just mentioned. In error detection, most methods such as Chodorow and Leacock (2000), Izumi et al. (2003), Nagata et al. (2005; 2006), and Han et al. (2004; 2006) use a POS tagger and/or a chunker to detect errors. Again, Roman words degrades their performances. When viewed from another perspective, Roman words play an interest</context>
</contexts>
<marker>Aarts, Granger, 1998</marker>
<rawString>Jan Aarts and Sylviane Granger. 1998. Tag sequences in learner corpora: a key to interlanguage grammar and discourse. Longman Pub Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Semisupervised Learning for Computational Linguistics.</title>
<date>2007</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="9213" citStr="Abney, 2007" startWordPosition="1508" endWordPosition="1509">ds is quite different from that of English. However, in the writing of Japanese learners of English, the two rules are often violated because of spelling errors. 4 Initial (but Failed) Idea This section discusses our initial idea for the task, which turned out to be a failure. Nevertheless, this section discusses it because it will play an important role later on. Our initial idea was as follows. As shown in Sect. 3, Roman words are based on a spelling system that is very different from that of English. The spelling system is so different that a clustering algorithm such as -means clustering (Abney, 2007) is able to distinguish Roman words from English words if the differences are represented well in the feature vector. A trigram-based feature vector is well-suited for capturing the differences. Each attribute in the vector corresponds to a certain trigram such as sam. The value corresponds to the number of occurrences of the trigram in a given word. For example, the value of the attribute corresponding to the trigram sam is 1 in the Roman word SAMURAI. The dummy symbols &amp;quot; and $ are appended to denote the beginning and end of a word, respectively. All words are converted entirely to lowercase </context>
</contexts>
<marker>Abney, 2007</marker>
<rawString>Steven Abney. 2007. Semisupervised Learning for Computational Linguistics. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Gary Kacmarcik</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically harvesting Katakana-English term pairs from search engine query logs.</title>
<date>2001</date>
<booktitle>In Proc. of 6th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>393--399</pages>
<contexts>
<context position="4765" citStr="Brill et al. (2001)" startWordPosition="757" endWordPosition="760">uces some knowledge of Roman words which is needed to understand the rest of this paper. Section 4 discusses our initial idea. Section 5 describes the method. Section 6 describes experiments conducted to evaluate the method and discusses the results. 2 Related Work Basically, no methods for recognizing Roman words have been proposed in the past. However, there have been a great deal of work related to Roman words. Transliteration and back-transliteration often involve romanization from Japanese Katakana words into their equivalents spelled in Roman alphabets as in Knight and Graehl (1998) and Brill et al. (2001). For example, Knight and Graehl (1998) backtransliterate Japanese Katakana words into English via Japanese romanized equivalents. Transliteration and back-transliteration, however, are different tasks from ours. Transliteration and back-transliteration are a task where given English and Japanese Katakana words are put into their corresponding Japanese Katakana and English words, respectively, whereas our task is to recognize Roman words in English text written by learners of English. More related to our task is loanword identification; our task can be viewed as loanword identification where l</context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001. Automatically harvesting Katakana-English term pairs from search engine query logs. In Proc. of 6th Natural Language Processing Pacific Rim Symposium, pages 393–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Users Reference Guide for the British National Corpus. version 1.0.</title>
<date>1995</date>
<institution>Oxford University Computing Services,</institution>
<location>Oxford.</location>
<contexts>
<context position="18829" citStr="Burnard, 1995" startWordPosition="3184" endWordPosition="3185">rpora statistics3. Evaluation was done on only unknown words in the target corpora since known Decision boundary R R E E + R R E E E E R + 31 Table 1: Target corpora statistics Corpus # sentences # words # diff. words # diff. unknown words # diff. Roman words Jr. high 2 9928 56724 1675 1040 275 Jr. high 3 10441 60546 2163 1334 500 Jr. high 2&amp;3 20369 117270 3299 2237 727 words can be easily recognized as English words by referring to an English word list. As an English word list, the 7,726 words (Leech et al., 2001) that occur at least 10 times per million words in the British National Corpus (Burnard, 1995) were combined with the English word list in Ispell, the spell checker. The whole list consisted of 19816 words. As already mentioned in Sect. 2, there has been no method for recognizing Roman words. Therefore, we set three baselines for comparison. In the first, all words that were not listed in the English word list were recognized as Roman words. In the second, -means clustering was used to recognize Roman words in the target corpora as described in Sect.4 (i.e., the initial idea). The -means clustering-based method was tested on each target corpora five times and the results were averaged </context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Lou Burnard. 1995. Users Reference Guide for the British National Corpus. version 1.0. Oxford University Computing Services, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In Proc. of 1st Meeting of the North America Chapter of ACL,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="2312" citStr="Chodorow and Leacock (2000)" startWordPosition="347" endWordPosition="351">in English writing. Roman words become noise in a variety of tasks. In the field of second language acquisition, researchers often use a Part-Of-Speech (POS) tagger &apos;For consistency, we print Roman words in all capitals. to analyze learner corpora (Aarts and Granger, 1998; Granger, 1998; Granger, 1993; Tono, 2000). Since Roman words are romanized Japanese words and thus are unknown to POS taggers, they degrades the performance of POS taggers. In spell checking, they are a major source of false positives because they are unknown words as just mentioned. In error detection, most methods such as Chodorow and Leacock (2000), Izumi et al. (2003), Nagata et al. (2005; 2006), and Han et al. (2004; 2006) use a POS tagger and/or a chunker to detect errors. Again, Roman words degrades their performances. When viewed from another perspective, Roman words play an interesting role in second language acquisition. It would be interesting to see what Roman words are used in the writing of Japanese learners of English. A frequency list of Roman words should be useful in vocabulary learning and teaching. English words corresponding to frequent Roman words should be taught because learners do not know the English words despite</context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock. 2000. An unsupervised method for detecting grammatical errors. In Proc. of 1st Meeting of the North America Chapter of ACL, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>The international corpus of learner English.</title>
<date>1993</date>
<booktitle>In English language corpora: Design, analysis and exploitation,</booktitle>
<pages>57--69</pages>
<publisher>Rodopi.</publisher>
<contexts>
<context position="1987" citStr="Granger, 1993" startWordPosition="295" endWordPosition="296">s are: SUKIYAKI&apos;, IPPAI (many), and GANBARU (work hard). Approximately 20% of different words are Roman words in a corpus consisting of texts written by Japanese second and third year junior high students. Part of the reason is that they are lacking in English vocabulary, which leads them to using Roman words in English writing. Roman words become noise in a variety of tasks. In the field of second language acquisition, researchers often use a Part-Of-Speech (POS) tagger &apos;For consistency, we print Roman words in all capitals. to analyze learner corpora (Aarts and Granger, 1998; Granger, 1998; Granger, 1993; Tono, 2000). Since Roman words are romanized Japanese words and thus are unknown to POS taggers, they degrades the performance of POS taggers. In spell checking, they are a major source of false positives because they are unknown words as just mentioned. In error detection, most methods such as Chodorow and Leacock (2000), Izumi et al. (2003), Nagata et al. (2005; 2006), and Han et al. (2004; 2006) use a POS tagger and/or a chunker to detect errors. Again, Roman words degrades their performances. When viewed from another perspective, Roman words play an interesting role in second language ac</context>
</contexts>
<marker>Granger, 1993</marker>
<rawString>Sylviane Granger. 1993. The international corpus of learner English. In English language corpora: Design, analysis and exploitation, pages 57–69. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>Prefabricated patterns in advanced EFL writing: collocations and formulae.</title>
<date>1998</date>
<pages>145--160</pages>
<editor>In A. P. Cowie, editor, Phraseology: theory, analysis, and application,</editor>
<contexts>
<context position="1957" citStr="Granger, 1998" startWordPosition="291" endWordPosition="292">eafter; examples of Roman words are: SUKIYAKI&apos;, IPPAI (many), and GANBARU (work hard). Approximately 20% of different words are Roman words in a corpus consisting of texts written by Japanese second and third year junior high students. Part of the reason is that they are lacking in English vocabulary, which leads them to using Roman words in English writing. Roman words become noise in a variety of tasks. In the field of second language acquisition, researchers often use a Part-Of-Speech (POS) tagger &apos;For consistency, we print Roman words in all capitals. to analyze learner corpora (Aarts and Granger, 1998; Granger, 1998; Granger, 1993; Tono, 2000). Since Roman words are romanized Japanese words and thus are unknown to POS taggers, they degrades the performance of POS taggers. In spell checking, they are a major source of false positives because they are unknown words as just mentioned. In error detection, most methods such as Chodorow and Leacock (2000), Izumi et al. (2003), Nagata et al. (2005; 2006), and Han et al. (2004; 2006) use a POS tagger and/or a chunker to detect errors. Again, Roman words degrades their performances. When viewed from another perspective, Roman words play an interest</context>
</contexts>
<marker>Granger, 1998</marker>
<rawString>Sylviane Granger. 1998. Prefabricated patterns in advanced EFL writing: collocations and formulae. In A. P. Cowie, editor, Phraseology: theory, analysis, and application, pages 145–160. Clarendon Press. Na-Rae Han, Martin Chodorow, and Claudia Leacock.</rawString>
</citation>
<citation valid="true">
<title>Detecting errors in English article usage with a maximum entropy classifier trained on a large, diverse corpus.</title>
<date>2004</date>
<booktitle>In Proc. of 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1625--1628</pages>
<contexts>
<context position="2390" citStr="(2004; 2006)" startWordPosition="365" endWordPosition="366">nguage acquisition, researchers often use a Part-Of-Speech (POS) tagger &apos;For consistency, we print Roman words in all capitals. to analyze learner corpora (Aarts and Granger, 1998; Granger, 1998; Granger, 1993; Tono, 2000). Since Roman words are romanized Japanese words and thus are unknown to POS taggers, they degrades the performance of POS taggers. In spell checking, they are a major source of false positives because they are unknown words as just mentioned. In error detection, most methods such as Chodorow and Leacock (2000), Izumi et al. (2003), Nagata et al. (2005; 2006), and Han et al. (2004; 2006) use a POS tagger and/or a chunker to detect errors. Again, Roman words degrades their performances. When viewed from another perspective, Roman words play an interesting role in second language acquisition. It would be interesting to see what Roman words are used in the writing of Japanese learners of English. A frequency list of Roman words should be useful in vocabulary learning and teaching. English words corresponding to frequent Roman words should be taught because learners do not know the English words despite the fact that they frequently use the Roman words. To the best knowledge, the</context>
</contexts>
<marker>2004</marker>
<rawString>2004. Detecting errors in English article usage with a maximum entropy classifier trained on a large, diverse corpus. In Proc. of 4th International Conference on Language Resources and Evaluation, pages 1625– 1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
</authors>
<title>Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara.</title>
<date>2003</date>
<booktitle>In Proc. of 41st Annual Meeting of ACL,</booktitle>
<pages>145--148</pages>
<marker>Izumi, 2003</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In Proc. of 41st Annual Meeting of ACL, pages 145–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kil S Jeong</author>
<author>Sung H Myaeng</author>
<author>Jae S Lee</author>
<author>KeySun Choi</author>
</authors>
<title>Automatic identification and backtransliteration of foreign words for information retrieval.</title>
<date>1999</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>35--523</pages>
<contexts>
<context position="5426" citStr="Jeong et al. (1999)" startWordPosition="854" endWordPosition="857">cktransliterate Japanese Katakana words into English via Japanese romanized equivalents. Transliteration and back-transliteration, however, are different tasks from ours. Transliteration and back-transliteration are a task where given English and Japanese Katakana words are put into their corresponding Japanese Katakana and English words, respectively, whereas our task is to recognize Roman words in English text written by learners of English. More related to our task is loanword identification; our task can be viewed as loanword identification where loanwords are Roman words in English text. Jeong et al. (1999) describe a method for distinguishing between foreign and pure Korean words in Korean text. Nwesri et al.(2006) propose a method for identifying foreign words in Arabic text. Khaltar et al. (2006) extract loanwords from Mongolian corpora using a Japanese loanword dictionary. These methods are fundamentally different from ours in the following two points. First, the target text in our task is full of spelling errors both in Roman and English words. Second, the above methods require annotated training data and/or other sources of knowledge such as a Japanese loanword dictionary that are hard to </context>
</contexts>
<marker>Jeong, Myaeng, Lee, Choi, 1999</marker>
<rawString>Kil S. Jeong, Sung H. Myaeng, Jae S. Lee, and KeySun Choi. 1999. Automatic identification and backtransliteration of foreign words for information retrieval. Information Processing and Management, 35:523–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Badam-Osor Khaltar</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Extracting loanwords from Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting ofACL,</booktitle>
<pages>657--664</pages>
<contexts>
<context position="5622" citStr="Khaltar et al. (2006)" startWordPosition="886" endWordPosition="890">k-transliteration are a task where given English and Japanese Katakana words are put into their corresponding Japanese Katakana and English words, respectively, whereas our task is to recognize Roman words in English text written by learners of English. More related to our task is loanword identification; our task can be viewed as loanword identification where loanwords are Roman words in English text. Jeong et al. (1999) describe a method for distinguishing between foreign and pure Korean words in Korean text. Nwesri et al.(2006) propose a method for identifying foreign words in Arabic text. Khaltar et al. (2006) extract loanwords from Mongolian corpora using a Japanese loanword dictionary. These methods are fundamentally different from ours in the following two points. First, the target text in our task is full of spelling errors both in Roman and English words. Second, the above methods require annotated training data and/or other sources of knowledge such as a Japanese loanword dictionary that are hard to obtain in our task. 3 Roman Words This section briefly introduces the spelling system of Roman words which is needed to understand the rest of this paper. For detailed discussion of Japanese-Engli</context>
</contexts>
<marker>Khaltar, Fujii, Ishikawa, 2006</marker>
<rawString>Badam-Osor Khaltar, Atsushi Fujii, and Tetsuya Ishikawa. 2006. Extracting loanwords from Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary. In Proc. of the 44th Annual Meeting ofACL, pages 657–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<pages>612</pages>
<contexts>
<context position="4741" citStr="Knight and Graehl (1998)" startWordPosition="752" endWordPosition="755">elated work. Section 3 introduces some knowledge of Roman words which is needed to understand the rest of this paper. Section 4 discusses our initial idea. Section 5 describes the method. Section 6 describes experiments conducted to evaluate the method and discusses the results. 2 Related Work Basically, no methods for recognizing Roman words have been proposed in the past. However, there have been a great deal of work related to Roman words. Transliteration and back-transliteration often involve romanization from Japanese Katakana words into their equivalents spelled in Roman alphabets as in Knight and Graehl (1998) and Brill et al. (2001). For example, Knight and Graehl (1998) backtransliterate Japanese Katakana words into English via Japanese romanized equivalents. Transliteration and back-transliteration, however, are different tasks from ours. Transliteration and back-transliteration are a task where given English and Japanese Katakana words are put into their corresponding Japanese Katakana and English words, respectively, whereas our task is to recognize Roman words in English text written by learners of English. More related to our task is loanword identification; our task can be viewed as loanwor</context>
<context position="6267" citStr="Knight and Graehl (1998)" startWordPosition="991" endWordPosition="994">om Mongolian corpora using a Japanese loanword dictionary. These methods are fundamentally different from ours in the following two points. First, the target text in our task is full of spelling errors both in Roman and English words. Second, the above methods require annotated training data and/or other sources of knowledge such as a Japanese loanword dictionary that are hard to obtain in our task. 3 Roman Words This section briefly introduces the spelling system of Roman words which is needed to understand the rest of this paper. For detailed discussion of Japanese-English romanization, see Knight and Graehl (1998). The spelling system has five vowels: a, i, u, e, o . It has 18 consonants : b, c, d, f, g, h, j, k, l, m, n, p, r, s, t, w, y, z . Note that some alphabets such as q and x are not used in Roman words. Roman words basically satisfy the following two rules: 1. Roman words end with either a vowel or n 2. A consonant is always followed by a vowel The first rule implies that one can tell that a word ending with a consonant except n is not a Roman word without looking at the whole word. There are two exceptions to the second rule. The first is that the consonant n sometimes behaves like a vowel an</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599– 612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Paul Rayson</author>
<author>Andrew Wilson</author>
</authors>
<title>Word Frequencies in Written and Spoken English: based on the British National Corpus.</title>
<date>2001</date>
<publisher>Longman.</publisher>
<contexts>
<context position="18735" citStr="Leech et al., 2001" startWordPosition="3165" endWordPosition="3168">rd year junior high students. The third was the combination of the two. Table 1 shows the target corpora statistics3. Evaluation was done on only unknown words in the target corpora since known Decision boundary R R E E + R R E E E E R + 31 Table 1: Target corpora statistics Corpus # sentences # words # diff. words # diff. unknown words # diff. Roman words Jr. high 2 9928 56724 1675 1040 275 Jr. high 3 10441 60546 2163 1334 500 Jr. high 2&amp;3 20369 117270 3299 2237 727 words can be easily recognized as English words by referring to an English word list. As an English word list, the 7,726 words (Leech et al., 2001) that occur at least 10 times per million words in the British National Corpus (Burnard, 1995) were combined with the English word list in Ispell, the spell checker. The whole list consisted of 19816 words. As already mentioned in Sect. 2, there has been no method for recognizing Roman words. Therefore, we set three baselines for comparison. In the first, all words that were not listed in the English word list were recognized as Roman words. In the second, -means clustering was used to recognize Roman words in the target corpora as described in Sect.4 (i.e., the initial idea). The -means clust</context>
</contexts>
<marker>Leech, Rayson, Wilson, 2001</marker>
<rawString>Geoffrey Leech, Paul Rayson, and Andrew Wilson. 2001. Word Frequencies in Written and Spoken English: based on the British National Corpus. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
</authors>
<title>Takahiro Wakana, Fumito Masui, Atsuo Kawai, and Naoki Isu.</title>
<date>2005</date>
<booktitle>In Proc. of 2nd International Joint Conference on Natural Language Processing,</booktitle>
<pages>815--826</pages>
<marker>Nagata, 2005</marker>
<rawString>Ryo Nagata, Takahiro Wakana, Fumito Masui, Atsuo Kawai, and Naoki Isu. 2005. Detecting article errors based on the mass count distinction. In Proc. of 2nd International Joint Conference on Natural Language Processing, pages 815–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Astuo Kawai</author>
<author>Koichiro Morihiro</author>
<author>Naoki Isu</author>
</authors>
<title>A feedback-augmented method for detecting errors in the writing of learners of English.</title>
<date>2006</date>
<booktitle>In Proc. of 44th Annual Meeting ofACL,</booktitle>
<pages>241--248</pages>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>Ryo Nagata, Astuo Kawai, Koichiro Morihiro, and Naoki Isu. 2006. A feedback-augmented method for detecting errors in the writing of learners of English. In Proc. of 44th Annual Meeting ofACL, pages 241–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdusalam F A Nwesri</author>
<author>Seyed M M Tahaghoghi</author>
<author>Falk Scholer</author>
</authors>
<title>Capturing out-of-vocabulary words in Arabic text.</title>
<date>2006</date>
<booktitle>In Proc. of 2006 Conference on EMNLP,</booktitle>
<pages>258--266</pages>
<marker>Nwesri, Tahaghoghi, Scholer, 2006</marker>
<rawString>Abdusalam F.A. Nwesri, Seyed M.M. Tahaghoghi, and Falk Scholer. 2006. Capturing out-of-vocabulary words in Arabic text. In Proc. of 2006 Conference on EMNLP, pages 258–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukio Tono</author>
</authors>
<title>A corpus-based analysis of interlanguage development: analysing POS tag sequences of EFL learner corpora.</title>
<date>2000</date>
<booktitle>In Practical Applications in Language Corpora,</booktitle>
<pages>123--132</pages>
<contexts>
<context position="2000" citStr="Tono, 2000" startWordPosition="297" endWordPosition="298">&apos;, IPPAI (many), and GANBARU (work hard). Approximately 20% of different words are Roman words in a corpus consisting of texts written by Japanese second and third year junior high students. Part of the reason is that they are lacking in English vocabulary, which leads them to using Roman words in English writing. Roman words become noise in a variety of tasks. In the field of second language acquisition, researchers often use a Part-Of-Speech (POS) tagger &apos;For consistency, we print Roman words in all capitals. to analyze learner corpora (Aarts and Granger, 1998; Granger, 1998; Granger, 1993; Tono, 2000). Since Roman words are romanized Japanese words and thus are unknown to POS taggers, they degrades the performance of POS taggers. In spell checking, they are a major source of false positives because they are unknown words as just mentioned. In error detection, most methods such as Chodorow and Leacock (2000), Izumi et al. (2003), Nagata et al. (2005; 2006), and Han et al. (2004; 2006) use a POS tagger and/or a chunker to detect errors. Again, Roman words degrades their performances. When viewed from another perspective, Roman words play an interesting role in second language acquisition. It</context>
</contexts>
<marker>Tono, 2000</marker>
<rawString>Yukio Tono. 2000. A corpus-based analysis of interlanguage development: analysing POS tag sequences of EFL learner corpora. In Practical Applications in Language Corpora, pages 123–132.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>