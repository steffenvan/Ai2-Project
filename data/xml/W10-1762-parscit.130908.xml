<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.549169">
Divide and Translate: Improving Long Distance Reordering in Statistical
Machine Translation
</title>
<author confidence="0.683514">
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki Nagata
</author>
<affiliation confidence="0.588107">
NTT Communication Science Laboratories
</affiliation>
<address confidence="0.87245">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
</address>
<email confidence="0.998385">
sudoh@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878043478261">
This paper proposes a novel method
for long distance, clause-level reordering
in statistical machine translation (SMT).
The proposed method separately translates
clauses in the source sentence and recon-
structs the target sentence using the clause
translations with non-terminals. The non-
terminals are placeholders of embedded
clauses, by which we reduce complicated
clause-level reordering into simple word-
level reordering. Its translation model
is trained using a bilingual corpus with
clause-level alignment, which can be au-
tomatically annotated by our alignment
algorithm with a syntactic parser in the
source language. We achieved signifi-
cant improvements of 1.4% in BLEU and
1.3% in TER by using Moses, and 2.2%
in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT, for
the English-to-Japanese translation of re-
search paper abstracts in the medical do-
main.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.953751754385965">
One of the common problems of statistical ma-
chine translation (SMT) is to overcome the differ-
ences in word order between the source and target
languages. This reordering problem is especially
serious for language pairs with very different word
orders, such as English-Japanese. Many previous
studies on SMT have addressed the problem by
incorporating probabilistic models into SMT re-
ordering. This approach faces the very large com-
putational cost of searching over many possibili-
ties, especially for long sentences. In practice the
search can be made tractable by limiting its re-
ordering distance, but this also renders long dis-
tance movements impossible. Some recent stud-
ies avoid the problem by reordering source words
prior to decoding. This approach faces difficul-
ties when the input phrases are long and require
significant word reordering, mainly because their
reordering model is not very accurate.
In this paper, we propose a novel method for
translating long sentences that is different from
the above approaches. Problematic long sentences
often include embedded clausesl such as rela-
tive clauses. Such an embedded (subordinate)
clause can usually be translated almost indepen-
dently of words outside the clause. From this
viewpoint, we propose a divide-and-conquer ap-
proach: we aim to translate the clauses sepa-
rately and reconstruct the target sentence using the
clause translations. We first segment a source sen-
tence into clauses using a syntactic parser. The
clauses can include non-terminals as placeholders
for nested clauses. Then we translate the clauses
with a standard SMT method, in which the non-
terminals are reordered as words. Finally we re-
construct the target sentence by replacing the non-
terminals with their corresponding clause transla-
tions. With this method, clause-level reordering is
reduced to word-level reordering and can be dealt
with efficiently. The models for clause translation
are trained using a bilingual corpus with clause-
level alignment. We also present an automatic
clause alignment algorithm that can be applied to
sentence-aligned bilingual corpora.
In our experiment on the English-to-Japanese
translation of multi-clause sentences, the proposed
method improved the translation performance by
1.4% in BLEU and 1.3% in TER by using Moses,
and by 2.2% in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT.
The main contribution of this paper is two-fold:
&apos;Although various definitions of a clause can be
considered, this paper follows the definition of ”S”
(sentence) in Enju. It basically follows the Penn Tree-
bank II scheme but also includes SINV, SQ, SBAR. See
http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-
output-spec.html#correspondence for details.
</bodyText>
<page confidence="0.818502">
418
</page>
<note confidence="0.9502385">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418–427,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.996191285714286">
1. We introduce the idea of explicit separa-
tion of in-clause and outside-clause reorder-
ing and reduction of outside-clause reorder-
ing into common word-level reordering.
2. We propose an automatic clause alignment
algorithm, by which our approach can be
used without manual clause-level alignment.
</listItem>
<bodyText confidence="0.9998305">
This paper is organized as follows. The next
section reviews related studies on reordering. Sec-
tion 3 describes the proposed method in detail.
Section 4 presents and discusses our experimen-
tal results. Finally, we conclude this paper with
our thoughts on future studies.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999661802325582">
Reordering in SMT can be roughly classified into
two approaches, namely a search in SMT decod-
ing and preprocessing.
The former approach is a straightforward way
that models reordering in noisy channel transla-
tion, and has been studied from the early period
of SMT research. Distance-based reordering is a
typical approach used in many previous studies re-
lated to word-based SMT (Brown et al., 1993) and
phrase-based SMT (Koehn et al., 2003). Along
with the advances in phrase-based SMT, lexical-
ized reordering with a block orientation model was
proposed (Tillmann, 2004; Koehn et al., 2005).
This kind of reordering is suitable and commonly
used in phrase-based SMT. On the other hand,
a syntax-based SMT naturally includes reorder-
ing in its translation model. A lot of research
work undertaken in this decade has used syntac-
tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,
2004; Galley et al., 2004; Liu et al., 2006). Wu
(1997) and Chiang (2007) focus on formal struc-
tures that can be extracted from parallel corpora,
instead of a syntactic parser trained using tree-
banks. These syntactic approaches can theoret-
ically model reordering over an arbitrary length,
however, long distance reordering still faces the
difficulty of searching over an extremely large
search space.
The preprocessing approach employs deter-
ministic reordering so that the following trans-
lation process requires only short distance re-
ordering (or even a monotone). Several previ-
ous studies have proposed syntax-driven reorder-
ing based on source-side parse trees. Xia and
McCord (2004) extracted reordering rules auto-
matically from bilingual corpora for English-to-
French translation; Collins et al. (2005) used
linguistically-motivated clause restructuring rules
for German-to-English translation; Li et al. (2007)
modeled reordering on parse tree nodes by us-
ing a maximum entropy model with surface and
syntactic features for Chinese-to-English trans-
lation; Katz-Brown and Collins (2008) applied
a very simple reverse ordering to Japanese-to-
English translation, which reversed the word order
in Japanese segments separated by a few simple
cues; Xu et al. (2009) utilized a dependency parser
with several hand-labeled precedence rules for re-
ordering English to subject-object-verb order like
Korean and Japanese. Tromble and Eisner (2009)
proposed another reordering approach based on a
linear ordering problem over source words with-
out a linguistically syntactic structure. These pre-
processing methods reorder source words close
to the target-side order by employing language-
dependent rules or statistical reordering models
based on automatic word alignment. Although
the use of language-dependent rules is a natural
and promising way of bridging gaps between lan-
guages with large syntactic differences, the rules
are usually unsuitable for other language groups.
On the other hand, statistical methods can be ap-
plied to any language pairs. However, it is very
difficult to reorder all source words so that they are
monotonic with the target words. This is because
automatic word alignment is not usually reliable
owing to data sparseness and the weak modeling
of many-to-many word alignments. Since such
a reordering is not complete or may even harm
word ordering consistency in the source language,
these previous methods further applied reordering
in their decoding. Li et al. (2007) used N-best
reordering hypotheses to overcome the reordering
ambiguity.
Our approach is different from those of previous
studies that aim to perform both short and long dis-
tance reordering at the same time. The proposed
method distinguishes the reordering of embedded
clauses from others and efficiently accomplishes it
by using a divide-and-conquer framework. The re-
maining (relatively short distance) reordering can
be realized in decoding and preprocessing by the
methods described above. The proposed frame-
work itself does not depend on a certain language
pair. It is based on the assumption that a source
</bodyText>
<page confidence="0.998583">
419
</page>
<bodyText confidence="0.99990859375">
language clause is translated to the corresponding
target language clause as a continuous segment.
The only language-dependent resource we need is
a syntactic parser of the source language. Note
that clause translation in the proposed method is a
standard MT problem and therefore any reordering
method can be employed for further improvement.
This work is inspired by syntax-based meth-
ods with respect to the use of non-terminals. Our
method can be seen as a variant of tree-to-string
translation that focuses only on the clause struc-
ture in parse trees and independently translates the
clauses. Although previous syntax-based methods
can theoretically model this kind of derivation, it
is practically difficult to decode long multi-clause
sentences as described above.
Our approach is also related to sentence sim-
plification and is intended to obtain simple and
short source sentences for better translation. Kim
and Ehara (1994) proposed a rule-based method
for splitting long Japanese sentences for Japanese-
to-English translation; Furuse et al. (1998) used
a syntactic structure to split ill-formed inputs in
speech translation. Their splitting approach splits
a sentence sequentially to obtain short segments,
and does not undertake their reordering.
Another related field is clause identification
(Tjong et al., 2001). The proposed method is not
limited to a specific clause identification method
and any method can be employed, if their clause
definition matches the proposed method where
clauses are independently translated.
</bodyText>
<sectionHeader confidence="0.99448" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.998015">
The proposed method consists of the following
steps illustrated in Figure 1.
</bodyText>
<listItem confidence="0.976997153846154">
During training:
1) clause segmentation of source sentences with
a syntactic parser (section 3.1)
2) alignment of target words with source clauses
to develop a clause-level aligned corpus (section
3.2)
3) training the clause translation models using
the corpus (section 3.3)
During testing:
1) clause translation with the clause translation
models (section 3.4)
2) sentence reconstruction based on non-
terminals (section 3.5)
</listItem>
<figureCaption confidence="0.995666">
Figure 1: Overview of proposed method.
</figureCaption>
<subsectionHeader confidence="0.999417">
3.1 Clause Segmentation of Source Sentences
</subsectionHeader>
<bodyText confidence="0.9975205625">
Clauses in source sentences are identified by a
syntactic parser. Figure 2 shows a parse tree for
the example sentence below. The example sen-
tence has a relative clause modifying the noun
book. Figure 3 shows the word alignment of this
example.
English: John lost the book that was borrowed
last week from Mary.
Japanese: john wa (topic marker) senshu (last
week) mary kara (from) kari (borrow) ta
(past tense marker) hon (book) o (direct ob-
ject marker) nakushi (lose) ta (past tense
marker) .
We segment the source sentence at the clause level
and the example is rewritten with two clauses as
follows.
</bodyText>
<listItem confidence="0.999668">
• John lost the book s0 .
• that was borrowed last week from Mary
</listItem>
<bodyText confidence="0.5409865">
s0 is a non-terminal symbol the serves as a place-
holder of the relative clause. We allow an arbitrary
</bodyText>
<figure confidence="0.99548737037037">
Bilingual
Corpus
(Training)
Original (sentence-aligned)
corpus can also be used
source
target
automatic clause alignment
LM training
word
alignment
Bilingual Corpus
(clause-aligned)
Source Sentences
(clause-segmented)
Target Word Bigram
Language Model
Word Alignment
Model
parse &amp; clause
segmentation
training from scratch
MERT
Bilingual
Corpus
(Development)
(clause-segmented)
Clause
Translation Models
(Phrase Table, N-gram LMs, ...)
Test Sentence
parse &amp;
clause
segmen-
tation
clause
clause
clause
clause
translation
clause
translation
clause
translation
sentence reconstruction
based on non-terminals
Sentence
Translation
translation
420
S
book S
that
was
</figure>
<figureCaption confidence="0.98696">
Figure 2: Parse tree for example English sentence.
Node labels are omitted except S.
</figureCaption>
<bodyText confidence="0.861221083333333">
John lost the book that was borrowed last week from Mary
john
wa
senshu
mary
kara
kari
ta
hon
o
nakushi
ta
</bodyText>
<figureCaption confidence="0.9446985">
Figure 3: Word alignment for example bilingual
sentence.
</figureCaption>
<bodyText confidence="0.999297">
number of non-terminals in each clause2. A nested
clause structure can be represented in the same
manner using such non-terminals recursively.
</bodyText>
<subsectionHeader confidence="0.9991835">
3.2 Alignment of Target Words with Source
Clauses
</subsectionHeader>
<bodyText confidence="0.999891545454545">
To translate source clauses with non-terminal sym-
bols, we need models trained using a clause-level
aligned bilingual corpus. A clause-level aligned
corpus is defined as a set of parallel, bilingual
clause pairs including non-terminals that represent
embedded clauses.
We assume that a sentence-aligned bilingual
corpus is available and consider the alignment of
target words with source clauses. We can manu-
ally align these Japanese words with the English
clauses as follows.
</bodyText>
<listItem confidence="0.981161">
• john wa s0 hon o nakushi ta .
</listItem>
<tableCaption confidence="0.513136333333333">
2In practice not so many clauses are embedded in a single
sentence but we found some examples with nine embedded
clauses for coordination in our corpora.
</tableCaption>
<bodyText confidence="0.849165">
John lost the book s0 .
</bodyText>
<listItem confidence="0.974558">
• senshu mary kara kari ta
</listItem>
<bodyText confidence="0.954879170212766">
that was borrowed last week from Mary
Since the cost of manual clause alignment is
high especially for a large-scale corpus, a natu-
ral question to ask is whether this resource can be
obtained from a sentence-aligned bilingual corpus
automatically with no human input. To answer
this, we now describe a simple method for deal-
ing with clause alignment data from scratch, us-
ing only the word alignment and language model
probabilities inferred from bilingual and monolin-
gual corpora.
Our method is based on the idea that automatic
clause alignment can be viewed as a classification
problem: for an English sentence with N words (e
_ (e1, e2, ... , eN)) and K clauses (˜e1,˜e2,. . . ,˜eK),
and its Japanese translation with M words (f
_ (f1, f2,. .. , fM)), the goal is to classify each
Japanese word into one of 11,. . . , K} classes. In-
tuitively, the probability that a Japanese word fm
is assigned to class k E 11, ... , K} depends on
two factors:
1. The probability of translating fm into the En-
glish words of clause k (i.e. Ee∈˜ek p(e|fm)).
We expect fm to be assigned to a clause
where this value is high.
2. The language model probability
(i.e. p(fm|fm_1)). If this value is high,
we expect fm and fm_1 to be assigned to the
same clause.
We implement this intuition using a graph-
based method. For each English-Japanese sen-
tence pair, we construct a graph with K clause
nodes (representing English clauses) and M word
nodes (representing Japanese words). The edge
weights between word and clause nodes are de-
fined as the sum of lexical translation probabilities
&amp;∈˜ek p(e|fm). The edge weights between words
are defined as the bigram probability p(fm|fm_1).
Each clause node is labeled with a class ID k E
11, ... , K}. We then propagate these K labels
along the graph to label the M word nodes. Fig-
ure 4 shows the graph for the example sentence.
Many label propagation algorithms are avail-
able. The important thing is to use an algo-
rithm that encourages node pairs with strong edge
weights to receive the same label. We use the label
propagation algorithm of (Zhu et al., 2003). If we
</bodyText>
<figure confidence="0.911195222222222">
John lost
the
from Mary
borrowed
last week
421
John lost the book that was borrowed ...
p(  |) p(  |) p(  |) p(  |)
wa john senshu wa mary senshu kara mary
</figure>
<figureCaption confidence="0.952938">
Figure 4: Graph-based representation of the ex-
ample sentence. We propagate the clause labels to
the Japanese word nodes on this graph to form the
clause alignments.
assume the labels are binary, the following objec-
tive is minimized:
</figureCaption>
<figure confidence="0.47517375">
�
argmin wij(li − lj)2 (1)
LERK+M
i,j
</figure>
<bodyText confidence="0.999766363636364">
where wij is the edge weight between nodes i
and j (1 &lt; i &lt; K + M, 1 &lt; j &lt; K +
M), and l (li E {0,1}) is a vector of labels
on the nodes. The first K elements of l, lc =
(l1, l2, ..., lK)T, are constant because the clause
nodes are pre-labeled. The remaining M ele-
ments, lf = (lK+1,lK+2, ..., lK+M)T, are un-
known and to be determined. Here, we consider
the decomposition of the weight matrix W = [wij]
into four blocks after the K-th row and column as
follows:
</bodyText>
<equation confidence="0.92673">
W = L Wcc W cf J (2)
L Wfc W ff
</equation>
<bodyText confidence="0.9751655">
The solution of eqn. (1), namely lf, is given by the
following equation:
</bodyText>
<equation confidence="0.971004">
lf = (Dff − Wff)−1Wfc lc (3)
</equation>
<bodyText confidence="0.998347888888889">
where D is the diagonal matrix with di = Ej wij
and is decomposed similarly to W. Each element
of lf is in the interval (0, 1) and can be regarded
as the label propagation probability. A detailed ex-
planation of this solution can be found in Section 2
of (Zhu et al., 2003). For our multi-label problem
with K labels, we slightly modified the algorithm
by expanding the vector l to an (M + K) x K
binary matrix L = [ l1 l2 ... lK ].
After the optimization, we can normalize Lf
to obtain the clause alignment scores t(lm =
k|fm) between each Japanese word fm and En-
glish clause k. Theoretically, we can simply out-
put the clause id k′ for each fm by finding k′ =
arg maxk t(lm = k|fm). In practice, this may
sometimes lead to Japanese clauses that have too
many gaps, so we employ a two-stage procedure
to extract clauses that are more contiguous.
First, we segment the Japanese sentence into K
clauses based on a dynamic programming algo-
rithm proposed by Malioutov and Barzilay (2006).
We define an M x M similarity matrix S = [sij]
with sij = exp(−||li−lj||) where li is (K + i)-th
row vector in the label matrix L. sij represents
the similarity between the i-th and j-th Japanese
words with respect to their clause alignment score
distributions; if the score distributions are sim-
ilar then sij is large. The details of this algo-
rithm can be found in (Malioutov and Barzilay,
2006). The clause segmentation gives us contigu-
ous Japanese clauses ˜f1, ˜f2, ..., ˜fK, thus min-
imizing inter-segment similarity and maximizing
intra-segment similarity. Second, we determine
the clause labels of the segmented clauses, based
on clause alignment scores T = [Tkk′] for English
and automatically-segmented Japanese clauses:
</bodyText>
<equation confidence="0.9972235">
Tkk′ = � t(lm = k|fm) (4)
fmE ˜fk′
</equation>
<bodyText confidence="0.999983666666667">
where ˜fk′ is the j′-th Japanese clause. In descend-
ing order of the clause alignment score, we greed-
ily determine the clause label 3.
</bodyText>
<subsectionHeader confidence="0.995507">
3.3 Training Clause Translation Models
</subsectionHeader>
<bodyText confidence="0.999992166666667">
We train clause translation models using the
clause-level aligned corpus. In addition we can
also include the original sentence-aligned corpus.
We emphasize that we can use standard techniques
for heuristically extracted phrase tables, word n-
gram language models, and so on.
</bodyText>
<subsectionHeader confidence="0.981152">
3.4 Clause Translation
</subsectionHeader>
<bodyText confidence="0.99979275">
By using the source language parser, a multi-
clause source sentence is reduced to a set of
clauses. We translate these clauses with a common
SMT method using the clause translation models.
Here we present another English example I
bought the magazine which Tom recommended
yesterday. This sentence is segmented into clauses
as follows.
</bodyText>
<footnote confidence="0.9872855">
3Although a full search is available when the number of
clauses is small, we employ a greedy search in this paper.
</footnote>
<equation confidence="0.785040714285714">
p(that  |)
kara
+ p(was  |)
kara
+ ...
p(John  |)
john
+ p(lost  |)
john
+ ...
clause(1) clause(2)
john wa senshu mary kara
topic
John marker last week Mary from
</equation>
<page confidence="0.989882">
422
</page>
<listItem confidence="0.998582">
• I bought the magazine s0 .
• which Tom recommended yersterday
These clauses are translated into Japanese:
• watashi (I) wa (topic marker) s0
zasshi (magazine) o (direct object marker)
kat (buy) ta (past tense marker).
• tom ga (subject marker) kino (yesterday)
susume (recommend) ta (past tense marker)
</listItem>
<subsectionHeader confidence="0.992234">
3.5 Sentence Reconstruction
</subsectionHeader>
<bodyText confidence="0.991595666666667">
We reconstruct the target sentence from the clause
translations, based on non-terminals. Starting
from the clause translation of the top clause, we re-
cursively replace non-terminal symbols with their
corresponding clause translations. Here, if a non-
terminal is eventually deleted in SMT decoding,
we simply concatenate the translation behind its
parent clause.
Using the example above, we replace the non-
terminal symbol s0 with the second clause and
obtain the Japanese sentence:
watashi wa tom ga kino susume ta zasshi o kat ta .
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999913111111111">
We conducted the following experiments on the
English-to-Japanese translation of research paper
abstracts in the medical domain. Such techni-
cal documents are logically and formally writ-
ten, and sentences are often so long and syntac-
tically complex that their translation needs long
distance reordering. We believe that the medical
domain is suitable as regards evaluating the pro-
posed method.
</bodyText>
<subsectionHeader confidence="0.97098">
4.1 Resources
</subsectionHeader>
<bodyText confidence="0.9999617">
Our bilingual resources were taken from the med-
ical domain. The parallel corpus consisted of
research paper abstracts in English taken from
PubMed4 and the corresponding Japanese transla-
tions.
The training portion consisted of 25,500 sen-
tences (no-clause-seg.; original sentences with-
out clause segmentation). 4,132 English sen-
tences in the corpus were composed of multi-
ple clauses and were separated at the clause level
</bodyText>
<footnote confidence="0.81414">
4http://www.ncbi.nlm.nih.gov/pubmed/
</footnote>
<bodyText confidence="0.9999303125">
by the procedure in section 3.1. As the syntac-
tic parser, we used the Enju5 (Miyao and Tsu-
jii, 2008) English HPSG parser. For these train-
ing sentences, we automatically aligned Japanese
words with each English clause as described in
section 3.2 and developed a clause-level aligned
corpus, called auto-aligned corpus. We prepared
manually-aligned (oracle) clauses for reference,
called oracle-aligned clauses. The clause align-
ment error rate of the auto-aligned corpus was
14% (number of wrong clause assignments di-
vided by total number of words). The develop-
ment and test portions each consisted of 1,032
multi-clause sentences. because this paper focuses
only on multi-clause sentences. Their English-
side was segmented into clauses in the same man-
ner as the training sentences, and the development
sentences had oracle clause alignment for MERT.
We also used the Life Science Dictionary6 for
training. We extracted 100,606 unique English
entries from the dictionary including entries with
multiple translation options, which we expanded
to one-to-one entries, and finally we obtained
155,692 entries.
English-side tokenization was obtained using
Enju, and we applied a simple preprocessing that
removed articles (a, an, the) and normalized plu-
ral forms to singular ones. Japanese-side tokeniza-
tion was obtained using MeCab7 with ComeJisyo8
(dictionary for Japanese medical document tok-
enization). Our resource statistics are summarized
in Table 1.
</bodyText>
<subsectionHeader confidence="0.996804">
4.2 Model and Decoder
</subsectionHeader>
<bodyText confidence="0.9998074">
We used two decoders in the experiments,
Moses9 (Koehn et al., 2007) and our in-
house hierarchical phrase-based SMT (almost
equivalent to Hiero (Chiang, 2007)). Moses
used a phrase table with a maximum phrase
length of 7, a lexicalized reordering model with
msd-bidirectional-fe, and a distortion
limit of 1210. Our hierarchical phrase-based SMT
used a phrase table with a maximum rule length of
7 and a window size (Hiero’s Λ) of 12 11. Both
</bodyText>
<footnote confidence="0.999575444444444">
5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
6http://lsd.pharm.kyoto-u.ac.jp/en/index.html
7http://mecab.sourceforge.net/
8http://sourceforge.jp/projects/comedic/ (in Japanese)
9http://www.statmt.org/moses/
10Unlimited distortion was also tested but the results were
worse.
11A larger window size could not be used due to its mem-
ory requirements.
</footnote>
<page confidence="0.999139">
423
</page>
<figure confidence="0.986480260869565">
Training
Corpus Type #words #sentences
Parallel E 690,536 25,550
(no-clause-seg.)
J 942,913
Parallel E 135,698 4,132
(auto-aligned) (10,766 clauses)
(oracle-aligned)
J 183,043
J 183,147
Dictionary E 263,175 155.692
(entries)
J 291,455
Development
Corpus Type #words #sentences
Parallel E 34,417 1,032
(oracle-aligned) (2,683 clauses)
J 46,480
Test
Corpus Type #words #sentences
Parallel E 34,433 1,032
(clause-seg.) (2,737 clauses)
J 45,975
</figure>
<tableCaption confidence="0.98521">
Table 1: Data statistics on training, development,
</tableCaption>
<bodyText confidence="0.920765444444445">
and test sets. All development and test sentences
are multi-clause sentences.
decoders employed two language models: a word
5-gram language model from the Japanese sen-
tences in the parallel corpus and a word 4-gram
language model from the Japanese entries in the
dictionary. The feature weights were optimized
for BLEU (Papineni et al., 2002) by MERT, using
the development sentences.
</bodyText>
<subsectionHeader confidence="0.981929">
4.3 Compared Methods
</subsectionHeader>
<bodyText confidence="0.999654595238095">
We compared four different training and test con-
ditions with respect to the use of clauses in training
and testing. The development (i.e., MERT) condi-
tions followed the test conditions. Two additional
conditions with oracle clause alignment were also
tested for reference.
Table 2 lists the compared methods. First,
the proposed method (proposed) used the auto-
aligned corpus in training and clause segmen-
tation in testing. Second, the baseline method
(baseline) did not use clause segmentation in ei-
ther training or testing. Using this standard base-
line method, we focused on the advantages of the
divide-and-conquer translation itself. Third, we
tested the same translation models as used with
the proposed method for test sentences without
clause segmentation, (comp.(1)). Although this
comparison method cannot employ the proposed
clause-level reordering, it was expected to be bet-
ter than the baseline method because its transla-
tion model can be trained more precisely using the
finely aligned clause-level corpus. Finally, the sec-
ond comparison method (comp.(2)) translated seg-
mented clauses with the baseline (without clause
segmentation) model, as if each of them was a sin-
gle sentence. Its translation of each clause was
expected to be better than that of the baseline be-
cause of the efficient search over shortened inputs,
while its reordering of clauses (non-terminals) was
unreliable due to the lack of clause information
in training. Its sentence reconstruction based on
non-terminals was the same as with the proposed
method. Although non-terminals in the second
comparison method were out-of-vocabulary words
and may be deleted in decoding, all of them sur-
vived and we could reconstruct sentences from
translated clauses throughout the experiments. In
addition, two other conditions were tested: us-
ing oracle-aligned clauses in training: the pro-
posed method trained using oracle-aligned (ora-
cle) clauses and the first comparison method using
oracle-aligned (oracle-comp.) clauses.
</bodyText>
<subsectionHeader confidence="0.923027">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999439269230769">
Table 3 shows the results in BLEU, Transla-
tion Edit Rate (TER) (Snover et al., 2006),
and Position-independent Word-error Rate (PER)
(Och et al., 2001), obtained with Moses and our
hierarchical phrase-based SMT, respectively. Bold
face results indicate the best scores obtained with
the compared methods (excluding oracles).
The proposed method consistently outper-
formed the baseline. The BLEU improve-
ments with the proposed method over the base-
line and comparison methods were statistically
significant according to the bootstrap sampling
test (p &lt; 0.05, 1,000 samples) (Zhang et al.,
2004). With Moses, the improvement when us-
ing the proposed method was 1.4% (33.19% to
34.60%) in BLEU and 1.3% (57.83% to 56.50%)
in TER, with a slight improvement in PER
(35.84% to 35.61%). We observed: oracle »
proposed» comp.(1)» baseline» comp.(2)
by the Bonferroni method, where the symbol
A » B means “A’s improvement over B is
statistically significant.” With the hierarchical
phrase-based SMT, the improvement was 2.2%
(32.39% to 34.55%) in BLEU, 3.5% (58.36% to
54.87%) in TER, and 1.5% in PER (36.42% to
34.79%). We observed: oracle » proposed »
</bodyText>
<page confidence="0.999314">
424
</page>
<tableCaption confidence="0.998886">
Table 2: Compared methods.
</tableCaption>
<table confidence="0.9852128">
Training w/ auto-aligned w/o aligned w/ oracle-aligned
��������
Test
clause-seg. proposed comp.(2) oracle
no-clause-seg. comp.(1) baseline oracle-comp.
</table>
<bodyText confidence="0.93513125">
{comp.(1), comp.(2)} ≫ baseline by the Bon-
ferroni method. The oracle results were better than
these obtained with the proposed method but the
differences were not very large.
</bodyText>
<subsectionHeader confidence="0.958418">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.997155042857143">
We think the advantage of the proposed method
arises from three possibilities: 1) better translation
model training using the fine-aligned corpus, 2) an
efficient decoder search over shortened inputs, and
3) an effective clause-level reordering model real-
ized by using non-terminals.
First, the results of the first comparison method
(comp.(1)) indicate an advantage of the transla-
tion models trained using the auto-aligned corpus.
The training of the translation models, namely
word alignment and phrase extraction, is difficult
for long sentences due to their large ambiguity.
This result suggests that the use of clause-level
alignment provides fine-grained word alignments
and precise translation models. We can also ex-
pect that the model of the proposed method will
work better for the translation of single-clause sen-
tences.
Second, the average and median lengths (in-
cluding non-terminals) of the clause-seg. test set
were 13.2 and 10 words, respectively. They were
much smaller than those of no-clause-seg. at 33.4
and 30 words and are expected to help realize
an efficient SMT search. Another observation is
the relationship between the number of clauses
and translation performance, as shown in Fig-
ure 5. The proposed method achieved a greater im-
provement in sentences with a greater number of
clauses. This suggests that our divide-and-conquer
approach works effectively for multi-clause sen-
tences. Here, the results of the second comparison
method (comp.(2)) with Moses were worse than
the baseline results, while there was an improve-
ment with our hierarchical phrase-based SMT.
This probably arose from the difference between
the decoders when translating out-of-vocabulary
words. The non-terminals were handled as out-of-
vocabulary words under the comp.(2) condition.
Figure 5: Relationship between TER and number
of clauses for proposed, baseline, and comp.(2)
when using our hierarchical phrase-based SMT.
Moses generated erroneous translations around
such non-terminals that can be identified at a
glance, while our hierarchical phrase-based SMT
generated relatively good translations. This may
be a decoder-dependent issue and is not an essen-
tial problem.
Third, the results obtained with the proposed
method reveal an advantage in reordering in ad-
dition to the previous two advantages. The differ-
ence between the PERs with the proposed method
and the baseline with Moses was small (0.2%)
in spite of the large differences in BLEU and
TER (about 1.5%). This suggests that the pro-
posed method is better in word ordering and im-
plies our method is also effective in reordering.
With the hierarchical phrase-based SMT, the pro-
posed method showed a large improvement from
the baseline and comparison methods, especially
in TER which was better than the best Moses
configuration (proposed). This suggests that the
decoding of long sentences with long-distance
reordering is not easy even for the hierarchical
phrase-based SMT due to its limited window size,
while the hierarchical framework itself can natu-
rally model a long-distance reordering. If we try to
find a derivation with such long-distance reorder-
ing, we will probably be faced with an intractable
search space and computation time. Therefore,
we can conclude that the proposed divide-and-
</bodyText>
<figure confidence="0.987969142857143">
66
64
62
TER (%)
60
58
56
54
baseline
comp.(2)
proposed
2 3 4 5
The number of clauses
52
</figure>
<page confidence="0.99812">
425
</page>
<tableCaption confidence="0.9923275">
Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,
TER, and PER.
</tableCaption>
<table confidence="0.98118425">
Moses: BLEU (%) / TER (%) / PER (%)
Training w/ auto-aligned w/o aligned w/ oracle-aligned
��������
Test
clause-seg. 34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42
no-clause-seg. 34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03
Hierarchical: BLEU (%) / TER (%) / PER (%)
Training w/ auto-aligned w/o aligned w/ oracle-aligned
��������
Test
clause-seg. 34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77
no-clause-seg. 33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96
</table>
<bodyText confidence="0.99989744">
conquer approach provides more practical long-
distance reordering at the clause level.
We also analyzed the difference between auto-
matic and manual clause alignment. Since auto-
aligned corpus had many obvious alignment er-
rors, we suspected these noisy clauses hurt the
clause translation model. However, they were not
serious in terms of final translation performance.
So we can conclude that our proposed divide-and-
conquer approach is promising for long sentence
translation. Although we aimed to see whether we
could bootstrap using existing bilingual corpora in
this paper, we imagine better clause alignment can
be obtained with some supervised classifiers.
One problem with the divide-and-conquer ap-
proach is that its independently-translated clauses
potentially cause disfluencies in final sentence
translations, mainly due to wrong inflections. A
promising solution is to optimize a whole sentence
translation by integrating search of each clause
translation but this may require a much larger
search space for decoding. More simply, we may
be able to approximate it using n-best clause trans-
lations. This problem should be addressed for fur-
ther improvement in future studies.
</bodyText>
<sectionHeader confidence="0.999307" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999584115384615">
In this paper we proposed a clause-based divide-
and-conquer approach for SMT that can re-
duce complicated clause-level reordering to sim-
ple word-level reordering. The proposed method
separately translates clauses with non-terminals by
using a well-known SMT method and reconstructs
a sentence based on the non-terminals, to reorder
long clauses. The clause translation models are
trained using a bilingual corpus with clause-level
alignment, which can be obtained with an un-
supervised graph-based method using sentence-
aligned corpora. The proposed method improves
the translation of long, multi-clause sentences and
is especially effective for language pairs with
large word order differences, such as English-to-
Japanese.
This paper focused only on clauses as segments
for division. However, other long segments such
as prepositional phrases are similarly difficult to
reorder correctly. The divide-and-conquer ap-
proach itself can be applied to long phrases, and
it is worth pursuing such an extension. As another
future direction, we must develop a more sophis-
ticated method for automatic clause alignment if
we are to use the proposed method for various lan-
guage pairs and domains.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999644">
We thank the U. S. National Library of Medicine
for the use of PubMed abstracts and Prof. Shuji
Kaneko of Kyoto University for the use of Life
Science Dictionary. We also thank the anonymous
reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999064">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Kuˆcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531–540.
</reference>
<page confidence="0.988596">
426
</page>
<reference confidence="0.999929543478261">
Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-
mamoto. 1998. Splitting long or ill-formed in-
put for robust spoken-language translation. In Proc.
COLING-ACL, pages 421–427.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. NAACL, pages 273–280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT-NAACL, pages 105–
112.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for Japanese-English
translation: MIT system description for NTCIR-7
patent translation task. In Proc. NTCIR-7, pages
409–414.
Yeun-Bae Kim and Terumasa Ehara. 1994. A method
for partitioning of long Japanese sentences with sub-
ject resolution in J/E machine translation. In Proc.
International Conference on Computer Processing
of Oriental Languages, pages 467–473.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 263–270.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177–180.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. ACL, pages 720–727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String alignment template for statistical machine
translation. In Proc. Coling-ACL, pages 609–616.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Coling-ACL, pages 25–32.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statis-
tical machine translation. In Proc. the ACL Work-
shop on Data-Driven Methods in Machine Transla-
tion, pages 55–62.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311–318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223–231.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proc.
HLT-NAACL, pages 101–104.
Erik F. Tjong, Kim Sang, and Herv´e D´ejean. 2001. In-
troduction to the CoNLL-2001 shared task: Clause
identification. In Proc. CoNLL, pages 53–57.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007–1016.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING, pages 508–514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
HLT-NAACL, pages 245–253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523–530.
Ying Zhang, Stephan Vogel, and Alex Weibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proc. LREC, pages 2051–2054.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proc. ICML, pages
912–919.
</reference>
<page confidence="0.998555">
427
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854487">
<title confidence="0.9991525">Divide and Translate: Improving Long Distance Reordering in Statistical Machine Translation</title>
<author confidence="0.993922">Katsuhito Sudoh</author>
<author confidence="0.993922">Kevin Duh</author>
<author confidence="0.993922">Hajime Tsukada</author>
<author confidence="0.993922">Tsutomu Hirao</author>
<author confidence="0.993922">Masaaki</author>
<affiliation confidence="0.999155">NTT Communication Science</affiliation>
<address confidence="0.983121">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237,</address>
<email confidence="0.987202">sudoh@cslab.kecl.ntt.co.jp</email>
<abstract confidence="0.995262708333333">This paper proposes a novel method for long distance, clause-level reordering in statistical machine translation (SMT). The proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals. The nonterminals are placeholders of embedded clauses, by which we reduce complicated clause-level reordering into simple wordlevel reordering. Its translation model is trained using a bilingual corpus with clause-level alignment, which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language. We achieved significant improvements of 1.4% in BLEU and 1.3% in TER by using Moses, and 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT, for the English-to-Japanese translation of research paper abstracts in the medical domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5092" citStr="Brown et al., 1993" startWordPosition="756" endWordPosition="759">ted studies on reordering. Section 3 describes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classified into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5691" citStr="Chiang (2007)" startWordPosition="853" endWordPosition="854"> et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and Mc</context>
<context position="22747" citStr="Chiang, 2007" startWordPosition="3624" endWordPosition="3625">ons, which we expanded to one-to-one entries, and finally we obtained 155,692 entries. English-side tokenization was obtained using Enju, and we applied a simple preprocessing that removed articles (a, an, the) and normalized plural forms to singular ones. Japanese-side tokenization was obtained using MeCab7 with ComeJisyo8 (dictionary for Japanese medical document tokenization). Our resource statistics are summarized in Table 1. 4.2 Model and Decoder We used two decoders in the experiments, Moses9 (Koehn et al., 2007) and our inhouse hierarchical phrase-based SMT (almost equivalent to Hiero (Chiang, 2007)). Moses used a phrase table with a maximum phrase length of 7, a lexicalized reordering model with msd-bidirectional-fe, and a distortion limit of 1210. Our hierarchical phrase-based SMT used a phrase table with a maximum rule length of 7 and a window size (Hiero’s Λ) of 12 11. Both 5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 6http://lsd.pharm.kyoto-u.ac.jp/en/index.html 7http://mecab.sourceforge.net/ 8http://sourceforge.jp/projects/comedic/ (in Japanese) 9http://www.statmt.org/moses/ 10Unlimited distortion was also tested but the results were worse. 11A larger window size could not</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˆcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˆcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˆcerov´a. 2005. Clause restructuring for statistical machine translation. In Proc. ACL, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osamu Furuse</author>
<author>Setsuo Yamada</author>
<author>Kazuhide Yamamoto</author>
</authors>
<title>Splitting long or ill-formed input for robust spoken-language translation.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>421--427</pages>
<contexts>
<context position="9774" citStr="Furuse et al. (1998)" startWordPosition="1465" endWordPosition="1468"> method can be seen as a variant of tree-to-string translation that focuses only on the clause structure in parse trees and independently translates the clauses. Although previous syntax-based methods can theoretically model this kind of derivation, it is practically difficult to decode long multi-clause sentences as described above. Our approach is also related to sentence simplification and is intended to obtain simple and short source sentences for better translation. Kim and Ehara (1994) proposed a rule-based method for splitting long Japanese sentences for Japaneseto-English translation; Furuse et al. (1998) used a syntactic structure to split ill-formed inputs in speech translation. Their splitting approach splits a sentence sequentially to obtain short segments, and does not undertake their reordering. Another related field is clause identification (Tjong et al., 2001). The proposed method is not limited to a specific clause identification method and any method can be employed, if their clause definition matches the proposed method where clauses are independently translated. 3 Proposed Method The proposed method consists of the following steps illustrated in Figure 1. During training: 1) clause</context>
</contexts>
<marker>Furuse, Yamada, Yamamoto, 1998</marker>
<rawString>Osamu Furuse, Setsuo Yamada, and Kazuhide Yamamoto. 1998. Splitting long or ill-formed input for robust spoken-language translation. In Proc. COLING-ACL, pages 421–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>Proc. NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="5643" citStr="Galley et al., 2004" startWordPosition="842" endWordPosition="845">many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reorder</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="5622" citStr="Graehl and Knight, 2004" startWordPosition="838" endWordPosition="841">typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed </context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. HLT-NAACL, pages 105– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Katz-Brown</author>
<author>Michael Collins</author>
</authors>
<title>Syntactic reordering in preprocessing for Japanese-English translation: MIT system description for NTCIR-7 patent translation task.</title>
<date>2008</date>
<booktitle>In Proc. NTCIR-7,</booktitle>
<pages>409--414</pages>
<contexts>
<context position="6705" citStr="Katz-Brown and Collins (2008)" startWordPosition="996" endWordPosition="999">ng so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent ru</context>
</contexts>
<marker>Katz-Brown, Collins, 2008</marker>
<rawString>Jason Katz-Brown and Michael Collins. 2008. Syntactic reordering in preprocessing for Japanese-English translation: MIT system description for NTCIR-7 patent translation task. In Proc. NTCIR-7, pages 409–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeun-Bae Kim</author>
<author>Terumasa Ehara</author>
</authors>
<title>A method for partitioning of long Japanese sentences with subject resolution in J/E machine translation.</title>
<date>1994</date>
<booktitle>In Proc. International Conference on Computer Processing of Oriental Languages,</booktitle>
<pages>467--473</pages>
<contexts>
<context position="9650" citStr="Kim and Ehara (1994)" startWordPosition="1448" endWordPosition="1451">mployed for further improvement. This work is inspired by syntax-based methods with respect to the use of non-terminals. Our method can be seen as a variant of tree-to-string translation that focuses only on the clause structure in parse trees and independently translates the clauses. Although previous syntax-based methods can theoretically model this kind of derivation, it is practically difficult to decode long multi-clause sentences as described above. Our approach is also related to sentence simplification and is intended to obtain simple and short source sentences for better translation. Kim and Ehara (1994) proposed a rule-based method for splitting long Japanese sentences for Japaneseto-English translation; Furuse et al. (1998) used a syntactic structure to split ill-formed inputs in speech translation. Their splitting approach splits a sentence sequentially to obtain short segments, and does not undertake their reordering. Another related field is clause identification (Tjong et al., 2001). The proposed method is not limited to a specific clause identification method and any method can be employed, if their clause definition matches the proposed method where clauses are independently translate</context>
</contexts>
<marker>Kim, Ehara, 1994</marker>
<rawString>Yeun-Bae Kim and Terumasa Ehara. 1994. A method for partitioning of long Japanese sentences with subject resolution in J/E machine translation. In Proc. International Conference on Computer Processing of Oriental Languages, pages 467–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="5134" citStr="Koehn et al., 2003" startWordPosition="763" endWordPosition="766">ibes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classified into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be ext</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. IWSLT.</booktitle>
<contexts>
<context position="5284" citStr="Koehn et al., 2005" startWordPosition="786" endWordPosition="789">ture studies. 2 Related Work Reordering in SMT can be roughly classified into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering ove</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="22658" citStr="Koehn et al., 2007" startWordPosition="3609" endWordPosition="3612">606 unique English entries from the dictionary including entries with multiple translation options, which we expanded to one-to-one entries, and finally we obtained 155,692 entries. English-side tokenization was obtained using Enju, and we applied a simple preprocessing that removed articles (a, an, the) and normalized plural forms to singular ones. Japanese-side tokenization was obtained using MeCab7 with ComeJisyo8 (dictionary for Japanese medical document tokenization). Our resource statistics are summarized in Table 1. 4.2 Model and Decoder We used two decoders in the experiments, Moses9 (Koehn et al., 2007) and our inhouse hierarchical phrase-based SMT (almost equivalent to Hiero (Chiang, 2007)). Moses used a phrase table with a maximum phrase length of 7, a lexicalized reordering model with msd-bidirectional-fe, and a distortion limit of 1210. Our hierarchical phrase-based SMT used a phrase table with a maximum rule length of 7 and a window size (Hiero’s Λ) of 12 11. Both 5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 6http://lsd.pharm.kyoto-u.ac.jp/en/index.html 7http://mecab.sourceforge.net/ 8http://sourceforge.jp/projects/comedic/ (in Japanese) 9http://www.statmt.org/moses/ 10Unlimite</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Minghui Li</author>
<author>Yi Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>720--727</pages>
<contexts>
<context position="6531" citStr="Li et al. (2007)" startWordPosition="970" endWordPosition="973"> long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem ove</context>
<context position="8104" citStr="Li et al. (2007)" startWordPosition="1210" endWordPosition="1213">with large syntactic differences, the rules are usually unsuitable for other language groups. On the other hand, statistical methods can be applied to any language pairs. However, it is very difficult to reorder all source words so that they are monotonic with the target words. This is because automatic word alignment is not usually reliable owing to data sparseness and the weak modeling of many-to-many word alignments. Since such a reordering is not complete or may even harm word ordering consistency in the source language, these previous methods further applied reordering in their decoding. Li et al. (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. Our approach is different from those of previous studies that aim to perform both short and long distance reordering at the same time. The proposed method distinguishes the reordering of embedded clauses from others and efficiently accomplishes it by using a divide-and-conquer framework. The remaining (relatively short distance) reordering can be realized in decoding and preprocessing by the methods described above. The proposed framework itself does not depend on a certain language pair. It is based on the assumption tha</context>
</contexts>
<marker>Li, Zhang, Li, Zhou, Li, Guan, 2007</marker>
<rawString>Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proc. ACL, pages 720–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-String alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. Coling-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="5662" citStr="Liu et al., 2006" startWordPosition="846" endWordPosition="849"> related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-String alignment template for statistical machine translation. In Proc. Coling-ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proc. Coling-ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="17528" citStr="Malioutov and Barzilay (2006)" startWordPosition="2803" endWordPosition="2806">y expanding the vector l to an (M + K) x K binary matrix L = [ l1 l2 ... lK ]. After the optimization, we can normalize Lf to obtain the clause alignment scores t(lm = k|fm) between each Japanese word fm and English clause k. Theoretically, we can simply output the clause id k′ for each fm by finding k′ = arg maxk t(lm = k|fm). In practice, this may sometimes lead to Japanese clauses that have too many gaps, so we employ a two-stage procedure to extract clauses that are more contiguous. First, we segment the Japanese sentence into K clauses based on a dynamic programming algorithm proposed by Malioutov and Barzilay (2006). We define an M x M similarity matrix S = [sij] with sij = exp(−||li−lj||) where li is (K + i)-th row vector in the label matrix L. sij represents the similarity between the i-th and j-th Japanese words with respect to their clause alignment score distributions; if the score distributions are similar then sij is large. The details of this algorithm can be found in (Malioutov and Barzilay, 2006). The clause segmentation gives us contiguous Japanese clauses ˜f1, ˜f2, ..., ˜fK, thus minimizing inter-segment similarity and maximizing intra-segment similarity. Second, we determine the clause label</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proc. Coling-ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="21218" citStr="Miyao and Tsujii, 2008" startWordPosition="3395" endWordPosition="3399">uitable as regards evaluating the proposed method. 4.1 Resources Our bilingual resources were taken from the medical domain. The parallel corpus consisted of research paper abstracts in English taken from PubMed4 and the corresponding Japanese translations. The training portion consisted of 25,500 sentences (no-clause-seg.; original sentences without clause segmentation). 4,132 English sentences in the corpus were composed of multiple clauses and were separated at the clause level 4http://www.ncbi.nlm.nih.gov/pubmed/ by the procedure in section 3.1. As the syntactic parser, we used the Enju5 (Miyao and Tsujii, 2008) English HPSG parser. For these training sentences, we automatically aligned Japanese words with each English clause as described in section 3.2 and developed a clause-level aligned corpus, called auto-aligned corpus. We prepared manually-aligned (oracle) clauses for reference, called oracle-aligned clauses. The clause alignment error rate of the auto-aligned corpus was 14% (number of wrong clause assignments divided by total number of words). The development and test portions each consisted of 1,032 multi-clause sentences. because this paper focuses only on multi-clause sentences. Their Engli</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient A* search algorithm for statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proc. the ACL Workshop on Data-Driven Methods in Machine Translation,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="26448" citStr="Och et al., 2001" startWordPosition="4161" endWordPosition="4164">though non-terminals in the second comparison method were out-of-vocabulary words and may be deleted in decoding, all of them survived and we could reconstruct sentences from translated clauses throughout the experiments. In addition, two other conditions were tested: using oracle-aligned clauses in training: the proposed method trained using oracle-aligned (oracle) clauses and the first comparison method using oracle-aligned (oracle-comp.) clauses. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically significant according to the bootstrap sampling test (p &lt; 0.05, 1,000 samples) (Zhang et al., 2004). With Moses, the improvement when using the proposed method was 1.4% (33.19% to 34.60%) in BLEU and 1.3% (57.83% to 56.50%) in TER, with a slight improvement i</context>
</contexts>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz Josef Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for statistical machine translation. In Proc. the ACL Workshop on Data-Driven Methods in Machine Translation, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="24226" citStr="Papineni et al., 2002" startWordPosition="3823" endWordPosition="3826">92 (entries) J 291,455 Development Corpus Type #words #sentences Parallel E 34,417 1,032 (oracle-aligned) (2,683 clauses) J 46,480 Test Corpus Type #words #sentences Parallel E 34,433 1,032 (clause-seg.) (2,737 clauses) J 45,975 Table 1: Data statistics on training, development, and test sets. All development and test sentences are multi-clause sentences. decoders employed two language models: a word 5-gram language model from the Japanese sentences in the parallel corpus and a word 4-gram language model from the Japanese entries in the dictionary. The feature weights were optimized for BLEU (Papineni et al., 2002) by MERT, using the development sentences. 4.3 Compared Methods We compared four different training and test conditions with respect to the use of clauses in training and testing. The development (i.e., MERT) conditions followed the test conditions. Two additional conditions with oracle clause alignment were also tested for reference. Table 2 lists the compared methods. First, the proposed method (proposed) used the autoaligned corpus in training and clause segmentation in testing. Second, the baseline method (baseline) did not use clause segmentation in either training or testing. Using this </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="26381" citStr="Snover et al., 2006" startWordPosition="4152" endWordPosition="4155">on based on non-terminals was the same as with the proposed method. Although non-terminals in the second comparison method were out-of-vocabulary words and may be deleted in decoding, all of them survived and we could reconstruct sentences from translated clauses throughout the experiments. In addition, two other conditions were tested: using oracle-aligned clauses in training: the proposed method trained using oracle-aligned (oracle) clauses and the first comparison method using oracle-aligned (oracle-comp.) clauses. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically significant according to the bootstrap sampling test (p &lt; 0.05, 1,000 samples) (Zhang et al., 2004). With Moses, the improvement when using the proposed method was 1.4% (33.19% to 34.60%) in B</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. AMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="5263" citStr="Tillmann, 2004" startWordPosition="784" endWordPosition="785">r thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classified into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. HLT-NAACL, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
<author>Kim Sang</author>
<author>Herv´e D´ejean</author>
</authors>
<title>Introduction to the CoNLL-2001 shared task: Clause identification.</title>
<date>2001</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>53--57</pages>
<marker>Tjong, Sang, D´ejean, 2001</marker>
<rawString>Erik F. Tjong, Kim Sang, and Herv´e D´ejean. 2001. Introduction to the CoNLL-2001 shared task: Clause identification. In Proc. CoNLL, pages 53–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="7055" citStr="Tromble and Eisner (2009)" startWordPosition="1048" endWordPosition="1051">nguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between languages with large syntactic differences, the rules are usually unsuitable for other language groups. On the other hand, statistical methods can be applied to any language pai</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proc. EMNLP, pages 1007–1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="5673" citStr="Wu (1997)" startWordPosition="850" endWordPosition="851">sed SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>508--514</pages>
<contexts>
<context position="6302" citStr="Xia and McCord (2004)" startWordPosition="942" endWordPosition="945">ang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency </context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proc. COLING, pages 508–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve SMT for Subject-Object-Verb languages. In</title>
<date>2009</date>
<booktitle>Proc. HLT-NAACL,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="6879" citStr="Xu et al. (2009)" startWordPosition="1024" endWordPosition="1027">side parse trees. Xia and McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between l</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve SMT for Subject-Object-Verb languages. In Proc. HLT-NAACL, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="5597" citStr="Yamada and Knight, 2001" startWordPosition="834" endWordPosition="837">ce-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difficulty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previo</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. ACL, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Weibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In</title>
<date>2004</date>
<booktitle>Proc. LREC,</booktitle>
<pages>2051--2054</pages>
<contexts>
<context position="26888" citStr="Zhang et al., 2004" startWordPosition="4225" endWordPosition="4228">comp.) clauses. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically significant according to the bootstrap sampling test (p &lt; 0.05, 1,000 samples) (Zhang et al., 2004). With Moses, the improvement when using the proposed method was 1.4% (33.19% to 34.60%) in BLEU and 1.3% (57.83% to 56.50%) in TER, with a slight improvement in PER (35.84% to 35.61%). We observed: oracle » proposed» comp.(1)» baseline» comp.(2) by the Bonferroni method, where the symbol A » B means “A’s improvement over B is statistically significant.” With the hierarchical phrase-based SMT, the improvement was 2.2% (32.39% to 34.55%) in BLEU, 3.5% (58.36% to 54.87%) in TER, and 1.5% in PER (36.42% to 34.79%). We observed: oracle » proposed » 424 Table 2: Compared methods. Training w/ auto-a</context>
</contexts>
<marker>Zhang, Vogel, Weibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Weibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proc. LREC, pages 2051–2054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>912--919</pages>
<contexts>
<context position="15520" citStr="Zhu et al., 2003" startWordPosition="2407" endWordPosition="2410">ge weights between word and clause nodes are defined as the sum of lexical translation probabilities &amp;∈˜ek p(e|fm). The edge weights between words are defined as the bigram probability p(fm|fm_1). Each clause node is labeled with a class ID k E 11, ... , K}. We then propagate these K labels along the graph to label the M word nodes. Figure 4 shows the graph for the example sentence. Many label propagation algorithms are available. The important thing is to use an algorithm that encourages node pairs with strong edge weights to receive the same label. We use the label propagation algorithm of (Zhu et al., 2003). If we John lost the from Mary borrowed last week 421 John lost the book that was borrowed ... p( |) p( |) p( |) p( |) wa john senshu wa mary senshu kara mary Figure 4: Graph-based representation of the example sentence. We propagate the clause labels to the Japanese word nodes on this graph to form the clause alignments. assume the labels are binary, the following objective is minimized: � argmin wij(li − lj)2 (1) LERK+M i,j where wij is the edge weight between nodes i and j (1 &lt; i &lt; K + M, 1 &lt; j &lt; K + M), and l (li E {0,1}) is a vector of labels on the nodes. The first K elements of l, lc =</context>
<context position="16818" citStr="Zhu et al., 2003" startWordPosition="2672" endWordPosition="2675">emaining M elements, lf = (lK+1,lK+2, ..., lK+M)T, are unknown and to be determined. Here, we consider the decomposition of the weight matrix W = [wij] into four blocks after the K-th row and column as follows: W = L Wcc W cf J (2) L Wfc W ff The solution of eqn. (1), namely lf, is given by the following equation: lf = (Dff − Wff)−1Wfc lc (3) where D is the diagonal matrix with di = Ej wij and is decomposed similarly to W. Each element of lf is in the interval (0, 1) and can be regarded as the label propagation probability. A detailed explanation of this solution can be found in Section 2 of (Zhu et al., 2003). For our multi-label problem with K labels, we slightly modified the algorithm by expanding the vector l to an (M + K) x K binary matrix L = [ l1 l2 ... lK ]. After the optimization, we can normalize Lf to obtain the clause alignment scores t(lm = k|fm) between each Japanese word fm and English clause k. Theoretically, we can simply output the clause id k′ for each fm by finding k′ = arg maxk t(lm = k|fm). In practice, this may sometimes lead to Japanese clauses that have too many gaps, so we employ a two-stage procedure to extract clauses that are more contiguous. First, we segment the Japan</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proc. ICML, pages 912–919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>