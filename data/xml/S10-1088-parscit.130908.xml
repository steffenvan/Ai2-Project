<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.107788">
<title confidence="0.950846">
UCF-WS: Domain Word Sense Disambiguation using Web Selectors
</title>
<author confidence="0.922855">
Hansen A. Schwartz and Fernando Gomez
</author>
<affiliation confidence="0.997884">
School of Electrical Engineering and Computer Science
University of Central Florida
</affiliation>
<address confidence="0.873454">
Orlando, FL 32816
</address>
<email confidence="0.999711">
{hschwartz,gomez}@cs.ucf.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924333333333">
This paper studies the application of the
Web Selectors word sense disambiguation
system on a specific domain. The system
was primarily applied without any domain
tuning, but the incorporation of domain
predominant sense information was ex-
plored. Results indicated that the system
performs relatively the same with domain
predominant sense information as without,
scoring well above a random baseline, but
still 5 percentage points below results of
using the first sense.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915391304348">
We explore the use of the Web Selectors word
sense disambiguation system for disambiguating
nouns and verbs of a domain text. Our method to
acquire selectors from the Web for WSD was first
described in (Schwartz and Gomez, 2008). The
system is extended for the all-words domain task
by including part of speech tags from the Stanford
Parser (Klein and Manning, 2003). Additionally, a
domain adaptation technique of using domain pre-
dominant senses (Koeling et al., 2005) is explored,
but our primary goal is concerned with evaluating
the performance of the existing Web Selectors sys-
tem on domain text.
In previous studies, the Web Selectors system
was applied to text of a general domain. However,
the system was not directly tuned for the general
domain. The system may perform just as strong
for domain WSD since the selectors, which are the
core of disambiguation, can come from any do-
main present on the Web. In this paper, we study
the application of the Web Selectors WSD algo-
rithm to an all-words task on a specific domain,
the SemEval 2010: Task 17 (Agirre et al., 2010).
</bodyText>
<sectionHeader confidence="0.969662" genericHeader="method">
2 Web Selectors
</sectionHeader>
<bodyText confidence="0.999086675675676">
Selectors are words which take the place of a given
target word within its local context (Lin, 1997). In
the case of acquiring selectors from the Web, we
search with the text of local context (Schwartz and
Gomez, 2008). For example, if one was search-
ing for selectors of ‘channel’ in the sentence, “The
navigation channel undergoes major shifts from
north to south banks”, then a search query would
be:
The navigation * undergoes major shifts from
north to south banks.
where * represents a wildcard to match every se-
lector. The query is shortened to produce more
results until at least 300 selectors are acquired or
the query is less than 6 words. The process of
acquiring selectors repeats for every content word
of the sentence. Example selectors that might be
returned for ‘channel’ include ‘route’, ‘pathway’,
and ‘passage’.
Selectors serve for the system to essentially
learn the areas or concepts of WordNet that the
sense of a word should be similar or related. The
target noun or verb is disambiguated by comparing
its senses with all selectors for itself (target selec-
tors), as well as with context selectors for other
nouns, verbs, adjective, adverbs, proper nouns,
and pronouns in the sentence. Figure 1 shows the
overall process undertaken to rank the senses of
an ambiguous word. A similarity measure is used
when comparing with target selectors and a relat-
edness measure is used when comparing with con-
text selectors. Referring to our previous example,
the senses of ‘channel’ are compared to its own
(target) selectors via similarity measures, while
relatedness measures are used for the context se-
lectors: noun selectors of ‘navigation’, ‘shifts’,
‘north’, ‘south’, and ‘banks’; the verb selectors of
</bodyText>
<page confidence="0.981803">
392
</page>
<note confidence="0.7593295">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 392–395,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999965">
Figure 1: The overall process undertaken to disambiguate a word using Web selectors.
</figureCaption>
<bodyText confidence="0.999489774193548">
‘undergoes’; plus the adjective selectors of ‘ma-
jor’. Adverbs, proper nouns, and pronouns are not
present in the sentence, and so no selectors from
those parts of speech are considered.
For this study, we implemented the Web Selec-
tors system that was presented in (Schwartz and
Gomez, 2009). This generalized version of the
system may annotate verbs in addition to nouns,
and it includes the previously unused context se-
lectors of adverbs. We used the path-based sim-
ilarity measure of (Jiang and Conrath, 1997) for
target selectors, and the gloss-based relatedness
measure of (Banerjee and Pedersen, 2002) for con-
text selectors.
The incorporation of a part of speech tagger was
a necessary addition to the existing system. Previ-
ous evaluations of Web Selectors relied on the test-
ing corpus to provide part of speech (POS) tags
for content words. In the case of SemEval-2010
Task 17, words were only marked as targets, but
their POS was not included. We used the POS
tags from the Stanford Parser (Klein and Manning,
2003). We chose this system since the dependency
relationship output was also useful for our domain
adaptation (described in section 2.1). A modifica-
tion was made to the POS tags given the knowl-
edge that the testing corpus only included nouns
and verbs as targets. Any target that was not ini-
tially tagged as a noun or verb was reassigned as
a noun, if the word existed as a noun in WordNet
(Miller et al., 1993), or as a verb if not.
</bodyText>
<subsectionHeader confidence="0.985274">
2.1 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.978865891891892">
Overall, the Web Selectors system is not explicitly
tuned to the general domain. Selectors themselves
can be from any domain. However, sense tagged
data may be used indirectly within the system.
First, the similarity and relatedness measures used
in the system may rely on SemCor data (Miller et
al., 1994). Also, the system breaks ties by choos-
ing the most frequent sense according to WordNet
frequency data (based on SemCor). These two as-
pects of the system can be seen as tuned to the
general domain, and thus, they are likely aspects
of the system for adaptation to a specific domain.
For this work, we focused on domain-adapting
the tie breaker aspect of the Web Selectors sys-
tem. The system defines a tie occurring when mul-
tiple sense choices are scored within 5% of the top
sense choice. In order to break the tie, the system
normally chooses the most frequent sense among
the tied senses. However, it would be ideal to
break the tie by choosing the most prevalent sense
over the testing domain. Because sense tagged do-
main data is not typically available, Koeling et al.
(2005) presented the idea of estimating the most
frequent sense of a domain by calculating sense
prevalence scores from unannotated domain text.
Several steps are taken to calculate the preva-
lence scores. First, a dependency database is cre-
ated, listing the frequencies that each dependency
relationship appears. In our case, we used the
Stanford Parser (Klein and Manning, 2003) on the
background data provided by the task organizers.
From the dependency database, a thesaurus is cre-
ated based on the method of (Lin, 1998). In our ap-
proach, we considered the following relationships
from the dependency database:
subject (agent, csubj, subjpass, nsubj, nsubjpass,
xsubj)
</bodyText>
<footnote confidence="0.4185835">
direct object (dobj)
indirect object (iobj)
</footnote>
<page confidence="0.997647">
393
</page>
<bodyText confidence="0.993782181818182">
adjective modifier (amod)
noun modifier (nn)
prepositional modifier (any preposition, exclud-
ing prep of and prep for)
(typed dependency names listed in parenthesis)
Finally, a prevalence score is calculated for each
sense of a noun or verb by finding the similarity
between it and the top 50 most similar words ac-
cording to the automatically created thesaurus. As
Koeling et al. did, we use the similarity measure
of (Jiang and Conrath, 1997).
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999982666666667">
The results of our system are given in Table 1. The
first set of results (WS) was a standard run of the
system without any domain adaptation, while the
second set (WSdom) was from a run including the
domain prevalence scores in order to break ties.
The results show our domain adaptation technique
did not lead to improved results. Overall, WS re-
sults came in ranked thirteenth among twenty-nine
participating system results.
We found that using the prevalence scores alone
to pick a sense (i.e. the ‘predominant sense’) re-
sulted in an F score of 0.514 (PS in Table 1).
Koeling et al. (2005) found the predominant
sense to perform significantly better than the first
sense baseline (1sense: equivalent to most fre-
quent sense for the English WordNet) on specific
domains (32% error reduction on a finance do-
main, and 62% error reduction on a sports do-
main). Interestingly, there was no significant error
reduction over the 1sense for this task, implying
either that the domain was more difficult to adapt
to or that our implementation of the predominant
sense algorithm was not as strong as that use by
Koeling et al. In any case, this lack of significant
error reduction over the 1sense may explain why
our WSdom results were not stronger than the WS
results. In WSdom, prevalence scores were used
instead of 1sense to break ties.
We computed a few figures to gain more in-
sights on the system’s handling of domain data.
Noun precision was 0.446 while verb precision
was 0.449. It was unexpected for verb disam-
biguation results to be as strong as nouns because
a previous study using Web Selectors found noun
sense disambiguation clearly stronger than verb
sense disambiguation on a coarse-grained corpus
</bodyText>
<table confidence="0.9995495">
P R F Pn P„
rand 0.23 0.23 0.23
1sense 0.505 0.505 0.505
WS 0.447 0.441 0.444 .446 .449
WSdom 0.440 0.434 0.437 .441 .438
PS 0.514 0.514 0.514 .53 .44
</table>
<tableCaption confidence="0.99102375">
Table 1: (P)recision, (R)ecall, and (F)-score of
various runs of the system on the Task 17 data.
Pn and P„ correspond to precision results broken
down by nouns and verbs.
</tableCaption>
<table confidence="0.999857333333333">
Pen1 Pen2 Pen3
WS 0.377 0.420 0.558
WSdom 0.384 0.415 0.531
</table>
<tableCaption confidence="0.997269">
Table 2: Precision scores based on the three docu-
</tableCaption>
<bodyText confidence="0.995626676470588">
ments of the English testing corpora (‘en1’, ‘en2’,
and ‘en3’).
(Schwartz and Gomez, 2009). Ideally, our results
for noun disambiguation would have been stronger
than the the 1sense and PS results. In order to
determine the effect of the POS tagger (parser in
this case) on the error, we determined 1.6% of the
error was due to the wrong POS tag at (0.9% of
all instances). Lastly, Table 2 shows the precision
scores for each of the three documents from which
the English testing corpus was created. Without
understanding the differences between the testing
documents it is difficult to explain why the preci-
sion varies, but the figures may be useful for com-
parisons by others.
Several aspects of the test data were unexpected
for our system. Some proper nouns were consid-
ered as target words. Our system was not orig-
inally intended to annotate proper nouns, but we
were able to adjust it to treat them simply as nouns.
To be sure this treatment was appropriate, we also
submitted results where proper nouns were ex-
cluded, and got a precision of 0.437 and recall
of 0.392. One would expect the precision to in-
crease at the expense of recall if the proper nouns
were more problematic for the system than other
instances. This was not the case, and we conclude
our handling of proper nouns was appropriate.
Unfortunately, another unexpected aspect of the
data was not handled correctly by our system. Our
system only considered senses from one form of
the target word according to WordNet, while the
key included multiple forms of a word. For exam-
ple, the key indicated low tide-1 was the answer to
</bodyText>
<page confidence="0.997451">
394
</page>
<bodyText confidence="0.9999283">
an instance where our system had only considered
senses of ‘tide’. We determined that for 10.2%
of the instances that were incorrect in our WS re-
sults we did not even consider the correct sense
as a possible prediction due to using an inventory
from only one form of the word. Since this issue
mostly applied to nouns it may explain the obser-
vation that the noun disambiguation performance
was not better than the verb disambiguation per-
formance as was expected.
</bodyText>
<sectionHeader confidence="0.996471" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.997680294117647">
In this paper we examined the application of the
Web Selectors WSD system to the SemEval-2010
Task 17: All-words WSD on a Specific Domain. A
primary goal was to apply the pre-existing system
with minimal changes. To do this we incorporated
automatic part of speech tags, which we found
only had a small impact on the error (incorrectly
tagged 0.9% of all target instances). Overall, the
results showed the system to perform below the
1sense baseline for both nouns and verbs. This is a
lower relative performance than past studies which
found the disambiguation performance above the
1sense for nouns. One reason for the lower noun
performance is that for 10.2 % of our errors, the
system did not consider the correct sense choice
as a possibility. Future versions of the system will
need to expand the sense inventory to include other
forms of a word (example: ‘low tide’ when disam-
biguating ‘tide’).
Toward domain adaptation, we ran an exper-
iment in which one aspect of our system was
tuned to the domain by using domain prevalence
scores (or ‘predominant senses’). We found no im-
provement from using this adaptation technique,
but we also discovered that results entirely based
on predictions of the domain predominant senses
were only minimally superior to 1sense (F-score
of 0.514 versus 0.505 for 1sense). Thus, future
studies will examine better implementation of the
predominant sense algorithm, as well as explore
other complimentary techniques for domain adap-
tation: customizing similarity measures for the
domain, or restricting areas of WordNet as sense
choices based on the domain.
</bodyText>
<sectionHeader confidence="0.973281" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.977363666666667">
This research was supported by the NASA
Engineering and Safety Center under
Grant/Cooperative Agreement NNX08AJ98A.
</bodyText>
<sectionHeader confidence="0.9958" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999785481481482">
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of SemEval-2010. Association for Computa-
tional Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Proceedings of the Third In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, Mexico City,
Mexico.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING X, Taiwan.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems 15, pages 3–10.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of
the conference on Human Language Technology
and Experimental Methods in NLP, pages 419–426,
Morristown, NJ, USA.
Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th annual meeting on Association
for Computational Linguistics, pages 64–71.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL 98, pages 768–774, Montreal, Canada. Morgan
Kaufmann.
George Miller, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
George A. Miller, Martin Chodorow, Shari L, Claudia
Leacock, and Robert G. Thomas. 1994. Using a se-
mantic concordance for sense identification. In In
Proc. of ARPA Human Language Technology Work-
shop.
Hansen A. Schwartz and Fernando Gomez. 2008. Ac-
quiring knowledge from the web to be used as se-
lectors for noun sense disambiguation. In CoNLL
2008: Proceedings of the Twelfth Conference on
Computational Natural Language Learning, pages
105–112, Manchester, England, August.
Hansen A. Schwartz and Fernando Gomez. 2009.
Using web selectors for the disambiguation of all
words. In Proceedings of the NAACL-2009 Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 28–36, Boulder,
Colorado, June.
</reference>
<page confidence="0.999119">
395
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.994810">
<title confidence="0.999841">UCF-WS: Domain Word Sense Disambiguation using Web Selectors</title>
<author confidence="0.999922">Hansen A Schwartz</author>
<author confidence="0.999922">Fernando Gomez</author>
<affiliation confidence="0.9995835">School of Electrical Engineering and Computer Science University of Central Florida</affiliation>
<address confidence="0.997692">Orlando, FL 32816</address>
<abstract confidence="0.999852076923077">This paper studies the application of the Web Selectors word sense disambiguation system on a specific domain. The system was primarily applied without any domain tuning, but the incorporation of domain predominant sense information was explored. Results indicated that the system performs relatively the same with domain predominant sense information as without, scoring well above a random baseline, but still 5 percentage points below results of using the first sense.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, Christiane Fellbaum,</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2010. Association for Computational Linguistics.</booktitle>
<location>Shu</location>
<marker>Agirre, 2010</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Christiane Fellbaum, Shu kai Hsieh, Maurizio Tesconi, Monica Monachini, Piek Vossen, and Roxanne Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Proceedings of SemEval-2010. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="4412" citStr="Banerjee and Pedersen, 2002" startWordPosition="704" endWordPosition="707">d using Web selectors. ‘undergoes’; plus the adjective selectors of ‘major’. Adverbs, proper nouns, and pronouns are not present in the sentence, and so no selectors from those parts of speech are considered. For this study, we implemented the Web Selectors system that was presented in (Schwartz and Gomez, 2009). This generalized version of the system may annotate verbs in addition to nouns, and it includes the previously unused context selectors of adverbs. We used the path-based similarity measure of (Jiang and Conrath, 1997) for target selectors, and the gloss-based relatedness measure of (Banerjee and Pedersen, 2002) for context selectors. The incorporation of a part of speech tagger was a necessary addition to the existing system. Previous evaluations of Web Selectors relied on the testing corpus to provide part of speech (POS) tags for content words. In the case of SemEval-2010 Task 17, words were only marked as targets, but their POS was not included. We used the POS tags from the Stanford Parser (Klein and Manning, 2003). We chose this system since the dependency relationship output was also useful for our domain adaptation (described in section 2.1). A modification was made to the POS tags given the </context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of ROCLING X,</booktitle>
<contexts>
<context position="4317" citStr="Jiang and Conrath, 1997" startWordPosition="691" endWordPosition="694">or Computational Linguistics Figure 1: The overall process undertaken to disambiguate a word using Web selectors. ‘undergoes’; plus the adjective selectors of ‘major’. Adverbs, proper nouns, and pronouns are not present in the sentence, and so no selectors from those parts of speech are considered. For this study, we implemented the Web Selectors system that was presented in (Schwartz and Gomez, 2009). This generalized version of the system may annotate verbs in addition to nouns, and it includes the previously unused context selectors of adverbs. We used the path-based similarity measure of (Jiang and Conrath, 1997) for target selectors, and the gloss-based relatedness measure of (Banerjee and Pedersen, 2002) for context selectors. The incorporation of a part of speech tagger was a necessary addition to the existing system. Previous evaluations of Web Selectors relied on the testing corpus to provide part of speech (POS) tags for content words. In the case of SemEval-2010 Task 17, words were only marked as targets, but their POS was not included. We used the POS tags from the Stanford Parser (Klein and Manning, 2003). We chose this system since the dependency relationship output was also useful for our d</context>
<context position="7521" citStr="Jiang and Conrath, 1997" startWordPosition="1233" endWordPosition="1236">ch, we considered the following relationships from the dependency database: subject (agent, csubj, subjpass, nsubj, nsubjpass, xsubj) direct object (dobj) indirect object (iobj) 393 adjective modifier (amod) noun modifier (nn) prepositional modifier (any preposition, excluding prep of and prep for) (typed dependency names listed in parenthesis) Finally, a prevalence score is calculated for each sense of a noun or verb by finding the similarity between it and the top 50 most similar words according to the automatically created thesaurus. As Koeling et al. did, we use the similarity measure of (Jiang and Conrath, 1997). 3 Results and Discussion The results of our system are given in Table 1. The first set of results (WS) was a standard run of the system without any domain adaptation, while the second set (WSdom) was from a run including the domain prevalence scores in order to break ties. The results show our domain adaptation technique did not lead to improved results. Overall, WS results came in ranked thirteenth among twenty-nine participating system results. We found that using the prevalence scores alone to pick a sense (i.e. the ‘predominant sense’) resulted in an F score of 0.514 (PS in Table 1). Koe</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity on corpus statistics and lexical taxonomy. In Proceedings of ROCLING X, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>3--10</pages>
<contexts>
<context position="1092" citStr="Klein and Manning, 2003" startWordPosition="161" endWordPosition="164">ion was explored. Results indicated that the system performs relatively the same with domain predominant sense information as without, scoring well above a random baseline, but still 5 percentage points below results of using the first sense. 1 Introduction We explore the use of the Web Selectors word sense disambiguation system for disambiguating nouns and verbs of a domain text. Our method to acquire selectors from the Web for WSD was first described in (Schwartz and Gomez, 2008). The system is extended for the all-words domain task by including part of speech tags from the Stanford Parser (Klein and Manning, 2003). Additionally, a domain adaptation technique of using domain predominant senses (Koeling et al., 2005) is explored, but our primary goal is concerned with evaluating the performance of the existing Web Selectors system on domain text. In previous studies, the Web Selectors system was applied to text of a general domain. However, the system was not directly tuned for the general domain. The system may perform just as strong for domain WSD since the selectors, which are the core of disambiguation, can come from any domain present on the Web. In this paper, we study the application of the Web Se</context>
<context position="4828" citStr="Klein and Manning, 2003" startWordPosition="778" endWordPosition="781">ously unused context selectors of adverbs. We used the path-based similarity measure of (Jiang and Conrath, 1997) for target selectors, and the gloss-based relatedness measure of (Banerjee and Pedersen, 2002) for context selectors. The incorporation of a part of speech tagger was a necessary addition to the existing system. Previous evaluations of Web Selectors relied on the testing corpus to provide part of speech (POS) tags for content words. In the case of SemEval-2010 Task 17, words were only marked as targets, but their POS was not included. We used the POS tags from the Stanford Parser (Klein and Manning, 2003). We chose this system since the dependency relationship output was also useful for our domain adaptation (described in section 2.1). A modification was made to the POS tags given the knowledge that the testing corpus only included nouns and verbs as targets. Any target that was not initially tagged as a noun or verb was reassigned as a noun, if the word existed as a noun in WordNet (Miller et al., 1993), or as a verb if not. 2.1 Domain Adaptation Overall, the Web Selectors system is not explicitly tuned to the general domain. Selectors themselves can be from any domain. However, sense tagged </context>
<context position="6738" citStr="Klein and Manning, 2003" startWordPosition="1109" endWordPosition="1112">ormally chooses the most frequent sense among the tied senses. However, it would be ideal to break the tie by choosing the most prevalent sense over the testing domain. Because sense tagged domain data is not typically available, Koeling et al. (2005) presented the idea of estimating the most frequent sense of a domain by calculating sense prevalence scores from unannotated domain text. Several steps are taken to calculate the prevalence scores. First, a dependency database is created, listing the frequencies that each dependency relationship appears. In our case, we used the Stanford Parser (Klein and Manning, 2003) on the background data provided by the task organizers. From the dependency database, a thesaurus is created based on the method of (Lin, 1998). In our approach, we considered the following relationships from the dependency database: subject (agent, csubj, subjpass, nsubj, nsubjpass, xsubj) direct object (dobj) indirect object (iobj) 393 adjective modifier (amod) noun modifier (nn) prepositional modifier (any preposition, excluding prep of and prep for) (typed dependency names listed in parenthesis) Finally, a prevalence score is calculated for each sense of a noun or verb by finding the simi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In In Advances in Neural Information Processing Systems 15, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Experimental Methods in NLP,</booktitle>
<pages>419--426</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1195" citStr="Koeling et al., 2005" startWordPosition="176" endWordPosition="179">ense information as without, scoring well above a random baseline, but still 5 percentage points below results of using the first sense. 1 Introduction We explore the use of the Web Selectors word sense disambiguation system for disambiguating nouns and verbs of a domain text. Our method to acquire selectors from the Web for WSD was first described in (Schwartz and Gomez, 2008). The system is extended for the all-words domain task by including part of speech tags from the Stanford Parser (Klein and Manning, 2003). Additionally, a domain adaptation technique of using domain predominant senses (Koeling et al., 2005) is explored, but our primary goal is concerned with evaluating the performance of the existing Web Selectors system on domain text. In previous studies, the Web Selectors system was applied to text of a general domain. However, the system was not directly tuned for the general domain. The system may perform just as strong for domain WSD since the selectors, which are the core of disambiguation, can come from any domain present on the Web. In this paper, we study the application of the Web Selectors WSD algorithm to an all-words task on a specific domain, the SemEval 2010: Task 17 (Agirre et a</context>
<context position="6365" citStr="Koeling et al. (2005)" startWordPosition="1051" endWordPosition="1054">be seen as tuned to the general domain, and thus, they are likely aspects of the system for adaptation to a specific domain. For this work, we focused on domain-adapting the tie breaker aspect of the Web Selectors system. The system defines a tie occurring when multiple sense choices are scored within 5% of the top sense choice. In order to break the tie, the system normally chooses the most frequent sense among the tied senses. However, it would be ideal to break the tie by choosing the most prevalent sense over the testing domain. Because sense tagged domain data is not typically available, Koeling et al. (2005) presented the idea of estimating the most frequent sense of a domain by calculating sense prevalence scores from unannotated domain text. Several steps are taken to calculate the prevalence scores. First, a dependency database is created, listing the frequencies that each dependency relationship appears. In our case, we used the Stanford Parser (Klein and Manning, 2003) on the background data provided by the task organizers. From the dependency database, a thesaurus is created based on the method of (Lin, 1998). In our approach, we considered the following relationships from the dependency da</context>
<context position="8139" citStr="Koeling et al. (2005)" startWordPosition="1341" endWordPosition="1344">97). 3 Results and Discussion The results of our system are given in Table 1. The first set of results (WS) was a standard run of the system without any domain adaptation, while the second set (WSdom) was from a run including the domain prevalence scores in order to break ties. The results show our domain adaptation technique did not lead to improved results. Overall, WS results came in ranked thirteenth among twenty-nine participating system results. We found that using the prevalence scores alone to pick a sense (i.e. the ‘predominant sense’) resulted in an F score of 0.514 (PS in Table 1). Koeling et al. (2005) found the predominant sense to perform significantly better than the first sense baseline (1sense: equivalent to most frequent sense for the English WordNet) on specific domains (32% error reduction on a finance domain, and 62% error reduction on a sports domain). Interestingly, there was no significant error reduction over the 1sense for this task, implying either that the domain was more difficult to adapt to or that our implementation of the predominant sense algorithm was not as strong as that use by Koeling et al. In any case, this lack of significant error reduction over the 1sense may </context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of the conference on Human Language Technology and Experimental Methods in NLP, pages 419–426, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="1922" citStr="Lin, 1997" startWordPosition="307" endWordPosition="308">on domain text. In previous studies, the Web Selectors system was applied to text of a general domain. However, the system was not directly tuned for the general domain. The system may perform just as strong for domain WSD since the selectors, which are the core of disambiguation, can come from any domain present on the Web. In this paper, we study the application of the Web Selectors WSD algorithm to an all-words task on a specific domain, the SemEval 2010: Task 17 (Agirre et al., 2010). 2 Web Selectors Selectors are words which take the place of a given target word within its local context (Lin, 1997). In the case of acquiring selectors from the Web, we search with the text of local context (Schwartz and Gomez, 2008). For example, if one was searching for selectors of ‘channel’ in the sentence, “The navigation channel undergoes major shifts from north to south banks”, then a search query would be: The navigation * undergoes major shifts from north to south banks. where * represents a wildcard to match every selector. The query is shortened to produce more results until at least 300 selectors are acquired or the query is less than 6 words. The process of acquiring selectors repeats for ever</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th annual meeting on Association for Computational Linguistics, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL 98,</booktitle>
<pages>768--774</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Montreal, Canada.</location>
<contexts>
<context position="6882" citStr="Lin, 1998" startWordPosition="1136" endWordPosition="1137">ng domain. Because sense tagged domain data is not typically available, Koeling et al. (2005) presented the idea of estimating the most frequent sense of a domain by calculating sense prevalence scores from unannotated domain text. Several steps are taken to calculate the prevalence scores. First, a dependency database is created, listing the frequencies that each dependency relationship appears. In our case, we used the Stanford Parser (Klein and Manning, 2003) on the background data provided by the task organizers. From the dependency database, a thesaurus is created based on the method of (Lin, 1998). In our approach, we considered the following relationships from the dependency database: subject (agent, csubj, subjpass, nsubj, nsubjpass, xsubj) direct object (dobj) indirect object (iobj) 393 adjective modifier (amod) noun modifier (nn) prepositional modifier (any preposition, excluding prep of and prep for) (typed dependency names listed in parenthesis) Finally, a prevalence score is calculated for each sense of a noun or verb by finding the similarity between it and the top 50 most similar words according to the automatically created thesaurus. As Koeling et al. did, we use the similari</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLINGACL 98, pages 768–774, Montreal, Canada. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>R Beckwith</author>
<author>Christiane Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Princeton University.</institution>
<contexts>
<context position="5235" citStr="Miller et al., 1993" startWordPosition="853" endWordPosition="856"> speech (POS) tags for content words. In the case of SemEval-2010 Task 17, words were only marked as targets, but their POS was not included. We used the POS tags from the Stanford Parser (Klein and Manning, 2003). We chose this system since the dependency relationship output was also useful for our domain adaptation (described in section 2.1). A modification was made to the POS tags given the knowledge that the testing corpus only included nouns and verbs as targets. Any target that was not initially tagged as a noun or verb was reassigned as a noun, if the word existed as a noun in WordNet (Miller et al., 1993), or as a verb if not. 2.1 Domain Adaptation Overall, the Web Selectors system is not explicitly tuned to the general domain. Selectors themselves can be from any domain. However, sense tagged data may be used indirectly within the system. First, the similarity and relatedness measures used in the system may rely on SemCor data (Miller et al., 1994). Also, the system breaks ties by choosing the most frequent sense according to WordNet frequency data (based on SemCor). These two aspects of the system can be seen as tuned to the general domain, and thus, they are likely aspects of the system for</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George Miller, R. Beckwith, Christiane Fellbaum, D. Gross, and K. Miller. 1993. Five papers on wordnet. Technical report, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Martin Chodorow</author>
<author>L Shari</author>
<author>Claudia Leacock</author>
<author>Robert G Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification. In</title>
<date>1994</date>
<booktitle>In Proc. of ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="5586" citStr="Miller et al., 1994" startWordPosition="912" endWordPosition="915">dification was made to the POS tags given the knowledge that the testing corpus only included nouns and verbs as targets. Any target that was not initially tagged as a noun or verb was reassigned as a noun, if the word existed as a noun in WordNet (Miller et al., 1993), or as a verb if not. 2.1 Domain Adaptation Overall, the Web Selectors system is not explicitly tuned to the general domain. Selectors themselves can be from any domain. However, sense tagged data may be used indirectly within the system. First, the similarity and relatedness measures used in the system may rely on SemCor data (Miller et al., 1994). Also, the system breaks ties by choosing the most frequent sense according to WordNet frequency data (based on SemCor). These two aspects of the system can be seen as tuned to the general domain, and thus, they are likely aspects of the system for adaptation to a specific domain. For this work, we focused on domain-adapting the tie breaker aspect of the Web Selectors system. The system defines a tie occurring when multiple sense choices are scored within 5% of the top sense choice. In order to break the tie, the system normally chooses the most frequent sense among the tied senses. However, </context>
</contexts>
<marker>Miller, Chodorow, Shari, Leacock, Thomas, 1994</marker>
<rawString>George A. Miller, Martin Chodorow, Shari L, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In In Proc. of ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hansen A Schwartz</author>
<author>Fernando Gomez</author>
</authors>
<title>Acquiring knowledge from the web to be used as selectors for noun sense disambiguation.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>105--112</pages>
<location>Manchester, England,</location>
<contexts>
<context position="954" citStr="Schwartz and Gomez, 2008" startWordPosition="138" endWordPosition="141">n a specific domain. The system was primarily applied without any domain tuning, but the incorporation of domain predominant sense information was explored. Results indicated that the system performs relatively the same with domain predominant sense information as without, scoring well above a random baseline, but still 5 percentage points below results of using the first sense. 1 Introduction We explore the use of the Web Selectors word sense disambiguation system for disambiguating nouns and verbs of a domain text. Our method to acquire selectors from the Web for WSD was first described in (Schwartz and Gomez, 2008). The system is extended for the all-words domain task by including part of speech tags from the Stanford Parser (Klein and Manning, 2003). Additionally, a domain adaptation technique of using domain predominant senses (Koeling et al., 2005) is explored, but our primary goal is concerned with evaluating the performance of the existing Web Selectors system on domain text. In previous studies, the Web Selectors system was applied to text of a general domain. However, the system was not directly tuned for the general domain. The system may perform just as strong for domain WSD since the selectors</context>
</contexts>
<marker>Schwartz, Gomez, 2008</marker>
<rawString>Hansen A. Schwartz and Fernando Gomez. 2008. Acquiring knowledge from the web to be used as selectors for noun sense disambiguation. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 105–112, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hansen A Schwartz</author>
<author>Fernando Gomez</author>
</authors>
<title>Using web selectors for the disambiguation of all words.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL-2009 Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>28--36</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4097" citStr="Schwartz and Gomez, 2009" startWordPosition="655" endWordPosition="658">on’, ‘shifts’, ‘north’, ‘south’, and ‘banks’; the verb selectors of 392 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 392–395, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Figure 1: The overall process undertaken to disambiguate a word using Web selectors. ‘undergoes’; plus the adjective selectors of ‘major’. Adverbs, proper nouns, and pronouns are not present in the sentence, and so no selectors from those parts of speech are considered. For this study, we implemented the Web Selectors system that was presented in (Schwartz and Gomez, 2009). This generalized version of the system may annotate verbs in addition to nouns, and it includes the previously unused context selectors of adverbs. We used the path-based similarity measure of (Jiang and Conrath, 1997) for target selectors, and the gloss-based relatedness measure of (Banerjee and Pedersen, 2002) for context selectors. The incorporation of a part of speech tagger was a necessary addition to the existing system. Previous evaluations of Web Selectors relied on the testing corpus to provide part of speech (POS) tags for content words. In the case of SemEval-2010 Task 17, words w</context>
<context position="9773" citStr="Schwartz and Gomez, 2009" startWordPosition="1622" endWordPosition="1625">nd noun sense disambiguation clearly stronger than verb sense disambiguation on a coarse-grained corpus P R F Pn P„ rand 0.23 0.23 0.23 1sense 0.505 0.505 0.505 WS 0.447 0.441 0.444 .446 .449 WSdom 0.440 0.434 0.437 .441 .438 PS 0.514 0.514 0.514 .53 .44 Table 1: (P)recision, (R)ecall, and (F)-score of various runs of the system on the Task 17 data. Pn and P„ correspond to precision results broken down by nouns and verbs. Pen1 Pen2 Pen3 WS 0.377 0.420 0.558 WSdom 0.384 0.415 0.531 Table 2: Precision scores based on the three documents of the English testing corpora (‘en1’, ‘en2’, and ‘en3’). (Schwartz and Gomez, 2009). Ideally, our results for noun disambiguation would have been stronger than the the 1sense and PS results. In order to determine the effect of the POS tagger (parser in this case) on the error, we determined 1.6% of the error was due to the wrong POS tag at (0.9% of all instances). Lastly, Table 2 shows the precision scores for each of the three documents from which the English testing corpus was created. Without understanding the differences between the testing documents it is difficult to explain why the precision varies, but the figures may be useful for comparisons by others. Several aspe</context>
</contexts>
<marker>Schwartz, Gomez, 2009</marker>
<rawString>Hansen A. Schwartz and Fernando Gomez. 2009. Using web selectors for the disambiguation of all words. In Proceedings of the NAACL-2009 Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 28–36, Boulder, Colorado, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>