<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.268273">
<title confidence="0.9895">
Measuring Transitivity Using Untrained Annotators
</title>
<author confidence="0.991141">
Nitin Madnania,b Jordan Boyd-Grabera Philip Resnika,c
</author>
<affiliation confidence="0.9994455">
aInstitute for Advanced Computer Studies
bDepartment of Computer Science
cDepartment of Linguistics
University of Maryland, College Park
</affiliation>
<email confidence="0.999377">
{nmadnani,jbg,resnik}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.993899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993981814814815">
Hopper and Thompson (1980) defined a multi-axis
theory of transitivity that goes beyond simple syn-
tactic transitivity and captures how much “action”
takes place in a sentence. Detecting these features
requires a deep understanding of lexical semantics
and real-world pragmatics. We propose two gen-
eral approaches for creating a corpus of sentences
labeled with respect to the Hopper-Thompson transi-
tivity schema using Amazon Mechanical Turk. Both
approaches assume no existing resources and incor-
porate all necessary annotation into a single system;
this is done to allow for future generalization to other
languages. The first task attempts to use language-
neutral videos to elicit human-composed sentences
with specified transitivity attributes. The second task
uses an iterative process to first label the actors and
objects in sentences and then annotate the sentences’
transitivity. We examine the success of these tech-
niques and perform a preliminary classification of
the transitivity of held-out data.
Hopper and Thompson (1980) created a multi-axis the-
ory of Transitivity1 that describes the volition of the sub-
ject, the affectedness of the object, and the duration of the
action. In short, this theory goes beyond the simple gram-
matical notion of transitivity (whether verbs take objects
— transitive — or not — intransitive) and captures how
much “action” takes place in a sentence. Such notions of
Transitivity are not apparent from surface features alone;
identical syntactic constructions can have vastly different
Transitivity. This well-established linguistic theory, how-
ever, is not useful for real-world applications without a
Transitivity-annotated corpus.
Given such a substantive corpus, conventional machine
learning techniques could help determine the Transitivity
of verbs within sentences. Transitivity has been found to
play a role in what is called “syntactic framing,” which
expresses implicit sentiment (Greene and Resnik, 2009).
1We use capital “T” to differentiate from conventional syntactic tran-
sitivity throughout the paper.
In these contexts, the perspective or sentiment of the
writer is reflected in the constructions used to express
ideas. For example, a less Transitive construction might
be used to deflect responsibility (e.g. “John was killed”
vs. “Benjamin killed John”).
In the rest of this paper, we review the Hopper-
Thompson transitivity schema and propose two relatively
language-neutral methods to collect Transitivity ratings.
The first asks humans to generate sentences with de-
sired Transitivity characteristics. The second asks hu-
mans to rate sentences on dimensions from the Hopper-
Thompson schema. We then discuss the difficulties of
collecting such linguistically deep data and analyze the
available results. We then pilot an initial classifier on the
Hopper-Thompson dimensions.
</bodyText>
<sectionHeader confidence="0.991735" genericHeader="keywords">
1 Transitivity
</sectionHeader>
<bodyText confidence="0.999957347826087">
Table 1 shows the subset of the Hopper-Thompson di-
mensions of Transitivity used in this study. We excluded
noun-specific aspects as we felt that these were well cov-
ered by existing natural language processing (NLP) ap-
proaches (e.g. whether the object / subject is person, ab-
stract entity, or abstract concept is handled well by exist-
ing named entity recognition systems) and also excluded
aspects which we felt had significant overlap with the
dimensions we were investigating (e.g. affirmation and
mode).
We also distinguished the original Hopper-Thompson
“Affectedness” aspect into separate “Benefit” and
“Harm” components, as we suspect that these data will
be useful to other applications such as sentiment analy-
sis.
We believe that these dimensions of transitivity are
simple and intuitive enough that they can be understood
and labeled by the people on Amazon Mechanical Turk,
a web service. Amazon Mechanical Turk (MTurk) allows
individuals to post jobs on MTurk with a set fee that are
then performed by workers on the Internet. MTurk con-
nects workers to people with tasks and handles the coor-
dination problems of payment and transferring data.
</bodyText>
<page confidence="0.987961">
188
</page>
<note confidence="0.7187185">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 188–194,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.998583666666667">
Kinesis Sentences where movement happens are perceived to be more Transitive.
“Sue jumped out of an airplane” vs. “The corporation jumped to a silly conclusion.”
Punctuality Sentences where the action happens quickly are perceived to be more Transitive.
“She touched her ID to the scanner to enter” vs. “I was touched by how much she helped me.”
Mode Sentences with no doubt about whether the action happened are perceived to be more Transitive.
“Bob was too busy to fix the drain” vs. “Bob fixed the drain.”
Affectedness Sentences where the object is more affected by the action are perceived to be more Transitive.
“The St. Bernard saved the climber” vs. “Melanie looked at the model.”
Volition Sentences where the actor chose to perform the action are perceived to be more Transitive.
“Paul jumped out of the bushes and startled his poor sister” vs. “The picture startled George.”
Aspect Sentences where the action is done to completion are perceived to be more Transitive.
“Walter is eating the hamburger” vs. “Walter ate the pudding up.”
</table>
<tableCaption confidence="0.9875675">
Table 1: The Hopper-Thompson dimensions of transitivity addressed in this paper. In experiments, “Affectedness” was divided into
“Harm” and “Benefit.”
</tableCaption>
<sectionHeader confidence="0.995735" genericHeader="introduction">
2 Experiments
</sectionHeader>
<bodyText confidence="0.999824">
Our goal is to create experiments for MTurk that will pro-
duce a large set of sentences with known values of Tran-
sitivity. With both experiments, we design the tasks to
be as language independent as possible, thus not depend-
ing on language-specific preprocessing tools. This allows
the data collection approach to be replicated in other lan-
guages.
</bodyText>
<subsectionHeader confidence="0.98344">
2.1 Elicitation
</subsectionHeader>
<bodyText confidence="0.9996335">
The first task is not corpus specific, and requires no
language-specific resources. We represent verbs using
videos (Ma and Cook, 2009). This also provides a form
of language independent sense disambiguation. We dis-
play videos illustrating verbs (Figure 1) and ask users on
MTurk to identify the action and give nouns that can do
the action and — in a separate task — the nouns that the
action can be done to. For quality control, Turkers must
match a previous Turker’s response for one of their an-
swers (a la the game show “Family Feud”).
</bodyText>
<figureCaption confidence="0.984272">
Figure 1: Stills from three videos depicting the verbs “receive,”
“hear,” and “help.”
</figureCaption>
<bodyText confidence="0.999817">
We initially found that subjects had difficulty distin-
guishing what things could do the action (subjects) vs.
what things the action could be done to (objects). In or-
der to suggest the appropriate syntactic frame, we use
javascript to form their inputs into protosentences as they
typed. For example, if they identified an action as “pick-
ing” and suggested “fruit” as a possible object, the pro-
tosentence “it is picking fruit” is displayed below their
input (Figure 2). This helped ensure consistent answers.
The subject and object tasks were done separately, and
for the object task, users were allowed to say that there
is nothing the action can be done to (for example, for an
intransitive verb).
</bodyText>
<figureCaption confidence="0.9961818">
Figure 2: A screenshot of a user completing a task to find ob-
jects of a particular verb, where the verb is represented by a
film. After the user has written a verb and a noun, a protosen-
tence is formed and shown to ensure that the user is using the
words in the appropriate roles.
</figureCaption>
<bodyText confidence="0.999772727272727">
These subjects and objects we collected were then used
as inputs for a second task. We showed workers videos
with potential subjects and objects and asked them to
create pairs of sentences with opposite Transitivity at-
tributes. For example, Write a sentence where the thing
to which the action is done benefits and Write a sentence
where the thing to which the action is done is not affected
by the action. For both sides of the Transitivity dimen-
sion, we allowed users to say that writing such a sentence
is impossible. We discuss the initial results of this task in
Section 3.
</bodyText>
<subsectionHeader confidence="0.998181">
2.2 Annotation
</subsectionHeader>
<bodyText confidence="0.99392">
Our second task—one of annotation—depends on having
a corpus available in the language of interest. For con-
</bodyText>
<page confidence="0.99492">
189
</page>
<bodyText confidence="0.99983425">
creteness and availability, we use Wikipedia, a free mul-
tilingual encyclopedia. We extract a large pool of sen-
tences from Wikipedia containing verbs of interest. We
apply light preprocessing to remove long, unclear (e.g.
starting with a pronoun), or uniquely Wikipedian sen-
tences (e.g. very short sentences of the form “See List
of Star Trek Characters”). We construct tasks, each for a
single verb, that ask users to identify the subject and ob-
ject for the verb in randomly selected sentences.2 Users
were prompted by an interactive javascript guide (Fig-
ure 3) that instructed them to click on the first word of the
subject (or object) and then to click on the last word that
made up the subject (or object). After they clicked, a text
box was automatically populated with their answer; this
decreased errors and made the tasks easier to finish. For
quality control, each HIT has a simple sentence where
subject and object were already determined by the au-
thors; the user must match the annotation on that sentence
for credit. We ended up rejecting less than one percent of
submitted hits.
</bodyText>
<figureCaption confidence="0.9949985">
Figure 3: A screenshot of the subject identification task. The
user has to click on the phrase that they believe is the subject.
</figureCaption>
<bodyText confidence="0.9949838">
Once objects and subjects have been identified, other
users rate the sentence’s Transitivity by answering the
following questions like, where $VERB represents the
verb of interest, $SUBJ is its subject and $OBJ is its ob-
ject3:
</bodyText>
<listItem confidence="0.999267916666667">
• Aspect. After reading this sentence, do you know
that $SUBJ is done $VERBing?
• Affirmation. From reading the sentence, how cer-
tain are you that $VERBing happened?
• Benefit. How much did $OBJ benefit?
• Harm. How much was $OBJ harmed?
• Kinesis. Did $SUBJ move?
• Punctuality. If you were to film $SUBJ’s act of
$VERBing in its entirety, how long would the movie
be?
• Volition. Did the $SUBJ make a conscious choice
to $VERB?
</listItem>
<bodyText confidence="0.983975333333333">
The answers were on a scale of 0 to 4 (higher num-
bers meant the sentence evinced more of the property in
2Our goal of language independence and the unreliable correspon-
dence between syntax and semantic roles precludes automatic labeling
of the subjects and objects.
3These questions were developed using Greene and Resnik’s (2009)
surveys as a foundation.
question), and each point in the scale had a description to
anchor raters and to ensure consistent results.
</bodyText>
<subsectionHeader confidence="0.981666">
2.3 Rewards
</subsectionHeader>
<bodyText confidence="0.999918571428571">
Table 2 summarizes the rewards for the tasks used in
these experiments. Rewards were set at the minimal rate
that could attract sufficient interest from users. For the
“Video Elicitation” task, where users wrote sentences
with specified Transitivity properties, we also offered
bonuses for clever, clear sentences. However, this was
our least popular task, and we struggled to attract users.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.99931">
3.1 Creative but Unusable Elicitation Results
</subsectionHeader>
<bodyText confidence="0.999972526315789">
We initially thought that we would have difficulty coax-
ing users to provide full sentences. This turned out not
to be the case. We had no difficulty getting (very imag-
inative) sentences, but the sentences were often incon-
sistent with the Transitivity aspects we are interested in.
This shows both the difficulty of writing concise instruc-
tions for non-experts and the differences between every-
day meanings of words and their meaning in linguistic
contexts.
For example, the “volitional” elicitation task asked
people to create sentences where the subject made a con-
scious decision to perform the action. In the cases where
we asked users to create sentences where the subject did
not make a conscious decision to perform an action, al-
most all of the sentences created by users focused on sen-
tences where a person (rather than employ other tactics
such as using a less individuated subject, e.g. replacing
“Bob” with “freedom”) was performing the action and
was coerced into doing the action. For example:
</bodyText>
<listItem confidence="0.998100857142857">
• Sellers often give gifts to their clients when they are
trying to make up for a wrongdoing.
• A man is forced to search for his money.
• The man, after protesting profusely, picked an exer-
cise class to attend
• The vegetarian Sherpa had to eat the pepperoni pizza
or he would surely have died.
</listItem>
<bodyText confidence="0.9978281">
While these data are likely still interesting for other pur-
poses, their biased distribution is unlikely to be useful for
helping identify whether an arbitrary sentence in a text
expresses the volitional Transitivity attribute. The users
prefer to have an animate agent that is compelled to take
the action rather than create sentences where the action
happens accidentally or is undertaken by an abstract or
inanimate actor.
Similarly, for the aspect dimension, many users simply
chose to represent actions that had not been completed
</bodyText>
<page confidence="0.971314">
190
</page>
<table confidence="0.999788625">
Task Questions / Hit Pay Repetition Tasks Total
Video Object 5 0.04 5 10 $2.00
Video Subject 5 0.04 5 10 $2.00
Corpus Object 10 0.03 5 50 $7.50
Corpus Subject 10 0.03 5 50 $7.50
Video Elicitation 5 0.10 2 70 $14.00
Corpus Annotation 7 0.03 3 400 $36.00
Total $69.00
</table>
<tableCaption confidence="0.965634">
Table 2: The reward structure for the tasks presented in this paper (not including bonuses or MTurk overhead). “Video Subject” and
“Video Object” are where users were presented with a video and supplied the subjects and objects of the depicted actions. “Corpus
Subject” and “Corpus Object” are the tasks where users identified the subject and objects of sentences from Wikipedia. “Video
Elicitation” refers to the task where users were asked to write sentences with specified Transitivity properties. “Corpus Annotation”
is where users are presented with sentences with previously identified subjects and objects and must rate various dimensions of
Transitivity.
</tableCaption>
<bodyText confidence="0.999826923076923">
using the future tense. For the kinesis task, users dis-
played amazing creativity in inventing situations where
movement was correlated with the action. Unfortunately,
as before, these data are not useful in generating predic-
tive features for capturing the properties of Transitivity.
We hope to improve experiments and instructions to
better align everyday intuitions with the linguistic proper-
ties of interest. While we have found that extensive direc-
tions tend to discourage users, perhaps there are ways in-
crementally building or modifying sentences that would
allow us to elicit sentences with the desired Transitivity
properties. This is discussed further in the conclusion,
Section 4.
</bodyText>
<subsectionHeader confidence="0.99963">
3.2 Annotation Task
</subsectionHeader>
<bodyText confidence="0.9999745">
For the annotation task, we observed that users often had
a hard time keeping their focus on the words in question
and not incorporating additional knowledge. For exam-
ple, for each of the following sentences:
</bodyText>
<listItem confidence="0.995725555555556">
• Bonosus dealt with the eastern cities so harshly that
his severity was remembered centuries later.
• On the way there, however, Joe and Jake pick an-
other fight.
• The Black Sea was a significant naval theatre of
World War I and saw both naval and land battles
during World War II .
• Bush claimed that Zubaydah gave information that
lead to al Shibh ’s capture.
</listItem>
<bodyText confidence="0.998970571428571">
some users said that the objects in bold were greatly
harmed, suggesting that users felt even abstract concepts
could be harmed in these sentences. A rigorous inter-
pretation of the affectedness dimension would argue that
these abstract concepts were incapable of being harmed.
We suspect that the negative associations (severity, fight,
battles, capture) present in this sentence are causing users
to make connections to harm, thus creating these ratings.
Similarly, world knowledge flavored other questions,
such as kinesis, where users were able to understand from
context that the person doing the action probably moved
at some point near the time of the event, even if move-
ment wasn’t a part of the act of, for example, “calling” or
“loving.”
</bodyText>
<subsectionHeader confidence="0.999109">
3.3 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.999411">
For the annotation task, we were able to get consistent
ratings of transitivity. Table 3 shows the proportion of
sentences where two or more annotators agreed on the
a Transitivity label of the sentences for that dimension.
All of the dimensions were significantly better than ran-
dom chance agreement (0.52); the best was harm, which
has an accessible, clear, and intuitive definition, and the
worst was kinesis, which was more ambiguous and prone
to disagreement among raters.
</bodyText>
<table confidence="0.998872555555556">
Dimension Sentences
with Agreement
HARM 0.87
AFFIRMATION 0.86
VOLITION 0.86
PUNCTUALITY 0.81
BENEFIT 0.81
ASPECT 0.80
KINESIS 0.70
</table>
<tableCaption confidence="0.997083666666667">
Table 3: For each of the dimensions of transitivity, the propor-
tion of sentences where at least two of three raters agreed on the
label. Random chance agreement is 0.52.
</tableCaption>
<bodyText confidence="0.95906575">
Figure 4 shows a distribution for each of the Transitiv-
ity data on the Wikipedia corpus. These data are consis-
tent with what one would expect from random sentences
from an encyclopedic dataset; most of the sentences en-
</bodyText>
<page confidence="0.980713">
191
</page>
<figure confidence="0.995101157894737">
AFFIRMATION ASPECT BENEFIT HARM
Count
250
200
250
200
150
100
150
100
50
50
0
0
KINESIS
PUNCTUALITY
VOLITIONALITY
0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4
Median Score
</figure>
<figureCaption confidence="0.999927">
Figure 4: Histograms of median scores from raters by Transitivity dimension. Higher values represent greater levels of Transitivity.
</figureCaption>
<bodyText confidence="0.999947">
code truthful statements, most actions have been com-
pleted, most objects are not affected, most events are over
a long time span, and there is a bimodal distribution over
volition. One surprising result is that for kinesis there
is a fairly flat distribution. One would expect a larger
skew toward non-kinetic words. Qualitative analysis of
the data suggest that raters used real-world knowledge to
associate motion with the context of actions (even if mo-
tion is not a part of the action), and that raters were less
confident about their answers, prompting more hedging
and a flat distribution.
</bodyText>
<subsectionHeader confidence="0.999217">
3.4 Predicting Transitivity
</subsectionHeader>
<bodyText confidence="0.999896727272728">
We also performed an set of initial experiments to investi-
gate our ability to predict Transitivity values for held out
data. We extracted three sets of features from the sen-
tences: lexical features, syntactic features, and features
derived from WordNet (Miller, 1990).
Lexical Features A feature was created for each word
in a sentence after being stemmed using the Porter stem-
mer (Porter, 1980).
Syntactic Features We parsed each sentence using the
Stanford Parser (Klein and Manning, 2003) and used
heuristics to identify cases where the main verb is tran-
sitive, where the subject is a nominalization (e.g. “run-
ning”), or whether the sentence is passive. If any of these
constructions appear in the sentence, we generate a corre-
sponding feature. These represent features identified by
Greene and Resnik (2009).
WordNet Features For each word in the sentence, we
extracted all the possible senses for each word. If any
possible sense was a hyponym (i.e. an instance of) one
of: artifact, living thing, abstract entity, location, or food,
we added a feature corresponding to that top level synset.
For example, the string “Lincoln” could be an instance
of both a location (Lincoln, Nebraska) and a living thing
(Abe Lincoln), so a feature was added for both the loca-
tion and living thing senses. In addition to these noun-
based features, features were added for each of the pos-
sible verb frames allowed by each of a word’s possible
senses (Fellbaum, 1998).
At first, we performed simple 5-way classification and
found that we could not beat the most frequent class base-
line for any dimension. We then decided to simplify the
classification task to make binary predictions of low-vs-
high instead of fine gradations along the particular di-
mension. To do this, we took all the rated sentences for
each of the seven dimensions and divided the ratings into
low (ratings of 0-1) and high (ratings of 2-4) values for
that dimension. Table 4 shows the results for these bi-
nary classification experiments using different classifiers.
All of the classification experiments were conducted us-
ing the Weka machine learning toolkit (Hall et al., 2009)
and used 10-fold stratified cross validation.
Successfully rating Transitivity requires knowledge
beyond individual tokens. For example, consider kine-
sis. Judging kinesis requires lexical semantics to realize
whether a certain actor is capable of movement, pragmat-
ics to determine if the described situation permits move-
ment, and differentiating literal and figurative movement.
One source of real-world knowledge is WordNet;
adding some initial features from WordNet appears to
help aid some of these classifications. For example, clas-
sifiers trained on the volitionality data were not able to
do better than the most frequent class baseline before the
addition of WordNet-based features. This is a reasonable
result, as WordNet features help the algorithm generalize
which actors are capable of making decisions.
</bodyText>
<page confidence="0.991039">
192
</page>
<table confidence="0.9999325">
Dimension Makeup Classifier Accuracy
Baseline NB VP SVM
-WN +WN -WN +WN -WN +WN
HARM 269/35 88.5 83.9 84.9 87.2 87.8 88.5 88.5
AFFIRMATION 380/20 95.0 92.5 92.0 94.3 95.0 95.0 95.0
VOLITION 209/98 68.1 66.4 69.4 67.1 73.3 68.1 68.1
PUNCTUALITY 158/149 51.5 59.6 61.2 57.0 59.6 51.5 51.5
BENEFIT 220/84 72.4 69.1 65.1 73.4 71.4 72.4 72.4
ASPECT 261/46 85.0 76.5 74.3 81.1 84.7 85.0 85.0
KINESIS 160/147 52.1 61.2 61.2 56.4 60.9 52.1 52.1
</table>
<tableCaption confidence="0.9714746">
Table 4: The results of preliminary binary classification experiments for predicting various transitivity dimensions using different
classifiers such as Naive Bayes (NB), Voted Perceptron (VP) and Support Vector Machines (SVM). Classifier accuracies for two
sets of experiments are shown: without WordNet features (-WN) and with WordNet features (+WN). The baseline simply predicts
the most frequent class. For each dimension, the split between low Transitivity (rated 0-1) and high Transitivity (rated 2-4) is shown
under the “Makeup” column. All reported accuracies are using 10-fold stratified cross validation.
</tableCaption>
<sectionHeader confidence="0.998012" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999990059701493">
We began with the goal of capturing a subtle linguistic
property for which annotated datasets were not available.
We created a annotated dataset of 400 sentences taken
from the real-word dataset Wikipedia annotated for seven
different Transitivity properties. Users were able to give
consistent answers, and we collected results in a man-
ner that is relatively language independent. Once we ex-
pand and improve this data collection scheme for English,
we hope to perform similar data collection in other lan-
guages. We have available the translated versions of the
questions used in this study for Arabic and German.
Our elicitation task was not as successful as we had
hoped. We learned that while we could form tasks using
everyday language that we thought captured these sub-
tle linguistic properties, we also had many unspoken as-
sumptions that the creative workers on MTurk did not
necessarily share. As we articulated these assumptions
in increasingly long instruction sets to workers, the sheer
size of the instructions began to intimidate and scare off
workers.
While it seems unlikely we can strike a balance that
will give us the answers we want with the elegant instruc-
tions that workers need to feel comfortable for the tasks
as we currently defined them, we hope to modify the task
to embed further linguistic assumptions. For example, we
hope to pilot another version of the elicitation task where
workers modify an existing sentence to change one Tran-
sitivity dimension. Instead of reading and understanding
a plodding discussion of potentially irrelevant details, the
user can simply see a list of sentence versions that are not
allowed.
Our initial classification results suggest that we do not
yet have enough data to always detect these Transitiv-
ity dimensions from unlabeled text or that our algorithms
are using features that do not impart enough information.
It is also possible that using another corpus might yield
greater variation in Transitivity that would aid classifica-
tion; Wikipedia by design attempts to keep a neutral tone
and eschews the highly charged prose that would contain
a great deal of Transitivity.
Another possibility is that, instead of just the Transi-
tivity ratings alone, tweaks to the data collection process
could also help guide classification algorithms (Zaidan et
al., 2008). Thus, instead of clicking on a single annota-
tion label in our current data collection process, Turkers
would click on a data label and the word that most helped
them make a decision.
Our attempts to predict Transitivity are not exhaus-
tive, and there are a number of reasonable algorithms
and resources which could also be applied to the prob-
lem; for example, one might expect semantic role label-
ing or sense disambiguation to possibly aid the prediction
of Transitivity. Determining which techniques are effec-
tive and the reasons why they are effective would aid not
just in predicting Transitivity, which we believe to be an
interesting problem, but also in understanding Transitiv-
ity.
Using services like MTurk allows us to tighten the loop
between data collection, data annotation, and machine
learning and better understand difficult problems. We
hope to refine the data collection process to provide more
consistent results on useful sentences, build classifiers,
and extract features that are able to discover the Transi-
tivity of unlabeled text. We believe that our efforts will
help cast an interesting aspect of theoretical linguistics
into a more pragmatic setting and make it accessible for
use in more practical problems like sentiment analysis.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984447">
C. Fellbaum, 1998. WordNet : An Electronic Lexi-
cal Database, chapter A semantic network of English
</reference>
<page confidence="0.989827">
193
</page>
<reference confidence="0.9992867">
verbs. MIT Press, Cambridge, MA.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 503–
511.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
(56):251–299.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 423–430.
Xiaojuan Ma and Perry R. Cook. 2009. How well do
visual verbs work in daily communication for young
and old adults? In international conference on Human
factors in computing systems, pages 361–364.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245–264.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2008. Machine learning with annotator rationales
to reduce annotation cost. In Proceedings of the
NIPS*2008 Workshop on Cost Sensitive Learning,
Whistler, BC, December. 10 pages.
</reference>
<page confidence="0.99879">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999942">Measuring Transitivity Using Untrained Annotators</title>
<author confidence="0.9977">Jordan Philip</author>
<degree confidence="0.404820666666667">for Advanced Computer of Computer of</degree>
<affiliation confidence="0.750475">University of Maryland, College Park</affiliation>
<abstract confidence="0.99911458227848">Hopper and Thompson (1980) defined a multi-axis theory of transitivity that goes beyond simple syntactic transitivity and captures how much “action” takes place in a sentence. Detecting these features requires a deep understanding of lexical semantics and real-world pragmatics. We propose two general approaches for creating a corpus of sentences labeled with respect to the Hopper-Thompson transitivity schema using Amazon Mechanical Turk. Both approaches assume no existing resources and incorporate all necessary annotation into a single system; this is done to allow for future generalization to other languages. The first task attempts to use languageneutral videos to elicit human-composed sentences with specified transitivity attributes. The second task uses an iterative process to first label the actors and objects in sentences and then annotate the sentences’ transitivity. We examine the success of these techniques and perform a preliminary classification of the transitivity of held-out data. Hopper and Thompson (1980) created a multi-axis theof that describes the volition of the subject, the affectedness of the object, and the duration of the action. In short, this theory goes beyond the simple grammatical notion of transitivity (whether verbs take objects — transitive — or not — intransitive) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called “syntactic framing,” which expresses implicit sentiment (Greene and Resnik, 2009). use capital “T” to differentiate from conventional syntactic transitivity throughout the paper. In these contexts, the perspective or sentiment of the writer is reflected in the constructions used to express ideas. For example, a less Transitive construction might be used to deflect responsibility (e.g. “John was killed” vs. “Benjamin killed John”). In the rest of this paper, we review the Hopper- Thompson transitivity schema and propose two relatively language-neutral methods to collect Transitivity ratings. The first asks humans to generate sentences with desired Transitivity characteristics. The second asks humans to rate sentences on dimensions from the Hopper- Thompson schema. We then discuss the difficulties of collecting such linguistically deep data and analyze the available results. We then pilot an initial classifier on the Hopper-Thompson dimensions. 1 Transitivity Table 1 shows the subset of the Hopper-Thompson dimensions of Transitivity used in this study. We excluded noun-specific aspects as we felt that these were well covered by existing natural language processing (NLP) approaches (e.g. whether the object / subject is person, abstract entity, or abstract concept is handled well by existing named entity recognition systems) and also excluded aspects which we felt had significant overlap with the dimensions we were investigating (e.g. affirmation and mode). We also distinguished the original Hopper-Thompson “Affectedness” aspect into separate “Benefit” and “Harm” components, as we suspect that these data will be useful to other applications such as sentiment analysis. We believe that these dimensions of transitivity are simple and intuitive enough that they can be understood and labeled by the people on Amazon Mechanical Turk, a web service. Amazon Mechanical Turk (MTurk) allows individuals to post jobs on MTurk with a set fee that are then performed by workers on the Internet. MTurk connects workers to people with tasks and handles the coordination problems of payment and transferring data.</abstract>
<note confidence="0.634964666666667">188 of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical pages 188–194, Angeles, California, June 2010. Association for Computational Linguistics</note>
<abstract confidence="0.994911161290322">Kinesis Sentences where movement happens are perceived to be more Transitive. “Sue jumped out of an airplane” vs. “The corporation jumped to a silly conclusion.” Punctuality Sentences where the action happens quickly are perceived to be more Transitive. “She touched her ID to the scanner to enter” vs. “I was touched by how much she helped me.” Mode Sentences with no doubt about whether the action happened are perceived to be more Transitive. “Bob was too busy to fix the drain” vs. “Bob fixed the drain.” Affectedness Sentences where the object is more affected by the action are perceived to be more Transitive. “The St. Bernard saved the climber” vs. “Melanie looked at the model.” Volition Sentences where the actor chose to perform the action are perceived to be more Transitive. “Paul jumped out of the bushes and startled his poor sister” vs. “The picture startled George.” Aspect Sentences where the action is done to completion are perceived to be more Transitive. “Walter is eating the hamburger” vs. “Walter ate the pudding up.” Table 1: The Hopper-Thompson dimensions of transitivity addressed in this paper. In experiments, “Affectedness” was divided into “Harm” and “Benefit.” 2 Experiments Our goal is to create experiments for MTurk that will produce a large set of sentences with known values of Transitivity. With both experiments, we design the tasks to be as language independent as possible, thus not depending on language-specific preprocessing tools. This allows the data collection approach to be replicated in other languages. 2.1 Elicitation The first task is not corpus specific, and requires no language-specific resources. We represent verbs using videos (Ma and Cook, 2009). This also provides a form of language independent sense disambiguation. We display videos illustrating verbs (Figure 1) and ask users on MTurk to identify the action and give nouns that can do the action and — in a separate task — the nouns that the action can be done to. For quality control, Turkers must match a previous Turker’s response for one of their answers (a la the game show “Family Feud”). Figure 1: Stills from three videos depicting the verbs “receive,” “hear,” and “help.” We initially found that subjects had difficulty distinguishing what things could do the action (subjects) vs. what things the action could be done to (objects). In order to suggest the appropriate syntactic frame, we use javascript to form their inputs into protosentences as they typed. For example, if they identified an action as “picking” and suggested “fruit” as a possible object, the protosentence “it is picking fruit” is displayed below their input (Figure 2). This helped ensure consistent answers. The subject and object tasks were done separately, and for the object task, users were allowed to say that there is nothing the action can be done to (for example, for an intransitive verb). Figure 2: A screenshot of a user completing a task to find objects of a particular verb, where the verb is represented by a film. After the user has written a verb and a noun, a protosentence is formed and shown to ensure that the user is using the words in the appropriate roles. These subjects and objects we collected were then used as inputs for a second task. We showed workers videos with potential subjects and objects and asked them to create pairs of sentences with opposite Transitivity at- For example, a sentence where the thing which the action is done benefits a sentence where the thing to which the action is done is not affected the For both sides of the Transitivity dimension, we allowed users to say that writing such a sentence is impossible. We discuss the initial results of this task in Section 3. 2.2 Annotation Our second task—one of annotation—depends on having corpus available in the language of interest. For con- 189 creteness and availability, we use Wikipedia, a free multilingual encyclopedia. We extract a large pool of sentences from Wikipedia containing verbs of interest. We apply light preprocessing to remove long, unclear (e.g. starting with a pronoun), or uniquely Wikipedian sen- (e.g. very short sentences of the form “See Star Trek We construct tasks, each for a single verb, that ask users to identify the subject and obfor the verb in randomly selected Users were prompted by an interactive javascript guide (Figure 3) that instructed them to click on the first word of the subject (or object) and then to click on the last word that made up the subject (or object). After they clicked, a text box was automatically populated with their answer; this decreased errors and made the tasks easier to finish. For quality control, each HIT has a simple sentence where subject and object were already determined by the authors; the user must match the annotation on that sentence for credit. We ended up rejecting less than one percent of submitted hits. Figure 3: A screenshot of the subject identification task. The user has to click on the phrase that they believe is the subject. Once objects and subjects have been identified, other users rate the sentence’s Transitivity by answering the following questions like, where $VERB represents the verb of interest, $SUBJ is its subject and $OBJ is its ob- • After reading this sentence, do you know that $SUBJ is done $VERBing? • From reading the sentence, how certain are you that $VERBing happened? • How much did $OBJ benefit? • How much was $OBJ harmed? • Did $SUBJ move? • If you were to film $SUBJ’s act of $VERBing in its entirety, how long would the movie be? • Did the $SUBJ make a conscious choice to $VERB? The answers were on a scale of 0 to 4 (higher numbers meant the sentence evinced more of the property in goal of language independence and the unreliable correspondence between syntax and semantic roles precludes automatic labeling of the subjects and objects. questions were developed using Greene and Resnik’s (2009) surveys as a foundation. question), and each point in the scale had a description to anchor raters and to ensure consistent results. 2.3 Rewards Table 2 summarizes the rewards for the tasks used in these experiments. Rewards were set at the minimal rate that could attract sufficient interest from users. For the “Video Elicitation” task, where users wrote sentences with specified Transitivity properties, we also offered bonuses for clever, clear sentences. However, this was our least popular task, and we struggled to attract users. 3 Results and Discussion 3.1 Creative but Unusable Elicitation Results We initially thought that we would have difficulty coaxing users to provide full sentences. This turned out not to be the case. We had no difficulty getting (very imaginative) sentences, but the sentences were often inconsistent with the Transitivity aspects we are interested in. This shows both the difficulty of writing concise instructions for non-experts and the differences between everyday meanings of words and their meaning in linguistic contexts. For example, the “volitional” elicitation task asked people to create sentences where the subject made a conscious decision to perform the action. In the cases where we asked users to create sentences where the subject did not make a conscious decision to perform an action, almost all of the sentences created by users focused on sentences where a person (rather than employ other tactics such as using a less individuated subject, e.g. replacing “Bob” with “freedom”) was performing the action and was coerced into doing the action. For example: • Sellers often give gifts to their clients when they are trying to make up for a wrongdoing. • A man is forced to search for his money. • The man, after protesting profusely, picked an exercise class to attend • The vegetarian Sherpa had to eat the pepperoni pizza or he would surely have died. While these data are likely still interesting for other purposes, their biased distribution is unlikely to be useful for helping identify whether an arbitrary sentence in a text expresses the volitional Transitivity attribute. The users prefer to have an animate agent that is compelled to take the action rather than create sentences where the action happens accidentally or is undertaken by an abstract or inanimate actor.</abstract>
<note confidence="0.870361">Similarly, for the aspect dimension, many users simply chose to represent actions that had not been completed 190 Task Questions / Hit Pay Repetition Tasks Total Video Object 5 0.04 5 10 $2.00 Video Subject 5 0.04 5 10 $2.00 Corpus Object 10 0.03 5 50 $7.50 Corpus Subject 10 0.03 5 50 $7.50 Video Elicitation 5 0.10 2 70 $14.00 Corpus Annotation 7 0.03 3 400 $36.00 Total $69.00 Table 2: The reward structure for the tasks presented in this paper (not including bonuses or MTurk overhead). “Video Subject” and “Video Object” are where users were presented with a video and supplied the subjects and objects of the depicted actions. “Corpus Subject” and “Corpus Object” are the tasks where users identified the subject and objects of sentences from Wikipedia. “Video Elicitation” refers to the task where users were asked to write sentences with specified Transitivity properties. “Corpus Annotation”</note>
<abstract confidence="0.949669">is where users are presented with sentences with previously identified subjects and objects and must rate various dimensions of Transitivity. using the future tense. For the kinesis task, users displayed amazing creativity in inventing situations where movement was correlated with the action. Unfortunately, as before, these data are not useful in generating predictive features for capturing the properties of Transitivity. We hope to improve experiments and instructions to better align everyday intuitions with the linguistic properties of interest. While we have found that extensive directions tend to discourage users, perhaps there are ways incrementally building or modifying sentences that would allow us to elicit sentences with the desired Transitivity properties. This is discussed further in the conclusion, Section 4. 3.2 Annotation Task For the annotation task, we observed that users often had a hard time keeping their focus on the words in question and not incorporating additional knowledge. For example, for each of the following sentences: • Bonosus dealt with the eastern cities so harshly that remembered centuries later. • On the way there, however, Joe and Jake pick an- • The Black Sea was a significant naval theatre of War I and saw both and land battles during World War II . Bush claimed that Zubaydah gave lead to al Shibh ’s capture. users said that the objects in greatly harmed, suggesting that users felt even abstract concepts could be harmed in these sentences. A rigorous interpretation of the affectedness dimension would argue that these abstract concepts were incapable of being harmed. We suspect that the negative associations (severity, fight, battles, capture) present in this sentence are causing users to make connections to harm, thus creating these ratings. Similarly, world knowledge flavored other questions, such as kinesis, where users were able to understand from context that the person doing the action probably moved at some point near the time of the event, even if movement wasn’t a part of the act of, for example, “calling” or “loving.” 3.3 Quantitative Results For the annotation task, we were able to get consistent ratings of transitivity. Table 3 shows the proportion of sentences where two or more annotators agreed on the a Transitivity label of the sentences for that dimension. All of the dimensions were significantly better than random chance agreement (0.52); the best was harm, which has an accessible, clear, and intuitive definition, and the worst was kinesis, which was more ambiguous and prone to disagreement among raters. Dimension with Agreement HARM 0.87 AFFIRMATION 0.86 VOLITION 0.86 PUNCTUALITY 0.81 BENEFIT 0.81 ASPECT 0.80 KINESIS 0.70 Table 3: For each of the dimensions of transitivity, the proportion of sentences where at least two of three raters agreed on the label. Random chance agreement is 0.52. Figure 4 shows a distribution for each of the Transitivity data on the Wikipedia corpus. These data are consistent with what one would expect from random sentences an encyclopedic dataset; most of the sentences en-</abstract>
<note confidence="0.603189684210526">191 AFFIRMATION ASPECT BENEFIT HARM Count 250 200 250 200 150 100 150 100 50 50 0 0 KINESIS PUNCTUALITY VOLITIONALITY 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4</note>
<author confidence="0.784349">Median Score</author>
<abstract confidence="0.997222176470589">Figure 4: Histograms of median scores from raters by Transitivity dimension. Higher values represent greater levels of Transitivity. code truthful statements, most actions have been completed, most objects are not affected, most events are over a long time span, and there is a bimodal distribution over volition. One surprising result is that for kinesis there is a fairly flat distribution. One would expect a larger skew toward non-kinetic words. Qualitative analysis of the data suggest that raters used real-world knowledge to associate motion with the context of actions (even if motion is not a part of the action), and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Features feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Features parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). Features each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one or we added a feature corresponding to that top level synset. For example, the string “Lincoln” could be an instance both a Nebraska) and a thing Lincoln), so a feature was added for both the locathing In addition to these nounbased features, features were added for each of the possible verb frames allowed by each of a word’s possible senses (Fellbaum, 1998). At first, we performed simple 5-way classification and found that we could not beat the most frequent class baseline for any dimension. We then decided to simplify the classification task to make binary predictions of low-vshigh instead of fine gradations along the particular dimension. To do this, we took all the rated sentences for each of the seven dimensions and divided the ratings into low (ratings of 0-1) and high (ratings of 2-4) values for that dimension. Table 4 shows the results for these binary classification experiments using different classifiers. All of the classification experiments were conducted using the Weka machine learning toolkit (Hall et al., 2009) and used 10-fold stratified cross validation. Successfully rating Transitivity requires knowledge beyond individual tokens. For example, consider kinesis. Judging kinesis requires lexical semantics to realize whether a certain actor is capable of movement, pragmatics to determine if the described situation permits movement, and differentiating literal and figurative movement. One source of real-world knowledge is WordNet; adding some initial features from WordNet appears to help aid some of these classifications. For example, classifiers trained on the volitionality data were not able to do better than the most frequent class baseline before the addition of WordNet-based features. This is a reasonable result, as WordNet features help the algorithm generalize which actors are capable of making decisions.</abstract>
<note confidence="0.521786">192</note>
<title confidence="0.775694">Dimension Makeup Classifier Accuracy Baseline NB VP SVM -WN +WN -WN +WN -WN +WN</title>
<note confidence="0.460926142857143">HARM 269/35 88.5 83.9 84.9 87.2 87.8 88.5 88.5 AFFIRMATION 380/20 95.0 92.5 92.0 94.3 95.0 95.0 95.0 VOLITION 209/98 68.1 66.4 69.4 67.1 73.3 68.1 68.1 PUNCTUALITY 158/149 51.5 59.6 61.2 57.0 59.6 51.5 51.5 BENEFIT 220/84 72.4 69.1 65.1 73.4 71.4 72.4 72.4 ASPECT 261/46 85.0 76.5 74.3 81.1 84.7 85.0 85.0 KINESIS 160/147 52.1 61.2 61.2 56.4 60.9 52.1 52.1</note>
<abstract confidence="0.999756493150685">Table 4: The results of preliminary binary classification experiments for predicting various transitivity dimensions using different classifiers such as Naive Bayes (NB), Voted Perceptron (VP) and Support Vector Machines (SVM). Classifier accuracies for two sets of experiments are shown: without WordNet features (-WN) and with WordNet features (+WN). The baseline simply predicts the most frequent class. For each dimension, the split between low Transitivity (rated 0-1) and high Transitivity (rated 2-4) is shown under the “Makeup” column. All reported accuracies are using 10-fold stratified cross validation. 4 Conclusion We began with the goal of capturing a subtle linguistic property for which annotated datasets were not available. We created a annotated dataset of 400 sentences taken from the real-word dataset Wikipedia annotated for seven different Transitivity properties. Users were able to give consistent answers, and we collected results in a manner that is relatively language independent. Once we expand and improve this data collection scheme for English, we hope to perform similar data collection in other languages. We have available the translated versions of the questions used in this study for Arabic and German. Our elicitation task was not as successful as we had hoped. We learned that while we could form tasks using everyday language that we thought captured these subtle linguistic properties, we also had many unspoken assumptions that the creative workers on MTurk did not necessarily share. As we articulated these assumptions in increasingly long instruction sets to workers, the sheer size of the instructions began to intimidate and scare off workers. While it seems unlikely we can strike a balance that will give us the answers we want with the elegant instructions that workers need to feel comfortable for the tasks as we currently defined them, we hope to modify the task to embed further linguistic assumptions. For example, we hope to pilot another version of the elicitation task where workers modify an existing sentence to change one Transitivity dimension. Instead of reading and understanding a plodding discussion of potentially irrelevant details, the user can simply see a list of sentence versions that are not allowed. Our initial classification results suggest that we do not yet have enough data to always detect these Transitivity dimensions from unlabeled text or that our algorithms are using features that do not impart enough information. It is also possible that using another corpus might yield greater variation in Transitivity that would aid classification; Wikipedia by design attempts to keep a neutral tone and eschews the highly charged prose that would contain a great deal of Transitivity. Another possibility is that, instead of just the Transitivity ratings alone, tweaks to the data collection process could also help guide classification algorithms (Zaidan et al., 2008). Thus, instead of clicking on a single annotation label in our current data collection process, Turkers click on a data label word that most helped them make a decision. Our attempts to predict Transitivity are not exhaustive, and there are a number of reasonable algorithms and resources which could also be applied to the problem; for example, one might expect semantic role labeling or sense disambiguation to possibly aid the prediction of Transitivity. Determining which techniques are effective and the reasons why they are effective would aid not just in predicting Transitivity, which we believe to be an problem, but also in Transitivity. Using services like MTurk allows us to tighten the loop between data collection, data annotation, and machine learning and better understand difficult problems. We hope to refine the data collection process to provide more consistent results on useful sentences, build classifiers, and extract features that are able to discover the Transitivity of unlabeled text. We believe that our efforts will help cast an interesting aspect of theoretical linguistics into a more pragmatic setting and make it accessible for use in more practical problems like sentiment analysis.</abstract>
<note confidence="0.776020428571429">References Fellbaum, 1998. : An Electronic Lexichapter A semantic network of English 193 verbs. MIT Press, Cambridge, MA. Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Conference of the North American Chapter of the Asfor Computational pages 503– 511. Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. 11(1). Paul J. Hopper and Sandra A. Thompson. 1980. in grammar and discourse. (56):251–299. Dan Klein and Christopher D. Manning. 2003. Accuunlexicalized parsing. In of the Assofor Computational pages 423–430. Xiaojuan Ma and Perry R. Cook. 2009. How well do</note>
<abstract confidence="0.843284538461539">visual verbs work in daily communication for young old adults? In conference on Human in computing pages 361–364. George A. Miller. 1990. Nouns in WordNet: A lexical system. Journal of Lexicog- 3(4):245–264. M. F. Porter. 1980. An algorithm for suffix stripping. 14(3):130–137. Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008. Machine learning with annotator rationales reduce annotation cost. In of the Workshop on Cost Sensitive Whistler, BC, December. 10 pages.</abstract>
<intro confidence="0.461357">194</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet : An Electronic Lexical Database, chapter A semantic network of English verbs.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="19404" citStr="Fellbaum, 1998" startWordPosition="3147" endWordPosition="3148">word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living thing, abstract entity, location, or food, we added a feature corresponding to that top level synset. For example, the string “Lincoln” could be an instance of both a location (Lincoln, Nebraska) and a living thing (Abe Lincoln), so a feature was added for both the location and living thing senses. In addition to these nounbased features, features were added for each of the possible verb frames allowed by each of a word’s possible senses (Fellbaum, 1998). At first, we performed simple 5-way classification and found that we could not beat the most frequent class baseline for any dimension. We then decided to simplify the classification task to make binary predictions of low-vshigh instead of fine gradations along the particular dimension. To do this, we took all the rated sentences for each of the seven dimensions and divided the ratings into low (ratings of 0-1) and high (ratings of 2-4) values for that dimension. Table 4 shows the results for these binary classification experiments using different classifiers. All of the classification exper</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum, 1998. WordNet : An Electronic Lexical Database, chapter A semantic network of English verbs. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<contexts>
<context position="2243" citStr="Greene and Resnik, 2009" startWordPosition="315" endWordPosition="318">e) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called “syntactic framing,” which expresses implicit sentiment (Greene and Resnik, 2009). 1We use capital “T” to differentiate from conventional syntactic transitivity throughout the paper. In these contexts, the perspective or sentiment of the writer is reflected in the constructions used to express ideas. For example, a less Transitive construction might be used to deflect responsibility (e.g. “John was killed” vs. “Benjamin killed John”). In the rest of this paper, we review the HopperThompson transitivity schema and propose two relatively language-neutral methods to collect Transitivity ratings. The first asks humans to generate sentences with desired Transitivity characteris</context>
<context position="18761" citStr="Greene and Resnik (2009)" startWordPosition="3034" endWordPosition="3037">l features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living thing, abstract entity, location, or food, we added a feature corresponding to that top level synset. For example, the string “Lincoln” could be an instance of both a location (Lincoln, Nebraska) and a living thing (Abe Lincoln), so a feature was added for both the location and living thing senses. In addition to these nounbased features, features were added for each of the possible verb frames allowed by each o</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 503– 511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="20085" citStr="Hall et al., 2009" startWordPosition="3257" endWordPosition="3260">that we could not beat the most frequent class baseline for any dimension. We then decided to simplify the classification task to make binary predictions of low-vshigh instead of fine gradations along the particular dimension. To do this, we took all the rated sentences for each of the seven dimensions and divided the ratings into low (ratings of 0-1) and high (ratings of 2-4) values for that dimension. Table 4 shows the results for these binary classification experiments using different classifiers. All of the classification experiments were conducted using the Weka machine learning toolkit (Hall et al., 2009) and used 10-fold stratified cross validation. Successfully rating Transitivity requires knowledge beyond individual tokens. For example, consider kinesis. Judging kinesis requires lexical semantics to realize whether a certain actor is capable of movement, pragmatics to determine if the described situation permits movement, and differentiating literal and figurative movement. One source of real-world knowledge is WordNet; adding some initial features from WordNet appears to help aid some of these classifications. For example, classifiers trained on the volitionality data were not able to do b</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Hopper</author>
<author>Sandra A Thompson</author>
</authors>
<date>1980</date>
<booktitle>Transitivity in grammar and discourse. Language,</booktitle>
<pages>56--251</pages>
<contexts>
<context position="1322" citStr="Hopper and Thompson (1980)" startWordPosition="180" endWordPosition="183">ma using Amazon Mechanical Turk. Both approaches assume no existing resources and incorporate all necessary annotation into a single system; this is done to allow for future generalization to other languages. The first task attempts to use languageneutral videos to elicit human-composed sentences with specified transitivity attributes. The second task uses an iterative process to first label the actors and objects in sentences and then annotate the sentences’ transitivity. We examine the success of these techniques and perform a preliminary classification of the transitivity of held-out data. Hopper and Thompson (1980) created a multi-axis theory of Transitivity1 that describes the volition of the subject, the affectedness of the object, and the duration of the action. In short, this theory goes beyond the simple grammatical notion of transitivity (whether verbs take objects — transitive — or not — intransitive) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications w</context>
</contexts>
<marker>Hopper, Thompson, 1980</marker>
<rawString>Paul J. Hopper and Sandra A. Thompson. 1980. Transitivity in grammar and discourse. Language, (56):251–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="18441" citStr="Klein and Manning, 2003" startWordPosition="2982" endWordPosition="2985">and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living thing, abstract entity, location, or food, we added a feature corresponding to that top level s</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojuan Ma</author>
<author>Perry R Cook</author>
</authors>
<title>How well do visual verbs work in daily communication for young and old adults?</title>
<date>2009</date>
<booktitle>In international conference on Human factors in computing systems,</booktitle>
<pages>361--364</pages>
<contexts>
<context position="6208" citStr="Ma and Cook, 2009" startWordPosition="936" endWordPosition="939">ansitivity addressed in this paper. In experiments, “Affectedness” was divided into “Harm” and “Benefit.” 2 Experiments Our goal is to create experiments for MTurk that will produce a large set of sentences with known values of Transitivity. With both experiments, we design the tasks to be as language independent as possible, thus not depending on language-specific preprocessing tools. This allows the data collection approach to be replicated in other languages. 2.1 Elicitation The first task is not corpus specific, and requires no language-specific resources. We represent verbs using videos (Ma and Cook, 2009). This also provides a form of language independent sense disambiguation. We display videos illustrating verbs (Figure 1) and ask users on MTurk to identify the action and give nouns that can do the action and — in a separate task — the nouns that the action can be done to. For quality control, Turkers must match a previous Turker’s response for one of their answers (a la the game show “Family Feud”). Figure 1: Stills from three videos depicting the verbs “receive,” “hear,” and “help.” We initially found that subjects had difficulty distinguishing what things could do the action (subjects) vs.</context>
</contexts>
<marker>Ma, Cook, 2009</marker>
<rawString>Xiaojuan Ma and Perry R. Cook. 2009. How well do visual verbs work in daily communication for young and old adults? In international conference on Human factors in computing systems, pages 361–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Nouns in WordNet: A lexical inheritance system.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="18217" citStr="Miller, 1990" startWordPosition="2948" endWordPosition="2949">a larger skew toward non-kinetic words. Qualitative analysis of the data suggest that raters used real-world knowledge to associate motion with the context of actions (even if motion is not a part of the action), and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we ext</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. Nouns in WordNet: A lexical inheritance system. International Journal of Lexicography, 3(4):245–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="18345" citStr="Porter, 1980" startWordPosition="2970" endWordPosition="2971">ate motion with the context of actions (even if motion is not a part of the action), and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Machine learning with annotator rationales to reduce annotation cost.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning,</booktitle>
<volume>10</volume>
<pages>pages.</pages>
<location>Whistler, BC,</location>
<contexts>
<context position="24291" citStr="Zaidan et al., 2008" startWordPosition="3915" endWordPosition="3918">o not yet have enough data to always detect these Transitivity dimensions from unlabeled text or that our algorithms are using features that do not impart enough information. It is also possible that using another corpus might yield greater variation in Transitivity that would aid classification; Wikipedia by design attempts to keep a neutral tone and eschews the highly charged prose that would contain a great deal of Transitivity. Another possibility is that, instead of just the Transitivity ratings alone, tweaks to the data collection process could also help guide classification algorithms (Zaidan et al., 2008). Thus, instead of clicking on a single annotation label in our current data collection process, Turkers would click on a data label and the word that most helped them make a decision. Our attempts to predict Transitivity are not exhaustive, and there are a number of reasonable algorithms and resources which could also be applied to the problem; for example, one might expect semantic role labeling or sense disambiguation to possibly aid the prediction of Transitivity. Determining which techniques are effective and the reasons why they are effective would aid not just in predicting Transitivity</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2008</marker>
<rawString>Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008. Machine learning with annotator rationales to reduce annotation cost. In Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning, Whistler, BC, December. 10 pages.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>