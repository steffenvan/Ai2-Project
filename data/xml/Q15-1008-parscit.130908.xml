<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<title confidence="0.997881">
A Bayesian Model of Grounded Color Semantics
</title>
<author confidence="0.993767">
Brian McMahan
</author>
<affiliation confidence="0.968487">
Rutgers University
</affiliation>
<email confidence="0.997421">
brian.mcmahan@rutgers.edu
</email>
<author confidence="0.916106">
Matthew Stone
</author>
<affiliation confidence="0.898209">
Rutgers University
</affiliation>
<email confidence="0.975586">
matthew.stone@rutgers.edu
</email>
<figure confidence="0.941655166666667">
0
Hue
Entropy (bits)
4
6
5
</figure>
<page confidence="0.599426">
3
2
1
</page>
<sectionHeader confidence="0.957929" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999646909090909">
Natural language meanings allow speakers to
encode important real-world distinctions, but
corpora of grounded language use also re-
veal that speakers categorize the world in
different ways and describe situations with
different terminology. To learn meanings
from data, we therefore need to link underly-
ing representations of meaning to models of
speaker judgment and speaker choice. This
paper describes a new approach to this prob-
lem: we model variability through uncertainty
in categorization boundaries and distributions
over preferred vocabulary. We apply the ap-
proach to a large data set of color descrip-
tions, where statistical evaluation documents
its accuracy. The results are available as a
Lexicon of Uncertain Color Standards (LUX),
which supports future efforts in grounded lan-
guage understanding and generation by prob-
abilistically mapping 829 English color de-
scriptions to potentially context-sensitive re-
gions in HSV color space.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949818181818">
To ground natural language semantics in real-world
data at large scale requires researchers to confront
the vocabulary problem (Furnas et al., 1987). Much
of what people say falls in a long tail of increas-
ingly infrequent and specialized items. Moreover,
the choice of how to categorize and describe real-
world data varies across people. We can’t account
for this complexity by deriving one definitive map-
ping between words and the world.
We see this complexity already in free text de-
scriptions of color patches. English has fewer than
</bodyText>
<figureCaption confidence="0.929100142857143">
Figure 1: A visualization of the variability of the de-
scriptions used to name colors within small bins of color
space. For each Hue value, the entropy values for
each bin along the Saturation and Value dimensions are
grouped and plotted as box plots. The dotted line cor-
responds to a random choice out of fourteen items and to
the perplexity of a histogram model trained on the corpus.
</figureCaption>
<bodyText confidence="0.999881">
a dozen basic color words (Berlin, 1991), but peo-
ple’s descriptions of colors are much more variable
than this would suggest. Measured on the corpus
described in Section 4.1, there’s an average of 3.845
bits of information in a color description given the
color it describes—comparable to rolling a 14-sided
die. Figure 1 summarizes the data and plots the en-
tropy of descriptions encountered within small bins
of color space. The bins are aggregated over the Sat-
uration and Value dimensions and indexed on the x-
axis by the Hue dimension. There’s little reason to
think that this variability conceals consistent mean-
ings. In formal semantics, one of the hallmarks of
vague language is that speakers can make it more
precise in alternative, incompatible ways (Barker,
2002). We see this in practice as well, for exam-
ple with the image of Figure 2, where subjects com-
</bodyText>
<page confidence="0.99416">
103
</page>
<note confidence="0.598684">
Transactions of the Association for Computational Linguistics, vol. 3, pp. 103–115, 2015. Action Editor: Lillian Lee.
Submission batch: 11/2014; Published 2/2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.952745">
Figure 2: Image by flickr user Joanne Bacon (jlbacon)
from the data set of Young et al. (2014), whose subjects
describe these dogs as a brown dog and a tan one or a tan
dog and a white one.
</figureCaption>
<bodyText confidence="0.999676150943397">
prehensibly describe either of two dogs as the tan
one. Systems that robustly understand or generate
descriptions of colors in situated dialogue need mod-
els of meaning that capture this variability.
This paper makes two key contributions towards
this challenge. First, we present a methodology
to infer a corpus-based model of meaning that ac-
counts for possible differences in word usage across
different speakers. As we explain in Section 2,
our approach differs from the typical perspective in
grounded semantics (Tellex et al., 2011a; Matuszek
et al., 2012; Krishnamurthy and Kollar, 2013),
where a meaning is reduced to a single classifier that
collapses patterns of variation. Instead, our model
allows for variability in meaning by positing uncer-
tainty in classification boundaries that can get re-
solved when a speaker chooses to use a word on a
specific occasion. We explain the model and its the-
oretical rationale in Section 3.
Second, we develop and release a Lexicon of
Uncertain Color Standards (LUX) by applying our
methodology to color descriptions. LUX is an inter-
pretation of 829 distinct English color descriptions
as distributions over regions of the Hue–Saturation-
Value color space that describe their possible mean-
ings. As we describe in Section 4, the model is
trained by machine learning methods from a subset
of Randall Munroe’s 2010 publicly-available cor-
pus of 3.4 million crowdsourced free-text descrip-
tions of color patches (Munroe, 2010). Data, models
and visualization software are available at http:
//mcmahan.io/lux/.
Statistical evaluation of our model against two
alternative approaches documents its effectiveness.
The model makes better quantitative predictions
than a brute-force memorization model; it seems to
generalize to unseen data in more meaningful ways.
At the same time, our meanings work as well as
special-purpose models to explain speaker choice,
even though our model supports diverse other rea-
soning. See Section 5.
We see color as the first of many applications of
our methodology, and are optimistic about learn-
ing vague meanings for other continuous domains
as quantity, space, and time. At the same time,
the methodology opens up new prospects for re-
search on negotiating meaning interactively (Lars-
son, 2013) with principled representations and with
broad coverage. In fact, many practical situated dia-
logue systems already identify unfamiliar objects by
color. We expect that LUX will provide a broadly
useful resource to extend the range of descriptions
such systems can generate and understand.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.987621076923077">
Grounded semantics is the task of mapping rep-
resentations of linguistic meaning to the physical
world, whether by perceptual mechanisms (Har-
nad, 1990) or with the assistance of social inter-
action (DeVault et al., 2006). In this paper, we
are particularly concerned with grounding the mean-
ings of primitive vocabulary. However, the ulti-
mate test of grounded semantics—whether it is un-
derstanding commands (Winograd, 1970; Tellex et
al., 2011b), describing states of the world (Chen and
Mooney, 2008), or identifying objects (Matuszek et
al., 2012; Krishnamurthy and Kollar, 2013; Dawson
et al., 2013)—is the ability to interpret or generate
utterances using lexical and compositional seman-
tics so as to evoke appropriate real-world referents.
Grounded semantics therefore involves more than
just quantifying the associations between words and
perceptual representations, as Chuang et al. (2008)
and Heer and Stone (2012) do for color. Grounded
semantics involves interpreting semantic primitives
in terms of composable categories that let systems
discriminate between cases where a word applies
and cases where the word does not apply. (Our eval-
uation compares models of grounded semantics to
more direct models of word–world associations.)
Previous research has modeled these categories as
</bodyText>
<page confidence="0.99846">
104
</page>
<bodyText confidence="0.999961895522388">
regions of suitable perceptual feature spaces. Re-
searchers have explored explicit spaces of high-level
perceptual attributes (Farhadi et al., 2009; Silberer
et al., 2013), approximations to such spaces (Ma-
tuszek et al., 2012), or low-level feature spaces such
as Bag of Visual Words (Bruni et al., 2012) or
Histogram of Gradients (Krishnamurthy and Kollar,
2013). We specifically follow G¨ardenfors (2000)
and J¨ager (2010) in assuming that color categories
are convex regions in an underlying color space, and
are not just determined by prototypical color values,
such as in Andreas and Klein (2014).
However, unlike previous grounded semantics,
we do not assume that words name categories un-
equivocally. Speakers may vary in how they inter-
pret a word, so we treat the link between words and
categories probabilistically. The difference makes
training our model more indirect than previous ap-
proaches to grounded meaning. In particular, our
model introduces a new layer of uncertainty that de-
scribes what category the speaker uses.
Similar kinds of uncertainty can be found in
Bayesian models of speaker strategy, such as that
of Smith et al. (2013). However, this research has
assumed that speakers aim to be as informative as
possible. We have no evidence that our speakers do
that. We assume only that speakers’ utterances are
reliable and mirror prevailing usage.
Prior work by cognitive scientists has studied
color terms extensively, but focused on basic ones—
monolexemic, top-level color words with general
application and high frequency in a language (Kay et
al., 2009; Lammens, 1994). These color categories
seem to shape people’s expectations and memory
for colors (Persaud and Hemmer, 2014), and pat-
terns of color naming can therefore enhance soft-
ware for helping people organize and interact with
color (Chuang et al., 2008; Heer and Stone, 2012).
Moreover, crosslinguistic evidence suggests that the
human perceptual system places strong biases on
the meanings of the basic color terms (Regier et al.,
2005), perhaps because basic terms must partition
the perceptual space in an efficient way (Regier et
al., 2007). We depart from research on basic color
naming in considering a much wider range of terms,
much like Andreas and Klein (2014). We consider
subordinate, non-basic terms like beige or lavender;
modified colors like light blue or bright green; and
named subcategories like olive green, navy blue or
brick red.
In order to use semantic primitives for under-
standing, it’s necessary to combine them into an
integrated sentence-level representation: this is the
problem of semantic parsing. Semantic parsers can
be built by hand (Winograd, 1970), induced through
inductive logic programming (Zelle and Mooney,
1996), or treated as a structured classification prob-
lem (Zettlemoyer and Collins, 2005). Once a suit-
able logical form is derived, interpretation typically
involves a recursive process of finding referents that
fit lexical categories and relationships (Mavridis and
Roy, 2006; Tellex et al., 2011a). While this pa-
per does not explicitly address how our meanings
might be used in conjunction with such techniques,
we see no fundamental obstacle to doing so—for ex-
ample, by resolving references probabilistically and
marginalizing over uncertainty in meaning.
</bodyText>
<sectionHeader confidence="0.888708" genericHeader="method">
3 Using Vague Color Terms: A Model
</sectionHeader>
<bodyText confidence="0.999982923076923">
Our model involves two significant innovations over
previous approaches to grounded meaning. The
first is to capture the vagueness and flexibility of
grounded meaning with semantic representations
that treat meaning as uncertain. We represent the
semantics of a color description with a distribu-
tion over color categories, which weights possible
meanings by the relative likelihood of a speaker us-
ing this meaning on any particular occasion. For
example, speakers might associate yellowish green
with a range of possible meanings, differing in how
far the color category extends into green hues. By
representing uncertainty about meaning, our model
makes room to capture variability in language use.
For example, it implicitly quantifies how likely
speakers are to use words differently, as with the two
interpretations of tan in Figure 2.
Our second contribution is our simple model of
the relationship between semantics and pragmatics.
We assume that speakers’ choices mirror established
patterns. In particular, the model learns a measure
of availability for each color term that tracks how
frequently speakers tend to use it when it is appli-
cable. For example, although the expressions yel-
lowish green and chartreuse are associated with very
similar color categories, people say yellowish green
</bodyText>
<page confidence="0.998743">
105
</page>
<bodyText confidence="0.999969857142857">
much more often: it has a higher availability. Empir-
ically, we find few terms with high availability and
a long tail of terms with lower availabilities. We as-
sume speakers simply sample applicable terms from
this distribution, which predicts the long tail of ob-
served responses.
Mathematically, we develop our approach
through the rational analysis methodology for
explaining human behavior proposed by Anderson
(1991), along with methodological insights from
the linguistics and philosophy of vagueness. In the
remainder of this section, we explain the theoretical
antecedents in perceptual science, linguistics and
cognitive modeling that inform our approach.
</bodyText>
<subsectionHeader confidence="0.996309">
3.1 Color Categories
</subsectionHeader>
<bodyText confidence="0.999958648648649">
Color can be defined as sensations by which the per-
ceptual system tracks the diffuse reflectance of ob-
jects, despite variability, uncertainty and ambiguity
in the visual input. Red, green, and blue cones in
the retina allow the visual system to coarsely es-
timate frequency bands in the spectrum of incom-
ing light. Cameras and screens that use the red–
green–blue (RGB) color space are designed roughly
to correspond to these responses. However, colors in
the visual system summarize spectral profiles rather
than mere wavelengths of light. For example, we see
colors like cyan (green plus blue without red), ma-
genta (blue plus red without green) and yellow (red
plus green without blue) as intermediate saturated
colors between the familiar primaries. This natu-
rally leads to a wheel of hues describing the relative
prominence of different spectral components along
a continuum. Fairchild (2013) provides an overview
of color appearance.
To capture this variation, we’ll work in the sim-
ple hue–saturation–value (HSV) color space that’s
common in computer graphics and color picker user
interfaces (Hughes et al., 2013) and implemented in
python’s native colorsys package. This coordinate
system represents colors with three distinct qualita-
tive dimensions: Hue (H) represents changes in tint
around a color wheel, Saturation (S) represents the
relative proportion of color versus gray, and Value
(V) represents the location on the white–black con-
tinuum. We will associate color categories with rect-
angular box-shaped regions in HSV space. More
sophisticated color spaces have been developed to
describe the psychophysics of color more precisely,
but they depend on the photometric illumination and
other aspects of the viewing context that were not
controlled in the collection of the data we are using
(Fairchild, 2013).
</bodyText>
<subsectionHeader confidence="0.999822">
3.2 Semantic Representation
</subsectionHeader>
<bodyText confidence="0.953898">
Our assumption is that color terms are associated
probabilistically with color categories. We illustrate
the idea for the color label yellowish-green through
the plot in Figure 3. The plot shows variation in use
of the term across the Hue dimension: the bar graph
is a scaled histogram of the responses in the data we
use. There is a range of colors where people use yel-
lowish green often, surrounded by borderline cases
where it becomes increasingly infrequent.
Hue
</bodyText>
<figureCaption confidence="0.9323712">
Figure 3: The LUX model for “yellowish green” on the
Hue axis plotted against the scaled histogram of the re-
sponses in the data. The φ curve represents the likeli-
hood of “yellowish green” for different Hue values. The
τ curves represent possible boundaries.
</figureCaption>
<bodyText confidence="0.9999619375">
We represent this variability by assuming that the
boundaries that delimit the color are uncertain. In
any utterance, yellowish green fits only those Hue
values that are above a minimum threshold τLower
and below a maximum threshold τUpper. However,
it is uncertain which thresholds a speaker will use.
The model describes this variability with probabil-
ity density functions. They are shown for yellowish
green in Figure 3 as the τ distributions. The figure
shows that there is a central range of hues, between
the τ distributions, that is definitely yellowish green.
The τ distributions peak at the most likely bound-
aries for yellowish green, encompassing a broad re-
gion that’s frequently called yellowish green. Fur-
ther away, threshold values and yellowish green ut-
terances alike become rapidly less likely.
</bodyText>
<figure confidence="0.998879833333334">
Probability
0.8
0.6
0.4
0.2
1.0
τLower
µLower µUpper
τUpper
φHue
Y ellowishGreen
Yellowish Green data
</figure>
<page confidence="0.988341">
106
</page>
<bodyText confidence="0.999171127659574">
Our representation is motivated by Barker (2002)
and Lassiter (2009), who show how sets of possi-
ble thresholds1 can account for many of our intu-
itions about the use of vague language. Their analy-
sis invites us to capture semantic variability through
two geometric constructs. First, there is a certain
interval, parameterized by two points, µLower and
µUpper, within which a color description definitely
applies. Outside this interval are regions of bor-
derline cases, delimited by probabilistically-varying
thresholds τLower and τUpper, where the color de-
scription sometimes applies. We represent the po-
sition of the threshold with a F(α, β) distribution, a
standard statistical tool to model processes that start,
continue indefinitely, and stop, like waiting times.2
We can determine a likelihood that a description fits
a color by marginalizing over the thresholds: this
gives the black curve visualized in Figure 3. As we
describe in Section 3.3, we can use this to account
for the graded responses from subjects that we ob-
serve near color boundaries.
We summarize with a formal definition of our se-
mantic representation. Let X be the 3D space of
HSV colors and let x E X be a measured color
value. Each color label k has definite boundaries,
µLower and µUpper in X, delimiting a box of HSV
color space. Surrounding the definite region are
regions of uncertainty: the set of possible bound-
aries beyond µ. These are represented by probabil-
ity distributions over lower and upper threshold val-
ues in each dimension. We’ll represent these thresh-
olds by τj,d
k where k E K indexes the color label,
j E {Lower/L, Upper/U} indexes the boundary,
and d E {H, S, V} indexes color components. We
assume the thresholds are distributed as follows:
The meaning of a color term is thus a “blurry box”.
The distribution lets us determine the probability of
1We treat the terms “boundary”, “threshold”, and “standard”
to be synonymous, but useful in different contexts.
2Γ distributions rise quickly away from the origin point,
then trail off from the peak in an open-ended exponential de-
cay. One intuition for applying them in this case is Graff Fara’s
(2000) suggestion that a particular categorization decision in-
volves waiting to find a natural break among salient colors.
However, we choose them for mathematical convenience rather
than psychological or linguistic considerations.
</bodyText>
<figureCaption confidence="0.9538522">
Figure 4: The Rational Observer observes a color patch,
x. The applicability of each label (k�11e) is based upon
the label parameters (α, β, µ) and x. The label (k�,,M)
is sampled proportional to the applicability and a back-
ground weight: how often a label is said when it applies.
</figureCaption>
<bodyText confidence="0.999731">
a point x falling into the color category k as in Eq. 2.
We also use the compact notation in Eq. 3.
</bodyText>
<equation confidence="0.9984613">
P(τLower, H &lt; xH &lt; τUpper, H) ×
Tk
Lower&apos; S &lt; xS &lt; &apos;rk Upper, S) /X
P(
P(τLower, V &lt; xV &lt; τUpper, V
k ) (2)
k
P(τL,d
k &lt; xdi &lt; τU,d
k ) (3)
</equation>
<subsectionHeader confidence="0.995511">
3.3 Rational Observer Model
</subsectionHeader>
<bodyText confidence="0.999990608695652">
Our goal is to learn probabilistic representations
of the meanings of color terms from subjects’ re-
sponses. To do this, we need not only a framework
for representing colors but also a model of how sub-
jects choose color terms. Inspired by rational anal-
ysis (Anderson, 1991), we assume that speakers’
choices match their communicative goals and their
semantic knowledge. We leverage this assumption
to derive a Bayes Rational Observer model linking
semantics to observed color descriptions.
The graphical model in Figure 4 formalizes our
approach. We start from an observed color patch, x.
The Rational Observer uses the τ-distributions for
each color description k to determine the likelihood
that the speaker judges k applicable. As defined in
Eq. 3, the likelihood is the subset of possible bound-
aries which contain the target color value. Normally,
many descriptions will be applicable. Which the
speaker chooses depends further on the availabil-
ity of the label—a background measure of how fre-
quently a label is chosen when it’s applicable. In-
tuitively, availability creates a bias for easy descrip-
tions, capturing how natural or ordinary a descrip-
</bodyText>
<figure confidence="0.991016">
τLower,d ∼µ Lower,d Lower ,d Lower ,d
k k − F(αk I βk )
Upper,d Upper,d Upper,d Upper,d
τ ∼ k + F(αk , βk ) (1)
µ
k
ri=
d
</figure>
<page confidence="0.984937">
107
</page>
<bodyText confidence="0.999574571428571">
tion is in language use, how easily it springs to mind
or how easily it is understood.
We formalize this as a generative model. As we
explain in Section 4, we infer the parameters from
our data. In Eq. 4, we consider the conditional dis-
tribution of a subject observing a color patch given
HSV value x and labeling it k:
</bodyText>
<equation confidence="0.908369">
P(ksaid, ktrue|x) = P(ksaid|ktrue)P(ktrue|x) (4)
</equation>
<bodyText confidence="0.994741142857143">
In this equation, ksaid is the event that the subject
responds to x with label k and ktrue is the event that
the subject judges k true of the HSV value x. The
two factors of Eq. 4 are respectively the availability
and applicability of the color label.
Availability: The prior P(ksaid|ktrue) quantifies
the rate at which label k is used when it applies. We
refer to this quantity as the availability and denote
it as αk. Availability captures the observed bias for
frequent color terms. When multiple color labels fit
a color value, those with higher availability will be
used more often, but those with lower availability
will still get used. This effect is partially responsible
for the long tail of subjects’ responses.
Applicability: The second factor, P(ktrue|x),
is the probability that k is true of, or applies to,
the color value x. We calculate the applicability
by marginalizing over all possible thresholds as in
Eq. 3. In other words, we calculate the probabil-
ity mass of the boundaries which allow for this de-
scription to apply. We treat each applicability judg-
ment as independent of others. This implies that the
relative frequency at which we see a color descrip-
tion used is directly proportional to the proportion of
boundaries which license it.
For clearer notation and parameter estimation, we
track thresholds with a piecewise function φdk(xd) as
in Eq. 5 and Figure 3.
</bodyText>
<equation confidence="0.998808857142857">
φdk(xd) = { P(xd &gt; τL,d
P(xd &lt; τU,d
1, otherwise
k ), xd &lt; µL,d
k ), xd &gt; µU,d
k
k (5)
</equation>
<bodyText confidence="0.940251666666667">
Finally, Eq. 6 rewrites Eq. 4 to make the applica-
bility and availability explicit. The model treats this
equation as the probability of success for a Bernoulli
trial and the data as sampled from Categorical dis-
tributions formed by the set of K Bernoulli random
variables. This is discussed further in Section 4.2.
</bodyText>
<equation confidence="0.651181">
P(ksaid, k true |X) = αk ri φdk(xd) (6)
</equation>
<bodyText confidence="0.61943">
d
</bodyText>
<sectionHeader confidence="0.972217" genericHeader="method">
4 Learning Experiment
</sectionHeader>
<bodyText confidence="0.999936833333333">
We worked with Randall Munroe’s crowdsourced
corpus of color judgments, and fit the model us-
ing the Metropolis-Hastings Markov Chain Monte
Carlo, a Gaussian random walk optimization
method. This form of approximate Bayesian infer-
ence is described in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.997693">
4.1 Munroe Color Corpus
</subsectionHeader>
<bodyText confidence="0.99992196875">
In 2010, Munroe elicited descriptions of color
patches over the web. His platform asked users
for background information such as sex, color-
blindness, and monitor type, then presented color
patches and let the user freely name them. The setup
didn’t ensure that users see controlled colors or that
users’ responses are reliable, but the experiment col-
lected over 3.4M items pairing RGB values with text
descriptions. Munroe’s methodology, data and re-
sults are published online (Munroe, 2010).3
Munroe summarizes his results with 954 idealized
colors—RGB values that best exemplify high fre-
quency color labels. In effect, Munroe’s summary
offers a prototype theory of color vocabulary, like
that of Andreas and Klein (2014). An alternative
theory, which we explore, is that variability in the
applicability of labels is an important part of peo-
ple’s knowledge of color semantics. We compare
the two theories explicitly in Section 5.
Our experiments focus on a subset of Munroe’s
data comprising 2,176,417 data points and 829 color
descriptions, divided into a training set of 70%,
a 5% development set, and a held-out test set of
25%. To minimize variability in language use, we
selected data from users who self-report as non-
colorblind English speakers. This accounts for
2.5M of Munroe’s 3.4M items. To get our sub-
set, we further restrict attention to labels used 100
times or more, to ensure that there’s substantial ev-
idence of each term’s breadth of applicability. We
hand curated the responses to correct some mi-
nor spelling variations involving a single-character
</bodyText>
<footnote confidence="0.9866885">
3http://blog.xkcd.com/2010/05/03/
color-survey-results/
</footnote>
<page confidence="0.995775">
108
</page>
<bodyText confidence="0.999924444444444">
change (“yellow green” vs “yellow-green”; “fuch-
sia” vs “fuschia”, “fushia”, “fuchia”, and “fucsia”)
and to remove high-frequency spam labels. We are
left with 829 color labels that fit these restrictions.
Finally, we used python’s colorsys to convert from
RGB to HSV, where we hypothesize color meanings
can be represented more simply. We include these
data sets with our release at http://mcmahan.
io/lux/ so our results can be replicated.
</bodyText>
<subsectionHeader confidence="0.985916">
4.2 Fitting the Model Parameters
</subsectionHeader>
<bodyText confidence="0.999978444444444">
Optimization of the model’s parameters is framed in
a Bayesian framework and interpreted as maximiz-
ing the likelihood of the data given the parameters.
We fit each label and each dimension independently.
The data on each dimension is binned, as in Figure 3,
so we have Binomial random variables for each bin.
For each color label k, the probability of success is
based on the model’s parameters. Non-k data in the
bin are observations of failure. This gives Eq. 7:
</bodyText>
<equation confidence="0.843746">
P(nd i,k|ndi , Zdk, φk) ∼ Bin(ndi , Zdkφdk(i)) (7)
</equation>
<bodyText confidence="0.999395857142857">
Here ndi is the number of data points in bin i on di-
mension d, ndi,k is the number of data points for la-
bel k in bin i on dimension d, and Zdk is a normal-
ization constant, implicitly reflecting both the avail-
ability αk and the distribution of responses of the
term across other color dimensions. The optimiza-
tion process is a parameter search method which
uses as an objective function the probability of ndi,k
in Eq. 7 for all d,i, and k.
Parameter Search: We adopt a Bayesian coor-
dinate descent which sequentially samples the cer-
tain region parameter, µ, and the shape and rate pa-
rameters (α and β) of the F distributions for all d
and k independently. It also samples the estimated
normalization constant, ZdK. More specifically, the
sampling is done using Metropolis-Hastings Markov
Chain Monte Carlo (Metropolis et al., 1953; Chib
and Greenberg, 1995), which performs a Gaussian
random walk on the parameters4. For each sample,
the likelihood of the data, derived from the Bino-
mial variables, is compared for the new and old set
</bodyText>
<footnote confidence="0.86632775">
4We set the standard deviation of the sampling Gaussian to
be 1 for each µ and 0.3 for each α and β after finding experi-
mentally that it led to effective parameter search (Gelman et al.,
1996).
</footnote>
<bodyText confidence="0.9998725625">
of parameters. The new parameters are accepted
proportionally to the ratio of the two likelihoods.
Multiple chains were run using 4 different bin sizes
per dimension and monitored for convergence using
the generalized Gelman-Rubin diagnostic method
(Brooks and Gelman, 1998). This methodology
leaves us not only with the Monte Carlo estimate
of the expected value for each parameter, but also a
sampling distribution that quantifies the uncertainty
in the parameters themselves.
Availability: Availability is estimated as the ratio
of the observed frequency of a label to its expected
frequency given the parameters which define its dis-
tribution. The expected frequency, a marginalization
of the color space for the φ function, is calculated
using the midpoint integration approximation.
</bodyText>
<equation confidence="0.989742">
αk = P(ksaid, ktrue) P (ktrue) (8)
count(k)/N
fx P(ktrue|x)P(x)
</equation>
<sectionHeader confidence="0.981175" genericHeader="method">
5 Model Evaluation
</sectionHeader>
<bodyText confidence="0.999862090909091">
LUX explains Munroe’s data via speakers’ rational
use of probabilistic meanings, represented as sim-
ple “blurry boxes”. In this section, we assess the
effectiveness of this explanation. We anticipate two
arguments against our model: first, that the represen-
tation is too simple; second, that factoring speakers’
choices through a model of meaning is too cumber-
some. We rebut these arguments by providing met-
rics and results that suggest that LUX escapes these
objections and captures almost all of the structure in
subjects’ responses.
</bodyText>
<subsectionHeader confidence="0.966564">
5.1 Alternative Models
</subsectionHeader>
<bodyText confidence="0.9999387">
To test LUX’s representations, we built a brute-force
histogram model (HM) that discretizes HSV space
and tracks frequency distributions of labels directly
in each discretized bin. Similar histogram models
have been developed by Chuang et al. (2008) and
(Heer and Stone, 2012) to build interfaces for inter-
acting with color that are informed by human cat-
egorization and naming. More precisely, our HM
uses a linear interpolation method (Chen and Good-
man, 1996) to combine three histograms of various
</bodyText>
<page confidence="0.99837">
109
</page>
<bodyText confidence="0.9998492">
granularity.5 This amounts to predicting responses
by querying the training data. HM has the potential
to expose whether LUX is missing important fea-
tures of the distribution of color descriptions.
We also built a direct model of subjects’ choices
of color terms. Instead of appealing to the applica-
bility and availability of a color label, it works with
the observed frequency of a color label and a Gaus-
sian model of the probability of a color value for
each label, as in Eq. 9:
</bodyText>
<equation confidence="0.90991">
P(ksaid, ktrue|x) ∝ P(x|ktrue)P(ksaid, ktrue) (9)
</equation>
<bodyText confidence="0.9997445">
This Gaussian model (GM) generalizes Munroe’s
pairing of labels with prototypical colors:
P(x|ktrue) is a Gaussian with diagonal covari-
ance, so it associates each color term with a mean
HSV value and with variances in each dimension
that determine a label-specific distance metric. GM
predicts speaker choice by weighting these distances
probabilistically against the priors. GM completely
sidesteps the need to model meaning categorically.
It therefore has the potential to expose whether our
assumptions about semantic representations and
speaker choices hinder LUX’s performance.
</bodyText>
<subsectionHeader confidence="0.996021">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999974571428571">
We evaluate the models using two classes of met-
rics on a held-out test set consisting of 25% of the
corpus. The first type is based upon the posterior
distribution over labels and the ranked position of
subjects’ actual labels of color values. The second
type is based upon the log likelihood of the models,
which quantifies model fit.
</bodyText>
<sectionHeader confidence="0.430574" genericHeader="method">
5.2.1 Decision-Based Metrics
</sectionHeader>
<bodyText confidence="0.998671473684211">
To answer how accurate a model’s predictions are,
we can locate subjects’ responses in the weighted
rankings computed by the models.
The TOPK Measures: Each model provides a
posterior distribution over the possible labels. The
most likely label of this posterior is the maximum
likelihood estimate (MLE). We track how often the
MLE color label is what the user actually said as
5Specifically, the histograms are of size (90,10,10), (45,5,5),
and (1,1,1) across Hue, Saturation, and Value with interpolation
weights of 0.322, 0.643, and 0.035 respectively. These parame-
ters were determined by taking the training set as 5-fold valida-
tion sets.
the TOP1 measure. For the Histogram Model, the
TOP1 approximates the most frequent label ob-
served in the data for a color value. We also measure
how often the correct label appears in the first 5 and
10 most likely labels. These are denoted TOP5 and
TOP10 respectively.
</bodyText>
<subsectionHeader confidence="0.46045">
5.2.2 Likelihood-Based Metrics
</subsectionHeader>
<bodyText confidence="0.999709375">
We can also measure how well a model explains
speaker choice using the log likelihood of the labels
given the model and the color values, denoted as
LLV (M). This is calculated using Eq. 10 across
all N data points in the held-out test set. LLV (M)
is used when computing perplexity and Aikake In-
formation Criterion (AIC). We report all measures
in bits.
</bodyText>
<equation confidence="0.9960354">
LLV (M) = lo92 PM(Ktrue, Ksaid|X)
�= lo92 PM(ktrue
i , ksaid
i |xi) (10)
i
</equation>
<bodyText confidence="0.999871823529412">
A more general measure of model fit is the log like-
lihood of the color values and their labels jointly
across the training set, LL(V ), given the model. It
is defined and calculated analogously.
Perplexity Perplexity has been used in past re-
search to measure the performance of statistical lan-
guage models (Jelinek et al., 1977; Brown et al.,
1992). Lower perplexity means that the model is less
surprised by the data and so describes it more pre-
cisely. We use it here to measure how well a model
encodes the regularities in color descriptions.
Akaike Information Criterion: AIC is derived
from information theory (Akaike, 1974) and bal-
ances the model’s fit to the data with the complexity
of the model by penalizing a larger number of pa-
rameters. The intuition is that a smaller AIC indi-
cates a better balance of parameters and model fit.
</bodyText>
<subsectionHeader confidence="0.998032">
5.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.946788">
Table 1 summarizes the decision-based evalua-
tion results.6 We see little penalty for LUX and
</bodyText>
<footnote confidence="0.938849142857143">
6There is a caveat to these performance measures. All of the
reported numbers are for the final data subset which we discuss
in Section 4.1. We choose to use a subset which did not include
color labels that had less than 100 occurrences. In the English-
speaking and American-citizenship subset, the rare description
tail accounts for 13% of the data—Roughly one third of the
tail data is unique descriptions. If the tail represents real world
</footnote>
<page confidence="0.990207">
110
</page>
<table confidence="0.99831825">
TOP1 TOP5 TOP10
LUX 39.55% 69.80% 80.46%
HM 39.40% 71.89% 82.53%
GM 39.05% 69.25% 79.99%
</table>
<tableCaption confidence="0.9979935">
Table 1: Decision-based results. The percentage of cor-
rect responses of 544,764 test-set data points are shown.
</tableCaption>
<table confidence="0.99991425">
−LL −LLV AIC Perp
LUX 1.13*107 2.05*106 4.13*106 13.61
HM 1.13*107 2.09*106 4.82*106 14.41
GM 1.34*107 2.08*106 4.17*106 14.14
</table>
<tableCaption confidence="0.8736075">
Table 2: Likelihood-based evaluation results: negative
log likelihood of the data, negative log likelihood of
labels given points, number of parameters, Akaike In-
formation Criterion and perplexity of labels given color
values. Parameter counts for AIC are 15751 for LUX,
315669 for HM and 5803 for GM.
</tableCaption>
<bodyText confidence="0.999879224137931">
GM’s constrained frameworks for modeling choices.
However, the differences in the table, though nu-
merically small, are significant (by Binomial test)
at p &lt; .02 or less. In particular, the fact that LUX
wins TOP1 hints that its representations enable bet-
ter generalization than HM or GM. The success of
HM at TOP5 and TOP10, meanwhile, suggests
that some qualitative aspects of people’s use of color
words do escape the strong assumptions of LUX and
GM—a point we return to below.
At the same time, we draw a general lesson from
the overall patterns of results in Table 1. Language
users must be quite uncertain about how speakers
will describe colors. Speakers do not seem to choose
the most likely color label in a majority of responses;
their behavior shows a long tail. These results are in
line with the probabilistic models of meaning and
speaker choice we have developed.
Table 2 summarizes the likelihood based metrics.
GM’s estimates don’t fit the distribution of the test
data as a whole: GM is a good model of what labels
speakers give but not a good model of the points that
get particular labels. By contrast, LUX tops out ev-
ery row in the table. HM is flexible enough in prin-
ciple to mirror LUX’s predictions; HM must suffer
circumstances, our model is only applicable 87% of the time,
and thus the performance metrics should be scaled down. We
do not explicitly report the scaled numbers.
from sparse data, given its vast number of parame-
ters. By contrast, LUX is able to capture the dis-
tributions of speaker responses in deeper and more
flexible ways by using semantics as an abstraction.
Our analysis of patterns of error in LUX sug-
gests that LUX would best improved by more faith-
ful models of linguistic meaning, rather than more
elaborate models of subjects’ choices or more pow-
erful learning methods. For one thing, neither LUX
nor the simple prototype model captures ambiguity,
which sometimes arises in Munroe’s data. An exam-
ple is the color label melon, which has a multimodal
distribution in the reddish-orange and green areas of
color space shown in Figure 5—most likely corre-
sponding to people thinking about the distinct col-
ors of the flesh of watermelon, cantaloupe and hon-
eydew. Interestingly, our model captures the more
common usage.
A different modeling challenge is illustrated by
the behavior of greenish in Figure 6. Greenish seems
to be an exception to the general assumption that
color terms label convex categories. Actually, green-
ish seems to fit the boundary of green—the areas that
are not definitely green but not definitely not green.
(Linguists often appeal to such concepts in the liter-
ature on vagueness.) This is not a convex area so,
not surprisingly, our model finds a poor match. Ad-
ditional research is needed to understand when it’s
appropriate to give meanings more complex repre-
sentations and how they can be learned.
</bodyText>
<sectionHeader confidence="0.99058" genericHeader="discussions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.87491925">
Natural language color descriptions provide an ex-
pressive, precise but open-ended vocabulary to char-
acterize real-world objects. This paper documents
Hue
</bodyText>
<figureCaption confidence="0.971384">
Figure 5: For the Hue dimension, the data for “melon” is
plotted against the LUX model’s φ curve.
</figureCaption>
<figure confidence="0.9986588">
Probability
1.0
0.8
0.6
0.4
0.2
0.0
����
Melon
Melon data
</figure>
<page confidence="0.830155">
111
</page>
<figureCaption confidence="0.99788">
Figure 6: For the Hue dimension, the data for “greenish”
is plotted against the LUX model’s φ curve.
</figureCaption>
<bodyText confidence="0.996690516129032">
and releases Lexicon of Uncertain Color Standards
(LUX), which provides semantic representations of
829 English color labels, derived from a large cor-
pus of attested descriptions. Our evaluation shows
that LUX provides a precise description of speak-
ers’ free-text labels of color patches. Our expec-
tation therefore is that LUX will serve as a useful
resource for building systems for situated language
understanding and generation that need to describe
colors to English-speaking users.
Our work in LUX has built closely on linguis-
tic approaches to color meaning and psychological
approaches to modeling experimental subjects. Be-
cause LUX bridges linguistic theory, psychologi-
cal data, and system building, LUX also affords a
unique set of resources for future research at the in-
tersection of semantics and pragmatics of dialogue.
For example, our work explains subjects’ deci-
sions as a straightforward reflection of their com-
municative goals in a probabilistic setting. Our
measures of availability and applicability can be
seen as offering computational interpretations of the
Gricean Maxims of Manner and Quality (Grice,
1975). However, these particular interpretations
don’t give rise to implicatures on our model—
largely because our Rational Observer is so inclusive
and variable in the descriptions it offers. To show
this, we can analyze what an idealized hearer learns
about an underlying color x when the speaker uses a
color term k: this is P(x|ksaid). The model predic-
tions are formalized in Eq. 11.
</bodyText>
<equation confidence="0.999492714285714">
P(x|ksaid) = P(x|ksaid, ktrue)
P(ksaid, ktrue|x)P(x)
P(ksaid, ktrue)
P(ksaid|ktrue)P(ktrue|x)P(x)
= P(ksaid|ktrue)P(ktrue)
= P(x|ktrue)
(11)
</equation>
<bodyText confidence="0.999930432432432">
We apply Bayes’s rule, exploiting our model as-
sumption that the speaker says k only when the
speaker first judges that k is true. Our model also
tells us that, given that k is true, the speaker’s choice
of whether to say k depends only on the availabil-
ity αk of the term k. Simplifying, we find that the
pragmatic posterior—what we think the speaker was
looking at when she said this word—coincides with
the semantic posterior—what we think the word is
true of. Intuitively, the hearer knows that the term is
true because the speaker has used the word, indepen-
dent of the color x the speaker is describing. Sim-
ilarly, in our model of speaker choice, the speaker
does not take x into account in choosing one of the
applicable words to say (one way the speaker could
do this, for example, would be to prefer terms that
were more informative about the target color x). In-
stead, the speaker simply samples from the candi-
dates. That’s why the speaker’s choice reveals only
what the semantics says about x.
Technically, this makes semantics a Nash equi-
librium, where the information the hearer recov-
ers from an utterance is exactly the information
the speaker intends to express—in keeping with a
longstanding tradition in the philosophy of language
(Lewis, 1969; Cumming, 2013). By contrast, re-
searchers such as Smith et al. (2013) adopt broadly
similar formal assumptions but predict asymme-
tries where sophisticated listeners can second-guess
naive speakers’ choices and recover “extra” infor-
mation that the speaker has revealed incidentally
and unintentionally. The difference between this ap-
proach and ours eventually leads to a difference in
the priors over utterances, but it’s best explained
through the different utilities that motivate speak-
ers’ different choices in the first place. Smith et al.
(2013) assume speakers want to be informative; we
</bodyText>
<figure confidence="0.997612">
Hue
Probability
1.0
0.8
0.6
0.4
0.2
0.0
0.005
0.004
0.003
0.002
0.001
0.000
τLower
µLower µUpper
φHue
Greenish
Greenish data
τUpper
αkP(ktrue|x)P(x)
αkP(ktrue)
</figure>
<page confidence="0.990242">
112
</page>
<bodyText confidence="0.9999913">
assume they want to fit in. The empirical success
of our approach on Munroe’s data motivates a larger
project to elicit data that can explicitly probe sub-
jects’ communicative goals in relation to semantic
coordination.
Meanwhile, our work formalizes probabilistic
theories of vagueness with new scale and preci-
sion. These naturally suggest that we test predictions
about the dynamics of conversation drawn from the
semantic literature on vagueness. For example, in
hearing a description for an object, we come to know
more about the standards governing the applicability
of the description. This is outlined by Barker (2002)
as having a meta-semantic effect on the common
ground among interlocutors. For example, hearing
a yellow-green object called yellowish green should
make objects in the same color range more likely
to be referred to as yellowish green. We could use
LUX straightforwardly to represent such conceptual
pacts (Brennan and Clark, 1996) via a posterior over
threshold parameters. It’s natural to look for empir-
ical evidence to assess the effectiveness of such rep-
resentations of dependent context.
A particularly important case involves descrip-
tive material that distinguishes a target referent from
salient alternatives, as in the understanding or gen-
eration of referring expressions (Krahmer and van
Deemter, 2012). Following Kyburg and Morreau
(2000), we could represent this using LUX via a pos-
terior over the threshold parameters that fit the target
but exclude its alternatives. Again, our model as-
sociates such goals with quantitative measures that
future research can explore empirically. Meo et al.
(2014) present an initial exploration of this idea.
These open questions complement the key advan-
tage that makes uncertainty about meaning crucial to
the success of the model and experiments we have
reported here. Many kinds of language use seem to
be highly variable, and approaches to grounded se-
mantics need ways to make room for this variabil-
ity both in the semantic representations they learn
and the algorithms that induce these representations
from language data. We have argued that uncertainty
about meaning is a powerful new tool to do this. We
look forward to future work addressing uncertainty
in grounded meanings in a wide range of continu-
ous domains—generalizing from color to quantity,
scales, space and time—and pursuing a wide range
of reasoning efforts, to corroborate our results and
to leverage them in grounded language use.
</bodyText>
<sectionHeader confidence="0.995497" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990026833333333">
This work was supported in part by NSF DGE-
0549115. This work has benefited from discus-
sion and feedback from the reviewers of TACL, Ma-
neesh Agrawala, David DeVault, Jason Eisner, Tarek
El-Gaaly, Katrin Erk, Vicky Froyen, Joshua Gang,
Pernille Hemmer, Alex Lascarides, and Tim Meo.
</bodyText>
<sectionHeader confidence="0.998183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997878394736842">
Hirotugu Akaike. 1974. A new look at the statistical
model identification. IEEE Transactions on Automatic
Control, 19(6):716–723.
John R. Anderson. 1991. The adaptive nature of human
categorization. Psychological Review, 98(3):409.
Jacob Andreas and Dan Klein. 2014. Grounding lan-
guage with points and paths in continuous spaces. In
Proceedings of the Eighteenth Conference on Com-
putational Natural Language Learning, pages 58–67,
June.
Chris Barker. 2002. The dynamics of vagueness. Lin-
guistics and Philosophy, 25(1):1–36.
Brent Berlin. 1991. Basic Color Terms: Their Univer-
sality and Evolution. Univ of California Press.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory and
Cognition, 22(6):1482–1493.
Stephen P. Brooks and Andrew Gelman. 1998. Gen-
eral methods for monitoring convergence of iterative
simulations. Journal of Computational and Graphical
Statistics, 7(4):434–455.
Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer,
Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An
estimate of an upper bound for the entropy of English.
Computational Linguistics, 18(1):31–40.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, pages
136–145.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proceedings of the 34th annual meeting on As-
sociation for Computational Linguistics, pages 310–
318.
David L. Chen and Raymond J. Mooney. 2008. Learning
to sportscast: a test of grounded language acquisition.
</reference>
<page confidence="0.996847">
113
</page>
<reference confidence="0.997184269230769">
In ICML ’08: Proceedings of the 25th international
conference on Machine learning, pages 128–135.
Siddhartha Chib and Edward Greenberg. 1995. Un-
derstanding the Metropolis–Hastings algorithm. The
American Statistician, 49(4):327–335.
Jason Chuang, Maureen Stone, and Pat Hanrahan. 2008.
A probabilistic model of the categorical association
between colors. In Color Imaging Conference, pages
6–11.
Sam Cumming. 2013. Coordination and content.
Philosophers’ Imprint, 13(4):1–16.
Colin R. Dawson, Jeremy Wright, Antons Rebguns,
Marco Valenzuela Esc´arcega, Daniel Fried, and
Paul R. Cohen. 2013. A generative probabilis-
tic framework for learning spatial language. In
2013 IEEE Third Joint International Conference on
Development and Learning and Epigenetic Robotics
(ICDL), pages 1–8. IEEE.
David DeVault, Iris Oved, and Matthew Stone. 2006. So-
cietal grounding is essential to meaningful language
use. In Proceedings of the Twenty-first National Con-
ference on Artificial Intelligence, pages 747–754.
Mark D. Fairchild. 2013. Color Appearance Models.
The Wiley-IS&amp;T Series in Imaging Science and Tech-
nology. Wiley.
Delia Graff Fara. 2000. Shifting sands: An interest-
relative theory of vagueness. Philosophical Topics,
28(1):45–81.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
2009 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 1778–1785, June.
George W. Furnas, Thomas K. Landauer, Louis M.
Gomez, and Susan T. Dumais. 1987. The vocabulary
problem in human-system communication. Communi-
cations of the ACM, 30(11):964–971.
Peter G¨ardenfors. 2000. Conceptual Spaces. MIT Press.
Andrew Gelman, Gareth O. Roberts, and Walter R. Gilks.
1996. Efficient Metropolis jumping rules. In J. M.
Bernardo, J. O. Berger, A. P. Dawid, and A. F. Smith,
editors, Bayesian Statistics 5, pages 599–607. Oxford
University Press.
Herbert P. Grice. 1975. Logic and conversation. In
P. Cole and J. Morgan, editors, Syntax and Semantics
III: Speech Acts, pages 41–58. Academic Press.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1–3):335–346.
Jeffrey Heer and Maureen Stone. 2012. Color naming
models for color selection, image editing and palette
design. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 1007–
1016.
John F. Hughes, Andries van Dam, Morgan McGuire,
David F. Sklar, James D. Foley, Steven K. Feiner, and
Kurt Akeley. 2013. Computer Graphics: Principles
and Practice (3rd Edition). Addison-Wesley Profes-
sional.
Gerhard J¨ager. 2010. Natural color categories are con-
vex sets. In Maria Aloni, Harald Bastiaanse, Tikitu
de Jager, and Katrin Schulz, editors, Logic, Language
and Meaning - 17th Amsterdam Colloquium, Amster-
dam, The Netherlands, December 16-18, 2009, Re-
vised Selected Papers, volume 6042 of Lecture Notes
in Computer Science, pages 11–20. Springer.
Fred Jelinek, Robert L. Mercer, Lalit R. Bahl, and
James K. Baker. 1977. Perplexity–a measure of the
difficulty of speech recognition tasks. The Journal of
the Acoustical Society of America, 62:S63.
Paul Kay, Brent Berlin, Luisa Maffi, William R. Merri-
field, and Richard Cook. 2009. The World Color Sur-
vey. CSLI.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38(1):173–218.
Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly
learning to parse and perceive: Connecting natural lan-
guage to the physical world. Transactions of the Asso-
ciation for Computational Linguistics, 1(2):193–206.
Alice Kyburg and Michael Morreau. 2000. Fitting
words: Vague words in context. Linguistics and Phi-
losophy, 23(6):577–597.
Johan Maurice Gisele Lammens. 1994. A computational
model of color perception and color naming. Ph.D.
thesis, SUNY Buffalo.
Staffan Larsson. 2013. Formal semantics for percep-
tual classification. Journal of Logic and Computa-
tion. Advance online publication. doi: 10.1093/log-
com/ext059.
Daniel Lassiter. 2009. Vagueness as probabilistic lin-
guistic knowledge. In Rick Nouwen, Robert van
Rooij, Uli Sauerland, and Hans-Christian Schmitz,
editors, Vagueness in Communication - International
Workshop, ViC 2009, held as part of ESSLLI 2009,
Bordeaux, France, July 20-24, 2009. Revised Selected
Papers, volume 6517 of Lecture Notes in Computer
Science, pages 127–150. Springer.
David K. Lewis. 1969. Convention: A Philosophical
Study. Harvard University Press, Cambridge, MA.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
pages 1671–1678.
</reference>
<page confidence="0.988346">
114
</page>
<reference confidence="0.999783028985507">
Nikolaos Mavridis and Deb Roy. 2006. Grounded
situation models for robots: Where words and per-
cepts meet. In Intelligent Robots and Systems, 2006
IEEE/RSJ International Conference on, pages 4690–
4697. IEEE.
Timothy Meo, Brian McMahan, and Matthew Stone.
2014. Generating and resolving vague color refer-
ences. In SEMDIAL 2014: THE 18th Workshop on the
Semantics and Pragmatics of Dialogue, pages 107–
115.
Nicholas Metropolis, Arianna W. Rosenbluth, Mar-
shall N. Rosenbluth, Augusta H. Teller, and Edward
Teller. 1953. Equation of state calculations by
fast computing machines. The Journal of Chemical
Physics, 21(6):1087–1092.
Randall Munroe. 2010. Color survey results. On-
line at http://blog.xkcd.com/2010/05/03/color-survey-
results/.
Kimele Persaud and Pernille Hemmer. 2014. The in-
fluence of knowledge and expectations for color on
episodic memory. In P Bello, M Guarini, M Mc-
Shane, and B Scassellati, editors, Proceedings of the
36th Annual Conference of the Cognitive Science So-
ciety, pages 1162–1167.
Terry Regier, Paul Kay, and Richard S. Cook. 2005. Fo-
cal colors are universal after all. Proceedings of the
National Academy of Sciences, 102:8386–8391.
Terry Regier, Paul Kay, and Naveen Khetarpal. 2007.
Color naming reflects optimal partitions of color
space. Proceedings of the National Academy of Sci-
ences, 104:1436–1441.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of Semantic Representation with Visual
Attributes. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics,
pages 572–582.
Nathaniel J. Smith, Noah D. Goodman, and Michael C.
Frank. 2013. Learning and using language via recur-
sive pragmatic reasoning about other agents. In Ad-
vances in Neural Information Processing Systems 26,
pages 3039–3047.
Stefanie Tellex, Thomas Kollar, and Steven Dickerson.
2011a. Approaching the symbol grounding problem
with probabilistic graphical models. AI magazine,
32(4):64–76.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R Walter, Ashis Gopal Banerjee, Seth J
Teller, and Nicholas Roy. 2011b. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proceedings of the Twenty-
Fifth AAAI Conference on Artificial Intelligence, pages
1507–1514.
Terry Winograd. 1970. Procedures as a representation
for data in a computer program for understanding nat-
ural language. Ph.D. thesis, MIT.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. 2014. From image descriptions to visual
denotations: New similarity metrics for semantic in-
ference over event descriptions. Transactions of the
Association for Computational Linguistics, 2:67–78.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Conference
on Artificial Intelligence, pages 1050–1055.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
UAI ’05, Proceedings of the 21st Conference in Un-
certainty in Artificial Intelligence, pages 658–666.
</reference>
<page confidence="0.9995965">
115
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591339">
<title confidence="0.999791">A Bayesian Model of Grounded Color Semantics</title>
<author confidence="0.995092">Brian</author>
<affiliation confidence="0.999965">Rutgers University</affiliation>
<email confidence="0.998748">brian.mcmahan@rutgers.edu</email>
<author confidence="0.902009">Matthew</author>
<affiliation confidence="0.999981">Rutgers University</affiliation>
<email confidence="0.977561">matthew.stone@rutgers.edu</email>
<note confidence="0.937950888888889">0 Hue Entropy (bits) 4 6 5 3 2 1</note>
<abstract confidence="0.99861747826087">Natural language meanings allow speakers to encode important real-world distinctions, but corpora of grounded language use also reveal that speakers categorize the world in different ways and describe situations with different terminology. To learn meanings from data, we therefore need to link underlying representations of meaning to models of speaker judgment and speaker choice. This paper describes a new approach to this problem: we model variability through uncertainty in categorization boundaries and distributions over preferred vocabulary. We apply the approach to a large data set of color descriptions, where statistical evaluation documents its accuracy. The results are available as a Lexicon of Uncertain Color Standards (LUX), which supports future efforts in grounded language understanding and generation by probabilistically mapping 829 English color descriptions to potentially context-sensitive regions in HSV color space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hirotugu Akaike</author>
</authors>
<title>A new look at the statistical model identification.</title>
<date>1974</date>
<journal>IEEE Transactions on Automatic Control,</journal>
<volume>19</volume>
<issue>6</issue>
<contexts>
<context position="32015" citStr="Akaike, 1974" startWordPosition="5161" endWordPosition="5162">measure of model fit is the log likelihood of the color values and their labels jointly across the training set, LL(V ), given the model. It is defined and calculated analogously. Perplexity Perplexity has been used in past research to measure the performance of statistical language models (Jelinek et al., 1977; Brown et al., 1992). Lower perplexity means that the model is less surprised by the data and so describes it more precisely. We use it here to measure how well a model encodes the regularities in color descriptions. Akaike Information Criterion: AIC is derived from information theory (Akaike, 1974) and balances the model’s fit to the data with the complexity of the model by penalizing a larger number of parameters. The intuition is that a smaller AIC indicates a better balance of parameters and model fit. 5.3 Evaluation Results Table 1 summarizes the decision-based evaluation results.6 We see little penalty for LUX and 6There is a caveat to these performance measures. All of the reported numbers are for the final data subset which we discuss in Section 4.1. We choose to use a subset which did not include color labels that had less than 100 occurrences. In the Englishspeaking and America</context>
</contexts>
<marker>Akaike, 1974</marker>
<rawString>Hirotugu Akaike. 1974. A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6):716–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Anderson</author>
</authors>
<title>The adaptive nature of human categorization.</title>
<date>1991</date>
<journal>Psychological Review,</journal>
<volume>98</volume>
<issue>3</issue>
<contexts>
<context position="12267" citStr="Anderson (1991)" startWordPosition="1909" endWordPosition="1910">peakers tend to use it when it is applicable. For example, although the expressions yellowish green and chartreuse are associated with very similar color categories, people say yellowish green 105 much more often: it has a higher availability. Empirically, we find few terms with high availability and a long tail of terms with lower availabilities. We assume speakers simply sample applicable terms from this distribution, which predicts the long tail of observed responses. Mathematically, we develop our approach through the rational analysis methodology for explaining human behavior proposed by Anderson (1991), along with methodological insights from the linguistics and philosophy of vagueness. In the remainder of this section, we explain the theoretical antecedents in perceptual science, linguistics and cognitive modeling that inform our approach. 3.1 Color Categories Color can be defined as sensations by which the perceptual system tracks the diffuse reflectance of objects, despite variability, uncertainty and ambiguity in the visual input. Red, green, and blue cones in the retina allow the visual system to coarsely estimate frequency bands in the spectrum of incoming light. Cameras and screens t</context>
<context position="19218" citStr="Anderson, 1991" startWordPosition="3048" endWordPosition="3049">bility and a background weight: how often a label is said when it applies. a point x falling into the color category k as in Eq. 2. We also use the compact notation in Eq. 3. P(τLower, H &lt; xH &lt; τUpper, H) × Tk Lower&apos; S &lt; xS &lt; &apos;rk Upper, S) /X P( P(τLower, V &lt; xV &lt; τUpper, V k ) (2) k P(τL,d k &lt; xdi &lt; τU,d k ) (3) 3.3 Rational Observer Model Our goal is to learn probabilistic representations of the meanings of color terms from subjects’ responses. To do this, we need not only a framework for representing colors but also a model of how subjects choose color terms. Inspired by rational analysis (Anderson, 1991), we assume that speakers’ choices match their communicative goals and their semantic knowledge. We leverage this assumption to derive a Bayes Rational Observer model linking semantics to observed color descriptions. The graphical model in Figure 4 formalizes our approach. We start from an observed color patch, x. The Rational Observer uses the τ-distributions for each color description k to determine the likelihood that the speaker judges k applicable. As defined in Eq. 3, the likelihood is the subset of possible boundaries which contain the target color value. Normally, many descriptions wil</context>
</contexts>
<marker>Anderson, 1991</marker>
<rawString>John R. Anderson. 1991. The adaptive nature of human categorization. Psychological Review, 98(3):409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>Grounding language with points and paths in continuous spaces.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>58--67</pages>
<contexts>
<context position="7820" citStr="Andreas and Klein (2014)" startWordPosition="1219" endWordPosition="1222">these categories as 104 regions of suitable perceptual feature spaces. Researchers have explored explicit spaces of high-level perceptual attributes (Farhadi et al., 2009; Silberer et al., 2013), approximations to such spaces (Matuszek et al., 2012), or low-level feature spaces such as Bag of Visual Words (Bruni et al., 2012) or Histogram of Gradients (Krishnamurthy and Kollar, 2013). We specifically follow G¨ardenfors (2000) and J¨ager (2010) in assuming that color categories are convex regions in an underlying color space, and are not just determined by prototypical color values, such as in Andreas and Klein (2014). However, unlike previous grounded semantics, we do not assume that words name categories unequivocally. Speakers may vary in how they interpret a word, so we treat the link between words and categories probabilistically. The difference makes training our model more indirect than previous approaches to grounded meaning. In particular, our model introduces a new layer of uncertainty that describes what category the speaker uses. Similar kinds of uncertainty can be found in Bayesian models of speaker strategy, such as that of Smith et al. (2013). However, this research has assumed that speakers</context>
<context position="9475" citStr="Andreas and Klein (2014)" startWordPosition="1483" endWordPosition="1486">e people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is </context>
<context position="23452" citStr="Andreas and Klein (2014)" startWordPosition="3762" endWordPosition="3765"> background information such as sex, colorblindness, and monitor type, then presented color patches and let the user freely name them. The setup didn’t ensure that users see controlled colors or that users’ responses are reliable, but the experiment collected over 3.4M items pairing RGB values with text descriptions. Munroe’s methodology, data and results are published online (Munroe, 2010).3 Munroe summarizes his results with 954 idealized colors—RGB values that best exemplify high frequency color labels. In effect, Munroe’s summary offers a prototype theory of color vocabulary, like that of Andreas and Klein (2014). An alternative theory, which we explore, is that variability in the applicability of labels is an important part of people’s knowledge of color semantics. We compare the two theories explicitly in Section 5. Our experiments focus on a subset of Munroe’s data comprising 2,176,417 data points and 829 color descriptions, divided into a training set of 70%, a 5% development set, and a held-out test set of 25%. To minimize variability in language use, we selected data from users who self-report as noncolorblind English speakers. This accounts for 2.5M of Munroe’s 3.4M items. To get our subset, we</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. Grounding language with points and paths in continuous spaces. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 58–67, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Barker</author>
</authors>
<title>The dynamics of vagueness.</title>
<date>2002</date>
<journal>Linguistics and Philosophy,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="2858" citStr="Barker, 2002" startWordPosition="452" endWordPosition="453"> in Section 4.1, there’s an average of 3.845 bits of information in a color description given the color it describes—comparable to rolling a 14-sided die. Figure 1 summarizes the data and plots the entropy of descriptions encountered within small bins of color space. The bins are aggregated over the Saturation and Value dimensions and indexed on the xaxis by the Hue dimension. There’s little reason to think that this variability conceals consistent meanings. In formal semantics, one of the hallmarks of vague language is that speakers can make it more precise in alternative, incompatible ways (Barker, 2002). We see this in practice as well, for example with the image of Figure 2, where subjects com103 Transactions of the Association for Computational Linguistics, vol. 3, pp. 103–115, 2015. Action Editor: Lillian Lee. Submission batch: 11/2014; Published 2/2015. c�2015 Association for Computational Linguistics. Figure 2: Image by flickr user Joanne Bacon (jlbacon) from the data set of Young et al. (2014), whose subjects describe these dogs as a brown dog and a tan one or a tan dog and a white one. prehensibly describe either of two dogs as the tan one. Systems that robustly understand or generate</context>
<context position="16071" citStr="Barker (2002)" startWordPosition="2506" endWordPosition="2507">ility density functions. They are shown for yellowish green in Figure 3 as the τ distributions. The figure shows that there is a central range of hues, between the τ distributions, that is definitely yellowish green. The τ distributions peak at the most likely boundaries for yellowish green, encompassing a broad region that’s frequently called yellowish green. Further away, threshold values and yellowish green utterances alike become rapidly less likely. Probability 0.8 0.6 0.4 0.2 1.0 τLower µLower µUpper τUpper φHue Y ellowishGreen Yellowish Green data 106 Our representation is motivated by Barker (2002) and Lassiter (2009), who show how sets of possible thresholds1 can account for many of our intuitions about the use of vague language. Their analysis invites us to capture semantic variability through two geometric constructs. First, there is a certain interval, parameterized by two points, µLower and µUpper, within which a color description definitely applies. Outside this interval are regions of borderline cases, delimited by probabilistically-varying thresholds τLower and τUpper, where the color description sometimes applies. We represent the position of the threshold with a F(α, β) distri</context>
<context position="41024" citStr="Barker (2002)" startWordPosition="6634" endWordPosition="6635"> want to fit in. The empirical success of our approach on Munroe’s data motivates a larger project to elicit data that can explicitly probe subjects’ communicative goals in relation to semantic coordination. Meanwhile, our work formalizes probabilistic theories of vagueness with new scale and precision. These naturally suggest that we test predictions about the dynamics of conversation drawn from the semantic literature on vagueness. For example, in hearing a description for an object, we come to know more about the standards governing the applicability of the description. This is outlined by Barker (2002) as having a meta-semantic effect on the common ground among interlocutors. For example, hearing a yellow-green object called yellowish green should make objects in the same color range more likely to be referred to as yellowish green. We could use LUX straightforwardly to represent such conceptual pacts (Brennan and Clark, 1996) via a posterior over threshold parameters. It’s natural to look for empirical evidence to assess the effectiveness of such representations of dependent context. A particularly important case involves descriptive material that distinguishes a target referent from salie</context>
</contexts>
<marker>Barker, 2002</marker>
<rawString>Chris Barker. 2002. The dynamics of vagueness. Linguistics and Philosophy, 25(1):1–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brent Berlin</author>
</authors>
<title>Basic Color Terms: Their Universality and Evolution. Univ of California</title>
<date>1991</date>
<publisher>Press.</publisher>
<contexts>
<context position="2127" citStr="Berlin, 1991" startWordPosition="332" endWordPosition="333">ount for this complexity by deriving one definitive mapping between words and the world. We see this complexity already in free text descriptions of color patches. English has fewer than Figure 1: A visualization of the variability of the descriptions used to name colors within small bins of color space. For each Hue value, the entropy values for each bin along the Saturation and Value dimensions are grouped and plotted as box plots. The dotted line corresponds to a random choice out of fourteen items and to the perplexity of a histogram model trained on the corpus. a dozen basic color words (Berlin, 1991), but people’s descriptions of colors are much more variable than this would suggest. Measured on the corpus described in Section 4.1, there’s an average of 3.845 bits of information in a color description given the color it describes—comparable to rolling a 14-sided die. Figure 1 summarizes the data and plots the entropy of descriptions encountered within small bins of color space. The bins are aggregated over the Saturation and Value dimensions and indexed on the xaxis by the Hue dimension. There’s little reason to think that this variability conceals consistent meanings. In formal semantics</context>
</contexts>
<marker>Berlin, 1991</marker>
<rawString>Brent Berlin. 1991. Basic Color Terms: Their Universality and Evolution. Univ of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Herbert H Clark</author>
</authors>
<title>Conceptual pacts and lexical choice in conversation.</title>
<date>1996</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<volume>22</volume>
<issue>6</issue>
<contexts>
<context position="41355" citStr="Brennan and Clark, 1996" startWordPosition="6683" endWordPosition="6686"> suggest that we test predictions about the dynamics of conversation drawn from the semantic literature on vagueness. For example, in hearing a description for an object, we come to know more about the standards governing the applicability of the description. This is outlined by Barker (2002) as having a meta-semantic effect on the common ground among interlocutors. For example, hearing a yellow-green object called yellowish green should make objects in the same color range more likely to be referred to as yellowish green. We could use LUX straightforwardly to represent such conceptual pacts (Brennan and Clark, 1996) via a posterior over threshold parameters. It’s natural to look for empirical evidence to assess the effectiveness of such representations of dependent context. A particularly important case involves descriptive material that distinguishes a target referent from salient alternatives, as in the understanding or generation of referring expressions (Krahmer and van Deemter, 2012). Following Kyburg and Morreau (2000), we could represent this using LUX via a posterior over the threshold parameters that fit the target but exclude its alternatives. Again, our model associates such goals with quantit</context>
</contexts>
<marker>Brennan, Clark, 1996</marker>
<rawString>Susan E. Brennan and Herbert H. Clark. 1996. Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology: Learning, Memory and Cognition, 22(6):1482–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen P Brooks</author>
<author>Andrew Gelman</author>
</authors>
<title>General methods for monitoring convergence of iterative simulations.</title>
<date>1998</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="26852" citStr="Brooks and Gelman, 1998" startWordPosition="4326" endWordPosition="4329">ssian random walk on the parameters4. For each sample, the likelihood of the data, derived from the Binomial variables, is compared for the new and old set 4We set the standard deviation of the sampling Gaussian to be 1 for each µ and 0.3 for each α and β after finding experimentally that it led to effective parameter search (Gelman et al., 1996). of parameters. The new parameters are accepted proportionally to the ratio of the two likelihoods. Multiple chains were run using 4 different bin sizes per dimension and monitored for convergence using the generalized Gelman-Rubin diagnostic method (Brooks and Gelman, 1998). This methodology leaves us not only with the Monte Carlo estimate of the expected value for each parameter, but also a sampling distribution that quantifies the uncertainty in the parameters themselves. Availability: Availability is estimated as the ratio of the observed frequency of a label to its expected frequency given the parameters which define its distribution. The expected frequency, a marginalization of the color space for the φ function, is calculated using the midpoint integration approximation. αk = P(ksaid, ktrue) P (ktrue) (8) count(k)/N fx P(ktrue|x)P(x) 5 Model Evaluation LUX</context>
</contexts>
<marker>Brooks, Gelman, 1998</marker>
<rawString>Stephen P. Brooks and Andrew Gelman. 1998. General methods for monitoring convergence of iterative simulations. Journal of Computational and Graphical Statistics, 7(4):434–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
<author>Stephen A Della Pietra</author>
<author>Jennifer C Lai</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="31735" citStr="Brown et al., 1992" startWordPosition="5113" endWordPosition="5116">his is calculated using Eq. 10 across all N data points in the held-out test set. LLV (M) is used when computing perplexity and Aikake Information Criterion (AIC). We report all measures in bits. LLV (M) = lo92 PM(Ktrue, Ksaid|X) �= lo92 PM(ktrue i , ksaid i |xi) (10) i A more general measure of model fit is the log likelihood of the color values and their labels jointly across the training set, LL(V ), given the model. It is defined and calculated analogously. Perplexity Perplexity has been used in past research to measure the performance of statistical language models (Jelinek et al., 1977; Brown et al., 1992). Lower perplexity means that the model is less surprised by the data and so describes it more precisely. We use it here to measure how well a model encodes the regularities in color descriptions. Akaike Information Criterion: AIC is derived from information theory (Akaike, 1974) and balances the model’s fit to the data with the complexity of the model by penalizing a larger number of parameters. The intuition is that a smaller AIC indicates a better balance of parameters and model fit. 5.3 Evaluation Results Table 1 summarizes the decision-based evaluation results.6 We see little penalty for </context>
</contexts>
<marker>Brown, Pietra, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An estimate of an upper bound for the entropy of English. Computational Linguistics, 18(1):31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="7523" citStr="Bruni et al., 2012" startWordPosition="1174" endWordPosition="1177">antic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evaluation compares models of grounded semantics to more direct models of word–world associations.) Previous research has modeled these categories as 104 regions of suitable perceptual feature spaces. Researchers have explored explicit spaces of high-level perceptual attributes (Farhadi et al., 2009; Silberer et al., 2013), approximations to such spaces (Matuszek et al., 2012), or low-level feature spaces such as Bag of Visual Words (Bruni et al., 2012) or Histogram of Gradients (Krishnamurthy and Kollar, 2013). We specifically follow G¨ardenfors (2000) and J¨ager (2010) in assuming that color categories are convex regions in an underlying color space, and are not just determined by prototypical color values, such as in Andreas and Klein (2014). However, unlike previous grounded semantics, we do not assume that words name categories unequivocally. Speakers may vary in how they interpret a word, so we treat the link between words and categories probabilistically. The difference makes training our model more indirect than previous approaches t</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="28467" citStr="Chen and Goodman, 1996" startWordPosition="4574" endWordPosition="4578">oviding metrics and results that suggest that LUX escapes these objections and captures almost all of the structure in subjects’ responses. 5.1 Alternative Models To test LUX’s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels directly in each discretized bin. Similar histogram models have been developed by Chuang et al. (2008) and (Heer and Stone, 2012) to build interfaces for interacting with color that are informed by human categorization and naming. More precisely, our HM uses a linear interpolation method (Chen and Goodman, 1996) to combine three histograms of various 109 granularity.5 This amounts to predicting responses by querying the training data. HM has the potential to expose whether LUX is missing important features of the distribution of color descriptions. We also built a direct model of subjects’ choices of color terms. Instead of appealing to the applicability and availability of a color label, it works with the observed frequency of a color label and a Gaussian model of the probability of a color value for each label, as in Eq. 9: P(ksaid, ktrue|x) ∝ P(x|ktrue)P(ksaid, ktrue) (9) This Gaussian model (GM) </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: a test of grounded language acquisition.</title>
<date>2008</date>
<contexts>
<context position="6424" citStr="Chen and Mooney, 2008" startWordPosition="1012" endWordPosition="1015">vide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and </context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: a test of grounded language acquisition.</rawString>
</citation>
<citation valid="false">
<booktitle>In ICML ’08: Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>128--135</pages>
<marker></marker>
<rawString>In ICML ’08: Proceedings of the 25th international conference on Machine learning, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddhartha Chib</author>
<author>Edward Greenberg</author>
</authors>
<title>Understanding the Metropolis–Hastings algorithm.</title>
<date>1995</date>
<journal>The American Statistician,</journal>
<volume>49</volume>
<issue>4</issue>
<contexts>
<context position="26206" citStr="Chib and Greenberg, 1995" startWordPosition="4218" endWordPosition="4221">ribution of responses of the term across other color dimensions. The optimization process is a parameter search method which uses as an objective function the probability of ndi,k in Eq. 7 for all d,i, and k. Parameter Search: We adopt a Bayesian coordinate descent which sequentially samples the certain region parameter, µ, and the shape and rate parameters (α and β) of the F distributions for all d and k independently. It also samples the estimated normalization constant, ZdK. More specifically, the sampling is done using Metropolis-Hastings Markov Chain Monte Carlo (Metropolis et al., 1953; Chib and Greenberg, 1995), which performs a Gaussian random walk on the parameters4. For each sample, the likelihood of the data, derived from the Binomial variables, is compared for the new and old set 4We set the standard deviation of the sampling Gaussian to be 1 for each µ and 0.3 for each α and β after finding experimentally that it led to effective parameter search (Gelman et al., 1996). of parameters. The new parameters are accepted proportionally to the ratio of the two likelihoods. Multiple chains were run using 4 different bin sizes per dimension and monitored for convergence using the generalized Gelman-Rub</context>
</contexts>
<marker>Chib, Greenberg, 1995</marker>
<rawString>Siddhartha Chib and Edward Greenberg. 1995. Understanding the Metropolis–Hastings algorithm. The American Statistician, 49(4):327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Maureen Stone</author>
<author>Pat Hanrahan</author>
</authors>
<title>A probabilistic model of the categorical association between colors.</title>
<date>2008</date>
<booktitle>In Color Imaging Conference,</booktitle>
<pages>6--11</pages>
<contexts>
<context position="6819" citStr="Chuang et al. (2008)" startWordPosition="1067" endWordPosition="1070">unding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evaluation compares models of grounded semantics to more direct models of word–world associations.) Previous research has modeled these categories as 104 regions of suitable perceptual feature spaces. Researchers have explored explicit spaces of high-level perceptual attributes (Farhadi et al., 2009; Silberer et al., 2013), approximations to such spac</context>
<context position="9057" citStr="Chuang et al., 2008" startWordPosition="1416" endWordPosition="1419">rmative as possible. We have no evidence that our speakers do that. We assume only that speakers’ utterances are reliable and mirror prevailing usage. Prior work by cognitive scientists has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. </context>
<context position="28256" citStr="Chuang et al. (2008)" startWordPosition="4539" endWordPosition="4542"> anticipate two arguments against our model: first, that the representation is too simple; second, that factoring speakers’ choices through a model of meaning is too cumbersome. We rebut these arguments by providing metrics and results that suggest that LUX escapes these objections and captures almost all of the structure in subjects’ responses. 5.1 Alternative Models To test LUX’s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels directly in each discretized bin. Similar histogram models have been developed by Chuang et al. (2008) and (Heer and Stone, 2012) to build interfaces for interacting with color that are informed by human categorization and naming. More precisely, our HM uses a linear interpolation method (Chen and Goodman, 1996) to combine three histograms of various 109 granularity.5 This amounts to predicting responses by querying the training data. HM has the potential to expose whether LUX is missing important features of the distribution of color descriptions. We also built a direct model of subjects’ choices of color terms. Instead of appealing to the applicability and availability of a color label, it w</context>
</contexts>
<marker>Chuang, Stone, Hanrahan, 2008</marker>
<rawString>Jason Chuang, Maureen Stone, and Pat Hanrahan. 2008. A probabilistic model of the categorical association between colors. In Color Imaging Conference, pages 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Cumming</author>
</authors>
<title>Coordination and content.</title>
<date>2013</date>
<journal>Philosophers’ Imprint,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="39658" citStr="Cumming, 2013" startWordPosition="6425" endWordPosition="6426">ker does not take x into account in choosing one of the applicable words to say (one way the speaker could do this, for example, would be to prefer terms that were more informative about the target color x). Instead, the speaker simply samples from the candidates. That’s why the speaker’s choice reveals only what the semantics says about x. Technically, this makes semantics a Nash equilibrium, where the information the hearer recovers from an utterance is exactly the information the speaker intends to express—in keeping with a longstanding tradition in the philosophy of language (Lewis, 1969; Cumming, 2013). By contrast, researchers such as Smith et al. (2013) adopt broadly similar formal assumptions but predict asymmetries where sophisticated listeners can second-guess naive speakers’ choices and recover “extra” information that the speaker has revealed incidentally and unintentionally. The difference between this approach and ours eventually leads to a difference in the priors over utterances, but it’s best explained through the different utilities that motivate speakers’ different choices in the first place. Smith et al. (2013) assume speakers want to be informative; we Hue Probability 1.0 0.</context>
</contexts>
<marker>Cumming, 2013</marker>
<rawString>Sam Cumming. 2013. Coordination and content. Philosophers’ Imprint, 13(4):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin R Dawson</author>
<author>Jeremy Wright</author>
<author>Antons Rebguns</author>
<author>Marco Valenzuela Esc´arcega</author>
<author>Daniel Fried</author>
<author>Paul R Cohen</author>
</authors>
<title>A generative probabilistic framework for learning spatial language.</title>
<date>2013</date>
<booktitle>In 2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL),</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<marker>Dawson, Wright, Rebguns, Esc´arcega, Fried, Cohen, 2013</marker>
<rawString>Colin R. Dawson, Jeremy Wright, Antons Rebguns, Marco Valenzuela Esc´arcega, Daniel Fried, and Paul R. Cohen. 2013. A generative probabilistic framework for learning spatial language. In 2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL), pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Iris Oved</author>
<author>Matthew Stone</author>
</authors>
<title>Societal grounding is essential to meaningful language use.</title>
<date>2006</date>
<booktitle>In Proceedings of the Twenty-first National Conference on Artificial Intelligence,</booktitle>
<pages>747--754</pages>
<contexts>
<context position="6144" citStr="DeVault et al., 2006" startWordPosition="969" endWordPosition="972">thodology opens up new prospects for research on negotiating meaning interactively (Larsson, 2013) with principled representations and with broad coverage. In fact, many practical situated dialogue systems already identify unfamiliar objects by color. We expect that LUX will provide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associa</context>
</contexts>
<marker>DeVault, Oved, Stone, 2006</marker>
<rawString>David DeVault, Iris Oved, and Matthew Stone. 2006. Societal grounding is essential to meaningful language use. In Proceedings of the Twenty-first National Conference on Artificial Intelligence, pages 747–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Fairchild</author>
</authors>
<title>Color Appearance Models.</title>
<date>2013</date>
<booktitle>The Wiley-IS&amp;T Series in Imaging Science and Technology.</booktitle>
<publisher>Wiley.</publisher>
<contexts>
<context position="13427" citStr="Fairchild (2013)" startWordPosition="2088" endWordPosition="2089">in the spectrum of incoming light. Cameras and screens that use the red– green–blue (RGB) color space are designed roughly to correspond to these responses. However, colors in the visual system summarize spectral profiles rather than mere wavelengths of light. For example, we see colors like cyan (green plus blue without red), magenta (blue plus red without green) and yellow (red plus green without blue) as intermediate saturated colors between the familiar primaries. This naturally leads to a wheel of hues describing the relative prominence of different spectral components along a continuum. Fairchild (2013) provides an overview of color appearance. To capture this variation, we’ll work in the simple hue–saturation–value (HSV) color space that’s common in computer graphics and color picker user interfaces (Hughes et al., 2013) and implemented in python’s native colorsys package. This coordinate system represents colors with three distinct qualitative dimensions: Hue (H) represents changes in tint around a color wheel, Saturation (S) represents the relative proportion of color versus gray, and Value (V) represents the location on the white–black continuum. We will associate color categories with r</context>
</contexts>
<marker>Fairchild, 2013</marker>
<rawString>Mark D. Fairchild. 2013. Color Appearance Models. The Wiley-IS&amp;T Series in Imaging Science and Technology. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delia Graff Fara</author>
</authors>
<title>Shifting sands: An interestrelative theory of vagueness.</title>
<date>2000</date>
<journal>Philosophical Topics,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>Fara, 2000</marker>
<rawString>Delia Graff Fara. 2000. Shifting sands: An interestrelative theory of vagueness. Philosophical Topics, 28(1):45–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1778--1785</pages>
<contexts>
<context position="7366" citStr="Farhadi et al., 2009" startWordPosition="1147" endWordPosition="1150">ons between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evaluation compares models of grounded semantics to more direct models of word–world associations.) Previous research has modeled these categories as 104 regions of suitable perceptual feature spaces. Researchers have explored explicit spaces of high-level perceptual attributes (Farhadi et al., 2009; Silberer et al., 2013), approximations to such spaces (Matuszek et al., 2012), or low-level feature spaces such as Bag of Visual Words (Bruni et al., 2012) or Histogram of Gradients (Krishnamurthy and Kollar, 2013). We specifically follow G¨ardenfors (2000) and J¨ager (2010) in assuming that color categories are convex regions in an underlying color space, and are not just determined by prototypical color values, such as in Andreas and Klein (2014). However, unlike previous grounded semantics, we do not assume that words name categories unequivocally. Speakers may vary in how they interpret </context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 1778–1785, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Louis M Gomez</author>
<author>Susan T Dumais</author>
</authors>
<title>The vocabulary problem in human-system communication.</title>
<date>1987</date>
<journal>Communications of the ACM,</journal>
<volume>30</volume>
<issue>11</issue>
<contexts>
<context position="1313" citStr="Furnas et al., 1987" startWordPosition="188" endWordPosition="191">zation boundaries and distributions over preferred vocabulary. We apply the approach to a large data set of color descriptions, where statistical evaluation documents its accuracy. The results are available as a Lexicon of Uncertain Color Standards (LUX), which supports future efforts in grounded language understanding and generation by probabilistically mapping 829 English color descriptions to potentially context-sensitive regions in HSV color space. 1 Introduction To ground natural language semantics in real-world data at large scale requires researchers to confront the vocabulary problem (Furnas et al., 1987). Much of what people say falls in a long tail of increasingly infrequent and specialized items. Moreover, the choice of how to categorize and describe realworld data varies across people. We can’t account for this complexity by deriving one definitive mapping between words and the world. We see this complexity already in free text descriptions of color patches. English has fewer than Figure 1: A visualization of the variability of the descriptions used to name colors within small bins of color space. For each Hue value, the entropy values for each bin along the Saturation and Value dimensions</context>
</contexts>
<marker>Furnas, Landauer, Gomez, Dumais, 1987</marker>
<rawString>George W. Furnas, Thomas K. Landauer, Louis M. Gomez, and Susan T. Dumais. 1987. The vocabulary problem in human-system communication. Communications of the ACM, 30(11):964–971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter G¨ardenfors</author>
</authors>
<title>Conceptual Spaces.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<marker>G¨ardenfors, 2000</marker>
<rawString>Peter G¨ardenfors. 2000. Conceptual Spaces. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gelman</author>
<author>Gareth O Roberts</author>
<author>Walter R Gilks</author>
</authors>
<title>Efficient Metropolis jumping rules.</title>
<date>1996</date>
<journal>Bayesian Statistics</journal>
<volume>5</volume>
<pages>599--607</pages>
<editor>In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. Smith, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="26576" citStr="Gelman et al., 1996" startWordPosition="4286" endWordPosition="4289">e F distributions for all d and k independently. It also samples the estimated normalization constant, ZdK. More specifically, the sampling is done using Metropolis-Hastings Markov Chain Monte Carlo (Metropolis et al., 1953; Chib and Greenberg, 1995), which performs a Gaussian random walk on the parameters4. For each sample, the likelihood of the data, derived from the Binomial variables, is compared for the new and old set 4We set the standard deviation of the sampling Gaussian to be 1 for each µ and 0.3 for each α and β after finding experimentally that it led to effective parameter search (Gelman et al., 1996). of parameters. The new parameters are accepted proportionally to the ratio of the two likelihoods. Multiple chains were run using 4 different bin sizes per dimension and monitored for convergence using the generalized Gelman-Rubin diagnostic method (Brooks and Gelman, 1998). This methodology leaves us not only with the Monte Carlo estimate of the expected value for each parameter, but also a sampling distribution that quantifies the uncertainty in the parameters themselves. Availability: Availability is estimated as the ratio of the observed frequency of a label to its expected frequency giv</context>
</contexts>
<marker>Gelman, Roberts, Gilks, 1996</marker>
<rawString>Andrew Gelman, Gareth O. Roberts, and Walter R. Gilks. 1996. Efficient Metropolis jumping rules. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. Smith, editors, Bayesian Statistics 5, pages 599–607. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert P Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics III: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. Morgan, editors,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="37862" citStr="Grice, 1975" startWordPosition="6126" endWordPosition="6127">ely on linguistic approaches to color meaning and psychological approaches to modeling experimental subjects. Because LUX bridges linguistic theory, psychological data, and system building, LUX also affords a unique set of resources for future research at the intersection of semantics and pragmatics of dialogue. For example, our work explains subjects’ decisions as a straightforward reflection of their communicative goals in a probabilistic setting. Our measures of availability and applicability can be seen as offering computational interpretations of the Gricean Maxims of Manner and Quality (Grice, 1975). However, these particular interpretations don’t give rise to implicatures on our model— largely because our Rational Observer is so inclusive and variable in the descriptions it offers. To show this, we can analyze what an idealized hearer learns about an underlying color x when the speaker uses a color term k: this is P(x|ksaid). The model predictions are formalized in Eq. 11. P(x|ksaid) = P(x|ksaid, ktrue) P(ksaid, ktrue|x)P(x) P(ksaid, ktrue) P(ksaid|ktrue)P(ktrue|x)P(x) = P(ksaid|ktrue)P(ktrue) = P(x|ktrue) (11) We apply Bayes’s rule, exploiting our model assumption that the speaker says</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Herbert P. Grice. 1975. Logic and conversation. In P. Cole and J. Morgan, editors, Syntax and Semantics III: Speech Acts, pages 41–58. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The symbol grounding problem. Physica D: Nonlinear Phenomena,</title>
<date>1990</date>
<pages>42--1</pages>
<contexts>
<context position="6076" citStr="Harnad, 1990" startWordPosition="958" endWordPosition="960">mains as quantity, space, and time. At the same time, the methodology opens up new prospects for research on negotiating meaning interactively (Larsson, 2013) with principled representations and with broad coverage. In fact, many practical situated dialogue systems already identify unfamiliar objects by color. We expect that LUX will provide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1–3):335–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Heer</author>
<author>Maureen Stone</author>
</authors>
<title>Color naming models for color selection, image editing and palette design.</title>
<date>2012</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="6845" citStr="Heer and Stone (2012)" startWordPosition="1072" endWordPosition="1075">imitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evaluation compares models of grounded semantics to more direct models of word–world associations.) Previous research has modeled these categories as 104 regions of suitable perceptual feature spaces. Researchers have explored explicit spaces of high-level perceptual attributes (Farhadi et al., 2009; Silberer et al., 2013), approximations to such spaces (Matuszek et al., 2012)</context>
<context position="9080" citStr="Heer and Stone, 2012" startWordPosition="1420" endWordPosition="1423">We have no evidence that our speakers do that. We assume only that speakers’ utterances are reliable and mirror prevailing usage. Prior work by cognitive scientists has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semanti</context>
<context position="28283" citStr="Heer and Stone, 2012" startWordPosition="4544" endWordPosition="4547">against our model: first, that the representation is too simple; second, that factoring speakers’ choices through a model of meaning is too cumbersome. We rebut these arguments by providing metrics and results that suggest that LUX escapes these objections and captures almost all of the structure in subjects’ responses. 5.1 Alternative Models To test LUX’s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels directly in each discretized bin. Similar histogram models have been developed by Chuang et al. (2008) and (Heer and Stone, 2012) to build interfaces for interacting with color that are informed by human categorization and naming. More precisely, our HM uses a linear interpolation method (Chen and Goodman, 1996) to combine three histograms of various 109 granularity.5 This amounts to predicting responses by querying the training data. HM has the potential to expose whether LUX is missing important features of the distribution of color descriptions. We also built a direct model of subjects’ choices of color terms. Instead of appealing to the applicability and availability of a color label, it works with the observed freq</context>
</contexts>
<marker>Heer, Stone, 2012</marker>
<rawString>Jeffrey Heer and Maureen Stone. 2012. Color naming models for color selection, image editing and palette design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1007– 1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Hughes</author>
<author>Andries van Dam</author>
<author>Morgan McGuire</author>
<author>David F Sklar</author>
<author>James D Foley</author>
<author>Steven K Feiner</author>
<author>Kurt Akeley</author>
</authors>
<title>Computer Graphics: Principles and Practice (3rd Edition).</title>
<date>2013</date>
<publisher>Addison-Wesley Professional.</publisher>
<marker>Hughes, van Dam, McGuire, Sklar, Foley, Feiner, Akeley, 2013</marker>
<rawString>John F. Hughes, Andries van Dam, Morgan McGuire, David F. Sklar, James D. Foley, Steven K. Feiner, and Kurt Akeley. 2013. Computer Graphics: Principles and Practice (3rd Edition). Addison-Wesley Professional.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard J¨ager</author>
</authors>
<title>Natural color categories are convex sets.</title>
<date>2010</date>
<booktitle>Logic, Language and Meaning - 17th Amsterdam Colloquium,</booktitle>
<volume>6042</volume>
<pages>11--20</pages>
<editor>In Maria Aloni, Harald Bastiaanse, Tikitu de Jager, and Katrin Schulz, editors,</editor>
<publisher>Springer.</publisher>
<location>Amsterdam, The Netherlands,</location>
<marker>J¨ager, 2010</marker>
<rawString>Gerhard J¨ager. 2010. Natural color categories are convex sets. In Maria Aloni, Harald Bastiaanse, Tikitu de Jager, and Katrin Schulz, editors, Logic, Language and Meaning - 17th Amsterdam Colloquium, Amsterdam, The Netherlands, December 16-18, 2009, Revised Selected Papers, volume 6042 of Lecture Notes in Computer Science, pages 11–20. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
<author>Robert L Mercer</author>
<author>Lalit R Bahl</author>
<author>James K Baker</author>
</authors>
<title>Perplexity–a measure of the difficulty of speech recognition tasks.</title>
<date>1977</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<pages>62--63</pages>
<contexts>
<context position="31714" citStr="Jelinek et al., 1977" startWordPosition="5109" endWordPosition="5112"> denoted as LLV (M). This is calculated using Eq. 10 across all N data points in the held-out test set. LLV (M) is used when computing perplexity and Aikake Information Criterion (AIC). We report all measures in bits. LLV (M) = lo92 PM(Ktrue, Ksaid|X) �= lo92 PM(ktrue i , ksaid i |xi) (10) i A more general measure of model fit is the log likelihood of the color values and their labels jointly across the training set, LL(V ), given the model. It is defined and calculated analogously. Perplexity Perplexity has been used in past research to measure the performance of statistical language models (Jelinek et al., 1977; Brown et al., 1992). Lower perplexity means that the model is less surprised by the data and so describes it more precisely. We use it here to measure how well a model encodes the regularities in color descriptions. Akaike Information Criterion: AIC is derived from information theory (Akaike, 1974) and balances the model’s fit to the data with the complexity of the model by penalizing a larger number of parameters. The intuition is that a smaller AIC indicates a better balance of parameters and model fit. 5.3 Evaluation Results Table 1 summarizes the decision-based evaluation results.6 We se</context>
</contexts>
<marker>Jelinek, Mercer, Bahl, Baker, 1977</marker>
<rawString>Fred Jelinek, Robert L. Mercer, Lalit R. Bahl, and James K. Baker. 1977. Perplexity–a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62:S63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kay</author>
<author>Brent Berlin</author>
<author>Luisa Maffi</author>
<author>William R Merrifield</author>
<author>Richard Cook</author>
</authors>
<title>The World Color Survey.</title>
<date>2009</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="8798" citStr="Kay et al., 2009" startWordPosition="1375" endWordPosition="1378">ew layer of uncertainty that describes what category the speaker uses. Similar kinds of uncertainty can be found in Bayesian models of speaker strategy, such as that of Smith et al. (2013). However, this research has assumed that speakers aim to be as informative as possible. We have no evidence that our speakers do that. We assume only that speakers’ utterances are reliable and mirror prevailing usage. Prior work by cognitive scientists has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming i</context>
</contexts>
<marker>Kay, Berlin, Maffi, Merrifield, Cook, 2009</marker>
<rawString>Paul Kay, Brent Berlin, Luisa Maffi, William R. Merrifield, and Richard Cook. 2009. The World Color Survey. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="3950" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="627" endWordPosition="630"> tan one or a tan dog and a white one. prehensibly describe either of two dogs as the tan one. Systems that robustly understand or generate descriptions of colors in situated dialogue need models of meaning that capture this variability. This paper makes two key contributions towards this challenge. First, we present a methodology to infer a corpus-based model of meaning that accounts for possible differences in word usage across different speakers. As we explain in Section 2, our approach differs from the typical perspective in grounded semantics (Tellex et al., 2011a; Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), where a meaning is reduced to a single classifier that collapses patterns of variation. Instead, our model allows for variability in meaning by positing uncertainty in classification boundaries that can get resolved when a speaker chooses to use a word on a specific occasion. We explain the model and its theoretical rationale in Section 3. Second, we develop and release a Lexicon of Uncertain Color Standards (LUX) by applying our methodology to color descriptions. LUX is an interpretation of 829 distinct English color descriptions as distributions over regions of the Hue–SaturationValue colo</context>
<context position="6503" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="1023" endWordPosition="1026"> systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evaluation compares models of grounde</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics, 1(2):193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Kyburg</author>
<author>Michael Morreau</author>
</authors>
<title>Fitting words: Vague words in context.</title>
<date>2000</date>
<journal>Linguistics and Philosophy,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="41772" citStr="Kyburg and Morreau (2000)" startWordPosition="6744" endWordPosition="6747">ed yellowish green should make objects in the same color range more likely to be referred to as yellowish green. We could use LUX straightforwardly to represent such conceptual pacts (Brennan and Clark, 1996) via a posterior over threshold parameters. It’s natural to look for empirical evidence to assess the effectiveness of such representations of dependent context. A particularly important case involves descriptive material that distinguishes a target referent from salient alternatives, as in the understanding or generation of referring expressions (Krahmer and van Deemter, 2012). Following Kyburg and Morreau (2000), we could represent this using LUX via a posterior over the threshold parameters that fit the target but exclude its alternatives. Again, our model associates such goals with quantitative measures that future research can explore empirically. Meo et al. (2014) present an initial exploration of this idea. These open questions complement the key advantage that makes uncertainty about meaning crucial to the success of the model and experiments we have reported here. Many kinds of language use seem to be highly variable, and approaches to grounded semantics need ways to make room for this variabi</context>
</contexts>
<marker>Kyburg, Morreau, 2000</marker>
<rawString>Alice Kyburg and Michael Morreau. 2000. Fitting words: Vague words in context. Linguistics and Philosophy, 23(6):577–597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Maurice Gisele Lammens</author>
</authors>
<title>A computational model of color perception and color naming.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>SUNY Buffalo.</institution>
<contexts>
<context position="8814" citStr="Lammens, 1994" startWordPosition="1379" endWordPosition="1380">ainty that describes what category the speaker uses. Similar kinds of uncertainty can be found in Bayesian models of speaker strategy, such as that of Smith et al. (2013). However, this research has assumed that speakers aim to be as informative as possible. We have no evidence that our speakers do that. We assume only that speakers’ utterances are reliable and mirror prevailing usage. Prior work by cognitive scientists has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a </context>
</contexts>
<marker>Lammens, 1994</marker>
<rawString>Johan Maurice Gisele Lammens. 1994. A computational model of color perception and color naming. Ph.D. thesis, SUNY Buffalo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
</authors>
<title>Formal semantics for perceptual classification.</title>
<date>2013</date>
<booktitle>Journal of Logic and Computation. Advance online publication.</booktitle>
<pages>10--1093</pages>
<contexts>
<context position="5621" citStr="Larsson, 2013" startWordPosition="889" endWordPosition="891">he model makes better quantitative predictions than a brute-force memorization model; it seems to generalize to unseen data in more meaningful ways. At the same time, our meanings work as well as special-purpose models to explain speaker choice, even though our model supports diverse other reasoning. See Section 5. We see color as the first of many applications of our methodology, and are optimistic about learning vague meanings for other continuous domains as quantity, space, and time. At the same time, the methodology opens up new prospects for research on negotiating meaning interactively (Larsson, 2013) with principled representations and with broad coverage. In fact, many practical situated dialogue systems already identify unfamiliar objects by color. We expect that LUX will provide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of</context>
</contexts>
<marker>Larsson, 2013</marker>
<rawString>Staffan Larsson. 2013. Formal semantics for perceptual classification. Journal of Logic and Computation. Advance online publication. doi: 10.1093/logcom/ext059.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lassiter</author>
</authors>
<title>Vagueness as probabilistic linguistic knowledge.</title>
<date>2009</date>
<booktitle>Vagueness in Communication - International Workshop, ViC 2009, held as part of ESSLLI 2009,</booktitle>
<volume>6517</volume>
<pages>127--150</pages>
<editor>In Rick Nouwen, Robert van Rooij, Uli Sauerland, and Hans-Christian Schmitz, editors,</editor>
<publisher>Springer.</publisher>
<location>Bordeaux, France,</location>
<contexts>
<context position="16091" citStr="Lassiter (2009)" startWordPosition="2509" endWordPosition="2510">tions. They are shown for yellowish green in Figure 3 as the τ distributions. The figure shows that there is a central range of hues, between the τ distributions, that is definitely yellowish green. The τ distributions peak at the most likely boundaries for yellowish green, encompassing a broad region that’s frequently called yellowish green. Further away, threshold values and yellowish green utterances alike become rapidly less likely. Probability 0.8 0.6 0.4 0.2 1.0 τLower µLower µUpper τUpper φHue Y ellowishGreen Yellowish Green data 106 Our representation is motivated by Barker (2002) and Lassiter (2009), who show how sets of possible thresholds1 can account for many of our intuitions about the use of vague language. Their analysis invites us to capture semantic variability through two geometric constructs. First, there is a certain interval, parameterized by two points, µLower and µUpper, within which a color description definitely applies. Outside this interval are regions of borderline cases, delimited by probabilistically-varying thresholds τLower and τUpper, where the color description sometimes applies. We represent the position of the threshold with a F(α, β) distribution, a standard s</context>
</contexts>
<marker>Lassiter, 2009</marker>
<rawString>Daniel Lassiter. 2009. Vagueness as probabilistic linguistic knowledge. In Rick Nouwen, Robert van Rooij, Uli Sauerland, and Hans-Christian Schmitz, editors, Vagueness in Communication - International Workshop, ViC 2009, held as part of ESSLLI 2009, Bordeaux, France, July 20-24, 2009. Revised Selected Papers, volume 6517 of Lecture Notes in Computer Science, pages 127–150. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Lewis</author>
</authors>
<title>Convention: A Philosophical Study.</title>
<date>1969</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="39642" citStr="Lewis, 1969" startWordPosition="6423" endWordPosition="6424">ice, the speaker does not take x into account in choosing one of the applicable words to say (one way the speaker could do this, for example, would be to prefer terms that were more informative about the target color x). Instead, the speaker simply samples from the candidates. That’s why the speaker’s choice reveals only what the semantics says about x. Technically, this makes semantics a Nash equilibrium, where the information the hearer recovers from an utterance is exactly the information the speaker intends to express—in keeping with a longstanding tradition in the philosophy of language (Lewis, 1969; Cumming, 2013). By contrast, researchers such as Smith et al. (2013) adopt broadly similar formal assumptions but predict asymmetries where sophisticated listeners can second-guess naive speakers’ choices and recover “extra” information that the speaker has revealed incidentally and unintentionally. The difference between this approach and ours eventually leads to a difference in the priors over utterances, but it’s best explained through the different utilities that motivate speakers’ different choices in the first place. Smith et al. (2013) assume speakers want to be informative; we Hue Pr</context>
</contexts>
<marker>Lewis, 1969</marker>
<rawString>David K. Lewis. 1969. Convention: A Philosophical Study. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas Fitzgerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML-12),</booktitle>
<pages>1671--1678</pages>
<contexts>
<context position="3917" citStr="Matuszek et al., 2012" startWordPosition="623" endWordPosition="626">gs as a brown dog and a tan one or a tan dog and a white one. prehensibly describe either of two dogs as the tan one. Systems that robustly understand or generate descriptions of colors in situated dialogue need models of meaning that capture this variability. This paper makes two key contributions towards this challenge. First, we present a methodology to infer a corpus-based model of meaning that accounts for possible differences in word usage across different speakers. As we explain in Section 2, our approach differs from the typical perspective in grounded semantics (Tellex et al., 2011a; Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), where a meaning is reduced to a single classifier that collapses patterns of variation. Instead, our model allows for variability in meaning by positing uncertainty in classification boundaries that can get resolved when a speaker chooses to use a word on a specific occasion. We explain the model and its theoretical rationale in Section 3. Second, we develop and release a Lexicon of Uncertain Color Standards (LUX) by applying our methodology to color descriptions. LUX is an interpretation of 829 distinct English color descriptions as distributions over region</context>
<context position="6471" citStr="Matuszek et al., 2012" startWordPosition="1019" endWordPosition="1022">ge of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evalu</context>
</contexts>
<marker>Matuszek, Fitzgerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1671–1678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Mavridis</author>
<author>Deb Roy</author>
</authors>
<title>Grounded situation models for robots: Where words and percepts meet.</title>
<date>2006</date>
<booktitle>In Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference on,</booktitle>
<pages>4690--4697</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="10228" citStr="Mavridis and Roy, 2006" startWordPosition="1594" endWordPosition="1597">tegories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is derived, interpretation typically involves a recursive process of finding referents that fit lexical categories and relationships (Mavridis and Roy, 2006; Tellex et al., 2011a). While this paper does not explicitly address how our meanings might be used in conjunction with such techniques, we see no fundamental obstacle to doing so—for example, by resolving references probabilistically and marginalizing over uncertainty in meaning. 3 Using Vague Color Terms: A Model Our model involves two significant innovations over previous approaches to grounded meaning. The first is to capture the vagueness and flexibility of grounded meaning with semantic representations that treat meaning as uncertain. We represent the semantics of a color description wi</context>
</contexts>
<marker>Mavridis, Roy, 2006</marker>
<rawString>Nikolaos Mavridis and Deb Roy. 2006. Grounded situation models for robots: Where words and percepts meet. In Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference on, pages 4690– 4697. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Meo</author>
<author>Brian McMahan</author>
<author>Matthew Stone</author>
</authors>
<title>Generating and resolving vague color references.</title>
<date>2014</date>
<booktitle>In SEMDIAL 2014: THE 18th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>107--115</pages>
<contexts>
<context position="42033" citStr="Meo et al. (2014)" startWordPosition="6786" endWordPosition="6789">ook for empirical evidence to assess the effectiveness of such representations of dependent context. A particularly important case involves descriptive material that distinguishes a target referent from salient alternatives, as in the understanding or generation of referring expressions (Krahmer and van Deemter, 2012). Following Kyburg and Morreau (2000), we could represent this using LUX via a posterior over the threshold parameters that fit the target but exclude its alternatives. Again, our model associates such goals with quantitative measures that future research can explore empirically. Meo et al. (2014) present an initial exploration of this idea. These open questions complement the key advantage that makes uncertainty about meaning crucial to the success of the model and experiments we have reported here. Many kinds of language use seem to be highly variable, and approaches to grounded semantics need ways to make room for this variability both in the semantic representations they learn and the algorithms that induce these representations from language data. We have argued that uncertainty about meaning is a powerful new tool to do this. We look forward to future work addressing uncertainty </context>
</contexts>
<marker>Meo, McMahan, Stone, 2014</marker>
<rawString>Timothy Meo, Brian McMahan, and Matthew Stone. 2014. Generating and resolving vague color references. In SEMDIAL 2014: THE 18th Workshop on the Semantics and Pragmatics of Dialogue, pages 107– 115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Metropolis</author>
<author>Arianna W Rosenbluth</author>
<author>Marshall N Rosenbluth</author>
<author>Augusta H Teller</author>
<author>Edward Teller</author>
</authors>
<title>Equation of state calculations by fast computing machines.</title>
<date>1953</date>
<journal>The Journal of Chemical Physics,</journal>
<volume>21</volume>
<issue>6</issue>
<contexts>
<context position="26179" citStr="Metropolis et al., 1953" startWordPosition="4214" endWordPosition="4217">ilability αk and the distribution of responses of the term across other color dimensions. The optimization process is a parameter search method which uses as an objective function the probability of ndi,k in Eq. 7 for all d,i, and k. Parameter Search: We adopt a Bayesian coordinate descent which sequentially samples the certain region parameter, µ, and the shape and rate parameters (α and β) of the F distributions for all d and k independently. It also samples the estimated normalization constant, ZdK. More specifically, the sampling is done using Metropolis-Hastings Markov Chain Monte Carlo (Metropolis et al., 1953; Chib and Greenberg, 1995), which performs a Gaussian random walk on the parameters4. For each sample, the likelihood of the data, derived from the Binomial variables, is compared for the new and old set 4We set the standard deviation of the sampling Gaussian to be 1 for each µ and 0.3 for each α and β after finding experimentally that it led to effective parameter search (Gelman et al., 1996). of parameters. The new parameters are accepted proportionally to the ratio of the two likelihoods. Multiple chains were run using 4 different bin sizes per dimension and monitored for convergence using</context>
</contexts>
<marker>Metropolis, Rosenbluth, Rosenbluth, Teller, Teller, 1953</marker>
<rawString>Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. 1953. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6):1087–1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randall Munroe</author>
</authors>
<title>Color survey results. Online at http://blog.xkcd.com/2010/05/03/color-surveyresults/.</title>
<date>2010</date>
<contexts>
<context position="4822" citStr="Munroe, 2010" startWordPosition="769" endWordPosition="770"> specific occasion. We explain the model and its theoretical rationale in Section 3. Second, we develop and release a Lexicon of Uncertain Color Standards (LUX) by applying our methodology to color descriptions. LUX is an interpretation of 829 distinct English color descriptions as distributions over regions of the Hue–SaturationValue color space that describe their possible meanings. As we describe in Section 4, the model is trained by machine learning methods from a subset of Randall Munroe’s 2010 publicly-available corpus of 3.4 million crowdsourced free-text descriptions of color patches (Munroe, 2010). Data, models and visualization software are available at http: //mcmahan.io/lux/. Statistical evaluation of our model against two alternative approaches documents its effectiveness. The model makes better quantitative predictions than a brute-force memorization model; it seems to generalize to unseen data in more meaningful ways. At the same time, our meanings work as well as special-purpose models to explain speaker choice, even though our model supports diverse other reasoning. See Section 5. We see color as the first of many applications of our methodology, and are optimistic about learni</context>
<context position="23221" citStr="Munroe, 2010" startWordPosition="3729" endWordPosition="3730">walk optimization method. This form of approximate Bayesian inference is described in Section 4.2. 4.1 Munroe Color Corpus In 2010, Munroe elicited descriptions of color patches over the web. His platform asked users for background information such as sex, colorblindness, and monitor type, then presented color patches and let the user freely name them. The setup didn’t ensure that users see controlled colors or that users’ responses are reliable, but the experiment collected over 3.4M items pairing RGB values with text descriptions. Munroe’s methodology, data and results are published online (Munroe, 2010).3 Munroe summarizes his results with 954 idealized colors—RGB values that best exemplify high frequency color labels. In effect, Munroe’s summary offers a prototype theory of color vocabulary, like that of Andreas and Klein (2014). An alternative theory, which we explore, is that variability in the applicability of labels is an important part of people’s knowledge of color semantics. We compare the two theories explicitly in Section 5. Our experiments focus on a subset of Munroe’s data comprising 2,176,417 data points and 829 color descriptions, divided into a training set of 70%, a 5% develo</context>
</contexts>
<marker>Munroe, 2010</marker>
<rawString>Randall Munroe. 2010. Color survey results. Online at http://blog.xkcd.com/2010/05/03/color-surveyresults/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimele Persaud</author>
<author>Pernille Hemmer</author>
</authors>
<title>The influence of knowledge and expectations for color on episodic memory.</title>
<date>2014</date>
<booktitle>Proceedings of the 36th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1162--1167</pages>
<editor>In P Bello, M Guarini, M McShane, and B Scassellati, editors,</editor>
<contexts>
<context position="8923" citStr="Persaud and Hemmer, 2014" startWordPosition="1393" endWordPosition="1396">Bayesian models of speaker strategy, such as that of Smith et al. (2013). However, this research has assumed that speakers aim to be as informative as possible. We have no evidence that our speakers do that. We assume only that speakers’ utterances are reliable and mirror prevailing usage. Prior work by cognitive scientists has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like </context>
</contexts>
<marker>Persaud, Hemmer, 2014</marker>
<rawString>Kimele Persaud and Pernille Hemmer. 2014. The influence of knowledge and expectations for color on episodic memory. In P Bello, M Guarini, M McShane, and B Scassellati, editors, Proceedings of the 36th Annual Conference of the Cognitive Science Society, pages 1162–1167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
<author>Paul Kay</author>
<author>Richard S Cook</author>
</authors>
<title>Focal colors are universal after all.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>102--8386</pages>
<contexts>
<context position="9242" citStr="Regier et al., 2005" startWordPosition="1444" endWordPosition="1447">s has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic</context>
</contexts>
<marker>Regier, Kay, Cook, 2005</marker>
<rawString>Terry Regier, Paul Kay, and Richard S. Cook. 2005. Focal colors are universal after all. Proceedings of the National Academy of Sciences, 102:8386–8391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
<author>Paul Kay</author>
<author>Naveen Khetarpal</author>
</authors>
<title>Color naming reflects optimal partitions of color space.</title>
<date>2007</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>104--1436</pages>
<contexts>
<context position="9349" citStr="Regier et al., 2007" startWordPosition="1461" endWordPosition="1464">eneral application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore enhance software for helping people organize and interact with color (Chuang et al., 2008; Heer and Stone, 2012). Moreover, crosslinguistic evidence suggests that the human perceptual system places strong biases on the meanings of the basic color terms (Regier et al., 2005), perhaps because basic terms must partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Moon</context>
</contexts>
<marker>Regier, Kay, Khetarpal, 2007</marker>
<rawString>Terry Regier, Paul Kay, and Naveen Khetarpal. 2007. Color naming reflects optimal partitions of color space. Proceedings of the National Academy of Sciences, 104:1436–1441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of Semantic Representation with Visual Attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>572--582</pages>
<contexts>
<context position="7390" citStr="Silberer et al., 2013" startWordPosition="1151" endWordPosition="1154">perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let systems discriminate between cases where a word applies and cases where the word does not apply. (Our evaluation compares models of grounded semantics to more direct models of word–world associations.) Previous research has modeled these categories as 104 regions of suitable perceptual feature spaces. Researchers have explored explicit spaces of high-level perceptual attributes (Farhadi et al., 2009; Silberer et al., 2013), approximations to such spaces (Matuszek et al., 2012), or low-level feature spaces such as Bag of Visual Words (Bruni et al., 2012) or Histogram of Gradients (Krishnamurthy and Kollar, 2013). We specifically follow G¨ardenfors (2000) and J¨ager (2010) in assuming that color categories are convex regions in an underlying color space, and are not just determined by prototypical color values, such as in Andreas and Klein (2014). However, unlike previous grounded semantics, we do not assume that words name categories unequivocally. Speakers may vary in how they interpret a word, so we treat the </context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of Semantic Representation with Visual Attributes. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathaniel J Smith</author>
<author>Noah D Goodman</author>
<author>Michael C Frank</author>
</authors>
<title>Learning and using language via recursive pragmatic reasoning about other agents.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3039--3047</pages>
<contexts>
<context position="8370" citStr="Smith et al. (2013)" startWordPosition="1308" endWordPosition="1311">ned by prototypical color values, such as in Andreas and Klein (2014). However, unlike previous grounded semantics, we do not assume that words name categories unequivocally. Speakers may vary in how they interpret a word, so we treat the link between words and categories probabilistically. The difference makes training our model more indirect than previous approaches to grounded meaning. In particular, our model introduces a new layer of uncertainty that describes what category the speaker uses. Similar kinds of uncertainty can be found in Bayesian models of speaker strategy, such as that of Smith et al. (2013). However, this research has assumed that speakers aim to be as informative as possible. We have no evidence that our speakers do that. We assume only that speakers’ utterances are reliable and mirror prevailing usage. Prior work by cognitive scientists has studied color terms extensively, but focused on basic ones— monolexemic, top-level color words with general application and high frequency in a language (Kay et al., 2009; Lammens, 1994). These color categories seem to shape people’s expectations and memory for colors (Persaud and Hemmer, 2014), and patterns of color naming can therefore en</context>
<context position="39712" citStr="Smith et al. (2013)" startWordPosition="6433" endWordPosition="6436">of the applicable words to say (one way the speaker could do this, for example, would be to prefer terms that were more informative about the target color x). Instead, the speaker simply samples from the candidates. That’s why the speaker’s choice reveals only what the semantics says about x. Technically, this makes semantics a Nash equilibrium, where the information the hearer recovers from an utterance is exactly the information the speaker intends to express—in keeping with a longstanding tradition in the philosophy of language (Lewis, 1969; Cumming, 2013). By contrast, researchers such as Smith et al. (2013) adopt broadly similar formal assumptions but predict asymmetries where sophisticated listeners can second-guess naive speakers’ choices and recover “extra” information that the speaker has revealed incidentally and unintentionally. The difference between this approach and ours eventually leads to a difference in the priors over utterances, but it’s best explained through the different utilities that motivate speakers’ different choices in the first place. Smith et al. (2013) assume speakers want to be informative; we Hue Probability 1.0 0.8 0.6 0.4 0.2 0.0 0.005 0.004 0.003 0.002 0.001 0.000 </context>
</contexts>
<marker>Smith, Goodman, Frank, 2013</marker>
<rawString>Nathaniel J. Smith, Noah D. Goodman, and Michael C. Frank. 2013. Learning and using language via recursive pragmatic reasoning about other agents. In Advances in Neural Information Processing Systems 26, pages 3039–3047.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kollar</author>
<author>Steven Dickerson</author>
</authors>
<title>Approaching the symbol grounding problem with probabilistic graphical models. AI magazine,</title>
<date>2011</date>
<pages>32--4</pages>
<contexts>
<context position="3893" citStr="Tellex et al., 2011" startWordPosition="619" endWordPosition="622">ects describe these dogs as a brown dog and a tan one or a tan dog and a white one. prehensibly describe either of two dogs as the tan one. Systems that robustly understand or generate descriptions of colors in situated dialogue need models of meaning that capture this variability. This paper makes two key contributions towards this challenge. First, we present a methodology to infer a corpus-based model of meaning that accounts for possible differences in word usage across different speakers. As we explain in Section 2, our approach differs from the typical perspective in grounded semantics (Tellex et al., 2011a; Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), where a meaning is reduced to a single classifier that collapses patterns of variation. Instead, our model allows for variability in meaning by positing uncertainty in classification boundaries that can get resolved when a speaker chooses to use a word on a specific occasion. We explain the model and its theoretical rationale in Section 3. Second, we develop and release a Lexicon of Uncertain Color Standards (LUX) by applying our methodology to color descriptions. LUX is an interpretation of 829 distinct English color descriptions as d</context>
<context position="6366" citStr="Tellex et al., 2011" startWordPosition="1003" endWordPosition="1006">nfamiliar objects by color. We expect that LUX will provide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let sy</context>
<context position="10249" citStr="Tellex et al., 2011" startWordPosition="1598" endWordPosition="1601">n, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is derived, interpretation typically involves a recursive process of finding referents that fit lexical categories and relationships (Mavridis and Roy, 2006; Tellex et al., 2011a). While this paper does not explicitly address how our meanings might be used in conjunction with such techniques, we see no fundamental obstacle to doing so—for example, by resolving references probabilistically and marginalizing over uncertainty in meaning. 3 Using Vague Color Terms: A Model Our model involves two significant innovations over previous approaches to grounded meaning. The first is to capture the vagueness and flexibility of grounded meaning with semantic representations that treat meaning as uncertain. We represent the semantics of a color description with a distribution ove</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kollar, and Steven Dickerson. 2011a. Approaching the symbol grounding problem with probabilistic graphical models. AI magazine, 32(4):64–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kollar</author>
<author>Steven Dickerson</author>
<author>Matthew R Walter</author>
<author>Ashis Gopal Banerjee</author>
<author>Seth J Teller</author>
<author>Nicholas Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>In Proceedings of the TwentyFifth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1507--1514</pages>
<contexts>
<context position="3893" citStr="Tellex et al., 2011" startWordPosition="619" endWordPosition="622">ects describe these dogs as a brown dog and a tan one or a tan dog and a white one. prehensibly describe either of two dogs as the tan one. Systems that robustly understand or generate descriptions of colors in situated dialogue need models of meaning that capture this variability. This paper makes two key contributions towards this challenge. First, we present a methodology to infer a corpus-based model of meaning that accounts for possible differences in word usage across different speakers. As we explain in Section 2, our approach differs from the typical perspective in grounded semantics (Tellex et al., 2011a; Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), where a meaning is reduced to a single classifier that collapses patterns of variation. Instead, our model allows for variability in meaning by positing uncertainty in classification boundaries that can get resolved when a speaker chooses to use a word on a specific occasion. We explain the model and its theoretical rationale in Section 3. Second, we develop and release a Lexicon of Uncertain Color Standards (LUX) by applying our methodology to color descriptions. LUX is an interpretation of 829 distinct English color descriptions as d</context>
<context position="6366" citStr="Tellex et al., 2011" startWordPosition="1003" endWordPosition="1006">nfamiliar objects by color. We expect that LUX will provide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable categories that let sy</context>
<context position="10249" citStr="Tellex et al., 2011" startWordPosition="1598" endWordPosition="1601">n, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is derived, interpretation typically involves a recursive process of finding referents that fit lexical categories and relationships (Mavridis and Roy, 2006; Tellex et al., 2011a). While this paper does not explicitly address how our meanings might be used in conjunction with such techniques, we see no fundamental obstacle to doing so—for example, by resolving references probabilistically and marginalizing over uncertainty in meaning. 3 Using Vague Color Terms: A Model Our model involves two significant innovations over previous approaches to grounded meaning. The first is to capture the vagueness and flexibility of grounded meaning with semantic representations that treat meaning as uncertain. We represent the semantics of a color description with a distribution ove</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth J Teller, and Nicholas Roy. 2011b. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the TwentyFifth AAAI Conference on Artificial Intelligence, pages 1507–1514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Procedures as a representation for data in a computer program for understanding natural language.</title>
<date>1970</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="6345" citStr="Winograd, 1970" startWordPosition="1001" endWordPosition="1002">ready identify unfamiliar objects by color. We expect that LUX will provide a broadly useful resource to extend the range of descriptions such systems can generate and understand. 2 Related Work Grounded semantics is the task of mapping representations of linguistic meaning to the physical world, whether by perceptual mechanisms (Harnad, 1990) or with the assistance of social interaction (DeVault et al., 2006). In this paper, we are particularly concerned with grounding the meanings of primitive vocabulary. However, the ultimate test of grounded semantics—whether it is understanding commands (Winograd, 1970; Tellex et al., 2011b), describing states of the world (Chen and Mooney, 2008), or identifying objects (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Dawson et al., 2013)—is the ability to interpret or generate utterances using lexical and compositional semantics so as to evoke appropriate real-world referents. Grounded semantics therefore involves more than just quantifying the associations between words and perceptual representations, as Chuang et al. (2008) and Heer and Stone (2012) do for color. Grounded semantics involves interpreting semantic primitives in terms of composable c</context>
<context position="9888" citStr="Winograd, 1970" startWordPosition="1548" endWordPosition="1549">ust partition the perceptual space in an efficient way (Regier et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is derived, interpretation typically involves a recursive process of finding referents that fit lexical categories and relationships (Mavridis and Roy, 2006; Tellex et al., 2011a). While this paper does not explicitly address how our meanings might be used in conjunction with such techniques, we see no fundamental obstacle to doing so—for example, by resolving references probabilistically and marginalizing over u</context>
</contexts>
<marker>Winograd, 1970</marker>
<rawString>Terry Winograd. 1970. Procedures as a representation for data in a computer program for understanding natural language. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="3262" citStr="Young et al. (2014)" startWordPosition="514" endWordPosition="517">son to think that this variability conceals consistent meanings. In formal semantics, one of the hallmarks of vague language is that speakers can make it more precise in alternative, incompatible ways (Barker, 2002). We see this in practice as well, for example with the image of Figure 2, where subjects com103 Transactions of the Association for Computational Linguistics, vol. 3, pp. 103–115, 2015. Action Editor: Lillian Lee. Submission batch: 11/2014; Published 2/2015. c�2015 Association for Computational Linguistics. Figure 2: Image by flickr user Joanne Bacon (jlbacon) from the data set of Young et al. (2014), whose subjects describe these dogs as a brown dog and a tan one or a tan dog and a white one. prehensibly describe either of two dogs as the tan one. Systems that robustly understand or generate descriptions of colors in situated dialogue need models of meaning that capture this variability. This paper makes two key contributions towards this challenge. First, we present a methodology to infer a corpus-based model of meaning that accounts for possible differences in word usage across different speakers. As we explain in Section 2, our approach differs from the typical perspective in grounded</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="9958" citStr="Zelle and Mooney, 1996" startWordPosition="1555" endWordPosition="1558"> et al., 2007). We depart from research on basic color naming in considering a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is derived, interpretation typically involves a recursive process of finding referents that fit lexical categories and relationships (Mavridis and Roy, 2006; Tellex et al., 2011a). While this paper does not explicitly address how our meanings might be used in conjunction with such techniques, we see no fundamental obstacle to doing so—for example, by resolving references probabilistically and marginalizing over uncertainty in meaning. 3 Using Vague Color Terms: A Model Our model in</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="10041" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1567" endWordPosition="1570"> a much wider range of terms, much like Andreas and Klein (2014). We consider subordinate, non-basic terms like beige or lavender; modified colors like light blue or bright green; and named subcategories like olive green, navy blue or brick red. In order to use semantic primitives for understanding, it’s necessary to combine them into an integrated sentence-level representation: this is the problem of semantic parsing. Semantic parsers can be built by hand (Winograd, 1970), induced through inductive logic programming (Zelle and Mooney, 1996), or treated as a structured classification problem (Zettlemoyer and Collins, 2005). Once a suitable logical form is derived, interpretation typically involves a recursive process of finding referents that fit lexical categories and relationships (Mavridis and Roy, 2006; Tellex et al., 2011a). While this paper does not explicitly address how our meanings might be used in conjunction with such techniques, we see no fundamental obstacle to doing so—for example, by resolving references probabilistically and marginalizing over uncertainty in meaning. 3 Using Vague Color Terms: A Model Our model involves two significant innovations over previous approaches to grounded meaning. Th</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, pages 658–666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>