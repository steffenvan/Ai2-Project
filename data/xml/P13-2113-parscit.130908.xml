<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033173">
<title confidence="0.996381">
Part-of-speech tagging with antagonistic adversaries
</title>
<author confidence="0.994847">
Anders Søgaard
</author>
<affiliation confidence="0.9975675">
Center for Language Technology
University of Copenhagen
</affiliation>
<address confidence="0.589362">
DK-2300 Copenhagen S
</address>
<email confidence="0.997752">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922045454546">
Supervised NLP tools and on-line services
are often used on data that is very dif-
ferent from the manually annotated data
used during development. The perfor-
mance loss observed in such cross-domain
applications is often attributed to covari-
ate shifts, with out-of-vocabulary effects
as an important subclass. Many discrim-
inative learning algorithms are sensitive to
such shifts because highly indicative fea-
tures may swamp other indicative features.
Regularized and adversarial learning algo-
rithms have been proposed to be more ro-
bust against covariate shifts. We present
a new perceptron learning algorithm us-
ing antagonistic adversaries and compare
it to previous proposals on 12 multilin-
gual cross-domain part-of-speech tagging
datasets. While previous approaches do
not improve on our supervised baseline,
our approach is better across the board
with an average 4% error reduction.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990668">
Most learning algorithms assume that training and
test data are governed by identical distributions;
and more specifically, in the case of part-of-speech
(POS) tagging, that training and test sentences
were sampled at random and that they are identi-
cally and independently distributed. Significance
is usually tested across data points in standard
NLP test sets. Such datasets typically contain run-
ning text rather than independently sampled sen-
tences, thereby violating the assumption that data
points are independently distributed and sampled
at random. More importantly, significance across
data points only says something about the likely-
hood of observing the same effect on more data
sampled the same way, but says nothing about
likely performance on sentences sampled from
different sources or different domains.
This paper considers the POS tagging problem,
i.e. where we have training and test data consist-
ing of sentences in which all words are assigned
a label y chosen from a finite set of class labels
{NOUN, VERB, DET,... }. We assume that we
are interested in performance across data sets or
domains rather than just performance across data
points, but that we do not know the target domain
in advance. This is often the case when we develop
NLP tools and on-line services. We will do cross-
domain experiments using several target domains
in order to compute significance across domains,
enabling us to say something about likely perfor-
mance on new domains.
Several authors have noted how POS tagging
performance is sensitive to cross-domain shifts
(Blitzer et al., 2006; Daume III, 2007; Jiang and
Zhai, 2007), and while most authors have as-
sumed known target distributions and pool unla-
beled target data in order to automatically correct
cross-domain bias (Jiang and Zhai, 2007; Fos-
ter et al., 2010), methods such as feature bag-
ging (Sutton et al., 2006), learning with random
adversaries (Globerson and Roweis, 2006) and
L∞-regularization (Dekel and Shamir, 2008) have
been proposed to improve performance on un-
known target distributions. These methods ex-
plicitly or implicitly try to minimize average or
worst-case expected error across a set of possi-
ble test distributions in various ways. These al-
gorithms are related because of the intimate rela-
tionship between adversarial corruption and reg-
ularization (Ghaoui and Lebret, 1997; Xu et al.,
</bodyText>
<page confidence="0.965849">
640
</page>
<bodyText confidence="0.958351264705882">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640–644,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
2009; Hinton et al., 2012). This paper presents a
new method based on learning with antagonistic
adversaries.
Outline. Section 2 introduces previous work on
robust perceptron learning, as well as the meth-
ods dicussed in the paper. Section 3 motivates
and introduces learning with antagonistic adver-
saries. Section 4 presents experiments on POS tag-
ging and discusses how to evaluate cross-domain
performance. Learning with antagonistic adver-
saries is superior to the other approaches across
10/12 datasets with an average error reduction of
4% over a supervised baseline.
Motivating example. The problem with
out-of-vocabulary effects can be illus-
trated using a small labeled data set:
{x1 = (1, (0, 1, 0)), x2 = (1, (0,1,1)), x3 =
(0, (0, 0, 0)), x4 = (1, (0, 0,1))1. Say we train
our model on x1−3 and evaluate it on the fourth
data point. Most discriminate learning algorithms
only update parameters when training examples
are misclassified. In this example, a model
initialized by zero weights would misclassify x1,
update the parameter associated with feature x2
at a fixed rate α, and the returned model would
then classify all data points correctly. Hence
the parameter associated with feature x3 would
never be updated, although this feature is also
correlated with class. If x2 is missing in our test
data (out-of-vocabulary), we end up classifying
all data points as negative. In this case, we would
wrongly predict that x4 is negative.
</bodyText>
<sectionHeader confidence="0.783785" genericHeader="method">
2 Robust perceptron learning
</sectionHeader>
<bodyText confidence="0.995931952380953">
Our framework will be averaged perceptron learn-
ing (Freund and Schapire, 1999; Collins, 2002).
We use an additive update algorithm and aver-
age parameters to prevent over-fitting. In adver-
sarial learning, adversaries corrupt the data point
by applying transformations to data points. An-
tagonistic adversaries choose transformations in-
formed by the current model parameters w, but
random adversaries randomly select transforma-
tions from a predefined set of possible transforma-
tions, e.g. deletions of at most k features (Glober-
son and Roweis, 2006).
Feature bagging. In feature bagging (Sutton et al.,
2006), the data is represented by different bags of
features or different views, and the models learned
using different feature bags are combined by aver-
aging. We can reformulate feature bagging as an
adversarial learning problem. For each pass, the
adversary chooses a deleting transformation cor-
responding to one of the feature bags. In Sut-
ton et al. (2006), the feature bags simply divide
the features into two or more representations. In
an online setting feature bagging can be modelled
as a game between a learner and an adversary, in
which (a) the adversary can only choose between
deleting transformations, (b) the adversary cannot
see model parameters when choosing a transfor-
mation, and in which (c) the adversary only moves
in between passes over the data.1
Learning with random adversaries
(LRA). Globerson and Roweis (2006) let an
adversary corrupt labeled data during training
to learn better models of test data with missing
features. They assume that missing features
are randomly distributed and show that the
optimization problem is a second-order cone
program. LRA is an adversarial game in which
the two players are unaware of the other player’s
current move, and in particular, where the ad-
versary does not see model parameters and only
randomly corrupts the data points. Globerson
and Roweis (2006) formulate LRA as a batch
learning problem of minimizing worst case loss
under deleting transformations deleting at most
k features. This is related to regularization in the
following way: If model parameters are chosen
to minimize expected error in the absence of any
k features, we explicitly prevent under-weighting
more than n − k features, i.e. the model must be
able to classify data well in the absence of any k
features. The sparsest possible model would thus
assign weights to k + 1 parameters.
L∞-regularization hedges its bets even more than
adversarial learning by minimizing expected er-
ror with max ||w ||&lt; C. In the online setting,
this corresponds to playing against an adversary
that clips any weight above a certain threshold C,
whether positive or negative (Dekel and Shamir,
2008). In geometric terms the weights are pro-
jected back onto the hyper-cube C. A related
approach, which is not explored in the experi-
ments below, is to regularize linear models toward
weights with low variance (Bergsma et al., 2010).
</bodyText>
<footnote confidence="0.815417">
1Note that the batch version of feature bagging is an in-
stance of group Ll regularization (Jacob et al., 2009; Schmidt
and Murphy, 2010; Martins et al., 2011). Often group regu-
larization is about finding sparser models rather than robust
models. Sparse models can be obtained by grouping corre-
lated features; non-sparse models can be obtained by using
independent, exhaustive views.
</footnote>
<page confidence="0.984069">
641
</page>
<figure confidence="0.9347712">
1: X = {(yi, xi)}Ni=1, 6 deletion rate
2: w0 = 0, v = 0, i = 0
3: fork E K do
4: for n E N do
5: 1 random.sample(P(1) = 1 − 6)
6: 2 ||w ||&lt; A||w ||+ a||w||
7: ( 1 + 2)(0,1)
8: if sign(w · xn o ) =� yn then
9: wi+1 �-- update(wi)
10: i i i + 1
11: end if
12: v �-- v + wi
13: end for
14: end for
15: return w = v/(N x K)
</figure>
<figureCaption confidence="0.999769">
Figure 1: Learning with antagonistic adversaries
</figureCaption>
<sectionHeader confidence="0.888457" genericHeader="method">
3 Learning with antagonistic adversaries
</sectionHeader>
<bodyText confidence="0.995929432432432">
The intuition behind learning with antagonistic ad-
versaries is that the adversary should focus on the
most predictive features. In the prediction game,
this would allow the adversary to inflict more dam-
age, corrupting data points by removing good fea-
tures (rather than random ones). If the adversary
focuses on the most predictive features, she is im-
plicitly regularizing the model to obtain a more
equal distribution of weights.
We draw random binary vectors with P(1) =
1 − 6 as in adversarial learning, but deletions are
only effective if �j = 0 and the weight wj is more
than a standard deviation (a||w||) from the mean
of the current absolute weight distribution (p||w||).
In other words, we only delete the predictive fea-
tures, with predictivity being relative to the current
mean weight.
The algorithm is presented in Figure 1. For each
data point, we draw a random binary vector �1
with 6 chance of zeros. S2 is a vector with the
ith scalar zero if and only if the absolute value of
the weight wi in w is more than a standard devia-
tion higher than the current mean. The ith scalar
in � is only zero if the ith scalars in both �1 and S2
are zero. The corresponding features are a random
subset of the predictive features.2
2The approach taken is similar in spirit to confidence-
weighted learning (Dredze et al., 2008). The intuition behind
confidence-weighted learning is to more agressively update
rare features or features that we are less confident about. In
learning with antagonistic adversaries the adversaries delete
predictive features; that is, features that we are confident
about. When these features are deleted, we do not update
the corresponding weights. In relative terms, we therefore
update rare features more aggressively than common ones.
Note also that by doing so we regularize toward weights with
low variance (Bergsma et al., 2010).
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999957765957447">
We consider part-of-speech (POS) tagging, i.e. the
problem of assigning syntactic categories to word
tokens in running text. POS tagging accuracy is
known to be very sensitive to domain shifts. Fos-
ter et al. (2011) report a POS tagging accuracy on
social media data of 84% using a tagger that ac-
chieves an accuracy of about 97% on newspaper
data. In the case of social media data, many errors
occur due to different spelling and capitalization
conventions. The main source of error, though, is
the increased out-of-vocabulary rate, i.e. the many
unknown words. While POS taggers can often re-
cover the part of speech of a previously unseen
word from the context it occurs in, this is harder
than for previously seen words.
We use the LXMLS toolkit3 as our baseline
with the default feature model, but use the PTB
tagset rather than the Google tagset (Petrov et
al., 2011) used by default in the LXMLS toolkit.
We use four groups of datasets. The first group
comes from the English Web Treebank (EWT),4
also used in the Parsing the Web shared task
(Petrov and McDonald, 2012). We train our tag-
ger on Sections 2–21 of the WSJ data in the Penn-
III Treebank (PTB), Ontonotes 4.0 release. The
EWT contains development and test data for five
domains: answers (from Yahoo!), emails (from
the Enron corpus), BBC newsgroups, Amazon re-
views, and weblogs. We use the emails develop-
ment section for development and test on the re-
maining four test sets. We also do experiments
with additional data from PTB. For these experi-
ments we use the 0th even split of the biomedical
section (PTB-biomedical) as development data,
the 9th split and the chemistry section (PTB-
chemistry) as test data, and the remaining biomed-
ical data (splits 1–8) as training data. This data
was also used for developing and testing in the
CoNLL 2007 Shared Task (Nivre et al., 2007).
Our third group of datasets also comes from
Ontonotes 4.0.5 We use the Chinese Ontonotes
(CHO) data, covering five different domains. We
use newswire for training data and randomly sam-
pled broadcasted news for development. Finally
we do experiments with the Danish section of the
Copenhagen Dependency Treebank (CDT). For
CDT we rely on the treebank meta-data and sin-
</bodyText>
<footnote confidence="0.999819333333333">
3https://github.com/gracaninja/lxmls-toolkit
4LDC Catalog No.: LDC2012T13.
5LDC Catalog No.: LDC2011T03.
</footnote>
<page confidence="0.992425">
642
</page>
<table confidence="0.9998136">
SP Our L∞ LRA
EWT-answers 86.04 86.06 85.90 86.06
EWT-newsgroups 87.70 87.92 87.78 87.66
EWT-reviews 85.96 86.10 85.80 86.00
EWT-weblogs 87.59 87.89 87.60 87.54
PTB-biomedical 95.05 95.26 95.46 94.43
PTB-chemistry 90.32 90.60 90.56 90.58
CHO-broadcast 78.38 78.42 78.27 78.28
CHO-magazines 78.50 78.57 76.80 78.29
CHO-weblogs 79.64 79.76 79.24 79.37
CDT-law 93.96 95.64 93.91 94.25
CDT-literature 93.93 94.19 94.15 94.15
CDT-magazines 94.95 95.06 94.71 95.04
Wilcoxon P &lt;0.01
macro-av. err.red 4.0 -1.2 -0.2
</table>
<tableCaption confidence="0.999948">
Table 1: Results (in %).
</tableCaption>
<bodyText confidence="0.999196461538462">
gle out the newspaper section as training data and
use held-out newspaper data for development.
We observe two characteristics about our
datasets: (a) The class distributions are relatively
stable across domains. For CDT, for example,
we see almost identical distributions of parts of
speech, except literature has more prepositions.
(b) The OOV rate is significantly higher across do-
mains than within domains. This holds even for
the PTB datasets, where the OOV rate is 14.6% on
the biomedical test data, but 43.3% on the chem-
istry test data. These two observations confirm
that cross-domain data is primarily biased by co-
variate shifts.
All learning algorithms do the same number of
passes over each training data set. The number
of iterations was set optimizing baseline system
performance on development data. For EWT and
CHO, we do 10 passes over the data. For PTB,
we do 15 passes over the data, and for CDT, we
do 25 passes over the data. The deletion rate in
adversarial learning was fixed to 0.1% (optimized
on the EWT emails data; not optimized on PTB,
CHO or CDT). In Lam-regularization, the parame-
ter C was optimized the same way and set to 20.
Results are averages over five runs.
</bodyText>
<subsectionHeader confidence="0.788789">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999991285714286">
The results are presented in Table 1. Learn-
ing with antagonistic adversaries performs signifi-
cantly better than structured perceptron (SP) learn-
ing, Lam-regularization and LRA across the board.
We follow Demsar (2006) in computing signif-
icance across datasets using a Wilcoxon signed
rank test. This is a strong result given that our al-
gorithm is as computationally efficient as SP and
does not pool unlabeled data to adapt to a spe-
cific target distribution. What we see is that let-
ting an antagonistic adversary corrupt our labeled
data - somewhat surprisingly, maybe - leads to bet-
ter cross-domain performance. Lam-regularization
leads to worse performance, and LRA performs
very similar to SP on average. Improvements
to LRA have also been explored in Trafalis and
Gilbert (2007) and Dekel and Shamir (2008).
We note that on the in-domain dataset (PTB-
biomedical), Lam-regularization performs best, but
our approach also performs better than the struc-
tured perceptron baseline on this dataset.
</bodyText>
<subsectionHeader confidence="0.984886">
4.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999996206896552">
The number of zero weights or very small weights
is significantly lower for learning with antagonis-
tic adversaries than for the baseline structured per-
ceptron. So our models become less sparse. On
the other hand, we have more parameters with av-
erage weights in our models. Weights are in other
words better distributed. We also observe that pa-
rameters are updated slightly more with antago-
nistic adversaries. In our PTB experiments, for
example, the mean weight is 14.2 in structured
perceptron learning, but 14.5 with antagonistic ad-
versaries. On the other hand, weight variance is
slightly lower; recall the connection to variance
regularization (Bergsma et al., 2010). Note that
Lam-regularization with C = 20 corresponds to
clipping all weights above 20, i.e. roughly a third
of the weights in this case. To validate our intu-
itions about what is going on, we also tried to in-
crease the deletion rate. If δ is increased to 1%,
the mean weight goes up to 19.2. The adversarial
model is less sparse than the baseline model.
A last observation is that the structured percep-
tron baseline model expectedly fits the training
data better than the robust models. On CDT, the
structured perceptron has an accuracy of 98.26%
on held-out training data, whereas our model has
an accuracy of only 97.85%. The Lam-regularized
has an accuracy of 97.82%, whereas LRA has an
accuracy of 98.18%.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999964857142857">
We presented a discriminative learning algorithms
for cross-domain structured prediction that seems
more robust to covariate shifts than previous ap-
proaches. Our approach was superior to previous
approaches across 12 multilingual cross-domain
POS tagging datasets, with an average error reduc-
tion of 4% over a structured perceptron baseline.
</bodyText>
<page confidence="0.998997">
643
</page>
<sectionHeader confidence="0.990258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908397260274">
Shane Bergsma, Dekang Lin, and Dale Schuurmans.
2010. Improved natural language learning via
variance-regularization support vector machines. In
CoNLL.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models. In EMNLP.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Ofer Dekel and Ohad Shamir. 2008. Learning to clas-
sify with missing and corrupted features. In ICML.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1–30.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277–296.
Laurent El Ghaoui and Herve Lebret. 1997. Robust
solutions to least-squares problems with uncertain
data. In SIAM Journal of Matrix Analysis and Ap-
plications.
Amir Globerson and Sam Roweis. 2006. Nightmare
at test time: robust learning by feature deletion. In
ICML.
Geoffrey Hinton, N. Srivastava, A. Krizhevsky,
I. Sutskever, and R. Salakhutdinov. 2012. Improv-
ing neural networks by preventing co-adaptation of
feature detectors. http://arxiv.org/abs/1207.0580.
Laurent Jacob, Guillaume Obozinski, and Jean-
Philippe Vert. 2009. Group lasso with overlap and
graph lasso. In ICML.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Andre Martins, Noah Smith, Pedro Aguiar, and Mario
Figueiredo. 2011. Structured sparsity in structured
prediction. In EMNLP.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In EMNLP-CoNLL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011. A universal part-of-speech tagset. CoRR
abs/1104.2086.
Mark Schmidt and Kevin Murphy. 2010. Convex
structure learning in log-linear models: beyond pair-
wise potentials. In AISTATS.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2006. Reducing weight undertraining in struc-
tured discriminative learning. In NAACL.
T Trafalis and R Gilbert. 2007. Robust support vec-
tor machines for classification and computational is-
sues. Optimization Methods and Software, 22:187–
198.
Huan Xu, Constantine Caramanis, and Shie Mannor.
2009. Robustness and regularization of support vec-
tor machines. In JMLR.
</reference>
<page confidence="0.998663">
644
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844137">
<title confidence="0.998426">Part-of-speech tagging with antagonistic adversaries</title>
<author confidence="0.945093">Anders</author>
<affiliation confidence="0.9987785">Center for Language University of</affiliation>
<address confidence="0.94795">DK-2300 Copenhagen</address>
<email confidence="0.993553">soegaard@hum.ku.dk</email>
<abstract confidence="0.997688173913043">Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development. The performance loss observed in such cross-domain applications is often attributed to covariate shifts, with out-of-vocabulary effects as an important subclass. Many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features. Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts. We present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilingual cross-domain part-of-speech tagging datasets. While previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4% error reduction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved natural language learning via variance-regularization support vector machines.</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="8048" citStr="Bergsma et al., 2010" startWordPosition="1261" endWordPosition="1264">ce of any k features. The sparsest possible model would thus assign weights to k + 1 parameters. L∞-regularization hedges its bets even more than adversarial learning by minimizing expected error with max ||w ||&lt; C. In the online setting, this corresponds to playing against an adversary that clips any weight above a certain threshold C, whether positive or negative (Dekel and Shamir, 2008). In geometric terms the weights are projected back onto the hyper-cube C. A related approach, which is not explored in the experiments below, is to regularize linear models toward weights with low variance (Bergsma et al., 2010). 1Note that the batch version of feature bagging is an instance of group Ll regularization (Jacob et al., 2009; Schmidt and Murphy, 2010; Martins et al., 2011). Often group regularization is about finding sparser models rather than robust models. Sparse models can be obtained by grouping correlated features; non-sparse models can be obtained by using independent, exhaustive views. 641 1: X = {(yi, xi)}Ni=1, 6 deletion rate 2: w0 = 0, v = 0, i = 0 3: fork E K do 4: for n E N do 5: 1 random.sample(P(1) = 1 − 6) 6: 2 ||w ||&lt; A||w ||+ a||w|| 7: ( 1 + 2)(0,1) 8: if sign(w · xn o ) =� yn then 9: wi</context>
<context position="10707" citStr="Bergsma et al., 2010" startWordPosition="1739" endWordPosition="1742">oach taken is similar in spirit to confidenceweighted learning (Dredze et al., 2008). The intuition behind confidence-weighted learning is to more agressively update rare features or features that we are less confident about. In learning with antagonistic adversaries the adversaries delete predictive features; that is, features that we are confident about. When these features are deleted, we do not update the corresponding weights. In relative terms, we therefore update rare features more aggressively than common ones. Note also that by doing so we regularize toward weights with low variance (Bergsma et al., 2010). 4 Experiments We consider part-of-speech (POS) tagging, i.e. the problem of assigning syntactic categories to word tokens in running text. POS tagging accuracy is known to be very sensitive to domain shifts. Foster et al. (2011) report a POS tagging accuracy on social media data of 84% using a tagger that acchieves an accuracy of about 97% on newspaper data. In the case of social media data, many errors occur due to different spelling and capitalization conventions. The main source of error, though, is the increased out-of-vocabulary rate, i.e. the many unknown words. While POS taggers can o</context>
<context position="16459" citStr="Bergsma et al., 2010" startWordPosition="2677" endWordPosition="2680">is significantly lower for learning with antagonistic adversaries than for the baseline structured perceptron. So our models become less sparse. On the other hand, we have more parameters with average weights in our models. Weights are in other words better distributed. We also observe that parameters are updated slightly more with antagonistic adversaries. In our PTB experiments, for example, the mean weight is 14.2 in structured perceptron learning, but 14.5 with antagonistic adversaries. On the other hand, weight variance is slightly lower; recall the connection to variance regularization (Bergsma et al., 2010). Note that Lam-regularization with C = 20 corresponds to clipping all weights above 20, i.e. roughly a third of the weights in this case. To validate our intuitions about what is going on, we also tried to increase the deletion rate. If δ is increased to 1%, the mean weight goes up to 19.2. The adversarial model is less sparse than the baseline model. A last observation is that the structured perceptron baseline model expectedly fits the training data better than the robust models. On CDT, the structured perceptron has an accuracy of 98.26% on held-out training data, whereas our model has an </context>
</contexts>
<marker>Bergsma, Lin, Schuurmans, 2010</marker>
<rawString>Shane Bergsma, Dekang Lin, and Dale Schuurmans. 2010. Improved natural language learning via variance-regularization support vector machines. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2656" citStr="Blitzer et al., 2006" startWordPosition="400" endWordPosition="403">a label y chosen from a finite set of class labels {NOUN, VERB, DET,... }. We assume that we are interested in performance across data sets or domains rather than just performance across data points, but that we do not know the target domain in advance. This is often the case when we develop NLP tools and on-line services. We will do crossdomain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞-regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distri</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5193" citStr="Collins, 2002" startWordPosition="803" endWordPosition="804">ample, a model initialized by zero weights would misclassify x1, update the parameter associated with feature x2 at a fixed rate α, and the returned model would then classify all data points correctly. Hence the parameter associated with feature x3 would never be updated, although this feature is also correlated with class. If x2 is missing in our test data (out-of-vocabulary), we end up classifying all data points as negative. In this case, we would wrongly predict that x4 is negative. 2 Robust perceptron learning Our framework will be averaged perceptron learning (Freund and Schapire, 1999; Collins, 2002). We use an additive update algorithm and average parameters to prevent over-fitting. In adversarial learning, adversaries corrupt the data point by applying transformations to data points. Antagonistic adversaries choose transformations informed by the current model parameters w, but random adversaries randomly select transformations from a predefined set of possible transformations, e.g. deletions of at most k features (Globerson and Roweis, 2006). Feature bagging. In feature bagging (Sutton et al., 2006), the data is represented by different bags of features or different views, and the mode</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Ohad Shamir</author>
</authors>
<title>Learning to classify with missing and corrupted features.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="3052" citStr="Dekel and Shamir, 2008" startWordPosition="463" endWordPosition="466">rder to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞-regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithms are related because of the intimate relationship between adversarial corruption and regularization (Ghaoui and Lebret, 1997; Xu et al., 640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640–644, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2009; Hinton et al., 2012).</context>
<context position="7819" citStr="Dekel and Shamir, 2008" startWordPosition="1222" endWordPosition="1225">llowing way: If model parameters are chosen to minimize expected error in the absence of any k features, we explicitly prevent under-weighting more than n − k features, i.e. the model must be able to classify data well in the absence of any k features. The sparsest possible model would thus assign weights to k + 1 parameters. L∞-regularization hedges its bets even more than adversarial learning by minimizing expected error with max ||w ||&lt; C. In the online setting, this corresponds to playing against an adversary that clips any weight above a certain threshold C, whether positive or negative (Dekel and Shamir, 2008). In geometric terms the weights are projected back onto the hyper-cube C. A related approach, which is not explored in the experiments below, is to regularize linear models toward weights with low variance (Bergsma et al., 2010). 1Note that the batch version of feature bagging is an instance of group Ll regularization (Jacob et al., 2009; Schmidt and Murphy, 2010; Martins et al., 2011). Often group regularization is about finding sparser models rather than robust models. Sparse models can be obtained by grouping correlated features; non-sparse models can be obtained by using independent, exha</context>
<context position="15590" citStr="Dekel and Shamir (2008)" startWordPosition="2541" endWordPosition="2544">board. We follow Demsar (2006) in computing significance across datasets using a Wilcoxon signed rank test. This is a strong result given that our algorithm is as computationally efficient as SP and does not pool unlabeled data to adapt to a specific target distribution. What we see is that letting an antagonistic adversary corrupt our labeled data - somewhat surprisingly, maybe - leads to better cross-domain performance. Lam-regularization leads to worse performance, and LRA performs very similar to SP on average. Improvements to LRA have also been explored in Trafalis and Gilbert (2007) and Dekel and Shamir (2008). We note that on the in-domain dataset (PTBbiomedical), Lam-regularization performs best, but our approach also performs better than the structured perceptron baseline on this dataset. 4.2 Analysis The number of zero weights or very small weights is significantly lower for learning with antagonistic adversaries than for the baseline structured perceptron. So our models become less sparse. On the other hand, we have more parameters with average weights in our models. Weights are in other words better distributed. We also observe that parameters are updated slightly more with antagonistic adver</context>
</contexts>
<marker>Dekel, Shamir, 2008</marker>
<rawString>Ofer Dekel and Ohad Shamir. 2008. Learning to classify with missing and corrupted features. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Demsar</author>
</authors>
<title>Statistical comparisons of classifiers over multiple data sets.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1</pages>
<contexts>
<context position="14997" citStr="Demsar (2006)" startWordPosition="2445" endWordPosition="2446">ta. For EWT and CHO, we do 10 passes over the data. For PTB, we do 15 passes over the data, and for CDT, we do 25 passes over the data. The deletion rate in adversarial learning was fixed to 0.1% (optimized on the EWT emails data; not optimized on PTB, CHO or CDT). In Lam-regularization, the parameter C was optimized the same way and set to 20. Results are averages over five runs. 4.1 Results The results are presented in Table 1. Learning with antagonistic adversaries performs significantly better than structured perceptron (SP) learning, Lam-regularization and LRA across the board. We follow Demsar (2006) in computing significance across datasets using a Wilcoxon signed rank test. This is a strong result given that our algorithm is as computationally efficient as SP and does not pool unlabeled data to adapt to a specific target distribution. What we see is that letting an antagonistic adversary corrupt our labeled data - somewhat surprisingly, maybe - leads to better cross-domain performance. Lam-regularization leads to worse performance, and LRA performs very similar to SP on average. Improvements to LRA have also been explored in Trafalis and Gilbert (2007) and Dekel and Shamir (2008). We no</context>
</contexts>
<marker>Demsar, 2006</marker>
<rawString>Janez Demsar. 2006. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="10170" citStr="Dredze et al., 2008" startWordPosition="1659" endWordPosition="1662">elete the predictive features, with predictivity being relative to the current mean weight. The algorithm is presented in Figure 1. For each data point, we draw a random binary vector �1 with 6 chance of zeros. S2 is a vector with the ith scalar zero if and only if the absolute value of the weight wi in w is more than a standard deviation higher than the current mean. The ith scalar in � is only zero if the ith scalars in both �1 and S2 are zero. The corresponding features are a random subset of the predictive features.2 2The approach taken is similar in spirit to confidenceweighted learning (Dredze et al., 2008). The intuition behind confidence-weighted learning is to more agressively update rare features or features that we are less confident about. In learning with antagonistic adversaries the adversaries delete predictive features; that is, features that we are confident about. When these features are deleted, we do not update the corresponding weights. In relative terms, we therefore update rare features more aggressively than common ones. Note also that by doing so we regularize toward weights with low variance (Bergsma et al., 2010). 4 Experiments We consider part-of-speech (POS) tagging, i.e. </context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2887" citStr="Foster et al., 2010" startWordPosition="438" endWordPosition="442">get domain in advance. This is often the case when we develop NLP tools and on-line services. We will do crossdomain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞-regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithms are related because of the intimate relationship between adversarial corruption and regularization (Ghaoui and Lebret, 1997; Xu et al., 640 Proceedings of the 51st Annual Meeting of the Ass</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Josef Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirde Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comments: Resources and benchmarks for parsing the language of Web 2.0.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="5177" citStr="Freund and Schapire, 1999" startWordPosition="799" endWordPosition="802">e misclassified. In this example, a model initialized by zero weights would misclassify x1, update the parameter associated with feature x2 at a fixed rate α, and the returned model would then classify all data points correctly. Hence the parameter associated with feature x3 would never be updated, although this feature is also correlated with class. If x2 is missing in our test data (out-of-vocabulary), we end up classifying all data points as negative. In this case, we would wrongly predict that x4 is negative. 2 Robust perceptron learning Our framework will be averaged perceptron learning (Freund and Schapire, 1999; Collins, 2002). We use an additive update algorithm and average parameters to prevent over-fitting. In adversarial learning, adversaries corrupt the data point by applying transformations to data points. Antagonistic adversaries choose transformations informed by the current model parameters w, but random adversaries randomly select transformations from a predefined set of possible transformations, e.g. deletions of at most k features (Globerson and Roweis, 2006). Feature bagging. In feature bagging (Sutton et al., 2006), the data is represented by different bags of features or different vie</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent El Ghaoui</author>
<author>Herve Lebret</author>
</authors>
<title>Robust solutions to least-squares problems with uncertain data. In</title>
<date>1997</date>
<journal>SIAM Journal of Matrix Analysis and Applications.</journal>
<marker>El Ghaoui, Lebret, 1997</marker>
<rawString>Laurent El Ghaoui and Herve Lebret. 1997. Robust solutions to least-squares problems with uncertain data. In SIAM Journal of Matrix Analysis and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
<author>Sam Roweis</author>
</authors>
<title>Nightmare at test time: robust learning by feature deletion.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="3005" citStr="Globerson and Roweis, 2006" startWordPosition="457" endWordPosition="460">omain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞-regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithms are related because of the intimate relationship between adversarial corruption and regularization (Ghaoui and Lebret, 1997; Xu et al., 640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640–644, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Comput</context>
<context position="5646" citStr="Globerson and Roweis, 2006" startWordPosition="868" endWordPosition="872">case, we would wrongly predict that x4 is negative. 2 Robust perceptron learning Our framework will be averaged perceptron learning (Freund and Schapire, 1999; Collins, 2002). We use an additive update algorithm and average parameters to prevent over-fitting. In adversarial learning, adversaries corrupt the data point by applying transformations to data points. Antagonistic adversaries choose transformations informed by the current model parameters w, but random adversaries randomly select transformations from a predefined set of possible transformations, e.g. deletions of at most k features (Globerson and Roweis, 2006). Feature bagging. In feature bagging (Sutton et al., 2006), the data is represented by different bags of features or different views, and the models learned using different feature bags are combined by averaging. We can reformulate feature bagging as an adversarial learning problem. For each pass, the adversary chooses a deleting transformation corresponding to one of the feature bags. In Sutton et al. (2006), the feature bags simply divide the features into two or more representations. In an online setting feature bagging can be modelled as a game between a learner and an adversary, in which</context>
<context position="7020" citStr="Globerson and Roweis (2006)" startWordPosition="1090" endWordPosition="1093">and in which (c) the adversary only moves in between passes over the data.1 Learning with random adversaries (LRA). Globerson and Roweis (2006) let an adversary corrupt labeled data during training to learn better models of test data with missing features. They assume that missing features are randomly distributed and show that the optimization problem is a second-order cone program. LRA is an adversarial game in which the two players are unaware of the other player’s current move, and in particular, where the adversary does not see model parameters and only randomly corrupts the data points. Globerson and Roweis (2006) formulate LRA as a batch learning problem of minimizing worst case loss under deleting transformations deleting at most k features. This is related to regularization in the following way: If model parameters are chosen to minimize expected error in the absence of any k features, we explicitly prevent under-weighting more than n − k features, i.e. the model must be able to classify data well in the absence of any k features. The sparsest possible model would thus assign weights to k + 1 parameters. L∞-regularization hedges its bets even more than adversarial learning by minimizing expected err</context>
</contexts>
<marker>Globerson, Roweis, 2006</marker>
<rawString>Amir Globerson and Sam Roweis. 2006. Nightmare at test time: robust learning by feature deletion. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>N Srivastava</author>
<author>A Krizhevsky</author>
<author>I Sutskever</author>
<author>R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors.</title>
<date>2012</date>
<note>http://arxiv.org/abs/1207.0580.</note>
<contexts>
<context position="3651" citStr="Hinton et al., 2012" startWordPosition="553" endWordPosition="556">kel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithms are related because of the intimate relationship between adversarial corruption and regularization (Ghaoui and Lebret, 1997; Xu et al., 640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640–644, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2009; Hinton et al., 2012). This paper presents a new method based on learning with antagonistic adversaries. Outline. Section 2 introduces previous work on robust perceptron learning, as well as the methods dicussed in the paper. Section 3 motivates and introduces learning with antagonistic adversaries. Section 4 presents experiments on POS tagging and discusses how to evaluate cross-domain performance. Learning with antagonistic adversaries is superior to the other approaches across 10/12 datasets with an average error reduction of 4% over a supervised baseline. Motivating example. The problem with out-of-vocabulary </context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. http://arxiv.org/abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Jacob</author>
<author>Guillaume Obozinski</author>
<author>JeanPhilippe Vert</author>
</authors>
<title>Group lasso with overlap and graph lasso.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8159" citStr="Jacob et al., 2009" startWordPosition="1281" endWordPosition="1284"> hedges its bets even more than adversarial learning by minimizing expected error with max ||w ||&lt; C. In the online setting, this corresponds to playing against an adversary that clips any weight above a certain threshold C, whether positive or negative (Dekel and Shamir, 2008). In geometric terms the weights are projected back onto the hyper-cube C. A related approach, which is not explored in the experiments below, is to regularize linear models toward weights with low variance (Bergsma et al., 2010). 1Note that the batch version of feature bagging is an instance of group Ll regularization (Jacob et al., 2009; Schmidt and Murphy, 2010; Martins et al., 2011). Often group regularization is about finding sparser models rather than robust models. Sparse models can be obtained by grouping correlated features; non-sparse models can be obtained by using independent, exhaustive views. 641 1: X = {(yi, xi)}Ni=1, 6 deletion rate 2: w0 = 0, v = 0, i = 0 3: fork E K do 4: for n E N do 5: 1 random.sample(P(1) = 1 − 6) 6: 2 ||w ||&lt; A||w ||+ a||w|| 7: ( 1 + 2)(0,1) 8: if sign(w · xn o ) =� yn then 9: wi+1 �-- update(wi) 10: i i i + 1 11: end if 12: v �-- v + wi 13: end for 14: end for 15: return w = v/(N x K) Fi</context>
</contexts>
<marker>Jacob, Obozinski, Vert, 2009</marker>
<rawString>Laurent Jacob, Guillaume Obozinski, and JeanPhilippe Vert. 2009. Group lasso with overlap and graph lasso. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2696" citStr="Jiang and Zhai, 2007" startWordPosition="407" endWordPosition="410">lass labels {NOUN, VERB, DET,... }. We assume that we are interested in performance across data sets or domains rather than just performance across data points, but that we do not know the target domain in advance. This is often the case when we develop NLP tools and on-line services. We will do crossdomain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞-regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithm</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Pedro Aguiar</author>
<author>Mario Figueiredo</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8208" citStr="Martins et al., 2011" startWordPosition="1289" endWordPosition="1292">arning by minimizing expected error with max ||w ||&lt; C. In the online setting, this corresponds to playing against an adversary that clips any weight above a certain threshold C, whether positive or negative (Dekel and Shamir, 2008). In geometric terms the weights are projected back onto the hyper-cube C. A related approach, which is not explored in the experiments below, is to regularize linear models toward weights with low variance (Bergsma et al., 2010). 1Note that the batch version of feature bagging is an instance of group Ll regularization (Jacob et al., 2009; Schmidt and Murphy, 2010; Martins et al., 2011). Often group regularization is about finding sparser models rather than robust models. Sparse models can be obtained by grouping correlated features; non-sparse models can be obtained by using independent, exhaustive views. 641 1: X = {(yi, xi)}Ni=1, 6 deletion rate 2: w0 = 0, v = 0, i = 0 3: fork E K do 4: for n E N do 5: 1 random.sample(P(1) = 1 − 6) 6: 2 ||w ||&lt; A||w ||+ a||w|| 7: ( 1 + 2)(0,1) 8: if sign(w · xn o ) =� yn then 9: wi+1 �-- update(wi) 10: i i i + 1 11: end if 12: v �-- v + wi 13: end for 14: end for 15: return w = v/(N x K) Figure 1: Learning with antagonistic adversaries 3 </context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andre Martins, Noah Smith, Pedro Aguiar, and Mario Figueiredo. 2011. Structured sparsity in structured prediction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<date>2007</date>
<booktitle>The CoNLL 2007 Shared Task on Dependency Parsing. In EMNLP-CoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="11796" citStr="Petrov and McDonald, 2012" startWordPosition="1927" endWordPosition="1930">ventions. The main source of error, though, is the increased out-of-vocabulary rate, i.e. the many unknown words. While POS taggers can often recover the part of speech of a previously unseen word from the context it occurs in, this is harder than for previously seen words. We use the LXMLS toolkit3 as our baseline with the default feature model, but use the PTB tagset rather than the Google tagset (Petrov et al., 2011) used by default in the LXMLS toolkit. We use four groups of datasets. The first group comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task (Petrov and McDonald, 2012). We train our tagger on Sections 2–21 of the WSJ data in the PennIII Treebank (PTB), Ontonotes 4.0 release. The EWT contains development and test data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs. We use the emails development section for development and test on the remaining four test sets. We also do experiments with additional data from PTB. For these experiments we use the 0th even split of the biomedical section (PTB-biomedical) as development data, the 9th split and the chemistry section (PTBchemistry) as test data,</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<note>CoRR abs/1104.2086.</note>
<contexts>
<context position="11593" citStr="Petrov et al., 2011" startWordPosition="1891" endWordPosition="1894">cial media data of 84% using a tagger that acchieves an accuracy of about 97% on newspaper data. In the case of social media data, many errors occur due to different spelling and capitalization conventions. The main source of error, though, is the increased out-of-vocabulary rate, i.e. the many unknown words. While POS taggers can often recover the part of speech of a previously unseen word from the context it occurs in, this is harder than for previously seen words. We use the LXMLS toolkit3 as our baseline with the default feature model, but use the PTB tagset rather than the Google tagset (Petrov et al., 2011) used by default in the LXMLS toolkit. We use four groups of datasets. The first group comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task (Petrov and McDonald, 2012). We train our tagger on Sections 2–21 of the WSJ data in the PennIII Treebank (PTB), Ontonotes 4.0 release. The EWT contains development and test data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs. We use the emails development section for development and test on the remaining four test sets. We also do experiments with add</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. CoRR abs/1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Schmidt</author>
<author>Kevin Murphy</author>
</authors>
<title>Convex structure learning in log-linear models: beyond pairwise potentials.</title>
<date>2010</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="8185" citStr="Schmidt and Murphy, 2010" startWordPosition="1285" endWordPosition="1288">n more than adversarial learning by minimizing expected error with max ||w ||&lt; C. In the online setting, this corresponds to playing against an adversary that clips any weight above a certain threshold C, whether positive or negative (Dekel and Shamir, 2008). In geometric terms the weights are projected back onto the hyper-cube C. A related approach, which is not explored in the experiments below, is to regularize linear models toward weights with low variance (Bergsma et al., 2010). 1Note that the batch version of feature bagging is an instance of group Ll regularization (Jacob et al., 2009; Schmidt and Murphy, 2010; Martins et al., 2011). Often group regularization is about finding sparser models rather than robust models. Sparse models can be obtained by grouping correlated features; non-sparse models can be obtained by using independent, exhaustive views. 641 1: X = {(yi, xi)}Ni=1, 6 deletion rate 2: w0 = 0, v = 0, i = 0 3: fork E K do 4: for n E N do 5: 1 random.sample(P(1) = 1 − 6) 6: 2 ||w ||&lt; A||w ||+ a||w|| 7: ( 1 + 2)(0,1) 8: if sign(w · xn o ) =� yn then 9: wi+1 �-- update(wi) 10: i i i + 1 11: end if 12: v �-- v + wi 13: end for 14: end for 15: return w = v/(N x K) Figure 1: Learning with anta</context>
</contexts>
<marker>Schmidt, Murphy, 2010</marker>
<rawString>Mark Schmidt and Kevin Murphy. 2010. Convex structure learning in log-linear models: beyond pairwise potentials. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Michael Sindelar</author>
<author>Andrew McCallum</author>
</authors>
<title>Reducing weight undertraining in structured discriminative learning.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2942" citStr="Sutton et al., 2006" startWordPosition="449" endWordPosition="452">evelop NLP tools and on-line services. We will do crossdomain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞-regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithms are related because of the intimate relationship between adversarial corruption and regularization (Ghaoui and Lebret, 1997; Xu et al., 640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640–644, </context>
<context position="5705" citStr="Sutton et al., 2006" startWordPosition="878" endWordPosition="881">ptron learning Our framework will be averaged perceptron learning (Freund and Schapire, 1999; Collins, 2002). We use an additive update algorithm and average parameters to prevent over-fitting. In adversarial learning, adversaries corrupt the data point by applying transformations to data points. Antagonistic adversaries choose transformations informed by the current model parameters w, but random adversaries randomly select transformations from a predefined set of possible transformations, e.g. deletions of at most k features (Globerson and Roweis, 2006). Feature bagging. In feature bagging (Sutton et al., 2006), the data is represented by different bags of features or different views, and the models learned using different feature bags are combined by averaging. We can reformulate feature bagging as an adversarial learning problem. For each pass, the adversary chooses a deleting transformation corresponding to one of the feature bags. In Sutton et al. (2006), the feature bags simply divide the features into two or more representations. In an online setting feature bagging can be modelled as a game between a learner and an adversary, in which (a) the adversary can only choose between deleting transfo</context>
</contexts>
<marker>Sutton, Sindelar, McCallum, 2006</marker>
<rawString>Charles Sutton, Michael Sindelar, and Andrew McCallum. 2006. Reducing weight undertraining in structured discriminative learning. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Trafalis</author>
<author>R Gilbert</author>
</authors>
<title>Robust support vector machines for classification and computational issues. Optimization Methods and Software,</title>
<date>2007</date>
<contexts>
<context position="15562" citStr="Trafalis and Gilbert (2007)" startWordPosition="2536" endWordPosition="2539">gularization and LRA across the board. We follow Demsar (2006) in computing significance across datasets using a Wilcoxon signed rank test. This is a strong result given that our algorithm is as computationally efficient as SP and does not pool unlabeled data to adapt to a specific target distribution. What we see is that letting an antagonistic adversary corrupt our labeled data - somewhat surprisingly, maybe - leads to better cross-domain performance. Lam-regularization leads to worse performance, and LRA performs very similar to SP on average. Improvements to LRA have also been explored in Trafalis and Gilbert (2007) and Dekel and Shamir (2008). We note that on the in-domain dataset (PTBbiomedical), Lam-regularization performs best, but our approach also performs better than the structured perceptron baseline on this dataset. 4.2 Analysis The number of zero weights or very small weights is significantly lower for learning with antagonistic adversaries than for the baseline structured perceptron. So our models become less sparse. On the other hand, we have more parameters with average weights in our models. Weights are in other words better distributed. We also observe that parameters are updated slightly </context>
</contexts>
<marker>Trafalis, Gilbert, 2007</marker>
<rawString>T Trafalis and R Gilbert. 2007. Robust support vector machines for classification and computational issues. Optimization Methods and Software, 22:187– 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huan Xu</author>
<author>Constantine Caramanis</author>
<author>Shie Mannor</author>
</authors>
<title>Robustness and regularization of support vector machines.</title>
<date>2009</date>
<booktitle>In JMLR.</booktitle>
<marker>Xu, Caramanis, Mannor, 2009</marker>
<rawString>Huan Xu, Constantine Caramanis, and Shie Mannor. 2009. Robustness and regularization of support vector machines. In JMLR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>