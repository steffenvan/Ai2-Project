<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000221">
<title confidence="0.998248">
Named Entity Learning and Verification:
Expectation Maximization in Large Corpora
</title>
<note confidence="0.391208">
Uwe QUASTHOFF, Christian BIEMANN, Christian WOLFF
CS Institute, Leipzig University
</note>
<keyword confidence="0.529535">
Augustusplatz 10/11
Leipzig, Germany, 04109
</keyword>
<sectionHeader confidence="0.973845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999641777777778">
The regularity of named entities is used to learn
names and to extract named entities. Having only
a few name elements and a set of patterns the al-
gorithm learns new names and its elements. A
verification step assures quality using a large
background corpus. Further improvement is
reached through classifying the newly learnt
elements on character level. Moreover, unsuper-
vised rule learning is discussed.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919173913044">
The task of recognizing person names in text
corpora is well examined in the field of Infor-
mation Extraction. Most of the approaches, us-
ing machine-learning or statistical methods
make excessive use of annotated data or large
gazetteers. We present a method that needs little
input knowledge and performs unsupervised
learning on unlabeled data, restricting ourselves
to person names.
In most languages, named entities form regular
patterns. Usually, a surname has a preceding
first name, which in turn might have a preceding
title or profession. Similar rules hold for differ-
ent kinds of named entities consisting of more
than one word. Moreover, some elements of
such multiwords (like president) are high fre-
quency items, well known and of high signifi-
cance for identifying a named entity. On the
other hand, there are elements that often appear
in named entities, but are not characteristic for
them (like the first name Israel).
Therefore, a verification step is included in the
learning algorithm.
</bodyText>
<sectionHeader confidence="0.989176" genericHeader="introduction">
2 The Algorithm
</sectionHeader>
<bodyText confidence="0.999966954545454">
Our learning algorithm starts with a set of pat-
terns and initial name elements. A large corpus
of more then 10 million sentences [cf. Quasthoff
&amp; Wolff 2000], taken from newspapers of the
last 10 years is used for both, the identification
of candidates for new name elements as well as
for verifying the candidates found. The algo-
rithm stops, if no more new name elements are
found.
The algorithm implements expectation maximi-
zation (EM) [cf. Dempster, 1977, Collins, 1999]
in the following way: The combination of a
learning step and a verification step are iterated.
If more name elements are found, the recall of
the verification step increases. The key property
of this algorithm is to assure high precision and
still get massive recall.
From another point of view our algorithm im-
plements bootstrapping [cf. Riloff 99], as it
starts from a small number of seed words and
uses knowledge found during the run to find
more candidates.
</bodyText>
<subsectionHeader confidence="0.991029">
2.1 Patterns and Pattern Rules
</subsectionHeader>
<bodyText confidence="0.999760730769231">
In a first step the text to be analysed is tagged in
the following way: We have two types of tags.
The first type is problem dependent. In the case
of persons, we have tags for title or profession
(TI), first name (FN) and surname (LN). The
second tag set is problem independent, but lan-
guage dependent. In our experiments, we
marked words as lower case (LC) or upper case
(UC) depending on the first letter. Punctuation
marks are marked as PM, determiners as DET.
Words can have multiple tags, e.g. UC and FN
at the same time.
The next step is to find tag sequences which are
typical for names, like TI-FN-LN . From here,
we can create rules like
TI-UC-LN ⇒ TI-FN-LN ,
which means that an upper case word between
title and last name is a candidate for a first name.
An overview of handmade start rules is given in
appendix 1.
Looking at the rules, it is possible to argue that a
rule like UC-LN ⇒ FN-LN is a massive over-
generalization. This would be true if we would
learn new name elements simply by applying
rules. However, the verification step ensures that
false friends are eliminated at high rate.
</bodyText>
<subsectionHeader confidence="0.999448">
2.2 The Outer Loop
</subsectionHeader>
<bodyText confidence="0.919834">
The algorithm is described as follows:
</bodyText>
<table confidence="0.910507538461538">
Load pattern rules.
Let unused name elements = in i tial
set of name elements
Loop:
For each unused name entity
Do the learning step and
collect new cand i dates
For each new candidate
Do the verification step
Output verified candidates
Let unused name elements =
ver i fied candidates
Search 30 random sentences contai n-
</table>
<tableCaption confidence="0.5946315">
ing the name element to be ver i -
fied (or all, if &lt;30).
</tableCaption>
<bodyText confidence="0.84409625">
If the ratio fulfilling at least
one right side of a pattern rule
is above some threshold, the ca n-
didate is a ccepted.
</bodyText>
<sectionHeader confidence="0.988845" genericHeader="method">
3 The Exhaustion Cycle
</sectionHeader>
<bodyText confidence="0.999974333333333">
The overall performance of the algorithm can be
estimated as follows: For simplicity let us as-
sume that the average number of items (in our
task: name elements) findable by any unused
item equals N. Then the number of items starts
to grow exponentially. Sooner or later, the total
number of unseen entities decreases. Hence,
most of the N items found are known already.
The numbers of new items found in each turn
decreases, until no more items can be reached.
So we discriminate between a phase of growth
and a phase of exhaustion.
The following figures visualize the number of
new items per turn and the accumulated total
number of items for each turn. Data was taken
from an experiment with 19 items of knowledge
(see appendix 2). The test was performed on the
German corpus and designed to find first and
last names only. The phase of growth lasts until
the 5th cycle, then exhaustion takes over, as can
be seen in figure 11.
</bodyText>
<subsectionHeader confidence="0.995839">
2.3 The Learning Step: Finding Candi-
dates
</subsectionHeader>
<bodyText confidence="0.999271">
Using the pattern rules and the current name
elements, new candidates are searched. Here we
use the corpus.
</bodyText>
<subsectionHeader confidence="0.81938">
Search 255 random sentences co n-
</subsectionHeader>
<bodyText confidence="0.980758">
taining the unused name entity (or
all, if &lt;255).
Use the pattern rules to identify
new candidates as d escribed above.
</bodyText>
<subsectionHeader confidence="0.996166">
2.4 The Verification Step
</subsectionHeader>
<bodyText confidence="0.984393">
In a verification step, each candidate is tested
before it is used to generate new candidates. We
test the following property: Does the candidate
appear often enough together with verified name
elements? Again, we use the corpus.
</bodyText>
<figure confidence="0.993846444444444">
items 30000
25000
20000
15000
10000
5000
0
0 1 2 3 4 5 6 7 8 9 1 0 1 1
cycle
</figure>
<figureCaption confidence="0.999964">
Figure 1: total and new items vs. cycles2
</figureCaption>
<bodyText confidence="0.821210285714286">
1 Additional runs with more start items produced the
same amount of total items in less cycles.
2 Note that 25&apos;000 name elements are responsible for
the detection of over 150&apos;000 full names.
New Items Total Items
Natural growth in the number of items takes
place under the following conditions:
</bodyText>
<listItem confidence="0.999978">
• Sufficient size of corpus
• Sufficient frequency of start items
• Suitable relation, e.g. names
</listItem>
<bodyText confidence="0.998643">
If the corpus is not large enough or it is not pos-
sible to find enough candidates from the start
items, exhaustion takes place immediately.
</bodyText>
<sectionHeader confidence="0.996855" genericHeader="method">
4 Examples
</sectionHeader>
<bodyText confidence="0.999948578947369">
Let us closely examine the learning of items by
example: From the known first name John, the
candidate for being a last name Hauberg was
found in the fragment &amp;quot;...by John Hauberg and..&amp;quot;
by the rule FN-UC-LC =&gt; FN-LN-LC and
verified in occurrences like &amp;quot;Robert Hauberg,
...&amp;quot; , &amp;quot;Robert Hauberg urges...&amp;quot; using the already
known first name Robert.
Errors occur in the case of words, which are
mainly used in positions which are often occu-
pied by first names. In German, the algorithm
extracts and verifies &amp;quot;Ära&amp;quot; (era) and &amp;quot;Transport-
panzer&amp;quot; (Army transportation tank) because of
the common usage &amp;quot;Ära Kohl&amp;quot; and the proper
name &amp;quot;Transportpanzer Fuchs&amp;quot; (fox tank). In the
case of &amp;quot;Ära&amp;quot;, this false first name supports the
classifications of the proper last names Hinrichs,
Strauß, Bangemann, Albrecht, Gorbatchow,
Jelzin and many more.
</bodyText>
<sectionHeader confidence="0.990061" genericHeader="method">
5 Precision and Recall
</sectionHeader>
<subsectionHeader confidence="0.969953">
5.1 Precision
</subsectionHeader>
<bodyText confidence="0.999979444444445">
Note that precision will be different for the dif-
ferent types of name elements. Usually surnames
are recognized with high precision. First names
may be confused with titles, for instance.
Moreover, precision is language dependent
mainly due to the different usage of capital let-
ters: In German, nouns start with capital letters
and can much easier be confused with names.
For German first names in the run mentioned
above, the algorithm yields a precision of
84.1%. Noise items mainly are titles and profes-
sion names, which are spelled with a capital
letter in German. Using the additional fact that
first names usually do not exceed 10 letters in
length, the precision for first names rose to
92.7%.
For last names, results were excellent with a
precision of more than 99%. The same holds for
titles, as further experiments showed.
The ratio number of first names vs. number of
last names happens to be about 1:3, overall pre-
cision for German scored 97.5%.
Because of the fewer capitalized words in Eng-
lish the precision for English first names is
higher, scoring 92.6% without further filtering.
Overall precision for English first and last names
was 98.7%.
</bodyText>
<subsectionHeader confidence="0.993607">
5.2 Recall
</subsectionHeader>
<bodyText confidence="0.999987894736842">
Recall mainly depends on the pattern rules used.
The experiments were performed with the 14
handmade rules given in appendix 1, which
surely are not sufficient.
Calculating the recall is not at all straightfor-
ward, because we do not know how many names
are contained in our corpora and experiments on
small corpora fail to show the natural growth of
items described in the previous section. Further,
recall will rise with a growing knowledge size.
So we modified the algorithm in a way that it
takes plain text as input, applies the rules to find
candidates and checks them in the big corpus.
Providing a large set of knowledge items, in an
experiment processing 1000 sentences, 71.4% of
the person names were extracted correctly.
To increase the coverage of the rules it is possi-
ble to add rules manually or start a process of
rule learning as described below.
</bodyText>
<subsectionHeader confidence="0.998707">
5.3. Propagation of Errors
</subsectionHeader>
<bodyText confidence="0.999020176470588">
During the run the error rate increases due to
finding candidates and verification through mis-
classified items. However, as the &amp;quot;era&amp;quot; example
(see section 4) illustrates, misclassified items
support the classification of goal items.
The amount of deterioration highly depends on
the pattern rules. Strict rules mean low recall but
high precision, whereas general rules have
greater coverage but find too much, resulting in
a trade-off between precision and recall.
Table 1 shows the error rate for first names for
the illustrated run (see section 3) over the course
of time.
From this we conclude that the algorithm is ro-
bust against errors and the quality of the classifi-
cations remains relatively stable during the run
when using appropriate rules.
</bodyText>
<table confidence="0.9997894">
total items Precision for Precision for
interval FN without FN with
length filter length filter
1-1000 87.1% 93.8%
1001-2000 90.0% 95.3%
4001-5000 88.1% 97.1%
9001-10000 83.2% 94.4%
19001-20000 83.7% 91.2%
21001-22000 86.2% 92.4%
24001-25000 83.0% 87.9%
</table>
<tableCaption confidence="0.999628">
Table 1: Propagation of Errors
</tableCaption>
<sectionHeader confidence="0.948072" genericHeader="method">
6 Classification on character level
</sectionHeader>
<bodyText confidence="0.978191806451613">
In German, most words misclassified as first
names were titles and professions. While they
cannot be distinguished by the rules used, they
differ strongly from the morphological view.
German titles are usually longer because they
are compounds, and parts of compounds are
used very frequently.
In this section, we introduce a method to distin-
guish between titles and first names at character
level, using the fact that the formation of words
follows language-dependent rules.
This procedure is implemented in the following
classifier A: Assume the property we are inter-
ested in is visible at the ending of a word (this is
basically true for different word classes in lan-
guages like English, French or German). We
build a decision tree [cf. McCarthy &amp; Lehnert
1995] reading the words character-by-character,
starting from the end. We stop if the feature is
uniquely determined.
Moreover, we could as well start from the be-
ginning of a word (classifier B). Finally, we can
use any connected substring of the word instead
of substrings containing the end or the beginning
(classifier C).
If the training set is large enough and the algo-
rithm of the classifier is appropriate, it will cover
both general rules as well as many exceptions.
Classifier A and B only differ on the direction a
word is analyzed. We build decision trees with
additional default child nodes as follows.
</bodyText>
<subsectionHeader confidence="0.98797">
6.1 Classifier A: Considering Prefixes
</subsectionHeader>
<bodyText confidence="0.994090081081081">
Step 1: Building the ordinary decision tree:
Given the training word list we construct
a prefix tree [cf. Gusfield 1999, Navarro
2001:38ff]. The leaves in the tree corre-
spond to word endings; here we store the
feature of the corresponding word.
Step 2: Reduction of the decision tree: If all
children of a given node have the same
feature, this feature is lifted to the parent
node and the children are deleted.
Step 3: Insertion of default features: If a node
does not yet have a feature, but one of
the features is very dominant (say, pres-
ent in 80% of the children), this feature
will be assigned as default feature.
For classification, the decision tree is used as
follows:
Step 1: Searching the tree: Reading the given
word from left to right we follow the
tree as far as possible. The reading proc-
ess stops in a certain node N.
Step 2: Classification: If the node N has an as-
signed feature F then return F. Other-
wise return no decision.
Figure 2 shows a part of the decision tree built
using first names Theoardis, Theobald, Theo-
derich, Theodor, Theresa, Therese, ... and the
singular title Theologe (which should be the only
title in our training list starting with Theo). As a
result, all children of Theo will be first names;
hence they get the feature firstname. The node
Theologe gets the feature title.
This turns out to be singular; hence their parent
Theo gets the default feature firstname. As a
consequence, Theophil will correctly be classi-
fied as firstname, while the exception Theologe
will still be classified as title.
</bodyText>
<figureCaption confidence="0.887011">
Figure 2: Prefix Decision Tree for Proper Names
</figureCaption>
<bodyText confidence="0.999967333333333">
As mentioned above, algorithm B works the
same way as algorithm A, using suffixes instead
of prefixes for the decision tree.
</bodyText>
<subsectionHeader confidence="0.995498">
6.2 Classifier C: Considering Substrings
</subsectionHeader>
<bodyText confidence="0.989732735294118">
Instead of concentrating on prefixes or suffixes,
we consider all relevant continuous substrings of
a given word. Unfortunately, there is no natural
tree structure for this set. Hence, we will con-
struct a decision list without default features.
Given is a training list containing pairs (word,
feature):
Construction of the decision list
Step 1: Collect all substring information. We
produce the following list L: For all
pairs (wordN, featureN) from the train-
ing list we generate all possible pairs of
the kind (continuous substring of
wordN, featureN). If wordN has length
n, we have n(n+1)/2 continuous sub-
strings. Finally the list is sorted alpha-
betically and duplicates are removed.
Step 2: Removing contradictions: If a substring
occurs with more then one feature, these
lines are deleted from L.
Step 3: Removing tails: If a certain string now
has a unique feature, all extensions of
this string should have the same feature
and the corresponding entries are re-
moved from L.
For classification, the decision list is used as
follows:
Step 1: Look-up of substrings: For a word to be
classified we generate its continuous
substrings and collect their features from
L.
Step 2: Classification: If all collected features
are equal, then return this feature. Oth-
erwise, return no decision.
</bodyText>
<subsectionHeader confidence="0.997364">
6.3 Properties of the classifiers
</subsectionHeader>
<bodyText confidence="0.999874666666667">
In the following, we assume that the classifiers
are trained with non-contradictory data. The
classifiers now have the following properties:
</bodyText>
<listItem confidence="0.921965625">
• The classifiers reproduce the results given in
the training set. Hence, they can also be
trained with rare exceptions.
• It is necessary to have a training set covering
all aspects of the data, otherwise the deci-
sion tree will be confused.
• It is appropriate to return no decision if the
classifier stops in the decision tree at a point
</listItem>
<bodyText confidence="0.997928166666667">
where children have mixed features.
Bagging [cf. Breiman 1996] the three classifiers,
we achieved a precision of 94.7% with 94.5%
recall, using merely a training set of 1368 exam-
ples on a test set of 683 items, distinguishing
between the three classes:
</bodyText>
<listItem confidence="0.999921333333333">
• First name (FN)
• Title (TI)
• None of these.
</listItem>
<bodyText confidence="0.805063666666667">
This method of postprocessing is applicable to
all features visible by the three classifiers, which
are:
</bodyText>
<listItem confidence="0.882146230769231">
• Features represented by word suffixes or
prefixes like inflection and some word for-
mation rules.
• Words carrying the same feature if they are
similar as strings. Candidates are all kinds of
proper names, as well as distinguishing
parts-of-speech.
• Words of languages for special purposes,
which are often built by combining parts
where some of them are very typical for a
given domain. Examples are chemical sub-
stances, professions and titles, or industrial
goods.
</listItem>
<figure confidence="0.998196071428571">
(root)
A
...
T
...
Z
Theo [default]
(FN)
Theodor
(FN)
Theobald
(FN)
Theologe
(TI)
</figure>
<sectionHeader confidence="0.680698" genericHeader="method">
7 Rule Learning
</sectionHeader>
<bodyText confidence="0.999906190476191">
Unlike most tasks in Inductive Logic Program-
ming (ILP) [cf. Dzeroski, 2001], our method
needs rules-of-thumb that find many candidates
like in boosting [cf. Collins, 1999], rather then a
rule precision of 100%.
For automatic rule induction we used a training
set of 236 sentences found automatically by
taking sentences containing known first names
and last names from the corpus. After excessive
annotation, all possible rules were built accord-
ing to the contexts of known items and after-
wards tested on the training set. To avoid rules
too general like UC-UC⇒FN-UC, the patterns
had to contain at least one problem specific tag
(i.e. FN, LN, TIT). The rules performing above a
certain precision threshold (in our experiments
we used 0.7) were taken as input for our algo-
rithm.
We obtained 106 rules for first names, 67 for
last names and 4 for titles, ranging from very
specific rules like
</bodyText>
<equation confidence="0.735009">
PM-PM-UC-LN ⇒ PM-PM-FN-LN
to very general ones like
TI-UC ⇒ TI-FN.
</equation>
<bodyText confidence="0.9934495">
In the table below some rules found by auto-
matic induction are shown.
</bodyText>
<table confidence="0.999582181818182">
Rule example context
FN-UC-LN Herbert Archibald
⇒ FN-FN-LN Miller
FN-LC-FN-UC Ilse und Maria Bode-
⇒ FN-LC-FN-LN mann
UC-UC-LN Präsident Bill Clinton
⇒ UC-FN-LN
FN-FN-UC Hans Christian An-
⇒ FN-FN-LN derson
TI-PM-UC-UC Dr. Helmut Kohl
⇒ TI-FS-FN-UC
</table>
<tableCaption confidence="0.997514">
Table 2: Rules Found by Automatic Induction
</tableCaption>
<bodyText confidence="0.998628666666667">
Using those rules as input for our algorithm, we
gained both, higher recall as well as higher pre-
cision compared to the handmade rules when
starting with the same knowledge. Table 3
shows precision rates for the three classes of
name elements, data was taken from a run with
19 start elements, the length filter for first names
was applied, and the string classifiers were not.
Due to less strict rules, precision decreases.
</bodyText>
<table confidence="0.997176625">
total items Prec. FN Prec. LN Prec. TIT
interval
1-1000 94,6% 99,6% 100%
1001-2000 94,8% 98,6% 100%
2001-3000 94,7% 98,4% 100%
4001-5000 84,7% 99,1% 100%
9001-10000 86,6% 98,6% 100%
24001-25000 74,0% 89,7% 100%
</table>
<tableCaption confidence="0.996279">
Table 2: Propagation of errors for inferred rules
</tableCaption>
<bodyText confidence="0.999833">
Percentage of first name items from the number
of total items was 23,3%, last name items made
75,2% of total items and title items yielded only
1,4%, because to the low number of title rules.
</bodyText>
<sectionHeader confidence="0.999018" genericHeader="method">
8 Future work
</sectionHeader>
<bodyText confidence="0.9999523">
Despite of the good results when using inferred
rules as described above for our algorithm, we
hope to improve the method as a whole with
respect to the size of the input knowledge.
Natural growth behaviour can be observed from
some 10 frequent start items, the string classifier
requires a couple of hundred words for training
whereas rule learning needs some 200 fully an-
notated sentences containing names. Experi-
ments with sparsely annotated training sentences
(100 knowledge items) yielded too specific and
too weak rules with poor performance w.r.t.
recall.
Another possibility would be to start with a
small set of seed rules [cf. Riloff 1999] and to
construct-by-example and rate rules during the
classification task.
Another interesting issue is the understanding of
relations suitable for this method from a theo-
retical viewpoint.
</bodyText>
<sectionHeader confidence="0.996809" genericHeader="method">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999159666666667">
The authors would like to thank Martin Läuter
for providing, implementing and testing the
three string classifiers.
</bodyText>
<sectionHeader confidence="0.996626" genericHeader="method">
10 References
</sectionHeader>
<reference confidence="0.993621295081967">
Apte, C.; Damerau, F.; Weiss, S. M. (1998) Text
Mining with Decision Trees and Decision Rules.
Proc. Conference on Automated Learning and Dis-
covery, Carnegie-Mellon University, June 1998.
Breiman, L. (1996) Bagging Predictors, Machine
Learning, Vol. 24, No. 2, pp. 123-140
Califf, M. E.; Mooney, R. J. (1997) Relational
Learning of Pattern-match Rules for Information
Extraction. Working Papers of the ACL-97 Work-
shop in NLP, 1997, 6-11.
Collins, M.; Singer, Y. (1999) Unsupervised Models
for Named Entity Classification. In: Proc. Of the
Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and very Large
Corpora.
Dempster, A.P.; Laird, N. M.; Rubin, D.B. (1977)
Maximum Likelihood from Incomplete Data via the
EM Algorithm, Journal of the Royal Statistical So-
ciety, Ser B, 39, 1-38.
Dzeroski, S.; Lavrac, N. (2001) Introduction to In-
ductive Logic Programming. In Saso Dzeroski and
Nada Lavrac, editors, Relational Data Mining,
pages 48-73. Springer-Verlag, Berlin
Freitag, D. (1998) Multistrategy Learning for Infor-
mation Extraction. Proc. 15th International Conf.
on Machine Learning, 161-169.
Gusfield, Dan (1999) Algorithms on Strings, Trees,
and Sequences. Cambridge University Press, UK.
McCarthy, J.; Lehnert, W. (1995) Using Decision
Trees for Coreference Resolution. In: Mellish, C.
(ed.) (1995). Proc. Fourteenth International Con-
ference on Artificial Intelligence, 1050-1055.
Nahm, U. Y.; Mooney, R. J. (2002) Text Mining with
Information Extraction. To appear in AAAI 2002
Spring Symposium on Mining Answers from Texts
and Knowledge Bases, Stanford, CA.
Navarro, G. (2001) A guided tour to approximate
string matching. ACM Computing Surveys 33(1)
(2001), 31-88.
Ng, H.; Lee, H. (1996) Integrating Multiple Knowl-
edge Sources to Disambiguate Word Sense: An Ex-
emplar-Based Approach. Proc. of the 34th Annual
Meeting of the ACL, 40-47.
Quasthoff, U.; Wolff, Ch. (2000) An Infrastructure
for Corpus-Based Monolingual Dictionaries. Proc.
LREC-2000. Second International Conference on
Language Resources and Evaluation. Athens, May
/ June 2000, Vol. I, 241-246.
Riloff, E.; Jones, R. (1999) Learning Dictionaries for
Information Extraction by Multi-Level Bootstrap-
ping. Proceedings of the sixteenth National Con-
ference on Artificial Intelligence (AAAI-99)
Roth, D. (1998) Learning to Resolve Natural Lan-
guage Ambiguities: A Unified Approach. Proc. of
the American Association of Artificial Intelligence,
806-813.
Witten, I. H.; Frank, E. (1999) Data Mining: Practi-
cal Machine Learning Tools and Techniques with
Java Implementations. San Francisco, CA: Morgan
Kaufman.
Appendix 1: Initial Handmade Rule Set
</reference>
<equation confidence="0.990283214285714">
UC-LN =&gt; FN-LN
PM-FN-PM-UC =&gt; PM-FN-PM-FN
TI-PM-UC-LN =&gt; TI-PM-FN-LN
FN-LN-PM-UC-LN =&gt; FN-LN-PM-FN-LN
FN-UC-PM =&gt; FN-LN-PM
FN-UC-LC =&gt; FN-LN-LC
TI-UC-LC =&gt; TI-LN-LC
TI-PM-UC-LC =&gt; TI-PM-LN-LC
LN-PM-FN-UC-PM =&gt; LN-PM-FN-LN-PM
UC-PM-FN-LN =&gt; TI-PM-FN-LN
UC-PM-LN =&gt; TI-PM-LN
DET-UC-FN-LN =&gt; DET-TI-FN-LN
DET-UC-FN-FN-LN =&gt; DET-TI-FN-FN-LN
DET-UC-LN =&gt; DET-TI-LN
</equation>
<bodyText confidence="0.9993355">
Note that the last three rules are specific for German
because titles are in upper case in this language.
</bodyText>
<sectionHeader confidence="0.8026555" genericHeader="method">
Appendix 2: 19 Start items used in the
experiments
</sectionHeader>
<figure confidence="0.90651225">
Name element Class
Schmidt LN
Reuter LN
Wagner LN
Schäuble LN
Vogts LN
Hoffmann LN
Schulz LN
Möller LN
Meyer LN
Beck LN
Michael FN
Thomas FN
Klaus FN
Wolfgang FN
Hans FN
Werner FN
Martin FN
Walter FN
Karl FN
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.633573">
<title confidence="0.9957975">Named Entity Learning and Expectation Maximization in Large Corpora</title>
<author confidence="0.954185">Uwe QUASTHOFF</author>
<author confidence="0.954185">Christian BIEMANN</author>
<author confidence="0.954185">Christian</author>
<affiliation confidence="0.8233045">CS Institute, Leipzig Augustusplatz</affiliation>
<address confidence="0.979177">Leipzig, Germany, 04109</address>
<abstract confidence="0.9914372">The regularity of named entities is used to learn names and to extract named entities. Having only a few name elements and a set of patterns the algorithm learns new names and its elements. A verification step assures quality using a large background corpus. Further improvement is reached through classifying the newly learnt elements on character level. Moreover, unsupervised rule learning is discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Apte</author>
<author>F Damerau</author>
<author>S M Weiss</author>
</authors>
<title>Text Mining with Decision Trees and Decision Rules.</title>
<date>1998</date>
<booktitle>Proc. Conference on Automated Learning and Discovery,</booktitle>
<institution>Carnegie-Mellon University,</institution>
<marker>Apte, Damerau, Weiss, 1998</marker>
<rawString>Apte, C.; Damerau, F.; Weiss, S. M. (1998) Text Mining with Decision Trees and Decision Rules. Proc. Conference on Automated Learning and Discovery, Carnegie-Mellon University, June 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging Predictors,</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<pages>123--140</pages>
<contexts>
<context position="15495" citStr="Breiman 1996" startWordPosition="2613" endWordPosition="2614">ure. Otherwise, return no decision. 6.3 Properties of the classifiers In the following, we assume that the classifiers are trained with non-contradictory data. The classifiers now have the following properties: • The classifiers reproduce the results given in the training set. Hence, they can also be trained with rare exceptions. • It is necessary to have a training set covering all aspects of the data, otherwise the decision tree will be confused. • It is appropriate to return no decision if the classifier stops in the decision tree at a point where children have mixed features. Bagging [cf. Breiman 1996] the three classifiers, we achieved a precision of 94.7% with 94.5% recall, using merely a training set of 1368 examples on a test set of 683 items, distinguishing between the three classes: • First name (FN) • Title (TI) • None of these. This method of postprocessing is applicable to all features visible by the three classifiers, which are: • Features represented by word suffixes or prefixes like inflection and some word formation rules. • Words carrying the same feature if they are similar as strings. Candidates are all kinds of proper names, as well as distinguishing parts-of-speech. • Wor</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Breiman, L. (1996) Bagging Predictors, Machine Learning, Vol. 24, No. 2, pp. 123-140</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Califf</author>
<author>R J Mooney</author>
</authors>
<title>Relational Learning of Pattern-match Rules for Information Extraction. Working Papers of the ACL-97 Workshop in NLP,</title>
<date>1997</date>
<pages>6--11</pages>
<marker>Califf, Mooney, 1997</marker>
<rawString>Califf, M. E.; Mooney, R. J. (1997) Relational Learning of Pattern-match Rules for Information Extraction. Working Papers of the ACL-97 Workshop in NLP, 1997, 6-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification. In:</title>
<date>1999</date>
<booktitle>Proc. Of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and very Large Corpora.</booktitle>
<marker>Collins, Singer, 1999</marker>
<rawString>Collins, M.; Singer, Y. (1999) Unsupervised Models for Named Entity Classification. In: Proc. Of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm,</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Ser B,</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A.P.; Laird, N. M.; Rubin, D.B. (1977) Maximum Likelihood from Incomplete Data via the EM Algorithm, Journal of the Royal Statistical Society, Ser B, 39, 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dzeroski</author>
<author>N Lavrac</author>
</authors>
<title>Introduction to Inductive Logic Programming.</title>
<date>2001</date>
<booktitle>In Saso Dzeroski and Nada Lavrac, editors, Relational Data Mining,</booktitle>
<pages>48--73</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin</location>
<marker>Dzeroski, Lavrac, 2001</marker>
<rawString>Dzeroski, S.; Lavrac, N. (2001) Introduction to Inductive Logic Programming. In Saso Dzeroski and Nada Lavrac, editors, Relational Data Mining, pages 48-73. Springer-Verlag, Berlin</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Multistrategy Learning for Information Extraction.</title>
<date>1998</date>
<booktitle>Proc. 15th International Conf. on Machine Learning,</booktitle>
<pages>161--169</pages>
<marker>Freitag, 1998</marker>
<rawString>Freitag, D. (1998) Multistrategy Learning for Information Extraction. Proc. 15th International Conf. on Machine Learning, 161-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences.</title>
<date>1999</date>
<publisher>Cambridge University Press, UK.</publisher>
<contexts>
<context position="11994" citStr="Gusfield 1999" startWordPosition="2023" endWordPosition="2024">beginning of a word (classifier B). Finally, we can use any connected substring of the word instead of substrings containing the end or the beginning (classifier C). If the training set is large enough and the algorithm of the classifier is appropriate, it will cover both general rules as well as many exceptions. Classifier A and B only differ on the direction a word is analyzed. We build decision trees with additional default child nodes as follows. 6.1 Classifier A: Considering Prefixes Step 1: Building the ordinary decision tree: Given the training word list we construct a prefix tree [cf. Gusfield 1999, Navarro 2001:38ff]. The leaves in the tree correspond to word endings; here we store the feature of the corresponding word. Step 2: Reduction of the decision tree: If all children of a given node have the same feature, this feature is lifted to the parent node and the children are deleted. Step 3: Insertion of default features: If a node does not yet have a feature, but one of the features is very dominant (say, present in 80% of the children), this feature will be assigned as default feature. For classification, the decision tree is used as follows: Step 1: Searching the tree: Reading the g</context>
</contexts>
<marker>Gusfield, 1999</marker>
<rawString>Gusfield, Dan (1999) Algorithms on Strings, Trees, and Sequences. Cambridge University Press, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>W Lehnert</author>
</authors>
<title>Using Decision Trees for Coreference Resolution. In:</title>
<date>1995</date>
<booktitle>Proc. Fourteenth International Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<editor>Mellish, C. (ed.)</editor>
<contexts>
<context position="11225" citStr="McCarthy &amp; Lehnert 1995" startWordPosition="1893" endWordPosition="1896">iffer strongly from the morphological view. German titles are usually longer because they are compounds, and parts of compounds are used very frequently. In this section, we introduce a method to distinguish between titles and first names at character level, using the fact that the formation of words follows language-dependent rules. This procedure is implemented in the following classifier A: Assume the property we are interested in is visible at the ending of a word (this is basically true for different word classes in languages like English, French or German). We build a decision tree [cf. McCarthy &amp; Lehnert 1995] reading the words character-by-character, starting from the end. We stop if the feature is uniquely determined. Moreover, we could as well start from the beginning of a word (classifier B). Finally, we can use any connected substring of the word instead of substrings containing the end or the beginning (classifier C). If the training set is large enough and the algorithm of the classifier is appropriate, it will cover both general rules as well as many exceptions. Classifier A and B only differ on the direction a word is analyzed. We build decision trees with additional default child nodes a</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>McCarthy, J.; Lehnert, W. (1995) Using Decision Trees for Coreference Resolution. In: Mellish, C. (ed.) (1995). Proc. Fourteenth International Conference on Artificial Intelligence, 1050-1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Y Nahm</author>
<author>R J Mooney</author>
</authors>
<title>Text Mining with Information Extraction.</title>
<date>2002</date>
<booktitle>AAAI 2002 Spring Symposium on Mining Answers from Texts and Knowledge Bases,</booktitle>
<location>Stanford, CA.</location>
<note>To appear in</note>
<marker>Nahm, Mooney, 2002</marker>
<rawString>Nahm, U. Y.; Mooney, R. J. (2002) Text Mining with Information Extraction. To appear in AAAI 2002 Spring Symposium on Mining Answers from Texts and Knowledge Bases, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys</journal>
<volume>33</volume>
<issue>1</issue>
<pages>31--88</pages>
<contexts>
<context position="12008" citStr="Navarro 2001" startWordPosition="2025" endWordPosition="2026">word (classifier B). Finally, we can use any connected substring of the word instead of substrings containing the end or the beginning (classifier C). If the training set is large enough and the algorithm of the classifier is appropriate, it will cover both general rules as well as many exceptions. Classifier A and B only differ on the direction a word is analyzed. We build decision trees with additional default child nodes as follows. 6.1 Classifier A: Considering Prefixes Step 1: Building the ordinary decision tree: Given the training word list we construct a prefix tree [cf. Gusfield 1999, Navarro 2001:38ff]. The leaves in the tree correspond to word endings; here we store the feature of the corresponding word. Step 2: Reduction of the decision tree: If all children of a given node have the same feature, this feature is lifted to the parent node and the children are deleted. Step 3: Insertion of default features: If a node does not yet have a feature, but one of the features is very dominant (say, present in 80% of the children), this feature will be assigned as default feature. For classification, the decision tree is used as follows: Step 1: Searching the tree: Reading the given word from</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Navarro, G. (2001) A guided tour to approximate string matching. ACM Computing Surveys 33(1) (2001), 31-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ng</author>
<author>H Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach.</title>
<date>1996</date>
<booktitle>Proc. of the 34th Annual Meeting of the ACL,</booktitle>
<pages>40--47</pages>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, H.; Lee, H. (1996) Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach. Proc. of the 34th Annual Meeting of the ACL, 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Quasthoff</author>
<author>Ch Wolff</author>
</authors>
<title>An Infrastructure for Corpus-Based Monolingual Dictionaries.</title>
<date>2000</date>
<booktitle>Proc. LREC-2000. Second International Conference on Language Resources and Evaluation.</booktitle>
<volume>Vol. I,</volume>
<pages>241--246</pages>
<location>Athens,</location>
<contexts>
<context position="1815" citStr="Quasthoff &amp; Wolff 2000" startWordPosition="280" endWordPosition="283">r rules hold for different kinds of named entities consisting of more than one word. Moreover, some elements of such multiwords (like president) are high frequency items, well known and of high significance for identifying a named entity. On the other hand, there are elements that often appear in named entities, but are not characteristic for them (like the first name Israel). Therefore, a verification step is included in the learning algorithm. 2 The Algorithm Our learning algorithm starts with a set of patterns and initial name elements. A large corpus of more then 10 million sentences [cf. Quasthoff &amp; Wolff 2000], taken from newspapers of the last 10 years is used for both, the identification of candidates for new name elements as well as for verifying the candidates found. The algorithm stops, if no more new name elements are found. The algorithm implements expectation maximization (EM) [cf. Dempster, 1977, Collins, 1999] in the following way: The combination of a learning step and a verification step are iterated. If more name elements are found, the recall of the verification step increases. The key property of this algorithm is to assure high precision and still get massive recall. From another p</context>
</contexts>
<marker>Quasthoff, Wolff, 2000</marker>
<rawString>Quasthoff, U.; Wolff, Ch. (2000) An Infrastructure for Corpus-Based Monolingual Dictionaries. Proc. LREC-2000. Second International Conference on Language Resources and Evaluation. Athens, May / June 2000, Vol. I, 241-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>Proceedings of the sixteenth National Conference on Artificial Intelligence (AAAI-99)</booktitle>
<marker>Riloff, Jones, 1999</marker>
<rawString>Riloff, E.; Jones, R. (1999) Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. Proceedings of the sixteenth National Conference on Artificial Intelligence (AAAI-99)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to Resolve Natural Language Ambiguities: A Unified Approach.</title>
<date>1998</date>
<journal>Proc. of the American Association of Artificial Intelligence,</journal>
<pages>806--813</pages>
<marker>Roth, 1998</marker>
<rawString>Roth, D. (1998) Learning to Resolve Natural Language Ambiguities: A Unified Approach. Proc. of the American Association of Artificial Intelligence, 806-813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</title>
<date>1999</date>
<publisher>Morgan Kaufman.</publisher>
<location>San Francisco, CA:</location>
<marker>Witten, Frank, 1999</marker>
<rawString>Witten, I. H.; Frank, E. (1999) Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. San Francisco, CA: Morgan Kaufman.</rawString>
</citation>
<citation valid="false">
<title>Appendix 1: Initial Handmade Rule Set</title>
<marker></marker>
<rawString>Appendix 1: Initial Handmade Rule Set</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>