<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038112">
<title confidence="0.9993745">
Travatar: A Forest-to-String Machine Translation Engine
based on Tree Transducers
</title>
<author confidence="0.997833">
Graham Neubig
</author>
<affiliation confidence="0.998979">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.835791">
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
</address>
<email confidence="0.997676">
neubig@is.naist.jp
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997523538461538">
In this paper we describe Travatar, a
forest-to-string machine translation (MT)
engine based on tree transducers. It pro-
vides an open-source C++ implementation
for the entire forest-to-string MT pipeline,
including rule extraction, tuning, decod-
ing, and evaluation. There are a number
of options for model training, and tuning
includes advanced options such as hyper-
graph MERT, and training of sparse fea-
tures through online learning. The train-
ing pipeline is modeled after that of the
popular Moses decoder, so users famil-
iar with Moses should be able to get
started quickly. We perform a valida-
tion experiment of the decoder on English-
Japanese machine translation, and find that
it is possible to achieve greater accuracy
than translation using phrase-based and
hierarchical-phrase-based translation. As
auxiliary results, we also compare differ-
ent syntactic parsers and alignment tech-
niques that we tested in the process of de-
veloping the decoder.
Travatar is available under the LGPL at
http://phontron.com/travatar
</bodyText>
<sectionHeader confidence="0.999598" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999287804347826">
One of the recent trends in statistical machine
translation (SMT) is the popularity of models that
use syntactic information to help solve problems
of long-distance reordering between the source
and target language text. These techniques can
be broadly divided into pre-ordering techniques,
which first parse and reorder the source sentence
into the target order before translating (Xia and
McCord, 2004; Isozaki et al., 2010b), and tree-
based decoding techniques, which take a tree or
forest as input and choose the reordering and
translation jointly (Yamada and Knight, 2001; Liu
et al., 2006; Mi et al., 2008). While pre-ordering is
not able to consider both translation and reorder-
ing in a joint model, it is useful in that it is done
before the actual translation process, so it can be
performed with a conventional translation pipeline
using a standard phrase-based decoder such as
Moses (Koehn et al., 2007). For tree-to-string sys-
tems, on the other hand, it is necessary to have
available or create a decoder that is equipped with
this functionality, which becomes a bottleneck in
the research and development process.
In this demo paper, we describe Travatar, an
open-source tree-to-string or forest-to-string trans-
lation system that can be used as a tool for transla-
tion using source-side syntax, and as a platform
for research into syntax-based translation meth-
ods. In particular, compared to other decoders
which mainly implement syntax-based translation
in the synchronous context-free grammar (SCFG)
framework (Chiang, 2007), Travatar is built upon
the tree transducer framework (Graehl and Knight,
2004), a richer formalism that can help capture
important distinctions between parse trees, as we
show in Section 2. Travatar includes a fully docu-
mented training and testing regimen that was mod-
eled around that of Moses, making it possible for
users familiar with Moses to get started with Tra-
vatar quickly. The framework of the software is
also designed to be extensible, so the toolkit is ap-
plicable for other tree-to-string transduction tasks.
In the evaluation of the decoder on English-
Japanese machine translation, we perform a com-
parison to Moses’s phrase-based, hierarchical-
phrase-based, and SCFG-based tree-to-string
</bodyText>
<page confidence="0.99074">
91
</page>
<note confidence="0.6264175">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 91–96,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.988056">
Figure 1: Tree-to-string translation rules for
SCFGs and tree transducers.
</figureCaption>
<bodyText confidence="0.997538142857143">
translation. Based on the results, we find that tree-
to-string, and particularly forest-to-string, transla-
tion using Travatar provides competitive or supe-
rior accuracy to all of these techniques. As aux-
iliary results, we also compare different syntactic
parsers and alignment techniques that we tested in
the process of developing the decoder.
</bodyText>
<sectionHeader confidence="0.969847" genericHeader="method">
2 Tree-to-String Translation
</sectionHeader>
<subsectionHeader confidence="0.874958">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999977446428572">
Tree-to-string translation uses syntactic informa-
tion to improve translation by first parsing the
source sentence, then using this source-side parse
tree to decide the translation and reordering of the
input. This method has several advantages, includ-
ing efficiency of decoding, relatively easy han-
dling of global reordering, and an intuitive repre-
sentation of de-lexicalized rules that express gen-
eral differences in order between the source and
target languages. Within tree-to-string translation
there are two major methodologies, synchronous
context-free grammars (Chiang, 2007), and tree
transducers (Graehl and Knight, 2004).
An example of tree-to-string translation rules
supported by SCFGs and tree transducers is shown
in Figure 1. In this example, the first rule is a
simple multi-word noun phrase, the second exam-
ple is an example of a delexicalized rule express-
ing translation from English SVO word order to
Japanese SOV word order. The third and fourth
examples are translations of a verb, noun phrase,
and prepositional phrase, where the third rule has
the preposition attatched to the verb, and the fourth
has the preposition attached to the noun.
For the SCFGs, it can be seen that on the source
side of the rule, there are placeholders correspond-
ing to syntactic phrases, and on the target side of
the rule there corresponding placeholders that do
not have a syntactic label. On the other hand in the
example of the translation rules using tree trans-
ducers, it can be seen that similar rules can be ex-
pressed, but the source rules are richer than simple
SCFG rules, also including the internal structure
of the parse tree. This internal structure is im-
portant for achieving translation results faithful to
the input parse. In particular, the third and fourth
rules show an intuitive example in which this in-
ternal structure can be important for translation.
Here the full tree structures demonstrate important
differences in the attachment of the prepositional
phrase to the verb or noun. While this is one of
the most difficult and important problems in syn-
tactic parsing, the source side in the SCFG is iden-
tical, losing the ability to distinguish between the
very information that parsers are designed to dis-
ambiguate.
In traditional tree-to-string translation methods,
the translator uses a single one-best parse tree out-
put by a syntactic parser, but parse errors have the
potential to degrade the quality of translation. An
important advance in tree-to-string translation that
helps ameliorate this difficulity is forest-to-string
translation, which represents a large number of
potential parses as a packed forest, allowing the
translator to choose between these parses during
the process of translation (Mi et al., 2008).
</bodyText>
<subsectionHeader confidence="0.999217">
2.2 The State of Open Source Software
</subsectionHeader>
<bodyText confidence="0.9994734">
There are a number of open-source software pack-
ages that support tree-to-string translation in the
SCFG framework. For example, Moses (Koehn et
al., 2007) and NiuTrans (Xiao et al., 2012) sup-
port the annotation of source-side syntactic labels,
and taking parse trees (or in the case of NiuTrans,
forests) as input.
There are also a few other decoders that sup-
port other varieties of using source-side syntax
to help improve translation or global reorder-
ing. For example, the cdec decoder (Dyer et al.,
2010) supports the context-free-reordering/finite-
state-translation framework described by Dyer and
Resnik (2010). The Akamon decoder (Wu et
al., 2012) supports translation using head-driven
</bodyText>
<page confidence="0.977557">
92
</page>
<bodyText confidence="0.999394222222222">
phrase structure grammars as described by Wu et
al. (2010).
However, to our knowledge, while there is a
general-purpose tool for tree automata in general
(May and Knight, 2006), there is no open-source
toolkit implementing the SMT pipeline in the tree
transducer framework, despite it being a target of
active research (Graehl and Knight, 2004; Liu et
al., 2006; Huang et al., 2006; Mi et al., 2008).
</bodyText>
<sectionHeader confidence="0.9284375" genericHeader="method">
3 The Travatar Machine Translation
Toolkit
</sectionHeader>
<bodyText confidence="0.999852666666667">
In this section, we describe the overall framework
of the Travatar decoder, following the order of the
training pipeline.
</bodyText>
<subsectionHeader confidence="0.998777">
3.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999984285714286">
This consists of parsing the source side sentence
and tokenizing the target side sentences. Travatar
can decode input in the bracketed format of the
Penn Treebank, or also in forest format. There is
documentation and scripts for using Travatar with
several parsers for English, Chinese, and Japanese
included with the toolkit.
</bodyText>
<subsectionHeader confidence="0.99948">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.999707642857143">
Once the data has been pre-processed, a tree-
to-string model can be trained with the training
pipeline included in the toolkit. Like the train-
ing pipeline for Moses, there is a single script that
performs alignment, rule extraction, scoring, and
parameter initialization. Language model training
can be performed using a separate toolkit, and in-
structions are provided in the documentation.
For word alignment, the Travatar training
pipeline is integrated with GIZA++ (Och and Ney,
2003) by default, but can also use alignments from
any other aligner.
Rule extraction is performed using the GHKM
algorithm (Galley et al., 2006) and its extension to
rule extraction from forests (Mi and Huang, 2008).
There are also a number of options implemented,
including rule composition, attachment of null-
aligned target words at either the highest point in
the tree, or at every possible position, and left and
right binarization (Galley et al., 2006; Wang et al.,
2007).
Rule scoring uses a standard set of forward
and backward conditional probabilities, lexical-
ized translation probabilities, phrase frequency,
and word and phrase counts. Rule scores are
stored as sparse vectors by default, which allows
for scoring using an arbitrarily large number of
feature functions.
</bodyText>
<subsectionHeader confidence="0.998161">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999749736842105">
Given a translation model Travatar is able to de-
code parsed input sentences to generate transla-
tions. The decoding itself is performed using the
bottom-up forest-to-string decoding algorithm of
Mi et al. (2008). Beam-search implemented us-
ing cube pruning (Chiang, 2007) is used to adjust
the trade-off between search speed and translation
accuracy.
The source side of the translation model is
stored using a space-efficient trie data structure
(Yata, 2012) implemented using the marisa-trie
toolkit.l Rule lookup is performed using left-to-
right depth-first search, which can be implemented
as prefix lookup in the trie for efficient search.
The language model storage uses the implemen-
tation in KenLM (Heafield, 2011), and particu-
larly the implementation that maintains left and
right language model states for syntax-based MT
(Heafield et al., 2011).
</bodyText>
<subsectionHeader confidence="0.995826">
3.4 Tuning and Evaluation
</subsectionHeader>
<bodyText confidence="0.999748523809524">
For tuning the parameters of the model, Travatar
natively supports minimum error rate training
(MERT) (Och, 2003) and is extension to hyper-
graphs (Kumar et al., 2009). This tuning can
be performed for evaluation measures including
BLEU (Papineni et al., 2002) and RIBES (Isozaki
et al., 2010a), with an easily extendable interface
that makes it simple to support other measures.
There is also a preliminary implementation of
online learning methods such as the structured per-
ceptron algorithm (Collins, 2002), and regularized
structured SVMs trained using FOBOS (Duchi
and Singer, 2009). There are plans to implement
more algorithms such as MIRA or AROW (Chi-
ang, 2012) in the near future.
The Travatar toolkit also provides an evaluation
program that can calculate the scores of transla-
tion output according to various evaluation mea-
sures, and calculate the significance of differ-
ences between systems using bootstrap resampling
(Koehn, 2004).
</bodyText>
<footnote confidence="0.646321">
lhttp://marisa-trie.googlecode.com
</footnote>
<page confidence="0.997376">
93
</page>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997901">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999939348837209">
In our experiments, we validated the performance
of the translation toolkit on English-Japanese
translation of Wikipedia articles, as specified by
the Kyoto Free Translation Task (KFTT) (Neubig,
2011). Training used the 405k sentences of train-
ing data of length under 60, tuning was performed
on the development set, and testing was performed
on the test set using the BLEU and RIBES mea-
sures. As baseline systems we use the Moses2 im-
plementation of phrase-based (MOSES-PBMT), hi-
erarchical phrase-based (MOSES-HIER), and tree-
to-string translation (MOSES-T2S). The phrase-
based and hierarchical phrase-based models were
trained with the default settings according to tuto-
rials on each web site.
For all systems, we use a 5-gram Kneser-Ney
smoothed language model. Alignment for each
system was performed using either GIZA++3 or
Nile4 with main results reported for the aligner
that achieved the best accuracy on the dev set, and
a further comparison shown in the auxiliary exper-
iments in Section 4.3. Tuning was performed with
minimum error rate training to maximize BLEU
over 200-best lists. Tokenization was performed
with the Stanford tokenizer for English, and the
KyTea word segmenter (Neubig et al., 2011) for
Japanese.
For all tree-to-string systems we use Egrets as
an English parser, as we found it to achieve high
accuracy, and it allows for the simple output of
forests. Rule extraction was performed using one-
best trees, which were right-binarized, and lower-
cased post-parsing. For Travatar, composed rules
of up to size 4 and a maximum of 2 non-terminals
and 7 terminals for each rule were used. Null-
aligned words were only attached to the top node,
and no count normalization was performed, in
contrast to Moses, which performs count normal-
ization and exhaustive null word attachment. De-
coding was performed over either one-best trees
(TRAV-T2S), or over forests including all edges in-
cluded in the parser 200-best list (TRAV-F2S), and
a pop limit of 1000 hypotheses was used for cube
</bodyText>
<footnote confidence="0.977521857142857">
2http://statmt.org/moses/
3http://code.google.com/p/giza-pp/
4http://code.google.com/p/nile/ As Nile is
a supervised aligner, we trained it on the alignments provided
with the KFTT.
5http://code.google.com/p/
egret-parser/
</footnote>
<tableCaption confidence="0.9377155">
Table 1: Translation results (BLEU, RIBES), rule
table size, and speed in sentences per second for
</tableCaption>
<bodyText confidence="0.92766825">
each system. Bold numbers indicate a statistically
significant difference over all other systems (boot-
strap resampling with p &gt; 0.05) (Koehn, 2004).
pruning.
</bodyText>
<subsectionHeader confidence="0.988657">
4.2 System Comparison
</subsectionHeader>
<bodyText confidence="0.999911421052632">
The comparison between the systems is shown in
Table 1. From these results we can see that the
systems utilizing source-side syntax significantly
outperform the PBMT and Hiero, validating the
usefulness of source side syntax on the English-to-
Japanese task. Comparing the two tree-to-string
sytems, we can see that TRAV-T2S has slightly
higher RIBES and slightly lower BLEU than
MOSES-T2S. One reason for the slightly higher
BLEU of MOSES-T2S is because Moses’s rule ex-
traction algorithm is more liberal in its attachment
of null-aligned words, resulting in a much larger
rule table (52.3M rules vs. 9.57M rules) and mem-
ory footprint. In this setting, TRAV-T2S is approx-
imately two times faster than MOSES-T2S. When
using forest based decoding in TRAV-F2S, we see
significant gains in accuracy over TRAV-T2S, with
BLEU slightly and RIBES greatly exceeding that
of MOSES-T2S.
</bodyText>
<subsectionHeader confidence="0.999962">
4.3 Effect of Alignment/Parsing
</subsectionHeader>
<bodyText confidence="0.99996525">
In addition, as auxiliary results, we present a com-
parison of Travatar’s tree-to-string and forest-to-
string systems using different alignment methods
and syntactic parsers to examine the results on
translation (Table 2).
For parsers, we compared Egret with the Stan-
ford parser.6 While we do not have labeled data
to calculate parse accuracies with, Egret is a clone
of the Berkeley parser, which has been reported to
achieve higher accuracy than the Stanford parser
on several domains (Kummerfeld et al., 2012).
From the translation results, we can see that STAN-
</bodyText>
<footnote confidence="0.958526">
6http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<table confidence="0.979435666666667">
BLEU RIBES Rules Sent/s.
MOSES-PBMT 22.27 68.37 10.1M 5.69
MOSES-HIER 22.04 70.29 34.2M 1.36
MOSES-T2S 23.81 72.01 52.3M 1.71
TRAV-T2S 23.15 72.32 9.57M 3.29
TRAV-F2S 23.97 73.27 9.57M 1.11
</table>
<page confidence="0.813266">
94
</page>
<table confidence="0.999311571428572">
GIZA++ Nile
BLEU RIBES BLEU RIBES
PBMT 22.28 68.37 22.37 68.43
HIER 22.05 70.29 21.77 69.31
STAN-T2S 21.47 70.94 22.44 72.02
EGRET-T2S 22.82 71.90 23.15 72.32
EGRET-F2S 23.35 71.77 23.97 73.27
</table>
<tableCaption confidence="0.761953">
Table 2: Translation results (BLEU, RIBES), for
several translation models (PBMT, Hiero, T2S,
F2S), aligners (GIZA++, Nile), and parsers (Stan-
ford, Egret).
</tableCaption>
<bodyText confidence="0.999088210526316">
T2S significantly underperforms EGRET-T2S, con-
firming that the effectiveness of the parser plays a
large effect on the translation accuracy.
Next, we compared the unsupervised aligner
GIZA++, with the supervised aligner Nile, which
uses syntactic information to improve alignment
accuracy (Riesa and Marcu, 2010). We held out
10% of the hand aligned data provided with the
KFTT, and found that GIZA++ achieves 58.32%
alignment F-measure, while Nile achieves 64.22%
F-measure. With respect to translation accuracy,
we found that for translation that does not use syn-
tactic information, improvements in alignment do
not necessarily increase translation accuracy, as
has been noted by Ganchev et al. (2008). How-
ever, for all tree-to-string systems, the improved
alignments result in significant improvements in
accuracy, showing that alignments are, in fact, im-
portant in our syntax-driven translation setup.
</bodyText>
<sectionHeader confidence="0.987836" genericHeader="conclusions">
5 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999878774193548">
In this paper, we introduced Travatar, an open-
source toolkit for forest-to-string translation using
tree transducers. We hope this decoder will be
useful to the research community as a test-bed for
forest-to-string systems. The software is already
sufficiently mature to be used as is, as evidenced
by the competitive, if not superior, results in our
English-Japanese evaluation.
We have a number of plans for future devel-
opment. First, we plan to support advanced rule
extraction techniques, such as fuller support for
count regularization and forest-based rule extrac-
tion (Mi and Huang, 2008), and using the EM
algorithm to choose attachments for null-aligned
words (Galley et al., 2006) or the direction of rule
binarization (Wang et al., 2007). We also plan
to incorporate advances in decoding to improve
search speed (Huang and Mi, 2010). In addition,
there is a preliminary implementation of the abil-
ity to introduce target-side syntactic information,
either through hard constraints as in tree-to-tree
translation systems (Graehl and Knight, 2004), or
through soft constraints, as in syntax-augmented
machine translation (Zollmann and Venugopal,
2006). Finally, we will provide better support of
parallelization through the entire pipeline to in-
crease the efficiency of training and decoding.
Acknowledgements: We thank Kevin Duh and an
anonymous reviewer for helpful comments. Part
of this work was supported by JSPS KAKENHI
Grant Number 25730136.
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999417542857143">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, pages 1159–1187.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1–8.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899–
2934.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT-
NAACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7–12.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, pages 961–968.
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar.
2008. Better alignments = better translations? In
Proc. ACL.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT, pages 105–112.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
</reference>
<page confidence="0.992274">
95
</page>
<reference confidence="0.998295773195877">
model state for syntactic machine translation. In
Proc. IWSLT.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proc. WMT, pages 187–
197.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proc. EMNLP, pages 273–283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66–73.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. EMNLP, pages 944–952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proc. WMT
and MetricsMATR.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177–180, Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. ACL,
pages 163–171.
Jonathan K Kummerfeld, David Hall, James R Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: an empirical investigation of er-
ror types in parser output. In Proc. EMNLP, pages
1048–1059.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL.
Jonathan May and Kevin Knight. 2006. Tiburon:
A weighted tree automata toolkit. In Implementa-
tion and Application of Automata, pages 102–113.
Springer.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. EMNLP, pages 206–
214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL, pages 192–199.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. ACL,
pages 529–533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311–318, Philadelphia, USA.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proc. ACL, pages
157–166.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proc. EMNLP, pages
746–754.
Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proc. ACL, pages 325–334.
Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsu-
jii. 2012. Akamon: An open source toolkit for
tree/forest-based statistical machine translation. In
Proceedings of the ACL 2012 System Demonstra-
tions, pages 127–132.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. Niutrans: An open source toolkit for phrase-
based and syntax-based machine translation. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 19–24.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL.
Susumu Yata. 2012. Dictionary compression using
nested prefix/Patricia tries (in Japanese). In Proc.
17th NLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. WMT.
</reference>
<page confidence="0.998415">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.483116">
<title confidence="0.9995575">Travatar: A Forest-to-String Machine Translation based on Tree Transducers</title>
<author confidence="0.936913">Graham</author>
<affiliation confidence="0.9995165">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.988709">8916-5 Takayama-cho, Ikoma-shi, Nara,</address>
<email confidence="0.988588">neubig@is.naist.jp</email>
<abstract confidence="0.984671269230769">In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on English- Japanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at</abstract>
<web confidence="0.884414">http://phontron.com/travatar</web>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2820" citStr="Chiang, 2007" startWordPosition="425" endWordPosition="426">to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods. In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG) framework (Chiang, 2007), Travatar is built upon the tree transducer framework (Graehl and Knight, 2004), a richer formalism that can help capture important distinctions between parse trees, as we show in Section 2. Travatar includes a fully documented training and testing regimen that was modeled around that of Moses, making it possible for users familiar with Moses to get started with Travatar quickly. The framework of the software is also designed to be extensible, so the toolkit is applicable for other tree-to-string transduction tasks. In the evaluation of the decoder on EnglishJapanese machine translation, we p</context>
<context position="4751" citStr="Chiang, 2007" startWordPosition="706" endWordPosition="707">tring Translation 2.1 Overview Tree-to-string translation uses syntactic information to improve translation by first parsing the source sentence, then using this source-side parse tree to decide the translation and reordering of the input. This method has several advantages, including efficiency of decoding, relatively easy handling of global reordering, and an intuitive representation of de-lexicalized rules that express general differences in order between the source and target languages. Within tree-to-string translation there are two major methodologies, synchronous context-free grammars (Chiang, 2007), and tree transducers (Graehl and Knight, 2004). An example of tree-to-string translation rules supported by SCFGs and tree transducers is shown in Figure 1. In this example, the first rule is a simple multi-word noun phrase, the second example is an example of a delexicalized rule expressing translation from English SVO word order to Japanese SOV word order. The third and fourth examples are translations of a verb, noun phrase, and prepositional phrase, where the third rule has the preposition attatched to the verb, and the fourth has the preposition attached to the noun. For the SCFGs, it c</context>
<context position="10123" citStr="Chiang, 2007" startWordPosition="1561" endWordPosition="1562">, 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Ev</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1159--1187</pages>
<contexts>
<context position="11400" citStr="Chiang, 2012" startWordPosition="1757" endWordPosition="1759">ly supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). lhttp://marisa-trie.googlecode.com 93 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-Japanese translation of Wikipedia articles, as specified by the Kyoto Free Translation Task (KFTT) (Neubig, 2011). Training used the 405k sentences of training da</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research, pages 1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="11240" citStr="Collins, 2002" startWordPosition="1732" endWordPosition="1733">ft and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). lhttp://marisa-trie.googlecode.com 93 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-Ja</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>10</volume>
<pages>2934</pages>
<contexts>
<context position="11318" citStr="Duchi and Singer, 2009" startWordPosition="1741" endWordPosition="1744">l., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). lhttp://marisa-trie.googlecode.com 93 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-Japanese translation of Wikipedia articles, as specified by the Kyoto Free Trans</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899– 2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Context-free reordering, finite-state translation.</title>
<date>2010</date>
<booktitle>In Proc. HLTNAACL.</booktitle>
<contexts>
<context position="7568" citStr="Dyer and Resnik (2010)" startWordPosition="1159" endWordPosition="1162">oftware There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder,</context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>Chris Dyer and Philip Resnik. 2010. Context-free reordering, finite-state translation. In Proc. HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="7461" citStr="Dyer et al., 2010" startWordPosition="1148" endWordPosition="1151">etween these parses during the process of translation (Mi et al., 2008). 2.2 The State of Open Source Software There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Trava</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="9204" citStr="Galley et al., 2006" startWordPosition="1417" endWordPosition="1420">he data has been pre-processed, a treeto-string model can be trained with the training pipeline included in the toolkit. Like the training pipeline for Moses, there is a single script that performs alignment, rule extraction, scoring, and parameter initialization. Language model training can be performed using a separate toolkit, and instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are stored as sparse vectors by default, which allows for scoring using an arbitraril</context>
<context position="17942" citStr="Galley et al., 2006" startWordPosition="2749" endWordPosition="2752">st-to-string translation using tree transducers. We hope this decoder will be useful to the research community as a test-bed for forest-to-string systems. The software is already sufficiently mature to be used as is, as evidenced by the competitive, if not superior, results in our English-Japanese evaluation. We have a number of plans for future development. First, we plan to support advanced rule extraction techniques, such as fuller support for count regularization and forest-based rule extraction (Mi and Huang, 2008), and using the EM algorithm to choose attachments for null-aligned words (Galley et al., 2006) or the direction of rule binarization (Wang et al., 2007). We also plan to incorporate advances in decoding to improve search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic information, either through hard constraints as in tree-to-tree translation systems (Graehl and Knight, 2004), or through soft constraints, as in syntax-augmented machine translation (Zollmann and Venugopal, 2006). Finally, we will provide better support of parallelization through the entire pipeline to increase the efficiency of training and</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Better alignments = better translations? In</title>
<date>2008</date>
<booktitle>Proc. ACL.</booktitle>
<marker>Ganchev, Grac¸a, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008. Better alignments = better translations? In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="2900" citStr="Graehl and Knight, 2004" startWordPosition="435" endWordPosition="438">le or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods. In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG) framework (Chiang, 2007), Travatar is built upon the tree transducer framework (Graehl and Knight, 2004), a richer formalism that can help capture important distinctions between parse trees, as we show in Section 2. Travatar includes a fully documented training and testing regimen that was modeled around that of Moses, making it possible for users familiar with Moses to get started with Travatar quickly. The framework of the software is also designed to be extensible, so the toolkit is applicable for other tree-to-string transduction tasks. In the evaluation of the decoder on EnglishJapanese machine translation, we perform a comparison to Moses’s phrase-based, hierarchicalphrase-based, and SCFG-</context>
<context position="4799" citStr="Graehl and Knight, 2004" startWordPosition="711" endWordPosition="714">o-string translation uses syntactic information to improve translation by first parsing the source sentence, then using this source-side parse tree to decide the translation and reordering of the input. This method has several advantages, including efficiency of decoding, relatively easy handling of global reordering, and an intuitive representation of de-lexicalized rules that express general differences in order between the source and target languages. Within tree-to-string translation there are two major methodologies, synchronous context-free grammars (Chiang, 2007), and tree transducers (Graehl and Knight, 2004). An example of tree-to-string translation rules supported by SCFGs and tree transducers is shown in Figure 1. In this example, the first rule is a simple multi-word noun phrase, the second example is an example of a delexicalized rule expressing translation from English SVO word order to Japanese SOV word order. The third and fourth examples are translations of a verb, noun phrase, and prepositional phrase, where the third rule has the preposition attatched to the verb, and the fourth has the preposition attached to the noun. For the SCFGs, it can be seen that on the source side of the rule, </context>
<context position="7992" citStr="Graehl and Knight, 2004" startWordPosition="1226" endWordPosition="1229"> improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the target side sentences. Travatar can decode input in the bracketed format of the Penn Treebank, or also in forest format. There is documentation and scripts for using Travatar with several parsers for English, Chinese, and Japanese included with the toolkit. 3.2 Training Once the data </context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. HLT, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Tetsuo Kiso</author>
<author>Marcello Federico</author>
</authors>
<title>Left language model state for syntactic machine translation.</title>
<date>2011</date>
<booktitle>In Proc. IWSLT.</booktitle>
<contexts>
<context position="10704" citStr="Heafield et al., 2011" startWordPosition="1647" endWordPosition="1650">emented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and</context>
</contexts>
<marker>Heafield, Hoang, Koehn, Kiso, Federico, 2011</marker>
<rawString>Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo Kiso, and Marcello Federico. 2011. Left language model state for syntactic machine translation. In Proc. IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>Kenlm: Faster and smaller language model queries. In</title>
<date>2011</date>
<booktitle>Proc. WMT,</booktitle>
<pages>187--197</pages>
<contexts>
<context position="10571" citStr="Heafield, 2011" startWordPosition="1629" endWordPosition="1630">The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learnin</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. Kenlm: Faster and smaller language model queries. In Proc. WMT, pages 187– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="18095" citStr="Huang and Mi, 2010" startWordPosition="2775" endWordPosition="2778"> The software is already sufficiently mature to be used as is, as evidenced by the competitive, if not superior, results in our English-Japanese evaluation. We have a number of plans for future development. First, we plan to support advanced rule extraction techniques, such as fuller support for count regularization and forest-based rule extraction (Mi and Huang, 2008), and using the EM algorithm to choose attachments for null-aligned words (Galley et al., 2006) or the direction of rule binarization (Wang et al., 2007). We also plan to incorporate advances in decoding to improve search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic information, either through hard constraints as in tree-to-tree translation systems (Graehl and Knight, 2004), or through soft constraints, as in syntax-augmented machine translation (Zollmann and Venugopal, 2006). Finally, we will provide better support of parallelization through the entire pipeline to increase the efficiency of training and decoding. Acknowledgements: We thank Kevin Duh and an anonymous reviewer for helpful comments. Part of this work was supported by JSPS KAKENHI Grant Num</context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proc. EMNLP, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="8030" citStr="Huang et al., 2006" startWordPosition="1234" endWordPosition="1237">For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the target side sentences. Travatar can decode input in the bracketed format of the Penn Treebank, or also in forest format. There is documentation and scripts for using Travatar with several parsers for English, Chinese, and Japanese included with the toolkit. 3.2 Training Once the data has been pre-processed, a treeto-strin</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA, pages 66–73.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs.</title>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>944--952</pages>
<marker>Isozaki, Hirao, Duh, </marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs. In Proc. EMNLP, pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head finalization: A simple reordering rule for SOV languages. In</title>
<date>2010</date>
<booktitle>Proc. WMT and MetricsMATR.</booktitle>
<contexts>
<context position="1708" citStr="Isozaki et al., 2010" startWordPosition="247" endWordPosition="250"> different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http://phontron.com/travatar 1 Introduction One of the recent trends in statistical machine translation (SMT) is the popularity of models that use syntactic information to help solve problems of long-distance reordering between the source and target language text. These techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the target order before translating (Xia and McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is e</context>
<context position="11023" citStr="Isozaki et al., 2010" startWordPosition="1698" endWordPosition="1701">h-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences betwe</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010b. Head finalization: A simple reordering rule for SOV languages. In Proc. WMT and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="2196" citStr="Koehn et al., 2007" startWordPosition="330" endWordPosition="333">ch first parse and reorder the source sentence into the target order before translating (Xia and McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods. In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG) </context>
<context position="7108" citStr="Koehn et al., 2007" startWordPosition="1088" endWordPosition="1091">ne-best parse tree output by a syntactic parser, but parse errors have the potential to degrade the quality of translation. An important advance in tree-to-string translation that helps ameliorate this difficulity is forest-to-string translation, which represents a large number of potential parses as a packed forest, allowing the translator to choose between these parses during the process of translation (Mi et al., 2008). 2.2 The State of Open Source Software There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010).</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="11674" citStr="Koehn, 2004" startWordPosition="1799" endWordPosition="1800"> that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). lhttp://marisa-trie.googlecode.com 93 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-Japanese translation of Wikipedia articles, as specified by the Kyoto Free Translation Task (KFTT) (Neubig, 2011). Training used the 405k sentences of training data of length under 60, tuning was performed on the development set, and testing was performed on the test set using the BLEU and RIBES measures. As baseline systems we use the Moses2 implementation of phrase-based (MOSES-PBMT), hierarchical phrase-based (MOSES-HIER), and tr</context>
<context position="14217" citStr="Koehn, 2004" startWordPosition="2185" endWordPosition="2186">2S), or over forests including all edges included in the parser 200-best list (TRAV-F2S), and a pop limit of 1000 hypotheses was used for cube 2http://statmt.org/moses/ 3http://code.google.com/p/giza-pp/ 4http://code.google.com/p/nile/ As Nile is a supervised aligner, we trained it on the alignments provided with the KFTT. 5http://code.google.com/p/ egret-parser/ Table 1: Translation results (BLEU, RIBES), rule table size, and speed in sentences per second for each system. Bold numbers indicate a statistically significant difference over all other systems (bootstrap resampling with p &gt; 0.05) (Koehn, 2004). pruning. 4.2 System Comparison The comparison between the systems is shown in Table 1. From these results we can see that the systems utilizing source-side syntax significantly outperform the PBMT and Hiero, validating the usefulness of source side syntax on the English-toJapanese task. Comparing the two tree-to-string sytems, we can see that TRAV-T2S has slightly higher RIBES and slightly lower BLEU than MOSES-T2S. One reason for the slightly higher BLEU of MOSES-T2S is because Moses’s rule extraction algorithm is more liberal in its attachment of null-aligned words, resulting in a much lar</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="10898" citStr="Kumar et al., 2009" startWordPosition="1678" endWordPosition="1681">rie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate t</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. In Proc. ACL, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser showdown at the wall street corral: an empirical investigation of error types in parser output.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1048--1059</pages>
<contexts>
<context position="15666" citStr="Kummerfeld et al., 2012" startWordPosition="2410" endWordPosition="2413"> over TRAV-T2S, with BLEU slightly and RIBES greatly exceeding that of MOSES-T2S. 4.3 Effect of Alignment/Parsing In addition, as auxiliary results, we present a comparison of Travatar’s tree-to-string and forest-tostring systems using different alignment methods and syntactic parsers to examine the results on translation (Table 2). For parsers, we compared Egret with the Stanford parser.6 While we do not have labeled data to calculate parse accuracies with, Egret is a clone of the Berkeley parser, which has been reported to achieve higher accuracy than the Stanford parser on several domains (Kummerfeld et al., 2012). From the translation results, we can see that STAN6http://nlp.stanford.edu/software/ lex-parser.shtml BLEU RIBES Rules Sent/s. MOSES-PBMT 22.27 68.37 10.1M 5.69 MOSES-HIER 22.04 70.29 34.2M 1.36 MOSES-T2S 23.81 72.01 52.3M 1.71 TRAV-T2S 23.15 72.32 9.57M 3.29 TRAV-F2S 23.97 73.27 9.57M 1.11 94 GIZA++ Nile BLEU RIBES BLEU RIBES PBMT 22.28 68.37 22.37 68.43 HIER 22.05 70.29 21.77 69.31 STAN-T2S 21.47 70.94 22.44 72.02 EGRET-T2S 22.82 71.90 23.15 72.32 EGRET-F2S 23.35 71.77 23.97 73.27 Table 2: Translation results (BLEU, RIBES), for several translation models (PBMT, Hiero, T2S, F2S), aligners (</context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K Kummerfeld, David Hall, James R Curran, and Dan Klein. 2012. Parser showdown at the wall street corral: an empirical investigation of error types in parser output. In Proc. EMNLP, pages 1048–1059.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1876" citStr="Liu et al., 2006" startWordPosition="275" endWordPosition="278">ravatar 1 Introduction One of the recent trends in statistical machine translation (SMT) is the popularity of models that use syntactic information to help solve problems of long-distance reordering between the source and target language text. These techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the target order before translating (Xia and McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-st</context>
<context position="8010" citStr="Liu et al., 2006" startWordPosition="1230" endWordPosition="1233">lobal reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the target side sentences. Travatar can decode input in the bracketed format of the Penn Treebank, or also in forest format. There is documentation and scripts for using Travatar with several parsers for English, Chinese, and Japanese included with the toolkit. 3.2 Training Once the data has been pre-proce</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Tiburon: A weighted tree automata toolkit.</title>
<date>2006</date>
<booktitle>In Implementation and Application of Automata,</booktitle>
<pages>102--113</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7825" citStr="May and Knight, 2006" startWordPosition="1200" endWordPosition="1203">king parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the target side sentences. Travatar can decode input in the bracketed format of the Penn Treebank, or also in forest forma</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. Tiburon: A weighted tree automata toolkit. In Implementation and Application of Automata, pages 102–113. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="9275" citStr="Mi and Huang, 2008" startWordPosition="1429" endWordPosition="1432">h the training pipeline included in the toolkit. Like the training pipeline for Moses, there is a single script that performs alignment, rule extraction, scoring, and parameter initialization. Language model training can be performed using a separate toolkit, and instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. 3.3 Decoding Given a translation m</context>
<context position="17847" citStr="Mi and Huang, 2008" startWordPosition="2734" endWordPosition="2737">on and Future Directions In this paper, we introduced Travatar, an opensource toolkit for forest-to-string translation using tree transducers. We hope this decoder will be useful to the research community as a test-bed for forest-to-string systems. The software is already sufficiently mature to be used as is, as evidenced by the competitive, if not superior, results in our English-Japanese evaluation. We have a number of plans for future development. First, we plan to support advanced rule extraction techniques, such as fuller support for count regularization and forest-based rule extraction (Mi and Huang, 2008), and using the EM algorithm to choose attachments for null-aligned words (Galley et al., 2006) or the direction of rule binarization (Wang et al., 2007). We also plan to incorporate advances in decoding to improve search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic information, either through hard constraints as in tree-to-tree translation systems (Graehl and Knight, 2004), or through soft constraints, as in syntax-augmented machine translation (Zollmann and Venugopal, 2006). Finally, we will provide better su</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. EMNLP, pages 206– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1894" citStr="Mi et al., 2008" startWordPosition="279" endWordPosition="282">tion One of the recent trends in statistical machine translation (SMT) is the popularity of models that use syntactic information to help solve problems of long-distance reordering between the source and target language text. These techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the target order before translating (Xia and McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-</context>
<context position="6914" citStr="Mi et al., 2008" startWordPosition="1057" endWordPosition="1060">al, losing the ability to distinguish between the very information that parsers are designed to disambiguate. In traditional tree-to-string translation methods, the translator uses a single one-best parse tree output by a syntactic parser, but parse errors have the potential to degrade the quality of translation. An important advance in tree-to-string translation that helps ameliorate this difficulity is forest-to-string translation, which represents a large number of potential parses as a packed forest, allowing the translator to choose between these parses during the process of translation (Mi et al., 2008). 2.2 The State of Open Source Software There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-tra</context>
<context position="10064" citStr="Mi et al. (2008)" startWordPosition="1551" endWordPosition="1554">sible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. ACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>529--533</pages>
<location>Portland, USA,</location>
<contexts>
<context position="12960" citStr="Neubig et al., 2011" startWordPosition="1992" endWordPosition="1995">cal phrase-based models were trained with the default settings according to tutorials on each web site. For all systems, we use a 5-gram Kneser-Ney smoothed language model. Alignment for each system was performed using either GIZA++3 or Nile4 with main results reported for the aligner that achieved the best accuracy on the dev set, and a further comparison shown in the auxiliary experiments in Section 4.3. Tuning was performed with minimum error rate training to maximize BLEU over 200-best lists. Tokenization was performed with the Stanford tokenizer for English, and the KyTea word segmenter (Neubig et al., 2011) for Japanese. For all tree-to-string systems we use Egrets as an English parser, as we found it to achieve high accuracy, and it allows for the simple output of forests. Rule extraction was performed using onebest trees, which were right-binarized, and lowercased post-parsing. For Travatar, composed rules of up to size 4 and a maximum of 2 non-terminals and 7 terminals for each rule were used. Nullaligned words were only attached to the top node, and no count normalization was performed, in contrast to Moses, which performs count normalization and exhaustive null word attachment. Decoding was</context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proc. ACL, pages 529–533, Portland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>The Kyoto free translation task.</title>
<date>2011</date>
<note>http://www.phontron.com/kftt.</note>
<contexts>
<context position="11951" citStr="Neubig, 2011" startWordPosition="1834" endWordPosition="1835"> implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). lhttp://marisa-trie.googlecode.com 93 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-Japanese translation of Wikipedia articles, as specified by the Kyoto Free Translation Task (KFTT) (Neubig, 2011). Training used the 405k sentences of training data of length under 60, tuning was performed on the development set, and testing was performed on the test set using the BLEU and RIBES measures. As baseline systems we use the Moses2 implementation of phrase-based (MOSES-PBMT), hierarchical phrase-based (MOSES-HIER), and treeto-string translation (MOSES-T2S). The phrasebased and hierarchical phrase-based models were trained with the default settings according to tutorials on each web site. For all systems, we use a 5-gram Kneser-Ney smoothed language model. Alignment for each system was performe</context>
</contexts>
<marker>Neubig, 2011</marker>
<rawString>Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9064" citStr="Och and Ney, 2003" startWordPosition="1394" endWordPosition="1397">tion and scripts for using Travatar with several parsers for English, Chinese, and Japanese included with the toolkit. 3.2 Training Once the data has been pre-processed, a treeto-string model can be trained with the training pipeline included in the toolkit. Like the training pipeline for Moses, there is a single script that performs alignment, rule extraction, scoring, and parameter initialization. Language model training can be performed using a separate toolkit, and instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phr</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="10845" citStr="Och, 2003" startWordPosition="1670" endWordPosition="1671">on model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit als</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10991" citStr="Papineni et al., 2002" startWordPosition="1692" endWordPosition="1695"> performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Daniel Marcu</author>
</authors>
<title>Hierarchical search for word alignment.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="16624" citStr="Riesa and Marcu, 2010" startWordPosition="2548" endWordPosition="2551">22.28 68.37 22.37 68.43 HIER 22.05 70.29 21.77 69.31 STAN-T2S 21.47 70.94 22.44 72.02 EGRET-T2S 22.82 71.90 23.15 72.32 EGRET-F2S 23.35 71.77 23.97 73.27 Table 2: Translation results (BLEU, RIBES), for several translation models (PBMT, Hiero, T2S, F2S), aligners (GIZA++, Nile), and parsers (Stanford, Egret). T2S significantly underperforms EGRET-T2S, confirming that the effectiveness of the parser plays a large effect on the translation accuracy. Next, we compared the unsupervised aligner GIZA++, with the supervised aligner Nile, which uses syntactic information to improve alignment accuracy (Riesa and Marcu, 2010). We held out 10% of the hand aligned data provided with the KFTT, and found that GIZA++ achieves 58.32% alignment F-measure, while Nile achieves 64.22% F-measure. With respect to translation accuracy, we found that for translation that does not use syntactic information, improvements in alignment do not necessarily increase translation accuracy, as has been noted by Ganchev et al. (2008). However, for all tree-to-string systems, the improved alignments result in significant improvements in accuracy, showing that alignments are, in fact, important in our syntax-driven translation setup. 5 Conc</context>
</contexts>
<marker>Riesa, Marcu, 2010</marker>
<rawString>Jason Riesa and Daniel Marcu. 2010. Hierarchical search for word alignment. In Proc. ACL, pages 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="9536" citStr="Wang et al., 2007" startWordPosition="1472" endWordPosition="1475">d instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to a</context>
<context position="18000" citStr="Wang et al., 2007" startWordPosition="2759" endWordPosition="2762">s decoder will be useful to the research community as a test-bed for forest-to-string systems. The software is already sufficiently mature to be used as is, as evidenced by the competitive, if not superior, results in our English-Japanese evaluation. We have a number of plans for future development. First, we plan to support advanced rule extraction techniques, such as fuller support for count regularization and forest-based rule extraction (Mi and Huang, 2008), and using the EM algorithm to choose attachments for null-aligned words (Galley et al., 2006) or the direction of rule binarization (Wang et al., 2007). We also plan to incorporate advances in decoding to improve search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic information, either through hard constraints as in tree-to-tree translation systems (Graehl and Knight, 2004), or through soft constraints, as in syntax-augmented machine translation (Zollmann and Venugopal, 2006). Finally, we will provide better support of parallelization through the entire pipeline to increase the efficiency of training and decoding. Acknowledgements: We thank Kevin Duh and an ano</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proc. EMNLP, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Fine-grained tree-to-string translation rule extraction.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>325--334</pages>
<contexts>
<context position="7707" citStr="Wu et al. (2010)" startWordPosition="1181" endWordPosition="1184">ehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the </context>
</contexts>
<marker>Wu, Matsuzaki, Tsujii, 2010</marker>
<rawString>Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2010. Fine-grained tree-to-string translation rule extraction. In Proc. ACL, pages 325–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Akamon: An open source toolkit for tree/forest-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="7606" citStr="Wu et al., 2012" startWordPosition="1166" endWordPosition="1169">ftware packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training p</context>
</contexts>
<marker>Wu, Matsuzaki, Tsujii, 2012</marker>
<rawString>Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2012. Akamon: An open source toolkit for tree/forest-based statistical machine translation. In Proceedings of the ACL 2012 System Demonstrations, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1686" citStr="Xia and McCord, 2004" startWordPosition="243" endWordPosition="246">sults, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http://phontron.com/travatar 1 Introduction One of the recent trends in statistical machine translation (SMT) is the popularity of models that use syntactic information to help solve problems of long-distance reordering between the source and target language text. These techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the target order before translating (Xia and McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or crea</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Hao Zhang</author>
<author>Qiang Li</author>
</authors>
<title>Niutrans: An open source toolkit for phrasebased and syntax-based machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>pages</pages>
<contexts>
<context position="7141" citStr="Xiao et al., 2012" startWordPosition="1094" endWordPosition="1097">tactic parser, but parse errors have the potential to degrade the quality of translation. An important advance in tree-to-string translation that helps ameliorate this difficulity is forest-to-string translation, which represents a large number of potential parses as a packed forest, allowing the translator to choose between these parses during the process of translation (Mi et al., 2008). 2.2 The State of Open Source Software There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while</context>
</contexts>
<marker>Xiao, Zhu, Zhang, Li, 2012</marker>
<rawString>Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012. Niutrans: An open source toolkit for phrasebased and syntax-based machine translation. In Proceedings of the ACL 2012 System Demonstrations, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1858" citStr="Yamada and Knight, 2001" startWordPosition="271" endWordPosition="274"> at http://phontron.com/travatar 1 Introduction One of the recent trends in statistical machine translation (SMT) is the popularity of models that use syntactic information to help solve problems of long-distance reordering between the source and target language text. These techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the target order before translating (Xia and McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Yata</author>
</authors>
<title>Dictionary compression using nested prefix/Patricia tries (in Japanese).</title>
<date>2012</date>
<booktitle>In Proc. 17th NLP.</booktitle>
<contexts>
<context position="10310" citStr="Yata, 2012" startWordPosition="1590" endWordPosition="1591">counts. Rule scores are stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.l Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). 3.4 Tuning and Evaluation For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tunin</context>
</contexts>
<marker>Yata, 2012</marker>
<rawString>Susumu Yata. 2012. Dictionary compression using nested prefix/Patricia tries (in Japanese). In Proc. 17th NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. WMT.</booktitle>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. WMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>