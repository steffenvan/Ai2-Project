<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004862">
<note confidence="0.983904666666667">
Proceeings of the Seventh CoNLL conference
held at HLT-NAACL 2003 , pp. 49-55
Edmonton, May-June 2003
</note>
<title confidence="0.995393">
Bootstrapping POS taggers using Unlabelled Data
</title>
<author confidence="0.999834">
Stephen Clark, James R. Curran and Miles Osborne
</author>
<affiliation confidence="0.946364666666667">
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh. EH8 9LW
</affiliation>
<email confidence="0.997996">
{stephenc,jamesc,osborne}@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975789473684">
This paper investigates booststrapping part-of-
speech taggers using co-training, in which two
taggers are iteratively re-trained on each other’s
output. Since the output of the taggers is noisy,
there is a question of which newly labelled ex-
amples to add to the training set. We investi-
gate selecting examples by directly maximising
tagger agreement on unlabelled data, a method
which has been theoretically and empirically
motivated in the co-training literature. Our
results show that agreement-based co-training
can significantly improve tagging performance
for small seed datasets. Further results show
that this form of co-training considerably out-
performs self-training. However, we find that
simply re-training on all the newly labelled data
can, in some cases, yield comparable results to
agreement-based co-training, with only a frac-
tion of the computational cost.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996641875">
Co-training (Blum and Mitchell, 1998), and several vari-
ants of co-training, have been applied to a number of
NLP problems, including word sense disambiguation
(Yarowsky, 1995), named entity recognition (Collins
and Singer, 1999), noun phrase bracketing (Pierce and
Cardie, 2001) and statistical parsing (Sarkar, 2001;
Steedman et al., 2003). In each case, co-training was
used successfully to bootstrap a model from only a small
amount of labelled data and a much larger pool of un-
labelled data. Previous co-training approaches have typ-
ically used the score assigned by the model as an indi-
cator of the reliability of a newly labelled example. In
this paper we take a different approach, based on theoret-
ical work by Dasgupta et al. (2002) and Abney (2002), in
which newly labelled training examples are selected us-
ing a greedy algorithm which explicitly maximises the
POS taggers’ agreement on unlabelled data.
We investigate whether co-training based upon di-
rectly maximising agreement can be successfully ap-
plied to a pair of part-of-speech (POS) taggers: the
Markov model TNT tagger (Brants, 2000) and the max-
imum entropy C&amp;C tagger (Curran and Clark, 2003).
There has been some previous work on boostrap-
ping POS taggers (e.g., Zavrel and Daelemans (2000) and
Cucerzan and Yarowsky (2002)), but to our knowledge
no previous work on co-training POS taggers.
The idea behind co-training the POS taggers is very
simple: use output from the TNT tagger as additional
labelled data for the maximum entropy tagger, and vice
versa, in the hope that one tagger can learn useful infor-
mation from the output of the other. Since the output of
both taggers is noisy, there is a question of which newly
labelled examples to add to the training set. The addi-
tional data should be accurate, but also useful, providing
the tagger with new information. Our work differs from
the Blum and Mitchell (1998) formulation of co-training
by using two different learning algorithms rather than two
independent feature sets (Goldman and Zhou, 2000).
Our results show that, when using very small amounts
of manually labelled seed data and a much larger amount
of unlabelled material, agreement-based co-training can
significantly improve POS tagger accuracy. We also show
that simply re-training on all of the newly labelled data
is surprisingly effective, with performance depending on
the amount of newly labelled data added at each itera-
tion. For certain sizes of newly labelled data, this sim-
ple approach is just as effective as the agreement-based
method. We also show that co-training can still benefit
both taggers when the performance of one tagger is ini-
tially much better than the other.
We have also investigated whether co-training can im-
prove the taggers already trained on large amounts of
manually annotated data. Using standard sections of the
WSJ Penn Treebank as seed data, we have been unable
to improve the performance of the taggers using self-
training or co-training.
Manually tagged data for English exists in large quan-
tities, which means that there is no need to create taggers
from small amounts of labelled material. However, our
experiments are relevant for languages for which there
is little or no annotated data. We only perform the ex-
periments in English for convenience. Our experiments
can also be seen as a vehicle for exploring aspects of co-
training.
</bodyText>
<sectionHeader confidence="0.826887" genericHeader="introduction">
2 Co-training
</sectionHeader>
<bodyText confidence="0.999440333333333">
Given two (or more) “views” (as described in
Blum and Mitchell (1998)) of a classification task,
co-training can be informally described as follows:
</bodyText>
<listItem confidence="0.999580428571428">
• Learn separate classifiers for each view using a
small amount of labelled seed data.
• Use each classifier to label some previously unla-
belled data.
• For each classifier, add some subset of the newly la-
belled data to the training data.
• Retrain the classifiers and repeat.
</listItem>
<bodyText confidence="0.99995796875">
The intuition behind the algorithm is that each classi-
fier is providing extra, informative labelled data for the
other classifier(s). Blum and Mitchell (1998) derive PAC-
like guarantees on learning by assuming that the two
views are individually sufficient for classification and the
two views are conditionally independent given the class.
Collins and Singer (1999) present a variant of the
Blum and Mitchell algorithm, which directly maximises
an objective function that is based on the level of
agreement between the classifiers on unlabelled data.
Dasgupta et al. (2002) provide a theoretical basis for this
approach by providing a PAC-like analysis, using the
same independence assumption adopted by Blum and
Mitchell. They prove that the two classifiers have low
generalisation error if they agree on unlabelled data.
Abney (2002) argues that the Blum and Mitchell in-
dependence assumption is very restrictive and typically
violated in the data, and so proposes a weaker indepen-
dence assumption, for which the Dasgupta et al. (2002)
results still hold. Abney also presents a greedy algorithm
that maximises agreement on unlabelled data, which pro-
duces comparable results to Collins and Singer (1999) on
their named entity classification task.
Goldman and Zhou (2000) show that, if the newly la-
belled examples used for re-training are selected care-
fully, co-training can still be successful even when the
views used by the classifiers do not satisfy the indepen-
dence assumption.
In remainder of the paper we present a practical
method for co-training POS taggers, and investigate the
extent to which example selection based on the work of
Dasgupta et al. and Abney can be effective.
</bodyText>
<sectionHeader confidence="0.990103" genericHeader="method">
3 The POS taggers
</sectionHeader>
<bodyText confidence="0.999934782608696">
The two POS taggers used in the experiments are TNT, a
publicly available Markov model tagger (Brants, 2000),
and a reimplementation of the maximum entropy (ME)
tagger MXPOST (Ratnaparkhi, 1996). The ME tagger,
which we refer to as C&amp;C, uses the same features as MX-
POST, but is much faster for training and tagging (Cur-
ran and Clark, 2003). Fast training and tagging times
are important for the experiments performed here, since
the bootstrapping process can require many tagging and
training iterations.
The model used by TNT is a standard tagging Markov
model, consisting of emission probabilities, and transi-
tion probabilities based on trigrams of tags. It also deals
with unknown words using a suffix analysis of the target
word (the word to be tagged). TNT is very fast for both
training and tagging.
The C&amp;C tagger differs in a number of ways from
TNT. First, it uses a conditional model of a tag sequence
given a string, rather than a joint model. Second, ME
models are used to define the conditional probabilities of
a tag given some context. The advantage of ME mod-
els over the Markov model used by TNT is that arbitrary
features can easily be included in the context; so as well
as considering the target word and the previous two tags
(which is the information TNT uses), the ME models also
consider the words either side of the target word and, for
unknown and infrequent words, various properties of the
string of the target word.
A disadvantage is that the training times for ME mod-
els are usually relatively slow, especially with iterative
scaling methods (see Malouf (2002) for alternative meth-
ods). Here we use Generalised Iterative Scaling (Dar-
roch and Ratcliff, 1972), but our implementation is much
faster than Ratnaparkhi’s publicly available tagger. The
C&amp;C tagger trains in less than 7 minutes on the 1 million
words of the Penn Treebank, and tags slightly faster than
TNT.
Since the taggers share many common features, one
might think they are not different enough for effective
co-training to be possible. In fact, both taggers are suffi-
ciently different for co-training to be effective. Section 4
shows that both taggers can benefit significantly from the
information contained in the other’s output.
The performance of the taggers on section 00 of the
WSJ Penn Treebank is given in Table 1, for different seed
set sizes (number of sentences). The seed data is taken
</bodyText>
<table confidence="0.992494666666667">
Tagger 50 seed 500 seed ;z� 40,000 seed
TNT 81.3 91.0 96.5
C&amp;C 73.2 88.3 96.8
</table>
<tableCaption confidence="0.99995">
Table 1: Tagger performance for different seed sets
</tableCaption>
<bodyText confidence="0.975312">
from sections 2–21 of the Treebank. The table shows that
the performance of TNT is significantly better than the
performance of C&amp;C when the size of the seed data is
very small.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999935475">
The co-training framework uses labelled examples from
one tagger as additional training data for the other. For
the purposes of this paper, a labelled example is a tagged
sentence. We chose complete sentences, rather than
smaller units, because this simplifies the experiments and
the publicly available version of TNT requires complete
tagged sentences for training. It is possible that co-
training with sub-sentential units might be more effective,
but we leave this as future work.
The co-training process is given in Figure 1. At
each stage in the process there is a cache of unla-
belled sentences (selected from the total pool of un-
labelled sentences) which is labelled by each tagger.
The cache size could be increased at each iteration,
which is a common practice in the co-training litera-
ture. A subset of those sentences labelled by TNT is
then added to the training data for C&amp;C, and vice versa.
Blum and Mitchell (1998) use the combined set of newly
labelled examples for training each view, but we fol-
low Goldman and Zhou (2000) in using separate labelled
sets. In the remainder of this section we consider two pos-
sible methods for selecting a subset. The cache is cleared
after each iteration.
There are various ways to select the labelled examples
for each tagger. A typical approach is to select those ex-
amples assigned a high score by the relevant classifier,
under the assumption that these examples will be the most
reliable. A score-based selection method is difficult to
apply in our experiments, however, since TNT does not
provide scores for tagged sentences.
We therefore tried two alternative selection methods.
The first is to simply add all of the cache labelled by one
tagger to the training data of the other. We refer to this
method as naive co-training. The second, more sophisti-
cated, method is to select that subset of the labelled cache
which maximises the agreement of the two taggers on un-
labelled data. We call this method agreement-based co-
training. For a large cache the number ofpossible subsets
makes exhaustive search intractable, and so we randomly
sample the subsets.
</bodyText>
<figure confidence="0.994734470588235">
S is a seed set of labelled sentences
LT is labelled training data for TNT
LC is labelled training data for C&amp;C
U is a large set of unlabelled sentences
C is a cache holding a small subset of U
initialise:
LT - LC - S
Train TNT and C&amp;C on S
loop:
Partition U into the disjoint sets C and U&apos;.
Label C with TNT and C&amp;C
Select sentences labelled by TNT and add to LC
Train C&amp;C on LC
Select sentences labelled by C&amp;C and add to LT
Train TNT on LT
U = U&apos;.
Until U is empty
</figure>
<figureCaption confidence="0.999477">
Figure 1: The general co-training process
</figureCaption>
<figure confidence="0.9904486">
C is a cache of sentences labelled by the other tagger
U is a set of sentences, used for measuring agreement
initialise:
cmax - 0; Amax - 0
Repeat n times:
Randomly sample c c C
Retrain current tagger using c as additional data
if new agreement rate, A, on U &gt; Amax
Amax - A; cmax - c
return cmax
</figure>
<figureCaption confidence="0.999981">
Figure 2: Agreement-based example selection
</figureCaption>
<bodyText confidence="0.999984172413793">
The pseudo-code for the agreement-based selection
method is given in Figure 2. The current tagger is the
one being retrained, while the other tagger is kept static.
The co-training process uses the selection method for se-
lecting sentences from the cache (which has been labelled
by one of the taggers). Note that during the selection pro-
cess, we repeatedly sample from all possible subsets of
the cache; this is done by first randomly choosing the
size of the subset and then randomly choosing sentences
based on the size. The number of subsets we consider is
determined by the number of times the loop is traversed
in Figure 2.
If TNT is being trained on the output of C&amp;C, then the
most recent version of C&amp;C is used to measure agreement
(and vice versa); so we first attempt to improve one tag-
ger, then the other, rather than both at the same time. The
agreement rate of the taggers on unlabelled sentences is
the per-token agreement rate; that is, the number of times
each word in the unlabelled set of sentences is assigned
the same tag by both taggers.
For the small seed set experiments, the seed data was
an arbitrarily chosen subset of sections 10–19 of the
WSJ Penn Treebank; the unlabelled training data was
taken from 50, 000 sentences of the 1994 WSJ section
of the North American News Corpus (NANC); and the
unlabelled data used to measure agreement was around
10, 000 sentences from sections 1–5 of the Treebank.
Section 00 of the Treebank was used to measure the ac-
curacy of the taggers. The cache size was 500 sentences.
</bodyText>
<subsectionHeader confidence="0.4958495">
4.1 Self-Training and Agreement-based Co-training
Results
</subsectionHeader>
<bodyText confidence="0.928087375">
Figure 3 shows the results for self-training, in which each
tagger is simply retrained on its own labelled cache at
each round. (By round we mean the re-training of a sin-
gle tagger, so there are two rounds per co-training itera-
tion.) TNT does improve using self-training, from 81.4%
to 82.2%, but C&amp;C is unaffected. Re-running these ex-
periments using a range of unlabelled training sets, from
a variety of sources, showed similar behaviour.
</bodyText>
<figure confidence="0.998909875">
Accuracy
0.83
0.82
0.81
0.79
0.78
0.77
0.76
0.75
0.74
0.73
0.8
TnT
C&amp;C
0 5 10 15 20 25 30 35 40 45 50
Number of rounds
</figure>
<figureCaption confidence="0.998927565217391">
Figure 3: Self-training TNT and C&amp;C (50 seed sen-
tences). The upper curve is for TNT; the lower curve is
for C&amp;C.
Figure 4 gives the results for the greedy agreement co-
training, using a cache size of 500 and searching through
100 subsets of the labelled cache to find the one that max-
imises agreement. Co-training improves the performance
of both taggers: TNT improves from 81.4% to 84.9%,
and C&amp;C improves from 73.2% to 84.3% (an error re-
duction of over 40%).
Figures 5 and 6 show the self-training results and
agreement-based results when a larger seed set, of 500
sentences, is used for each tagger. In this case, self-
training harms TNT and C&amp;C is again unaffected. Co-
training continues to be beneficial.
Figure 7 shows how the size of the labelled data set (the
number of sentences) grows for each tagger per round.
Figure 4: Agreement-based co-training between
TNT and C&amp;C (50 seed sentences). The curve that
starts at a higher value is for TNT.
Figure 5: Self-training TNT and C&amp;C (500 seed sen-
tences). The upper curve is for TNT; the lower curve is
for C&amp;C.
</figureCaption>
<bodyText confidence="0.998885142857143">
Towards the end of the co-training run, more material is
being selected for C&amp;C than TNT. The experiments us-
ing a seed set size of 50 showed a similar trend, but the
difference between the two taggers was less marked. By
examining the subsets chosen from the labelled cache at
each round, we also observed that a large proportion of
the cache was being selected for both taggers.
</bodyText>
<subsectionHeader confidence="0.950069">
4.2 Naive Co-training Results
</subsectionHeader>
<bodyText confidence="0.995689285714286">
Agreement-based co-training for POS taggers is effective
but computationally demanding. The previous two agree-
ment maximisation experiments involved retraining each
tagger 2, 500 times. Given this, and the observation that
maximisation generally has a preference for selecting a
large proportion of the labelled cache, we looked at naive
co-training: simply retraining upon all available mate-
</bodyText>
<figure confidence="0.999177">
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
TnT
C&amp;C
0 5 10 15 20 25 30 35 40 45 50
Number of rounds
Accuracy
Accuracy
0.915
0.905
0.895
0.885
0.91
0.89
0.88
0.9
TnT
C&amp;C
0 5 10 15 20 25 30 35 40 45 50
Number of rounds
0 5 10 15 20 25 30 35 40 45 50
Number of rounds
</figure>
<figureCaption confidence="0.996144666666667">
Figure 6: Agreement-based co-training between
TNT and C&amp;C (500 seed sentences). The curve that
starts at a higher value is for TNT.
</figureCaption>
<figure confidence="0.970756">
0 5 10 15 20 25 30 35 40 45 50
</figure>
<figureCaption confidence="0.992589">
Figure 7: Growth in training-set sizes for co-training
</figureCaption>
<bodyText confidence="0.982313277777778">
TNT and C&amp;C (500 seed sentences). The upper curve
is for C&amp;C.
rial (i.e. the whole cache) at each round. Table 2 shows
the naive co-training results after 50 rounds of co-training
when varying the size of the cache. 50 manually labelled
sentences were used as the seed material. Table 3 shows
results for the same experiment, but this time with a seed
set of 500 manually labelled sentences.
We see that naive co-training improves as the cache
size increases. For a large cache, the performance lev-
els for naive co-training are very similar to those pro-
duced by our agreement-based co-training method. Af-
ter 50 rounds of co-training using 50 seed sentences,
the agreement rates for naive and agreement-based co-
training were very similar: from an initial value of 73%
to 97% agreement.
Naive co-training is more efficient than agreement-
based co-training. For the parameter settings used in
</bodyText>
<table confidence="0.906293">
Amount added TNT C&amp;C
0 81.3 73.2
50 82.9 82.7
100 83.5 83.3
150 84.4 84.3
300 85.0 84.9
500 85.3 85.1
</table>
<tableCaption confidence="0.852427">
Table 2: Naive co-training accuracy results when varying
the amount added after each round (50 seed sentences)
</tableCaption>
<table confidence="0.8853405">
Amount added TNT C&amp;C
0 91.0 88.3
100 92.0 91.9
300 92.0 91.9
500 92.1 92.0
1000 92.0 91.9
</table>
<tableCaption confidence="0.761992">
Table 3: Naive co-training accuracy results when varying
the amount added after each round (500 seed sentences)
</tableCaption>
<bodyText confidence="0.9998656875">
the previous experiments, agreement-based co-training
required the taggers to be re-trained 10 to 100 times
more often then naive co-training. There are advan-
tages to agreement-based co-training, however. First,
the agreement-based method dynamically selects the best
sample at each stage, which may not be the whole cache.
In particular, when the agreement rate cannot be im-
proved upon, the selected sample can be rejected. For
naive co-training, new samples will always be added,
and so there is a possibility that the noise accumulated
at later stages will start to degrade performance (see
Pierce and Cardie (2001)). Second, for naive co-training,
the optimal amount of data to be added at each round (i.e.
the cache size) is a parameter that needs to be determined
on held out data, whereas the agreement-based method
determines this automatically.
</bodyText>
<subsectionHeader confidence="0.998985">
4.3 Larger-Scale Experiments
</subsectionHeader>
<bodyText confidence="0.9998545">
We also performed a number of experiments using much
more unlabelled training material than before. Instead
of using 50, 000 sentences from the 1994 WSJ section of
the North American News Corpus, we used 417, 000 sen-
tences (from the same section) and ran the experiments
until the unlabelled data had been exhausted.
One experiment used naive co-training, with 50 seed
sentences and a cache of size 500. This led to an agree-
ment rate of 99%, with performance levels of 85.4% and
85.4% for TNT and C&amp;C respectively. 230, 000 sen-
tences (;z:� 5 million words) had been processed and were
used as training material by the taggers. The other ex-
periment used our agreement-based co-training approach
(50 seed sentences, cache size of 1, 000 sentences, explor-
</bodyText>
<figure confidence="0.998584904761905">
TnT
C&amp;C
12000
10000
8000
6000
4000
2000
0
C&amp;C
TnT
tnt
Accuracy 0.92
0.915
0.91
0.905
0.9
0.895
0.89
0.885
0.88
</figure>
<bodyText confidence="0.999684111111111">
ing at most 10 subsets in the maximisation process per
round). The agreement rate was 98%, with performance
levels of 86.0% and 85.9% for both taggers. 124, 000
sentences had been processed, of which 30, 000 labelled
sentences were selected for training TNT and 44, 000 la-
belled sentences were selected for training C&amp;C.
Co-training using this much larger amount of unla-
belled material did improve our previously mentioned re-
sults, but not by a large margin.
</bodyText>
<subsectionHeader confidence="0.999865">
4.4 Co-training using Imbalanced Views
</subsectionHeader>
<bodyText confidence="0.99448825">
It is interesting to consider what happens when one view
is initially much more accurate than the other view. We
trained one of the taggers on much more labelled seed
data than the other, to see how this affects the co-training
process. Both taggers were initialised with either 500 or
50 seed sentences, and agreement-based co-training was
applied, using a cache size of 500 sentences. The results
are shown in Table 4.
</bodyText>
<table confidence="0.997935">
Seed material Initial Perf Final Perf
TNT C&amp;C TNT C&amp;C TNT C&amp;C
50 500 81.3 88.3 90.0 89.4
500 50 91.0 73.2 91.3 91.3
</table>
<tableCaption confidence="0.999812">
Table 4: Co-training Results for Imbalanced Views
</tableCaption>
<bodyText confidence="0.9997835">
Co-training continues to be effective, even when the
two taggers are imbalanced. Also, the final performance
of the taggers is around the same value, irrespective of
the direction of the imbalance.
</bodyText>
<subsectionHeader confidence="0.988566">
4.5 Large Seed Experiments
</subsectionHeader>
<bodyText confidence="0.999534857142857">
Although bootstrapping from unlabelled data is particu-
larly valuable when only small amounts of training ma-
terial are available, it is also interesting to see if self-
training or co-training can improve state of the art POS
taggers.
For these experiments, both C&amp;C and TNT were ini-
tially trained on sections 00–18 of the WSJ Penn Tree-
bank, and sections 19–21 and 22–24 were used as the
development and test sets. The 1994–1996 WSJ text
from the NANC was used as unlabelled material to fill the
cache.
The cache size started out at 8000 sentences and in-
creased by 10% in each round to match the increasing
labelled training data. In each round of self-training or
naive co-training 10% of the cache was randomly se-
lected and added to the labelled training data. The ex-
periments ran for 40 rounds.
The performance of the different training regimes is
listed in Table 5. These results show no significant im-
provement using either self-training or co-training with
very large seed datasets. Self-training shows only a slight
</bodyText>
<table confidence="0.9993332">
Method WSJ19–21 WSJ22–24
C&amp;C TNT C&amp;C TNT
Initial 96.71 96.50 96.78 96.46
Self-train 96.77 96.45 96.87 96.42
Naive co-train 96.74 96.48 96.76 96.46
</table>
<tableCaption confidence="0.99993">
Table 5: Performance with large seed sets
</tableCaption>
<bodyText confidence="0.8443845">
improvement for C&amp;C1 while naive co-training perfor-
mance is always worse.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996918918919">
We have shown that co-training is an effective technique
for bootstrapping POS taggers trained on small amounts
of labelled data. Using unlabelled data, we are able to
improve TNT from 81.3% to 86.0%, whilst C&amp;C shows
a much more dramatic improvement of 73.2% to 85.9%.
Our agreement-based co-training results support
the theoretical arguments of Abney (2002) and
Dasgupta et al. (2002), that directly maximising the
agreement rates between the two taggers reduces gen-
eralisation error. Examination of the selected subsets
showed a preference for a large proportion of the cache.
This led us to propose a naive co-training approach,
which significantly reduced the computational cost
without a significant performance penalty.
We also showed that naive co-training was unable to
improve the performance of the taggers when they had
already been trained on large amounts of manually anno-
tated data. It is possible that agreement-based co-training,
using more careful selection, would result in an improve-
ment. We leave these experiments to future work, but
note that there is a large computational cost associated
with such experiments.
The performance of the bootstrapped taggers is still
a long way behind a tagger trained on a large amount
of manually annotated data. This finding is in accord
with earlier work on bootstrapping taggers using EM (El-
worthy, 1994; Merialdo, 1994). An interesting question
would be to determine the minimum number of manually
labelled examples that need to be used to seed the sys-
tem before we can achieve comparable results as using
all available manually labelled sentences.
For our experiments, co-training never led to a de-
crease in performance, regardless of the number of itera-
tions. The opposite behaviour has been observed in other
applications of co-training (Pierce and Cardie, 2001).
Whether this robustness is a property of the tagging prob-
lem or our approach is left for future work.
</bodyText>
<footnote confidence="0.948144">
1This is probably by chance selection of better subsets.
</footnote>
<sectionHeader confidence="0.987289" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999968222222222">
This work has grown out of many fruitful discus-
sions with the 2002 JHU Summer Workshop team that
worked on weakly supervised bootstrapping of statistical
parsers. The first author was supported by EPSRC grant
GR/M96889, and the second author by a Commonwealth
scholarship and a Sydney University Travelling scholar-
ship. We would like to thank the anonymous reviewers
for their helpful comments, and also Iain Rae for com-
puter support.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.89947477027027">
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Associationfor Compu-
tational Linguistics, pages 360–367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92–100, Madisson, WI.
Thorsten Brants. 2000. TnT - a statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, pages 224–231.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Empirical Methods in NLP Conference, pages
100–110, University of Maryland, MD.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th Workshop on
Computational Language Learning, Taipei, Taiwan.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and Smoothing for Maximum Entropy Taggers.
In Proceedings of the 11th Annual Meeting of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Budapest, Hungary. (to appear).
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. The Annals ofMath-
ematical Statistics, 43(5):1470–1480.
Sanjoy Dasgupta, Michael Littman, and David
McAllester. 2002. PAC generalization bounds
for co-training. In T. G. Dietterich, S. Becker,
and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14, pages 375–382,
Cambridge, MA. MIT Press.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of the 4th Conference
on Applied Natural Language Processing, pages 53–
58, Stuttgart, Germany.
Sally Goldman and Yan Zhou. 2000. Enhancing super-
vised learning with unlabeled data. In Proceedings of
the 17th International Conference on Machine Learn-
ing, Stanford, CA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Language
Learning, pages 49–55, Taipei, Taiwan.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the Empirical Methods in
NLP Conference, Pittsburgh, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP Con-
ference, pages 133–142, Philadelphia, PA.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of the 2nd Annual
Meeting of the NAACL, pages 95–102, Pittsburgh, PA.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the 11th Annual Meeting of the European
Chapter of the Association for Computational Linguis-
tics, Budapest, Hungary. (to appear).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189–196, Cam-
bridge, MA.
Jakub Zavrel and Walter Daelemans. 2000. Bootstrap-
ping a tagged corpus through combination of exist-
ing heterogeneous taggers. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation, pages 17–20, Athens, Greece.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335092">
<note confidence="0.962313">Proceeings of the Seventh CoNLL conference held at HLT-NAACL 2003 , pp. 49-55 Edmonton, May-June 2003</note>
<title confidence="0.999108">using Unlabelled Data</title>
<author confidence="0.999439">Stephen Clark</author>
<author confidence="0.999439">James R Curran</author>
<author confidence="0.999439">Miles Osborne</author>
<affiliation confidence="0.9967585">School of University of</affiliation>
<address confidence="0.394275">2 Buccleuch Place, Edinburgh. EH8</address>
<abstract confidence="0.9976097">This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>360--367</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1978" citStr="Abney (2002)" startWordPosition="295" endWordPosition="296">disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&amp;C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS tagg</context>
<context position="5859" citStr="Abney (2002)" startWordPosition="922" endWordPosition="923">iews are individually sufficient for classification and the two views are conditionally independent given the class. Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data. Dasgupta et al. (2002) provide a theoretical basis for this approach by providing a PAC-like analysis, using the same independence assumption adopted by Blum and Mitchell. They prove that the two classifiers have low generalisation error if they agree on unlabelled data. Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold. Abney also presents a greedy algorithm that maximises agreement on unlabelled data, which produces comparable results to Collins and Singer (1999) on their named entity classification task. Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifie</context>
<context position="22918" citStr="Abney (2002)" startWordPosition="3859" endWordPosition="3860">4 C&amp;C TNT C&amp;C TNT Initial 96.71 96.50 96.78 96.46 Self-train 96.77 96.45 96.87 96.42 Naive co-train 96.74 96.48 96.76 96.46 Table 5: Performance with large seed sets improvement for C&amp;C1 while naive co-training performance is always worse. 5 Conclusion We have shown that co-training is an effective technique for bootstrapping POS taggers trained on small amounts of labelled data. Using unlabelled data, we are able to improve TNT from 81.3% to 86.0%, whilst C&amp;C shows a much more dramatic improvement of 73.2% to 85.9%. Our agreement-based co-training results support the theoretical arguments of Abney (2002) and Dasgupta et al. (2002), that directly maximising the agreement rates between the two taggers reduces generalisation error. Examination of the selected subsets showed a preference for a large proportion of the cache. This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty. We also showed that naive co-training was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data. It is possible that agreement-based co-training, using more c</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steven Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 360–367, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<location>Madisson, WI.</location>
<contexts>
<context position="1259" citStr="Blum and Mitchell, 1998" startWordPosition="175" endWordPosition="178">xamples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled ex</context>
<context position="3111" citStr="Blum and Mitchell (1998)" startWordPosition="481" endWordPosition="484">ucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs from the Blum and Mitchell (1998) formulation of co-training by using two different learning algorithms rather than two independent feature sets (Goldman and Zhou, 2000). Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy. We also show that simply re-training on all of the newly labelled data is surprisingly effective, with performance depending on the amount of newly labelled data added at each iteration. For certain sizes of newly labelled data, this simple approach is j</context>
<context position="4667" citStr="Blum and Mitchell (1998)" startWordPosition="732" endWordPosition="735">ections of the WSJ Penn Treebank as seed data, we have been unable to improve the performance of the taggers using selftraining or co-training. Manually tagged data for English exists in large quantities, which means that there is no need to create taggers from small amounts of labelled material. However, our experiments are relevant for languages for which there is little or no annotated data. We only perform the experiments in English for convenience. Our experiments can also be seen as a vehicle for exploring aspects of cotraining. 2 Co-training Given two (or more) “views” (as described in Blum and Mitchell (1998)) of a classification task, co-training can be informally described as follows: • Learn separate classifiers for each view using a small amount of labelled seed data. • Use each classifier to label some previously unlabelled data. • For each classifier, add some subset of the newly labelled data to the training data. • Retrain the classifiers and repeat. The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s). Blum and Mitchell (1998) derive PAClike guarantees on learning by assuming that the two views are individuall</context>
<context position="10371" citStr="Blum and Mitchell (1998)" startWordPosition="1680" endWordPosition="1683">version of TNT requires complete tagged sentences for training. It is possible that cotraining with sub-sentential units might be more effective, but we leave this as future work. The co-training process is given in Figure 1. At each stage in the process there is a cache of unlabelled sentences (selected from the total pool of unlabelled sentences) which is labelled by each tagger. The cache size could be increased at each iteration, which is a common practice in the co-training literature. A subset of those sentences labelled by TNT is then added to the training data for C&amp;C, and vice versa. Blum and Mitchell (1998) use the combined set of newly labelled examples for training each view, but we follow Goldman and Zhou (2000) in using separate labelled sets. In the remainder of this section we consider two possible methods for selecting a subset. The cache is cleared after each iteration. There are various ways to select the labelled examples for each tagger. A typical approach is to select those examples assigned a high score by the relevant classifier, under the assumption that these examples will be the most reliable. A score-based selection method is difficult to apply in our experiments, however, sinc</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92–100, Madisson, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="2323" citStr="Brants, 2000" startWordPosition="347" endWordPosition="348">d data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&amp;C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled</context>
<context position="6836" citStr="Brants, 2000" startWordPosition="1082" endWordPosition="1083">999) on their named entity classification task. Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption. In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective. 3 The POS taggers The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagger MXPOST (Ratnaparkhi, 1996). The ME tagger, which we refer to as C&amp;C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003). Fast training and tagging times are important for the experiments performed here, since the bootstrapping process can require many tagging and training iterations. The model used by TNT is a standard tagging Markov model, consisting of emission probabilities, and transition probabilities based on trigrams of tags. It also deals with unknown words using a suffix ana</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - a statistical part-of-speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Empirical Methods in NLP Conference,</booktitle>
<pages>100--110</pages>
<institution>University of Maryland, MD.</institution>
<contexts>
<context position="1450" citStr="Collins and Singer, 1999" startWordPosition="203" endWordPosition="206">ment-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy </context>
<context position="5389" citStr="Collins and Singer (1999)" startWordPosition="847" endWordPosition="850">classifiers for each view using a small amount of labelled seed data. • Use each classifier to label some previously unlabelled data. • For each classifier, add some subset of the newly labelled data to the training data. • Retrain the classifiers and repeat. The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s). Blum and Mitchell (1998) derive PAClike guarantees on learning by assuming that the two views are individually sufficient for classification and the two views are conditionally independent given the class. Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data. Dasgupta et al. (2002) provide a theoretical basis for this approach by providing a PAC-like analysis, using the same independence assumption adopted by Blum and Mitchell. They prove that the two classifiers have low generalisation error if they agree on unlabelled data. Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Empirical Methods in NLP Conference, pages 100–110, University of Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Bootstrapping a multilingual part-of-speech tagger in one person-day.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Workshop on Computational Language Learning,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="2514" citStr="Cucerzan and Yarowsky (2002)" startWordPosition="377" endWordPosition="380">e a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&amp;C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs from the Blum and Mitchell (1998) fo</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2002. Bootstrapping a multilingual part-of-speech tagger in one person-day. In Proceedings of the 6th Workshop on Computational Language Learning, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and Smoothing for Maximum Entropy Taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Budapest, Hungary.</location>
<note>(to appear).</note>
<contexts>
<context position="2383" citStr="Curran and Clark, 2003" startWordPosition="356" endWordPosition="359">ally used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&amp;C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data sh</context>
<context position="7067" citStr="Curran and Clark, 2003" startWordPosition="1120" endWordPosition="1124">y the classifiers do not satisfy the independence assumption. In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective. 3 The POS taggers The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagger MXPOST (Ratnaparkhi, 1996). The ME tagger, which we refer to as C&amp;C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003). Fast training and tagging times are important for the experiments performed here, since the bootstrapping process can require many tagging and training iterations. The model used by TNT is a standard tagging Markov model, consisting of emission probabilities, and transition probabilities based on trigrams of tags. It also deals with unknown words using a suffix analysis of the target word (the word to be tagged). TNT is very fast for both training and tagging. The C&amp;C tagger differs in a number of ways from TNT. First, it uses a conditional model of a tag sequence given a string, rather than</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and Smoothing for Maximum Entropy Taggers. In Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics, Budapest, Hungary. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models. The Annals ofMathematical</title>
<date>1972</date>
<journal>Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<contexts>
<context position="8414" citStr="Darroch and Ratcliff, 1972" startWordPosition="1349" endWordPosition="1353">ntage of ME models over the Markov model used by TNT is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information TNT uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word. A disadvantage is that the training times for ME models are usually relatively slow, especially with iterative scaling methods (see Malouf (2002) for alternative methods). Here we use Generalised Iterative Scaling (Darroch and Ratcliff, 1972), but our implementation is much faster than Ratnaparkhi’s publicly available tagger. The C&amp;C tagger trains in less than 7 minutes on the 1 million words of the Penn Treebank, and tags slightly faster than TNT. Since the taggers share many common features, one might think they are not different enough for effective co-training to be possible. In fact, both taggers are sufficiently different for co-training to be effective. Section 4 shows that both taggers can benefit significantly from the information contained in the other’s output. The performance of the taggers on section 00 of the WSJ Pen</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals ofMathematical Statistics, 43(5):1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC generalization bounds for co-training. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>375--382</pages>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1961" citStr="Dasgupta et al. (2002)" startWordPosition="290" endWordPosition="293">lems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&amp;C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-</context>
<context position="5597" citStr="Dasgupta et al. (2002)" startWordPosition="880" endWordPosition="883">ning data. • Retrain the classifiers and repeat. The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s). Blum and Mitchell (1998) derive PAClike guarantees on learning by assuming that the two views are individually sufficient for classification and the two views are conditionally independent given the class. Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data. Dasgupta et al. (2002) provide a theoretical basis for this approach by providing a PAC-like analysis, using the same independence assumption adopted by Blum and Mitchell. They prove that the two classifiers have low generalisation error if they agree on unlabelled data. Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold. Abney also presents a greedy algorithm that maximises agreement on unlabelled data, which produces comparable result</context>
<context position="22945" citStr="Dasgupta et al. (2002)" startWordPosition="3862" endWordPosition="3865"> Initial 96.71 96.50 96.78 96.46 Self-train 96.77 96.45 96.87 96.42 Naive co-train 96.74 96.48 96.76 96.46 Table 5: Performance with large seed sets improvement for C&amp;C1 while naive co-training performance is always worse. 5 Conclusion We have shown that co-training is an effective technique for bootstrapping POS taggers trained on small amounts of labelled data. Using unlabelled data, we are able to improve TNT from 81.3% to 86.0%, whilst C&amp;C shows a much more dramatic improvement of 73.2% to 85.9%. Our agreement-based co-training results support the theoretical arguments of Abney (2002) and Dasgupta et al. (2002), that directly maximising the agreement rates between the two taggers reduces generalisation error. Examination of the selected subsets showed a preference for a large proportion of the cache. This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty. We also showed that naive co-training was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data. It is possible that agreement-based co-training, using more careful selection, would res</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2002</marker>
<rawString>Sanjoy Dasgupta, Michael Littman, and David McAllester. 2002. PAC generalization bounds for co-training. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 375–382, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied Natural Language Processing,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="23922" citStr="Elworthy, 1994" startWordPosition="4016" endWordPosition="4018">ive co-training was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data. It is possible that agreement-based co-training, using more careful selection, would result in an improvement. We leave these experiments to future work, but note that there is a large computational cost associated with such experiments. The performance of the bootstrapped taggers is still a long way behind a tagger trained on a large amount of manually annotated data. This finding is in accord with earlier work on bootstrapping taggers using EM (Elworthy, 1994; Merialdo, 1994). An interesting question would be to determine the minimum number of manually labelled examples that need to be used to seed the system before we can achieve comparable results as using all available manually labelled sentences. For our experiments, co-training never led to a decrease in performance, regardless of the number of iterations. The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). Whether this robustness is a property of the tagging problem or our approach is left for future work. 1This is probably by chance selec</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proceedings of the 4th Conference on Applied Natural Language Processing, pages 53– 58, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Goldman</author>
<author>Yan Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="3247" citStr="Goldman and Zhou, 2000" startWordPosition="500" endWordPosition="503">rs is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs from the Blum and Mitchell (1998) formulation of co-training by using two different learning algorithms rather than two independent feature sets (Goldman and Zhou, 2000). Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy. We also show that simply re-training on all of the newly labelled data is surprisingly effective, with performance depending on the amount of newly labelled data added at each iteration. For certain sizes of newly labelled data, this simple approach is just as effective as the agreement-based method. We also show that co-training can still benefit both taggers when the performance of one</context>
<context position="6294" citStr="Goldman and Zhou (2000)" startWordPosition="988" endWordPosition="991">lysis, using the same independence assumption adopted by Blum and Mitchell. They prove that the two classifiers have low generalisation error if they agree on unlabelled data. Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold. Abney also presents a greedy algorithm that maximises agreement on unlabelled data, which produces comparable results to Collins and Singer (1999) on their named entity classification task. Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption. In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective. 3 The POS taggers The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagge</context>
<context position="10481" citStr="Goldman and Zhou (2000)" startWordPosition="1700" endWordPosition="1703">ial units might be more effective, but we leave this as future work. The co-training process is given in Figure 1. At each stage in the process there is a cache of unlabelled sentences (selected from the total pool of unlabelled sentences) which is labelled by each tagger. The cache size could be increased at each iteration, which is a common practice in the co-training literature. A subset of those sentences labelled by TNT is then added to the training data for C&amp;C, and vice versa. Blum and Mitchell (1998) use the combined set of newly labelled examples for training each view, but we follow Goldman and Zhou (2000) in using separate labelled sets. In the remainder of this section we consider two possible methods for selecting a subset. The cache is cleared after each iteration. There are various ways to select the labelled examples for each tagger. A typical approach is to select those examples assigned a high score by the relevant classifier, under the assumption that these examples will be the most reliable. A score-based selection method is difficult to apply in our experiments, however, since TNT does not provide scores for tagged sentences. We therefore tried two alternative selection methods. The </context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>Sally Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. In Proceedings of the 17th International Conference on Machine Learning, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Workshop on Natural Language Learning,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8317" citStr="Malouf (2002)" startWordPosition="1337" endWordPosition="1338"> used to define the conditional probabilities of a tag given some context. The advantage of ME models over the Markov model used by TNT is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information TNT uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word. A disadvantage is that the training times for ME models are usually relatively slow, especially with iterative scaling methods (see Malouf (2002) for alternative methods). Here we use Generalised Iterative Scaling (Darroch and Ratcliff, 1972), but our implementation is much faster than Ratnaparkhi’s publicly available tagger. The C&amp;C tagger trains in less than 7 minutes on the 1 million words of the Penn Treebank, and tags slightly faster than TNT. Since the taggers share many common features, one might think they are not different enough for effective co-training to be possible. In fact, both taggers are sufficiently different for co-training to be effective. Section 4 shows that both taggers can benefit significantly from the informa</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Workshop on Natural Language Learning, pages 49–55, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="23939" citStr="Merialdo, 1994" startWordPosition="4019" endWordPosition="4020">was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data. It is possible that agreement-based co-training, using more careful selection, would result in an improvement. We leave these experiments to future work, but note that there is a large computational cost associated with such experiments. The performance of the bootstrapped taggers is still a long way behind a tagger trained on a large amount of manually annotated data. This finding is in accord with earlier work on bootstrapping taggers using EM (Elworthy, 1994; Merialdo, 1994). An interesting question would be to determine the minimum number of manually labelled examples that need to be used to seed the system before we can achieve comparable results as using all available manually labelled sentences. For our experiments, co-training never led to a decrease in performance, regardless of the number of iterations. The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). Whether this robustness is a property of the tagging problem or our approach is left for future work. 1This is probably by chance selection of better su</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of the Empirical Methods in NLP Conference,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1500" citStr="Pierce and Cardie, 2001" startWordPosition="210" endWordPosition="213">gging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS tagge</context>
<context position="18825" citStr="Pierce and Cardie (2001)" startWordPosition="3173" endWordPosition="3176">nces) the previous experiments, agreement-based co-training required the taggers to be re-trained 10 to 100 times more often then naive co-training. There are advantages to agreement-based co-training, however. First, the agreement-based method dynamically selects the best sample at each stage, which may not be the whole cache. In particular, when the agreement rate cannot be improved upon, the selected sample can be rejected. For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). Second, for naive co-training, the optimal amount of data to be added at each round (i.e. the cache size) is a parameter that needs to be determined on held out data, whereas the agreement-based method determines this automatically. 4.3 Larger-Scale Experiments We also performed a number of experiments using much more unlabelled training material than before. Instead of using 50, 000 sentences from the 1994 WSJ section of the North American News Corpus, we used 417, 000 sentences (from the same section) and ran the experiments until the unlabelled data had been exhausted. One experiment use</context>
<context position="24385" citStr="Pierce and Cardie, 2001" startWordPosition="4088" endWordPosition="4091">hind a tagger trained on a large amount of manually annotated data. This finding is in accord with earlier work on bootstrapping taggers using EM (Elworthy, 1994; Merialdo, 1994). An interesting question would be to determine the minimum number of manually labelled examples that need to be used to seed the system before we can achieve comparable results as using all available manually labelled sentences. For our experiments, co-training never led to a decrease in performance, regardless of the number of iterations. The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). Whether this robustness is a property of the tagging problem or our approach is left for future work. 1This is probably by chance selection of better subsets. Acknowledgements This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers. The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship. We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for compu</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of the Empirical Methods in NLP Conference, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6922" citStr="Ratnaparkhi, 1996" startWordPosition="1094" endWordPosition="1095">, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption. In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective. 3 The POS taggers The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagger MXPOST (Ratnaparkhi, 1996). The ME tagger, which we refer to as C&amp;C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003). Fast training and tagging times are important for the experiments performed here, since the bootstrapping process can require many tagging and training iterations. The model used by TNT is a standard tagging Markov model, consisting of emission probabilities, and transition probabilities based on trigrams of tags. It also deals with unknown words using a suffix analysis of the target word (the word to be tagged). TNT is very fast for both training a</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the NAACL,</booktitle>
<pages>95--102</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1538" citStr="Sarkar, 2001" startWordPosition="217" endWordPosition="218"> results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We i</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the 2nd Annual Meeting of the NAACL, pages 95–102, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Budapest, Hungary.</location>
<note>(to appear).</note>
<contexts>
<context position="1562" citStr="Steedman et al., 2003" startWordPosition="219" endWordPosition="222">that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-tr</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics, Budapest, Hungary. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1397" citStr="Yarowsky, 1995" startWordPosition="198" endWordPosition="199">ing literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly la</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Bootstrapping a tagged corpus through combination of existing heterogeneous taggers.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>17--20</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2481" citStr="Zavrel and Daelemans (2000)" startWordPosition="372" endWordPosition="375">ed example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&amp;C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs fro</context>
</contexts>
<marker>Zavrel, Daelemans, 2000</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 2000. Bootstrapping a tagged corpus through combination of existing heterogeneous taggers. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, pages 17–20, Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>