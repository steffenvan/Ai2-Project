<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.161942">
<title confidence="0.705151">
Bayesian Hidden Markov Models and Extensions
</title>
<author confidence="0.421062">
Invited Talk
</author>
<bodyText confidence="0.9670864">
Zoubin Ghahramani
Engineering Department, University of Cambridge, Cambridge, UK
zoubin@eng.cam.ac.uk
Hidden Markov models (HMMs) are one of the cornerstones of time-series modelling. I will review
HMMs, motivations for Bayesian approaches to inference in them, and our work on variational Bayesian
learning. I will then focus on recent nonparametric extensions to HMMs. Traditionally, HMMs have
a known structure with a fixed number of states and are trained using maximum likelihood techniques.
The infinite HMM (iHMM) allows a potentially unbounded number of hidden states, letting the model
use as many states as it needs for the data. The recent development of ’Beam Sampling’ — an efficient
inference algorithm for iHMMs based on dynamic programming — makes it possible to apply iHMMs to
large problems. I will show some applications of iHMMs to unsupervised POS tagging and experiments
with parallel and distributed implementations. I will also describe a factorial generalisation of the iHMM
which makes it possible to have an unbounded number of binary state variables, and can be thought of
as a time-series generalisation of the Indian buffet process. I will conclude with thoughts on future
directions in Bayesian modelling of sequential data.
</bodyText>
<page confidence="0.960908">
56
</page>
<reference confidence="0.6065265">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 56,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.106055">
<title confidence="0.972043">Bayesian Hidden Markov Models and Extensions</title>
<note confidence="0.371009666666667">Invited Zoubin Engineering Department, University of Cambridge, Cambridge,</note>
<email confidence="0.990293">zoubin@eng.cam.ac.uk</email>
<abstract confidence="0.943951357142857">Hidden Markov models (HMMs) are one of the cornerstones of time-series modelling. I will review HMMs, motivations for Bayesian approaches to inference in them, and our work on variational Bayesian learning. I will then focus on recent nonparametric extensions to HMMs. Traditionally, HMMs have a known structure with a fixed number of states and are trained using maximum likelihood techniques. The infinite HMM (iHMM) allows a potentially unbounded number of hidden states, letting the model use as many states as it needs for the data. The recent development of ’Beam Sampling’ — an efficient inference algorithm for iHMMs based on dynamic programming — makes it possible to apply iHMMs to large problems. I will show some applications of iHMMs to unsupervised POS tagging and experiments with parallel and distributed implementations. I will also describe a factorial generalisation of the iHMM which makes it possible to have an unbounded number of binary state variables, and can be thought of as a time-series generalisation of the Indian buffet process. I will conclude with thoughts on future directions in Bayesian modelling of sequential data. 56 of the Fourteenth Conference on Computational Natural Language page</abstract>
<intro confidence="0.583882">Sweden, 15-16 July 2010. Association for Computational Linguistics</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>56</pages>
<location>Uppsala,</location>
<marker>2010</marker>
<rawString>Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 56, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>