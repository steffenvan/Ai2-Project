<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.222312">
<title confidence="0.99444">
Automated Tutoring Dialogues for Training in Shipboard
Damage Control
</title>
<author confidence="0.999224">
John Fry, Matt Ginzton, Stanley Peters, Brady Clark &amp; Heather Pon-Barry
</author>
<affiliation confidence="0.878337">
Stanford University
Center for the Study of Language Information
</affiliation>
<address confidence="0.863275">
Stanford CA 94305-4115 USA
</address>
<email confidence="0.999676">
{fry,mginzton,peters,bzack,ponbarry}@csli.stanford.edu
</email>
<sectionHeader confidence="0.995674" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999095">
This paper describes an application
of state-of-the-art spoken language
technology (OAA/Gemini/Nuance)
to a new problem domain: engaging
students in automated tutorial dia-
logues in order to evaluate and im-
prove their performance in a train-
ing simulator.
</bodyText>
<sectionHeader confidence="0.997908" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.971805321428571">
Shipboard damage control refers to the task of
containing the effects of fire, explosions, hull
breaches, flooding, and other critical events
that can occur aboard Naval vessels. The
high-stakes, high-stress nature of this task, to-
gether with limited opportunities for real-life
training, make damage control an ideal target
for AI-enabled educational technologies like
training simulators and tutoring systems.
This paper describes the spoken dialogue
system we developed for automated critiquing
of student performance on a damage control
training simulator. The simulator is DC-
TRAIN (Bulitko and Wilkins, 1999), an im-
mersive, multimedia training environment for
damage control. DC-TRAIN’s training sce-
narios simulate a mixture of physical phenom-
ena (e.g., fire, flooding) and personnel issues
(e.g., casualties, communications, standard-
ized procedures). Our current tutoring sys-
tem is restricted fire damage scenarios only,
and in particular to the twelve fire scenar-
ios available in DC-TRAIN version 2.5, but
in future versions we plan to support post-
session critiques for all of the damage phe-
nomena that will be modeled by DC-TRAIN
4.0: fire, flooding, missile damage, and wall
or firemain ruptures.
</bodyText>
<sectionHeader confidence="0.976188" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.997542">
Eliciting self-explanation from a student has
been shown to be a highly effective tutoring
method (Chi et al., 1994). For this reason,
a number of automated tutoring systems cur-
rently use NLP techniques to engage students
in reflective dialogues. Three notable exam-
ples are the medical CIRCSIM tutor (Zhou et
al., 1999); the Basic Electricity and Electron-
ics (BE&amp;E) tutor (Ros´e et al., 1999); and
the computer literacy AUToTUToR (Wiemer-
Hastings et al., 1999).
Our system shares several features with
these three tutoring systems:
A knowledge base Our system encodes
all domain knowledge relevant to supporting
intelligent tutoring feedback into a structure
called an Expert Session Summary (Section
4). These expert summaries encode causal
relationships between events on the ship as
well as the proper and improper responses to
shipboard crises.
Tutoring strategies In our system, as in
those above, the flow of dialogue is controlled
by (essentially) a finite-state transition net-
work (Fig. 1).
An interpretation component In our
system, the student’s speech is recognized and
parsed into logical forms (Section 3). A dia-
logue manager inspects the current dialogue
information state to determine how best to
incorporate each new utterance into the dia-
logue (Lemon et al., 2001).
</bodyText>
<figure confidence="0.998025129032258">
START
Brief
summary of
session
performance
Evaluate
Summary
of damage
student’s
event 1
Correct
student’s
report Prompt for
reflection on
Prompt
student review
of actions
&amp;quot;You handled
this one well&amp;quot;
reflections
Correct
student’s
&amp;quot;OK, let’s
continue&amp;quot;
errors
Summary
of damage
event N...
END
main points
Review
</figure>
<figureCaption confidence="0.999992">
Figure 1: Post-session dialogue move graph (simplified)
</figureCaption>
<bodyText confidence="0.99995708">
However, an important difference is that
the three systems above are entirely text-
based, whereas ours is a spoken dialogue sys-
tem. Our speech interface offers greater natu-
ralness than keyboard-based input. In this re-
spect, our system is similar to COVE (Roberts,
2000), a training simulator for conning Navy
ships that uses speech to interact with the
student. But whereas COVE uses short conver-
sational exchanges to coach the student dur-
ing the simulation, our system engages in ex-
tended tutorial dialogues after the simulation
has ended. Besides being more natural, spo-
ken language systems are also better suited to
multimodal interactions (viz., one can point
and click while talking but not while typing).
An additional significant difference between
our system and a number of other automated
tutoring systems is our use of ‘deep’ process-
ing techniques. While other systems utilize
‘shallow’ statistical approaches like Latent Se-
mantic Analysis (e.g. AutoTutor), our system
utilizes Gemini, a symbolic grammar. This
approach enables us to provide precise and
reliable meaning representations.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999747416666667">
To facilitate the implementation of multi-
modal, mixed-initiative tutoring interactions,
we decided to implement our system within
the Open Agent Architecture (OAA) (Martin
et al., 1999). OAA is a framework for coor-
dinating multiple asynchronous communicat-
ing processes. The core of OAA is a ‘facilita-
tor’ which manages message passing between
a number of software agents that specialize
in certain tasks (e.g., speech recognition or
database queries). Our system uses OAA to
coordinate the following five agents:
</bodyText>
<listItem confidence="0.992631090909091">
1. The Gemini NLP system (Dowding et
al., 1993). Gemini uses a single unifi-
cation grammar both for parsing strings
of words into logical forms (LFs) and for
generating sentences from LF inputs.
2. A Nuance speech recognition server,
which converts spoken utterances to
strings of words. The Nuance server re-
lies on a language model, which is com-
piled directly from the Gemini grammar,
ensuring that every recognized utterance
is assigned an LF.
3. The Festival text-to-speech system,
which ‘speaks’ word strings generated by
Gemini.
4. A Dialogue Manager which coordi-
nates inputs from the user, interprets the
user’s dialogue moves, updates the dia-
logue context, and delivers speech and
graphical outputs to the user.
5. A Critique Planner, described below
in Section 4.
</listItem>
<bodyText confidence="0.999500222222222">
Agents 1-3 are reusable, ‘off-the-shelf’ dia-
logue system components (apart from the
Gemini grammar, which must be modified for
each application). We implemented agents 4
and 5 in Java specifically for this application.
Variants of this OAA/Gemini/Nuance ar-
chitecture have been deployed successfully in
other dialogue systems, notably SRI’s Com-
mandTalk (Stent et al., 1999) and an un-
</bodyText>
<figureCaption confidence="0.991085">
Figure 2: Screen shot of post-session tutorial dialogue system
</figureCaption>
<bodyText confidence="0.9418555">
manned helicopter interface developed in our
laboratory (Lemon et al., 2001).
</bodyText>
<sectionHeader confidence="0.980197" genericHeader="method">
4 Planning the dialogue
</sectionHeader>
<bodyText confidence="0.986885662790698">
Each student session with DC-TRAIN pro-
duces a session transcript, i.e. a time-stamped
record of every event (both computer- and
student-initiated) that occurred during the
simulation. These transcripts serve as the
input to our post-session Critique Planner
(CP).
The CP plans a post-session tutorial di-
alogue in two steps. In the first step, an
Expert Session Summary (ESS) is cre-
ated from the session transcript. The ESS
is a tree whose parent nodes represent dam-
age events and whose leaves represent actions
taken in response to those damage events.
Each student-initiated action in the ESS is
evaluated as to its timeliness and conformance
to damage control doctrine. Actions that the
student should have taken but did not are also
inserted into the ESS and flagged as such.
Each action node in the ESS therefore falls
into one of three classes: (i) correct actions;
(ii) errors of commission (e.g., the student
sets fire containment boundaries incorrectly);
and (iii) errors of omission (e.g., the student
fails to secure permission from the captain be-
fore flooding certain compartments).
Our current tutoring system covers scenar-
ios generated by DC-Train 2.5, which covers
fire scenarios only. Future versions will use
scenarios generated by DC-Train 4.0, which
covers damage control scenarios involving fire,
smoke, flooding, pipe and hull ruptures, and
equipment deactivation. Our current tutor-
ing system is based on an ESS graph that is
generated by an expert model that consists
of an ad-hoc set of firefighting rules. Future
versions will be based on an ESS graph that
is generated by an successor to the Minerva-
DCA expert model (Bulitko and Wilkins,
1999), an extended Petri Net envisionment-
based reasoning system. The new expert
model is designed to produce an ESS graph
during the course of problem solving that con-
tains nodes for all successful and unsuccessful
plan and goal achievement events, along with
an explanation structure for each graph node.
The second step in planning the post-
session tutorial dialogue is to produce a di-
alogue move graph (Fig. 1). This is a di-
rected graph that encodes all possible configu-
rations of dialogue structure and content that
can be handled by the system.
Generating an appropriate dialogue move
graph from an ESS requires pedagogical
knowledge, and in particular a tutoring strat-
egy. The tutoring strategy we adopted is
based on our analysis of videotapes of fifteen
actual DC-TRAIN post-session critiques con-
ducted by instructors at the Navy’s Surface
Warfare Officer’s School in Newport, RI. The
strategy we observed in these critiques, and
implemented in our system, can be outlined
as follows:
1. Summarize the results of the simulation
(e.g., the final condition of the ship).
2. For each major damage event in the ESS:
(a) Ask the student to review his ac-
tions, correcting his recollections as
necessary.
(b) Evaluate the correctness of each stu-
dent action.
(c) If the student committed errors,
ask him how these could have been
avoided, and evaluate the correct-
ness of his responses.
3. Finally, review each type of error that
arose in step (2c).
A screen shot of the tutoring system in
action is shown in Fig. 2. As soon as a
DC-TRAIN simulation ends, the dialogue sys-
tem starts up and the dialogue manager be-
gins traversing the dialogue move graph. As
the dialogue unfolds, a graphical representa-
tion of the ESS is revealed to the student in
piecemeal fashion as depicted in the top right
frame of Fig. 2.
</bodyText>
<sectionHeader confidence="0.997096" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997334">
This work is supported by the Depart-
ment of the Navy under research grant
N000140010660, a multidisciplinary univer-
sity research initiative on natural language in-
teraction with intelligent tutoring systems.
</bodyText>
<sectionHeader confidence="0.99063" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997647829787234">
V V. Bulitko and D C. Wilkins. 1999. Automated
instructor assistant for ship damage control. In
Proceedings of AAAI-99, Orlando, FL, July.
M. T. H. Chi, N. de Leeuw, M. Chiu, and C.
LaVancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science,
18(3):439–477.
J. Dowding, J. Gawron, D. Appelt, J. Bear, L.
Cherny, R. C. Moore, and D. Moran. 1993.
Gemini: A natural language system for spoken-
language understanding. In Proceedings of the
ARPA Workshop on Human Language Technol-
ogy.
O. Lemon, A. Bracy, A. Gruenstein, and S. Pe-
ters. 2001. A multi-modal dialogue system for
human-robot conversation. In Proceedings of
NAACL 2001.
D. Martin, A. Cheyer, and D. Moran. 1999.
The Open Agent Architecture: a framework for
building distributed software systems. Applied
Artificial Intelligence, 13(1-2).
B. Roberts. 2000. Coaching driving skills in
a shiphandling trainer. In Proceedings of the
AAAI Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. P. Ros´e, B. Di Eugenio, and J. D. Moore. 1999.
A dialogue based tutoring system for basic elec-
tricity and electronics. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED’99), pages 759–
761. IOS Press, Amsterdam.
A. Stent, J. Dowding, J. Gawron, E. O. Bratt, and
R. C. Moore. 1999. The CommandTalk spoken
dialogue system. In Proceedings of ACL ’99,
pages 183–190, College Park, MD.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelli-
gent tutor’s comprehension of students with la-
tent semantic analysis. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED’99), pages 535–
542. IOS Press, Amsterdam.
Y. Zhou, R. Freedman, M. Glass, J. A. Michael,
A. A. Rovick, and M. W. Evens. 1999. Deliver-
ing hints in a dialogue-based intelligent tutoring
system. In Proceedings of AAAI-99, Orlando,
FL, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704464">
<title confidence="0.9749225">Automated Tutoring Dialogues for Training in Damage Control</title>
<author confidence="0.975163">John Fry</author>
<author confidence="0.975163">Matt Ginzton</author>
<author confidence="0.975163">Stanley Peters</author>
<author confidence="0.975163">Brady Clark</author>
<author confidence="0.975163">Heather</author>
<affiliation confidence="0.8969715">Stanford Center for the Study of Language</affiliation>
<address confidence="0.95749">Stanford CA 94305-4115</address>
<abstract confidence="0.996788222222222">This paper describes an application of state-of-the-art spoken language technology (OAA/Gemini/Nuance) to a new problem domain: engaging students in automated tutorial dialogues in order to evaluate and improve their performance in a training simulator.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V V Bulitko</author>
<author>D C Wilkins</author>
</authors>
<title>Automated instructor assistant for ship damage control.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99,</booktitle>
<location>Orlando, FL,</location>
<contexts>
<context position="1180" citStr="Bulitko and Wilkins, 1999" startWordPosition="158" endWordPosition="161">Introduction Shipboard damage control refers to the task of containing the effects of fire, explosions, hull breaches, flooding, and other critical events that can occur aboard Naval vessels. The high-stakes, high-stress nature of this task, together with limited opportunities for real-life training, make damage control an ideal target for AI-enabled educational technologies like training simulators and tutoring systems. This paper describes the spoken dialogue system we developed for automated critiquing of student performance on a damage control training simulator. The simulator is DCTRAIN (Bulitko and Wilkins, 1999), an immersive, multimedia training environment for damage control. DC-TRAIN’s training scenarios simulate a mixture of physical phenomena (e.g., fire, flooding) and personnel issues (e.g., casualties, communications, standardized procedures). Our current tutoring system is restricted fire damage scenarios only, and in particular to the twelve fire scenarios available in DC-TRAIN version 2.5, but in future versions we plan to support postsession critiques for all of the damage phenomena that will be modeled by DC-TRAIN 4.0: fire, flooding, missile damage, and wall or firemain ruptures. 2 Previ</context>
<context position="8049" citStr="Bulitko and Wilkins, 1999" startWordPosition="1229" endWordPosition="1232">m the captain before flooding certain compartments). Our current tutoring system covers scenarios generated by DC-Train 2.5, which covers fire scenarios only. Future versions will use scenarios generated by DC-Train 4.0, which covers damage control scenarios involving fire, smoke, flooding, pipe and hull ruptures, and equipment deactivation. Our current tutoring system is based on an ESS graph that is generated by an expert model that consists of an ad-hoc set of firefighting rules. Future versions will be based on an ESS graph that is generated by an successor to the MinervaDCA expert model (Bulitko and Wilkins, 1999), an extended Petri Net envisionmentbased reasoning system. The new expert model is designed to produce an ESS graph during the course of problem solving that contains nodes for all successful and unsuccessful plan and goal achievement events, along with an explanation structure for each graph node. The second step in planning the postsession tutorial dialogue is to produce a dialogue move graph (Fig. 1). This is a directed graph that encodes all possible configurations of dialogue structure and content that can be handled by the system. Generating an appropriate dialogue move graph from an ES</context>
</contexts>
<marker>Bulitko, Wilkins, 1999</marker>
<rawString>V V. Bulitko and D C. Wilkins. 1999. Automated instructor assistant for ship damage control. In Proceedings of AAAI-99, Orlando, FL, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T H Chi</author>
<author>N de Leeuw</author>
<author>M Chiu</author>
<author>C LaVancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Chi, de Leeuw, Chiu, LaVancher, 1994</marker>
<rawString>M. T. H. Chi, N. de Leeuw, M. Chiu, and C. LaVancher. 1994. Eliciting self-explanations improves understanding. Cognitive Science, 18(3):439–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>J Gawron</author>
<author>D Appelt</author>
<author>J Bear</author>
<author>L Cherny</author>
<author>R C Moore</author>
<author>D Moran</author>
</authors>
<title>Gemini: A natural language system for spokenlanguage understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="5111" citStr="Dowding et al., 1993" startWordPosition="763" endWordPosition="766">ecise and reliable meaning representations. 3 Implementation To facilitate the implementation of multimodal, mixed-initiative tutoring interactions, we decided to implement our system within the Open Agent Architecture (OAA) (Martin et al., 1999). OAA is a framework for coordinating multiple asynchronous communicating processes. The core of OAA is a ‘facilitator’ which manages message passing between a number of software agents that specialize in certain tasks (e.g., speech recognition or database queries). Our system uses OAA to coordinate the following five agents: 1. The Gemini NLP system (Dowding et al., 1993). Gemini uses a single unification grammar both for parsing strings of words into logical forms (LFs) and for generating sentences from LF inputs. 2. A Nuance speech recognition server, which converts spoken utterances to strings of words. The Nuance server relies on a language model, which is compiled directly from the Gemini grammar, ensuring that every recognized utterance is assigned an LF. 3. The Festival text-to-speech system, which ‘speaks’ word strings generated by Gemini. 4. A Dialogue Manager which coordinates inputs from the user, interprets the user’s dialogue moves, updates the di</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Bear, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, J. Gawron, D. Appelt, J. Bear, L. Cherny, R. C. Moore, and D. Moran. 1993. Gemini: A natural language system for spokenlanguage understanding. In Proceedings of the ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>A Bracy</author>
<author>A Gruenstein</author>
<author>S Peters</author>
</authors>
<title>A multi-modal dialogue system for human-robot conversation.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="3069" citStr="Lemon et al., 2001" startWordPosition="452" endWordPosition="455">led an Expert Session Summary (Section 4). These expert summaries encode causal relationships between events on the ship as well as the proper and improper responses to shipboard crises. Tutoring strategies In our system, as in those above, the flow of dialogue is controlled by (essentially) a finite-state transition network (Fig. 1). An interpretation component In our system, the student’s speech is recognized and parsed into logical forms (Section 3). A dialogue manager inspects the current dialogue information state to determine how best to incorporate each new utterance into the dialogue (Lemon et al., 2001). START Brief summary of session performance Evaluate Summary of damage student’s event 1 Correct student’s report Prompt for reflection on Prompt student review of actions &amp;quot;You handled this one well&amp;quot; reflections Correct student’s &amp;quot;OK, let’s continue&amp;quot; errors Summary of damage event N... END main points Review Figure 1: Post-session dialogue move graph (simplified) However, an important difference is that the three systems above are entirely textbased, whereas ours is a spoken dialogue system. Our speech interface offers greater naturalness than keyboard-based input. In this respect, our system</context>
<context position="6356" citStr="Lemon et al., 2001" startWordPosition="957" endWordPosition="960">rs speech and graphical outputs to the user. 5. A Critique Planner, described below in Section 4. Agents 1-3 are reusable, ‘off-the-shelf’ dialogue system components (apart from the Gemini grammar, which must be modified for each application). We implemented agents 4 and 5 in Java specifically for this application. Variants of this OAA/Gemini/Nuance architecture have been deployed successfully in other dialogue systems, notably SRI’s CommandTalk (Stent et al., 1999) and an unFigure 2: Screen shot of post-session tutorial dialogue system manned helicopter interface developed in our laboratory (Lemon et al., 2001). 4 Planning the dialogue Each student session with DC-TRAIN produces a session transcript, i.e. a time-stamped record of every event (both computer- and student-initiated) that occurred during the simulation. These transcripts serve as the input to our post-session Critique Planner (CP). The CP plans a post-session tutorial dialogue in two steps. In the first step, an Expert Session Summary (ESS) is created from the session transcript. The ESS is a tree whose parent nodes represent damage events and whose leaves represent actions taken in response to those damage events. Each student-initiate</context>
</contexts>
<marker>Lemon, Bracy, Gruenstein, Peters, 2001</marker>
<rawString>O. Lemon, A. Bracy, A. Gruenstein, and S. Peters. 2001. A multi-modal dialogue system for human-robot conversation. In Proceedings of NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martin</author>
<author>A Cheyer</author>
<author>D Moran</author>
</authors>
<title>The Open Agent Architecture: a framework for building distributed software systems.</title>
<date>1999</date>
<journal>Applied Artificial Intelligence,</journal>
<pages>13--1</pages>
<contexts>
<context position="4736" citStr="Martin et al., 1999" startWordPosition="703" endWordPosition="706">alking but not while typing). An additional significant difference between our system and a number of other automated tutoring systems is our use of ‘deep’ processing techniques. While other systems utilize ‘shallow’ statistical approaches like Latent Semantic Analysis (e.g. AutoTutor), our system utilizes Gemini, a symbolic grammar. This approach enables us to provide precise and reliable meaning representations. 3 Implementation To facilitate the implementation of multimodal, mixed-initiative tutoring interactions, we decided to implement our system within the Open Agent Architecture (OAA) (Martin et al., 1999). OAA is a framework for coordinating multiple asynchronous communicating processes. The core of OAA is a ‘facilitator’ which manages message passing between a number of software agents that specialize in certain tasks (e.g., speech recognition or database queries). Our system uses OAA to coordinate the following five agents: 1. The Gemini NLP system (Dowding et al., 1993). Gemini uses a single unification grammar both for parsing strings of words into logical forms (LFs) and for generating sentences from LF inputs. 2. A Nuance speech recognition server, which converts spoken utterances to str</context>
</contexts>
<marker>Martin, Cheyer, Moran, 1999</marker>
<rawString>D. Martin, A. Cheyer, and D. Moran. 1999. The Open Agent Architecture: a framework for building distributed software systems. Applied Artificial Intelligence, 13(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roberts</author>
</authors>
<title>Coaching driving skills in a shiphandling trainer.</title>
<date>2000</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Building Dialogue Systems for Tutorial Applications.</booktitle>
<contexts>
<context position="3704" citStr="Roberts, 2000" startWordPosition="551" endWordPosition="552"> of session performance Evaluate Summary of damage student’s event 1 Correct student’s report Prompt for reflection on Prompt student review of actions &amp;quot;You handled this one well&amp;quot; reflections Correct student’s &amp;quot;OK, let’s continue&amp;quot; errors Summary of damage event N... END main points Review Figure 1: Post-session dialogue move graph (simplified) However, an important difference is that the three systems above are entirely textbased, whereas ours is a spoken dialogue system. Our speech interface offers greater naturalness than keyboard-based input. In this respect, our system is similar to COVE (Roberts, 2000), a training simulator for conning Navy ships that uses speech to interact with the student. But whereas COVE uses short conversational exchanges to coach the student during the simulation, our system engages in extended tutorial dialogues after the simulation has ended. Besides being more natural, spoken language systems are also better suited to multimodal interactions (viz., one can point and click while talking but not while typing). An additional significant difference between our system and a number of other automated tutoring systems is our use of ‘deep’ processing techniques. While oth</context>
</contexts>
<marker>Roberts, 2000</marker>
<rawString>B. Roberts. 2000. Coaching driving skills in a shiphandling trainer. In Proceedings of the AAAI Fall Symposium on Building Dialogue Systems for Tutorial Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>B Di Eugenio</author>
<author>J D Moore</author>
</authors>
<title>A dialogue based tutoring system for basic electricity and electronics.</title>
<date>1999</date>
<booktitle>Artificial Intelligence in Education (Proceedings of AIED’99),</booktitle>
<pages>759--761</pages>
<editor>In S. P. Lajoie and M. Vivet, editors,</editor>
<publisher>IOS Press,</publisher>
<location>Amsterdam.</location>
<marker>Ros´e, Di Eugenio, Moore, 1999</marker>
<rawString>C. P. Ros´e, B. Di Eugenio, and J. D. Moore. 1999. A dialogue based tutoring system for basic electricity and electronics. In S. P. Lajoie and M. Vivet, editors, Artificial Intelligence in Education (Proceedings of AIED’99), pages 759– 761. IOS Press, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J Gawron</author>
<author>E O Bratt</author>
<author>R C Moore</author>
</authors>
<title>The CommandTalk spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL ’99,</booktitle>
<pages>183--190</pages>
<location>College Park, MD.</location>
<contexts>
<context position="6207" citStr="Stent et al., 1999" startWordPosition="934" endWordPosition="937"> Gemini. 4. A Dialogue Manager which coordinates inputs from the user, interprets the user’s dialogue moves, updates the dialogue context, and delivers speech and graphical outputs to the user. 5. A Critique Planner, described below in Section 4. Agents 1-3 are reusable, ‘off-the-shelf’ dialogue system components (apart from the Gemini grammar, which must be modified for each application). We implemented agents 4 and 5 in Java specifically for this application. Variants of this OAA/Gemini/Nuance architecture have been deployed successfully in other dialogue systems, notably SRI’s CommandTalk (Stent et al., 1999) and an unFigure 2: Screen shot of post-session tutorial dialogue system manned helicopter interface developed in our laboratory (Lemon et al., 2001). 4 Planning the dialogue Each student session with DC-TRAIN produces a session transcript, i.e. a time-stamped record of every event (both computer- and student-initiated) that occurred during the simulation. These transcripts serve as the input to our post-session Critique Planner (CP). The CP plans a post-session tutorial dialogue in two steps. In the first step, an Expert Session Summary (ESS) is created from the session transcript. The ESS is</context>
</contexts>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>A. Stent, J. Dowding, J. Gawron, E. O. Bratt, and R. C. Moore. 1999. The CommandTalk spoken dialogue system. In Proceedings of ACL ’99, pages 183–190, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wiemer-Hastings</author>
<author>K Wiemer-Hastings</author>
<author>A Graesser</author>
</authors>
<title>Improving an intelligent tutor’s comprehension of students with latent semantic analysis.</title>
<date>1999</date>
<booktitle>Artificial Intelligence in Education (Proceedings of AIED’99),</booktitle>
<pages>535--542</pages>
<editor>In S. P. Lajoie and M. Vivet, editors,</editor>
<publisher>IOS Press,</publisher>
<location>Amsterdam.</location>
<marker>Wiemer-Hastings, Wiemer-Hastings, Graesser, 1999</marker>
<rawString>P. Wiemer-Hastings, K. Wiemer-Hastings, and A. Graesser. 1999. Improving an intelligent tutor’s comprehension of students with latent semantic analysis. In S. P. Lajoie and M. Vivet, editors, Artificial Intelligence in Education (Proceedings of AIED’99), pages 535– 542. IOS Press, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
<author>R Freedman</author>
<author>M Glass</author>
<author>J A Michael</author>
<author>A A Rovick</author>
<author>M W Evens</author>
</authors>
<title>Delivering hints in a dialogue-based intelligent tutoring system.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99,</booktitle>
<location>Orlando, FL,</location>
<contexts>
<context position="2108" citStr="Zhou et al., 1999" startWordPosition="304" endWordPosition="307">ly, and in particular to the twelve fire scenarios available in DC-TRAIN version 2.5, but in future versions we plan to support postsession critiques for all of the damage phenomena that will be modeled by DC-TRAIN 4.0: fire, flooding, missile damage, and wall or firemain ruptures. 2 Previous Work Eliciting self-explanation from a student has been shown to be a highly effective tutoring method (Chi et al., 1994). For this reason, a number of automated tutoring systems currently use NLP techniques to engage students in reflective dialogues. Three notable examples are the medical CIRCSIM tutor (Zhou et al., 1999); the Basic Electricity and Electronics (BE&amp;E) tutor (Ros´e et al., 1999); and the computer literacy AUToTUToR (WiemerHastings et al., 1999). Our system shares several features with these three tutoring systems: A knowledge base Our system encodes all domain knowledge relevant to supporting intelligent tutoring feedback into a structure called an Expert Session Summary (Section 4). These expert summaries encode causal relationships between events on the ship as well as the proper and improper responses to shipboard crises. Tutoring strategies In our system, as in those above, the flow of dialo</context>
</contexts>
<marker>Zhou, Freedman, Glass, Michael, Rovick, Evens, 1999</marker>
<rawString>Y. Zhou, R. Freedman, M. Glass, J. A. Michael, A. A. Rovick, and M. W. Evens. 1999. Delivering hints in a dialogue-based intelligent tutoring system. In Proceedings of AAAI-99, Orlando, FL, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>