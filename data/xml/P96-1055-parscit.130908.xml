<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010676">
<title confidence="0.9714105">
The Selection of the Most Probable Dependency Structure in
Japanese Using Mutual Information
</title>
<author confidence="0.989812">
Eduardo de Paiva Alves
</author>
<affiliation confidence="0.993437">
University of Electro-Communications
</affiliation>
<address confidence="0.476012">
1-5-1 Chofugaoka Chofushi Tokyo Japan
</address>
<email confidence="0.62577">
ealves©phaeton.cs.uec.acjp
</email>
<sectionHeader confidence="0.986793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999723">
We use a statistical method to select the
most probable structure or parse for a given
sentence. It takes as input the dependency
structures generated for the sentence by
a dependency grammar, finds all triple of
modifier, particle and modificant relations,
calculates mutual information of each re-
lation and chooses the structure for which
the product of the mutual information of
its relations is the highest.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999614272727273">
Computer Aided Instruction (CAI) systems are im-
portant and effective tools, especially for teaching
foreign languages. Many students of Japanese as a
foreign language are aware of the Computer Assisted
TEchnical Reading System (CATERS) that provides
helpful information for reading texts in science and
technology fields (Kano and Yamamoto, 1995).
One of the difficulties in learning Japanese lies in
recognizing dependency relations in Japanese sen-
tences. This is because the language allows relatively
free word orders. Take an example from a leading
newspaper:
tz.v, 0)3.*:rsZ91 t.11/14 tz.V,
We would like to expect a prompt study of the causes, based
on a national investigation
To understand this sentence it is necessary to
know that MM4: (investigation) modifies
Llt.: (based) but not 11A41- (expect); t &apos;Jo
(based) modifies &apos;AM (study) but not W, (cause).
CATERS is useful because it provides such infor-
mation through several user-friendly functions.
As effective as it is for foreign students, however,
the texts in CATERS are fixed and the dependency
structure of every sentence in them is all hand-coded.
This inability to handle new text poses a serious
problem in its general applicability and extensibility.
This paper describes a method for selecting the
right or most probable structure for a Japanese sen-
tence among multiple probable structures generated
by Restricted Dependency Grammar (RDG) (Fuku-
moto, 1992). If this method works, then its results
will be quite valuable for facilitating the develop-
ment of new texts for CAI systems like CATERS.
</bodyText>
<sectionHeader confidence="0.975842" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.995037292682927">
As pointed out earlier, the dependency relation of
elements in Japanese sentences are fairly compli-
cated due to relatively free word orders. RDG is
designed to determine dependency relations among
words and phrases in sentences. To do so, it clas-
sifies the phrases according to grammatical cate-
gories and syntactic attributes. However, it fails to
reject semantically unacceptable dependency struc-
tures. The inevitable consequence is that RDG often
produces multiple parses even for a simple sentence.
Kurohashi and Nagao (1993) try to determine the
dependency relations of a sentence by means of using
sample sentences. When the sentence is structurally
ambiguous, they determine its structure by compar-
ing it to structurally similar patterns taken from a
manually generated set of examples and calculating
similarity values.
Our method, on the contrary, uses a statistical ap-
proach to select the most probable structure or parse
of a given sentence. It takes as input dependency
structures generated by RDG for a sentence, finds all
of modifier-particle-modificant relations, calculates
their mutual information and chooses the structure
for which the product of the mutual information of
its relations is the highest.
In order to calculate the mutual information
for any modifier-particle-modificant pattern, we use
the Conceptual Dictionaryl (CD) to build a tax-
onomic hierarchy of the modifiers which occur
&apos;The Co-occurrence Dictionary and Conceptual Dic-
tionary used in the process are part of a set of machine
readable Japanese dictionaries compiled by the Japan
Electronic Dictionary Research Institute (EDR, 1993).
The Conceptual Dictionary is a set of graphs consisting
of 400,000 concepts and a number of taxonomic as well as
functional relations between them. The Co-occurrence
Dictionary consist of a list of 1,100,000 dependency re-
lations (modifier, particle and modificant) taken from a
corpus. Each entry includes syntactic information, con-
cept identifiers (a numerical code) and the number of
occurrences in the corpus.
</bodyText>
<page confidence="0.99201">
372
</page>
<bodyText confidence="0.99998205">
with the particle-modificant sub-pattern in the Co-
occurrence Dictionary (COD). The mutual informa-
tion for any pattern is the maximum mutual infor-
mation between the sub-pattern and the concepts in
the taxonomic hierarchy which generalize the modi-
fier in the pattern.
Resnik and Hearst (1993) use a similar approach
to calculate preferences for prepositional phrase at-
tachment. While they use data on word groups, our
method directly uses word co-occurrence data to es-
timate the preferences using the CD to identify the
most adequate grouping for each relation.
While Kurohashi and Nagao compare the sentence
with a single sample of patterns, we use all occur-
rences of the pattern in COD to calculate the mu-
tual information. Our approach automatically ex-
tracts the occurrences from the dictionary as well as
builds the taxonomic hierarchy. Unlike Kurohashi
and Nagao (1993), which uses only verb and adjec-
tive patterns, we cover all dependency relations.
</bodyText>
<sectionHeader confidence="0.8952895" genericHeader="method">
3 Selecting the Most Probable
Structure
</sectionHeader>
<bodyText confidence="0.978685739130435">
RDG identifies all possible dependency structures
which consist of modifier-modificant relations be-
tween elements in a sentence. The arcs in the fol-
lowing example show modifier-modificant relations
which can be combined into six different dependency
structures.
I El I rit4 r—r1
nRENi.: 6 L-iotz cti 10141..izt,1
national investigation baseaTairse7557n7 siiaTexpect
Our objective is to develop a method to automat-
ically select the correct dependency structures accu-
rately or at least those which have the highest prob-
ability of being correct. We evaluate the various
possible structures according to the mutual infor-
mation between modifiers and particle-modificants.
In some cases there is no particle and the modifi-
cant directly precedes the modifier (see example in
section 3.2). To calculate the mutual information
for each relation, we obtain form the COD the con-
ceptual identifiers (a numerical code) for the mod-
ifiers that appear with the particle-modificant and
the number of their occurrences in the corpus. If the
pattern is not present, backing off, we search this in-
formation for the modificant only. For each of those
concept identifiers we obtain from the CD all gen-
eralizers (concept identifiers that express a similar
meaning in a more general way) and build a taxo-
nomic hierarchy with them. Using the number of
occurrences obtained, we calculate the mutual infor-
mation for the concepts in the taxonomic hierarchy.
We also build a taxonomic hierarchy for the modi-
fier that appears with the particle-modificant in the
sentence. Then comparing these two taxonomic hi-
erarchies (one for the modifiers in the COD, one for
the modifiers in the sentence), we look for the con-
cept identifier common to both hierarchies that has
the highest mutual information. This is the mutual
information for the relation itself. For each depen-
dency structure we calculate a score by multiplying
the mutual information for all ambiguous relations
(the non-ambiguous do not contribute to the evalua-
tion). The dependency structure with highest prob-
ability of being correct is the one with the highest
score. Since all structures have the same number of
relations, this multiplication reflects the likelyhood
of the structure.
</bodyText>
<subsectionHeader confidence="0.998892">
3.1 The Algorithm
</subsectionHeader>
<bodyText confidence="0.9990175">
The process described above is written in an algo-
rithmic form as follows:
</bodyText>
<listItem confidence="0.94614034375">
1. Select the ambiguous relations (those with more
than one modificant) for each structure.
2. Search COD for the particle-modificant sub-
pattern, in the corresponding positions. If there
is no entry, search for the modificant only.
3. Obtain from the COD the concept identifiers for
the modificant (there may be multiple mean-
ings) and the concept identifiers with the num-
ber of their occurrences in the corpus for
the modifiers which occur with the particle-
modificant pattern.
4. For each modificant concept identifier, build a
taxonomic hierarchy with its modifiers using
CD to find the generalizer for each concept iden-
tifier.
5. Calculate the mutual information 2
for all the concept identifiers in the taxonomic
hierarchies.
6. For the modifiers in the sentence, extract their
concept identifiers from COD and build the tax-
onomic hierarchies using CD to find the gener-
alizers for each concept identifier.
7. For each relation (modifier-particle-modificant
pattern), search the concept identifier that gen-
eralizes the modifier word and has maximum
mutual information. This value is the mutual
information for the relation.
8. For each dependency structure, multiply the
mutual information of its ambiguous depen-
dency relations to obtain the score for that
structure.
9. Arrange the structures according to their scores.
</listItem>
<footnote confidence="0.613982666666667">
2The mutual information tells how much information
one outcome gives about the other and is given by the
formula:
</footnote>
<equation confidence="0.9702375">
( P(tel,w2)
P(wi)P(tv2))
I(wi,w2) = In
(1)
</equation>
<page confidence="0.998318">
373
</page>
<subsectionHeader confidence="0.993946">
3.2 Examples
</subsectionHeader>
<bodyText confidence="0.999606333333333">
The following figure shows the output from RDG for
a given sentence. The arrows in the figure indicate
the dependency relations.
</bodyText>
<equation confidence="0.686399333333333">
ir—i 1-41r-1r-14
glksiat&gt; VD17 t*IHt 114,T fib &lt;
lItorclul= change ilcohynalain Progress work people stress
grow worse
The ambiguous relations are 44I*0 I9 /-u-e,
&lt;A, * &lt;}&apos;VX, AA; &lt;
</equation>
<bodyText confidence="0.966150909090909">
and i -C.-&amp;quot;D0)-- -C. Accordingly the occurrences for
the modificants in these relations (blab&apos;, t&apos;fi&lt; ,
&lt;, Its, and -D 0)Z ) are extracted from COD, ob-
taining a list of modifier concept identifiers with the
number of their occurrences. Note that in the pat-
tern * &lt; A and * &lt; A 1- 1. A the modificant pre-
cedes the modifier. The following figure shows some
modifiers for * &lt; (work) with their number of oc-
currences.
A trit -LIM g133- I* IN *I* 4/PAM
person woman mother drive each person factory wife fact worker
</bodyText>
<equation confidence="0.744695">
32 18 6 6 3 3 3 2 2 2
</equation>
<bodyText confidence="0.958663333333333">
Next, the taxonomic hierarchy for each particle-
modificant is built and the mutual information cal-
culated for each concept identifier. An extract of the
hierarchy for * &lt; is shown in the following figure.
iRo ,C,S04t 1tE0D 1 0 %iie
sudden re -MOW a7s73-fizvrarsi-Es--4 pressure more than 1070 was
</bodyText>
<sectionHeader confidence="0.989215" genericHeader="evaluation">
4 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999152695652174">
We have applied our method to 35 sentences taken
from a leading newspaper and included with RDG
software. The average number of dependency struc-
tures per sentence is 8.68. The method we used se-
lected the correct structures for 25 sentences. The
correct structures for 8 sentences were found as the
second most probable structure by the method.
In another experiment, we parsed 70 sentences us-
ing a grammar similar to the one used in Kurohashi
and Nagao (1993). Our method selected the most
likely relation among the multiple generated in 95%
of the cases.
Although the size of the test data is small, we say
that our method provided a way to identify the most
probable structure more efficiently than RDG. Since
the sentences used are extracted from a newspaper,
it&apos;s also general in its applicability. Therefore it can
be used in preparing teaching materials such as the
structures used by a CAI system such as CATERS,
saving the instructor of hand-coding them. In future
work we shall extract the co-occurrences directly
from the corpora, and use other grouping techniques
to replace the CD.
</bodyText>
<figure confidence="0.992449176470588">
1.60 3.40
rt I 3r,11, Frzliri. 3-6
WI:Mt
concept pseudo-still life
\k
AAAI
1:S3UYA.VIt
Mtvq.1-Ztf* s411 life abstract product
tifiliz1141IDZ
relative to action
t (3.40)
force
human or similar
live body
human
&amp;quot; A (3.61)
person
</figure>
<bodyText confidence="0.984180923076923">
Next the generalizers for (Oh, A, and A 1- 1/ A )
are searched in the hierarchies for their modificants
to obtain the mutual information for the relations.
For * &lt; A (working person) it happened to be the
concept A (person) itself with mutual information
of 3.61. For * &lt; A 1- 1,, A (working stress) the match
occurred for )7 (force) giving a mutual information
of 0.69.
Multiply the mutual information for all the depen-
dency relations in each structure. For the example
sentence the mutual information for the ambiguous
relations are as follows: trizy-6.&apos;ciNK1-4A1
kAMI7Y Rip #0,i*Z-hl 1.-C Oh&lt; Ao A h 1/ A
</bodyText>
<page confidence="0.920224">
12.95
</page>
<bodyText confidence="0.997498333333333">
From this the algorithm selects the parse with
highest score which is drawn in thick lines. The next
figure shows the result for the first example sentence.
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="conclusions">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999523666666667">
I am thankful to my thesis advisor Dr. T. Furugori
and the anonymous referees for their suggestions and
comments.
</bodyText>
<sectionHeader confidence="0.997759" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997872">
Fukumoto, F.; Sano H., Saitoh, Y.; and Fukumoto
J. 1992. A Framework for Dependency Gram-
mar based on the word&apos;s modifiability level -
Restricted Dependency Grammar. In Trans. IPS
Japan, 33(10), (in Japanese).
Resnik,P. and Hearst M. 1993. Structural Ambiguity
and Conceptual Relations. In Proceedings of the
Workshop on Very Large Corpora: Academic and
Industrial Perspectives. Ohio State University.
Japan Electronic Dictionary Research Institute, Ltd.
1993. EDR Electronic Dictionary Specifications
Guide (in Japanese).
Kano, C. and Yamamoto, H. 1995. A System for
Reading Scientific and Technical Texts, Class-
room, Instruction and Evaluation. In Jinbunka-
gaku to computer 27(1) (in Japanese).
Kurohashi, S., and Nagao, M. 1993. Structural
Disambiguation in Japanese by Evaluating Case
Structures based on Examples in Case Frame Dic-
tionary. In Proceedings of IWPT93.
</reference>
<page confidence="0.999022">
374
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945743">
<title confidence="0.998093">The Selection of the Most Probable Dependency Structure in Japanese Using Mutual Information</title>
<author confidence="0.998331">Eduardo de_Paiva Alves</author>
<affiliation confidence="0.999837">University of Electro-Communications</affiliation>
<address confidence="0.964487">1-5-1 Chofugaoka Chofushi Tokyo Japan</address>
<email confidence="0.990712">ealves©phaeton.cs.uec.acjp</email>
<abstract confidence="0.999419">We use a statistical method to select the most probable structure or parse for a given sentence. It takes as input the dependency structures generated for the sentence by a dependency grammar, finds all triple of modifier, particle and modificant relations, calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>H Sano</author>
<author>Y Saitoh</author>
<author>J Fukumoto</author>
</authors>
<title>A Framework for Dependency Grammar based on the word&apos;s modifiability level -Restricted Dependency Grammar. In Trans.</title>
<date>1992</date>
<journal>IPS Japan,</journal>
<volume>33</volume>
<issue>10</issue>
<note>(in Japanese).</note>
<marker>Fukumoto, Sano, Saitoh, Fukumoto, 1992</marker>
<rawString>Fukumoto, F.; Sano H., Saitoh, Y.; and Fukumoto J. 1992. A Framework for Dependency Grammar based on the word&apos;s modifiability level -Restricted Dependency Grammar. In Trans. IPS Japan, 33(10), (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>M Hearst</author>
</authors>
<title>Structural Ambiguity and Conceptual Relations.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora: Academic and</booktitle>
<institution>Industrial Perspectives. Ohio State University.</institution>
<contexts>
<context position="4564" citStr="Resnik and Hearst (1993)" startWordPosition="690" endWordPosition="693">umber of taxonomic as well as functional relations between them. The Co-occurrence Dictionary consist of a list of 1,100,000 dependency relations (modifier, particle and modificant) taken from a corpus. Each entry includes syntactic information, concept identifiers (a numerical code) and the number of occurrences in the corpus. 372 with the particle-modificant sub-pattern in the Cooccurrence Dictionary (COD). The mutual information for any pattern is the maximum mutual information between the sub-pattern and the concepts in the taxonomic hierarchy which generalize the modifier in the pattern. Resnik and Hearst (1993) use a similar approach to calculate preferences for prepositional phrase attachment. While they use data on word groups, our method directly uses word co-occurrence data to estimate the preferences using the CD to identify the most adequate grouping for each relation. While Kurohashi and Nagao compare the sentence with a single sample of patterns, we use all occurrences of the pattern in COD to calculate the mutual information. Our approach automatically extracts the occurrences from the dictionary as well as builds the taxonomic hierarchy. Unlike Kurohashi and Nagao (1993), which uses only v</context>
</contexts>
<marker>Resnik, Hearst, 1993</marker>
<rawString>Resnik,P. and Hearst M. 1993. Structural Ambiguity and Conceptual Relations. In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives. Ohio State University.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>EDR Electronic Dictionary Specifications Guide (in Japanese).</booktitle>
<institution>Japan Electronic Dictionary Research Institute, Ltd.</institution>
<contexts>
<context position="2762" citStr="(1993)" startWordPosition="419" endWordPosition="419">tating the development of new texts for CAI systems like CATERS. 2 Background As pointed out earlier, the dependency relation of elements in Japanese sentences are fairly complicated due to relatively free word orders. RDG is designed to determine dependency relations among words and phrases in sentences. To do so, it classifies the phrases according to grammatical categories and syntactic attributes. However, it fails to reject semantically unacceptable dependency structures. The inevitable consequence is that RDG often produces multiple parses even for a simple sentence. Kurohashi and Nagao (1993) try to determine the dependency relations of a sentence by means of using sample sentences. When the sentence is structurally ambiguous, they determine its structure by comparing it to structurally similar patterns taken from a manually generated set of examples and calculating similarity values. Our method, on the contrary, uses a statistical approach to select the most probable structure or parse of a given sentence. It takes as input dependency structures generated by RDG for a sentence, finds all of modifier-particle-modificant relations, calculates their mutual information and chooses th</context>
<context position="4564" citStr="(1993)" startWordPosition="693" endWordPosition="693"> as well as functional relations between them. The Co-occurrence Dictionary consist of a list of 1,100,000 dependency relations (modifier, particle and modificant) taken from a corpus. Each entry includes syntactic information, concept identifiers (a numerical code) and the number of occurrences in the corpus. 372 with the particle-modificant sub-pattern in the Cooccurrence Dictionary (COD). The mutual information for any pattern is the maximum mutual information between the sub-pattern and the concepts in the taxonomic hierarchy which generalize the modifier in the pattern. Resnik and Hearst (1993) use a similar approach to calculate preferences for prepositional phrase attachment. While they use data on word groups, our method directly uses word co-occurrence data to estimate the preferences using the CD to identify the most adequate grouping for each relation. While Kurohashi and Nagao compare the sentence with a single sample of patterns, we use all occurrences of the pattern in COD to calculate the mutual information. Our approach automatically extracts the occurrences from the dictionary as well as builds the taxonomic hierarchy. Unlike Kurohashi and Nagao (1993), which uses only v</context>
<context position="10697" citStr="(1993)" startWordPosition="1695" endWordPosition="1695">hown in the following figure. iRo ,C,S04t 1tE0D 1 0 %iie sudden re -MOW a7s73-fizvrarsi-Es--4 pressure more than 1070 was 4 Results and Evaluation We have applied our method to 35 sentences taken from a leading newspaper and included with RDG software. The average number of dependency structures per sentence is 8.68. The method we used selected the correct structures for 25 sentences. The correct structures for 8 sentences were found as the second most probable structure by the method. In another experiment, we parsed 70 sentences using a grammar similar to the one used in Kurohashi and Nagao (1993). Our method selected the most likely relation among the multiple generated in 95% of the cases. Although the size of the test data is small, we say that our method provided a way to identify the most probable structure more efficiently than RDG. Since the sentences used are extracted from a newspaper, it&apos;s also general in its applicability. Therefore it can be used in preparing teaching materials such as the structures used by a CAI system such as CATERS, saving the instructor of hand-coding them. In future work we shall extract the co-occurrences directly from the corpora, and use other grou</context>
</contexts>
<marker>1993</marker>
<rawString>Japan Electronic Dictionary Research Institute, Ltd. 1993. EDR Electronic Dictionary Specifications Guide (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kano</author>
<author>H Yamamoto</author>
</authors>
<title>A System for Reading Scientific and Technical Texts, Classroom, Instruction and Evaluation.</title>
<date>1995</date>
<booktitle>In Jinbunkagaku to computer</booktitle>
<volume>27</volume>
<issue>1</issue>
<note>(in Japanese).</note>
<contexts>
<context position="1000" citStr="Kano and Yamamoto, 1995" startWordPosition="141" endWordPosition="144">d for the sentence by a dependency grammar, finds all triple of modifier, particle and modificant relations, calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest. 1 Introduction Computer Aided Instruction (CAI) systems are important and effective tools, especially for teaching foreign languages. Many students of Japanese as a foreign language are aware of the Computer Assisted TEchnical Reading System (CATERS) that provides helpful information for reading texts in science and technology fields (Kano and Yamamoto, 1995). One of the difficulties in learning Japanese lies in recognizing dependency relations in Japanese sentences. This is because the language allows relatively free word orders. Take an example from a leading newspaper: tz.v, 0)3.*:rsZ91 t.11/14 tz.V, We would like to expect a prompt study of the causes, based on a national investigation To understand this sentence it is necessary to know that MM4: (investigation) modifies Llt.: (based) but not 11A41- (expect); t &apos;Jo (based) modifies &apos;AM (study) but not W, (cause). CATERS is useful because it provides such information through several user-friend</context>
</contexts>
<marker>Kano, Yamamoto, 1995</marker>
<rawString>Kano, C. and Yamamoto, H. 1995. A System for Reading Scientific and Technical Texts, Classroom, Instruction and Evaluation. In Jinbunkagaku to computer 27(1) (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Structural Disambiguation in Japanese by Evaluating Case Structures based on Examples in Case Frame Dictionary.</title>
<date>1993</date>
<booktitle>In Proceedings of IWPT93.</booktitle>
<contexts>
<context position="2762" citStr="Kurohashi and Nagao (1993)" startWordPosition="416" endWordPosition="419"> valuable for facilitating the development of new texts for CAI systems like CATERS. 2 Background As pointed out earlier, the dependency relation of elements in Japanese sentences are fairly complicated due to relatively free word orders. RDG is designed to determine dependency relations among words and phrases in sentences. To do so, it classifies the phrases according to grammatical categories and syntactic attributes. However, it fails to reject semantically unacceptable dependency structures. The inevitable consequence is that RDG often produces multiple parses even for a simple sentence. Kurohashi and Nagao (1993) try to determine the dependency relations of a sentence by means of using sample sentences. When the sentence is structurally ambiguous, they determine its structure by comparing it to structurally similar patterns taken from a manually generated set of examples and calculating similarity values. Our method, on the contrary, uses a statistical approach to select the most probable structure or parse of a given sentence. It takes as input dependency structures generated by RDG for a sentence, finds all of modifier-particle-modificant relations, calculates their mutual information and chooses th</context>
<context position="5145" citStr="Kurohashi and Nagao (1993)" startWordPosition="784" endWordPosition="787">fier in the pattern. Resnik and Hearst (1993) use a similar approach to calculate preferences for prepositional phrase attachment. While they use data on word groups, our method directly uses word co-occurrence data to estimate the preferences using the CD to identify the most adequate grouping for each relation. While Kurohashi and Nagao compare the sentence with a single sample of patterns, we use all occurrences of the pattern in COD to calculate the mutual information. Our approach automatically extracts the occurrences from the dictionary as well as builds the taxonomic hierarchy. Unlike Kurohashi and Nagao (1993), which uses only verb and adjective patterns, we cover all dependency relations. 3 Selecting the Most Probable Structure RDG identifies all possible dependency structures which consist of modifier-modificant relations between elements in a sentence. The arcs in the following example show modifier-modificant relations which can be combined into six different dependency structures. I El I rit4 r—r1 nRENi.: 6 L-iotz cti 10141..izt,1 national investigation baseaTairse7557n7 siiaTexpect Our objective is to develop a method to automatically select the correct dependency structures accurately or at </context>
<context position="10697" citStr="Kurohashi and Nagao (1993)" startWordPosition="1692" endWordPosition="1695">erarchy for * &lt; is shown in the following figure. iRo ,C,S04t 1tE0D 1 0 %iie sudden re -MOW a7s73-fizvrarsi-Es--4 pressure more than 1070 was 4 Results and Evaluation We have applied our method to 35 sentences taken from a leading newspaper and included with RDG software. The average number of dependency structures per sentence is 8.68. The method we used selected the correct structures for 25 sentences. The correct structures for 8 sentences were found as the second most probable structure by the method. In another experiment, we parsed 70 sentences using a grammar similar to the one used in Kurohashi and Nagao (1993). Our method selected the most likely relation among the multiple generated in 95% of the cases. Although the size of the test data is small, we say that our method provided a way to identify the most probable structure more efficiently than RDG. Since the sentences used are extracted from a newspaper, it&apos;s also general in its applicability. Therefore it can be used in preparing teaching materials such as the structures used by a CAI system such as CATERS, saving the instructor of hand-coding them. In future work we shall extract the co-occurrences directly from the corpora, and use other grou</context>
</contexts>
<marker>Kurohashi, Nagao, 1993</marker>
<rawString>Kurohashi, S., and Nagao, M. 1993. Structural Disambiguation in Japanese by Evaluating Case Structures based on Examples in Case Frame Dictionary. In Proceedings of IWPT93.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>