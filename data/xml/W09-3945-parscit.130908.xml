<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011073">
<title confidence="0.9980425">
Attention and Interaction Control in
a Human-Human-Computer Dialogue Setting
</title>
<author confidence="0.993136">
Gabriel Skantze Joakim Gustafson
</author>
<affiliation confidence="0.7602355">
Dept. of Speech Music and Hearing Dept. of Speech Music and Hearing
KTH, Stockholm, Sweden KTH, Stockholm, Sweden
</affiliation>
<email confidence="0.985613">
gabriel@speech.kth.se jocke@speech.kth.se
</email>
<sectionHeader confidence="0.995397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999242909090909">
This paper presents a simple, yet effective
model for managing attention and interaction
control in multimodal spoken dialogue sys-
tems. The model allows the user to switch at-
tention between the system and other hu-
mans, and the system to stop and resume
speaking. An evaluation in a tutoring setting
shows that the user&apos;s attention can be effec-
tively monitored using head pose tracking,
and that this is a more reliable method than
using push-to-talk.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999902125">
Most spoken dialogue systems are based on the
assumption that there is a clear beginning and
ending of the dialogue, during which the user
pays attention to the system constantly. However,
as the use of dialogue systems is extended to
settings where several humans are involved, or
where the user needs to attend to other things
during the dialogue, this assumption is obviously
too simplistic (Bohus &amp; Horvitz, 2009). When it
comes to interaction, a strict turn-taking protocol
is often assumed, where user and system wait for
their turn and deliver their contributions in whole
utterance-sized chunks. If system utterances are
interrupted, they are treated as either fully
delivered or basically unsaid.
This paper presents a simple, yet effective
model for managing attention and interaction
control in multimodal (face-to-face) spoken dia-
logue systems, which avoids these simplifying
assumptions. We also present an evaluation in a
tutoring setting where we explore the use of head
tracking for monitoring user attention, and com-
pare it with a more traditional method: push-to-
talk.
</bodyText>
<sectionHeader confidence="0.736114" genericHeader="method">
2 Monitoring user attention
</sectionHeader>
<bodyText confidence="0.999991375">
In multi-party dialogue settings, gaze has been
identified as an effective cue to help disambi-
guate the addressee of a spoken utterance
(Vertegaal et al., 2001). When it comes to hu-
man-machine interaction, Maglio et al. (2000)
showed that users tend to look at speech-
controlled devices when talking to them, even if
they do not have the manifestation of an embo-
died agent. Bakx et al. (2003) investigated the
use of head pose for identifying the addressee in
a multi-party interaction between two humans
and an information kiosk. The results indicate
that head pose should be combined with acoustic
and linguistic features such as utterances length.
Facial orientation in combination with speech-
related features was investigated by Katzenmaier
et al. (2004) in a human-human-robot interaction,
confirming that a combination of cues was most
effective. A common finding in these studies is
that if a user does not look at the system while
talking he is most likely not addressing it. How-
ever, when the user looks at the system while
speaking, there is a considerable probability that
she is actually addressing a bystander.
</bodyText>
<sectionHeader confidence="0.996115" genericHeader="method">
3 The MonAMI Reminder
</sectionHeader>
<bodyText confidence="0.999420636363636">
This study is part of the 6th framework IP project
MonAMI1. The goal of the MonAMI project is to
develop and evaluate services for elderly and
disabled people. Based on interviews with poten-
tial users in the target group, we have developed
the MonAMI Reminder, a multimodal spoken
dialogue system which can assist elderly and dis-
abled people in organising and initiating their
daily activities (Beskow et al., 2009). The dia-
logue system uses Google Calendar as a back-
bone to answer questions about events. However,
</bodyText>
<footnote confidence="0.982257">
1 http://www.monami.info/
</footnote>
<note confidence="0.474231">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 310–313,
</note>
<affiliation confidence="0.827834">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999475">
310
</page>
<bodyText confidence="0.9992834">
it can also take the initiative and give reminders
to the user.
The MonAMI Reminder is based on the HIG-
GINS platform (Skantze, 2007). The architecture
is shown in Figure 1. A microphone and a cam-
era are used for system input (speech recognition
and head tracking), and a speaker and a display
are used for system output (an animated talking
head). This is pretty much a standard dialogue
system architecture, with some exceptions. Di-
alogue management is split into a Discourse
Modeller and an Action Manager, which consults
the discourse model and decides what to do next.
There is also an Attention and Interaction Con-
troller (AIC), which will be discussed next.
</bodyText>
<figureCaption confidence="0.991164">
Figure 1. The system architecture in the MonAMI
Reminder.
</figureCaption>
<sectionHeader confidence="0.856906" genericHeader="method">
4 Attention and interaction model
</sectionHeader>
<bodyText confidence="0.996251529411765">
The purpose of the AIC is to act as a low level
monitor and controller of the system&apos;s speaking
and attentional behaviour. The AIC uses a state-
based model to track the attentional and interac-
tional state of the user and the system, shown in
Figure 2. The states shown in the boxes can be
regarded as the combined state of the system
(columns) and the user (rows)2. Depending on
the combined state, events from input and output
components will have different effects. As can be
seen in the figure, some combination of states
cannot be realised, such as the system and user
speaking at the same time (if the user speaks
while the system is speaking, it will automati-
cally change to the state INTERRUPTED). Of
course, the user might speak while the system is
speaking without the system detecting this, but
</bodyText>
<footnote confidence="0.8778815">
2 This is somewhat similar to the &amp;quot;engagement state&amp;quot; used
in Bohus &amp; Horvitz (2009).
</footnote>
<bodyText confidence="0.999957107142857">
the model should be regarded from the system&apos;s
perspective, not from an observer.
The user&apos;s attention is monitored using a cam-
era and an off-the-shelf head tracking software.
As the user starts to look at the system, the state
changes from NONATTENTIVE to ATTENTIVE.
When the user starts to speak, a UserStartSpeak
event from the ASR will trigger a change to the
LISTENING state. The Action Manager might
then trigger a SystemResponse event (together
with what should be said), causing a change into
the SPEAKING state. Now, if the user would look
away while the system is speaking, the system
would enter the HOLDING state — the system
would pause and then resume when the user
looks back. If the user starts to speak while the
system is speaking, the controller will enter the
INTERRUPTED state. The Action Manager might
then either decide to answer the new request,
resume speaking (e.g., if there was just a back-
channel or the confidence was too low), or abort
speaking (e.g., if the user told the system to shut
up).
There is also a CALLING state, in which the
system might try to grab the user&apos;s attention.
This is very important for the current application
when the system needs to remind the user about
something.
</bodyText>
<sectionHeader confidence="0.462032" genericHeader="method">
4.1 Incremental multimodal speech
synthesis
</sectionHeader>
<bodyText confidence="0.9998689">
The speech synthesiser used must be capable of
reporting the timestamp of each word in the
synthesised string. These are two reasons for this.
First, it must be possible to resume speaking
after returning from the states INTERRUPTED and
HOLDING. Second, the AIC is responsible for
reporting what has actually been said by the
system back to the Discourse Modeller for
continuous self monitoring (there is a direct
feedback loop as can be seen in Figure 1). This
way, the Discourse Modeller may relate what the
system says to what the user says on a high
resolution time scale (which is necessary for
handling phenomena such as backchannels, as
discussed in Skantze &amp; Schlangen, 2009).
Currently, the system may pause and resume
speaking at any word boundary and there is no
specific prosodic modelling of these events. The
synthesis of interrupted speech is something that
we will need to improve.
</bodyText>
<figure confidence="0.993307333333333">
Google
Calendar
Speaker
Attention and Inter-
action Controller
Multimodal Speec
Synthesis
Utterance
Generation
Action
Manager
Display
Camera
Head
Tracker
GALATEA:
Discourse Modeller
PICKERING:
Semantic Parsing
Microphone
ASR
311
Timeout
Not attending
NonAttentive
Attending
SystemInitiative
Speaking
Calling
Pausing
Holding
System
User
Not attending
UserStopLook UserStartLook UserStartLook UserStartLook (resume) UserStopLook
SystemIgnore
Attentive
Listening
UserStartSpeak
SystemStopSpeak
SystemInitiative
SystemResponse
SystemResponse
SystemStopSpeak
Speaking
SystemAbortSpeak
UserStartSpeak SystemIgnore (resume)
SystemResponse (restart)
Interrupted
Attending
Speaking
</figure>
<figureCaption confidence="0.9998345">
Figure 2. The attention and interaction model. Dashed lines indicate events coming from input modules. Solid
lines indicate events from output modules. Note that some events and transitions are not shown in the figure.
</figureCaption>
<bodyText confidence="0.999954545454545">
An animated talking head is shown on a display,
synchronised with the synthesised speech
(Beskow, 2003). The head is making small con-
tinuous movements (recorded from real human
head movements), giving it a more life-like ap-
pearance. The head pose and facial gestures are
triggered by the different states and events in the
AIC, as can be seen in Figure 3. Thus, when the
user approaches the system and starts to look at it,
the system will look up, giving a clear signal that
it is now attending to the user and ready to listen.
</bodyText>
<subsectionHeader confidence="0.505907">
NonAttentive Attentive Listening SystemIgnore
</subsectionHeader>
<figureCaption confidence="0.9430395">
Figure 3. Examples of facial animations triggered by
the different states and events shown in Figure 2.
</figureCaption>
<sectionHeader confidence="0.994142" genericHeader="conclusions">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99991930952381">
In the evaluation, we not only wanted to check
whether the AIC model worked, but also to un-
derstand whether user attention could be effec-
tively modelled using head tracking. Similarly to
Oh et al. (2002), we wanted to compare &amp;quot;look-to-
talk&amp;quot; with &amp;quot;push-to-talk&amp;quot;. To do this, we used a
human-human-computer dialogue setting, where
a tutor was explaining the system to a subject
(shown in Figure 4). Thus, the subject needed to
frequently switch between speaking to the tutor
and the system. A second version of the system
was also implemented where the head tracker
was not used, but where the subject instead
pushed a button to switch between the attentional
states (a sort-of push-to-talk). The tutor first ex-
plained both versions of the system to the subject
and let her try both. The tutor gave the subjects
hints on how to express themselves, but avoided
to remind them about how to control the atten-
tion of the system, as this was what we wanted to
test. After the introduction, the tutor gave the
subject a task where both of them were supposed
to find a suitable slot in their calendars to plan a
dinner or lunch together. The tutor used a paper
calendar, while the subject used the MonAMI
Reminder. At the end of the experiment, the tutor
interviewed the subject about her experience of
using the system. 7 subjects (4 women and 3 men)
were used in the evaluation, 3 lab members and 4
elderly persons in the target group (recruited by
the Swedish Handicap Institute).
There was no clear consensus on which ver-
sion of the system was the best. Most subjects
liked the head tracking version better when it
worked but were frustrated when the head
tracker occasionally failed. They reported that a
combined version would perhaps be the best,
where head pose could be the main method for
handling attention, but where a button or a verbal
call for attention could be used as a fall-back.
When looking at the interaction from an objec-
tive point of view, however, the head tracking
</bodyText>
<page confidence="0.996769">
312
</page>
<figureCaption confidence="0.993721">
Figure 4. The human-human-computer dialogue set-
ting used in the evaluation. The tutor is sitting on the
left side and the subject on the right side
</figureCaption>
<bodyText confidence="0.999975170731707">
version was clearly more successful in terms of
number of misdirected utterances. When talking
to the system, the subjects always looked at the
system in the head tracking condition and never
forgot to activate it in the push-to-talk condition.
However, on average 24.8% of all utterances
addressed to the tutor in the push-to-talk condi-
tion were picked up by the system, since the user
had forgotten to deactivate it. The number of ut-
terances addressed to the tutor while looking at
the system in the head tracking condition was
significantly lower, only 5.1% on average (paired
t-test; p&lt;0.05).
These findings partly contradict findings from
previous studies, where head pose has not been
that successful as a sole indicator when the user
is looking at the system, as discussed in section 2
above. One explanation for this might be that the
subjects were explicitly instructed about how the
system worked. Another explanation is the clear
feedback (and entrainment) that the agent&apos;s head
pose provided.
Two of the elderly subjects had no previous
computer experience. During pre-interviews they
reported that they were intimidated by com-
puters, and that they got nervous just thinking
about having to operate them. However, after
only a short tutorial session with the spoken in-
terface, they were able to navigate through a
computerized calendar in order to find two
empty slots. We think that having a human tutor
that guides the user through their first interac-
tions with this kind of system is very important.
One of the tutor&apos;s tasks is to explain why the sys-
tem fails to understand out-of-vocabulary ex-
pressions. By doing this, the users&apos; trust in the
system is increased and they become less con-
fused and frustrated. We are confident that moni-
toring and modelling the user&apos;s attention is a key
component of spoken dialogue systems that are
to be used in tutoring settings.
</bodyText>
<sectionHeader confidence="0.988814" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.88766225">
This research is supported by MonAMI, an Integrated
Project under the European Commission&apos;s 6th Frame-
work Program (IP-035147), and the Swedish research
council project GENDIAL (VR #2007-6431).
</bodyText>
<sectionHeader confidence="0.991735" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999533363636363">
Bakx, I., van Turnhout, K., &amp; Terken, J. (2003). Fa-
cial orientation during multi-party interaction with
information kiosks. In Proceedings of the Interact
2003.
Beskow, J., Edlund, J., Granstrom, B., Gustafson, J.,
Skantze, G., &amp; Tobiasson, H. (2009). The
MonAMI Reminder: a spoken dialogue system for
face-to-face interaction. In Proceedings of Inter-
speech 2009.
Beskow, J. (2003). Talking heads - Models and appli-
cations for multimodal speech synthesis. Doctoral
dissertation, KTH, Department of Speech, Music
and Hearing, Stockholm, Sweden.
Bohus, D., &amp; Horvitz, E. (2009). Open-World Dialog:
Challenges, Directions, and Prototype. In Proceed-
ings of IJCAI&apos;2009 Workshop on Knowledge and
Reasoning in Practical Dialogue Systems. Pasade-
na, CA.
Katzenmaier, M., Stiefelhagen, R., Schultz, T., Rogi-
na, I., &amp; Waibel, A. (2004). Identifying the Ad-
dressee in Human-Human-Robot Interactions
based on Head Pose and Speech. In Proceedings of
ICMI2004.
Maglio, P. P., Matlock, T., Campbell, C. S., Zhai, S.,
&amp; Smith, B. A. (2000). Gaze and speech in atten-
tive user interfaces. In Proceedings ofICMI 2000.
Oh, A., Fox, H., Van Kleek, M., Adler, A., Gajos, K.,
Morency, L-P., &amp; Darrell, T. (2002). Evaluating
Look-to-Talk: A Gaze-Aware Interface in a Col-
laborative Environment. In Proceedings of CHI
2002.
Skantze, G., &amp; Schlangen, D. (2009). Incremental
dialogue processing in a micro-domain. In Pro-
ceedings of EACL-09. Athens, Greece.
Skantze, G. (2007). Error Handling in Spoken Dia-
logue Systems - Managing Uncertainty, Grounding
and Miscommunication. Doctoral dissertation,
KTH, Department of Speech, Music and Hearing,
Stockholm, Sweden.
Vertegaal, R., Slagter, R., van der Veer, G., &amp; Nijholt,
A. (2001). Eye gaze patterns in conversations:
there is more to conversational agents than meets
the eyes. In Proceedings of ACM Conf. on Human
Factors in Computing Systems.
</reference>
<page confidence="0.999509">
313
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705038">
<title confidence="0.930853">Attention and Interaction Control a Human-Human-Computer Dialogue Setting</title>
<author confidence="0.998102">Gabriel Skantze Joakim Gustafson</author>
<affiliation confidence="0.995137">Dept. of Speech Music and Hearing Dept. of Speech Music and Hearing</affiliation>
<address confidence="0.935032">KTH, Stockholm, Sweden KTH, Stockholm, Sweden</address>
<email confidence="0.876288">gabriel@speech.kth.sejocke@speech.kth.se</email>
<abstract confidence="0.99881125">This paper presents a simple, yet effective model for managing attention and interaction control in multimodal spoken dialogue systems. The model allows the user to switch attention between the system and other humans, and the system to stop and resume speaking. An evaluation in a tutoring setting shows that the user&apos;s attention can be effectively monitored using head pose tracking, and that this is a more reliable method than using push-to-talk.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Bakx</author>
<author>K van Turnhout</author>
<author>J Terken</author>
</authors>
<title>Facial orientation during multi-party interaction with information kiosks.</title>
<date>2003</date>
<booktitle>In Proceedings of the Interact</booktitle>
<marker>Bakx, van Turnhout, Terken, 2003</marker>
<rawString>Bakx, I., van Turnhout, K., &amp; Terken, J. (2003). Facial orientation during multi-party interaction with information kiosks. In Proceedings of the Interact 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Beskow</author>
<author>J Edlund</author>
<author>B Granstrom</author>
<author>J Gustafson</author>
<author>G Skantze</author>
<author>H Tobiasson</author>
</authors>
<title>The MonAMI Reminder: a spoken dialogue system for face-to-face interaction.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="3415" citStr="Beskow et al., 2009" startWordPosition="538" endWordPosition="541">lking he is most likely not addressing it. However, when the user looks at the system while speaking, there is a considerable probability that she is actually addressing a bystander. 3 The MonAMI Reminder This study is part of the 6th framework IP project MonAMI1. The goal of the MonAMI project is to develop and evaluate services for elderly and disabled people. Based on interviews with potential users in the target group, we have developed the MonAMI Reminder, a multimodal spoken dialogue system which can assist elderly and disabled people in organising and initiating their daily activities (Beskow et al., 2009). The dialogue system uses Google Calendar as a backbone to answer questions about events. However, 1 http://www.monami.info/ Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 310–313, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 310 it can also take the initiative and give reminders to the user. The MonAMI Reminder is based on the HIGGINS platform (Skantze, 2007). The architecture is shown in Figure 1. A microphone and a camera are used for system input (speech recognition an</context>
</contexts>
<marker>Beskow, Edlund, Granstrom, Gustafson, Skantze, Tobiasson, 2009</marker>
<rawString>Beskow, J., Edlund, J., Granstrom, B., Gustafson, J., Skantze, G., &amp; Tobiasson, H. (2009). The MonAMI Reminder: a spoken dialogue system for face-to-face interaction. In Proceedings of Interspeech 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Beskow</author>
</authors>
<title>Talking heads - Models and applications for multimodal speech synthesis.</title>
<date>2003</date>
<booktitle>Doctoral dissertation, KTH, Department of Speech, Music and Hearing,</booktitle>
<location>Stockholm, Sweden.</location>
<contexts>
<context position="8571" citStr="Beskow, 2003" startWordPosition="1360" endWordPosition="1361">tartLook UserStartLook (resume) UserStopLook SystemIgnore Attentive Listening UserStartSpeak SystemStopSpeak SystemInitiative SystemResponse SystemResponse SystemStopSpeak Speaking SystemAbortSpeak UserStartSpeak SystemIgnore (resume) SystemResponse (restart) Interrupted Attending Speaking Figure 2. The attention and interaction model. Dashed lines indicate events coming from input modules. Solid lines indicate events from output modules. Note that some events and transitions are not shown in the figure. An animated talking head is shown on a display, synchronised with the synthesised speech (Beskow, 2003). The head is making small continuous movements (recorded from real human head movements), giving it a more life-like appearance. The head pose and facial gestures are triggered by the different states and events in the AIC, as can be seen in Figure 3. Thus, when the user approaches the system and starts to look at it, the system will look up, giving a clear signal that it is now attending to the user and ready to listen. NonAttentive Attentive Listening SystemIgnore Figure 3. Examples of facial animations triggered by the different states and events shown in Figure 2. 5 Evaluation In the eval</context>
</contexts>
<marker>Beskow, 2003</marker>
<rawString>Beskow, J. (2003). Talking heads - Models and applications for multimodal speech synthesis. Doctoral dissertation, KTH, Department of Speech, Music and Hearing, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>Open-World Dialog: Challenges, Directions, and Prototype.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI&apos;2009 Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<location>Pasadena, CA.</location>
<contexts>
<context position="1158" citStr="Bohus &amp; Horvitz, 2009" startWordPosition="176" endWordPosition="179">. An evaluation in a tutoring setting shows that the user&apos;s attention can be effectively monitored using head pose tracking, and that this is a more reliable method than using push-to-talk. 1 Introduction Most spoken dialogue systems are based on the assumption that there is a clear beginning and ending of the dialogue, during which the user pays attention to the system constantly. However, as the use of dialogue systems is extended to settings where several humans are involved, or where the user needs to attend to other things during the dialogue, this assumption is obviously too simplistic (Bohus &amp; Horvitz, 2009). When it comes to interaction, a strict turn-taking protocol is often assumed, where user and system wait for their turn and deliver their contributions in whole utterance-sized chunks. If system utterances are interrupted, they are treated as either fully delivered or basically unsaid. This paper presents a simple, yet effective model for managing attention and interaction control in multimodal (face-to-face) spoken dialogue systems, which avoids these simplifying assumptions. We also present an evaluation in a tutoring setting where we explore the use of head tracking for monitoring user at</context>
<context position="5411" citStr="Bohus &amp; Horvitz (2009)" startWordPosition="874" endWordPosition="877">wn in the boxes can be regarded as the combined state of the system (columns) and the user (rows)2. Depending on the combined state, events from input and output components will have different effects. As can be seen in the figure, some combination of states cannot be realised, such as the system and user speaking at the same time (if the user speaks while the system is speaking, it will automatically change to the state INTERRUPTED). Of course, the user might speak while the system is speaking without the system detecting this, but 2 This is somewhat similar to the &amp;quot;engagement state&amp;quot; used in Bohus &amp; Horvitz (2009). the model should be regarded from the system&apos;s perspective, not from an observer. The user&apos;s attention is monitored using a camera and an off-the-shelf head tracking software. As the user starts to look at the system, the state changes from NONATTENTIVE to ATTENTIVE. When the user starts to speak, a UserStartSpeak event from the ASR will trigger a change to the LISTENING state. The Action Manager might then trigger a SystemResponse event (together with what should be said), causing a change into the SPEAKING state. Now, if the user would look away while the system is speaking, the system wou</context>
</contexts>
<marker>Bohus, Horvitz, 2009</marker>
<rawString>Bohus, D., &amp; Horvitz, E. (2009). Open-World Dialog: Challenges, Directions, and Prototype. In Proceedings of IJCAI&apos;2009 Workshop on Knowledge and Reasoning in Practical Dialogue Systems. Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Katzenmaier</author>
<author>R Stiefelhagen</author>
<author>T Schultz</author>
<author>I Rogina</author>
<author>A Waibel</author>
</authors>
<title>Identifying the Addressee in Human-Human-Robot Interactions based on Head Pose and Speech.</title>
<date>2004</date>
<booktitle>In Proceedings of ICMI2004.</booktitle>
<contexts>
<context position="2612" citStr="Katzenmaier et al. (2004)" startWordPosition="403" endWordPosition="406">nce (Vertegaal et al., 2001). When it comes to human-machine interaction, Maglio et al. (2000) showed that users tend to look at speechcontrolled devices when talking to them, even if they do not have the manifestation of an embodied agent. Bakx et al. (2003) investigated the use of head pose for identifying the addressee in a multi-party interaction between two humans and an information kiosk. The results indicate that head pose should be combined with acoustic and linguistic features such as utterances length. Facial orientation in combination with speechrelated features was investigated by Katzenmaier et al. (2004) in a human-human-robot interaction, confirming that a combination of cues was most effective. A common finding in these studies is that if a user does not look at the system while talking he is most likely not addressing it. However, when the user looks at the system while speaking, there is a considerable probability that she is actually addressing a bystander. 3 The MonAMI Reminder This study is part of the 6th framework IP project MonAMI1. The goal of the MonAMI project is to develop and evaluate services for elderly and disabled people. Based on interviews with potential users in the targ</context>
</contexts>
<marker>Katzenmaier, Stiefelhagen, Schultz, Rogina, Waibel, 2004</marker>
<rawString>Katzenmaier, M., Stiefelhagen, R., Schultz, T., Rogina, I., &amp; Waibel, A. (2004). Identifying the Addressee in Human-Human-Robot Interactions based on Head Pose and Speech. In Proceedings of ICMI2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Maglio</author>
<author>T Matlock</author>
<author>C S Campbell</author>
<author>S Zhai</author>
<author>B A Smith</author>
</authors>
<title>Gaze and speech in attentive user interfaces.</title>
<date>2000</date>
<booktitle>In Proceedings ofICMI</booktitle>
<contexts>
<context position="2081" citStr="Maglio et al. (2000)" startWordPosition="318" endWordPosition="321">sents a simple, yet effective model for managing attention and interaction control in multimodal (face-to-face) spoken dialogue systems, which avoids these simplifying assumptions. We also present an evaluation in a tutoring setting where we explore the use of head tracking for monitoring user attention, and compare it with a more traditional method: push-totalk. 2 Monitoring user attention In multi-party dialogue settings, gaze has been identified as an effective cue to help disambiguate the addressee of a spoken utterance (Vertegaal et al., 2001). When it comes to human-machine interaction, Maglio et al. (2000) showed that users tend to look at speechcontrolled devices when talking to them, even if they do not have the manifestation of an embodied agent. Bakx et al. (2003) investigated the use of head pose for identifying the addressee in a multi-party interaction between two humans and an information kiosk. The results indicate that head pose should be combined with acoustic and linguistic features such as utterances length. Facial orientation in combination with speechrelated features was investigated by Katzenmaier et al. (2004) in a human-human-robot interaction, confirming that a combination of</context>
</contexts>
<marker>Maglio, Matlock, Campbell, Zhai, Smith, 2000</marker>
<rawString>Maglio, P. P., Matlock, T., Campbell, C. S., Zhai, S., &amp; Smith, B. A. (2000). Gaze and speech in attentive user interfaces. In Proceedings ofICMI 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oh</author>
<author>H Fox</author>
<author>M Van Kleek</author>
<author>A Adler</author>
<author>K Gajos</author>
<author>L-P Morency</author>
<author>T Darrell</author>
</authors>
<title>Evaluating Look-to-Talk: A Gaze-Aware Interface in a Collaborative Environment.</title>
<date>2002</date>
<booktitle>In Proceedings of CHI</booktitle>
<marker>Oh, Fox, Van Kleek, Adler, Gajos, Morency, Darrell, 2002</marker>
<rawString>Oh, A., Fox, H., Van Kleek, M., Adler, A., Gajos, K., Morency, L-P., &amp; Darrell, T. (2002). Evaluating Look-to-Talk: A Gaze-Aware Interface in a Collaborative Environment. In Proceedings of CHI 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>D Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09.</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="7363" citStr="Skantze &amp; Schlangen, 2009" startWordPosition="1206" endWordPosition="1209">the timestamp of each word in the synthesised string. These are two reasons for this. First, it must be possible to resume speaking after returning from the states INTERRUPTED and HOLDING. Second, the AIC is responsible for reporting what has actually been said by the system back to the Discourse Modeller for continuous self monitoring (there is a direct feedback loop as can be seen in Figure 1). This way, the Discourse Modeller may relate what the system says to what the user says on a high resolution time scale (which is necessary for handling phenomena such as backchannels, as discussed in Skantze &amp; Schlangen, 2009). Currently, the system may pause and resume speaking at any word boundary and there is no specific prosodic modelling of these events. The synthesis of interrupted speech is something that we will need to improve. Google Calendar Speaker Attention and Interaction Controller Multimodal Speec Synthesis Utterance Generation Action Manager Display Camera Head Tracker GALATEA: Discourse Modeller PICKERING: Semantic Parsing Microphone ASR 311 Timeout Not attending NonAttentive Attending SystemInitiative Speaking Calling Pausing Holding System User Not attending UserStopLook UserStartLook UserStartL</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Skantze, G., &amp; Schlangen, D. (2009). Incremental dialogue processing in a micro-domain. In Proceedings of EACL-09. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
</authors>
<title>Error Handling in Spoken Dialogue Systems - Managing Uncertainty, Grounding and Miscommunication. Doctoral dissertation, KTH, Department of Speech, Music and Hearing,</title>
<date>2007</date>
<location>Stockholm, Sweden.</location>
<contexts>
<context position="3900" citStr="Skantze, 2007" startWordPosition="615" endWordPosition="616">gue system which can assist elderly and disabled people in organising and initiating their daily activities (Beskow et al., 2009). The dialogue system uses Google Calendar as a backbone to answer questions about events. However, 1 http://www.monami.info/ Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 310–313, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 310 it can also take the initiative and give reminders to the user. The MonAMI Reminder is based on the HIGGINS platform (Skantze, 2007). The architecture is shown in Figure 1. A microphone and a camera are used for system input (speech recognition and head tracking), and a speaker and a display are used for system output (an animated talking head). This is pretty much a standard dialogue system architecture, with some exceptions. Dialogue management is split into a Discourse Modeller and an Action Manager, which consults the discourse model and decides what to do next. There is also an Attention and Interaction Controller (AIC), which will be discussed next. Figure 1. The system architecture in the MonAMI Reminder. 4 Attentio</context>
</contexts>
<marker>Skantze, 2007</marker>
<rawString>Skantze, G. (2007). Error Handling in Spoken Dialogue Systems - Managing Uncertainty, Grounding and Miscommunication. Doctoral dissertation, KTH, Department of Speech, Music and Hearing, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vertegaal</author>
<author>R Slagter</author>
<author>G van der Veer</author>
<author>A Nijholt</author>
</authors>
<title>Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM Conf. on Human Factors in Computing Systems.</booktitle>
<marker>Vertegaal, Slagter, van der Veer, Nijholt, 2001</marker>
<rawString>Vertegaal, R., Slagter, R., van der Veer, G., &amp; Nijholt, A. (2001). Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes. In Proceedings of ACM Conf. on Human Factors in Computing Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>