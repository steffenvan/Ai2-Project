<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001049">
<title confidence="0.859353">
Considering Automatic Aids to Corpus Annotation
</title>
<author confidence="0.604286">
David Day and Benjamin Weliner
</author>
<affiliation confidence="0.423008">
MITRE Corporation
</affiliation>
<address confidence="0.754218">
Mail Stop K329
202 Burlington Road
Bedford, MA 01730, USA
</address>
<email confidence="0.722165">
dayAmitre.org, wellnerOmitre.org
</email>
<sectionHeader confidence="0.960003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999628565217391">
In this paper we view mixed-initiative corpus
annotation from the perspective of knowl-
edge engineering, and discuss some of the
opportunities, challenges and dangers that are
presented by using mixed-initiative annotation
tools. We begin this discussion by describing
an existing mixed-initiative annotation tool
for open-ended phrase-level annotation, the
Alembic Workbench. We discuss how this
tool currently operates, the nature of its skill
acquisition component, and our plans to extend
it in a number of ways, including incorporating
an active learning capability. Having set the
stage with a concrete example, we identify a
number of opportunities and challenges that
are presented by the mixed-initiative approach
to corpus annotation, including the benefits
that might accrue when supporting &amp;quot;layered&amp;quot;
annotation environments, the adoption of
intensional/procedural annotation paradigms,
the inclusion of lexical resource construction
interleaved with corpus annotation, and other
topics.
</bodyText>
<sectionHeader confidence="0.996392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872309523809">
One way of viewing corpus annotation is as a
form of &amp;quot;knowledge engineering,&amp;quot; where the an-
notator intends to enable a machine to repro-
duce the behavior being performed. A motiva-
tion for adopting such a view is that there is
a practical interest in having machines be able
to automatically perform some types of annota-
tion. For example, &amp;quot;named entity&amp;quot; tagging, the
ability to identify proper names that refer to en-
tities of a particular restricted set of semantic
classes (e.g., person, location, organization) was
initially developed merely as a means to mea-
sure the contribution of this stage of linguis-
tic analysis to a set of more complex domain-
specific information extraction tasks. In recent
years this capability has been shown to be valu-
able as a constituent to quite different informa-
tion processing tasks, including topic detection
and tracking, information retrieval, and others.
Another motivation for automating the an-
notation process is simply to increase the pro-
ductivity of the corpus annotation process it-
self. Even if the ultimate goal of a particular
annotation process is to build a static reposi-
tory of annotated data to support fundamental
linguistic research and analysis, there is a great
benefit in producing as much of a given type of
annotation as possible within restricted sched-
ules and budgets. In general, the greater the
size of the corpus, the more informed and statis-
tically well-founded are the conclusions that can
be drawn. From the point of view of knowledge
engineering, most forms of corpus annotation
involve a model of &amp;quot;learning by example,&amp;quot; where
some number of positive examples are meant to
drive the skill acquisition component. In prac-
tice this skill acquisition is often carried out by a
mix of human engineering (e.g., programming),
machine-aided analysis, and machine learning
techniques when possible. In this paper we want
to expand on this skill acquisition model in a
number of ways:
</bodyText>
<listItem confidence="0.926299571428571">
• Argue how these techniques can and should
be applied across the full range of linguistic
annotation tasks.
• Expand the notion of &amp;quot;mixed initiative&amp;quot;
(or &amp;quot;incremental bootstrapping&amp;quot;) annota-
tion to incorporate not just learning by ex-
ample, but other methods that increase the
</listItem>
<page confidence="0.995257">
71
</page>
<note confidence="0.5343435">
Mixed Initiative Annotation Methodology
Used in the Alembic Workbench
</note>
<figure confidence="0.997379608695652">
3 Apply phrase-
finding rules
to raw text
Rule
Sets
If ...
Then...
Manually
1
annotate
raw text
Source
Texts
2
4
Invoke machine
learning to derive
annotation rules
Manually correct
machine-annotated
text
Training &amp; Test
Corpus
</figure>
<bodyText confidence="0.995430205882353">
might be best described as &amp;quot;phrasal&amp;quot; any
contiguous character sequence or multi-word
sequence may be annotated by the user and
associated with some &amp;quot;tag.&amp;quot; The Workbench
has been extended for a number of special
purpose and general purpose annotation tasks,
such as MUC6-style co-reference tagging, and
general &amp;quot;relational&amp;quot; annotations. Neither of
these tagging enhancements have yet been
closely integrated with our machine-learning
techniques even though we are presently
working on machine-learning approaches to
both of these problems outside the specific
scope of mixed-initiative annotation. These
tags may be defined using the SGML/XML
mechanisms for specifying generic identifiers
of annotation elements and their associated
attribute/value pairs. While this notational
scheme allows for the expression of complex
relationships among particular tags, the
machine learning component currently used
to support mixed-initiative annotation in
the Workbench ignores these subtleties and
reduces all SGML/XML elements with distinct
structures into what are essentially unique,
un-interpreted symbols.
The transformation-based rule sequence
learning technique used in Alembic has proved
very effective for deriving accurate annotation
performance on the basis of exceedingly
impoverished amounts of training data. We
have observed F-measures in the range of
55-75 within 15-20 minutes of beginning a
new annotation task in both English and
Spanish texts, and lower but still helpful
values in the same amount of annotation of
Chinese and Japanese texts. (See section
3.1 for more background on the phrase-rule
learning behavior of Alembic.) This is
important in producing enhanced productivity
through bootstrapping an automatic tagging
procedure. The earlier the machine can provide
pre-annotated data at a reasonable level of
performance, the faster the combined activities
of human and machine can build up a large
corpus. In addition, the precision and recall
of the automatically derived rule sequences
increase as the size of the available training
corpus increases. The shape of this learning
curve is invariably asymptotic. The asymptote
peaks at different levels (on blind test data) for
different tasks and/or for different parameters
of the learning environment, for reasons
that are important, but not always easy to
determine. The two main reasons appear to
be the size and typicality of the corpus, and
the representational power of the rule patterns
available to the rule learner.
In building our current transformation-based
learning (TBL) system we have made a num-
ber of design decisions that have had an impact
on training speed. These decision have to do
with the need to actually apply newly induced
rules to the training data and subsequently re-
compute the corpus-wide statistics that drive
the next rule induction cycle. While the sys-
tem is able to achieve very fast learning times
(5-30 minutes) on small to moderate amounts of
training data (5,000 to 75,000 exemplars), much
longer training times result when the training
corpus reaches hundreds of thousands or even
millions of exemplars.
Of course, there is no need to iterate the
learning algorithm over all the training data af-
ter each document. Indeed, it seems reasonable
and practical to invoke the learning algorithm
less and less frequently as the size of the corpus
increases and the performance of the automat-
ically derived rule sequences differ less and less
from each other with each incremental addition
of a human-annotated file. In order to avoid
having to restart the learning algorithm from an
initial null state after each mixed-initiative in-
vocation of the learning algorithm, it is straight-
forward within a TBL model to begin learning
on top of of an existing level of competence (a
previously derived rule sequence). In this case
the training corpus might consist of only those
few files that have been annotated since the last
learning procedure was called, and the newly de-
rived rules are concatenated onto the end of the
existing rule sequence. However, eventually it is
desirable to start fresh, since it becomes more
and more likely that new opportunities for gen-
eralization can be found in a larger training set,
leading to increases in the ability of the new rule
sequence to apply successfully to unseen data.
Nonetheless, we are also interested ad-
dressing the problem of learning performance
directly. One approach we intend to pursue
is the incorporation of MITRE&apos;s HMM-based
&amp;quot;Phrag&amp;quot; (Palmer et al., 1999) phrase-parsing
</bodyText>
<page confidence="0.993455">
73
</page>
<bodyText confidence="0.982802411764706">
learner within the Workbench&apos;s mixed-initiative
repertoire, which we imagine could be increas-
ingly relied upon as the size of the training set
reaches very large proportions.
Currently the default &amp;quot;granularity&amp;quot; of mixed-
initiative annotation within the Workbench is
that of a document or file. As long as a single file
is fully annotated, it can be used as the basis of
phrase-rule learning, either alone or in combina-
tion with a corpus consisting of other annotated
documents/files. Of course, documents can be
arbitrarily reduced to smaller chunks if there is
a strong need for this. Ideally one would like the
granularity used in mixed-initiative annotation
(1) to be identified and adopted directly by the
system itself, rather than relying on the annota-
tor to make such decisions; and (2) to be a func-
tion of the annotation task being performed.
For example, phrase tagging (and many other
annotation tasks such as sentence parsing) could
be segmented at the sentence level. Updating
the existing pre-annotation procedure could be
invoked based on the amount of performance
improvement achieved in the previous two invo-
cations, as well as other heuristics that design-
ers might identify. Other annotation tasks, such
as co-reference annotation, discourse structure
and entity and relation extraction, etc., might
require segmentation at the document level.
3.1 Why Alembic phrase rule learning
appears to work
In the past few years we have often been sur-
prised at the ability of Alembic&apos;s phrase-rule
learning apparatus to create quite reasonable
tagging performance with only meager amounts
of data annotated to the user&apos;s specifications.
We have often had cause to wonder: Was our
learning algorithm and associated Alembic in-
frastructure really so good? How did it squeeze
out such good performance (e.g., around 70-
75 F-measure) on such a paltry example base
as 1,500 words of annotated Spanish newswire
text? We would like to perform a detailed anal-
ysis, but our informal conclusions are already
leading us to establish new priorities in our at-
tempts to build rapidly portable natural lan-
guage processing capabilities. We devote a sub-
section to each of these conclusions below.
3.1.1 The &amp;quot;right&amp;quot; level of analysis
Like many other systems designed in the
course of the last five years, Alembic has
been built using a number of important
natural language processing components,
each of which had become newly available
in the previous years. These components
include tokenization (word segmentation),
sentence tagging, and morphological analysis
(part-of-speech tagging). Empirically it has
become clear that many useful types of
general-purpose and specialized phrase tagging
tasks (from named entity tagging to sentence
chunking) can find all of the information
they need from this mix of information made
available by pre-processing. In other words,
this is due to a successful application of the
&amp;quot;divide and conquer&amp;quot; principle adopted by
the computational linguistics community as a
whole over the past ten years.
</bodyText>
<subsectionHeader confidence="0.937638">
3.1.2 Locality of influence
</subsectionHeader>
<bodyText confidence="0.991132444444444">
In a similar vein, these same tagging tasks (per-
haps best exemplified by &amp;quot;named entity tag-
ging&amp;quot;) have adopted a decision environment in
which a fairly strict locality of influence is re-
spected, and this locality has been sufficient for
addressing the phrasal phenomena of interest.
Not only has this seemed to be true for the rule
schemata used in rule-based tagging systems,
but also for the modeling techniques adopted
in Hidden-Markov Model approaches to phrase
tagging as well.
3.1.3 The &amp;quot;right&amp;quot; lexical resources and
built-in predicates
We believe that the most specific reason that
the Alembic phrase rule learner has managed
to perform so well with very limited amounts
of training data has been the considerable lex-
ical resources that we have made available to
the learner. It so happens that when Alembic
has exhibited these surprisingly good learning
behaviors it is often the case that the result-
ing rules include a liberal mixture of references
to one or another of the special-purpose word
lists that we have developed in the course of
manually building various natural language pro-
cessing capabilities. (Other frequently occur-
ring rule patterns exhibited in successful rule
</bodyText>
<page confidence="0.993546">
74
</page>
<bodyText confidence="0.999974290909091">
sequences are those making use of the part-of-
speech of lexical items. In CJK languages par-
ticular character prefixes and suffixes are also
highly represented.)
These word lists are derived in a wide va-
riety of ways: names extracted from the US
Census; hand-coded lists expanded from core
words easily predicted to be contextually im-
portant markers; expansions of words using the-
sauri, dictionaries or similar resources; words
found from an analysis of the internal and exter-
nal contexts of annotated phrases in manually
tagged training data (supervised context analy-
sis); and sometimes words found in these same
contexts but from large collections of automat-
ically tagged data (unsupervised context analy-
sis). Regardless of the particulars of how they
are derived, these resources allow for a boost
in the generality of rules learned from a small
corpus. While the learner might happen to pick
references to these word lists for purely local
(and perhaps almost arbitrary) reasons in the
context of some very small annotated corpus,
this serendipity will lead to many more correct
applications when different word choices are en-
countered in previously unseen data.
Our current presumption is that replicating
efficient mixed-initiative successes for other
tasks and in other arenas of language processing
will rely heavily on providing similar advantages
as those identified above. For example, in order
to support the rapid mixed-initiative annotation
of certain types of relation/event data (e.g., the
&amp;quot;template relation&amp;quot; and &amp;quot;scenario template&amp;quot;
tasks of the various MUC evaluations (Def,
1995; Grishman and Sundheim, 1996)), one
must make available to the learning component
the same notion of &amp;quot;locality&amp;quot; as is warranted
for such distinct phenomena. This kind of
locality might be exemplified by an interme-
diate &amp;quot;SVO&amp;quot; (Subject/Verb/Object/modifier)
representation of a given sentence, which could
be derived in a variety of ways, either via
treebank-style parses, or from dependency-
like syntactic models such as &amp;quot;grammatical
relations.&amp;quot;). We are particularly interested
in merging the mixed-initiative development
of lexical resources with the mixed-initiative
development of annotated corpora. (Previous
work of others in this area includes (Riloff and
Jones, 1999; Blum and Mitchell, 1998).) We
anticipate that a host of unsupervised learning
techniques will be especially useful in helping
to quickly bootstrap the acquisition of useful
word lists.
</bodyText>
<sectionHeader confidence="0.989136" genericHeader="method">
4 Active learning
</sectionHeader>
<bodyText confidence="0.999871933333333">
One way of increasing the effective productiv-
ity of the human annotator while holding the
capabilities of the skill acquisition component
constant is by increasing the utility of the anno-
tated data being supplied to the skill acquisition
component. In the event that bootstrapping
is being performed manually through heuris-
tic insights, the annotator may try to tune the
corpus sampling mechanism to favor sentences,
paragraphs or documents that would seem to
provide the greatest opportunity for instructing
and testing the emerging automated annotation
component. It is also possible to perform this
sample selection of raw data through automatic
means. This interplay between learner and ex-
ample selection is sometimes referred to as &amp;quot;ac-
tive learning&amp;quot; (or sample selection). (Lewis and
Catlett, 1994)
Engelson and Dagan (Engelson and Dagan,
1996) demonstrated an automatic method for
selecting part-of-speech training sentences us-
ing a votes from a set of automated annotation
&amp;quot;experts.&amp;quot; This and other work prompted us
to look at how such techniques could be incor-
porated into the Workbench&apos;s mixed-initiative
model. Alembic&apos;s phrase-rule learner contains
a number of parameters that are well suited to
construct such a family of experts.
The basic insight of active learning is that not
all training data are equally informative, and
that the &amp;quot;confidence&amp;quot; of the induced decision
system in classifying (tagging) some particular
exemplar is inversely proportional to the likely
utility of that exemplar, were it to be correctly
classified. If a particular annotation decision is
made very confidently, it is likely due to the
fact that many exemplars have informed the
decision rule, and so increased the associated
level of confidence. But how is &amp;quot;confidence&amp;quot; ex-
pressed in transformational rule sequences? In
most cases, there is no analog to confidence in
transformation rule sequences. However, if one
can build a mixture of experts, then one ana-
log to confidence in such systems is the num-
ber of experts that voted for the same tagging
</bodyText>
<page confidence="0.993815">
75
</page>
<listItem confidence="0.996544375">
1. Induce N different decision criteria by using
varying parameter values.
2. Apply N decision criteria to unseen data.
3. Select for manual annotation those sen-
tences for which there are sufficiently di-
vergent classifications.
4. Annotate manually (with or without pre-
tagging).
</listItem>
<figureCaption confidence="0.9963415">
Figure 2: Active learning algorithm used in
Alembic Workbench experiments
</figureCaption>
<bodyText confidence="0.966807339285714">
decision independent of the nature of the de-
cision mechanisms used in the constituent de-
cision systems. The basic active learning algo-
rithm used in our recent experiments with the
Workbench is presented in Figure 2.
The Alembic transformation-based rule learn-
ing algorithm selects a rule at each epoch of
the learning algorithm. We have experimented
with a number of evaluation criteria for this step
of the process: &amp;quot;yield minus sacrifice&amp;quot; (count
the number of new, correct annotations created
by applying a rule, then subtracting from this
value the number of incorrect annotations cre-
ated by applying this same rule); &amp;quot;log likeli-
hood:&amp;quot; and &amp;quot;F-measure&amp;quot; (harmonic mean of the
recall and precision measures for this rule), pa-
rameterized by beta, which indicates the relative
weight given to the recall measure compared to
the precision measure. We eventually adopted
the F-measure approach, not only because it
tended to give us the best empirical results on
the problems we are addressing at that time,
but also because it provided us the opportunity
to transparently weight the performance more
towards recall or more towards precision, which
can be an important practical difference in var-
ious real world application contexts.
Varying the decision criterion by varying the
beta value of the objective function allows us to
easily define sets of experts from which &amp;quot;confi-
dence&amp;quot; measures can be induced through their
level of agreement. Indeed, the F-measure met-
ric alone offers the opportunity for deriving a
family of decision experts simply by modify-
ing the single beta parameter. We would also
like to use the Phrag HMM-based tagger on
the same data to create an expert with a quite
different bias. We are in the early stages of
experimenting with this form of active learn-
ing, selecting sentences and/or documents on
the basis of the degree to which multiple sep-
arately derived rule sequence &amp;quot;experts&amp;quot; agree
on annotation assignments. These early results
are encouraging. From an initial training set of
442 sentences (containing 705 target phrases), a
subsequent unannotated corpus of 1,462 words
was used as the universe of possible sentences
for subsequent manual annotation. Approxi-
mately 10% were down-selected based on two
different criteria: random selection or using low
confidence measures as derived from voting as
described above. Following the manual annota-
tion of these two incremental additions to the
training set, we observed that the performance
of separately trained automatic taggers differed
on test data by about 5% .1
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.994647692307692">
Corpus annotation has implications not just
for providing productivity enhancements for
linguists (computational and otherwise), but
also as a model for how useful information
extraction systems (an important class of
intelligent agents) can be derived through a
largely example-based knowledge engineer-
ing/acquisition process. With both of these
contexts in mind, it is useful to reflect on
some of the outstanding opportunities in
mixed-initiative annotation, as well as the
difficulties and dangers that accompany them.
5.1 Layered annotations for
multi-staged mixed-initiative
corpus development
Some annotation tasks depend so strongly on
&amp;quot;earlier&amp;quot; annotations that it will become impor-
tant to build annotation environments in which
these earlier annotation &amp;quot;layers&amp;quot; are made ap-
parent to the annotator. For example, when
annotating grammatical relations (Ferro et al.,
1999; Ferro, 1998), the job of the annotator is
to establish various pre-specified types of rela-
tionships among sentence &amp;quot;chunks,&amp;quot; where these
chunks consist of simple phrases such as &amp;quot;noun
group,&amp;quot; &amp;quot;verb group,&amp;quot; &amp;quot;preposition group,&amp;quot; and
</bodyText>
<footnote confidence="0.647210666666667">
1-We are in the midst of our explorations of this task;
we hope to be able to report the results of more robust
experiments soon.
</footnote>
<page confidence="0.979891">
76
</page>
<bodyText confidence="0.986537086956522">
the like. Thus, instead of being presented with
a text in a standard Workbench textual display
(and being able to draw relationships between
arbitrary pairs of words), it is important that
these sub-groupings are already visually appar-
ent and made to control the interface so that
asserting only group-level relationships is possi-
ble.2
Other opportunities for such interdependence
of annotation tasks can be seen when annotat-
ing discourse-level relations and events (e.g.,
MUG-style &amp;quot;template relations&amp;quot; and &amp;quot;scenario
templates&amp;quot;). While particular relationships
may be asserted in a variety of ways, the ability
to view and operate directly on, for example,
an &amp;quot;SVO&amp;quot; (subject-verb-object-modifier) rep-
resentation of a set of sentences might enhance
not only the productivity of the annotator, but
also build in important links across processing
levels that are important to one&apos;s method of
attacking a given computational linguistics
problem. This ability to build upon the layers
of annotation derived previously will become an
increasingly important technique for building
mixed-initiative annotation tools. It could
prove especially fruitful in the support for a
richer language by which the human annotator
can directly influence the mixed-initiative
process, as discussed in the next section.
5.2 From extensional to intensional
annotation methods
We remarked earlier about how the bootstrap-
ping of annotation can incorporate not just au-
tomatically derived annotation heuristics but
also those derived from the human annotator,
implemented usually as computer programs or
simply regular expression macros. This has
been a method frequently relied on within the
computational linguistics community, since the
skills for deriving the heuristics and implement-
ing them as procedures are readily available.
One of the open problems of mixed-initiative an-
notation environments is to provide some kind
of support for more direct human intervention
in the bootstrapping process other than simply
adding yet another example. Of course, there a
</bodyText>
<footnote confidence="0.934746333333333">
2Such an annotation tool has been developed specifi-
cally for the grammatical relations annotation task being
performed internally at MITRE.
</footnote>
<bodyText confidence="0.999782365853658">
wide variety of pattern languages and annota-
tion representations from which those inclined
to write pre-annotation heuristics can choose.
But are there ways in which the results of such
heuristic annotation methods can be viewed and
combined with example based annotations with-
out creating confusion?
For example, if someone composes a rule and
it applies to one hundred instances within a
corpus, the annotator might like to view the
resulting sentences directly perhaps within a
keyword-in-context type viewer that is also in-
tegrated directly with the extensional annota-
tion environment. This way exceptions to this
rule can be noticed and modified easily and di-
rectly by the annotator. If this were truly a
mixed-initiative environment, then such a sys-
tem might on the next cycle derive a rule which
starts with the human-authored heuristic, but
derives rules (or some other representation) for
capturing the exceptions identified extension-
ally by the annotator.
Interestingly, there has recently been a very
careful empirical study (Brill and Ngai, 1999;
Ngai and Yarowsky, 2000) exploring the ad-
vantages and disadvantages of extensional and
intensional mixed-initiative methods for anno-
tating a corpus. This study was carried out
by Grace Ngai and David Yarowsky at Johns
Hopkins University, and compared the abilities
of relatively sophisticated pattern rule authors
against machine learning methods for deriving
tagging rules in a mixed-initiative annotation
environment. The results indicate that rule-
writing, while intuitively powerful, may prove
difficult for supporting a mixed-initiative ap-
proach to corpus annotation. This is a provoca-
tive study, seeming at variance with our intu-
itions as computational linguists. The annota-
tion community should explore these issues and
discuss them fully.
</bodyText>
<subsectionHeader confidence="0.66478">
5.3 The real world of task definition
</subsectionHeader>
<bodyText confidence="0.997225">
and collaborative development
In our own case studies and in our research fo-
cussed on mixed-initiative annotation we have
often concentrated on well-defined annotation
tasks and how they can most quickly be auto-
mated. In the real world, however, we know
quite well from first-hand experience that the
annotation process is a very long and tortuous
</bodyText>
<page confidence="0.994982">
77
</page>
<bodyText confidence="0.993714631578948">
road, where many of the initial steps are con-
cerned less with getting large amounts of an-
notated data quickly, but rather with exploring
the very definition of the task at hand. As many
of the contributors to formal language process-
ing evaluations will tell you, much of the diffi-
culty in starting up a new tagging task is due to
the social and linguistic barriers to easy catego-
rization. So how do the techniques we have de-
scribed support and/or improve such task defi-
nition endeavors?
At the heart of any collaborative annota-
tion effort is the detailed analysis and associ-
ated discussion of different interpretations of
the linguistic phenomena, which is most often
captured and brought to light through inter-
annotator annotation analysis. At first this
analysis is largely qualitative, and depends on
detecting the anomalies in order to promote
their discussion. Recently there has been a
study of this collaborative behavior, and an as-
sociated automated method was developed that
was modeled on it (Wiebe et al., 1999). Subse-
quently the emphasis moves towards quantita-
tive inter-annotator analysis and the categoriza-
tion of those differences. In both of these phases
techniques that can boost the number and kinds
of linguistic artifacts that have been annotated
by one person or another can only help in the
process of annotation understanding and inter-
annotator reconciliation. Of course, it cannot
sidestep the necessity of discussion and reflec-
tion that is necessary to come to terms with the
motivations and other issues relevant to a new
annotation task.
Nonetheless, there are clearly opportunities
and challenges for mixed-initiative techniques
that respect the collaborative nature of the
annotation process. One area of interest is
in building new automatic annotators by com-
bining the existing annotation capabilities de-
rived from separate human annotators interact-
ing with mixed-initiative systems. For example,
one could imagine new collaborative tasks could
be defined through the application and analy-
sis of distinct skills (tagging procedures, rule
sequences, etc.) derived independently. This
same ability may be appropriate for trying to
identify and adapt to the inevitable &amp;quot;concept
shift&amp;quot; that occurs with computational artifacts
put to use on a daily basis.
5.4 The Tension between Naturally
Occurring Phenomena and
Focussed Inquiry
There is a potential danger that attends any
technique that introduces labor saving meth-
ods, and mixed-initiative annotation is no ex-
ception. One of the most important problems
is predicted to lie in the area of recall. As the
automated pre-annotation process increases its
capabilities, there will be a psychological ten-
dency of human annotators to trust its guesses.
And while precision errors will be fairly easy to
spot (since the machine will display some text
and assign a fallacious tag to it), recall errors
errors of omission cannot be highlighted in
principle, and so requires the human annotator
to be forever vigilant and to notice &amp;quot;the tag that
wasn&apos;t.&amp;quot; This problem is perhaps accentuated
even more with the adoption of active learning
techniques. It is not known to what extent the
introduction of active learning might introduce
a vicious cycle of ignorance, whereby recall er-
rors are never corrected due to tacit agreement
(aligned errors) from all of the constituent de-
cision components.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999950666666667">
There are still opportunities for building, re-
fining and applying mixed-initiative corpus an-
notation tools and environments. In this pa-
per we have identified some of these opportuni-
ties, the challenges they pose and their poten-
tial for unintentional side effects. We grounded
this discussion with a description of the Alem-
bic Workbench tool, describing its current capa-
bilities and the direction of our research to ex-
pand them. Successful mechanisms for quickly
deriving machine-aided corpus annotation sys-
tems will have an important impact on the cor-
pus linguistics research community. It will also
lead eventually to portable, trainable language
processing systems for use by non-specialists to
perform customized information discovery and
extraction from the glut of information available
today.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995823">
John Aberdeen, John Burger, David Day,
Lynette Hirschman, Patricia Robinson, and
Marc Vilain. 1995. Description of the alem-
bic system used for MUC-6. In Proceedings of
</reference>
<page confidence="0.974446">
78
</page>
<reference confidence="0.999793351648352">
the Sixth Message Understanding Conference
(MUC-6), pages 141-155, Columbia, Mary-
land, November.
Scott W. Bennett, Chinatsu Aone, and Craig
Lovell. 1997. Learning to tag multilingual
texts through observation. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-97),
Providence, RI, USA.
A. Blum and T. Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In
In Proceedings of the 11th Annual Conference
on Computational Learning Theory. ACM.
Thorsten Brants, Wojciech Skut, and Brigitte
Krenn. 1997. Tagging grammatical func-
tions. In Proceedings of the Conference on
Empirical Methods in Natural Language Pro-
cessing (EMNLP-97), Providence, RI, USA.
Eric Brill and Grace Ngai. 1999. Man vs ma-
chine: A case study in base noun phrase
learning. In Proceedings of Association of
Computational Linguistics. Association of
Computational Linguistics.
Eric Brill. 1993. A Corpus-Based Approach to
Language Learning. Ph.D. thesis, University
of Pennsylvania, Philadelphia, Penn.
David Day, John Aberdeen, Lynette Hirschman,
Robyn Kozierok, Patricia Robinson, and
Marc Vilain. 1997. Mixed-initiative devel-
opment of language processing systems. In
Fifth Conference on Applied Natural Lan-
guage Processing, Washington, D.C., March.
Association for Computational Linguistics.
Defense Advanced Research Projects Agency.
1995. Sixth Message Understanding Confer-
ence (MUC-6), Columbia, Maryland, Novem-
ber. Morgan Kaufmann.
Sean P. Engelson and Ido Dagan. 1996. Mini-
mizing manual annotation cost in supervised
training from corpora. Computation and
Linguistic E-Print Service, cmp-lg/9606030,
June.
L. Ferro, M. Vilain, and A. Yeh. 1999. Learn-
ing transformation rules to find grammatical
relations. In Computational natural language
learning (CoNLL-99), pages 43-52. EACL&apos;99
workshop, cs .CL/9906015.
L. Ferro. 1998. Guidelines for annotating gram-
matical relations. Unpublished annotation
guidelines.
Ralph Grishman and Beth Sundheim. 1996.
Message understanding conference: A brief
history. In International Conference on Com-
putational Linguistics, Copenhagen, Den-
mark, August. The International Committee
on Computational Linguistics.
David Lewis and Jason Catlett. 1994. Hetero-
geneous uncertainty sampling for supervised
learning. In Machine Learning: Proceedings
of the Eleventh International Conference on
Machine Learning, pages 148-156, San Fran-
cisco, CA. Morgan Kaufmann.
Grace Ngai and David Yarowsky. 2000. Rule
writing or annotation: Cost-efficient resource
usage for base noun phrase chunking. In
Proceedings of The 38th Annual Meeting. As-
sociation for Computational Linguistics.
David D. Palmer, John D. Burger, and Mari Os-
tendorf. 1999. Information extraction from
broadcast news speech data. In Proceedings
of the 1999 DARPA Broadcast News Work-
shop (Hub-4), February.
Ellen Riloff and Rosie Jones. 1999. Learn-
ing dictionaries for information extraction by
multi-level bootstrapping. In Proceedings of
the Nth National Coreference on Artificial In-
telligence. American Association for Artificial
Intelligence.
Marc Vilain and David Day. 1996. Finite-
state parsing by rule sequences. In In Inter-
national Conference on Computational Lin-
guistics, Copenhagen, Denmark, August. The
International Committee on Computational
Linguistics.
Janyce Wiebe, Rebecca Bruce, and Thomas
O&apos;Hara. 1999. Development and use of a
gold-standard data set for subjectivity clas-
sifications. In Proceedings of the 37th An-
nual Meeting of the Association of Computa-
tional Linguistics, pages 246-253. Association
of Computational Linguistics.
</reference>
<page confidence="0.999049">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325950">
<title confidence="0.999986">Considering Automatic Aids to Corpus Annotation</title>
<author confidence="0.996249">David Day</author>
<author confidence="0.996249">Benjamin</author>
<affiliation confidence="0.825105">MITRE</affiliation>
<address confidence="0.781612666666667">Mail Stop 202 Burlington Bedford, MA 01730,</address>
<email confidence="0.995549">dayAmitre.org,wellnerOmitre.org</email>
<abstract confidence="0.995934125">In this paper we view mixed-initiative corpus annotation from the perspective of knowledge engineering, and discuss some of the opportunities, challenges and dangers that are presented by using mixed-initiative annotation tools. We begin this discussion by describing an existing mixed-initiative annotation tool for open-ended phrase-level annotation, the Alembic Workbench. We discuss how this tool currently operates, the nature of its skill acquisition component, and our plans to extend it in a number of ways, including incorporating an active learning capability. Having set the stage with a concrete example, we identify a number of opportunities and challenges that are presented by the mixed-initiative approach to corpus annotation, including the benefits that might accrue when supporting &amp;quot;layered&amp;quot; annotation environments, the adoption of paradigms, the inclusion of lexical resource construction interleaved with corpus annotation, and other topics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Aberdeen</author>
<author>John Burger</author>
<author>David Day</author>
<author>Lynette Hirschman</author>
<author>Patricia Robinson</author>
<author>Marc Vilain</author>
</authors>
<title>Description of the alembic system used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>141--155</pages>
<location>Columbia, Maryland,</location>
<marker>Aberdeen, Burger, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain. 1995. Description of the alembic system used for MUC-6. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 141-155, Columbia, Maryland, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott W Bennett</author>
<author>Chinatsu Aone</author>
<author>Craig Lovell</author>
</authors>
<title>Learning to tag multilingual texts through observation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-97),</booktitle>
<location>Providence, RI, USA.</location>
<marker>Bennett, Aone, Lovell, 1997</marker>
<rawString>Scott W. Bennett, Chinatsu Aone, and Craig Lovell. 1997. Learning to tag multilingual texts through observation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-97), Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training. In</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="14788" citStr="Blum and Mitchell, 1998" startWordPosition="2289" endWordPosition="2292">mponent the same notion of &amp;quot;locality&amp;quot; as is warranted for such distinct phenomena. This kind of locality might be exemplified by an intermediate &amp;quot;SVO&amp;quot; (Subject/Verb/Object/modifier) representation of a given sentence, which could be derived in a variety of ways, either via treebank-style parses, or from dependencylike syntactic models such as &amp;quot;grammatical relations.&amp;quot;). We are particularly interested in merging the mixed-initiative development of lexical resources with the mixed-initiative development of annotated corpora. (Previous work of others in this area includes (Riloff and Jones, 1999; Blum and Mitchell, 1998).) We anticipate that a host of unsupervised learning techniques will be especially useful in helping to quickly bootstrap the acquisition of useful word lists. 4 Active learning One way of increasing the effective productivity of the human annotator while holding the capabilities of the skill acquisition component constant is by increasing the utility of the annotated data being supplied to the skill acquisition component. In the event that bootstrapping is being performed manually through heuristic insights, the annotator may try to tune the corpus sampling mechanism to favor sentences, para</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In In Proceedings of the 11th Annual Conference on Computational Learning Theory. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
</authors>
<title>Tagging grammatical functions.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-97),</booktitle>
<location>Providence, RI, USA.</location>
<marker>Brants, Skut, Krenn, 1997</marker>
<rawString>Thorsten Brants, Wojciech Skut, and Brigitte Krenn. 1997. Tagging grammatical functions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-97), Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Grace Ngai</author>
</authors>
<title>Man vs machine: A case study in base noun phrase learning.</title>
<date>1999</date>
<booktitle>In Proceedings of Association of Computational Linguistics. Association of Computational Linguistics.</booktitle>
<contexts>
<context position="24407" citStr="Brill and Ngai, 1999" startWordPosition="3771" endWordPosition="3774">sulting sentences directly perhaps within a keyword-in-context type viewer that is also integrated directly with the extensional annotation environment. This way exceptions to this rule can be noticed and modified easily and directly by the annotator. If this were truly a mixed-initiative environment, then such a system might on the next cycle derive a rule which starts with the human-authored heuristic, but derives rules (or some other representation) for capturing the exceptions identified extensionally by the annotator. Interestingly, there has recently been a very careful empirical study (Brill and Ngai, 1999; Ngai and Yarowsky, 2000) exploring the advantages and disadvantages of extensional and intensional mixed-initiative methods for annotating a corpus. This study was carried out by Grace Ngai and David Yarowsky at Johns Hopkins University, and compared the abilities of relatively sophisticated pattern rule authors against machine learning methods for deriving tagging rules in a mixed-initiative annotation environment. The results indicate that rulewriting, while intuitively powerful, may prove difficult for supporting a mixed-initiative approach to corpus annotation. This is a provocative stud</context>
</contexts>
<marker>Brill, Ngai, 1999</marker>
<rawString>Eric Brill and Grace Ngai. 1999. Man vs machine: A case study in base noun phrase learning. In Proceedings of Association of Computational Linguistics. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, Penn.</location>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania, Philadelphia, Penn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Day</author>
<author>John Aberdeen</author>
<author>Lynette Hirschman</author>
<author>Robyn Kozierok</author>
<author>Patricia Robinson</author>
<author>Marc Vilain</author>
</authors>
<title>Mixed-initiative development of language processing systems.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Washington, D.C.,</location>
<marker>Day, Aberdeen, Hirschman, Kozierok, Robinson, Vilain, 1997</marker>
<rawString>David Day, John Aberdeen, Lynette Hirschman, Robyn Kozierok, Patricia Robinson, and Marc Vilain. 1997. Mixed-initiative development of language processing systems. In Fifth Conference on Applied Natural Language Processing, Washington, D.C., March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>Defense Advanced Research Projects Agency.</title>
<date>1995</date>
<booktitle>Sixth Message Understanding Conference (MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia, Maryland,</location>
<marker>1995</marker>
<rawString>Defense Advanced Research Projects Agency. 1995. Sixth Message Understanding Conference (MUC-6), Columbia, Maryland, November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>Computation and Linguistic E-Print Service, cmp-lg/9606030,</booktitle>
<contexts>
<context position="15816" citStr="Engelson and Dagan, 1996" startWordPosition="2446" endWordPosition="2449">l acquisition component. In the event that bootstrapping is being performed manually through heuristic insights, the annotator may try to tune the corpus sampling mechanism to favor sentences, paragraphs or documents that would seem to provide the greatest opportunity for instructing and testing the emerging automated annotation component. It is also possible to perform this sample selection of raw data through automatic means. This interplay between learner and example selection is sometimes referred to as &amp;quot;active learning&amp;quot; (or sample selection). (Lewis and Catlett, 1994) Engelson and Dagan (Engelson and Dagan, 1996) demonstrated an automatic method for selecting part-of-speech training sentences using a votes from a set of automated annotation &amp;quot;experts.&amp;quot; This and other work prompted us to look at how such techniques could be incorporated into the Workbench&apos;s mixed-initiative model. Alembic&apos;s phrase-rule learner contains a number of parameters that are well suited to construct such a family of experts. The basic insight of active learning is that not all training data are equally informative, and that the &amp;quot;confidence&amp;quot; of the induced decision system in classifying (tagging) some particular exemplar is inve</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. Computation and Linguistic E-Print Service, cmp-lg/9606030, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferro</author>
<author>M Vilain</author>
<author>A Yeh</author>
</authors>
<title>Learning transformation rules to find grammatical relations.</title>
<date>1999</date>
<booktitle>In Computational natural language learning (CoNLL-99),</booktitle>
<pages>43--52</pages>
<note>EACL&apos;99 workshop, cs .CL/9906015.</note>
<contexts>
<context position="20804" citStr="Ferro et al., 1999" startWordPosition="3224" endWordPosition="3227"> a largely example-based knowledge engineering/acquisition process. With both of these contexts in mind, it is useful to reflect on some of the outstanding opportunities in mixed-initiative annotation, as well as the difficulties and dangers that accompany them. 5.1 Layered annotations for multi-staged mixed-initiative corpus development Some annotation tasks depend so strongly on &amp;quot;earlier&amp;quot; annotations that it will become important to build annotation environments in which these earlier annotation &amp;quot;layers&amp;quot; are made apparent to the annotator. For example, when annotating grammatical relations (Ferro et al., 1999; Ferro, 1998), the job of the annotator is to establish various pre-specified types of relationships among sentence &amp;quot;chunks,&amp;quot; where these chunks consist of simple phrases such as &amp;quot;noun group,&amp;quot; &amp;quot;verb group,&amp;quot; &amp;quot;preposition group,&amp;quot; and 1-We are in the midst of our explorations of this task; we hope to be able to report the results of more robust experiments soon. 76 the like. Thus, instead of being presented with a text in a standard Workbench textual display (and being able to draw relationships between arbitrary pairs of words), it is important that these sub-groupings are already visually appa</context>
</contexts>
<marker>Ferro, Vilain, Yeh, 1999</marker>
<rawString>L. Ferro, M. Vilain, and A. Yeh. 1999. Learning transformation rules to find grammatical relations. In Computational natural language learning (CoNLL-99), pages 43-52. EACL&apos;99 workshop, cs .CL/9906015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferro</author>
</authors>
<title>Guidelines for annotating grammatical relations. Unpublished annotation guidelines.</title>
<date>1998</date>
<contexts>
<context position="20818" citStr="Ferro, 1998" startWordPosition="3228" endWordPosition="3229">ased knowledge engineering/acquisition process. With both of these contexts in mind, it is useful to reflect on some of the outstanding opportunities in mixed-initiative annotation, as well as the difficulties and dangers that accompany them. 5.1 Layered annotations for multi-staged mixed-initiative corpus development Some annotation tasks depend so strongly on &amp;quot;earlier&amp;quot; annotations that it will become important to build annotation environments in which these earlier annotation &amp;quot;layers&amp;quot; are made apparent to the annotator. For example, when annotating grammatical relations (Ferro et al., 1999; Ferro, 1998), the job of the annotator is to establish various pre-specified types of relationships among sentence &amp;quot;chunks,&amp;quot; where these chunks consist of simple phrases such as &amp;quot;noun group,&amp;quot; &amp;quot;verb group,&amp;quot; &amp;quot;preposition group,&amp;quot; and 1-We are in the midst of our explorations of this task; we hope to be able to report the results of more robust experiments soon. 76 the like. Thus, instead of being presented with a text in a standard Workbench textual display (and being able to draw relationships between arbitrary pairs of words), it is important that these sub-groupings are already visually apparent and made </context>
</contexts>
<marker>Ferro, 1998</marker>
<rawString>L. Ferro. 1998. Guidelines for annotating grammatical relations. Unpublished annotation guidelines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference: A brief history.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="14119" citStr="Grishman and Sundheim, 1996" startWordPosition="2193" endWordPosition="2196">f some very small annotated corpus, this serendipity will lead to many more correct applications when different word choices are encountered in previously unseen data. Our current presumption is that replicating efficient mixed-initiative successes for other tasks and in other arenas of language processing will rely heavily on providing similar advantages as those identified above. For example, in order to support the rapid mixed-initiative annotation of certain types of relation/event data (e.g., the &amp;quot;template relation&amp;quot; and &amp;quot;scenario template&amp;quot; tasks of the various MUC evaluations (Def, 1995; Grishman and Sundheim, 1996)), one must make available to the learning component the same notion of &amp;quot;locality&amp;quot; as is warranted for such distinct phenomena. This kind of locality might be exemplified by an intermediate &amp;quot;SVO&amp;quot; (Subject/Verb/Object/modifier) representation of a given sentence, which could be derived in a variety of ways, either via treebank-style parses, or from dependencylike syntactic models such as &amp;quot;grammatical relations.&amp;quot;). We are particularly interested in merging the mixed-initiative development of lexical resources with the mixed-initiative development of annotated corpora. (Previous work of others in</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference: A brief history. In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Machine Learning: Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="15770" citStr="Lewis and Catlett, 1994" startWordPosition="2439" endWordPosition="2442">the annotated data being supplied to the skill acquisition component. In the event that bootstrapping is being performed manually through heuristic insights, the annotator may try to tune the corpus sampling mechanism to favor sentences, paragraphs or documents that would seem to provide the greatest opportunity for instructing and testing the emerging automated annotation component. It is also possible to perform this sample selection of raw data through automatic means. This interplay between learner and example selection is sometimes referred to as &amp;quot;active learning&amp;quot; (or sample selection). (Lewis and Catlett, 1994) Engelson and Dagan (Engelson and Dagan, 1996) demonstrated an automatic method for selecting part-of-speech training sentences using a votes from a set of automated annotation &amp;quot;experts.&amp;quot; This and other work prompted us to look at how such techniques could be incorporated into the Workbench&apos;s mixed-initiative model. Alembic&apos;s phrase-rule learner contains a number of parameters that are well suited to construct such a family of experts. The basic insight of active learning is that not all training data are equally informative, and that the &amp;quot;confidence&amp;quot; of the induced decision system in classify</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Machine Learning: Proceedings of the Eleventh International Conference on Machine Learning, pages 148-156, San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of The 38th Annual Meeting. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24433" citStr="Ngai and Yarowsky, 2000" startWordPosition="3775" endWordPosition="3778">ctly perhaps within a keyword-in-context type viewer that is also integrated directly with the extensional annotation environment. This way exceptions to this rule can be noticed and modified easily and directly by the annotator. If this were truly a mixed-initiative environment, then such a system might on the next cycle derive a rule which starts with the human-authored heuristic, but derives rules (or some other representation) for capturing the exceptions identified extensionally by the annotator. Interestingly, there has recently been a very careful empirical study (Brill and Ngai, 1999; Ngai and Yarowsky, 2000) exploring the advantages and disadvantages of extensional and intensional mixed-initiative methods for annotating a corpus. This study was carried out by Grace Ngai and David Yarowsky at Johns Hopkins University, and compared the abilities of relatively sophisticated pattern rule authors against machine learning methods for deriving tagging rules in a mixed-initiative annotation environment. The results indicate that rulewriting, while intuitively powerful, may prove difficult for supporting a mixed-initiative approach to corpus annotation. This is a provocative study, seeming at variance wit</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of The 38th Annual Meeting. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>John D Burger</author>
<author>Mari Ostendorf</author>
</authors>
<title>Information extraction from broadcast news speech data.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 DARPA Broadcast News Workshop (Hub-4),</booktitle>
<contexts>
<context position="8232" citStr="Palmer et al., 1999" startWordPosition="1267" endWordPosition="1270">ve been annotated since the last learning procedure was called, and the newly derived rules are concatenated onto the end of the existing rule sequence. However, eventually it is desirable to start fresh, since it becomes more and more likely that new opportunities for generalization can be found in a larger training set, leading to increases in the ability of the new rule sequence to apply successfully to unseen data. Nonetheless, we are also interested addressing the problem of learning performance directly. One approach we intend to pursue is the incorporation of MITRE&apos;s HMM-based &amp;quot;Phrag&amp;quot; (Palmer et al., 1999) phrase-parsing 73 learner within the Workbench&apos;s mixed-initiative repertoire, which we imagine could be increasingly relied upon as the size of the training set reaches very large proportions. Currently the default &amp;quot;granularity&amp;quot; of mixedinitiative annotation within the Workbench is that of a document or file. As long as a single file is fully annotated, it can be used as the basis of phrase-rule learning, either alone or in combination with a corpus consisting of other annotated documents/files. Of course, documents can be arbitrarily reduced to smaller chunks if there is a strong need for th</context>
</contexts>
<marker>Palmer, Burger, Ostendorf, 1999</marker>
<rawString>David D. Palmer, John D. Burger, and Mari Ostendorf. 1999. Information extraction from broadcast news speech data. In Proceedings of the 1999 DARPA Broadcast News Workshop (Hub-4), February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Nth National Coreference on Artificial Intelligence. American Association for Artificial Intelligence.</booktitle>
<contexts>
<context position="14762" citStr="Riloff and Jones, 1999" startWordPosition="2285" endWordPosition="2288">lable to the learning component the same notion of &amp;quot;locality&amp;quot; as is warranted for such distinct phenomena. This kind of locality might be exemplified by an intermediate &amp;quot;SVO&amp;quot; (Subject/Verb/Object/modifier) representation of a given sentence, which could be derived in a variety of ways, either via treebank-style parses, or from dependencylike syntactic models such as &amp;quot;grammatical relations.&amp;quot;). We are particularly interested in merging the mixed-initiative development of lexical resources with the mixed-initiative development of annotated corpora. (Previous work of others in this area includes (Riloff and Jones, 1999; Blum and Mitchell, 1998).) We anticipate that a host of unsupervised learning techniques will be especially useful in helping to quickly bootstrap the acquisition of useful word lists. 4 Active learning One way of increasing the effective productivity of the human annotator while holding the capabilities of the skill acquisition component constant is by increasing the utility of the annotated data being supplied to the skill acquisition component. In the event that bootstrapping is being performed manually through heuristic insights, the annotator may try to tune the corpus sampling mechanis</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the Nth National Coreference on Artificial Intelligence. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>David Day</author>
</authors>
<title>Finitestate parsing by rule sequences.</title>
<date>1996</date>
<booktitle>In In International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark,</location>
<marker>Vilain, Day, 1996</marker>
<rawString>Marc Vilain and David Day. 1996. Finitestate parsing by rule sequences. In In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rebecca Bruce</author>
<author>Thomas O&apos;Hara</author>
</authors>
<title>Development and use of a gold-standard data set for subjectivity classifications.</title>
<date>1999</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>246--253</pages>
<contexts>
<context position="26566" citStr="Wiebe et al., 1999" startWordPosition="4114" endWordPosition="4117">techniques we have described support and/or improve such task definition endeavors? At the heart of any collaborative annotation effort is the detailed analysis and associated discussion of different interpretations of the linguistic phenomena, which is most often captured and brought to light through interannotator annotation analysis. At first this analysis is largely qualitative, and depends on detecting the anomalies in order to promote their discussion. Recently there has been a study of this collaborative behavior, and an associated automated method was developed that was modeled on it (Wiebe et al., 1999). Subsequently the emphasis moves towards quantitative inter-annotator analysis and the categorization of those differences. In both of these phases techniques that can boost the number and kinds of linguistic artifacts that have been annotated by one person or another can only help in the process of annotation understanding and interannotator reconciliation. Of course, it cannot sidestep the necessity of discussion and reflection that is necessary to come to terms with the motivations and other issues relevant to a new annotation task. Nonetheless, there are clearly opportunities and challeng</context>
</contexts>
<marker>Wiebe, Bruce, O&apos;Hara, 1999</marker>
<rawString>Janyce Wiebe, Rebecca Bruce, and Thomas O&apos;Hara. 1999. Development and use of a gold-standard data set for subjectivity classifications. In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics, pages 246-253. Association of Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>