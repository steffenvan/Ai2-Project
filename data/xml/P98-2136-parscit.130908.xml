<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002859">
<title confidence="0.901413">
Confirmation in Multimodal Systems
</title>
<author confidence="0.993773">
David R. McGee, Philip R. Cohen and Sharon Oviatt
</author>
<affiliation confidence="0.986567333333333">
Center for Human-Computer Communication,
Department of Computer Science and Engineering
Oregon Graduate Institute
</affiliation>
<note confidence="0.357883">
P.O. Box 91000, Portland, Oregon 97291-1000
dmcgee. pcohen. oviatt I @cse.ogi.edu
</note>
<sectionHeader confidence="0.949747" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.99992975">
Systems that attempt to understand natural human input
make mistakes, even humans. However, humans avoid
misunderstandings by confirming doubtful input.
Multimodal .systems—those that combine simultaneous
input from more than one modality, for example speech
and gesture—have historically been designed so that
they either request confirmation of speech, their primary
modality, or not at all. Instead, we experimented with
delaying confirmation until after the speech and gesture
were combined into a complete multimodal command.
In controlled experiments, subjects achieved more
commands per minute at a lower error rate when the
system delayed confirmation, than compared to when
subjects confirmed only speech. In addition, this style of
late confirmation meets the usees expectation that
confirmed commands should be executable.
</bodyText>
<keyword confidence="0.767684">
KEYWORDS: multimodal, confirmation, uncertainty,
disambiguation
</keyword>
<figureCaption confidence="0.529299">
&amp;quot;Mistakes are inevitable in dialog.. .In practice, conversation
breaks down almost instantly in the absence of a facility to
recognize and repair errors, ask clarification questions, give
confirmation, and perform disambiguation. [1]&amp;quot;
</figureCaption>
<sectionHeader confidence="0.996424" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99991031372549">
We claim that multimodal systems [2, 3] that issue
commands based on speech and gesture input should not
request confirmation of words or ink. Rather, these
systems should when there is doubt, request
confirmation of their understanding of the combined
meaning of each coordinated language act. The purpose
of any confirmation act, after all, is to reach agreement
on the overall meaning of each command. To test these
claims we have extended our multimodal map system,
QuickSet [4, 5], so that it can be tuned to request
confirmation either before or after integration of
modalities. Using QuickSet, we have conducted an
empirical study that indicates agreement about the
correctness of commands can be reached quicker if
confirmation is delayed until after blending. This paper
describes QuickSet, our experiences with it, an
experiment that compares early and late confirmation
strategies, the results of that experiment, and our
conclusions.
Command-driven conversational systems need to
identify hindrances to accurate understanding and
execution of commands in order to avoid
miscommunication. These hindrances can arise from at
least three sources:
Uncettairay—lack of confidence in interpretation of the input,
Ambiguity—equally likely interpretations of input, and
Infeasibility—an inability to perform the command
Suppose that we use a recognition system that interprets
natural human input [6], that is capable of multimodal
interaction [2, 3], and that will let users place simulated
military units and related objects on a map. When we
use this system, our words and stylus movements are
simultaneously recognized, interpreted, and blended
together. A user calls out the names of objects, such as
&amp;quot;ROMEO ONE EAGLE,&amp;quot; while marking the map with a
gesture. If the system is confident of its recognition of
the input, it might interpret this command in the
following !limiec a unit should be placed on the map at
the specified location. Another equally likely
interpretation, looking only at the results of speech
recognition, might be to select an existing &amp;quot;ROMEO ONE
EAGLE.&amp;quot; Since this multimodal system is performing
recognition, uncertainty inevitably exists in the
recognizer&apos;s hypotheses. &amp;quot;ROMEO ONE EAGLE&amp;quot; may
not be recognized with a high degree of confidence. It
may not even be the most likely hypothesis.
One way to disambiguate the hypotheses is with the
multimodal language specification itself, the way we
allow modalities to combine. Since different modalities
tend to capture complementary information [7-9], we
can leverage this facility by combining ambiguous
</bodyText>
<page confidence="0.997933">
823
</page>
<bodyText confidence="0.999916352941176">
spoken interpretations with disimilar gestures. For
example, we might specify that selection gestures
(circling) combine with the ambiguous speech from
above to produce a selection command. Another way of
disambiguating the spoken utterance is to enforce a
precondition for the command: for example, for the
selection command to be possible the object must
already exist on the map. Thus, under such a
precondition, if &amp;quot;ROMEO ONE EAGLE&amp;quot; is not already
present on the map, the user cannot select it. We call
these techniques multimodal disambiguation techniques.
Regardless, if a system receives input that it fmds
uncertain, ambiguous, or infeasible, or if its effect might
be profound, risky, costly, or irreversible, it may want to
verify its interpretation of the command with the user.
For example, a system prepared to execute the
command &amp;quot;DESTROY ALL DATA&amp;quot; should give the
speaker a chance to change or correct the command.
Otherwise, the cost of such errors is task-dependent and
can be immeasurable [6, 10].
Therefore, we claim that conversational systems should
be able to request the user to confirm the command, as
humans tend to do [11-14]. Such confirmations are used
&amp;quot;to achieve common ground&amp;quot; in human-human dialogue
[15]. On their way to achieving common ground,
participants attempt to minimize their collaborative
effort, &amp;quot;the work that both do from the initiation of [a
command] to its completion.&amp;quot; [15] Herein we will
further define collaborative effort in terms of work in a
command-based collaborative dialogue, where an
increase in the rate at which commands can be
successfully performed corresponds to a reduction in the
collaborative effort. We know that confirmations are an
important way to reduce miscommunication [13, 16,
17], and thus collaborative effort. In fact, the more likely
miscommunication, the more frequently people
introduce confirmations [16, 17].
To ensure that common ground is achieved,
miscommunication is avoided, and collaborative effort is
reduced, system designers must determine when and
how confirmations ought to be requested. Should a
confirmation occur for each modality or should
confirmation be delayed until the modalities have been
blended? Choosing to confirm speech and gesture
separately, or speech alone (as many contemporary
multimodal systems do), might simplify the process of
confirmation. For example, confirmations could be
performed immediately after recognition of one or both
modalities. However, we will show that collaborative
effort can be reduced if multimodal systems delay
confirmation until after blending.
</bodyText>
<sectionHeader confidence="0.99089" genericHeader="method">
1 MOTIVATION
</sectionHeader>
<bodyText confidence="0.999947888888889">
Historically, multimodal systems have either not
confirmed input [18-22] or confirmed only the primary
modality of such systems—speech. This is reasonable,
considering the evolution of multimodal systems from
their speech-based roots. Observations of QuickSet
prototypes last year, however, showed that simply
confirming the results of speech recognition was often
problematic—users had the expectation that whenever a
command was confirmed, it would be executed. We
observed that confirming speech prior to multimodal
integration led to three possible cases where this
expectation might not be met: ambiguous gestures, non-
meaningful speech, and delayed confirmation.
The first problem with speech-only confirmation was
that the gesture recognizer produced results that were
often ambiguous. For example, recognition of the ink in
Figure 1 could result in confusion. The arc (left) in the
figure provides some semantic content, but it may be
incomplete. The user may have been selecting
something or she may have been creating an area, line,
or route. On the other hand, the circle-like gesture
(middle) might not be designating an area or specifying
a selection; it might be indicating a circuitous route or
line. Without more information from other modalities, it
is difficult to guess the intentions behind these gestures.
Figure 1 demonstrates how, oftentimes, it is difficult to
determine which interpretation is correct. Some gestures
can be assumed to be fully specified by themselves (at
right, an editor&apos;s mark meaning &amp;quot;cut&amp;quot;). However, most
rely on complementary input for complete
interpretation. If the gesture recognizer misinterprets the
gesture, failure will not occur until integration. The
speech hypothesis might not combine with any of the
gesture hypotheses. Also, earlier versions of our speech
recognition agent were limited to a single recognition
hypothesis and one that might not even be syntactically
</bodyText>
<figure confidence="0.567918">
C
</figure>
<figureCaption confidence="0.994291">
Figure 1. Ambiguous Gestures
</figureCaption>
<page confidence="0.983598">
824
</page>
<bodyText confidence="0.999988151515152">
correct, in which case integration would always fail.
Finally, the confirmation act itself could delay the arrival
of speech into the process of multimodal integration. If
the user chose to correct the speech recognition output
or to delay confirmation for any other reason, integration
itself could fail due to sensitivity in the multimodal
architecture.
In all three cases, users were asked to confirm a
command that could not be executed. An important
lesson learned from these observations is that when
confirming a command, users think they are giving
approval; thus, they expect that the command can be
executed without hindrance. Due to these early
observations, we wished to determine whether delaying
confirmation until after modalities have combined
would enhance the human-computer dialogue in
multimodal systems. Therefore, we hypothesize that
late-stage confirmations will lead to three improvements
in dialogue. First, because late-stage systems can be
designed to present only feasible commands for
confirmation, blended inputs that fail to produce a
feasible command can be immediately flagged as a non-
understanding and presented to the user as such, rather
than as a possible command. Second, because of
multimodal disambiguation, misunderstandings can be
reduced, and therefore the number of conversational
turns required to reach mutual understanding can be
reduced as well. Finally, a reduction in turns combined
with a reduction in time spent will lead to reducing the
&amp;quot;collaborative effort&amp;quot; in the dialogue. To examine our
hypotheses, we designed an experiment using QuickSet
to determine if late-stage confirmations enhance human-
computer conversational performance.
</bodyText>
<sectionHeader confidence="0.995516" genericHeader="method">
2 QUICKSET
</sectionHeader>
<bodyText confidence="0.999865">
This section describes QuickSet, a suite of agents for
multimodal human-computer communication [4,5].
</bodyText>
<subsectionHeader confidence="0.992423">
2.1 A Multi-Agent Architecture
</subsectionHeader>
<bodyText confidence="0.945441571428572">
Underneath the QuickSet suite of agents lies a
distributed, blackboard-based, multi-agent architecture
based on the Open Agent Architecture&apos; [23]. The
blackboard acts as a repository of shared information
and facilitator. The agents rely on it for brokering,
message distribution, and notification.
The Open Agent Architecture is a trademark of SRI International.
</bodyText>
<subsectionHeader confidence="0.993195">
2.2 The QuickSet Agents
</subsectionHeader>
<bodyText confidence="0.999818333333333">
The following section briefly summarizes the
responsibilities of each agent, their interaction, and the
results of their computation.
</bodyText>
<subsubsectionHeader confidence="0.999398">
2.2.1 User Interface
</subsubsectionHeader>
<bodyText confidence="0.99855975">
The user draws on and speaks to the interface (see
Figure 2 for a snapshot of the interface) to place objects
on the map, assign attributes and behaviors to them,
and ask questions about them.
</bodyText>
<figure confidence="0.842499">
,s&amp; otr I imrixe■ 3 (.6 471.71 aoalei .
</figure>
<figureCaption confidence="0.999525">
Figure 2. QuickSet Early Confirmation Mode
</figureCaption>
<subsubsectionHeader confidence="0.971901">
2.2.2 Gesture Recognition
</subsubsectionHeader>
<bodyText confidence="0.9994298">
The gesture recognition agent recognizes gestures from
strokes drawn on the map. Along with coordinate
values, each stroke from the user interface provides
contextual information about objects touched or
encircled by the stroke. Recognition results are an n-best
list (top n-ranked) of interpretations. The interpretations
are encoded as typed feature structures [5], which
represent each of the potential semantic contributions of
the gesture. This list is then passed to the multimodal
integrator.
</bodyText>
<subsubsectionHeader confidence="0.998997">
2.2.3 Speech Recognition
</subsubsectionHeader>
<bodyText confidence="0.999938666666667">
The Whisper speech recognition engine from Microsoft
Corp. [24] drives the speech recognition agent. It offers
speaker-independent, continuous recognition in close to
real time. QuickSet relies upon a context-free domain
grammar, specifically designed for each application, to
constrain the speech recognizer. The speech recognizer
</bodyText>
<page confidence="0.993363">
825
</page>
<bodyText confidence="0.999652">
agent&apos;s output is also an n-best list of hypotheses and
their probability estimates. These results are passed on
for natural language interpretation.
</bodyText>
<subsubsectionHeader confidence="0.997118">
2.2.4 Natural Language Interpretation
</subsubsectionHeader>
<bodyText confidence="0.999957777777778">
The natural language interpretation agent parses the
output of the speech recognizer attempting to provide
meaningful semantic interpretations based on a domain-
specific grammar. This process may introduce further
ambiguity; that is, more hypotheses. The results of
parsing are, again, in the form of an n-best list of typed
feature structures. When complete, the results of natural
language interpretation are passed to the integrator for
multimodal integration.
</bodyText>
<subsubsectionHeader confidence="0.999646">
2.2.5 Multimodal Integration
</subsubsectionHeader>
<bodyText confidence="0.999948">
The multimodal integration agent accepts typed feature
structures from the gesture and natural language
interpretation agents, and unifies them [5]. The process
of integration ensures that modes combine according to
a multimodal language specification, and that they meet
certain multimodal timing and command-specific
constraints. These constraints place limits on when
different input can occur, thus reducing errors [7]. If after
unification and constraint satisfaction, there is more than
one completely specified command, the agent then
computes the joint probabilities for each and passes the
feature structure with the highest to the bridge. If, on the
other hand, no completely specified command exists, a
message is sent to the user interface, asking it to inform
the user of the non-understanding.
</bodyText>
<subsubsectionHeader confidence="0.986052">
2.2.6 Bridge to Application Systems
</subsubsectionHeader>
<bodyText confidence="0.999943">
The bridge agent acts as a single message-based
interface to domain applications. When it receives a
feature structure, it sends a message to the appropriate
applications, requesting that they execute the command.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
3 CONFIRMATION STRATEGIES
</sectionHeader>
<bodyText confidence="0.9998776">
QuickSet supports two modes of confirmation: early,
which uses the speech recognition hypothesis; and late,
which renders the confirmation act graphically using the
entire integrated multimodal command. These two
modes are detailed in the following subsections.
</bodyText>
<subsectionHeader confidence="0.946864">
3.1 Early Confirmation
</subsectionHeader>
<bodyText confidence="0.999903375">
Under the early confirmation strategy (see Figure 3),
speech and gesture are immediately passed to their
respective recognizers ( la and lb). Electronic ink is used
for immediate visual feedback of the gesture input. The
highest-scoring speech-recognition hypothesis is
returned to the user interface and displayed for
confirmation (2). Gesture recognition results are
forwarded to the integrator after processing (4).
</bodyText>
<figureCaption confidence="0.990733">
Figure 3. Early Confmnation Message Flow
</figureCaption>
<bodyText confidence="0.999803833333333">
After confirmation of the speech, QuickSet passes the
selected sentence to the parser (3) and the process of
integration follows (4). If, during confirmation, the
system fails to present the correct spoken interpretation,
users are given the choice of selecting it from a pop-up
menu or respealcing the command (see Figure 2).
</bodyText>
<subsectionHeader confidence="0.979259">
3.2 Late Confirmation
</subsectionHeader>
<bodyText confidence="0.7697765">
In order to meet the user&apos;s expectations, it was proposed
that confirmations occur after integration of the
</bodyText>
<figureCaption confidence="0.8951948">
multimodal inputs. Notice that in Figure 4, as opposed to
Figure 3, no confirmation act impedes input as it
progresses towards integration, thus eliminating the
timing issues of prior QuickSet architectures.
Figure 4. Late Confirmation Message Flow
</figureCaption>
<bodyText confidence="0.997697166666667">
Figure 5 is a snapshot of QuickSet in late confirmation
mode. The user is indicating the placement of
checkpoints on the terrain. She has just touched the map
with her pen, while saying &amp;quot;YELLOW&amp;quot; to name the next
checkpoint. In response, QuickSet has combined the
gesture with the speech and graphically presented the
</bodyText>
<page confidence="0.996843">
826
</page>
<bodyText confidence="0.9244275">
logical consequence of the command: a checkpoint icon
(which looks like an upside-down pencil).
</bodyText>
<figure confidence="0.884402333333333">
.-1,4maSti 1 7 1 Bela 11 - Flange 400 ei 23 Palm,
TA
p.„,
</figure>
<figureCaption confidence="0.999142">
Figure 5. QuickSet in Late Confirmation Mode
</figureCaption>
<bodyText confidence="0.9998995">
To confirm or disconfirm an object in either mode, the
user can push either the SEND (checicmark) or the ERASE
(eraser) buttons, respectively. Alternatively, to confirm
the command in late confirmation mode, the user can
rely on implicit confirmation, wherein QuickSet treats
non-contradiction as a confirmation [25-27]. In other
words, if the user proceeds to the next command, she
implicitly confirms the previous command.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="method">
4 EXPERIMENTAL METHOD
</sectionHeader>
<bodyText confidence="0.999928">
This section describes this experiment, its design, and
how data were collected and evaluated.
</bodyText>
<subsectionHeader confidence="0.993671">
4.1 Subjects, Tasks, and Procedure
</subsectionHeader>
<bodyText confidence="0.999990894736842">
Eight subjects, 2 male and 6 female adults, half with a
computer science background and half without, were
recruited from the OGI campus and asked to spend one
hour using a prototypical system for disaster rescue
planning.
During training, subjects received a set of written
instructions that described how users could interact with
the system. Before each task, subjects received oral
instructions regarding how the system would request
confirmations. The subjects were equipped with
microphone and pen, and asked to perform 20 typical
commands as practice prior to data collection. They
performed these commands in one of the two
confirmation modes. After they had completed either
the flood or the fire scenario, the other scenario was
introduced and the remaining confirmation mode was
explained. At this time, the subject was given a chance
to practice commands in the new confirmation mode,
and then conclude the experiment.
</bodyText>
<subsectionHeader confidence="0.899695">
4.2 Research Design and Data Capture
</subsectionHeader>
<bodyText confidence="0.999982333333333">
The research design was within-subjects with a single
factor, confirmation mode, and repeated measures. Each
of the eight subjects completed one fire-fighting and one
flood-control rescue task, composed of approximately
the same number and types of commands, for a strict
recipe of about 50 multimodal commands. We
counterbalanced the order of confirmation mode and
task, resulting in four different task and confirmation
mode orderings.
</bodyText>
<subsectionHeader confidence="0.875179">
4.3 Transcript Preparation and Coding
</subsectionHeader>
<bodyText confidence="0.999787">
The QuickSet user interface was videotaped and
microphone input was recorded while each of the
subjects interacted with the system. The following
dependent measures were coded from the videotaped
sessions: time to complete each task, and the number of
commands and repairs.
</bodyText>
<subsubsectionHeader confidence="0.978461">
4.3.1 Time to complete task
</subsubsectionHeader>
<bodyText confidence="0.999542333333333">
The total elapsed time in minutes and seconds taken to
complete each task was measured: from the first contact
of the pen on the interface until the task was complete.
</bodyText>
<subsubsectionHeader confidence="0.914498">
4.3.2 Commands, repairs, turns
</subsubsectionHeader>
<bodyText confidence="0.999952166666667">
The number of commands attempted for each task was
tabulated. Some subjects skipped commands, and most
tended to add commands to each task, typically to
navigate on the map (e.g., &amp;quot;PAN&amp;quot; and &amp;quot;zoom&amp;quot;). If the
system misunderstood, the subjects were asked to
attempt a command up to three times (repair), then
proceed to the next one. Completely unsuccessful
commands and the time spent on them, including
repairs, were factored out of this study (1% of all
commands). The number of turns to complete each task
is the sum of the total number of commands attempted
and any repairs.
</bodyText>
<subsubsectionHeader confidence="0.994262">
4.3.3 Derived Measures
</subsubsectionHeader>
<bodyText confidence="0.999860166666667">
Several measures were derived from the dependent
measures. Turns per command (tpc) describes how
many turns it takes to successfully complete a
command. Turns per minute (tpm) measures the speed
with which the user interacts. A multimodal error rate
was calculated based on how often repairs were
</bodyText>
<equation confidence="0.645076333333333">
ir ENE CXF.044 rs•
pr 1
V
</equation>
<page confidence="0.985888">
827
</page>
<bodyText confidence="0.999608">
necessary. Commands per minute (cpm) represents the
rate at which the subject is able to issue successful
commands, estimating the collaborative effort.
</bodyText>
<sectionHeader confidence="0.868122" genericHeader="method">
5 UL
</sectionHeader>
<table confidence="0.998126714285714">
[IP Means One-tailed t-test (df=7)
Early Late
Time(rnin.) 13.5 10.7 t = 2.802, p &lt;0.011
trc 1.2 1.1 t= 1.759,p &lt;0.061
tpm 4.5 5.3 t = -4.00, p &lt; 0.003
Error rate 20% 14% t= 1.90, p &lt; 0.05
cpm 3.8 4.8 t= -3.915,p &lt;0.003
</table>
<bodyText confidence="0.998726857142857">
These results show that when comparing late with early
confirmation: 1) subjects complete commands in fewer
turns (the error rate and tpc are reduced, resulting in a
30% error reduction); 2) they complete turns at a faster
rate (tpm is increased by 21%); and 3) they complete
more commands in less time (cpm is increased by 26%).
These results confirm all of our predictions.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="evaluation">
6 DISCUSSION
</sectionHeader>
<bodyText confidence="0.999975807692308">
There are two likely reasons why late confirmation
outperforms early confirmation: implicit confirmation
and multimodal disambiguation. HeisterIcamp theorized
that implicit confirmation could reduce the number of
turns in dialogue [25]. Rudnicicy proved in a speech-
only digit-entry system that implicit confirmation
improved throughput when compared to explicit
confirmation [27], and our results confirm their findings.
Lavie and colleagues have shown the usefulness of late-
stage disambiguation, during which speech-
understanding systems pass multiple interpretations
through the system, using context in the final stages of
processing to disambiguate the recognition hypotheses
[28]. However, we have demonstrated and empirically
shown the advantage in combining these two strategies
in a multimodal system.
It can be argued that implicit confirmation is equivalent
to being able to undo the last command, as some
multimodal systems allow [3]. However, commands that
are infeasible, profound, risky, costly, or irreversible are
difficult to undo. For this reason, we argue that implicit
confirmation is often superior to the option of undoing
the previous command. Implicit confirmation, when
combined with late confirmation, contributes to a
smoother, faster, and more accurate collaboration
between human and computer.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="conclusions">
7 CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.99999165">
We have developed a system that meets the following
expectation: when the proposition being confirmed is a
command, it should be one that the system believes can
be executed. To meet this expectation and increase the
conversational performance of multimodal systems, we
have argued that confirmations should occur late in the
system&apos;s understanding process, at a point after blending
has enhanced its understanding. This research has
compared two strategies: one in which confirmation is
performed immediately after speech recognition, and
one in which it is delayed until after multimodal
integration. The comparison shows that late
confirmation reduces the time to perform map
manipulation tasks with a multimodal interface. Users
can interact faster and complete commands in fewer
turns, leading to a reduction in collaborative effort.
A direction for future research is to adopt a strategy for
determining whether a confirmation is necessary [29,
30], rather than confirming every utterance, and
measuring this strategy&apos;s effectiveness.
</bodyText>
<sectionHeader confidence="0.998142" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.984205833333333">
This work is supported in part by the Information
Technology and Information Systems offices of DARPA
under contract number DABT63-95-C-007, and in part
by ONR grant number N00014-95-1-1164. It has been
done in collaboration with the US Navy&apos;s NCCOSC
RDT&amp;E Division (NRaD). Thanks to the faculty, staff,
and students who contributed to this research, including
Joshua Clow, Peter Heeman, Michael Johnston, Ira
Smith, Stephen Sutton, and Karen Ward. Special thanks
to Donald Hanley for his insightful editorial comment
and friendship. Finally, sincere thanks to the people who
volunteered to participate as subjects in this research.
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9970145">
[1] D. Perlis and K. Purang, &amp;quot;Conversational adequacy:
Mistakes are the essence,&amp;quot; in Proceedings of Workshop on
Detecting, Repairing, and Preventing Human-Machine
Miscommunication, AAAI96, 1996.
[2] R. Bolt, &amp;quot;Put-That-There: Voice and gesture at the
graphics interface,&amp;quot; Computer Graphics, vol. 14, pp. 262-270,
1980.
[3] M. T. Vo and C. Wood, &amp;quot;Building an Application
Framework for Speech and Pen Input Integration in
Multimodal Learning Interfaces,&amp;quot; in Proceedings of IEEE
International Conference on Acoustics, Speech, and Signal
Processing, ICASSP 96, Atlanta, GA, 1996.
</reference>
<page confidence="0.992324">
828
</page>
<reference confidence="0.999623177570093">
[4] P. It Cohen, M. Johnston, D. McGee, I. Smith, J. Pittman,
L. Chen, and J. Clow, &amp;quot;Multimodal interaction for distributed
interactive simulation,&amp;quot; in Proceedings of Innovative
Applications of Artificial Intelligence Conference, I4AI97,
Menlo Park, CA, 1997.
[5] M. Johnston, P. R. Cohen, D. McGee, S. L Oviatt, J. A.
Pittman, and L Smith, &amp;quot;Unification-based multimodal
integration,&amp;quot; in Proceedings of 35th Annual Meeting of the
Association for Computational Linguistics, ACL97, Madrid,
Spain, 1997.
[6] J. It Rhyne and C. G Wolf, &amp;quot;Chapter 7: Recognition-
based user interfaces,&amp;quot; in Advances in Human-Computer
Interaction, vol. 4, H. R. Hanson and D. Hix, Eds., pp. 191-
250, 1992.
[7] S. Oviatt, A. DeAngeli, and K. Kuhn, &amp;quot;Integration and
synchronization of input modes during multimodal human-
computer interaction,&amp;quot; in Proceedings of Conference on
Human Factors in Computing Systems, CHI97, pp. 415-422,
Atlanta, GA, 1997.
[8] P. Lefebvre, G Duncan, and F. Poirier, &amp;quot;Speaking with
computers: A multimodal approach,&amp;quot; in Proceedings of
EUROSPEECH93 Conference, pp. 1665-1668, Berlin,
Germany, 1993.
[9] P. Morin and J. Junqua, &amp;quot;Habitable interaction in goal-
oriented multimodal dialogue systems,&amp;quot; in Proceedings of
EUROSPEECH93 Conference, pp. 1669-1672, Berlin,
Germany, 1993.
[10] L. Hirschman and C. Pao, &apos;The cost of errors in a spoken
language system,&amp;quot; in Proceedings of EUROSPEECH93
Conference, pp. 1419-1422, Berlin, Germany, 1993.
[11] H. Clark and D. Wilkes-Gibbs, &amp;quot;Referring as a
collaborative process,&amp;quot; Cognition, vol. 13, pp. 259-294, 1986.
[12] P. R. Cohen and H. J. Levesque, &amp;quot;Confirmations and joint
action,&amp;quot; in Proceedings of International Joint Conference on
Artificial Intelligence, pp. 951-957, 1991.
[13] D. Cl Novick and S. Sutton, &amp;quot;An empirical model of
acknowledgment for spoken-language systems,&amp;quot; in
Proceedings of 32nd Annual Meeting of the Association for
Computational Linguistics, ACL94, pp. 96-101, Las Cruces,
New Mexico, 1994.
[14] D. Traum, &amp;quot;A Computational Theory of Grounding in
Natural Language Conversation,&amp;quot; Computer Science
Department, University of Rochester, Rochester, NY, Ph.D.
1994.
[15] H. H. Clark and E. F. Schaefer, &amp;quot;Contributing to
discourse,&amp;quot; Cognitive Science, vol. 13, pp. 259-294, 1989.
[16] S. L Oviatt, P. It Cohen, and A. M. Podlozny, &amp;quot;Spoken
language and performance during interpretation,&amp;quot; in
Proceedings of International Conference on Spoken Language
Processing, ICSLP90, pp. 1305-1308, Kobe, Japan, 1990.
[17] S. L. Oviatt and P. It Cohen, &amp;quot;Spoken language in
interpreted telephone dialogues,&amp;quot; Computer Speech and
Language, vol. 6, pp. 277-302, 1992.
[18] Cl Ferguson, J. Allen, and B. Miller, &apos;The design and
implementation of the TRAINS-96 system: A prototype mixed-
initiative planning assistant,&amp;quot; University of Rochester,
Rochester, NY, TRAINS Technical Note 96-5, October 1996
1996.
[19] Cl Ferguson, J. Allen, and B. Miller, &apos;TRAINS-95:
Towards a mixed-initiative planning assistant,&amp;quot; in Proceedings
of Third Conference on Artificial Intelligence Planning
Systems, AIPS 96, pp. 70-77, 1996.
[20] D. Goddeau, E. Brill, J. Glass, C. Pao, M. Phillips, J.
Polifiuni, S. Seneff, and V. Zue, &amp;quot;GALAXY: A Human-
Language Interface to On-Line Travel Information,&amp;quot; in
Proceedings of International Conference on Spoken Language
Processing, ICSLP 94, pp. 707-710, Yokohama, Japan, 1994.
[21] It Lau, G Flammia, C. Pao, and V. Zoe, &amp;quot;WebGALAXY:
Spoken language access to information space from your
favorite browser,&amp;quot; Macsnchusetts Institute of Technology,
Cambridge, MA, URL
http://www.sls.lcs.mitedu/SLSPublications.html, December
1997 1997.
[22] V. The, &amp;quot;Navigating the information superhighway using
spoken language interfaces,&amp;quot; IEEE Expert, pp. 39-43, 1995.
[23] P. R. Cohen, A. Cheyer, M. Wang, and S. C. Baeg, &amp;quot;An
open agent architecture,&amp;quot; in Proceedings of AAAI 1994 Spring
Symposium on Software Agents, pp. 1-8, 1994.
[24] X Huang, A. Acero, F. Alleva, M.-Y. Hwang, L Jiang,
and M. Mahajan, &amp;quot;Microsoft Windows Highly Intelligent
Speech Recognizer Whisper,&amp;quot; in Proceedings of IEEE
International Conference on Acoustics, Speech, and Signal
Processing, ICASSP95, 1995.
[25] P. Heisterlcamp, &amp;quot;Ambiguity and uncertainty in spoken
dialogue,&amp;quot; in Proceedings of EUROSPEECH93 Conference,
pp. 1657-1660, Berlin, Germany, 1993.
[26] Y. Takebayashi, &amp;quot;Chapter 14: Integration of understanding
and synthesis functions for multimedia interfaces,&amp;quot; in
Multimedia interface design, M. M. Blattner and R. B.
Dannenberg, Eds. New York, NY: ACM Press, pp. 233-256,
1992.
[27] A. L Rudnicky and A. Cl Hauptxnann, &amp;quot;Chapter 10:
Multimodal interaction in speech systems,&amp;quot; in Multimedia
Interface Design, M. M. Blattner and R. B. Darmenberg, Eds.
New York, NY: ACM Press, pp. 147-171,1992.
[28] A. Lavie, L Levin, Y. Qu, A. Waibel, and D. Gates,
&apos;Dialogue processing in a conversational speech translation
system,&amp;quot; in Proceedings of International Conference on
Spoken Language Processing, ICSLP 96, pp. 554-557, 1996.
[29] It W. Smith, &amp;quot;An evaluation of strategies for selective
utterance verification for spoken natural language dialog,&amp;quot; in
Proceedings of Fifth Conference on Applied Natural Language
Processing, ANLP96, pp. 41-48,1996.
[30] Y. Niimi and Y. Kobayashi, &amp;quot;A dialog control strategy
based on the reliability of speech recognition,&amp;quot; in Proceedings
of International Conference on Spoken Language Processing,
ICSLP 96, pp. 534-537, 1996.
</reference>
<page confidence="0.998949">
829
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.504941">
<title confidence="0.999981">Confirmation in Multimodal Systems</title>
<author confidence="0.999955">David R McGee</author>
<author confidence="0.999955">Philip R Cohen</author>
<author confidence="0.999955">Sharon Oviatt</author>
<affiliation confidence="0.996981333333333">Center for Human-Computer Communication, Department of Computer Science and Engineering Oregon Graduate Institute</affiliation>
<address confidence="0.999582">P.O. Box 91000, Portland, Oregon 97291-1000</address>
<email confidence="0.974594">dmcgee.pcohen.oviattI@cse.ogi.edu</email>
<abstract confidence="0.967275">Systems that attempt to understand natural human input make mistakes, even humans. However, humans avoid misunderstandings by confirming doubtful input. that combine simultaneous input from more than one modality, for example speech and gesture—have historically been designed so that they either request confirmation of speech, their primary modality, or not at all. Instead, we experimented with delaying confirmation until after the speech and gesture were combined into a complete multimodal command. In controlled experiments, subjects achieved more commands per minute at a lower error rate when the system delayed confirmation, than compared to when subjects confirmed only speech. In addition, this style of confirmation meets the expectation that confirmed commands should be executable. KEYWORDS: multimodal, confirmation, uncertainty, disambiguation &amp;quot;Mistakes are inevitable in dialog.. .In practice, conversation breaks down almost instantly in the absence of a facility to recognize and repair errors, ask clarification questions, give confirmation, and perform disambiguation. [1]&amp;quot;</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Perlis</author>
<author>K Purang</author>
</authors>
<title>Conversational adequacy: Mistakes are the essence,&amp;quot;</title>
<date>1996</date>
<booktitle>in Proceedings of Workshop on Detecting, Repairing, and Preventing Human-Machine Miscommunication, AAAI96,</booktitle>
<contexts>
<context position="1421" citStr="[1]" startWordPosition="191" endWordPosition="191">d. In controlled experiments, subjects achieved more commands per minute at a lower error rate when the system delayed confirmation, than compared to when subjects confirmed only speech. In addition, this style of late confirmation meets the usees expectation that confirmed commands should be executable. KEYWORDS: multimodal, confirmation, uncertainty, disambiguation &amp;quot;Mistakes are inevitable in dialog.. .In practice, conversation breaks down almost instantly in the absence of a facility to recognize and repair errors, ask clarification questions, give confirmation, and perform disambiguation. [1]&amp;quot; INTRODUCTION We claim that multimodal systems [2, 3] that issue commands based on speech and gesture input should not request confirmation of words or ink. Rather, these systems should when there is doubt, request confirmation of their understanding of the combined meaning of each coordinated language act. The purpose of any confirmation act, after all, is to reach agreement on the overall meaning of each command. To test these claims we have extended our multimodal map system, QuickSet [4, 5], so that it can be tuned to request confirmation either before or after integration of modalities. </context>
</contexts>
<marker>[1]</marker>
<rawString>D. Perlis and K. Purang, &amp;quot;Conversational adequacy: Mistakes are the essence,&amp;quot; in Proceedings of Workshop on Detecting, Repairing, and Preventing Human-Machine Miscommunication, AAAI96, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bolt</author>
</authors>
<title>Put-That-There: Voice and gesture at the graphics interface,&amp;quot;</title>
<date>1980</date>
<journal>Computer Graphics,</journal>
<volume>14</volume>
<pages>262--270</pages>
<contexts>
<context position="1475" citStr="[2, 3]" startWordPosition="198" endWordPosition="199">e commands per minute at a lower error rate when the system delayed confirmation, than compared to when subjects confirmed only speech. In addition, this style of late confirmation meets the usees expectation that confirmed commands should be executable. KEYWORDS: multimodal, confirmation, uncertainty, disambiguation &amp;quot;Mistakes are inevitable in dialog.. .In practice, conversation breaks down almost instantly in the absence of a facility to recognize and repair errors, ask clarification questions, give confirmation, and perform disambiguation. [1]&amp;quot; INTRODUCTION We claim that multimodal systems [2, 3] that issue commands based on speech and gesture input should not request confirmation of words or ink. Rather, these systems should when there is doubt, request confirmation of their understanding of the combined meaning of each coordinated language act. The purpose of any confirmation act, after all, is to reach agreement on the overall meaning of each command. To test these claims we have extended our multimodal map system, QuickSet [4, 5], so that it can be tuned to request confirmation either before or after integration of modalities. Using QuickSet, we have conducted an empirical study t</context>
<context position="2890" citStr="[2, 3]" startWordPosition="407" endWordPosition="408">ares early and late confirmation strategies, the results of that experiment, and our conclusions. Command-driven conversational systems need to identify hindrances to accurate understanding and execution of commands in order to avoid miscommunication. These hindrances can arise from at least three sources: Uncettairay—lack of confidence in interpretation of the input, Ambiguity—equally likely interpretations of input, and Infeasibility—an inability to perform the command Suppose that we use a recognition system that interprets natural human input [6], that is capable of multimodal interaction [2, 3], and that will let users place simulated military units and related objects on a map. When we use this system, our words and stylus movements are simultaneously recognized, interpreted, and blended together. A user calls out the names of objects, such as &amp;quot;ROMEO ONE EAGLE,&amp;quot; while marking the map with a gesture. If the system is confident of its recognition of the input, it might interpret this command in the following !limiec a unit should be placed on the map at the specified location. Another equally likely interpretation, looking only at the results of speech recognition, might be to select</context>
</contexts>
<marker>[2]</marker>
<rawString>R. Bolt, &amp;quot;Put-That-There: Voice and gesture at the graphics interface,&amp;quot; Computer Graphics, vol. 14, pp. 262-270, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Vo</author>
<author>C Wood</author>
</authors>
<title>Building an Application Framework for Speech and Pen Input Integration in Multimodal Learning Interfaces,&amp;quot;</title>
<date>1996</date>
<booktitle>in Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 96,</booktitle>
<location>Atlanta, GA,</location>
<contexts>
<context position="1475" citStr="[2, 3]" startWordPosition="198" endWordPosition="199">e commands per minute at a lower error rate when the system delayed confirmation, than compared to when subjects confirmed only speech. In addition, this style of late confirmation meets the usees expectation that confirmed commands should be executable. KEYWORDS: multimodal, confirmation, uncertainty, disambiguation &amp;quot;Mistakes are inevitable in dialog.. .In practice, conversation breaks down almost instantly in the absence of a facility to recognize and repair errors, ask clarification questions, give confirmation, and perform disambiguation. [1]&amp;quot; INTRODUCTION We claim that multimodal systems [2, 3] that issue commands based on speech and gesture input should not request confirmation of words or ink. Rather, these systems should when there is doubt, request confirmation of their understanding of the combined meaning of each coordinated language act. The purpose of any confirmation act, after all, is to reach agreement on the overall meaning of each command. To test these claims we have extended our multimodal map system, QuickSet [4, 5], so that it can be tuned to request confirmation either before or after integration of modalities. Using QuickSet, we have conducted an empirical study t</context>
<context position="2890" citStr="[2, 3]" startWordPosition="407" endWordPosition="408">ares early and late confirmation strategies, the results of that experiment, and our conclusions. Command-driven conversational systems need to identify hindrances to accurate understanding and execution of commands in order to avoid miscommunication. These hindrances can arise from at least three sources: Uncettairay—lack of confidence in interpretation of the input, Ambiguity—equally likely interpretations of input, and Infeasibility—an inability to perform the command Suppose that we use a recognition system that interprets natural human input [6], that is capable of multimodal interaction [2, 3], and that will let users place simulated military units and related objects on a map. When we use this system, our words and stylus movements are simultaneously recognized, interpreted, and blended together. A user calls out the names of objects, such as &amp;quot;ROMEO ONE EAGLE,&amp;quot; while marking the map with a gesture. If the system is confident of its recognition of the input, it might interpret this command in the following !limiec a unit should be placed on the map at the specified location. Another equally likely interpretation, looking only at the results of speech recognition, might be to select</context>
<context position="21052" citStr="[3]" startWordPosition="3162" endWordPosition="3162"> compared to explicit confirmation [27], and our results confirm their findings. Lavie and colleagues have shown the usefulness of latestage disambiguation, during which speechunderstanding systems pass multiple interpretations through the system, using context in the final stages of processing to disambiguate the recognition hypotheses [28]. However, we have demonstrated and empirically shown the advantage in combining these two strategies in a multimodal system. It can be argued that implicit confirmation is equivalent to being able to undo the last command, as some multimodal systems allow [3]. However, commands that are infeasible, profound, risky, costly, or irreversible are difficult to undo. For this reason, we argue that implicit confirmation is often superior to the option of undoing the previous command. Implicit confirmation, when combined with late confirmation, contributes to a smoother, faster, and more accurate collaboration between human and computer. 7 CONCLUSIONS We have developed a system that meets the following expectation: when the proposition being confirmed is a command, it should be one that the system believes can be executed. To meet this expectation and inc</context>
</contexts>
<marker>[3]</marker>
<rawString>M. T. Vo and C. Wood, &amp;quot;Building an Application Framework for Speech and Pen Input Integration in Multimodal Learning Interfaces,&amp;quot; in Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 96, Atlanta, GA, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P It Cohen</author>
<author>M Johnston</author>
<author>D McGee</author>
<author>I Smith</author>
<author>J Pittman</author>
<author>L Chen</author>
<author>J Clow</author>
</authors>
<title>Multimodal interaction for distributed interactive simulation,&amp;quot;</title>
<date>1997</date>
<booktitle>in Proceedings of Innovative Applications of Artificial Intelligence Conference, I4AI97,</booktitle>
<location>Menlo Park, CA,</location>
<contexts>
<context position="1921" citStr="[4, 5]" startWordPosition="270" endWordPosition="271">cognize and repair errors, ask clarification questions, give confirmation, and perform disambiguation. [1]&amp;quot; INTRODUCTION We claim that multimodal systems [2, 3] that issue commands based on speech and gesture input should not request confirmation of words or ink. Rather, these systems should when there is doubt, request confirmation of their understanding of the combined meaning of each coordinated language act. The purpose of any confirmation act, after all, is to reach agreement on the overall meaning of each command. To test these claims we have extended our multimodal map system, QuickSet [4, 5], so that it can be tuned to request confirmation either before or after integration of modalities. Using QuickSet, we have conducted an empirical study that indicates agreement about the correctness of commands can be reached quicker if confirmation is delayed until after blending. This paper describes QuickSet, our experiences with it, an experiment that compares early and late confirmation strategies, the results of that experiment, and our conclusions. Command-driven conversational systems need to identify hindrances to accurate understanding and execution of commands in order to avoid mis</context>
<context position="10415" citStr="[4,5]" startWordPosition="1540" endWordPosition="1540">mmand. Second, because of multimodal disambiguation, misunderstandings can be reduced, and therefore the number of conversational turns required to reach mutual understanding can be reduced as well. Finally, a reduction in turns combined with a reduction in time spent will lead to reducing the &amp;quot;collaborative effort&amp;quot; in the dialogue. To examine our hypotheses, we designed an experiment using QuickSet to determine if late-stage confirmations enhance humancomputer conversational performance. 2 QUICKSET This section describes QuickSet, a suite of agents for multimodal human-computer communication [4,5]. 2.1 A Multi-Agent Architecture Underneath the QuickSet suite of agents lies a distributed, blackboard-based, multi-agent architecture based on the Open Agent Architecture&apos; [23]. The blackboard acts as a repository of shared information and facilitator. The agents rely on it for brokering, message distribution, and notification. The Open Agent Architecture is a trademark of SRI International. 2.2 The QuickSet Agents The following section briefly summarizes the responsibilities of each agent, their interaction, and the results of their computation. 2.2.1 User Interface The user draws on and sp</context>
</contexts>
<marker>[4]</marker>
<rawString>P. It Cohen, M. Johnston, D. McGee, I. Smith, J. Pittman, L. Chen, and J. Clow, &amp;quot;Multimodal interaction for distributed interactive simulation,&amp;quot; in Proceedings of Innovative Applications of Artificial Intelligence Conference, I4AI97, Menlo Park, CA, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>P R Cohen</author>
<author>D McGee</author>
<author>S L Oviatt</author>
<author>J A Pittman</author>
<author>L Smith</author>
</authors>
<title>Unification-based multimodal integration,&amp;quot;</title>
<date>1997</date>
<booktitle>in Proceedings of 35th Annual Meeting of the Association for Computational Linguistics, ACL97,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1921" citStr="[4, 5]" startWordPosition="270" endWordPosition="271">cognize and repair errors, ask clarification questions, give confirmation, and perform disambiguation. [1]&amp;quot; INTRODUCTION We claim that multimodal systems [2, 3] that issue commands based on speech and gesture input should not request confirmation of words or ink. Rather, these systems should when there is doubt, request confirmation of their understanding of the combined meaning of each coordinated language act. The purpose of any confirmation act, after all, is to reach agreement on the overall meaning of each command. To test these claims we have extended our multimodal map system, QuickSet [4, 5], so that it can be tuned to request confirmation either before or after integration of modalities. Using QuickSet, we have conducted an empirical study that indicates agreement about the correctness of commands can be reached quicker if confirmation is delayed until after blending. This paper describes QuickSet, our experiences with it, an experiment that compares early and late confirmation strategies, the results of that experiment, and our conclusions. Command-driven conversational systems need to identify hindrances to accurate understanding and execution of commands in order to avoid mis</context>
<context position="10415" citStr="[4,5]" startWordPosition="1540" endWordPosition="1540">mmand. Second, because of multimodal disambiguation, misunderstandings can be reduced, and therefore the number of conversational turns required to reach mutual understanding can be reduced as well. Finally, a reduction in turns combined with a reduction in time spent will lead to reducing the &amp;quot;collaborative effort&amp;quot; in the dialogue. To examine our hypotheses, we designed an experiment using QuickSet to determine if late-stage confirmations enhance humancomputer conversational performance. 2 QUICKSET This section describes QuickSet, a suite of agents for multimodal human-computer communication [4,5]. 2.1 A Multi-Agent Architecture Underneath the QuickSet suite of agents lies a distributed, blackboard-based, multi-agent architecture based on the Open Agent Architecture&apos; [23]. The blackboard acts as a repository of shared information and facilitator. The agents rely on it for brokering, message distribution, and notification. The Open Agent Architecture is a trademark of SRI International. 2.2 The QuickSet Agents The following section briefly summarizes the responsibilities of each agent, their interaction, and the results of their computation. 2.2.1 User Interface The user draws on and sp</context>
<context position="11659" citStr="[5]" startWordPosition="1725" endWordPosition="1725"> snapshot of the interface) to place objects on the map, assign attributes and behaviors to them, and ask questions about them. ,s&amp; otr I imrixe■ 3 (.6 471.71 aoalei . Figure 2. QuickSet Early Confirmation Mode 2.2.2 Gesture Recognition The gesture recognition agent recognizes gestures from strokes drawn on the map. Along with coordinate values, each stroke from the user interface provides contextual information about objects touched or encircled by the stroke. Recognition results are an n-best list (top n-ranked) of interpretations. The interpretations are encoded as typed feature structures [5], which represent each of the potential semantic contributions of the gesture. This list is then passed to the multimodal integrator. 2.2.3 Speech Recognition The Whisper speech recognition engine from Microsoft Corp. [24] drives the speech recognition agent. It offers speaker-independent, continuous recognition in close to real time. QuickSet relies upon a context-free domain grammar, specifically designed for each application, to constrain the speech recognizer. The speech recognizer 825 agent&apos;s output is also an n-best list of hypotheses and their probability estimates. These results are pa</context>
<context position="12981" citStr="[5]" startWordPosition="1909" endWordPosition="1909">agent parses the output of the speech recognizer attempting to provide meaningful semantic interpretations based on a domainspecific grammar. This process may introduce further ambiguity; that is, more hypotheses. The results of parsing are, again, in the form of an n-best list of typed feature structures. When complete, the results of natural language interpretation are passed to the integrator for multimodal integration. 2.2.5 Multimodal Integration The multimodal integration agent accepts typed feature structures from the gesture and natural language interpretation agents, and unifies them [5]. The process of integration ensures that modes combine according to a multimodal language specification, and that they meet certain multimodal timing and command-specific constraints. These constraints place limits on when different input can occur, thus reducing errors [7]. If after unification and constraint satisfaction, there is more than one completely specified command, the agent then computes the joint probabilities for each and passes the feature structure with the highest to the bridge. If, on the other hand, no completely specified command exists, a message is sent to the user inter</context>
</contexts>
<marker>[5]</marker>
<rawString>M. Johnston, P. R. Cohen, D. McGee, S. L Oviatt, J. A. Pittman, and L Smith, &amp;quot;Unification-based multimodal integration,&amp;quot; in Proceedings of 35th Annual Meeting of the Association for Computational Linguistics, ACL97, Madrid, Spain, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J It Rhyne</author>
<author>C G Wolf</author>
</authors>
<title>Chapter 7: Recognitionbased user interfaces,&amp;quot;</title>
<date>1992</date>
<booktitle>in Advances in Human-Computer Interaction,</booktitle>
<volume>4</volume>
<pages>191--250</pages>
<contexts>
<context position="2840" citStr="[6]" startWordPosition="400" endWordPosition="400">ur experiences with it, an experiment that compares early and late confirmation strategies, the results of that experiment, and our conclusions. Command-driven conversational systems need to identify hindrances to accurate understanding and execution of commands in order to avoid miscommunication. These hindrances can arise from at least three sources: Uncettairay—lack of confidence in interpretation of the input, Ambiguity—equally likely interpretations of input, and Infeasibility—an inability to perform the command Suppose that we use a recognition system that interprets natural human input [6], that is capable of multimodal interaction [2, 3], and that will let users place simulated military units and related objects on a map. When we use this system, our words and stylus movements are simultaneously recognized, interpreted, and blended together. A user calls out the names of objects, such as &amp;quot;ROMEO ONE EAGLE,&amp;quot; while marking the map with a gesture. If the system is confident of its recognition of the input, it might interpret this command in the following !limiec a unit should be placed on the map at the specified location. Another equally likely interpretation, looking only at the</context>
<context position="5045" citStr="[6, 10]" startWordPosition="747" endWordPosition="748">ndition, if &amp;quot;ROMEO ONE EAGLE&amp;quot; is not already present on the map, the user cannot select it. We call these techniques multimodal disambiguation techniques. Regardless, if a system receives input that it fmds uncertain, ambiguous, or infeasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully per</context>
</contexts>
<marker>[6]</marker>
<rawString>J. It Rhyne and C. G Wolf, &amp;quot;Chapter 7: Recognitionbased user interfaces,&amp;quot; in Advances in Human-Computer Interaction, vol. 4, H. R. Hanson and D. Hix, Eds., pp. 191-250, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oviatt</author>
<author>A DeAngeli</author>
<author>K Kuhn</author>
</authors>
<title>Integration and synchronization of input modes during multimodal humancomputer interaction,&amp;quot;</title>
<date>1997</date>
<booktitle>in Proceedings of Conference on Human Factors in Computing Systems, CHI97,</booktitle>
<pages>415--422</pages>
<location>Atlanta, GA,</location>
<contexts>
<context position="3972" citStr="[7, 8, 9]" startWordPosition="578" endWordPosition="578">pecified location. Another equally likely interpretation, looking only at the results of speech recognition, might be to select an existing &amp;quot;ROMEO ONE EAGLE.&amp;quot; Since this multimodal system is performing recognition, uncertainty inevitably exists in the recognizer&apos;s hypotheses. &amp;quot;ROMEO ONE EAGLE&amp;quot; may not be recognized with a high degree of confidence. It may not even be the most likely hypothesis. One way to disambiguate the hypotheses is with the multimodal language specification itself, the way we allow modalities to combine. Since different modalities tend to capture complementary information [7, 8, 9], we can leverage this facility by combining ambiguous 823 spoken interpretations with disimilar gestures. For example, we might specify that selection gestures (circling) combine with the ambiguous speech from above to produce a selection command. Another way of disambiguating the spoken utterance is to enforce a precondition for the command: for example, for the selection command to be possible the object must already exist on the map. Thus, under such a precondition, if &amp;quot;ROMEO ONE EAGLE&amp;quot; is not already present on the map, the user cannot select it. We call these techniques multimodal disamb</context>
<context position="13256" citStr="[7]" startWordPosition="1947" endWordPosition="1947">ist of typed feature structures. When complete, the results of natural language interpretation are passed to the integrator for multimodal integration. 2.2.5 Multimodal Integration The multimodal integration agent accepts typed feature structures from the gesture and natural language interpretation agents, and unifies them [5]. The process of integration ensures that modes combine according to a multimodal language specification, and that they meet certain multimodal timing and command-specific constraints. These constraints place limits on when different input can occur, thus reducing errors [7]. If after unification and constraint satisfaction, there is more than one completely specified command, the agent then computes the joint probabilities for each and passes the feature structure with the highest to the bridge. If, on the other hand, no completely specified command exists, a message is sent to the user interface, asking it to inform the user of the non-understanding. 2.2.6 Bridge to Application Systems The bridge agent acts as a single message-based interface to domain applications. When it receives a feature structure, it sends a message to the appropriate applications, reques</context>
</contexts>
<marker>[7]</marker>
<rawString>S. Oviatt, A. DeAngeli, and K. Kuhn, &amp;quot;Integration and synchronization of input modes during multimodal humancomputer interaction,&amp;quot; in Proceedings of Conference on Human Factors in Computing Systems, CHI97, pp. 415-422, Atlanta, GA, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lefebvre</author>
<author>G Duncan</author>
<author>F Poirier</author>
</authors>
<title>Speaking with computers: A multimodal approach,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of EUROSPEECH93 Conference,</booktitle>
<pages>1665--1668</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="3972" citStr="[7, 8, 9]" startWordPosition="578" endWordPosition="578">pecified location. Another equally likely interpretation, looking only at the results of speech recognition, might be to select an existing &amp;quot;ROMEO ONE EAGLE.&amp;quot; Since this multimodal system is performing recognition, uncertainty inevitably exists in the recognizer&apos;s hypotheses. &amp;quot;ROMEO ONE EAGLE&amp;quot; may not be recognized with a high degree of confidence. It may not even be the most likely hypothesis. One way to disambiguate the hypotheses is with the multimodal language specification itself, the way we allow modalities to combine. Since different modalities tend to capture complementary information [7, 8, 9], we can leverage this facility by combining ambiguous 823 spoken interpretations with disimilar gestures. For example, we might specify that selection gestures (circling) combine with the ambiguous speech from above to produce a selection command. Another way of disambiguating the spoken utterance is to enforce a precondition for the command: for example, for the selection command to be possible the object must already exist on the map. Thus, under such a precondition, if &amp;quot;ROMEO ONE EAGLE&amp;quot; is not already present on the map, the user cannot select it. We call these techniques multimodal disamb</context>
</contexts>
<marker>[8]</marker>
<rawString>P. Lefebvre, G Duncan, and F. Poirier, &amp;quot;Speaking with computers: A multimodal approach,&amp;quot; in Proceedings of EUROSPEECH93 Conference, pp. 1665-1668, Berlin, Germany, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Morin</author>
<author>J Junqua</author>
</authors>
<title>Habitable interaction in goaloriented multimodal dialogue systems,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of EUROSPEECH93 Conference,</booktitle>
<pages>1669--1672</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="3972" citStr="[7, 8, 9]" startWordPosition="578" endWordPosition="578">pecified location. Another equally likely interpretation, looking only at the results of speech recognition, might be to select an existing &amp;quot;ROMEO ONE EAGLE.&amp;quot; Since this multimodal system is performing recognition, uncertainty inevitably exists in the recognizer&apos;s hypotheses. &amp;quot;ROMEO ONE EAGLE&amp;quot; may not be recognized with a high degree of confidence. It may not even be the most likely hypothesis. One way to disambiguate the hypotheses is with the multimodal language specification itself, the way we allow modalities to combine. Since different modalities tend to capture complementary information [7, 8, 9], we can leverage this facility by combining ambiguous 823 spoken interpretations with disimilar gestures. For example, we might specify that selection gestures (circling) combine with the ambiguous speech from above to produce a selection command. Another way of disambiguating the spoken utterance is to enforce a precondition for the command: for example, for the selection command to be possible the object must already exist on the map. Thus, under such a precondition, if &amp;quot;ROMEO ONE EAGLE&amp;quot; is not already present on the map, the user cannot select it. We call these techniques multimodal disamb</context>
</contexts>
<marker>[9]</marker>
<rawString>P. Morin and J. Junqua, &amp;quot;Habitable interaction in goaloriented multimodal dialogue systems,&amp;quot; in Proceedings of EUROSPEECH93 Conference, pp. 1669-1672, Berlin, Germany, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>C Pao</author>
</authors>
<title>The cost of errors in a spoken language system,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of EUROSPEECH93 Conference,</booktitle>
<pages>1419--1422</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="5045" citStr="[6, 10]" startWordPosition="747" endWordPosition="748">ndition, if &amp;quot;ROMEO ONE EAGLE&amp;quot; is not already present on the map, the user cannot select it. We call these techniques multimodal disambiguation techniques. Regardless, if a system receives input that it fmds uncertain, ambiguous, or infeasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully per</context>
</contexts>
<marker>[10]</marker>
<rawString>L. Hirschman and C. Pao, &apos;The cost of errors in a spoken language system,&amp;quot; in Proceedings of EUROSPEECH93 Conference, pp. 1419-1422, Berlin, Germany, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process,&amp;quot;</title>
<date>1986</date>
<journal>Cognition,</journal>
<volume>13</volume>
<pages>259--294</pages>
<contexts>
<context position="5191" citStr="[11, 12, 13, 14]" startWordPosition="771" endWordPosition="771">ation techniques. Regardless, if a system receives input that it fmds uncertain, ambiguous, or infeasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16,</context>
</contexts>
<marker>[11]</marker>
<rawString>H. Clark and D. Wilkes-Gibbs, &amp;quot;Referring as a collaborative process,&amp;quot; Cognition, vol. 13, pp. 259-294, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>H J Levesque</author>
</authors>
<title>Confirmations and joint action,&amp;quot; in</title>
<date>1991</date>
<booktitle>Proceedings of International Joint Conference on Artificial Intelligence,</booktitle>
<pages>951--957</pages>
<contexts>
<context position="5191" citStr="[11, 12, 13, 14]" startWordPosition="771" endWordPosition="771">ation techniques. Regardless, if a system receives input that it fmds uncertain, ambiguous, or infeasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16,</context>
</contexts>
<marker>[12]</marker>
<rawString>P. R. Cohen and H. J. Levesque, &amp;quot;Confirmations and joint action,&amp;quot; in Proceedings of International Joint Conference on Artificial Intelligence, pp. 951-957, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cl Novick</author>
<author>S Sutton</author>
</authors>
<title>An empirical model of acknowledgment for spoken-language systems,&amp;quot;</title>
<date>1994</date>
<booktitle>in Proceedings of 32nd Annual Meeting of the Association for Computational Linguistics, ACL94,</booktitle>
<pages>96--101</pages>
<location>Las Cruces, New</location>
<contexts>
<context position="5191" citStr="[11, 12, 13, 14]" startWordPosition="771" endWordPosition="771">ation techniques. Regardless, if a system receives input that it fmds uncertain, ambiguous, or infeasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16,</context>
</contexts>
<marker>[13]</marker>
<rawString>D. Cl Novick and S. Sutton, &amp;quot;An empirical model of acknowledgment for spoken-language systems,&amp;quot; in Proceedings of 32nd Annual Meeting of the Association for Computational Linguistics, ACL94, pp. 96-101, Las Cruces, New Mexico, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
</authors>
<title>A Computational Theory of Grounding in Natural Language Conversation,&amp;quot;</title>
<date>1994</date>
<institution>Computer Science Department, University of Rochester,</institution>
<location>Rochester, NY, Ph.D.</location>
<contexts>
<context position="5191" citStr="[11, 12, 13, 14]" startWordPosition="771" endWordPosition="771">ation techniques. Regardless, if a system receives input that it fmds uncertain, ambiguous, or infeasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16,</context>
</contexts>
<marker>[14]</marker>
<rawString>D. Traum, &amp;quot;A Computational Theory of Grounding in Natural Language Conversation,&amp;quot; Computer Science Department, University of Rochester, Rochester, NY, Ph.D. 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>E F Schaefer</author>
</authors>
<title>Contributing to discourse,&amp;quot;</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<volume>13</volume>
<pages>259--294</pages>
<contexts>
<context position="5276" citStr="[15]" startWordPosition="783" endWordPosition="783">feasible, or if its effect might be profound, risky, costly, or irreversible, it may want to verify its interpretation of the command with the user. For example, a system prepared to execute the command &amp;quot;DESTROY ALL DATA&amp;quot; should give the speaker a chance to change or correct the command. Otherwise, the cost of such errors is task-dependent and can be immeasurable [6, 10]. Therefore, we claim that conversational systems should be able to request the user to confirm the command, as humans tend to do [11, 12, 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16, 17], and thus collaborative effort. In fact, the more likely miscommunication, the m</context>
</contexts>
<marker>[15]</marker>
<rawString>H. H. Clark and E. F. Schaefer, &amp;quot;Contributing to discourse,&amp;quot; Cognitive Science, vol. 13, pp. 259-294, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>P It Cohen</author>
<author>A M Podlozny</author>
</authors>
<title>Spoken language and performance during interpretation,&amp;quot;</title>
<date>1990</date>
<booktitle>in Proceedings of International Conference on Spoken Language Processing, ICSLP90,</booktitle>
<pages>1305--1308</pages>
<location>Kobe, Japan,</location>
<contexts>
<context position="5795" citStr="[13, 16, 17]" startWordPosition="861" endWordPosition="863"> 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16, 17], and thus collaborative effort. In fact, the more likely miscommunication, the more frequently people introduce confirmations [16, 17]. To ensure that common ground is achieved, miscommunication is avoided, and collaborative effort is reduced, system designers must determine when and how confirmations ought to be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmati</context>
</contexts>
<marker>[16]</marker>
<rawString>S. L Oviatt, P. It Cohen, and A. M. Podlozny, &amp;quot;Spoken language and performance during interpretation,&amp;quot; in Proceedings of International Conference on Spoken Language Processing, ICSLP90, pp. 1305-1308, Kobe, Japan, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>P It Cohen</author>
</authors>
<title>Spoken language in interpreted telephone dialogues,&amp;quot;</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<pages>277--302</pages>
<contexts>
<context position="5795" citStr="[13, 16, 17]" startWordPosition="861" endWordPosition="863"> 13, 14]. Such confirmations are used &amp;quot;to achieve common ground&amp;quot; in human-human dialogue [15]. On their way to achieving common ground, participants attempt to minimize their collaborative effort, &amp;quot;the work that both do from the initiation of [a command] to its completion.&amp;quot; [15] Herein we will further define collaborative effort in terms of work in a command-based collaborative dialogue, where an increase in the rate at which commands can be successfully performed corresponds to a reduction in the collaborative effort. We know that confirmations are an important way to reduce miscommunication [13, 16, 17], and thus collaborative effort. In fact, the more likely miscommunication, the more frequently people introduce confirmations [16, 17]. To ensure that common ground is achieved, miscommunication is avoided, and collaborative effort is reduced, system designers must determine when and how confirmations ought to be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmati</context>
</contexts>
<marker>[17]</marker>
<rawString>S. L. Oviatt and P. It Cohen, &amp;quot;Spoken language in interpreted telephone dialogues,&amp;quot; Computer Speech and Language, vol. 6, pp. 277-302, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cl Ferguson</author>
<author>J Allen</author>
<author>B Miller</author>
</authors>
<title>The design and implementation of the TRAINS-96 system: A prototype mixedinitiative planning assistant,&amp;quot;</title>
<date>1996</date>
<tech>TRAINS Technical Note 96-5,</tech>
<institution>University of Rochester,</institution>
<location>Rochester, NY,</location>
<contexts>
<context position="6726" citStr="[18, 19, 20, 21, 22]" startWordPosition="991" endWordPosition="991">o be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmation. For example, confirmations could be performed immediately after recognition of one or both modalities. However, we will show that collaborative effort can be reduced if multimodal systems delay confirmation until after blending. 1 MOTIVATION Historically, multimodal systems have either not confirmed input [18, 19, 20, 21, 22] or confirmed only the primary modality of such systems—speech. This is reasonable, considering the evolution of multimodal systems from their speech-based roots. Observations of QuickSet prototypes last year, however, showed that simply confirming the results of speech recognition was often problematic—users had the expectation that whenever a command was confirmed, it would be executed. We observed that confirming speech prior to multimodal integration led to three possible cases where this expectation might not be met: ambiguous gestures, nonmeaningful speech, and delayed confirmation. The </context>
</contexts>
<marker>[18]</marker>
<rawString>Cl Ferguson, J. Allen, and B. Miller, &apos;The design and implementation of the TRAINS-96 system: A prototype mixedinitiative planning assistant,&amp;quot; University of Rochester, Rochester, NY, TRAINS Technical Note 96-5, October 1996 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cl Ferguson</author>
<author>J Allen</author>
<author>B Miller</author>
</authors>
<title>TRAINS-95: Towards a mixed-initiative planning assistant,&amp;quot;</title>
<date>1996</date>
<booktitle>in Proceedings of Third Conference on Artificial Intelligence Planning Systems, AIPS 96,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="6726" citStr="[18, 19, 20, 21, 22]" startWordPosition="991" endWordPosition="991">o be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmation. For example, confirmations could be performed immediately after recognition of one or both modalities. However, we will show that collaborative effort can be reduced if multimodal systems delay confirmation until after blending. 1 MOTIVATION Historically, multimodal systems have either not confirmed input [18, 19, 20, 21, 22] or confirmed only the primary modality of such systems—speech. This is reasonable, considering the evolution of multimodal systems from their speech-based roots. Observations of QuickSet prototypes last year, however, showed that simply confirming the results of speech recognition was often problematic—users had the expectation that whenever a command was confirmed, it would be executed. We observed that confirming speech prior to multimodal integration led to three possible cases where this expectation might not be met: ambiguous gestures, nonmeaningful speech, and delayed confirmation. The </context>
</contexts>
<marker>[19]</marker>
<rawString>Cl Ferguson, J. Allen, and B. Miller, &apos;TRAINS-95: Towards a mixed-initiative planning assistant,&amp;quot; in Proceedings of Third Conference on Artificial Intelligence Planning Systems, AIPS 96, pp. 70-77, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goddeau</author>
<author>E Brill</author>
<author>J Glass</author>
<author>C Pao</author>
<author>M Phillips</author>
<author>J Polifiuni</author>
<author>S Seneff</author>
<author>V Zue</author>
</authors>
<title>GALAXY: A HumanLanguage Interface to On-Line Travel Information,&amp;quot;</title>
<date>1994</date>
<booktitle>in Proceedings of International Conference on Spoken Language Processing, ICSLP 94,</booktitle>
<pages>707--710</pages>
<location>Yokohama, Japan,</location>
<contexts>
<context position="6726" citStr="[18, 19, 20, 21, 22]" startWordPosition="991" endWordPosition="991">o be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmation. For example, confirmations could be performed immediately after recognition of one or both modalities. However, we will show that collaborative effort can be reduced if multimodal systems delay confirmation until after blending. 1 MOTIVATION Historically, multimodal systems have either not confirmed input [18, 19, 20, 21, 22] or confirmed only the primary modality of such systems—speech. This is reasonable, considering the evolution of multimodal systems from their speech-based roots. Observations of QuickSet prototypes last year, however, showed that simply confirming the results of speech recognition was often problematic—users had the expectation that whenever a command was confirmed, it would be executed. We observed that confirming speech prior to multimodal integration led to three possible cases where this expectation might not be met: ambiguous gestures, nonmeaningful speech, and delayed confirmation. The </context>
</contexts>
<marker>[20]</marker>
<rawString>D. Goddeau, E. Brill, J. Glass, C. Pao, M. Phillips, J. Polifiuni, S. Seneff, and V. Zue, &amp;quot;GALAXY: A HumanLanguage Interface to On-Line Travel Information,&amp;quot; in Proceedings of International Conference on Spoken Language Processing, ICSLP 94, pp. 707-710, Yokohama, Japan, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>It Lau</author>
<author>G Flammia</author>
<author>C Pao</author>
<author>V Zoe</author>
</authors>
<title>WebGALAXY: Spoken language access to information space from your favorite browser,&amp;quot;</title>
<date>1997</date>
<institution>Macsnchusetts Institute of Technology,</institution>
<location>Cambridge, MA, URL http://www.sls.lcs.mitedu/SLSPublications.html,</location>
<contexts>
<context position="6726" citStr="[18, 19, 20, 21, 22]" startWordPosition="991" endWordPosition="991">o be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmation. For example, confirmations could be performed immediately after recognition of one or both modalities. However, we will show that collaborative effort can be reduced if multimodal systems delay confirmation until after blending. 1 MOTIVATION Historically, multimodal systems have either not confirmed input [18, 19, 20, 21, 22] or confirmed only the primary modality of such systems—speech. This is reasonable, considering the evolution of multimodal systems from their speech-based roots. Observations of QuickSet prototypes last year, however, showed that simply confirming the results of speech recognition was often problematic—users had the expectation that whenever a command was confirmed, it would be executed. We observed that confirming speech prior to multimodal integration led to three possible cases where this expectation might not be met: ambiguous gestures, nonmeaningful speech, and delayed confirmation. The </context>
</contexts>
<marker>[21]</marker>
<rawString>It Lau, G Flammia, C. Pao, and V. Zoe, &amp;quot;WebGALAXY: Spoken language access to information space from your favorite browser,&amp;quot; Macsnchusetts Institute of Technology, Cambridge, MA, URL http://www.sls.lcs.mitedu/SLSPublications.html, December 1997 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V The</author>
</authors>
<title>Navigating the information superhighway using spoken language interfaces,&amp;quot;</title>
<date>1995</date>
<journal>IEEE Expert,</journal>
<pages>39--43</pages>
<contexts>
<context position="6726" citStr="[18, 19, 20, 21, 22]" startWordPosition="991" endWordPosition="991">o be requested. Should a confirmation occur for each modality or should confirmation be delayed until the modalities have been blended? Choosing to confirm speech and gesture separately, or speech alone (as many contemporary multimodal systems do), might simplify the process of confirmation. For example, confirmations could be performed immediately after recognition of one or both modalities. However, we will show that collaborative effort can be reduced if multimodal systems delay confirmation until after blending. 1 MOTIVATION Historically, multimodal systems have either not confirmed input [18, 19, 20, 21, 22] or confirmed only the primary modality of such systems—speech. This is reasonable, considering the evolution of multimodal systems from their speech-based roots. Observations of QuickSet prototypes last year, however, showed that simply confirming the results of speech recognition was often problematic—users had the expectation that whenever a command was confirmed, it would be executed. We observed that confirming speech prior to multimodal integration led to three possible cases where this expectation might not be met: ambiguous gestures, nonmeaningful speech, and delayed confirmation. The </context>
</contexts>
<marker>[22]</marker>
<rawString>V. The, &amp;quot;Navigating the information superhighway using spoken language interfaces,&amp;quot; IEEE Expert, pp. 39-43, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>A Cheyer</author>
<author>M Wang</author>
<author>S C Baeg</author>
</authors>
<title>An open agent architecture,&amp;quot;</title>
<date>1994</date>
<booktitle>in Proceedings of AAAI 1994 Spring Symposium on Software Agents,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10593" citStr="[23]" startWordPosition="1563" endWordPosition="1563">be reduced as well. Finally, a reduction in turns combined with a reduction in time spent will lead to reducing the &amp;quot;collaborative effort&amp;quot; in the dialogue. To examine our hypotheses, we designed an experiment using QuickSet to determine if late-stage confirmations enhance humancomputer conversational performance. 2 QUICKSET This section describes QuickSet, a suite of agents for multimodal human-computer communication [4,5]. 2.1 A Multi-Agent Architecture Underneath the QuickSet suite of agents lies a distributed, blackboard-based, multi-agent architecture based on the Open Agent Architecture&apos; [23]. The blackboard acts as a repository of shared information and facilitator. The agents rely on it for brokering, message distribution, and notification. The Open Agent Architecture is a trademark of SRI International. 2.2 The QuickSet Agents The following section briefly summarizes the responsibilities of each agent, their interaction, and the results of their computation. 2.2.1 User Interface The user draws on and speaks to the interface (see Figure 2 for a snapshot of the interface) to place objects on the map, assign attributes and behaviors to them, and ask questions about them. ,s&amp; otr I</context>
</contexts>
<marker>[23]</marker>
<rawString>P. R. Cohen, A. Cheyer, M. Wang, and S. C. Baeg, &amp;quot;An open agent architecture,&amp;quot; in Proceedings of AAAI 1994 Spring Symposium on Software Agents, pp. 1-8, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
<author>A Acero</author>
<author>F Alleva</author>
<author>M-Y Hwang</author>
<author>L Jiang</author>
<author>M Mahajan</author>
</authors>
<title>Microsoft Windows Highly Intelligent Speech Recognizer Whisper,&amp;quot;</title>
<date>1995</date>
<booktitle>in Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP95,</booktitle>
<contexts>
<context position="11881" citStr="[24]" startWordPosition="1757" endWordPosition="1757">ure Recognition The gesture recognition agent recognizes gestures from strokes drawn on the map. Along with coordinate values, each stroke from the user interface provides contextual information about objects touched or encircled by the stroke. Recognition results are an n-best list (top n-ranked) of interpretations. The interpretations are encoded as typed feature structures [5], which represent each of the potential semantic contributions of the gesture. This list is then passed to the multimodal integrator. 2.2.3 Speech Recognition The Whisper speech recognition engine from Microsoft Corp. [24] drives the speech recognition agent. It offers speaker-independent, continuous recognition in close to real time. QuickSet relies upon a context-free domain grammar, specifically designed for each application, to constrain the speech recognizer. The speech recognizer 825 agent&apos;s output is also an n-best list of hypotheses and their probability estimates. These results are passed on for natural language interpretation. 2.2.4 Natural Language Interpretation The natural language interpretation agent parses the output of the speech recognizer attempting to provide meaningful semantic interpretati</context>
</contexts>
<marker>[24]</marker>
<rawString>X Huang, A. Acero, F. Alleva, M.-Y. Hwang, L Jiang, and M. Mahajan, &amp;quot;Microsoft Windows Highly Intelligent Speech Recognizer Whisper,&amp;quot; in Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP95, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heisterlcamp</author>
</authors>
<title>Ambiguity and uncertainty in spoken dialogue,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of EUROSPEECH93 Conference,</booktitle>
<pages>1657--1660</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="16213" citStr="[25, 26, 27]" startWordPosition="2398" endWordPosition="2398">nse, QuickSet has combined the gesture with the speech and graphically presented the 826 logical consequence of the command: a checkpoint icon (which looks like an upside-down pencil). .-1,4maSti 1 7 1 Bela 11 - Flange 400 ei 23 Palm, TA p.„, Figure 5. QuickSet in Late Confirmation Mode To confirm or disconfirm an object in either mode, the user can push either the SEND (checicmark) or the ERASE (eraser) buttons, respectively. Alternatively, to confirm the command in late confirmation mode, the user can rely on implicit confirmation, wherein QuickSet treats non-contradiction as a confirmation [25, 26, 27]. In other words, if the user proceeds to the next command, she implicitly confirms the previous command. 4 EXPERIMENTAL METHOD This section describes this experiment, its design, and how data were collected and evaluated. 4.1 Subjects, Tasks, and Procedure Eight subjects, 2 male and 6 female adults, half with a computer science background and half without, were recruited from the OGI campus and asked to spend one hour using a prototypical system for disaster rescue planning. During training, subjects received a set of written instructions that described how users could interact with the syste</context>
<context position="20344" citStr="[25]" startWordPosition="3060" endWordPosition="3060">hat when comparing late with early confirmation: 1) subjects complete commands in fewer turns (the error rate and tpc are reduced, resulting in a 30% error reduction); 2) they complete turns at a faster rate (tpm is increased by 21%); and 3) they complete more commands in less time (cpm is increased by 26%). These results confirm all of our predictions. 6 DISCUSSION There are two likely reasons why late confirmation outperforms early confirmation: implicit confirmation and multimodal disambiguation. HeisterIcamp theorized that implicit confirmation could reduce the number of turns in dialogue [25]. Rudnicicy proved in a speechonly digit-entry system that implicit confirmation improved throughput when compared to explicit confirmation [27], and our results confirm their findings. Lavie and colleagues have shown the usefulness of latestage disambiguation, during which speechunderstanding systems pass multiple interpretations through the system, using context in the final stages of processing to disambiguate the recognition hypotheses [28]. However, we have demonstrated and empirically shown the advantage in combining these two strategies in a multimodal system. It can be argued that impl</context>
</contexts>
<marker>[25]</marker>
<rawString>P. Heisterlcamp, &amp;quot;Ambiguity and uncertainty in spoken dialogue,&amp;quot; in Proceedings of EUROSPEECH93 Conference, pp. 1657-1660, Berlin, Germany, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Takebayashi</author>
</authors>
<title>Chapter 14: Integration of understanding and synthesis functions for multimedia interfaces,&amp;quot; in Multimedia interface</title>
<date>1992</date>
<pages>233--256</pages>
<publisher>ACM Press,</publisher>
<location>Eds. New York, NY:</location>
<contexts>
<context position="16213" citStr="[25, 26, 27]" startWordPosition="2398" endWordPosition="2398">nse, QuickSet has combined the gesture with the speech and graphically presented the 826 logical consequence of the command: a checkpoint icon (which looks like an upside-down pencil). .-1,4maSti 1 7 1 Bela 11 - Flange 400 ei 23 Palm, TA p.„, Figure 5. QuickSet in Late Confirmation Mode To confirm or disconfirm an object in either mode, the user can push either the SEND (checicmark) or the ERASE (eraser) buttons, respectively. Alternatively, to confirm the command in late confirmation mode, the user can rely on implicit confirmation, wherein QuickSet treats non-contradiction as a confirmation [25, 26, 27]. In other words, if the user proceeds to the next command, she implicitly confirms the previous command. 4 EXPERIMENTAL METHOD This section describes this experiment, its design, and how data were collected and evaluated. 4.1 Subjects, Tasks, and Procedure Eight subjects, 2 male and 6 female adults, half with a computer science background and half without, were recruited from the OGI campus and asked to spend one hour using a prototypical system for disaster rescue planning. During training, subjects received a set of written instructions that described how users could interact with the syste</context>
</contexts>
<marker>[26]</marker>
<rawString>Y. Takebayashi, &amp;quot;Chapter 14: Integration of understanding and synthesis functions for multimedia interfaces,&amp;quot; in Multimedia interface design, M. M. Blattner and R. B. Dannenberg, Eds. New York, NY: ACM Press, pp. 233-256, 1992.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A L Rudnicky</author>
<author>A Cl Hauptxnann</author>
</authors>
<title>Chapter 10: Multimodal interaction in speech systems,&amp;quot; in Multimedia Interface</title>
<pages>147--171</pages>
<publisher>ACM Press,</publisher>
<location>Eds. New York, NY:</location>
<contexts>
<context position="16213" citStr="[25, 26, 27]" startWordPosition="2398" endWordPosition="2398">nse, QuickSet has combined the gesture with the speech and graphically presented the 826 logical consequence of the command: a checkpoint icon (which looks like an upside-down pencil). .-1,4maSti 1 7 1 Bela 11 - Flange 400 ei 23 Palm, TA p.„, Figure 5. QuickSet in Late Confirmation Mode To confirm or disconfirm an object in either mode, the user can push either the SEND (checicmark) or the ERASE (eraser) buttons, respectively. Alternatively, to confirm the command in late confirmation mode, the user can rely on implicit confirmation, wherein QuickSet treats non-contradiction as a confirmation [25, 26, 27]. In other words, if the user proceeds to the next command, she implicitly confirms the previous command. 4 EXPERIMENTAL METHOD This section describes this experiment, its design, and how data were collected and evaluated. 4.1 Subjects, Tasks, and Procedure Eight subjects, 2 male and 6 female adults, half with a computer science background and half without, were recruited from the OGI campus and asked to spend one hour using a prototypical system for disaster rescue planning. During training, subjects received a set of written instructions that described how users could interact with the syste</context>
<context position="20488" citStr="[27]" startWordPosition="3079" endWordPosition="3079">a 30% error reduction); 2) they complete turns at a faster rate (tpm is increased by 21%); and 3) they complete more commands in less time (cpm is increased by 26%). These results confirm all of our predictions. 6 DISCUSSION There are two likely reasons why late confirmation outperforms early confirmation: implicit confirmation and multimodal disambiguation. HeisterIcamp theorized that implicit confirmation could reduce the number of turns in dialogue [25]. Rudnicicy proved in a speechonly digit-entry system that implicit confirmation improved throughput when compared to explicit confirmation [27], and our results confirm their findings. Lavie and colleagues have shown the usefulness of latestage disambiguation, during which speechunderstanding systems pass multiple interpretations through the system, using context in the final stages of processing to disambiguate the recognition hypotheses [28]. However, we have demonstrated and empirically shown the advantage in combining these two strategies in a multimodal system. It can be argued that implicit confirmation is equivalent to being able to undo the last command, as some multimodal systems allow [3]. However, commands that are infeasi</context>
</contexts>
<marker>[27]</marker>
<rawString>A. L Rudnicky and A. Cl Hauptxnann, &amp;quot;Chapter 10: Multimodal interaction in speech systems,&amp;quot; in Multimedia Interface Design, M. M. Blattner and R. B. Darmenberg, Eds. New York, NY: ACM Press, pp. 147-171,1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>L Levin</author>
<author>Y Qu</author>
<author>A Waibel</author>
<author>D Gates</author>
</authors>
<title>Dialogue processing in a conversational speech translation system,&amp;quot; in</title>
<date>1996</date>
<booktitle>Proceedings of International Conference on Spoken Language Processing, ICSLP 96,</booktitle>
<pages>554--557</pages>
<contexts>
<context position="20792" citStr="[28]" startWordPosition="3121" endWordPosition="3121">on: implicit confirmation and multimodal disambiguation. HeisterIcamp theorized that implicit confirmation could reduce the number of turns in dialogue [25]. Rudnicicy proved in a speechonly digit-entry system that implicit confirmation improved throughput when compared to explicit confirmation [27], and our results confirm their findings. Lavie and colleagues have shown the usefulness of latestage disambiguation, during which speechunderstanding systems pass multiple interpretations through the system, using context in the final stages of processing to disambiguate the recognition hypotheses [28]. However, we have demonstrated and empirically shown the advantage in combining these two strategies in a multimodal system. It can be argued that implicit confirmation is equivalent to being able to undo the last command, as some multimodal systems allow [3]. However, commands that are infeasible, profound, risky, costly, or irreversible are difficult to undo. For this reason, we argue that implicit confirmation is often superior to the option of undoing the previous command. Implicit confirmation, when combined with late confirmation, contributes to a smoother, faster, and more accurate col</context>
</contexts>
<marker>[28]</marker>
<rawString>A. Lavie, L Levin, Y. Qu, A. Waibel, and D. Gates, &apos;Dialogue processing in a conversational speech translation system,&amp;quot; in Proceedings of International Conference on Spoken Language Processing, ICSLP 96, pp. 554-557, 1996.</rawString>
</citation>
<citation valid="false">
<authors>
<author>It W Smith</author>
</authors>
<title>An evaluation of strategies for selective utterance verification for spoken natural language dialog,&amp;quot;</title>
<booktitle>in Proceedings of Fifth Conference on Applied Natural Language Processing, ANLP96,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="22399" citStr="[29, 30]" startWordPosition="3360" endWordPosition="3361">ng process, at a point after blending has enhanced its understanding. This research has compared two strategies: one in which confirmation is performed immediately after speech recognition, and one in which it is delayed until after multimodal integration. The comparison shows that late confirmation reduces the time to perform map manipulation tasks with a multimodal interface. Users can interact faster and complete commands in fewer turns, leading to a reduction in collaborative effort. A direction for future research is to adopt a strategy for determining whether a confirmation is necessary [29, 30], rather than confirming every utterance, and measuring this strategy&apos;s effectiveness. ACKNOWLEDGEMENTS This work is supported in part by the Information Technology and Information Systems offices of DARPA under contract number DABT63-95-C-007, and in part by ONR grant number N00014-95-1-1164. It has been done in collaboration with the US Navy&apos;s NCCOSC RDT&amp;E Division (NRaD). Thanks to the faculty, staff, and students who contributed to this research, including Joshua Clow, Peter Heeman, Michael Johnston, Ira Smith, Stephen Sutton, and Karen Ward. Special thanks to Donald Hanley for his insight</context>
</contexts>
<marker>[29]</marker>
<rawString>It W. Smith, &amp;quot;An evaluation of strategies for selective utterance verification for spoken natural language dialog,&amp;quot; in Proceedings of Fifth Conference on Applied Natural Language Processing, ANLP96, pp. 41-48,1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Niimi</author>
<author>Y Kobayashi</author>
</authors>
<title>A dialog control strategy based on the reliability of speech recognition,&amp;quot;</title>
<date>1996</date>
<booktitle>in Proceedings of International Conference on Spoken Language Processing, ICSLP 96,</booktitle>
<pages>534--537</pages>
<contexts>
<context position="22399" citStr="[29, 30]" startWordPosition="3360" endWordPosition="3361">ng process, at a point after blending has enhanced its understanding. This research has compared two strategies: one in which confirmation is performed immediately after speech recognition, and one in which it is delayed until after multimodal integration. The comparison shows that late confirmation reduces the time to perform map manipulation tasks with a multimodal interface. Users can interact faster and complete commands in fewer turns, leading to a reduction in collaborative effort. A direction for future research is to adopt a strategy for determining whether a confirmation is necessary [29, 30], rather than confirming every utterance, and measuring this strategy&apos;s effectiveness. ACKNOWLEDGEMENTS This work is supported in part by the Information Technology and Information Systems offices of DARPA under contract number DABT63-95-C-007, and in part by ONR grant number N00014-95-1-1164. It has been done in collaboration with the US Navy&apos;s NCCOSC RDT&amp;E Division (NRaD). Thanks to the faculty, staff, and students who contributed to this research, including Joshua Clow, Peter Heeman, Michael Johnston, Ira Smith, Stephen Sutton, and Karen Ward. Special thanks to Donald Hanley for his insight</context>
</contexts>
<marker>[30]</marker>
<rawString>Y. Niimi and Y. Kobayashi, &amp;quot;A dialog control strategy based on the reliability of speech recognition,&amp;quot; in Proceedings of International Conference on Spoken Language Processing, ICSLP 96, pp. 534-537, 1996.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>