<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000386">
<note confidence="0.740009666666667">
COMPUTATIONAL COMPLEXITY IN TWO-LEVEL
MORPHOLOGY
G. Edward Barton, Jr.
M.I.T. Artificial Intelligence Laboratory
545 Technology Square
Cambridge, MA 02139
</note>
<sectionHeader confidence="0.741364" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999860428571429">
Morphological analysis must take into account the
spelling-change processes of a language as well as its possi-
ble configurations of stems, affixes, and inflectional mark-
ings. The computational difficulty of the task can be clari-
fied by investigating specific models of morphological pro-
cessing. The use of finite-state machinery in the &amp;quot;two-
level&amp;quot; model by Kimmo Koskenniemi gives it the appear-
ance of computational efficiency, but closer examination
shows the model does not guarantee efficient processing.
Reductions of the satisfiability problem show that finding
the proper lexical/surface correspondence in a two-level
generation or recognition problem can be computationally
difficult. The difficulty increases if unrestricted deletions
(null characters) are allowed.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999969016666667">
The &amp;quot;dictionary lookup&amp;quot; stage in a natural-language
system can involve much more than simple retrieval. In-
flectional endings, prefixes, suffixes, spelling-change pro-
cesses, reduplication, non-concatenative morphology, and
clitics may cause familiar words to show up in heavily dis-
guised form, requiring substantial morphological analysis.
Superficially, it seems that word recognition might poten-
tially be complicated and difficult.
This paper examines the question more formally by in-
vestigating the computational characteristics of the &amp;quot;two-
level&amp;quot; model of morphological processes. Given the kinds
of constraints that can be encoded in two-level systems,
how difficult could it be to translate between lexical and
surface forms? Although the use of finite-state machin-
ery in the two-level model gives it the appearance of com-
putational efficiency, the model itself does not guarantee
efficient processing. Taking the Kimmo system (K art-
tunen, 1983) for concreteness, it will be shown that the
general problem of mapping between lexical and surface
forms in two-level systems is computationally difficult in
the worst case; extensive backtracking is possible. If null
characters are excluded, the generation and recognition
problems are NP-complete in the worst case. If null charac-
ters are completely unrestricted, the problems is PSPACE-
complete, thus probably even harder. The fundamental
difficulty of the problems does not seem to be a precompi-
lation effect.
In addition to knowing the stems, affixes, and co-
occurrence restrictions of a language, a successful morpho-
logical analyzer must take into account the spelling-change
processes that often accompany affixation. In English,
the program must expect love+ing to appear as loving,
fly+s as flies, lie+ing as lying, and big+er as bigger.
Its knowledge must be sufficiently sophisticated to distin-
guish such surface forms as hopped and hoped. Cross-
linguistically, spelling-change processes may span either a
limited or a more extended range of characters, and the
material that triggers a change may occur either before or
after the character that is affected. (Reduplication, a com-
plex copying process that may also be found, will not be
considered here.)
The Kimmo system described by Karttunen (1983) is
attractive for putting morphological knowledge to use in
processing. Kimmo is an implementation of the &amp;quot;two-level&amp;quot;
model of morphology that Kimmo Koskenniemi proposed
and developed in his Ph.D. thesis.1 A system of lexicons in
the dictionary component regulates the sequence of roots
and affixes at the lexical level, while several finite-state
transducers in the automaton component — 20 transduc-
ers for Finnish, for instance — mediate the correspondence
between lexical and surface forms. Null characters allow
the automata to handle insertion and deletion processes.
The overall system can be used either for generation or for
recognition.
The finite-state transducers of the automaton compo-
nent serve to implement spelling changes, which may be
triggered by either left or right context and which may
ignore irrelevant intervening characters. As an example,
the following automaton describes a simplified &amp;quot;Y-change&amp;quot;
process that changes y to ± before suffix es:
</bodyText>
<affiliation confidence="0.942325">
&apos;University of Helsinki, Finland, circa Fall 1983.
</affiliation>
<page confidence="0.991302">
53
</page>
<equation confidence="0.98524625">
&amp;quot;Y-Change&amp;quot; 5 5
y + &amp; = (lexical characters)
y = s = (surface characters)
4 1 1 1 (normal state)
0 3 0 0 (require +s)
0 0 1 0 (require s)
4 5 1 1 (forbid +s)
4 1 0 1 (forbids)
</equation>
<bodyText confidence="0.9999295">
The details of this notation will not be explained here;
basic familiarity with the Kimmo system is assumed. For
further introduction, see Barton (1985), Karttunen (1983),
and references cited therein.
</bodyText>
<sectionHeader confidence="0.918643" genericHeader="method">
THE SEEDS
OF COMPLEXITY
</sectionHeader>
<bodyText confidence="0.999986976470589">
At first glance, the finite-state machines of the two-
level model appear to promise unfailing computational ef-
ficiency. Both recognition and generation are built on the
simple process of stepping the machines through the input.
Lexical lookup is also fast, interleaved character by charac-
ter with the quick left-to-right steps of the automata. The
fundamental efficiency of finite-state machines promises to
make the speed of Kimmo processing largely independent
of the nature of the constraints that the automata encode:
The most important technical feature of Kosken-
niemi&apos;s and our implementation of the Two-level
model is that morphological rules are represented
in the processor as automata, more specifically, as
finite state transducers .... One important conse-
quence of compiling [the grammar rules into au-
tomata] is that the complexity of the linguistic de-
scription of a language has no significant effect on
the speed at which the forms of that language can
be recognized or generated. This is due to the fact
that finite state machines are very fast to operate
because of their simplicity .... Although Finnish,
for example, is morphologically a much more com-
plicated language than English, there is no differ-
ence of the same magnitude in the processing times
for the two languages .... [This fact] has some psy-
cholinguistic interest because of the common sense
observation that we talk about &amp;quot;simple&amp;quot; and &amp;quot;com-
plex&amp;quot; languages but not about &amp;quot;fast&amp;quot; and &amp;quot;slow&amp;quot;
ones. (Karttunen, 1983:166f)
For this kind of interest in the model to be sustained, it
must be the model itself that wipes out processing diffi-
culty, rather than some accidental property of the encoded
morphological constraints.
Examined in detail, the runtime complexity of Kimmo
processing can be traced to three main sources. The rec-
ognizer and generator must both run the finite-state ma-
chines of the automaton component; in addition, the recog-
nizer must descend the letter trees that make up a lexicon.
The recognizer must also decide which suffix lexicon to ex-
plore at the end of an entry. Finally, both the recognizer
and the generator must discover the correct lexical-surface
correspondence.
All these aspects of runtime processing are apparent
in traces of implemented Kimmo recognition, for instance
when the recognizer analyzes the English surface form
spiel (in 61 steps) according to Karttunen and Witten-
burg&apos;s (1983) analysis (Figure 1). The stepping of trans-
ducers and letter-trees is ubiquitous. The search for the
lexical-surface correspondence is also clearly displayed; for
example, before backtracking to discover the correct lexi-
cal entry spiel, the recognizer considers the lexical string
spy+ with y surfacing as i and + as e. Finally, after finding
the putative root spy the recognizer must decide whether
to search the lexicon I that contains the zero verbal ending
of the present indicative, the lexicon AG storing the agen-
tive suffix +er, or one of several other lexicons inhabited
by inflectional endings such as +ed.
The finite-state framework makes it easy to step the
automata; the letter-trees are likewise computationally
well-behaved. It is more troublesome to navigate through
the lexicons of the dictionary component, and the cur-
rent implementation spends considerable time wandering
about. However, changing the implementation of the dic-
tionary component can sharply reduce this source of com-
plexity; a merged dictionary with bit-vectors reduces the
number of choices among alternative lexicons by allowing
several to be searched at once (Barton, 1985).
More ominous with respect to worst-case behavior is
the backtracking that results from local ambiguity in the
construction of the lexical-surface correspondence. Even
if only one possibility is globally compatible with the con-
straints imposed by the lexicon and the automata, there
may not be enough evidence at every point in processing
to choose the correct lexical-surface pair. Search behavior
results.
In English examples, misguided search subtrees are
necessarily shallow because the relevant spelling-change
processes are local in character. Since long-distance har-
mony processes are also possible, there can potentially be
a long interval before the acceptability of a lexical-surface
pair is ultimately determined. For instance, when vowel
alternations within a verb stem are conditioned by the oc-
currence of particular tense suffixes, the recognizer must
sometimes see the end of the word before making final de-
cisions about the stem.
</bodyText>
<figure confidence="0.501175">
y
</figure>
<figureCaption confidence="0.9979922">
state 1: 2
state 2. 0
state 3. 0
state 4: 2
state 6: 2
</figureCaption>
<page confidence="0.818243">
54
</page>
<figure confidence="0.82844935483871">
Recognizing surface form &amp;quot;spiel&amp;quot;.
1 s 1,4,1,2.1,1
2 sp 1.1.1.2.1.1
3 spy 1,3.4,3,1,1
4 &amp;quot;spy&amp;quot; ends, new lexicon N
6 &amp;quot;0&amp;quot; ends, new lexicon Cl
6 spy XXX extra input
7(5) spy+ 1,5,16,4,1,1
8 spy+ XXX
9(5) sPY+ 1.6.1,4,1,1
10 spy+ XXX
11 (4) &amp;quot;spy&amp;quot; ends, new lexicon I
12 spy XXX extra input
13 (4) &amp;quot;spy&amp;quot; ends, new lexicon P3
14 spy+ 1,6.1.4.1.1
16 sPY+ XXX
16 (14) spy+ 1,5,16,4,1,1
17 spy+ XXX
18 (4) &amp;quot;spy&amp;quot; ends, new lexicon PS
19 spy+ 1.6.1,4,1,1
20 spy+e 1,1,1,1,4,1
21 spy+e XXX
22(20) spy+e 1,1,4,1,3,1
23 spy+e XXX
24 (19) spy+ 1.5.16.4,1.1
25 spy+e XXX Epenthesis
26 (4) &amp;quot;spy&amp;quot; ends, new lexicon PP
27 spy+ 1.6.1.4,1.1
28 spy+e 1,1,1,1,4,1
29 spy+e XXX
30 (28) spy+e 1,1,4,1,3,1
31 spy+e XXX
32 (27) spy+ 1.6.16.4,1.1
33 spy+. XXX Epenthesis
34 (4) &amp;quot;spy&amp;quot; ends, new lexicon PR
36 spy+ 1.6,1,4,1,1
38 sPY+ XXX
37 (36) spy+ 1,5,16,4,1,1
38 spy+ XXX
39 (4) &amp;quot;spy&amp;quot; ends, new lexicon AG
40 spy+ 1.6,1.4.1,1
41 spy+e 1,1.1,1.4,1
42 spy+e XXX
43(41) spy+e 1.1.4.1.3.1
44 spy+e XXX
45 (40) spy&amp;quot; 1,5,16,4,1,1
46 spy+e XXX Epenthesis
47 (4) &amp;quot;spy&amp;quot; ends, new lexicon All
48 sPY+ 1,6,1,4,1,1
49 spy+ XXX
50 (48) spy+ 1,6,16.4.1.1
51 spy+ XXX
52(3) spi 1,1,4,1,2.5
53 spie 1,1,16.1,6.1
54 spie XXX
55(53) sole 1,1.16.1.5,6
56 spiel 1.1,16,2,1,1
57 &amp;quot;spiel&amp;quot; ends, new lexicon N
58 &amp;quot;0&amp;quot; ends, new lexicon Cl
59 &amp;quot;spiel&amp;quot; .&amp;quot;&amp;quot; result
60(58) spiel+ 1.1,16,1,1,1
61 spiel+ XXX
-+---+---+LLL+LLL+III+
---+XXX+
LLL+III+
LLL+---+XXX+
I-I-+XXX+
LLL+---+---+XXX+
1 —I-+XXX+
---+AAA+
LLL+---+---+XXX+
---+AAA+
LLL+---+XXX+
1 -I-+XXX+
LLL+---+---+XXX+
I-I-+XXX+
---+AAA+
LLL+---+XXX+
---+XXX+
---+---+XXX+
1
---+---+LLL+LLL
---+XXX+
Key to tree nodes:
normal traversal
LLL new lexicon
AAA blocking by automata
XXX no lexical-surface pairs
compatible with surface
char and dictionary
III blocking by leftover input
analysis found
((&amp;quot;spiel&amp;quot; (N SG)))
</figure>
<figureCaption confidence="0.977601">
Figure 1: These traces show the steps that the KiMMOrecognizer for English goes through while
</figureCaption>
<bodyText confidence="0.887918416666667">
analyzing the surface form spiel. Each line of the table on the left shows the lexical string and
automaton states at the end of a step. If sonic automaton blocked, the automaton states are replaced
by an XXX entry. An XXX entry with no automaton mune indicates that the lexical string could not
be extended because the surface character and lexical letter tree together ruled out all feasible pairs.
After an XXX or *** entry, the recognizer backtracks and picks up from a previous choice point.
indicated by the parenthesized step number before the lexical string. The tree on the right depicts
the search graphically. reading from left. to right and top to bottom with vertical bars linking the
choices at each choice point. The figures were generated with a KIM nit) implementation written in an
augmented version of MACLIST based initially on Karttuneds (1983:182W) algorithm description; the
dictionary and automaton components for English were taken front Karttunen and Wittenburg (1983)
with minor changes. This implementation searches depth-first as Karttunen&apos;s does, but explores the
alternatives at a given depth in a different order from Karttuneds.
</bodyText>
<page confidence="0.993745">
55
</page>
<bodyText confidence="0.999622666666667">
Ignoring the problem of choosing among alternative
lexicons, it is easy to see that the use of finite-state ma-
chinery helps control only one of the two remaining sources
of complexity. Stepping the automata should be fast, but
the finite-state framework does not guarantee speed in the
task of guessing the correct lexical-surface correspondence.
The search required to find the correspondence may pre-
dominate. In fact, the Kimmo recognition and generation
problems bear an uncomfortable resemblance to problems
in the computational class NP. Informally, problems in NP
have solutions that may be hard to guess but are easy to
verify — just the situation that might hold in the discov-
ery of a Kimmo lexical-surface correspondence, since the
automata can verify an acceptable correspondence quickly
but may need search to discover one.
</bodyText>
<sectionHeader confidence="0.99426" genericHeader="method">
THE COMPLEXITY
OF
TWO-LEVEL MORPHOLOGY
</sectionHeader>
<bodyText confidence="0.997682953488372">
The Kimmo algorithms contain the seeds of complex-
ity, for local evidence does not always show how to con-
struct a lexical-surface correspondence that will satisfy
the constraints expressed in a set of two-level automata.
These seeds can be exploited in mathematical reductions
to show that two-level automata can describe computa-
tionally difficult problems in a very natural way. It fol-
lows that the finite-state two-level framework itself cannot
guarantee computational efficiency. If the words of natural
languages are easy to analyze, the efficiency of processing
must result from some additional property that natural
languages have, beyond those that are captured in the two-
level model. Otherwise, computationally difficult problems
might turn up in the two-level automata for some natural
language, just as they do in the artificially constructed lan-
guages here. In fact, the reductions are abstractly modeled
on the Kimmo treatment of harmony processes and other
long-distance dependencies in natural languages.
The reductions use the computationally difficult
Boolean satisfiability problems SAT and 3SAT, which in-
volve deciding whether a CNF formula has a satisfying
truth-assignment. It is easy to encode an arbitrary SAT
problem as a Kimmo generation problem, hence the gen-
eral problem of mapping from lexical to surface forms in
Kimmo systems is NP-complete.2 Given a CNF formula cp,
first construct a string a by notational translation: use a
minus sign for negation, a comma for conjunction, and no
explicit operator for disjunction. Then the a corresponding
to the formula (Y V y)Sz(g v z)Sz(r v y v z) is -xy , -yz ,xyz.
&apos;Membership in NP is also required for this conclusion. A later
section (&amp;quot;The Effect of Nulls&amp;quot;) shows membership in NP by sketching
how a nondeterministic machine could quickly solve Kimmo generation
and recognition problems.
The notation is unambiguous without parentheses because
co is required to be in CNF. Second, construct a Kimmo
automaton component A in three parts. (A varies from
formula to formula only when the formulas involve differ-
ent sets of variables.) The alphabet specification should
list the variables in a together with the special characters
T, F, minus sign, and comma; the equals sign should be
declared as the Kimmo wildcard character, as usual. The
consistency automata, one for each variable in a, should
be constructed on the following model:
</bodyText>
<equation confidence="0.95325475">
&amp;quot;x-consistency&amp;quot; 3 3
x x = (lexical characters)
T F = (surface characters)
1: 2 3 1 (x undecided)
</equation>
<bodyText confidence="0.907734875">
2: 2 0 2 (x true)
3: 0 3 3 (x false)
The consistency automaton for variable x constrains the
mapping from variables in the lexical string to truth-values
in the surface string, ensuring that whatever value is as-
signed to x in one occurrence must be assigned to x in
every occurrence. Finally, use the following satisfaction
automaton, which does not vary from formula to formula:
</bodyText>
<equation confidence="0.756890333333333">
&amp;quot;satisfaction&amp;quot; 3 4
= _ (lexical characters)
T F - , (surface characters)
1. 2 1 3 0 (no true seen in this group)
2: 2 2 2 1 (true seen in this group)
3. 1 2 0 0 (-F counts as true)
</equation>
<bodyText confidence="0.998499458333333">
The satisfaction automaton determines whether the truth-
values assigned to the variables cause the formula to come
out true. Since the formula is in CNF, the requirement is
that the groups between commas must all contain at least
one true value.
The net result of the constraints imposed by the consis-
tency and satisfaction automata is that some surface string
can be generated from a just in case the original formula co
has a satisfying truth-assignment. Furthermore, A and a
can be constructed in time polynomial in the length of co;
thus SAT is polynomial-time reduced to the Kimmo gener-
ation problem, and the general case of Kimmo generation
is at least as hard as SAT. Incidentally, note that it is local
rather than global ambiguity that causes trouble; the gen-
erator system in the reduction can go through quite a bit of
search even when there is just one final answer. Figure 2
traces the operation of the Kimmo generation algorithm
on a (uniquely) satisfiable formula.
Like the generator, the Kimmo recognizer can also be
used to solve computationally difficult problems. One easy
reduction treats 3SAT rather than SAT, uses negated al-
phabet symbols instead of a negation sign, and replaces
the satisfaction automaton with constraints from the dic-
tionary component; see Barton (1985) for details.
</bodyText>
<page confidence="0.996748">
56
</page>
<table confidence="0.988601282051282">
Generating from lexical form &amp;quot;-xy,-yz,-y-z,xyz&amp;quot;.
1 - 1,1,1,3 38 + -FF,-FT,-F-T,FFT 3,3,2,2
2 -F 3,1,1,2 39 &amp;quot;-FF,-FT,-F-T,FFT&amp;quot; *** result
3 -FF 3,3,1,2 40 (3) -FT 3,2,1,2
4 -FF, 3,3,1,1 41 -FT, 3,2,1,1
5 -FF,- 3,3,1,3 42 -FT,- 3,2,1,3
8 -FF,-T XXX y-con. 43 -FT. -F XXX y-con.
7 + -FF,-F 3,3,1,2 44 + -FT,-T 3,2,1,1
8 -FF,-FF 3,3,3,2 45 -FT,-TF 3,2,3,1
9 -FF,-FF, 3,3,3,1 46 -FT,-TF, XXX satis.
10 -FF,-FF,- 3,3,3,3 47 (45) -FT,-TT 3,2,2,2
11 -FF,-FF,-T XXX y-con. 48 -FT,-TT. 3,2,2,1
12 + -FF,-FF,-F 3,3,3,2 49 -FT,-TT,- 3,2.2,3
13 -FF,-FF,-F- 3,3,3,2 60 -FT,-TT,-F XXX y-con.
14 -FF,-FF,-F-T XXX z-con. 51 + -FT,-TT,-T 3,2,2,1
15 + -FF,-FF,-F-F 3,3,3,2 52 -FT,-TT,-T- 3,2,2,3
16 -FF,-FF,-F-F, 3,3,3,1 63 -FT,-TT,-T-F XXX z-con.
17 -FF,-FF,-F-F,T XXX x-con. 64 4- -FT,-TT,-T-T 3,2,2,1
18 + -FF,-FF,-F-F,F 3,3,3,1 55 -FT,-TT.-T-T, XXX satis.
19 -FF,-FF,-F-F,FT XXX y-con. 56 (2) -T 2,1,1,1
20 + -FF,-FF,-F-F,FF 3.3.3.1 67 -TF 2.3,1,1
21 -FF,-FF,-F-F,FFT XXX z-con. 68 -TF, XXX satis.
22 + -FF,-FF,-F-F,FFF 3,3,3,1 59 (67) -TT 2,2,1,2
23 -FF,-FF,-F-F,FFF XXX satis. nf. 60 -TT, 2,2,1,1
24 (8) -FF,-FT 3,3,2,2 81 -TT,- 2,2,1,3
25 -FF,-FT, 3,3,2,1 62 -TT,-F XXX y-con.
26 -FF,-FT,- 3,3,2,3 63 + -TT,-T 2,2,1,1
27 -FF,-FT,-T XXX y—con. 64 -TT,-TF 2,2,3,1
28 + -FF,-FT, -F 3,3,2,2 65 -TT,-TF, XXX satis.
29 -FF,-FT,-F- 3,3,2,2 66 (84) -TT,-TT 2,2.2,2
30 -FF,-FT,-F-F XXX z-con. 67 -TT,-TT, 2,2.2,1
31 + -FF,-FT,-F-T 3,3,2,2 68 -TT,-TT,- 2,2,2,3
32 -FF,-FT,-F-T, 3,3,2,1 69 -TT,-TT,-F XXX y-con.
33 -FF,-FT,-F-T,T XXX x-con. 70 + -TT,-TT.-T 2,2,2,1
34 + -FF,-FT,-F-T,F 3,3,2,1 71 -TT,-TT.-T- 2,2,2,3
35 -FF,-FT,-F-T,FT XXX y—con. 72 -TT,-TT,-T-F XXX z-con.
36 + -FF,-FT,-F-T,FF 3,3,2,1 73 + -TT,-TT,-T-T 2,2,2,1
37 -FF,-FT,-F-T,FFF XXX z-con. 74 -TT,-TT, -T-T, XXX satis.
(&amp;quot;-FF,-FT.-F-T,FFT&amp;quot;)
</table>
<figureCaption confidence="0.99852">
Figure 2: The generator system for deciding the satisfiability of Boolean formulas in x, y,
</figureCaption>
<bodyText confidence="0.999814166666667">
and z goes through these steps when applied to the encoded version of the (satisfiable) formula
Ci V y)(44 v z)8( v 7)8z(x v y v z). Though only one truth-assignment will satisfy the formula,
it takes quite a bit of backtracking to find it. The notation used here for describing generator actions is
similar to that used to describe recognizer actions in Figure ??, but a surface rather than a lexical string
is the goal. A +-entry in the backtracking column indicates backtracking from an immediate failure in the
preceding step, which does not require the full backtracking mechanism to be invoked.
</bodyText>
<sectionHeader confidence="0.8508125" genericHeader="method">
THE EFFECT
OF PRECOMPILATION
</sectionHeader>
<bodyText confidence="0.999974435897436">
Since the above reductions require both the lan-
guage description and the input string to vary with the
SAT/3SAT problem to be solved, there arises the question
of whether some computationally intensive form of pre-
compilation could blunt the force of the reduction, paying
a large compilation cost once and allowing Kimmo run-
time for a fixed grammar to be uniformly fast thereafter.
This section considers four aspects of the precompilation
question.
First, the external description of a Kimmo automator
or lexicon is not the same as the form used at runtime. In-
stead, the external descriptions are converted to internal
forms: RMACHINE and GMACHINE forms for automata,
letter trees for lexicons (Gajek et al., 1983). Hence the
complexity implied by the reduction might actually apply
to the construction of these internal forms; the complexity
of the generation problem (for instance) might be concen-
trated in the construction of the &amp;quot;feasible-pair list&amp;quot; and
the GMACHINE. This possibility can be disposed of by
reformulating the reduction so that the formal problems
and the construction specify machines in terms of their in-
ternal forms rather than their external descriptions. The
GMACHINEs for the class of machines created in the con-
struction have a regular structure, and it is easy to build
them directly instead of building descriptions in external.
format. As traces of recognizer operation suggest, it is
runtime processing that makes translated SAT problems
difficult for a Kimmo system to solve.
Second, there is another kind of preprocessing that
might be expected to help. It is possible to compile a
set of Kimmo automata into a single large automaton (a
BIGMACHINE) that will run faster than the original set.
The system Will usually run faster with one large automa-
ton than with several small ones, since it has only one
machine to step and the speed of stepping a machine is
largely independent of its size. Since it can take exponen-
tial time to build the BIGMACHINE for a translated SAT
problem, the reduction formally allows the possibility that
BIGMACHINE precompilation could make runtime pro-
</bodyText>
<page confidence="0.994672">
57
</page>
<bodyText confidence="0.999992089552239">
cessing uniformly efficient. However, an expensive BIG-
MACHINE precompilation step does not help runtime pro-
cessing enough to change the fundamental complexity of
the algorithms. Recall that the main ingredients of Kimmo
runtime complexity are the mechanical operation of the
automata, the difficulty of finding the right lexical-surface
correspondence, and the necessity of choosing among alter-
native lexicons. BIGMACHINE precompilation will speed
up the mechanical operation of the automata, but it will
not help in the difficult task of deciding which lexical-
surface pair will be globally acceptable. Precompilation
oils the machinery, but accomplishes no radical changes.
Third, BIGMACHINE precompilation also sheds light
on another precompilation question. Though BIGMA-
CHINE precompilation involves exponential blowup in the
worst case (for example, with the SAT automata), in prac-
tice the size of the BIGMACHINE varies — thus naturally
raising the question of what distinguishes the &amp;quot;explosive&amp;quot;
sets of automata from those with more civilized behav-
ior. It is sometimes suggested that the degree of inter-
action among constraints determines the amount of BIG-
MACHINE blowup. Since the computational difficulty of
SAT problems results in large measure from their &amp;quot;global&amp;quot;
character, the size of the BIGMACHINE for the SAT sys-
tem comes as no surprise under the interaction theory.
However, a slight change in the SAT automata demon-
strates that BIGMACHINE size is not a good measure
of interaction among constraints. Eliminate the satisfac-
tion automaton from the generator system, leaving only
the consistency automata for the variables. Then the sys-
tem will not search for a satisfying truth-assignment, but
merely for one that is internally consistent. This change
entirely eliminates interactions among the automata; yet
the BIGMACHINE must still be exponentially larger than
the collection of individual automata, for its states must
distinguish all the possible truth-assignments to the vari-
ables in order to enforce consistency. In fact, the lack of
interactions can actually increase the size of the BIGMA-
CHINE, since interactions constrain the set of reachable
state-combinations.
Finally, it is worth considering whether the nondeter-
minism involved in constructing the lexical-surface cor-
respondence can be removed by standard determiniza-
tion techniques. Every nondeterministic finite-state ma-
chine has a deterministic counterpart that is equivalent in
the weak sense that it accepts the same language; aren&apos;t
Kimmo automata just ordinary finite-state machines op-
erating over an alphabet that consists of pairs of ordinary
characters? Ignoring subtleties associated with null char-
acters, Kimmo automata can indeed be viewed in this way
when they are used to verify or reject hypothesized pairs of
lexical and surface strings. However, in this use they do not
need determinizing, for each cell of an automaton descrip-
tion already lists just one state. In the cases of primary
interest — generation and recognition — the machines are
used as genuine transducers rather than acceptors.
The determinizing algorithms that apply to finite-state
acceptors will not work on transducers, and in fact many
finite-state transducers are not determinizable at all. Upon
seeing the first occurrence of a variable in a SAT problem,
a deterministic transducer cannot know in general whether
to output T or F. It also cannot wait and output a truth-
value later, since the variable might occur an unbounded
number of times before there was sufficient evidence to
assign the truth-value. A finite-state transducer would not
be able in general to remember how many outputs had
been deferred.
</bodyText>
<sectionHeader confidence="0.953743" genericHeader="method">
THE EFFECT OF NULLS
</sectionHeader>
<bodyText confidence="0.999956914285714">
Since Kimmo systems can encode NP-complete prob-
lems, the general Kimmo generation and recognition prob-
lems are at least as hard as the difficult problems in NP.
But could they be even harder? The answer depends on
whether null characters are allowed. If nulls are completely
forbidden, the problems are in NP, hence (given the pre-
vious result) NP-complete. If nulls are completely unre-
stricted, the problems are PSPACE-complete, thus prob-
ably even harder than the problems in NP. However, the
full power of unrestricted null characters is not needed for
linguistically relevant processing.
If null characters are disallowed, the generation prob-
lem for Kimmo systems can be solved quickly on a nonde-
terministic machine. Given a set of automata and a lex-
ical string, the basic nondeterminism of the machine can
be used to guess the lexical-surface correspondence, which
the automata can then quickly verify. Since nulls are not
permitted, the size of the guess cannot get out of hand;
the lexical and surface strings will have the same length.
The recognition problem can be solved in the same way
except that the machine must also guess a path through
the dictionary.
If null characters are completely unrestricted, the
above argument fails; the lexical and surface strings may
differ so radically in length that the lexical-surface cor-
respondence cannot be proposed or verified in time poly-
nomial in input length. The problem becomes PSPACE-
complete — as hard as checking for a forced win from
certain N x N Go configurations, for instance, and prob-
ably even harder than NP-complete problems (cf. Garey
and Johnson, 1979:171fF). The proof involves showing that
Kimmo systems with unrestricted nulls can easily be in-
duced to work out, in the space between two input char-
acters, a solution to the difficult Finite State Automata
Intersection problem.
</bodyText>
<page confidence="0.992001">
58
</page>
<bodyText confidence="0.999968666666667">
The PSPACE-completeness reduction shows that if
two-level morphology is formally characterized in a way
that leaves null characters completely unrestricted, it can
be very hard for the recognizer to reconstruct the superfi-
cially null characters that may lexically intervene between
two surface characters. However, unrestricted nulls surely
are not needed for linguistically relevant Kimmo systems.
Processing complexity can be reduced by any restriction
that prevents the number of nulls between surface charac-
ters from getting too large. As a crude approximation to
a reasonable constraint, the PSPACE-completeness reduc-
tion could be ruled out by forbidding entire lexicon entries
from being deleted on the surface. A suitable restriction
would make the general Kimmo recognition problems only
NP-complete.
Both of the reductions remind us that problems involv-
ing finite-state machines can be hard. Determining mem-
bership in a finite-state language may be easy, but using
finite-state machines for different tasks such as parsing or
transduction can lead to problems that are computation-
ally more difficult.
</bodyText>
<sectionHeader confidence="0.990855" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999984785714286">
This report describes research done at the Artificial
Intelligence Laboratory of the Massachusetts Institute of
Technology. Support for the Laboratory&apos;s artificial intel-
ligence research has been provided in part by the Ad-
vanced Research Projects Agency of the Department of
Defense under Office of Naval Research contract N00014-
80-C-0505. A version of this paper was presented to
the Workshop on Finite-State Morphology, Center for the
Study of Language and Information, Stanford University,
July 29-30, 1985; the author is grateful to Lauri Kart-
tunen for making that presentation possible. This research
has benefited from guidance and commentary from Bob
Berwick, and Bonnie Dorr and Eric Grimson have also
helped improve the paper.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997163538461538">
Barton, E. (1985). &amp;quot;The Computational Complexity of
Two-Level Morphology,&amp;quot; A.I. Memo No. 856, M.I.T.
Artificial Intelligence Laboratory, Cambridge, Mass.
Gajek, 0., H. Beck, D. Elder, and G. Whittemore (1983).
&amp;quot;LISP Implementation [of the KImmo system],&amp;quot; Texas
Linguistic Forum 22:187-202.
Garey, M., and D. Johnson (1979). Computers and In-
tractability. San Francisco: W. H. Freeman and Co.
Karttunen, L. (1983). &amp;quot;KIMMO: A Two-Level Morpho-
logical Analyzer,&amp;quot; Texas Linguistic Forum 22:165-186.
Karttunen, L., and K. Wittenburg (1983). &amp;quot;A Two-Level
Morphological Analysis of English,&amp;quot; Texas Linguistic
Forum 22:217-228.
</reference>
<page confidence="0.99926">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931767">
<title confidence="0.988635">COMPUTATIONAL COMPLEXITY IN TWO-LEVEL MORPHOLOGY</title>
<author confidence="0.998575">G Edward Barton</author>
<affiliation confidence="0.999805">M.I.T. Artificial Intelligence Laboratory</affiliation>
<address confidence="0.9977995">545 Technology Square Cambridge, MA 02139</address>
<abstract confidence="0.9970608">Morphological analysis must take into account the spelling-change processes of a language as well as its possible configurations of stems, affixes, and inflectional markings. The computational difficulty of the task can be clarified by investigating specific models of morphological processing. The use of finite-state machinery in the &amp;quot;twolevel&amp;quot; model by Kimmo Koskenniemi gives it the appearance of computational efficiency, but closer examination shows the model does not guarantee efficient processing. Reductions of the satisfiability problem show that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult. The difficulty increases if unrestricted deletions (null characters) are allowed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Barton</author>
</authors>
<title>The Computational Complexity of Two-Level Morphology,&amp;quot;</title>
<date>1985</date>
<booktitle>A.I. Memo No. 856, M.I.T. Artificial Intelligence Laboratory,</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4560" citStr="Barton (1985)" startWordPosition="689" endWordPosition="690">, which may be triggered by either left or right context and which may ignore irrelevant intervening characters. As an example, the following automaton describes a simplified &amp;quot;Y-change&amp;quot; process that changes y to ± before suffix es: &apos;University of Helsinki, Finland, circa Fall 1983. 53 &amp;quot;Y-Change&amp;quot; 5 5 y + &amp; = (lexical characters) y = s = (surface characters) 4 1 1 1 (normal state) 0 3 0 0 (require +s) 0 0 1 0 (require s) 4 5 1 1 (forbid +s) 4 1 0 1 (forbids) The details of this notation will not be explained here; basic familiarity with the Kimmo system is assumed. For further introduction, see Barton (1985), Karttunen (1983), and references cited therein. THE SEEDS OF COMPLEXITY At first glance, the finite-state machines of the twolevel model appear to promise unfailing computational efficiency. Both recognition and generation are built on the simple process of stepping the machines through the input. Lexical lookup is also fast, interleaved character by character with the quick left-to-right steps of the automata. The fundamental efficiency of finite-state machines promises to make the speed of Kimmo processing largely independent of the nature of the constraints that the automata encode: The m</context>
<context position="8192" citStr="Barton, 1985" startWordPosition="1260" endWordPosition="1261">several other lexicons inhabited by inflectional endings such as +ed. The finite-state framework makes it easy to step the automata; the letter-trees are likewise computationally well-behaved. It is more troublesome to navigate through the lexicons of the dictionary component, and the current implementation spends considerable time wandering about. However, changing the implementation of the dictionary component can sharply reduce this source of complexity; a merged dictionary with bit-vectors reduces the number of choices among alternative lexicons by allowing several to be searched at once (Barton, 1985). More ominous with respect to worst-case behavior is the backtracking that results from local ambiguity in the construction of the lexical-surface correspondence. Even if only one possibility is globally compatible with the constraints imposed by the lexicon and the automata, there may not be enough evidence at every point in processing to choose the correct lexical-surface pair. Search behavior results. In English examples, misguided search subtrees are necessarily shallow because the relevant spelling-change processes are local in character. Since long-distance harmony processes are also po</context>
<context position="17506" citStr="Barton (1985)" startWordPosition="2779" endWordPosition="2780">entally, note that it is local rather than global ambiguity that causes trouble; the generator system in the reduction can go through quite a bit of search even when there is just one final answer. Figure 2 traces the operation of the Kimmo generation algorithm on a (uniquely) satisfiable formula. Like the generator, the Kimmo recognizer can also be used to solve computationally difficult problems. One easy reduction treats 3SAT rather than SAT, uses negated alphabet symbols instead of a negation sign, and replaces the satisfaction automaton with constraints from the dictionary component; see Barton (1985) for details. 56 Generating from lexical form &amp;quot;-xy,-yz,-y-z,xyz&amp;quot;. 1 - 1,1,1,3 38 + -FF,-FT,-F-T,FFT 3,3,2,2 2 -F 3,1,1,2 39 &amp;quot;-FF,-FT,-F-T,FFT&amp;quot; *** result 3 -FF 3,3,1,2 40 (3) -FT 3,2,1,2 4 -FF, 3,3,1,1 41 -FT, 3,2,1,1 5 -FF,- 3,3,1,3 42 -FT,- 3,2,1,3 8 -FF,-T XXX y-con. 43 -FT. -F XXX y-con. 7 + -FF,-F 3,3,1,2 44 + -FT,-T 3,2,1,1 8 -FF,-FF 3,3,3,2 45 -FT,-TF 3,2,3,1 9 -FF,-FF, 3,3,3,1 46 -FT,-TF, XXX satis. 10 -FF,-FF,- 3,3,3,3 47 (45) -FT,-TT 3,2,2,2 11 -FF,-FF,-T XXX y-con. 48 -FT,-TT. 3,2,2,1 12 + -FF,-FF,-F 3,3,3,2 49 -FT,-TT,- 3,2.2,3 13 -FF,-FF,-F- 3,3,3,2 60 -FT,-TT,-F XXX y-con. 14 -FF</context>
</contexts>
<marker>Barton, 1985</marker>
<rawString>Barton, E. (1985). &amp;quot;The Computational Complexity of Two-Level Morphology,&amp;quot; A.I. Memo No. 856, M.I.T. Artificial Intelligence Laboratory, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Beck</author>
<author>D Elder</author>
<author>G Whittemore</author>
</authors>
<title>LISP Implementation [of the KImmo system],&amp;quot; Texas Linguistic Forum</title>
<date>1983</date>
<pages>22--187</pages>
<marker>Beck, Elder, Whittemore, 1983</marker>
<rawString>Gajek, 0., H. Beck, D. Elder, and G. Whittemore (1983). &amp;quot;LISP Implementation [of the KImmo system],&amp;quot; Texas Linguistic Forum 22:187-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garey</author>
<author>D Johnson</author>
</authors>
<title>Computers and Intractability.</title>
<date>1979</date>
<location>San Francisco:</location>
<contexts>
<context position="27466" citStr="Garey and Johnson, 1979" startWordPosition="4358" endWordPosition="4361">nd surface strings will have the same length. The recognition problem can be solved in the same way except that the machine must also guess a path through the dictionary. If null characters are completely unrestricted, the above argument fails; the lexical and surface strings may differ so radically in length that the lexical-surface correspondence cannot be proposed or verified in time polynomial in input length. The problem becomes PSPACEcomplete — as hard as checking for a forced win from certain N x N Go configurations, for instance, and probably even harder than NP-complete problems (cf. Garey and Johnson, 1979:171fF). The proof involves showing that Kimmo systems with unrestricted nulls can easily be induced to work out, in the space between two input characters, a solution to the difficult Finite State Automata Intersection problem. 58 The PSPACE-completeness reduction shows that if two-level morphology is formally characterized in a way that leaves null characters completely unrestricted, it can be very hard for the recognizer to reconstruct the superficially null characters that may lexically intervene between two surface characters. However, unrestricted nulls surely are not needed for linguist</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M., and D. Johnson (1979). Computers and Intractability. San Francisco: W. H. Freeman and Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>KIMMO: A Two-Level Morphological Analyzer,&amp;quot; Texas Linguistic Forum</title>
<date>1983</date>
<pages>22--165</pages>
<contexts>
<context position="3209" citStr="Karttunen (1983)" startWordPosition="467" endWordPosition="468">t often accompany affixation. In English, the program must expect love+ing to appear as loving, fly+s as flies, lie+ing as lying, and big+er as bigger. Its knowledge must be sufficiently sophisticated to distinguish such surface forms as hopped and hoped. Crosslinguistically, spelling-change processes may span either a limited or a more extended range of characters, and the material that triggers a change may occur either before or after the character that is affected. (Reduplication, a complex copying process that may also be found, will not be considered here.) The Kimmo system described by Karttunen (1983) is attractive for putting morphological knowledge to use in processing. Kimmo is an implementation of the &amp;quot;two-level&amp;quot; model of morphology that Kimmo Koskenniemi proposed and developed in his Ph.D. thesis.1 A system of lexicons in the dictionary component regulates the sequence of roots and affixes at the lexical level, while several finite-state transducers in the automaton component — 20 transducers for Finnish, for instance — mediate the correspondence between lexical and surface forms. Null characters allow the automata to handle insertion and deletion processes. The overall system can be </context>
<context position="4578" citStr="Karttunen (1983)" startWordPosition="691" endWordPosition="692">triggered by either left or right context and which may ignore irrelevant intervening characters. As an example, the following automaton describes a simplified &amp;quot;Y-change&amp;quot; process that changes y to ± before suffix es: &apos;University of Helsinki, Finland, circa Fall 1983. 53 &amp;quot;Y-Change&amp;quot; 5 5 y + &amp; = (lexical characters) y = s = (surface characters) 4 1 1 1 (normal state) 0 3 0 0 (require +s) 0 0 1 0 (require s) 4 5 1 1 (forbid +s) 4 1 0 1 (forbids) The details of this notation will not be explained here; basic familiarity with the Kimmo system is assumed. For further introduction, see Barton (1985), Karttunen (1983), and references cited therein. THE SEEDS OF COMPLEXITY At first glance, the finite-state machines of the twolevel model appear to promise unfailing computational efficiency. Both recognition and generation are built on the simple process of stepping the machines through the input. Lexical lookup is also fast, interleaved character by character with the quick left-to-right steps of the automata. The fundamental efficiency of finite-state machines promises to make the speed of Kimmo processing largely independent of the nature of the constraints that the automata encode: The most important tech</context>
<context position="6123" citStr="Karttunen, 1983" startWordPosition="937" endWordPosition="938">f a language has no significant effect on the speed at which the forms of that language can be recognized or generated. This is due to the fact that finite state machines are very fast to operate because of their simplicity .... Although Finnish, for example, is morphologically a much more complicated language than English, there is no difference of the same magnitude in the processing times for the two languages .... [This fact] has some psycholinguistic interest because of the common sense observation that we talk about &amp;quot;simple&amp;quot; and &amp;quot;complex&amp;quot; languages but not about &amp;quot;fast&amp;quot; and &amp;quot;slow&amp;quot; ones. (Karttunen, 1983:166f) For this kind of interest in the model to be sustained, it must be the model itself that wipes out processing difficulty, rather than some accidental property of the encoded morphological constraints. Examined in detail, the runtime complexity of Kimmo processing can be traced to three main sources. The recognizer and generator must both run the finite-state machines of the automaton component; in addition, the recognizer must descend the letter trees that make up a lexicon. The recognizer must also decide which suffix lexicon to explore at the end of an entry. Finally, both the recogni</context>
</contexts>
<marker>Karttunen, 1983</marker>
<rawString>Karttunen, L. (1983). &amp;quot;KIMMO: A Two-Level Morphological Analyzer,&amp;quot; Texas Linguistic Forum 22:165-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>K Wittenburg</author>
</authors>
<title>A Two-Level Morphological Analysis of English,&amp;quot; Texas Linguistic Forum</title>
<date>1983</date>
<pages>22--217</pages>
<contexts>
<context position="12103" citStr="Karttunen and Wittenburg (1983)" startWordPosition="1890" endWordPosition="1893">ogether ruled out all feasible pairs. After an XXX or *** entry, the recognizer backtracks and picks up from a previous choice point. indicated by the parenthesized step number before the lexical string. The tree on the right depicts the search graphically. reading from left. to right and top to bottom with vertical bars linking the choices at each choice point. The figures were generated with a KIM nit) implementation written in an augmented version of MACLIST based initially on Karttuneds (1983:182W) algorithm description; the dictionary and automaton components for English were taken front Karttunen and Wittenburg (1983) with minor changes. This implementation searches depth-first as Karttunen&apos;s does, but explores the alternatives at a given depth in a different order from Karttuneds. 55 Ignoring the problem of choosing among alternative lexicons, it is easy to see that the use of finite-state machinery helps control only one of the two remaining sources of complexity. Stepping the automata should be fast, but the finite-state framework does not guarantee speed in the task of guessing the correct lexical-surface correspondence. The search required to find the correspondence may predominate. In fact, the Kimmo</context>
</contexts>
<marker>Karttunen, Wittenburg, 1983</marker>
<rawString>Karttunen, L., and K. Wittenburg (1983). &amp;quot;A Two-Level Morphological Analysis of English,&amp;quot; Texas Linguistic Forum 22:217-228.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>