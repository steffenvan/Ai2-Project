<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.578189">
<title confidence="0.990262">
Discriminative Models for Semi-Supervised Natural Language Learning
</title>
<author confidence="0.963935">
Sajib Dasgupta and Vincent Ng
</author>
<affiliation confidence="0.9819725">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.814196">
Richardson, TX 75083-0688
</address>
<email confidence="0.999459">
{sajib,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.993043" genericHeader="method">
1 Discriminative vs. Generative Models
</sectionHeader>
<bodyText confidence="0.999986481481482">
An interesting question surrounding semi-
supervised learning for NLP is: should we use
discriminative models or generative models? De-
spite the fact that generative models have been
frequently employed in a semi-supervised setting
since the early days of the statistical revolution
in NLP, we advocate the use of discriminative
models. The ability of discriminative models to
handle complex, high-dimensional feature spaces
and their strong theoretical guarantees have made
them a very appealing alternative to their gen-
erative counterparts. Perhaps more importantly,
discriminative models have been shown to offer
competitive performance on a variety of sequential
and structured learning tasks in NLP that are
traditionally tackled via generative models , such
as letter-to-phoneme conversion (Jiampojamarn
et al., 2008), semantic role labeling (Toutanova
et al., 2005), syntactic parsing (Taskar et al.,
2004), language modeling (Roark et al., 2004), and
machine translation (Liang et al., 2006). While
generative models allow the seamless integration
of prior knowledge, discriminative models seem
to outperform generative models in a “no prior”,
agnostic learning setting. See Ng and Jordan (2002)
and Toutanova (2006) for insightful comparisons of
generative and discriminative models.
</bodyText>
<sectionHeader confidence="0.995954" genericHeader="method">
2 Discriminative EM?
</sectionHeader>
<bodyText confidence="0.994893333333333">
A number of semi-supervised learning systems can
bootstrap from small amounts of labeled data using
discriminative learners, including self-training, co-
</bodyText>
<page confidence="0.972274">
84
</page>
<bodyText confidence="0.999911911764706">
training (Blum and Mitchell, 1998), and transduc-
tive SVM (Joachims, 1999). However, none of them
seems to outperform the others across different do-
mains, and each has its pros and cons. Self-training
can be used in combination with any discriminative
learning model, but it does not take into account the
confidence associated with the label of each data
point, for instance, by placing more weight on the
(perfectly labeled) seeds than on the (presumably
noisily labeled) bootstrapped data during the learn-
ing process. Co-training is a natural choice if the
data possesses two independent, redundant feature
splits. However, this conditional independence as-
sumption is a fairly strict assumption and can rarely
be satisfied in practice; worse still, it is typically not
easy to determine the extent to which a dataset sat-
isfies this assumption. Transductive SVM tends to
learn better max-margin hyperplanes with the use
of unlabeled data, but its optimization procedure is
non-trivial and its performance tends to deteriorate if
a sufficiently large amount of unlabeled data is used.
Recently, Brefeld and Scheffer (2004) have pro-
posed a new semi-supervised learning technique,
EM-SVM, which is interesting in that it incorpo-
rates a discriminative model in an EM setting. Un-
like self-training, EM-SVM takes into account the
confidence of the new labels, ensuring that the in-
stances that are labeled with less confidence by the
SVM have less impact on the training process than
the confidently-labeled instances. So far, EM-SVM
has been tested on text classification problems, out-
performing transductive SVM. It would be interest-
ing to see whether EM-SVM can beat existing semi-
supervised learners for other NLP tasks.
</bodyText>
<note confidence="0.9511095">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.865732" genericHeader="method">
3 Effectiveness of Bootstrapping
</sectionHeader>
<bodyText confidence="0.99998">
How effective are the aforementioned semi-
supervised learning systems in bootstrapping from
small amounts of labeled data? While there are quite
a few success stories reporting considerable perfor-
mance gains over an inductive baseline (e.g., parsing
(McClosky et al., 2008), coreference resolution (Ng
and Cardie, 2003), and machine translation (Ueff-
ing et al., 2007)), there are negative results too (see
Pierce and Cardie (2001), He and Gildea (2006),
Duh and Kirchhoff (2006)). Bootstrapping perfor-
mance can be sensitive to the setting of the param-
eters of these semi-supervised learners (e.g., when
to stop, how many instances to be added to the la-
beled data in each iteration). To date, however, re-
searchers have relied on various heuristics for pa-
rameter selection, but what we need is a principled
method for addressing this problem. Recently, Mc-
Closky et al. (2008) have characterized the condi-
tions under which self-training would be effective
for semi-supervised syntactic parsing. We believe
that the NLP community needs to perform more re-
search of this kind, which focuses on identifying the
algorithm(s) that achieve good performance under a
given setting (e.g., few initial seeds, large amounts
of unlabeled data, complex feature space, skewed
class distributions).
</bodyText>
<sectionHeader confidence="0.996655" genericHeader="method">
4 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.9998981">
Domain adaptation has recently become a popular
research topic in the NLP community. Labeled data
for one domain might be used to train a initial classi-
fier for another (possibly related) domain, and then
bootstrapping can be employed to learn new knowl-
edge from the new domain (Blitzer et al., 2007). It
would be interesting to see if we can come up with
a similar semi-supervised learning model for pro-
jecting resources from a resource-rich language to
a resource-scarce language.
</bodyText>
<sectionHeader confidence="0.995245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998537115384615">
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT.
Ulf Brefeld and Tobias Scheffer. 2004. Co-EM support
vector learning. In Proceedings ofICML.
Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acqui-
sition for dialectal Arabic using transductive learning.
In Proceedings ofEMNLP.
Shan He and Daniel Gildea. 2006. Self-training and co-
training for semantic role labeling.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proceed-
ings ofACL-08:HLT.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings ofICML.
Percy Liang, Alexandre Bouchard, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In Proceedings of the ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings ofHLT-NAACL.
Andrew Ng and Michael Jordan. 2002. On discrimina-
tive vs.generative classifiers: A comparison of logistic
regression and Naive Bayes. In Advances in NIPS.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of EMNLP.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proceedings of the ACL.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proceedings ofEMNLP.
Kristina Toutanova, Aria Haghighi, , and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of the ACL.
Kristina Toutanova. 2006. Competitive generative mod-
els with structure learning for NLP classification tasks.
In Proceedings ofEMNLP.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the ACL.
</reference>
<page confidence="0.9997">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.004577">
<title confidence="0.996433">Discriminative Models for Semi-Supervised Natural Language Learning</title>
<author confidence="0.87832">Dasgupta</author>
<affiliation confidence="0.76863175">Human Language Technology Research University of Texas at Richardson, TX 1 Discriminative vs. Generative Models</affiliation>
<abstract confidence="0.994754943396226">An interesting question surrounding semisupervised learning for NLP is: should we use discriminative models or generative models? Despite the fact that generative models have been frequently employed in a semi-supervised setting since the early days of the statistical revolution in NLP, we advocate the use of discriminative models. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using learners, including self-training, co- 84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others across different domains, and each has its pros and cons. Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data point, for instance, by placing more weight on the (perfectly labeled) seeds than on the (presumably noisily labeled) bootstrapped data during the learning process. Co-training is a natural choice if the data possesses two independent, redundant feature splits. However, this conditional independence assumption is a fairly strict assumption and can rarely be satisfied in practice; worse still, it is typically not easy to determine the extent to which a dataset satisfies this assumption. Transductive SVM tends to learn better max-margin hyperplanes with the use of unlabeled data, but its optimization procedure is non-trivial and its performance tends to deteriorate if a sufficiently large amount of unlabeled data is used. Recently, Brefeld and Scheffer (2004) have proposed a new semi-supervised learning technique, EM-SVM, which is interesting in that it incorporates a discriminative model in an EM setting. Unlike self-training, EM-SVM takes into account the confidence of the new labels, ensuring that the instances that are labeled with less confidence by the SVM have less impact on the training process than the confidently-labeled instances. So far, EM-SVM has been tested on text classification problems, outperforming transductive SVM. It would be interesting to see whether EM-SVM can beat existing semisupervised learners for other NLP tasks. of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language pages 84–85, Colorado, June 2009. Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, Mc- Closky et al. (2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing. We believe that the NLP community needs to perform more research of this kind, which focuses on identifying the algorithm(s) that achieve good performance under a given setting (e.g., few initial seeds, large amounts of unlabeled data, complex feature space, skewed class distributions). 4 Domain Adaptation Domain adaptation has recently become a popular research topic in the NLP community. Labeled data for one domain might be used to train a initial classifier for another (possibly related) domain, and then bootstrapping can be employed to learn new knowledge from the new domain (Blitzer et al., 2007). It would be interesting to see if we can come up with a similar semi-supervised learning model for projecting resources from a resource-rich language to a resource-scarce language.</abstract>
<note confidence="0.873079">References John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In</note>
<abstract confidence="0.844276707317073">of the Blum and Tom Mitchell. 1998. Combining laand unlabeled data with co-training. In Proceedof Ulf Brefeld and Tobias Scheffer. 2004. Co-EM support learning. In Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acquisition for dialectal Arabic using transductive learning. Shan He and Daniel Gildea. 2006. Self-training and cotraining for semantic role labeling. Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative for letter-to-phoneme conversion. In Proceed- Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In Percy Liang, Alexandre Bouchard, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach machine translation. In of the David McClosky, Eugene Charniak, and Mark Johnson. 2008. When is self-training effective for parsing? In of Vincent Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In Andrew Ng and Michael Jordan. 2002. On discriminative vs.generative classifiers: A comparison of logistic and Naive Bayes. In in David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large In of Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron al- In of the Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin pars- In Kristina Toutanova, Aria Haghighi, , and Christopher D. Manning. 2005. Joint learning improves semantic role In of the Kristina Toutanova. 2006. Competitive generative models with structure learning for NLP classification tasks.</abstract>
<note confidence="0.516884">Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine In of the 85</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="1769" citStr="Blum and Mitchell, 1998" startWordPosition="239" endWordPosition="242">ctic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others across different domains, and each has its pros and cons. Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data point, for instance, by placing more weight on the (perfectly labeled) seeds than on the (presumably noisily labeled) bootstrapped data during the learning process. Co-training is a natural choice if the data possesses two independent, redundant feature splits. However, this con</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Brefeld</author>
<author>Tobias Scheffer</author>
</authors>
<title>Co-EM support vector learning.</title>
<date>2004</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="2857" citStr="Brefeld and Scheffer (2004)" startWordPosition="409" endWordPosition="412">ng the learning process. Co-training is a natural choice if the data possesses two independent, redundant feature splits. However, this conditional independence assumption is a fairly strict assumption and can rarely be satisfied in practice; worse still, it is typically not easy to determine the extent to which a dataset satisfies this assumption. Transductive SVM tends to learn better max-margin hyperplanes with the use of unlabeled data, but its optimization procedure is non-trivial and its performance tends to deteriorate if a sufficiently large amount of unlabeled data is used. Recently, Brefeld and Scheffer (2004) have proposed a new semi-supervised learning technique, EM-SVM, which is interesting in that it incorporates a discriminative model in an EM setting. Unlike self-training, EM-SVM takes into account the confidence of the new labels, ensuring that the instances that are labeled with less confidence by the SVM have less impact on the training process than the confidently-labeled instances. So far, EM-SVM has been tested on text classification problems, outperforming transductive SVM. It would be interesting to see whether EM-SVM can beat existing semisupervised learners for other NLP tasks. Proc</context>
</contexts>
<marker>Brefeld, Scheffer, 2004</marker>
<rawString>Ulf Brefeld and Tobias Scheffer. 2004. Co-EM support vector learning. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Lexicon acquisition for dialectal Arabic using transductive learning.</title>
<date>2006</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="4154" citStr="Duh and Kirchhoff (2006)" startWordPosition="604" endWordPosition="607"> Language Processing, pages 84–85, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing. We believe that the NLP community needs to perform more research of this kind, which f</context>
</contexts>
<marker>Duh, Kirchhoff, 2006</marker>
<rawString>Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acquisition for dialectal Arabic using transductive learning. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shan He</author>
<author>Daniel Gildea</author>
</authors>
<title>Self-training and cotraining for semantic role labeling.</title>
<date>2006</date>
<contexts>
<context position="4128" citStr="He and Gildea (2006)" startWordPosition="600" endWordPosition="603">d Learning for Natural Language Processing, pages 84–85, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing. We believe that the NLP community needs to perform more rese</context>
</contexts>
<marker>He, Gildea, 2006</marker>
<rawString>Shan He and Daniel Gildea. 2006. Self-training and cotraining for semantic role labeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:HLT.</booktitle>
<contexts>
<context position="1089" citStr="Jiampojamarn et al., 2008" startWordPosition="144" endWordPosition="147">loyed in a semi-supervised setting since the early days of the statistical revolution in NLP, we advocate the use of discriminative models. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discrimin</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In Proceedings ofACL-08:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="1808" citStr="Joachims, 1999" startWordPosition="247" endWordPosition="248">eling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others across different domains, and each has its pros and cons. Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data point, for instance, by placing more weight on the (perfectly labeled) seeds than on the (presumably noisily labeled) bootstrapped data during the learning process. Co-training is a natural choice if the data possesses two independent, redundant feature splits. However, this conditional independence assumption is a f</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1265" citStr="Liang et al., 2006" startWordPosition="170" endWordPosition="173">dle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others ac</context>
</contexts>
<marker>Liang, Bouchard, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>When is self-training effective for parsing?</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="3949" citStr="McClosky et al., 2008" startWordPosition="572" endWordPosition="575">ransductive SVM. It would be interesting to see whether EM-SVM can beat existing semisupervised learners for other NLP tasks. Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008)</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2008</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2008. When is self-training effective for parsing? In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="3995" citStr="Ng and Cardie, 2003" startWordPosition="578" endWordPosition="581">whether EM-SVM can beat existing semisupervised learners for other NLP tasks. Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincent Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>On discriminative vs.generative classifiers: A comparison of logistic regression and Naive Bayes.</title>
<date>2002</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="1469" citStr="Ng and Jordan (2002)" startWordPosition="199" endWordPosition="202"> models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others across different domains, and each has its pros and cons. Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with th</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Ng and Michael Jordan. 2002. On discriminative vs.generative classifiers: A comparison of logistic regression and Naive Bayes. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4106" citStr="Pierce and Cardie (2001)" startWordPosition="596" endWordPosition="599">Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing. We believe that the NLP community need</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1219" citStr="Roark et al., 2004" startWordPosition="163" endWordPosition="166">s. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, </context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="1179" citStr="Taskar et al., 2004" startWordPosition="157" endWordPosition="160"> advocate the use of discriminative models. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and tran</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Toutanova, Haghighi, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, , and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
</authors>
<title>Competitive generative models with structure learning for NLP classification tasks.</title>
<date>2006</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="1490" citStr="Toutanova (2006)" startWordPosition="204" endWordPosition="205">o offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co84 training (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others across different domains, and each has its pros and cons. Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data </context>
</contexts>
<marker>Toutanova, 2006</marker>
<rawString>Kristina Toutanova. 2006. Competitive generative models with structure learning for NLP classification tasks. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="4043" citStr="Ueffing et al., 2007" startWordPosition="585" endWordPosition="589"> learners for other NLP tasks. Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-super</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>