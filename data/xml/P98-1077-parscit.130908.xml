<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<author confidence="0.64837">
Efficient Linear Logic Meaning Assembly
</author>
<affiliation confidence="0.846934666666667">
Vineet Gupta
Caelum Research Corporation
NASA Ames Research Center
</affiliation>
<address confidence="0.540045833333333">
Moffett Field CA 94035
vguptaOptolemy.arc.nasa.gov
John Lamping
Xerox PARC
3333 Coyote Hill Road
Palo Alto CA 94304 USA
</address>
<email confidence="0.95318">
lamping@parc.xerox.com
</email>
<figure confidence="0.9537835">
&apos;LEAVE&apos;
[PRED &apos;CAT&apos;
g.. SPEC &apos;EVERY&apos;
1 MODS { [ PRED &apos;GRAY&apos;]}
(1) -FRED
f: SUBJ
</figure>
<sectionHeader confidence="0.799328" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987049128205128">
The &amp;quot;glue&amp;quot; approach to semantic composition
in Lexical-Functional Grammar uses linear logic
to assemble meanings from syntactic analyses
(Dalrymple et al., 1993). It has been compu-
tationally feasible in practice (Dalrymple et al.,
1997b). Yet deduction in linear logic is known
to be intractable. Even the propositional ten-
sor fragment is NP complete(Kanovich, 1992).
In this paper, we investigate what has made
the glue approach computationally feasible and
show how to exploit that to efficiently deduce
underspecified representations.
In the next section, we identify a restricted
pattern of use of linear logic in the glue analyses
we are aware of, including those in (Crouch and
Genabith, 1997; Dalrymple et al., 1996; Dal-
rymple et al., 1995). And we show why that
fragment is computationally feasible. In other
words, while the glue approach could be used
to express computationally intractable analyses,
actual analyses have adhered to a pattern of use
of linear logic that is tractable.
The rest of the paper shows how this pat-
tern of use can be exploited to efficiently cap-
ture all possible deductions. We present a con-
servative extension of linear logic that allows a
reformulation of the semantic contributions to
better exploit this pattern, almost turning them
into Horn clauses. We present a deduction algo-
rithm for this formulation that yields a compact
description of the possible deductions. And fi-
nally, we show how that description of deduc-
tions can be turned into a compact underspeci-
fied description of the possible meanings.
Throughout the paper we will use the illus-
trative sentence &amp;quot;every gray cat left&amp;quot;. It has
functional structure
and semantic contributions
leave :Vx. –o f,,leave(x)
</bodyText>
<listItem confidence="0.693293333333333">
cat :Vs. (g, –o (g, RESTR)-.* ca(x)
gray :VP. [Vx. (g, VAR)---*x —o (g, RESTR)--.P (x)]
—o Ns. (g, VAR)-+x
</listItem>
<equation confidence="0.9691536">
—o (ga RESTR).-s--gray(P)(x)]
every :V H , R, S.
[Vx.(g(7 VAR)&apos;-+x —0 Oa RESTR).---+R(x)]
0[Vx. –o H,S(s)]
–o H,every(R,S)
</equation>
<bodyText confidence="0.998825666666667">
For our purposes, it is more convenient to fol-
low (Dalrymple et al., 1997a) and separate the
two parts of the semantic contributions: use a
lambda term to capture the meaning formulas,
and a type to capture the connections to the
f-structure. In this form, the contributions are
</bodyText>
<equation confidence="0.889913666666666">
leave: Ax.leave(x): g„ –o f,
cat : Ax.cai(x): (g(7 VAR) —o (g„ RESTR)
gray: AP.Ax.gray(P)(x):
((g(7 VAR) —o (g(7 RESTR))
—o VAR) —0 (g, RESTR)
every: AR.AS.every(R,S) :
VH. (((g(7 VAR) —o (gc, RESTR))
0(g, –0 H))
–o H
</equation>
<bodyText confidence="0.999171625">
With this separation, the possible derivations
are determined solely by the &amp;quot;types&amp;quot;, the con-
nections to the f-structure. The meaning is as-
sembled by applying the lambda terms in ac-
cordance with a proof of a type for the sen-
tence. We give the formal system behind this
approach, C in Figure 1 — this is a different
presentation of the system given in (Dalrymple
</bodyText>
<page confidence="0.999269">
464
</page>
<bodyText confidence="0.9999794">
et al., 1997a), adding the two standard rules
for tensor, using pairing for meanings. For the
types, the system merely consists of the linear
logic rules for the glue fragment.
We give the proof for our example in Figure 2,
where we have written the types only, and have
omitted the trivial proofs at the top of the tree.
The meaning every(gray(cat),left) may be as-
sembled by putting the meanings back in ac-
cording to the rules of C and i&apos;-reduction.
</bodyText>
<equation confidence="0.862464833333333">
r N : A A, IVI[N 1 x] : B 1-c R
1-, A, : A -0 BI-c R
r, y: M[Y1x]:B(y new)
r I--c Axivl : A -0 B
: A,N : B R r I- Al:A Al-N:B
r, (Al, N) :AOBI-R (M, N) : A B
</equation>
<figureCaption confidence="0.987841">
Figure 1: The system C. M,N are meanings,
and x, y are meaning variables. A, B are types,
and X, Y are type variables. P,Q,R are formu-
</figureCaption>
<bodyText confidence="0.999684431818182">
las of the kind Al : A. P, are multisets of
formulas.
2 Skeleton references and modifier ref-
erences
The terms that describe atomic types, terms like
g, and (y, VAR), are semantic structure refer-
ences, the type atoms that connect the semantic
assembly to the syntax. There is a pattern to
how they occur in glue analyses, which reflects
their function in the semantics.
Consider a particular type atom in the ex-
ample, such as g„. It occurs once positively in
the contribution of &amp;quot;every&amp;quot; and once negatively
in the contribution of &amp;quot;leave&amp;quot;. A slightly more
complicated example, the type (g, RESTR) oc-
curs once positively in the contribution of &amp;quot;cat&amp;quot;,
once negatively in the contribution of &amp;quot;every&amp;quot;,
and once each positively and negatively in the
contribution of &amp;quot;gray&amp;quot;.
The pattern is that every type atom occurs
once positively in one contribution, once nega-
tively in one contribution, and once each posi-
tively and negatively in zero or more other con-
tributions. (To make this generalization hold,
we add a negative occurrence or &amp;quot;consumer&amp;quot; of
f„, the final meaning of the sentence.) This pat-
tern holds in all the glue analyses we know of,
with one exception that we will treat shortly.
We call the independent occurrences the skele-
ton occurrences, and the occurrences that occur
paired in a contribution modifier occurrences.
The pattern reflects the functions of the lex-
ical entries in LFG. For the type that corre-
sponds to a particular f-structure, the idea is
that, the entry corresponding to the head makes
a positive skeleton contribution, the entry that
subcategorizes for the f-structure makes a neg-
ative skeleton contribution, and modifiers on
the f-structure make both positive and negative
modifier contributions.
Here are the contributions for the example
sentence again, with the occurrences classified.
Each occurrence is marked positive or negative,
and the skeleton occurrences are underlined.
</bodyText>
<equation confidence="0.988463428571429">
leave: g -o f,+
cat : (ga VAR)- -0 (ga RESTR)I-
gray : ((ga vAR)+ -0 (ga REsTR)-)
-0 (ga VAR)- -0 (ga RESTR)F
every: VH. (((ga vAR)+ -0 (ga REsTR)- )
0(g,+ -o 1-1-))
H+
</equation>
<bodyText confidence="0.99989195">
This pattern explains the empirical tractabil-
ity of glue inference. In the general case of
multiplicative linear logic, there can be complex
combinatorics in matching up positive and neg-
ative occurrences of literals, which leads to NP-
completeness (Kanovich, 1992). But in the glue
fragment, on the other hand, the only combina-
torial question is the relative ordering of modi-
fiers. In the common case, each of those order-
ings is legal and gives rise to a different mean-
ing. So the combinatorics of inference tends to
be proportional to the degree of semantic am-
biguity. The complexity per possible reading is
thus roughly linear in the size of the utterance.
But, this simple combinatoric structure sug-
gests a better way to exploit the pattern.
Rather than have inference explore all the corn-
binatorics of different modifier orders, we can
get a single underspecified representation that
captures all possible orders, without having to
</bodyText>
<figure confidence="0.65928775">
Al : Ahc M&apos; : A
where MM&apos;
F , Al : A[B I Xj1-c R
r,M: VX.A Fc R
r,P,Q,A1-c R
F,Q,P,A1-c R
Al : A[Y I X] tY new)
T }-c Al :VX.A
</figure>
<page confidence="0.845424">
465
</page>
<equation confidence="0.682816">
cat I— (g, VAR) —o (ga RESTR) (gc, VAR) —0 (gz, RESTR) Oa VAR) —0 Oa RESTR)
cat, ((g,, VAR) —o (g,, RESTR)) —o (ga VAR) —o (ga RESTR) (gc, VAR) —0 RESTR)
gray, cat h (ga VAR) —o (g, RESTR) leave I- ga —o f,
gray, cat, leave ((ga VAR) —o (ga RESTR)) 0 (ga —o f„) f f-
gray, cat, leave, (((ga VAR) —0 (ga RESTR)) (ga —o f„ ) ) —o f„ f,
every, gray, cat, leave 1- f,
</equation>
<figureCaption confidence="0.998449">
Figure 2: Proof of &amp;quot;Every gray cat left&amp;quot;, omitting the lambda terms
</figureCaption>
<bodyText confidence="0.997043571428571">
explore them.
The idea is to do a preliminary deduction in-
volving just the skeleton, ignoring the modifier
occurrences. This will be completely determin-
istic and linear in the total length of the for-
mulas. Once we have this skeletal deduction,
we know that the sentence is well-formed and
has a meaning, since modifier occurrences es-
sentially occur as instances of the identity ax-
iom and do not contribute to the type of the
sentence. Then the system can determine the
meaning terms, and describe how the modifiers
can be attached to get the final meaning term.
That is the goal of the rest of the paper.
</bodyText>
<sectionHeader confidence="0.913043" genericHeader="method">
3 Conversion toward horn clauses
</sectionHeader>
<bodyText confidence="0.997951233333333">
The first hurdle is that the distinction between
skeleton and modifier applies to atomic types,
not to entire contributions. The contribution of
&amp;quot;every&amp;quot;, for example, has skeleton contributions
for ga, (gcr VAR), and (g, RESTR), but modifier
contributions for H. Furthermore, the nested
implication structure allows no nice way to dis-
entangle the two kinds of occurrences. When a
deduction interacts with the skeletal g, in the
hypothetical it also brings in the modifier H.
If the problematic hypothetical could be con-
verted to Horn clauses, then we could get a bet-
ter separation of the two types of occurrences.
We can approximate this by going to an in-
dexed linear logic, a conservative extension of
the system of Figure 1, similar to Hepple&apos;s sys-
tem(Hepple, 1996).
To handle nested implications, we introduce
the type constructor A{B}, which indicates an
A whose derivation made use of B. This is sim-
ilar to Hepple&apos;s use of indices, except that we
indicate dependence on types, rather than on in-
dices. This is sufficient in our application, since
each such type has a unique positive skeletal
occurrence.
We can eliminate problematic nested impli-
cations by translating them into this construct,
in accordance with the following rule:
For a nested hypothetical at top level that has
a mix of skeleton and modifier types:
</bodyText>
<equation confidence="0.88783">
M : (A —o B) --0 C
replace it with
x : A, ill : (B{A} —0 C)
</equation>
<bodyText confidence="0.9907235">
where x is a new variable, and reduce complex
dependency formulas as follows:
</bodyText>
<listItem confidence="0.9948645">
1. Replace A{B —o C} with A{C{B}}.
2. Replace (A —0 B){C} with A —0 B{C}.
</listItem>
<bodyText confidence="0.9983605">
The semantics of the new type constructors
is captured by the additional proof rule:
</bodyText>
<equation confidence="0.93634">
r,x:AI-M: B
F, x : A I- Ax .M : B{A}
</equation>
<bodyText confidence="0.7124095">
The translation is sound with respect to this
rule:
</bodyText>
<construct confidence="0.557641166666667">
Theorem 1 If r is a set of sentences in the
unextended system of Figure 1, A is a sentence
in that system, and F&apos; results from r by applying
the above conversion rules, then r h A in the
system of Figure 1 F- A in the extended
system.
</construct>
<bodyText confidence="0.9999722">
The analysis of pronouns present a different
problem, which we discuss in section 5. For all
other glue analyses we know of, these conver-
sions are sufficient to separate items that mix
interaction and modification into statements of
</bodyText>
<page confidence="0.998313">
466
</page>
<bodyText confidence="0.9980139">
the form S, M, or S –0 M, where S is pure
skeleton and M is pure modifier. Furthermore,
M will be of the form A –o A, where A may be
a formula, not just an atom. In other words, the
type of the modifier will be an identity axiom.
The modifier will consume some meaning and
produce a modified meaning of the same type.
In our example, the contribution of &amp;quot;every&amp;quot;,
can be transformed by two applications of the
nested hypothetical rule to
</bodyText>
<equation confidence="0.987634">
every :AR.AS.every(R,S):
V H (g, REsTR){(g, v AR)}
–0 H{g,} –0 H
x :(g, VAR)
y :g,
</equation>
<bodyText confidence="0.989582830188679">
Here, the last two sentences are pure skele-
ton, producing (g, VAR) and 9,, respectively.
The first is of the form S –o M, consuming
(g, RESTR), to produce a pure modifier.
While the rule for nested hypotheticals could
be generalized to eliminate all nested implica-
tions, as Hepple does, that is not our goal, be-
cause that does remove the combinatorial com-
bination of different modifier orders. We use the
rule only to segregate skeleton atoms from mod-
ifier atoms. Since we want modifiers to end up
looking like the identity axiom, we leave them
in the A –o A form, even if A contains further
implications. For example, we would not apply
the nested hypothetical rule to simplify the en-
try for gray any further, since it is already in
the form A –o A.
Handling intensional verbs requires a more
precise definition of skeleton and modifier. The
type part of an intensional verb contribution
looks like (V F.(h, –o F) –o F) –o g, –o fo.
(Dalrymple et al., 1996).
First, we have to deal with the small
technical problem that the VF gets in the
way of the nested hypothetical translation
rule. This is easily resolved by introducing
a skolem constant, S, turning the type into
((he –o 5) –o S) –0 g, –o f. Now, the
nested hypothetical rule can be applied to yield
(h, –o S) and S{S{h4} –o g,, –o f
But now we have the interesting question of
whether the occurrences of the skolem constant,
S, are skeleton or modifier. If we observe how S
resources get produced and consumed in a de-
duction involving the intensional verb, we find
that (h, –o S) produces an S, which may be
modified by quantifiers, and then gets consumed
by S{S{h,}} –o g, –o f. So unlike a modifier,
which takes an existing resource from the envi-
ronment and puts it back, the intentional verb
places the initial resource into the environment,
allows modifiers to act on it, and then takes it
out. In other words, the intensional verb is act-
ing like a combination of a skeleton producer
and a skeleton consumer.
So just because an atom occurs twice in a
contribution doesn&apos;t make the contribution a
modifier. It is a modifier if its atoms must in-
teract with the outside, rather than with each
other. Roughly, paired modifier atoms function
as f –o f, rather than as f f1, as do the S
atoms of intensional verbs.
Stated precisely:
</bodyText>
<construct confidence="0.88795">
Definition 2 Assume two occurrences of the
same type atom occur in a single contribution.
Convert the formula to a normal form consist-
ing of just 0, 19 , and 1 on atoms by converting
subformulas A –o B to the equivalent 22 B,
and then using DeMorgan&apos;s laws to push all 1&apos;s
down to atoms. Now, if the occurrences of the
same type atom occur with opposite polarity and
the connective between the two subexpressions in
which they occur is 28 , then the occurrences are
modifiers. All other occurrences are skeleton.
</construct>
<bodyText confidence="0.99858625">
For the glue analyses we are aware of, this def-
inition identifies exactly one positive and one
negative skeleton occurrence of each type among
all the contributions for a sentence.
</bodyText>
<sectionHeader confidence="0.984795" genericHeader="method">
4 Efficient deduction of underspecified
representation
</sectionHeader>
<bodyText confidence="0.999672692307692">
In the converted form, the skeleton deductions
can be done independently of the modifier de-
ductions. Furthermore, the skeleton deductions
are completely trivial, they require just a lin-
ear time algorithm: since each type occurs once
positively and once negatively, the algorithm
just resolves the matching positive and nega-
tive skeleton occurrences. The result is several
deductions starting from the contributions, that
collectively use all of the contributions. One of
the deductions produces a meaning for fcr, for
the whole f-structure. The others produce pure
modifiers — these are of the form A –o A. For
</bodyText>
<page confidence="0.990611">
467
</page>
<bodyText confidence="0.523973">
Lexical contributions in indexed logic:
</bodyText>
<equation confidence="0.933037857142857">
leave:
cat:
gray:
everyi :
every2 :
every3 :
Ax.leave(x): g -o f„
Ax.cat(x): (g, VAR) -o (g, RESTR)
AP.Ax.gray(P)(x): ((g, VAR) -o (g, RESTR)) -o (g, VAR) -o (g, RESTR)
AR.AS.every(R, 5) :VH. (g, REsTR){(g, VAR)} --0 H {go} -0 H
x : (g„ VAR)
y : g,
The following can now be proved using the extended system:
gray I- A.P.Ax.gray(P)(x): ((g, VAR) -o (g, RESTR)) -o (g, VAR) (g„ RESTR)
</equation>
<figureCaption confidence="0.7857888">
every2, cat, everyi AS.every(Ax.cat(x), 5) :VH. H{g,} -o H
every3, leave I- leave(y): f,
Figure 3: Skeleton deductions for &amp;quot;Every gray cat left&amp;quot;.
the example sentence, the results are shown in
Figure 3.
</figureCaption>
<bodyText confidence="0.999978744680851">
These skeleton deductions provide a compact
representation of all possible complete proofs.
Complete proofs can be read off from the skele-
ton proofs by interpolating the deduced modi-
fiers into the skeleton deduction. One way to
think about interpolating the modifiers is in
terms of proof nets. A modifier is interpolated
by disconnecting the arcs of the proof net that
connect the type or types it modifies, and recon-
necting them through the modifier. Quantifiers,
which turn into modifiers of type V F.F -0 F,
can choose which type they modify.
Not all interpolations of modifiers are le-
gal, however. For example, a quantifier must
outscope its noun phrase. The indices of the
modifier record these limitations. In the case
of the modifier resulting from &amp;quot;every cat&amp;quot;,
VH.H{ga} -0 H, it records that it must
outscope &amp;quot;every cat&amp;quot; in the {ga}. The in-
dices determine a partial order of what modi-
fiers must outscope other modifiers or skeleton
terms.
In this particular example, there is no choice
about where modifiers will act or what their rel-
ative order is. In general, however, there will be
choices, as in the sentence &amp;quot;someone likes every
cat&amp;quot;, analyzed in Figure 4.
To summarize so far, the skeleton proofs pro-
vide a compact representation of all possible de-
ductions. Particular deductions are read off by
interpolating modifiers into the proofs, subject
to the constraints. But we are usually more in-
terested in all possible meanings than in all pos-
sible deductions. Fortunately, we can extract a
compact representation of all possible meanings
from the skeleton proofs.
We do this by treating the meanings of the
skeleton deductions as trees, with their arcs an-
notated with the types that correspond to the
types of values that flow along the arcs. Just as
modifiers were interpolated into the proof net
links, now modifiers are interpolated into the
links of the meaning trees. Constraints on what
modifiers must outscope become constraints on
what tree nodes a modifier must dominate.
Returning to our original example, the skele-
ton deductions yield the following three trees:
</bodyText>
<equation confidence="0.961515538461538">
H
vAR)ever
—o Hg
ga RESTR) /
\
y
I fa , Ax. , VAR.) --0
kgc, REsTR)I ge, RESTR)
leave gray
1g° (g VAR)cLat
VAR) —o
gc, RESTR)
leave(y) AS.every(Ax.cat(x), S) AP.Ax.gray(P)(x)
</equation>
<bodyText confidence="0.997333769230769">
Notice that higher order arguments
are reflected as structured types, like
(g, VAR) -0 (g, RESTR). These trees are
a compact description of the possible meanings,
in this case the one possible meaning. We
believe it will be possible to translate this rep-
resentation into a UDRS representation(Reyle,
1993), or other similar representations for
ambiguous sentences.
We can also use the trees directly as an un-
derspecified representation. To read out a par-
ticular meaning, we just interpolate modifiers
into the arcs they modify. Dependencies on a
</bodyText>
<page confidence="0.998216">
468
</page>
<table confidence="0.860057142857143">
The functional structure of &amp;quot;Someone likes every cat&amp;quot;.
FRED &apos;LIKE&apos;
SUBJ h:[ FRED &apos;SOMEONE&apos;
f:
OBJ g:[PRED &apos;CAT&apos;
SPEC &apos;EVERY&apos;
The lexical entries after conversion to indexed form:
</table>
<equation confidence="0.967613285714286">
like:
cat :
someonei :
someone2 :
everyi :
every2 :
every3 :
Ax.Ay.like(x,y): (h, gc) -o f,
Ax.cat(x): (g, VAR) -o (g, RESTR)
z : h,
AS.some(person, S) :VH. H {h,} -o H
AR.AS.every(R, 5): VH. (g„ REsTR){(g, VAR)) -o H{g,} -o
x : (g„ VAR)
y : g,
</equation>
<bodyText confidence="0.771226">
From these we can prove:
someonei, every3, like I- like(z,y):
</bodyText>
<note confidence="0.435751">
someone2 AS.some(person, S) :V H. H{h,} -o H
</note>
<figureCaption confidence="0.753782666666667">
every2, cat, everyi AS.every(cat, S) : VH. H {g,} -o H
Figure 4: Skeleton deductions for &amp;quot;Someone likes every cat&amp;quot;
modifier&apos;s type indicate that a lambda abstrac-
tion is also needed. So, when &amp;quot;every cat&amp;quot; mod-
ifies the sentence meaning, its antecedent, in-
stantiated to f,{g,} indicates that it lambda
abstracts over the variable annotated with g,
and replaces the term annotated f,. So the re-
sult is:
</figureCaption>
<figure confidence="0.967605571428572">
I fa
every
(g, VAR) -‘o \g, -o
(g, RESTR) \/
Ax. Ay.
cat leave
(g, VAR) ga
</figure>
<bodyText confidence="0.94879525">
Similarly &amp;quot;gray&amp;quot; can modify this by splicing
it into the line labeled (g, VAR) -o (g, RESTR)
to yield (after it-reduction, and removing labels
on the arcs).
</bodyText>
<construct confidence="0.6251825">
f,
every
gray leave
cat
</construct>
<bodyText confidence="0.904395625">
This gets us the expected meaning
every(gray(cat),leave).
In some cases, the link called for by a higher
order modifier is not directly present in the tree,
and we need to do A-abstraction to support
it. Consider the sentence &amp;quot;John read Hamlet
quickly&amp;quot;. We get the following two trees from
the skeleton deductions:
</bodyText>
<figure confidence="0.848275">
I fa
cl
g/rea \h, quickly
ga f,
John Hamlet -0
read(John,Hamlet) AP.Ax.quickly(P)(x)
</figure>
<bodyText confidence="0.9108032">
There is no link labeled g, -o f, to be modi-
fied. The left tree however may be converted by
A-abstraction to the following tree, which has a
required link. The © symbol represents A ap-
plication of the right subtree to the left.
</bodyText>
<figure confidence="0.908059857142857">
fa
g° f&apos;Y
Ax. John
I fa
read
gv \h,
Hamlet
</figure>
<bodyText confidence="0.945797">
Now quickly can be interpolated into the
link labeled g, -o h. to get the desired
meaning quickly(read(Hamlet), John), after
reduction. The cases where A-abstraction is re-
quired can be detected by scanning the modi-
fiers and noting whether the links to be mod-
ified are present in the skeleton trees. If not,
A-abstraction can introduce them into the un-
fa
(ga REsTR)I
</bodyText>
<page confidence="0.998713">
469
</page>
<bodyText confidence="0.992984333333333">
derspecified representation. Furthermore, the
introduction is unavoidable, as the link will be
present in any final meaning.
</bodyText>
<sectionHeader confidence="0.996651" genericHeader="method">
5 Anaphora
</sectionHeader>
<bodyText confidence="0.999835928571429">
As mentioned earlier, anaphoric pronouns
present a different challenge to separating skele-
ton and modifier. Their analysis yields types
like fcr —o 0 g„) where g, is skeleton and f,
is modifier. We sketch how to separate them.
We introduce another type constructor (B)A,
informally indicating that A has not been fully
used, but is also used to get B.
This lets us break apart an implication whose
right hand side is a product in accordance with
the following rule:
For an implication that occurs at top level,
and has a product on the right hand side that
mixes skeleton and modifier types:
</bodyText>
<construct confidence="0.382239666666667">
Ax.(M,N): A —o (BOC)
replace it with
Ax.M : (C)A —o B, N : C
</construct>
<bodyText confidence="0.957383">
The semantics of this constructor is captured
by the two rules:
</bodyText>
<listItem confidence="0.6627155">
: Ai, • : Ari M : A
: (B)Ai, • • • Mn (B)An M : (B)A
</listItem>
<bodyText confidence="0.972488">
F, Mi: (B)A, M2 :BHN:C
F&apos;, M: A, : B : C
where the primed terms are obtained by
replacing free x&apos;s with what was applied to
the Ax. in the deduction of (B)A
With these rules, we get the analogue of The-
orem 1 for the conversion rule. In doing the
skeleton deduction we don&apos;t worry about the
(B)A constructor, but we introduce constraints
on modifier positioning that require that a hy-
pothetical dependency can&apos;t be satisfied by a
deduction that uses only part of the resource it
requires.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="conclusions">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999922">
We would like to thank Mary Dalrymple, John
Fry, Stephan Kauffmann, and Hadar Shemtov
for discussions of these ideas and for comments
on this paper.
</bodyText>
<sectionHeader confidence="0.989338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979338333333333">
Richard Crouch and Josef van Genabith. 1997.
How to glue a donkey to an f-structure, or
porting a dynamic meaning representation
into LFG&apos;s linear logic based glue-language
semantics. Paper to be presented at the Sec-
ond International Workshop on Computa-
tional Semantics, Tilburg, The Netherlands,
January 1997.
Mary Dalrymple, John Lamping, and Vijay
Saraswat. 1993. LFG semantics via con-
straints. In Proceedings of the Sixth Meeting
of the European ACL, pages 97-105, Univer-
sity of Utrecht. European Chapter of the As-
sociation for Computational Linguistics.
Mary Dalrymple, John Lamping, Fernando
C. N. Pereira, and Vijay Saraswat. 1995. Lin-
ear logic for meaning assembly. In Proceed-
ings of CLNLP, Edinburgh.
Mary Dalrymple, John Lamping, Fernando
C. N. Pereira, and Vijay Saraswat. 1996. In-
tensional verbs without type-raising or lexical
ambiguity. In Jerry Seligman and Dag West-
erstahl, editors, Logic, Language and Com-
putation, pages 167-182. CSLI Publications,
Stanford University.
Mary Dalrymple, Vineet Gupta, John Lamp-
ing, and Vijay Saraswat. 1997a. Relating
resource-based semantics to categorial seman-
tics. In Proceedings of the Fifth Meeting on
Mathematics of Language (MOL5), Schloss
Dagstuhl, Saarbriicken, Germany.
Mary Dalrymple, John Lamping, Fernando
C. N. Pereira, and Vijay Saraswat. 1997b.
Quantifiers, anaphora, and intensionality.
Journal of Logic, Language, and Information,
6(3):219-273.
Mark Hepple. 1996. A compilation-chart
method for linear categorical deduction. In
Proceedings of COLING-96, Copenhagen.
Max I. Kanovich. 1992. Horn programming in
linear logic is NP-complete. In Seventh An-
nual IEEE Symposium on Logic in Computer
Science, pages 200-210, Los Alamitos, Cali-
fornia. IEEE Computer Society Press.
Uwe Reyle. 1993. Dealing with ambiguities by
underspecification: Construction, representa-
tion, and deduction. Journal of Semantics,
10:123-179.
</reference>
<page confidence="0.998203">
470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.108128">
<title confidence="0.997708">Efficient Linear Logic Meaning Assembly</title>
<author confidence="0.997429">Vineet Gupta</author>
<affiliation confidence="0.999087">Caelum Research Corporation NASA Ames Research Center</affiliation>
<address confidence="0.999665">Moffett Field CA 94035</address>
<email confidence="0.968847">vguptaOptolemy.arc.nasa.gov</email>
<author confidence="0.993342">John Lamping</author>
<affiliation confidence="0.964203">Xerox PARC</affiliation>
<address confidence="0.996704">3333 Coyote Hill Road Palo Alto CA 94304 USA</address>
<email confidence="0.999661">lamping@parc.xerox.com</email>
<title confidence="0.6075765">apos;LEAVE&apos; [PRED &apos;CAT&apos;</title>
<author confidence="0.35627">SPEC &apos;EVERY&apos;</author>
<affiliation confidence="0.262449">apos;GRAY&apos;</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Crouch</author>
<author>Josef van Genabith</author>
</authors>
<title>How to glue a donkey to an f-structure, or porting a dynamic meaning representation into LFG&apos;s linear logic based glue-language semantics.</title>
<date>1997</date>
<booktitle>the Second International Workshop on Computational Semantics,</booktitle>
<location>Tilburg, The Netherlands,</location>
<note>Paper to be presented at</note>
<marker>Crouch, van Genabith, 1997</marker>
<rawString>Richard Crouch and Josef van Genabith. 1997. How to glue a donkey to an f-structure, or porting a dynamic meaning representation into LFG&apos;s linear logic based glue-language semantics. Paper to be presented at the Second International Workshop on Computational Semantics, Tilburg, The Netherlands, January 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>John Lamping</author>
<author>Vijay Saraswat</author>
</authors>
<title>LFG semantics via constraints.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Meeting of the European ACL,</booktitle>
<pages>97--105</pages>
<institution>University of Utrecht. European Chapter of the Association for Computational Linguistics.</institution>
<marker>Dalrymple, Lamping, Saraswat, 1993</marker>
<rawString>Mary Dalrymple, John Lamping, and Vijay Saraswat. 1993. LFG semantics via constraints. In Proceedings of the Sixth Meeting of the European ACL, pages 97-105, University of Utrecht. European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>John Lamping</author>
<author>Fernando C N Pereira</author>
<author>Vijay Saraswat</author>
</authors>
<title>Linear logic for meaning assembly.</title>
<date>1995</date>
<booktitle>In Proceedings of CLNLP,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="1092" citStr="Dalrymple et al., 1995" startWordPosition="162" endWordPosition="166">tic analyses (Dalrymple et al., 1993). It has been computationally feasible in practice (Dalrymple et al., 1997b). Yet deduction in linear logic is known to be intractable. Even the propositional tensor fragment is NP complete(Kanovich, 1992). In this paper, we investigate what has made the glue approach computationally feasible and show how to exploit that to efficiently deduce underspecified representations. In the next section, we identify a restricted pattern of use of linear logic in the glue analyses we are aware of, including those in (Crouch and Genabith, 1997; Dalrymple et al., 1996; Dalrymple et al., 1995). And we show why that fragment is computationally feasible. In other words, while the glue approach could be used to express computationally intractable analyses, actual analyses have adhered to a pattern of use of linear logic that is tractable. The rest of the paper shows how this pattern of use can be exploited to efficiently capture all possible deductions. We present a conservative extension of linear logic that allows a reformulation of the semantic contributions to better exploit this pattern, almost turning them into Horn clauses. We present a deduction algorithm for this formulation </context>
</contexts>
<marker>Dalrymple, Lamping, Pereira, Saraswat, 1995</marker>
<rawString>Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1995. Linear logic for meaning assembly. In Proceedings of CLNLP, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>John Lamping</author>
<author>Fernando C N Pereira</author>
<author>Vijay Saraswat</author>
</authors>
<title>Intensional verbs without type-raising or lexical ambiguity.</title>
<date>1996</date>
<booktitle>Logic, Language and Computation,</booktitle>
<pages>167--182</pages>
<editor>In Jerry Seligman and Dag Westerstahl, editors,</editor>
<publisher>CSLI Publications, Stanford University.</publisher>
<contexts>
<context position="1067" citStr="Dalrymple et al., 1996" startWordPosition="158" endWordPosition="161">ble meanings from syntactic analyses (Dalrymple et al., 1993). It has been computationally feasible in practice (Dalrymple et al., 1997b). Yet deduction in linear logic is known to be intractable. Even the propositional tensor fragment is NP complete(Kanovich, 1992). In this paper, we investigate what has made the glue approach computationally feasible and show how to exploit that to efficiently deduce underspecified representations. In the next section, we identify a restricted pattern of use of linear logic in the glue analyses we are aware of, including those in (Crouch and Genabith, 1997; Dalrymple et al., 1996; Dalrymple et al., 1995). And we show why that fragment is computationally feasible. In other words, while the glue approach could be used to express computationally intractable analyses, actual analyses have adhered to a pattern of use of linear logic that is tractable. The rest of the paper shows how this pattern of use can be exploited to efficiently capture all possible deductions. We present a conservative extension of linear logic that allows a reformulation of the semantic contributions to better exploit this pattern, almost turning them into Horn clauses. We present a deduction algori</context>
<context position="11759" citStr="Dalrymple et al., 1996" startWordPosition="2073" endWordPosition="2076">he combinatorial combination of different modifier orders. We use the rule only to segregate skeleton atoms from modifier atoms. Since we want modifiers to end up looking like the identity axiom, we leave them in the A –o A form, even if A contains further implications. For example, we would not apply the nested hypothetical rule to simplify the entry for gray any further, since it is already in the form A –o A. Handling intensional verbs requires a more precise definition of skeleton and modifier. The type part of an intensional verb contribution looks like (V F.(h, –o F) –o F) –o g, –o fo. (Dalrymple et al., 1996). First, we have to deal with the small technical problem that the VF gets in the way of the nested hypothetical translation rule. This is easily resolved by introducing a skolem constant, S, turning the type into ((he –o 5) –o S) –0 g, –o f. Now, the nested hypothetical rule can be applied to yield (h, –o S) and S{S{h4} –o g,, –o f But now we have the interesting question of whether the occurrences of the skolem constant, S, are skeleton or modifier. If we observe how S resources get produced and consumed in a deduction involving the intensional verb, we find that (h, –o S) produces an S, whi</context>
</contexts>
<marker>Dalrymple, Lamping, Pereira, Saraswat, 1996</marker>
<rawString>Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1996. Intensional verbs without type-raising or lexical ambiguity. In Jerry Seligman and Dag Westerstahl, editors, Logic, Language and Computation, pages 167-182. CSLI Publications, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>Vineet Gupta</author>
<author>John Lamping</author>
<author>Vijay Saraswat</author>
</authors>
<title>Relating resource-based semantics to categorial semantics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Meeting on Mathematics of Language (MOL5), Schloss Dagstuhl,</booktitle>
<location>Saarbriicken, Germany.</location>
<contexts>
<context position="2358" citStr="Dalrymple et al., 1997" startWordPosition="372" endWordPosition="375">sible deductions. And finally, we show how that description of deductions can be turned into a compact underspecified description of the possible meanings. Throughout the paper we will use the illustrative sentence &amp;quot;every gray cat left&amp;quot;. It has functional structure and semantic contributions leave :Vx. –o f,,leave(x) cat :Vs. (g, –o (g, RESTR)-.* ca(x) gray :VP. [Vx. (g, VAR)---*x —o (g, RESTR)--.P (x)] —o Ns. (g, VAR)-+x —o (ga RESTR).-s--gray(P)(x)] every :V H , R, S. [Vx.(g(7 VAR)&apos;-+x —0 Oa RESTR).---+R(x)] 0[Vx. –o H,S(s)] –o H,every(R,S) For our purposes, it is more convenient to follow (Dalrymple et al., 1997a) and separate the two parts of the semantic contributions: use a lambda term to capture the meaning formulas, and a type to capture the connections to the f-structure. In this form, the contributions are leave: Ax.leave(x): g„ –o f, cat : Ax.cai(x): (g(7 VAR) —o (g„ RESTR) gray: AP.Ax.gray(P)(x): ((g(7 VAR) —o (g(7 RESTR)) —o VAR) —0 (g, RESTR) every: AR.AS.every(R,S) : VH. (((g(7 VAR) —o (gc, RESTR)) 0(g, –0 H)) –o H With this separation, the possible derivations are determined solely by the &amp;quot;types&amp;quot;, the connections to the f-structure. The meaning is assembled by applying the lambda terms i</context>
</contexts>
<marker>Dalrymple, Gupta, Lamping, Saraswat, 1997</marker>
<rawString>Mary Dalrymple, Vineet Gupta, John Lamping, and Vijay Saraswat. 1997a. Relating resource-based semantics to categorial semantics. In Proceedings of the Fifth Meeting on Mathematics of Language (MOL5), Schloss Dagstuhl, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>John Lamping</author>
<author>Fernando C N Pereira</author>
<author>Vijay Saraswat</author>
</authors>
<title>Quantifiers, anaphora, and intensionality.</title>
<date>1997</date>
<journal>Journal of Logic, Language, and Information,</journal>
<pages>6--3</pages>
<contexts>
<context position="2358" citStr="Dalrymple et al., 1997" startWordPosition="372" endWordPosition="375">sible deductions. And finally, we show how that description of deductions can be turned into a compact underspecified description of the possible meanings. Throughout the paper we will use the illustrative sentence &amp;quot;every gray cat left&amp;quot;. It has functional structure and semantic contributions leave :Vx. –o f,,leave(x) cat :Vs. (g, –o (g, RESTR)-.* ca(x) gray :VP. [Vx. (g, VAR)---*x —o (g, RESTR)--.P (x)] —o Ns. (g, VAR)-+x —o (ga RESTR).-s--gray(P)(x)] every :V H , R, S. [Vx.(g(7 VAR)&apos;-+x —0 Oa RESTR).---+R(x)] 0[Vx. –o H,S(s)] –o H,every(R,S) For our purposes, it is more convenient to follow (Dalrymple et al., 1997a) and separate the two parts of the semantic contributions: use a lambda term to capture the meaning formulas, and a type to capture the connections to the f-structure. In this form, the contributions are leave: Ax.leave(x): g„ –o f, cat : Ax.cai(x): (g(7 VAR) —o (g„ RESTR) gray: AP.Ax.gray(P)(x): ((g(7 VAR) —o (g(7 RESTR)) —o VAR) —0 (g, RESTR) every: AR.AS.every(R,S) : VH. (((g(7 VAR) —o (gc, RESTR)) 0(g, –0 H)) –o H With this separation, the possible derivations are determined solely by the &amp;quot;types&amp;quot;, the connections to the f-structure. The meaning is assembled by applying the lambda terms i</context>
</contexts>
<marker>Dalrymple, Lamping, Pereira, Saraswat, 1997</marker>
<rawString>Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1997b. Quantifiers, anaphora, and intensionality. Journal of Logic, Language, and Information, 6(3):219-273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hepple</author>
</authors>
<title>A compilation-chart method for linear categorical deduction.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="8866" citStr="Hepple, 1996" startWordPosition="1537" endWordPosition="1539">for example, has skeleton contributions for ga, (gcr VAR), and (g, RESTR), but modifier contributions for H. Furthermore, the nested implication structure allows no nice way to disentangle the two kinds of occurrences. When a deduction interacts with the skeletal g, in the hypothetical it also brings in the modifier H. If the problematic hypothetical could be converted to Horn clauses, then we could get a better separation of the two types of occurrences. We can approximate this by going to an indexed linear logic, a conservative extension of the system of Figure 1, similar to Hepple&apos;s system(Hepple, 1996). To handle nested implications, we introduce the type constructor A{B}, which indicates an A whose derivation made use of B. This is similar to Hepple&apos;s use of indices, except that we indicate dependence on types, rather than on indices. This is sufficient in our application, since each such type has a unique positive skeletal occurrence. We can eliminate problematic nested implications by translating them into this construct, in accordance with the following rule: For a nested hypothetical at top level that has a mix of skeleton and modifier types: M : (A —o B) --0 C replace it with x : A, i</context>
</contexts>
<marker>Hepple, 1996</marker>
<rawString>Mark Hepple. 1996. A compilation-chart method for linear categorical deduction. In Proceedings of COLING-96, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max I Kanovich</author>
</authors>
<title>Horn programming in linear logic is NP-complete.</title>
<date>1992</date>
<booktitle>In Seventh Annual IEEE Symposium on Logic in Computer Science,</booktitle>
<pages>200--210</pages>
<publisher>IEEE Computer Society Press.</publisher>
<location>Los Alamitos, California.</location>
<contexts>
<context position="711" citStr="Kanovich, 1992" startWordPosition="103" endWordPosition="104">rch Center Moffett Field CA 94035 vguptaOptolemy.arc.nasa.gov John Lamping Xerox PARC 3333 Coyote Hill Road Palo Alto CA 94304 USA lamping@parc.xerox.com &apos;LEAVE&apos; [PRED &apos;CAT&apos; g.. SPEC &apos;EVERY&apos; 1 MODS { [ PRED &apos;GRAY&apos;]} (1) -FRED f: SUBJ 1 Introduction The &amp;quot;glue&amp;quot; approach to semantic composition in Lexical-Functional Grammar uses linear logic to assemble meanings from syntactic analyses (Dalrymple et al., 1993). It has been computationally feasible in practice (Dalrymple et al., 1997b). Yet deduction in linear logic is known to be intractable. Even the propositional tensor fragment is NP complete(Kanovich, 1992). In this paper, we investigate what has made the glue approach computationally feasible and show how to exploit that to efficiently deduce underspecified representations. In the next section, we identify a restricted pattern of use of linear logic in the glue analyses we are aware of, including those in (Crouch and Genabith, 1997; Dalrymple et al., 1996; Dalrymple et al., 1995). And we show why that fragment is computationally feasible. In other words, while the glue approach could be used to express computationally intractable analyses, actual analyses have adhered to a pattern of use of lin</context>
<context position="6228" citStr="Kanovich, 1992" startWordPosition="1056" endWordPosition="1057">Here are the contributions for the example sentence again, with the occurrences classified. Each occurrence is marked positive or negative, and the skeleton occurrences are underlined. leave: g -o f,+ cat : (ga VAR)- -0 (ga RESTR)Igray : ((ga vAR)+ -0 (ga REsTR)-) -0 (ga VAR)- -0 (ga RESTR)F every: VH. (((ga vAR)+ -0 (ga REsTR)- ) 0(g,+ -o 1-1-)) H+ This pattern explains the empirical tractability of glue inference. In the general case of multiplicative linear logic, there can be complex combinatorics in matching up positive and negative occurrences of literals, which leads to NPcompleteness (Kanovich, 1992). But in the glue fragment, on the other hand, the only combinatorial question is the relative ordering of modifiers. In the common case, each of those orderings is legal and gives rise to a different meaning. So the combinatorics of inference tends to be proportional to the degree of semantic ambiguity. The complexity per possible reading is thus roughly linear in the size of the utterance. But, this simple combinatoric structure suggests a better way to exploit the pattern. Rather than have inference explore all the cornbinatorics of different modifier orders, we can get a single underspecif</context>
</contexts>
<marker>Kanovich, 1992</marker>
<rawString>Max I. Kanovich. 1992. Horn programming in linear logic is NP-complete. In Seventh Annual IEEE Symposium on Logic in Computer Science, pages 200-210, Los Alamitos, California. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>Dealing with ambiguities by underspecification: Construction, representation, and deduction.</title>
<date>1993</date>
<journal>Journal of Semantics,</journal>
<pages>10--123</pages>
<contexts>
<context position="17648" citStr="Reyle, 1993" startWordPosition="3083" endWordPosition="3084">at tree nodes a modifier must dominate. Returning to our original example, the skeleton deductions yield the following three trees: H vAR)ever —o Hg ga RESTR) / \ y I fa , Ax. , VAR.) --0 kgc, REsTR)I ge, RESTR) leave gray 1g° (g VAR)cLat VAR) —o gc, RESTR) leave(y) AS.every(Ax.cat(x), S) AP.Ax.gray(P)(x) Notice that higher order arguments are reflected as structured types, like (g, VAR) -0 (g, RESTR). These trees are a compact description of the possible meanings, in this case the one possible meaning. We believe it will be possible to translate this representation into a UDRS representation(Reyle, 1993), or other similar representations for ambiguous sentences. We can also use the trees directly as an underspecified representation. To read out a particular meaning, we just interpolate modifiers into the arcs they modify. Dependencies on a 468 The functional structure of &amp;quot;Someone likes every cat&amp;quot;. FRED &apos;LIKE&apos; SUBJ h:[ FRED &apos;SOMEONE&apos; f: OBJ g:[PRED &apos;CAT&apos; SPEC &apos;EVERY&apos; The lexical entries after conversion to indexed form: like: cat : someonei : someone2 : everyi : every2 : every3 : Ax.Ay.like(x,y): (h, gc) -o f, Ax.cat(x): (g, VAR) -o (g, RESTR) z : h, AS.some(person, S) :VH. H {h,} -o H AR.AS.e</context>
</contexts>
<marker>Reyle, 1993</marker>
<rawString>Uwe Reyle. 1993. Dealing with ambiguities by underspecification: Construction, representation, and deduction. Journal of Semantics, 10:123-179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>