<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008051">
<title confidence="0.984618">
Comparing Corpora using Frequency Profiling
</title>
<author confidence="0.7673535">
Paul RANSON Roger GARSIDE
Computing Department, Computing Department,
</author>
<affiliation confidence="0.999467">
Lancaster University Lancaster University
</affiliation>
<address confidence="0.776344">
Lancaster, UK, Lancaster, UK,
</address>
<email confidence="0.993264">
paul@complancs.ac.uk rgg@complancs.ac.uk
</email>
<sectionHeader confidence="0.993736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965642857143">
This paper describes a method of comparing
corpora which uses frequency profiling. The
method can be used to discover key words in
the corpora which differentiate one corpus
from another. Using annotated corpora, it
can be applied to discover key grammatical
or word-sense categories. This can be used
as a quick way in to find the differences
between the corpora and is shown to have
applications in the study of social
differentiation in the use of English
vocabulary, profiling of learner English and
document analysis in the software
engineering process.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993953451612903">
Corpus-based techniques have increasingly been
used to compare language usage in recent years.
One of the largest early studies was the
comparison of one million words of American
English (the Brown corpus) with one million
words of British English (the LOB corpus) by
Hofland and Johansson (1982). A difference
coefficient defined by Yule (1944) showed the
relative frequency of a word in the two corpora.
A statistical goodness-of-fit test, the Chi-squared
test, was also used to compare word frequencies
across the two corpora. They noted any resulting
chi-squared values which indicated that a
statistically significant difference at the 5%, 1%,
or 0.1% level had been detected between the
frequency of a word in American English and in
British English. The null hypothesis of the test is
that there is no difference between the observed
frequencies.
More recently, this size of corpus comparison
is becoming the standard even for postgraduate
studies with the increasing availability of
corpora and reasoning that one million words
gives sufficient evidence for higher frequency
words. However, with the production of large
corpora such as the British National Corpus
(BNC) containing one hundred million words
(Aston &amp; Burnard, 1998), frequency
comparisons are available across millions of
words of text. There are two main types of
corpus comparison:
</bodyText>
<listItem confidence="0.9906885">
• comparison of a sample corpus to a large(r)
corpus
• comparison of two (roughly-) equal sized
corpora
</listItem>
<bodyText confidence="0.997954769230769">
In the first type, we refer to the large(r) corpus
as a horrnative&apos; corpus since it provides a text
norm (or standard) against which we can
compare. These two main types of comparison
can be extended to the comparison of more than
two corpora. For example, we may compare one
normative corpus to several smaller corpora at
the same time, or compare three or more equal
sized corpora to each other. In general, however,
this makes the results more difficult to interpret.
There are also a number of issues which need
to be considered when comparing two (or more)
corpora:
</bodyText>
<listItem confidence="0.9978432">
• representativeness
• homogeneity within the corpora
• comparability of the corpora
• reliability of statistical tests (for different sized
corpora and other factors)
</listItem>
<bodyText confidence="0.9914984">
Representativeness (Biber, 1993) is a
particularly important attribute for a normative
corpus when comparing a sample corpus to a
large normative corpus (such as the BNC) which
contains sections from many different text types
</bodyText>
<page confidence="0.966273">
1
</page>
<bodyText confidence="0.999955525641026">
and domains. To be representative a corpus
should contain samples of all major text types
(Leech, 1993) and if possible in some way
proportional to their usage in bvery day
language&apos; (Clear, 1992). This first type of
comparison is intended to discover features in
the sample corpus with significantly different
usage (i.e. frequency) to that found in kenerar
language.
The second type of comparison is one that
views corpora as equals (as in the Brown and
LOB comparison). It aims to discover features in
the corpora that distinguish one from another.
Homogeneity within each of the corpora is
important here since we may find that the results
reflect sections within one of the corpora which
are unlike other sections in either of the corpora
under consideration (Kilgarriff 1997).
Comparability is of interest too, since the
corpora should have been sampled for in the
same way. In other words, the corpora should
have been built using the same stratified
sampling method and with, if possible,
randomised methods of sample selection. This is
the case with Brown and LOB, since LOB was
designed to be comparable to the Brown corpus.
The final issue, which has been addressed
elsewhere, is the one regarding the reliability of
the statistical tests in relation to the size of the
corpora under consideration. Kilgarriff (1996)
points out that in the Brown versus LOB
comparison many common words are marked as
having significant chi-squared values, and that
because words are not selected at random in
language we will always see a large number of
differences in two such text collections. He
selects the Mann-Whitney test that uses ranks of
frequency data rather than the frequency values
themselves to compute the statistic. However, he
observes that even with the new test 60% of
words are marked as significant Ignoring the
actual frequency of occurrence as in the Mann-
Whitney test discards most of the evidence we
have about the distribution of words. The test is
often used when comparing ordinal rating scales
(Oakes 1998: 17).
Dunning (1993) reports that we should not rely
on the assumption of a normal distribution when
performing statistical text analysis and suggests
that parametric analysis based on the binomial or
multinomial distributions is a better alternative
for smaller texts. The chi-squared value becomes
unreliable when the expected frequency is less
than 5 and possibly overestimates with high
frequency words and when comparing a
relatively small corpus to a much larger one. He
proposes the log-likelihood ratio as an
alternative to Pearson chi-squared test. For this
reason, we chose to use the log-likelihood ratio
in our work as described in the next section. In
fact, Cressie and Read (1984) show that
Pearson b X2 (chi-squared) and the likelihood
ratio 02 (Dunning log-likelihood) are two
statistics in a continuum defined by the power-
divergence family of statistics. They go on to
describe this family in later work (1988, 1989)
where they also make reference to the long and
continuing discussion of the normal and chi-
squared approximations for X2 and G2.
We have applied the goodness-of-fit test for
comparison of linguistically annotated corpora.
The frequency distributions of part-of-speech
and semantic tags are sharply different to words.
In these comparisons, we are unlikely to observe
rare events such as tags occurring once.
However, much higher frequencies will occur
and so the log-likelihood test is less likely to
overestimate significance in these cases.
</bodyText>
<sectionHeader confidence="0.98955" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.998075833333333">
The method is fairly simple and straightforward
to apply. Given two corpora we wish to
compare, we produce a frequency list for each
corpus. Normally, this would be a word
frequency list, but as described above and as
with examples in the following application
section, it can be a part-of-speech (POS) or
semantic tag frequency list. However, let us
assume for now that we are performing a
comparison at the word level&apos;. For each word in
the two frequency lists we calculate the log-
likelihood (henceforth LL) statistic. This is
performed by constructing a contingency table
as in Table 1.
The application of this technique to POS or
semantic tag frequency lists is achieved by
constructing the contingency table with tag rather
than word frequencies.
</bodyText>
<page confidence="0.997797">
2
</page>
<tableCaption confidence="0.999613">
Table 1 Contigency table for word frequencies
</tableCaption>
<table confidence="0.989917">
CORPUS CORPUS TOTAL
ONE TWO
Freq a b a+b
of word
Freq c-a d-b c+d-a-b
of other
words
,TOTAL c d c+d
</table>
<bodyText confidence="0.999739571428571">
Note that the value b&apos; corresponds to the
number of words in corpus one, and
corresponds to the number of words in corpus
two (N values). The values h.&apos; and b&apos; are called
the observed values (0). We need to calculate
the expected values (E) according to the
following formula:
</bodyText>
<equation confidence="0.73552">
—
2 z
</equation>
<bodyText confidence="0.983112542857143">
In our case Ni = c, and N2 = d. So, for this
word, El = c*(a+b) / (c+d) and E2 = d*(a+b) /
(c+d). The calculation for the expected values
takes account of the size of the two corpora, so
we do not need to normalise the figures before
applying the formula. We can then calculate the
log-likehood value according to this formula:
—2In A= 2Z0 14°1)
This equates to calculating LL as follows:
LL = 2*((a*log (a/E1)) + (b*log (b/E2)))
The word frequency list is then sorted by the
resulting LL values. This gives the effect of
placing the largest LL value at the top of the list
representing the word which has the most
significant relative frequency difference between
the two corpora. In this way, we can see the
words most indicative (or characteristic) of one
corpus, as compared to the other corpus, at the
top of the list. The words which appear with
roughly similar relative frequencies in the two
corpora appear lower down the list. Note that we
do not use the hypothesis-test by comparing the
LL values to a chi-squared distribution table. As
Kilgarriff &amp; Rose (1998) note, even Pearson
X2 is suitable without the hypothesis-testing
link! Given the non-random nature of words in
a text, we are always likely to find frequencies
of words which differ across any two texts, and
the higher the frequencies, the more information
the statistical test has to work with. Hence, it is
at this point that the researcher must intervene
and qualitatively examine examples of the
significant words highlighted by this technique.
We are not proposing a completely automated
approach.
</bodyText>
<sectionHeader confidence="0.997858" genericHeader="method">
3 Applications
</sectionHeader>
<bodyText confidence="0.999828578947369">
This method has already been applied to study
social differentiation in the use of English
vocabulary and profiling of learner English. In
Rayson et al (1997), selective quantitative
analyses of the demographically sampled spoken
English component of the BNC were carried out
This is a subcorpus of circa 4.5 million words, in
which speakers and respondents are identified
by such factors as gender, age, social group and
geographical region. Using the method, a
comparison was performed of the vocabulary of
speakers, highlighting those differences which
are marked by a very high value of significant
difference between different sectors of the
corpus according to gender, age and social
group.
In Granger and Rayson (1998), two similar-
sized corpora of native and non-native writing
were compared at the lexical level. The corpora
were analysed by a part-of-speech tagger, and
this permitted a comparison at the major word-
class level. The patterns of significant overuse
and underuse for POS categories demonstrated
that the learner data displayed many of the
stylistic features of spoken rather than written
English.
The same technique has more recently been
applied to compare corpora analysed at the
semantic level in a systems engineering domain
and this is the main focus of this section. The
motivation for this work is that despite natural
language&apos;s well-documented shortcomings as a
medium for precise technical description, its use
in software-intensive systems engineering
remains inescapable. This poses many problems
for engineers who must derive problem
understanding and synthesise precise solution
descriptions from free text. This is true both for
</bodyText>
<page confidence="0.993546">
3
</page>
<bodyText confidence="0.999673833333333">
the largely unstructured textual descriptions
from which system requirements are derived,
and for more formal documents, such as
standards, which impose requirements on system
development processes. We describe an
experiment that has been carried out in the
REVERE project (Rayson et al, 2000) to
investigate the use of probabilistic natural
language processing techniques to provide
systems engineering support.
The target documents are field reports of a
series of ethnographic studies at an air traffic
control (ATC) centre. This formed part of a
study of ATC as an example of a system that
supports collaborative user tasks (Bentley et al,
1992). The documents consist of both the
verbatim transcripts of the ethnographerS
observations and interviews with controllers,
and of reports compiled by the ethnographer for
later analysis by a multi-disciplinary team of
social scientists and systems engineers. The field
reports form an interesting study because they
exhibit many characteristics typical of
documents seen by a systems engineer. The
volume of the information is fairly high (103
pages) and the documents are not structured in a
way designed to help the extraction of
requirements (say around business processes or
system architecture).
The text is analysed by a part-of-speech tagger,
CLAWS (Garside and Smith, 1997), and a
semantic analyser (Rayson and Wilson, 1996)
which assigns semantic tags that represent the
semantic field (word-sense) of words from a
lexicon of single words and an idiom list of
multi-word combinations (e.g. as a rule). These
resources contain approximately 52,000 words
and idioms.
The normative corpus that we used was a 2.3
million-word subset of the BNC derived from
the transcripts of spoken English. Using this
corpus, the most over-represented semantic
categories in the ATC field reports are shown in
Table 2. The log-likelihood test is applied as
described in the previous section and represents
the semantic tag&apos;s frequency deviation from the
normative corpus. The higher the figure, the
greater the deviation.
</bodyText>
<tableCaption confidence="0.963637">
Table 2. Over-represented categories in ATC
field reports
</tableCaption>
<table confidence="0.902839133333333">
Log- Tag Word sense (examples
likelihood from the text)
3366 S7.1 power, organising
(bontroller; thief)
2578 M5 flying (Plane, flight;
airport)
988 02 general objects (Strip;
holder; tack)
643 03 electrical equipment
(iadnr; blip)
535 Y1 science and technology
(`PH)
449 W3 geographical terms
(Pole Hill; Dish Sea)
432 Q1.2 paper documents and
</table>
<equation confidence="0.8940233125">
writing (Writing;
&apos;written; hotes)
372 N3.7 measurement (length;
height; Uistance;
levels; 1000ft)
318 LI life and living things
(live)
310 A 1 0 indicating actions
(pointing; indicating;
Uisplay)
306 X4.2 mental objects
(systems&apos;, approach&apos;,
Mode; tactical;
procedure)
290 A4.1 kinds, groups (Sector;
Sectors)
</equation>
<bodyText confidence="0.999977823529412">
With the exception of Yl (an anomaly caused
by an interviewees initials being mistaken for
the PH unit of acidity), all of these semantic
categories include important objects, roles,
functions, etc. in the ATC domain. The
frequency with which some of these occur, such
as M5 (flying), are unsurprising. Others are
more revealing about the domain of ATC.
Figure 1 shows some of the occurrences of the
semantic category 02 (general objects). The
important information extracted here is the
importance of Strips&apos; (formally, flight strips).
These are small pieces of cardboard with printed
flight details that are the most fundamental
artefact used by the air traffic controllers to
manage their air space. Examination of other
words in this category also shows that flight
</bodyText>
<page confidence="0.995644">
4
</page>
<bodyText confidence="0.998438636363636">
to write &amp;quot; 1268L&apos;1 n red on a strip
he Isle of Nen ... Swot; This strip
toted by the beacon printed in to:
on printed in bot • B • of the strip
nrrival tine over thatteaccn ( bax
iviassly only approsimote- some strips
ot line near the callsigi on a strip
sea much busier . Theremere 16 strips
:revere 16 strips in one of his racks
, that talking and using on input
hat talking and using an input device
</bodyText>
<note confidence="0.712514">
: &amp;mat: the nice thine about strins
</note>
<figureCaption confidence="0.995767">
Figure 1. Browsing the
</figureCaption>
<listItem confidence="0.9994158">
• whilst at thence* time instru
was towards • the bottom of one
• 8 • of the strip ( second lef
( second left ) Strlasseemed hr
• A • ) This was ctviously only
</listItem>
<bodyText confidence="0.893065032258065">
were out oFposition , and I got
to indicate anununol speed .
in one of his rocks . alb. A ;
. ; • -.alb- alt;Tide Sat
device micpt also be , but that
might also be , but that the pr
is filet r flex ibi I tv . tant: a
semantic category 02
strips are held in tacks&apos; to organise them
according to (for example) aircraft tirne-of-
arrival.
Similarly, browsing the context for Q1.2
(paper documents and writing) would allow us
to discover that controllers annotate flight strips
to record deviations from flight plans, and LI
(life, living things) would reveal that some strips
are live, that is, they refer to aircraft currently
traversing the controller&apos;s sector. Notice also that
the semantic categories&apos; deviation from the
normative corpus can also be expected to reveal
domain roles (actors). In this example, the
frequency of S7.1 (power, organising) shows the
importance of the roles of Controllers&apos; and
Chiefs&apos;.
Using the frequency profiling method does not
automate the task of identifying abstractions,
much less does it produce fully formed
requirements that can be pasted into a
specification document. Instead, it helps the
engineer quickly isolate potentially significant
domain abstractions that require closer analysis.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999943451612903">
This paper has described a method of comparing
corpora which uses frequency profiling. The
method has been shown to discover key items in
the corpora which differentiate one corpus from
another. It has been applied at the word level,
part-of-speech tag level, and semantic tag level.
It can be used as a quick way in to find the
differences between the corpora and is shown to
have applications in the study of social
differentiation in the use of English vocabulary,
profiling of learner English and documeni
analysis in the software engineering process.
Future directions in which we aim to research
include a more precise specification of the
reliability of the statistical tests (LL, Pearson
X2 and others) under the effects of corpus size,
ratio of the corpora being compared and word
(or tag) frequency.
We do not propose a completely automated
approach. The tools suggest a group of key items
by decreasing order of significance which
distinguish one corpus from another. It is then
that the researcher should investigate
occurrences of the significant items in the
corpora using standard corpus techniques such
as KWIC (key-word in context). The reasons
behind their significance can be discovered and
explanations sought for the patterns displayed.
By this process, we can compare the corpora
under investigation and make hypotheses about
the language use that they represent.
</bodyText>
<sectionHeader confidence="0.994726" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999935142857143">
Our thanks go to Geoffrey Leech and the
anonymous reviewers who commented on
earlier versions of this paper. The REVERE
project is supported under the EPSRC Systems
Engineering for Business Process Change
(SEBPC) programme, project number
GR/M04846.
</bodyText>
<sectionHeader confidence="0.998254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99379425">
Aston, G. and Bumard, L. (1998). The BNC
Handbook: Exploring the British National Corpus
with SARA, Edinburgh University Press.
Bentley R., Rodden T., Sawyer P., Sommerville I,
Hughes J., Randall D., Shapiro D. (1992).
Ethnographically-informed systems design for air
traffic control, In Proceedings of Computer-
Supported Cooperative Work (CSCW) &apos;92,
Toronto, November 1992.
Biber, D. (1993). Representativeness in Corpus
Design. Literary and Linguistic Computing, 8,
Issue 4, Oxford University Press, pp. 243-257.
</reference>
<page confidence="0.956814">
5
</page>
<reference confidence="0.998602093333333">
Clear, J. (1992). Corpus sampling. In G. Leitner (ed.)
New directions in English language corpora.
Mouton-de-Gruyter, Berlin, pp. 21 - 31.
Cressie, N. and Read, T. R. C. (1984) Multinomial
Goodness-of-Fit Tests. Journal of the Royal
Statistical Society. Series B (Methodological), Vol.
46, No. 3, pp. 440- 464.
Cressie, N. and Read, T. R. C. (1989). Pearson.f X2
and the Loglikelihood Ratio Statistic G2: A
comparative review. International Statistical
Review, 57, 1, Belfast University Press, N.I., pp.
19-43.
Dunning, T. (1993). Accurate Methods for the
Statistics of Surprise and Coincidence.
Computational Linguistics, 19, 1, March 1993, pp.
61-74.
Garside, R. and Smith, N. (1997). A Hybrid
Grammatical Tagger: CLAWS4, in Garside, R.,
Leech, G., and McEnery, A. (eds.) Corpus
Annotation: Linguistic Information from Computer
Text Corpora, Longman, London.
Granger, S. and Rayson, P. (1998). Automatic
profiling of learner texts. In S. Granger (ed.)
Learner English on Computer. Longman, London
and New York, pp. 119-131.
Holland, K. and Johansson, S. (1982). Word
frequencies in British and American English. The
Norwegian Computing Centre for the Humanities,
Bergen, Norway.
Kilgarriff, A. (1996) Why chi-square doesn&apos;t work,
and an improved LOB-Brown comparison. ALLC-
ACH Conference, June 1996, Bergen, Norway.
Kilgarriff, A. (1997). Using word frequency lists to
measure corpus homogeneity and similarity
between corpora. Proceedings 5th ACL workshop
on very large corpora. Beijing and Hong Kong.
Kilgarriff, A. and Rose, T. (1998). Measures for
corpus similarity and homogeneity. In proceedings
of the 3rd conference on Empirical Methods in
Natural Language Processing, Granada, Spain, pp.
46 - 52.
Leech, G. (1993). 100 million words of English: a
description of the background, nature and
prospects of the British National Corpus project.
English Today 33, Vol. 9, No. 1, Cambridge
University Press.
Oakes, M. P. (1998). Statistics for Corpus
Linguistics. Edinburgh University Press,
Edinburgh.
Rayson, P., and Wilson, A. (1996). The ACAMRIT
semantic tagging system: progress report, In L. J.
Evett, and T. G. Rose (eds.) Language Engineering
for Document Analysis and Recognition, LEDAR,
AISB96 Workshop proceedings, pp 13-20.
Brighton, England.
Rayson, P., Leech, G., and Hodges, M. (1997). Social
differentiation in the use of English vocabulary:
some analyses of the conversational component of
the British National Corpus. International Journal
of Corpus Linguistics. 2 (1). pp. 133 - 152. John
Benjamins, Amsterdam/Philadelphia.
Rayson, P., Garside, R., and Sawyer, P. (2000).
Assisting requirements engineering with semantic
document analysis. In Proceedings of RIAO 2000
(Recherche d&apos;Inforrnations Assistie par Ordinateur,
Computer-Assisted Information Retrieval)
International Conference, College de France, Paris,
France, April 12-14, 2000. C.I.D., Paris, pp. 1363 -
1371.
Read, T. R. C. and Cressie, N. A. C. (1988).
Goodness-of-fit statistics for discrete multivariate
data. Springer series in statistics. Springer-Verlag,
New York.
Yule, G. (1944). The Statistical Study of Literary
Vocabulary. Cambridge University Press.
</reference>
<page confidence="0.998791">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894620">
<title confidence="0.999784">Comparing Corpora using Frequency Profiling</title>
<author confidence="0.995895">RANSON GARSIDE</author>
<affiliation confidence="0.999973">Computing Department, Computing Department, Lancaster University Lancaster University</affiliation>
<address confidence="0.94285">Lancaster, UK, Lancaster, UK,</address>
<email confidence="0.981845">paul@complancs.ac.ukrgg@complancs.ac.uk</email>
<abstract confidence="0.997946933333333">This paper describes a method of comparing corpora which uses frequency profiling. The method can be used to discover key words in the corpora which differentiate one corpus from another. Using annotated corpora, it can be applied to discover key grammatical or word-sense categories. This can be used as a quick way in to find the differences between the corpora and is shown to have applications in the study of social differentiation in the use of English vocabulary, profiling of learner English and document analysis in the software engineering process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aston</author>
<author>L Bumard</author>
</authors>
<date>1998</date>
<booktitle>The BNC Handbook: Exploring the British National Corpus with SARA,</booktitle>
<publisher>Edinburgh University Press.</publisher>
<marker>Aston, Bumard, 1998</marker>
<rawString>Aston, G. and Bumard, L. (1998). The BNC Handbook: Exploring the British National Corpus with SARA, Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bentley</author>
<author>T Rodden</author>
<author>P Sawyer</author>
<author>I Sommerville</author>
<author>J Hughes</author>
<author>D Randall</author>
<author>D Shapiro</author>
</authors>
<title>Ethnographically-informed systems design for air traffic control,</title>
<date>1992</date>
<booktitle>In Proceedings of ComputerSupported Cooperative Work (CSCW) &apos;92,</booktitle>
<location>Toronto,</location>
<contexts>
<context position="11850" citStr="Bentley et al, 1992" startWordPosition="1905" endWordPosition="1908">ual descriptions from which system requirements are derived, and for more formal documents, such as standards, which impose requirements on system development processes. We describe an experiment that has been carried out in the REVERE project (Rayson et al, 2000) to investigate the use of probabilistic natural language processing techniques to provide systems engineering support. The target documents are field reports of a series of ethnographic studies at an air traffic control (ATC) centre. This formed part of a study of ATC as an example of a system that supports collaborative user tasks (Bentley et al, 1992). The documents consist of both the verbatim transcripts of the ethnographerS observations and interviews with controllers, and of reports compiled by the ethnographer for later analysis by a multi-disciplinary team of social scientists and systems engineers. The field reports form an interesting study because they exhibit many characteristics typical of documents seen by a systems engineer. The volume of the information is fairly high (103 pages) and the documents are not structured in a way designed to help the extraction of requirements (say around business processes or system architecture)</context>
</contexts>
<marker>Bentley, Rodden, Sawyer, Sommerville, Hughes, Randall, Shapiro, 1992</marker>
<rawString>Bentley R., Rodden T., Sawyer P., Sommerville I, Hughes J., Randall D., Shapiro D. (1992). Ethnographically-informed systems design for air traffic control, In Proceedings of ComputerSupported Cooperative Work (CSCW) &apos;92, Toronto, November 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Biber</author>
</authors>
<date>1993</date>
<booktitle>Representativeness in Corpus Design. Literary and Linguistic Computing, 8, Issue 4,</booktitle>
<pages>243--257</pages>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="3043" citStr="Biber, 1993" startWordPosition="469" endWordPosition="470">main types of comparison can be extended to the comparison of more than two corpora. For example, we may compare one normative corpus to several smaller corpora at the same time, or compare three or more equal sized corpora to each other. In general, however, this makes the results more difficult to interpret. There are also a number of issues which need to be considered when comparing two (or more) corpora: • representativeness • homogeneity within the corpora • comparability of the corpora • reliability of statistical tests (for different sized corpora and other factors) Representativeness (Biber, 1993) is a particularly important attribute for a normative corpus when comparing a sample corpus to a large normative corpus (such as the BNC) which contains sections from many different text types 1 and domains. To be representative a corpus should contain samples of all major text types (Leech, 1993) and if possible in some way proportional to their usage in bvery day language&apos; (Clear, 1992). This first type of comparison is intended to discover features in the sample corpus with significantly different usage (i.e. frequency) to that found in kenerar language. The second type of comparison is on</context>
</contexts>
<marker>Biber, 1993</marker>
<rawString>Biber, D. (1993). Representativeness in Corpus Design. Literary and Linguistic Computing, 8, Issue 4, Oxford University Press, pp. 243-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clear</author>
</authors>
<title>Corpus sampling.</title>
<date>1992</date>
<booktitle>New directions in English language corpora. Mouton-de-Gruyter,</booktitle>
<pages>21--31</pages>
<editor>In G. Leitner (ed.)</editor>
<location>Berlin,</location>
<contexts>
<context position="3435" citStr="Clear, 1992" startWordPosition="534" endWordPosition="535"> (or more) corpora: • representativeness • homogeneity within the corpora • comparability of the corpora • reliability of statistical tests (for different sized corpora and other factors) Representativeness (Biber, 1993) is a particularly important attribute for a normative corpus when comparing a sample corpus to a large normative corpus (such as the BNC) which contains sections from many different text types 1 and domains. To be representative a corpus should contain samples of all major text types (Leech, 1993) and if possible in some way proportional to their usage in bvery day language&apos; (Clear, 1992). This first type of comparison is intended to discover features in the sample corpus with significantly different usage (i.e. frequency) to that found in kenerar language. The second type of comparison is one that views corpora as equals (as in the Brown and LOB comparison). It aims to discover features in the corpora that distinguish one from another. Homogeneity within each of the corpora is important here since we may find that the results reflect sections within one of the corpora which are unlike other sections in either of the corpora under consideration (Kilgarriff 1997). Comparability</context>
</contexts>
<marker>Clear, 1992</marker>
<rawString>Clear, J. (1992). Corpus sampling. In G. Leitner (ed.) New directions in English language corpora. Mouton-de-Gruyter, Berlin, pp. 21 - 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cressie</author>
<author>T R C Read</author>
</authors>
<title>Multinomial Goodness-of-Fit Tests.</title>
<date>1984</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>46</volume>
<pages>440--464</pages>
<contexts>
<context position="5963" citStr="Cressie and Read (1984)" startWordPosition="943" endWordPosition="946">of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts. The chi-squared value becomes unreliable when the expected frequency is less than 5 and possibly overestimates with high frequency words and when comparing a relatively small corpus to a much larger one. He proposes the log-likelihood ratio as an alternative to Pearson chi-squared test. For this reason, we chose to use the log-likelihood ratio in our work as described in the next section. In fact, Cressie and Read (1984) show that Pearson b X2 (chi-squared) and the likelihood ratio 02 (Dunning log-likelihood) are two statistics in a continuum defined by the powerdivergence family of statistics. They go on to describe this family in later work (1988, 1989) where they also make reference to the long and continuing discussion of the normal and chisquared approximations for X2 and G2. We have applied the goodness-of-fit test for comparison of linguistically annotated corpora. The frequency distributions of part-of-speech and semantic tags are sharply different to words. In these comparisons, we are unlikely to ob</context>
</contexts>
<marker>Cressie, Read, 1984</marker>
<rawString>Cressie, N. and Read, T. R. C. (1984) Multinomial Goodness-of-Fit Tests. Journal of the Royal Statistical Society. Series B (Methodological), Vol. 46, No. 3, pp. 440- 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cressie</author>
<author>T R C Read</author>
</authors>
<title>Pearson.f X2 and the Loglikelihood Ratio Statistic G2: A comparative review.</title>
<date>1989</date>
<journal>International Statistical Review,</journal>
<volume>57</volume>
<pages>pp.</pages>
<publisher>University Press, N.I.,</publisher>
<location>Belfast</location>
<marker>Cressie, Read, 1989</marker>
<rawString>Cressie, N. and Read, T. R. C. (1989). Pearson.f X2 and the Loglikelihood Ratio Statistic G2: A comparative review. International Statistical Review, 57, 1, Belfast University Press, N.I., pp. 19-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>61--74</pages>
<contexts>
<context position="5289" citStr="Dunning (1993)" startWordPosition="839" endWordPosition="840">uared values, and that because words are not selected at random in language we will always see a large number of differences in two such text collections. He selects the Mann-Whitney test that uses ranks of frequency data rather than the frequency values themselves to compute the statistic. However, he observes that even with the new test 60% of words are marked as significant Ignoring the actual frequency of occurrence as in the MannWhitney test discards most of the evidence we have about the distribution of words. The test is often used when comparing ordinal rating scales (Oakes 1998: 17). Dunning (1993) reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts. The chi-squared value becomes unreliable when the expected frequency is less than 5 and possibly overestimates with high frequency words and when comparing a relatively small corpus to a much larger one. He proposes the log-likelihood ratio as an alternative to Pearson chi-squared test. For this reason, we chose to use the log-likelihood ratio in o</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. (1993). Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19, 1, March 1993, pp. 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>N Smith</author>
</authors>
<title>A Hybrid Grammatical Tagger: CLAWS4,</title>
<date>1997</date>
<booktitle>Corpus Annotation: Linguistic Information from Computer Text Corpora,</booktitle>
<editor>in Garside, R., Leech, G., and McEnery, A. (eds.)</editor>
<location>Longman, London.</location>
<contexts>
<context position="12532" citStr="Garside and Smith, 1997" startWordPosition="2007" endWordPosition="2010">f the ethnographerS observations and interviews with controllers, and of reports compiled by the ethnographer for later analysis by a multi-disciplinary team of social scientists and systems engineers. The field reports form an interesting study because they exhibit many characteristics typical of documents seen by a systems engineer. The volume of the information is fairly high (103 pages) and the documents are not structured in a way designed to help the extraction of requirements (say around business processes or system architecture). The text is analysed by a part-of-speech tagger, CLAWS (Garside and Smith, 1997), and a semantic analyser (Rayson and Wilson, 1996) which assigns semantic tags that represent the semantic field (word-sense) of words from a lexicon of single words and an idiom list of multi-word combinations (e.g. as a rule). These resources contain approximately 52,000 words and idioms. The normative corpus that we used was a 2.3 million-word subset of the BNC derived from the transcripts of spoken English. Using this corpus, the most over-represented semantic categories in the ATC field reports are shown in Table 2. The log-likelihood test is applied as described in the previous section </context>
</contexts>
<marker>Garside, Smith, 1997</marker>
<rawString>Garside, R. and Smith, N. (1997). A Hybrid Grammatical Tagger: CLAWS4, in Garside, R., Leech, G., and McEnery, A. (eds.) Corpus Annotation: Linguistic Information from Computer Text Corpora, Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>P Rayson</author>
</authors>
<title>Automatic profiling of learner texts.</title>
<date>1998</date>
<booktitle>Learner English on Computer. Longman, London and</booktitle>
<pages>119--131</pages>
<editor>In S. Granger (ed.)</editor>
<location>New York,</location>
<contexts>
<context position="10257" citStr="Granger and Rayson (1998)" startWordPosition="1662" endWordPosition="1665">y and profiling of learner English. In Rayson et al (1997), selective quantitative analyses of the demographically sampled spoken English component of the BNC were carried out This is a subcorpus of circa 4.5 million words, in which speakers and respondents are identified by such factors as gender, age, social group and geographical region. Using the method, a comparison was performed of the vocabulary of speakers, highlighting those differences which are marked by a very high value of significant difference between different sectors of the corpus according to gender, age and social group. In Granger and Rayson (1998), two similarsized corpora of native and non-native writing were compared at the lexical level. The corpora were analysed by a part-of-speech tagger, and this permitted a comparison at the major wordclass level. The patterns of significant overuse and underuse for POS categories demonstrated that the learner data displayed many of the stylistic features of spoken rather than written English. The same technique has more recently been applied to compare corpora analysed at the semantic level in a systems engineering domain and this is the main focus of this section. The motivation for this work </context>
</contexts>
<marker>Granger, Rayson, 1998</marker>
<rawString>Granger, S. and Rayson, P. (1998). Automatic profiling of learner texts. In S. Granger (ed.) Learner English on Computer. Longman, London and New York, pp. 119-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Holland</author>
<author>S Johansson</author>
</authors>
<title>Word frequencies in British and American English. The Norwegian Computing Centre for the Humanities,</title>
<date>1982</date>
<location>Bergen, Norway.</location>
<marker>Holland, Johansson, 1982</marker>
<rawString>Holland, K. and Johansson, S. (1982). Word frequencies in British and American English. The Norwegian Computing Centre for the Humanities, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Why chi-square doesn&apos;t work, and an improved LOB-Brown comparison.</title>
<date>1996</date>
<booktitle>ALLCACH Conference,</booktitle>
<location>Bergen, Norway.</location>
<contexts>
<context position="4566" citStr="Kilgarriff (1996)" startWordPosition="719" endWordPosition="720">sections in either of the corpora under consideration (Kilgarriff 1997). Comparability is of interest too, since the corpora should have been sampled for in the same way. In other words, the corpora should have been built using the same stratified sampling method and with, if possible, randomised methods of sample selection. This is the case with Brown and LOB, since LOB was designed to be comparable to the Brown corpus. The final issue, which has been addressed elsewhere, is the one regarding the reliability of the statistical tests in relation to the size of the corpora under consideration. Kilgarriff (1996) points out that in the Brown versus LOB comparison many common words are marked as having significant chi-squared values, and that because words are not selected at random in language we will always see a large number of differences in two such text collections. He selects the Mann-Whitney test that uses ranks of frequency data rather than the frequency values themselves to compute the statistic. However, he observes that even with the new test 60% of words are marked as significant Ignoring the actual frequency of occurrence as in the MannWhitney test discards most of the evidence we have ab</context>
</contexts>
<marker>Kilgarriff, 1996</marker>
<rawString>Kilgarriff, A. (1996) Why chi-square doesn&apos;t work, and an improved LOB-Brown comparison. ALLCACH Conference, June 1996, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Using word frequency lists to measure corpus homogeneity and similarity between corpora.</title>
<date>1997</date>
<booktitle>Proceedings 5th ACL workshop on very large corpora. Beijing</booktitle>
<contexts>
<context position="4020" citStr="Kilgarriff 1997" startWordPosition="629" endWordPosition="630">ery day language&apos; (Clear, 1992). This first type of comparison is intended to discover features in the sample corpus with significantly different usage (i.e. frequency) to that found in kenerar language. The second type of comparison is one that views corpora as equals (as in the Brown and LOB comparison). It aims to discover features in the corpora that distinguish one from another. Homogeneity within each of the corpora is important here since we may find that the results reflect sections within one of the corpora which are unlike other sections in either of the corpora under consideration (Kilgarriff 1997). Comparability is of interest too, since the corpora should have been sampled for in the same way. In other words, the corpora should have been built using the same stratified sampling method and with, if possible, randomised methods of sample selection. This is the case with Brown and LOB, since LOB was designed to be comparable to the Brown corpus. The final issue, which has been addressed elsewhere, is the one regarding the reliability of the statistical tests in relation to the size of the corpora under consideration. Kilgarriff (1996) points out that in the Brown versus LOB comparison ma</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Kilgarriff, A. (1997). Using word frequency lists to measure corpus homogeneity and similarity between corpora. Proceedings 5th ACL workshop on very large corpora. Beijing and Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>T Rose</author>
</authors>
<title>Measures for corpus similarity and homogeneity.</title>
<date>1998</date>
<booktitle>In proceedings of the 3rd conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>46--52</pages>
<location>Granada,</location>
<contexts>
<context position="9013" citStr="Kilgarriff &amp; Rose (1998)" startWordPosition="1467" endWordPosition="1470">y list is then sorted by the resulting LL values. This gives the effect of placing the largest LL value at the top of the list representing the word which has the most significant relative frequency difference between the two corpora. In this way, we can see the words most indicative (or characteristic) of one corpus, as compared to the other corpus, at the top of the list. The words which appear with roughly similar relative frequencies in the two corpora appear lower down the list. Note that we do not use the hypothesis-test by comparing the LL values to a chi-squared distribution table. As Kilgarriff &amp; Rose (1998) note, even Pearson X2 is suitable without the hypothesis-testing link! Given the non-random nature of words in a text, we are always likely to find frequencies of words which differ across any two texts, and the higher the frequencies, the more information the statistical test has to work with. Hence, it is at this point that the researcher must intervene and qualitatively examine examples of the significant words highlighted by this technique. We are not proposing a completely automated approach. 3 Applications This method has already been applied to study social differentiation in the use o</context>
</contexts>
<marker>Kilgarriff, Rose, 1998</marker>
<rawString>Kilgarriff, A. and Rose, T. (1998). Measures for corpus similarity and homogeneity. In proceedings of the 3rd conference on Empirical Methods in Natural Language Processing, Granada, Spain, pp. 46 - 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English: a description of the background, nature and prospects of the British National Corpus project.</title>
<date>1993</date>
<journal>English Today</journal>
<volume>33</volume>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3342" citStr="Leech, 1993" startWordPosition="518" endWordPosition="519">o interpret. There are also a number of issues which need to be considered when comparing two (or more) corpora: • representativeness • homogeneity within the corpora • comparability of the corpora • reliability of statistical tests (for different sized corpora and other factors) Representativeness (Biber, 1993) is a particularly important attribute for a normative corpus when comparing a sample corpus to a large normative corpus (such as the BNC) which contains sections from many different text types 1 and domains. To be representative a corpus should contain samples of all major text types (Leech, 1993) and if possible in some way proportional to their usage in bvery day language&apos; (Clear, 1992). This first type of comparison is intended to discover features in the sample corpus with significantly different usage (i.e. frequency) to that found in kenerar language. The second type of comparison is one that views corpora as equals (as in the Brown and LOB comparison). It aims to discover features in the corpora that distinguish one from another. Homogeneity within each of the corpora is important here since we may find that the results reflect sections within one of the corpora which are unlike</context>
</contexts>
<marker>Leech, 1993</marker>
<rawString>Leech, G. (1993). 100 million words of English: a description of the background, nature and prospects of the British National Corpus project. English Today 33, Vol. 9, No. 1, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Oakes</author>
</authors>
<title>Statistics for Corpus Linguistics.</title>
<date>1998</date>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh.</location>
<contexts>
<context position="5268" citStr="Oakes 1998" startWordPosition="836" endWordPosition="837">ignificant chi-squared values, and that because words are not selected at random in language we will always see a large number of differences in two such text collections. He selects the Mann-Whitney test that uses ranks of frequency data rather than the frequency values themselves to compute the statistic. However, he observes that even with the new test 60% of words are marked as significant Ignoring the actual frequency of occurrence as in the MannWhitney test discards most of the evidence we have about the distribution of words. The test is often used when comparing ordinal rating scales (Oakes 1998: 17). Dunning (1993) reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts. The chi-squared value becomes unreliable when the expected frequency is less than 5 and possibly overestimates with high frequency words and when comparing a relatively small corpus to a much larger one. He proposes the log-likelihood ratio as an alternative to Pearson chi-squared test. For this reason, we chose to use the log-</context>
</contexts>
<marker>Oakes, 1998</marker>
<rawString>Oakes, M. P. (1998). Statistics for Corpus Linguistics. Edinburgh University Press, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>A Wilson</author>
</authors>
<title>The ACAMRIT semantic tagging system: progress report, In</title>
<date>1996</date>
<booktitle>Language Engineering for Document Analysis and Recognition, LEDAR, AISB96 Workshop proceedings,</booktitle>
<pages>13--20</pages>
<editor>L. J. Evett, and T. G. Rose (eds.)</editor>
<location>Brighton, England.</location>
<contexts>
<context position="12583" citStr="Rayson and Wilson, 1996" startWordPosition="2015" endWordPosition="2018">h controllers, and of reports compiled by the ethnographer for later analysis by a multi-disciplinary team of social scientists and systems engineers. The field reports form an interesting study because they exhibit many characteristics typical of documents seen by a systems engineer. The volume of the information is fairly high (103 pages) and the documents are not structured in a way designed to help the extraction of requirements (say around business processes or system architecture). The text is analysed by a part-of-speech tagger, CLAWS (Garside and Smith, 1997), and a semantic analyser (Rayson and Wilson, 1996) which assigns semantic tags that represent the semantic field (word-sense) of words from a lexicon of single words and an idiom list of multi-word combinations (e.g. as a rule). These resources contain approximately 52,000 words and idioms. The normative corpus that we used was a 2.3 million-word subset of the BNC derived from the transcripts of spoken English. Using this corpus, the most over-represented semantic categories in the ATC field reports are shown in Table 2. The log-likelihood test is applied as described in the previous section and represents the semantic tag&apos;s frequency deviati</context>
</contexts>
<marker>Rayson, Wilson, 1996</marker>
<rawString>Rayson, P., and Wilson, A. (1996). The ACAMRIT semantic tagging system: progress report, In L. J. Evett, and T. G. Rose (eds.) Language Engineering for Document Analysis and Recognition, LEDAR, AISB96 Workshop proceedings, pp 13-20. Brighton, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>G Leech</author>
<author>M Hodges</author>
</authors>
<title>Social differentiation in the use of English vocabulary: some analyses of the conversational component of the British National Corpus.</title>
<date>1997</date>
<journal>International Journal of Corpus Linguistics.</journal>
<volume>2</volume>
<issue>1</issue>
<pages>133--152</pages>
<publisher>John Benjamins, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="9690" citStr="Rayson et al (1997)" startWordPosition="1575" endWordPosition="1578">testing link! Given the non-random nature of words in a text, we are always likely to find frequencies of words which differ across any two texts, and the higher the frequencies, the more information the statistical test has to work with. Hence, it is at this point that the researcher must intervene and qualitatively examine examples of the significant words highlighted by this technique. We are not proposing a completely automated approach. 3 Applications This method has already been applied to study social differentiation in the use of English vocabulary and profiling of learner English. In Rayson et al (1997), selective quantitative analyses of the demographically sampled spoken English component of the BNC were carried out This is a subcorpus of circa 4.5 million words, in which speakers and respondents are identified by such factors as gender, age, social group and geographical region. Using the method, a comparison was performed of the vocabulary of speakers, highlighting those differences which are marked by a very high value of significant difference between different sectors of the corpus according to gender, age and social group. In Granger and Rayson (1998), two similarsized corpora of nat</context>
</contexts>
<marker>Rayson, Leech, Hodges, 1997</marker>
<rawString>Rayson, P., Leech, G., and Hodges, M. (1997). Social differentiation in the use of English vocabulary: some analyses of the conversational component of the British National Corpus. International Journal of Corpus Linguistics. 2 (1). pp. 133 - 152. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>R Garside</author>
<author>P Sawyer</author>
</authors>
<title>Assisting requirements engineering with semantic document analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of RIAO 2000 (Recherche d&apos;Inforrnations Assistie par Ordinateur, Computer-Assisted Information Retrieval) International Conference, College de France,</booktitle>
<pages>1363--1371</pages>
<location>Paris, France,</location>
<contexts>
<context position="11494" citStr="Rayson et al, 2000" startWordPosition="1848" endWordPosition="1851">tural language&apos;s well-documented shortcomings as a medium for precise technical description, its use in software-intensive systems engineering remains inescapable. This poses many problems for engineers who must derive problem understanding and synthesise precise solution descriptions from free text. This is true both for 3 the largely unstructured textual descriptions from which system requirements are derived, and for more formal documents, such as standards, which impose requirements on system development processes. We describe an experiment that has been carried out in the REVERE project (Rayson et al, 2000) to investigate the use of probabilistic natural language processing techniques to provide systems engineering support. The target documents are field reports of a series of ethnographic studies at an air traffic control (ATC) centre. This formed part of a study of ATC as an example of a system that supports collaborative user tasks (Bentley et al, 1992). The documents consist of both the verbatim transcripts of the ethnographerS observations and interviews with controllers, and of reports compiled by the ethnographer for later analysis by a multi-disciplinary team of social scientists and sys</context>
</contexts>
<marker>Rayson, Garside, Sawyer, 2000</marker>
<rawString>Rayson, P., Garside, R., and Sawyer, P. (2000). Assisting requirements engineering with semantic document analysis. In Proceedings of RIAO 2000 (Recherche d&apos;Inforrnations Assistie par Ordinateur, Computer-Assisted Information Retrieval) International Conference, College de France, Paris, France, April 12-14, 2000. C.I.D., Paris, pp. 1363 -1371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R C Read</author>
<author>N A C Cressie</author>
</authors>
<title>Goodness-of-fit statistics for discrete multivariate data. Springer series in statistics.</title>
<date>1988</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<marker>Read, Cressie, 1988</marker>
<rawString>Read, T. R. C. and Cressie, N. A. C. (1988). Goodness-of-fit statistics for discrete multivariate data. Springer series in statistics. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Yule</author>
</authors>
<title>The Statistical Study of Literary Vocabulary.</title>
<date>1944</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1156" citStr="Yule (1944)" startWordPosition="168" endWordPosition="169">ick way in to find the differences between the corpora and is shown to have applications in the study of social differentiation in the use of English vocabulary, profiling of learner English and document analysis in the software engineering process. 1 Introduction Corpus-based techniques have increasingly been used to compare language usage in recent years. One of the largest early studies was the comparison of one million words of American English (the Brown corpus) with one million words of British English (the LOB corpus) by Hofland and Johansson (1982). A difference coefficient defined by Yule (1944) showed the relative frequency of a word in the two corpora. A statistical goodness-of-fit test, the Chi-squared test, was also used to compare word frequencies across the two corpora. They noted any resulting chi-squared values which indicated that a statistically significant difference at the 5%, 1%, or 0.1% level had been detected between the frequency of a word in American English and in British English. The null hypothesis of the test is that there is no difference between the observed frequencies. More recently, this size of corpus comparison is becoming the standard even for postgraduat</context>
</contexts>
<marker>Yule, 1944</marker>
<rawString>Yule, G. (1944). The Statistical Study of Literary Vocabulary. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>