<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001360">
<title confidence="0.974788">
Shedding (a Thousand Points of) Light on Biased Language
</title>
<author confidence="0.996203">
Tae Yano
</author>
<affiliation confidence="0.935025666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.991818">
taey@cs.cmu.edu
</email>
<author confidence="0.995633">
Philip Resnik
</author>
<affiliation confidence="0.9971075">
Department of Linguistics and UMIACS
University of Maryland
</affiliation>
<address confidence="0.849257">
College Park, MD 20742, USA
</address>
<email confidence="0.996355">
resnik@umiacs.umd.edu
</email>
<author confidence="0.997568">
Noah A. Smith
</author>
<affiliation confidence="0.930910333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.99846">
nasmith@cs.cmu.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868363636364">
This paper considers the linguistic indicators of bias
in political text. We used Amazon Mechanical Turk
judgments about sentences from American political
blogs, asking annotators to indicate whether a sen-
tence showed bias, and if so, in which political di-
rection and through which word tokens. We also
asked annotators questions about their own political
views. We conducted a preliminary analysis of the
data, exploring how different groups perceive bias in
different blogs, and showing some lexical indicators
strongly associated with perceived bias.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913823529412">
Bias and framing are central topics in the study of com-
munications, media, and political discourse (Scheufele,
1999; Entman, 2007), but they have received relatively
little attention in computational linguistics. What are the
linguistic indicators of bias? Are there lexical, syntactic,
topical, or other clues that can be computationally mod-
eled and automatically detected?
Here we use Amazon Mechanical Turk (MTurk) to en-
gage in a systematic, empirical study of linguistic indi-
cators of bias in the political domain, using text drawn
from political blogs. Using the MTurk framework, we
collected judgments connected with the two dominant
schools of thought in American politics, as exhibited in
single sentences. Since no one person can claim to be an
unbiased judge of political bias in language, MTurk is an
attractive framework that lets us measure perception of
bias across a population.
</bodyText>
<sectionHeader confidence="0.981533" genericHeader="method">
2 Annotation Task
</sectionHeader>
<bodyText confidence="0.998015">
We drew sentences from a corpus of American political
blog posts from 2008. (Details in Section 2.1.) Sentences
were presented to participants one at a time, without con-
text. Participants were asked to judge the following (see
Figure 1 for interface design):
</bodyText>
<listItem confidence="0.998853">
• To what extent a sentence or clause is biased (none,
somewhat, very);
• The nature of the bias (very liberal, moderately lib-
eral, moderately conservative, very conservative, bi-
ased but not sure which direction); and
• Which words in the sentence give away the author’s
bias, similar to “rationale” annotations in Zaidan et
al. (2007).
</listItem>
<bodyText confidence="0.99498852173913">
For example, a participant might identify a moderate
liberal bias in this sentence,
Without Sestak’s challenge, we would have
Specter, comfortably ensconced as a Democrat
in name only.
adding checkmarks on the underlined words. A more
neutral paraphrase is:
Without Sestak’s challenge, Specter would
have no incentive to side more frequently with
Democrats.
It is worth noting that “bias,” in the sense we are us-
ing it here, is distinct from “subjectivity” as that topic
has been studied in computational linguistics. Wiebe
et al. (1999) characterize subjective sentences as those
that “are used to communicate the speaker’s evaluations,
opinions, and speculations,” as distinguished from sen-
tences whose primary intention is “to objectively com-
municate material that is factual to the reporter.” In con-
trast, a biased sentence reflects a “tendency or preference
towards a particular perspective, ideology or result.”1 A
subjective sentence can be unbiased (I think that movie
was terrible), and a biased sentence can purport to com-
municate factually (Nationalizing our health care system
</bodyText>
<footnote confidence="0.993943">
1http://en.wikipedia.org/wiki/Bias as of 13 April,
2010.
</footnote>
<page confidence="0.968252">
152
</page>
<note confidence="0.756994">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 152–158,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995024">
is a point of no return for government interference in the
lives of its citizens2).
In addition to annotating sentences, each participant
was asked to complete a brief questionnaire about his or
her own political views. The survey asked:
</bodyText>
<listItem confidence="0.993402222222222">
1. Whether the participant is a resident of the United
States;
2. Who the participant voted for in the 2008 U.S.
presidential election (Barack Obama, John McCain,
other, decline to answer);
3. Which side of political spectrum he/she identified
with for social issues (liberal, conservative, decline
to answer); and
4. Which side of political spectrum he/she identified
</listItem>
<bodyText confidence="0.919647">
with for fiscal/economic issues (liberal, conserva-
tive, decline to answer).
This information was gathered to allow us to measure
variation in bias perception as it relates to the stance of
the annotator, e.g., whether people who view themselves
as liberal perceive more bias in conservative sources, and
vice versa.
</bodyText>
<subsectionHeader confidence="0.931931">
2.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9995385">
We extracted our sentences from the collection of blog
posts in Eisenstein and Xing (2010). The corpus con-
sists of 2008 blog posts gathered from six sites focused
on American politics:
</bodyText>
<listItem confidence="0.9998825">
• American Thinker (conservative),3
• Digby (liberal),4
• Hot Air (conservative),5
• Michelle Malkin (conservative),6
• Think Progress (liberal),7 and
• Talking Points Memo (liberal).8
</listItem>
<bodyText confidence="0.9457095">
13,246 posts were gathered in total, and 261,073 sen-
tences were extracted using WebHarvest9 and OpenNLP
1.3.0.10 Conservative and liberal sites are evenly rep-
resented (130,980 sentences from conservative sites,
130,093 from liberal sites). OpenNLP was also used for
tokenization.
</bodyText>
<footnote confidence="0.9999079">
2Sarah Palin, http://www.facebook.com/note.php?
note_id=113851103434, August 7, 2009.
3http://www.americanthinker.com
4http://digbysblog.blogspot.com
5http://hotair.com
6http://michellemalkin.com
7http://thinkprogress.org
8http://www.talkingpointsmemo.com
9http://web-harvest.sourceforge.net
10http://opennlp.sourceforge.net
</footnote>
<table confidence="0.999652">
Liberal Conservative
thinkprogress org exit question
video thinkprogress hat tip
et rally ed lasky
org 2008 hot air
gi bill tony rezko
wonk room ed morrissey
dana perino track record
phil gramm confirmed dead
senator mccain american thinker
abu ghraib illegal alien
</table>
<tableCaption confidence="0.999971">
Table 1: Top ten “sticky” partisan bigrams for each side.
</tableCaption>
<subsectionHeader confidence="0.99784">
2.2 Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999986542857143">
To support exploratory data analysis, we sought a di-
verse sample of sentences for annotation, but we were
also guided by some factors known or likely to correlate
with bias. We extracted sentences from our corpus that
matched at least one of the categories below, filtering to
keep those of length between 8 and 40 tokens. Then, for
each category, we first sampled 100 sentences without re-
placement. We then randomly extracted sentences up to
1,100 from the remaining pool. We selected the sentences
this way so that the collection has variety, while including
enough examples for individual categories. Our goal was
to gather at least 1,000 annotated sentences; ultimately
we collected 1,041. The categories are as follows.
“Sticky” partisan bigrams. One likely indicator of
bias is the use of terms that are particular to one side or
the other in a debate (Monroe et al., 2008). In order to
identify such terms, we independently created two lists
of “sticky” (i.e., strongly associated) bigrams in liberal
and conservative subcorpora, measuring association us-
ing the log-likelihood ratio (Dunning, 1993) and omitting
bigrams containing stopwords.11 We identified a bigram
as “liberal” if it was among the top 1,000 bigrams from
the liberal blogs, as measured by strength of association,
and was also not among the top 1,000 bigrams on the con-
servative side. The reverse definition yielded the “conser-
vative” bigrams. The resulting liberal list contained 495
bigrams, and the conservative list contained 539. We then
manually filtered cases that were clearly remnant HTML
tags and other markup, arriving at lists of 433 and 535,
respectively. Table 1 shows the strongest weighted bi-
grams.
As an example, consider this sentence (with a preced-
ing sentence of context), which contains gi bill. There is
no reason to think the bigram itself is inherently biased
(in contrast to, for example, death tax, which we would
</bodyText>
<footnote confidence="0.976402">
11We made use of Pedersen’s N-gram Statistics Package (Banerjee
and Pedersen, 2003).
</footnote>
<page confidence="0.998634">
153
</page>
<bodyText confidence="0.920305820895522">
perceive as biased in virtually any unquoted context), but
we do perceive bias in the full sentence.
Their hard fiscal line softens in the face of
American imperialist adventures. According to
CongressDaily the Bush dogs are also whining
because one of their members, Stephanie Her-
seth Sandlin, didn’t get HER GI Bill to the floor
in favor of Jim Webb’s .
Emotional lexical categories. Emotional words might
be another indicator of bias. We extracted four categories
of words from Pennebaker’s LIWC dictionary: Nega-
tive Emotion, Positive Emotion, Causation, and Anger.12
The following is one example of a biased sentence in our
dataset that matched these lexicons, in this case the Anger
category; the match is in bold.
A bunch of ugly facts are nailing the biggest
scare story in history.
The five most frequent matches in the corpus for each
category are as follows.13
Negative Emotion: war attack* problem* numb* argu*
Positive Emotion: like well good party* secur*
Causation: how because lead* make why
Anger: war attack* argu* fight* threat*
Kill verbs. Greene and Resnik (2009) discuss the rel-
evance of syntactic structure to the perception of senti-
ment. For example, their psycholinguistic experiments
would predict that when comparing Millions of people
starved under Stalin (inchoative) with Stalin starved mil-
lions ofpeople (transitive), the latter will be perceived as
more negative toward Stalin, because the transitive syn-
tactic frame tends to be connected with semantic prop-
erties such as intended action by the subject and change
of state in the object. “Kill verbs” provide particularly
strong examples of such phenomena, because they ex-
hibit a large set of semantic properties canonically as-
sociated with the transitive frame (Dowty, 1991). The
study by Greene and Resnik used 11 verbs of killing and
similar action to study the effect of syntactic “packag-
ing” on perceptions of sentiment.14 We included mem-
bership on this list (in any morphological form) as a se-
lection criterion, both because these verbs may be likely
12http://www.liwc.net. See Pennebaker et al. (2007) for de-
tailed description of background theory, and how these lexicons were
constructed. Our gratitude to Jamie Pennebaker for the use of this dic-
tionary.
13Note that some LIWC lexical entries are specified as pre-
fixes/stems, e.g. ugl*, which matches ugly uglier, etc.
14The verbs are: kill, slaughter, assassinate, shoot, poison, strangle,
smother, choke, drown, suffocate, and starve.
to appear in sentences containing bias (they overlap sig-
nificantly with Pennebaker’s Negative Emotion list), and
because annotation of bias will provide further data rel-
evant to Greene and Resnik’s hypothesis about the con-
nections among semantic propeties, syntactic structures,
and positive or negative perceptions (which are strongly
connected with bias).
In our final 1,041-sentence sample, “sticky bigrams”
occur 235 times (liberal 113, conservative 122), the lexi-
cal category features occur 1,619 times (Positive Emotion
577, Negative Emotion 466, Causation 332, and Anger
244), and “kill” verbs appear as a feature in 94 sentences.
Note that one sentence often matches multiple selection
criteria. Of the 1,041-sentence sample, 232 (22.3%) are
from American Thinker, 169 (16.2%) from Digby, 246
(23.6%) from Hot Air, 73 (7.0%) from Michelle Malkin,
166 (15.9%) from Think Progress, and 155 (14.9%) from
Talking Points Memo.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="method">
3 Mechanical Turk Experiment
</sectionHeader>
<bodyText confidence="0.999994846153846">
We prepared 1,100 Human Intelligence Tasks (HITs),
each containing one sentence annotation task. 1,041 sen-
tences were annotated five times each (5,205 judgements
total). One annotation task consists of three bias judge-
ment questions plus four survey questions. We priced
each HIT between $0.02 and $0.04 (moving from less
to more to encourage faster completion). The total cost
was $212.15 We restricted access to our tasks to those
who resided in United States and who had above 90% ap-
proval history, to ensure quality and awareness of Amer-
ican political issues. We also discarded HITs annotated
by workers with particularly low agreement scores. The
time allowance for each HIT was set at 5 minutes.
</bodyText>
<subsectionHeader confidence="0.9972955">
3.1 Annotation Results
3.1.1 Distribution of Judgments
</subsectionHeader>
<bodyText confidence="0.942028">
Overall, more than half the judgments are “not biased,”
and the “very biased” label is used sparingly (Table 2).
There is a slight tendency among the annotators to assign
the “very conservative” label, although moderate bias is
distributed evenly on both side (Table 3). Interestingly,
there are many “biased, but not sure” labels, indicating
that the annotators are capable of perceiving bias (or ma-
nipulative language), without fully decoding the intent of
the author, given sentences out of context.
Bias 1 1.5 2 2.5 3
% judged 36.0 26.6 25.5 9.4 2.4
</bodyText>
<tableCaption confidence="0.960036666666667">
Table 2: Strength of perceived bias per sentence, averaged over
the annotators (rounded to nearest half point). Annotators rate
bias on a scale of 1 (no bias), 2 (some bias), and 3 (very biased).
</tableCaption>
<footnote confidence="0.700414">
15This includes the cost for the discarded annotations.
</footnote>
<page confidence="0.99952">
154
</page>
<figureCaption confidence="0.9735865">
Figure 1: HIT: Three judgment questions. We first ask for the strength of bisa, then the direction. For the word-level annotation
question (right), workers are asked to check the box to indicate the region which “give away” the bias.
</figureCaption>
<table confidence="0.847309">
Bias type VL ML NB MC VC B
% judged 4.0 8.5 54.8 8.2 6.7 17.9
</table>
<tableCaption confidence="0.998925">
Table 3: Direction of perceived bias, per judgment (very lib-
eral, moderately liberal, no bias, moderately conservative, very
conservative, biased but not sure which).
</tableCaption>
<table confidence="0.9996915">
Social Economic
L M C NA
L 20.1 10.1 4.9 0.7
M 0.0 21.9 4.7 0.0
C 0.1 0.4 11.7 0.0
NA 0.1 0.0 11.2 14.1
</table>
<tableCaption confidence="0.923169">
Table 4: Distribution of judgements by annotators’ self-
identification on social issues (row) and fiscal issue (column);
{L, C, M, NA} denote liberal, conservative, moderate, and de-
cline to answer, respectively.
</tableCaption>
<subsectionHeader confidence="0.847503">
3.1.2 Annotation Quality
</subsectionHeader>
<bodyText confidence="0.9999308">
In this study, we are interested in where the wisdom of
the crowd will take us, or where the majority consensus
on bias may emerge. For this reason we did not contrive a
gold standard for “correct” annotation. We are, however,
mindful of its overall quality—whether annotations have
reasonable agreement, and whether there are fraudulent
responses tainting the results.
To validate our data, we measured the pair-wise Kappa
statistic (Cohen, 1960) among the 50 most frequent work-
ers16 and took the average over all the scores.17. The
average of the agreement score for the first question is
0.55, and the second 0.50. Those are within the range of
reasonable agreement for moderately difficult task. We
also inspected per worker average scores for frequent
workers18 and found one with consistently low agreement
scores. We discarded all the HITs by this worker from our
results. We also manually inspected the first 200 HITs for
apparent frauds. The annotations appeared to be consis-
tent. Often annotators agreed (many “no bias” cases were
unanimous), or differed in only the degree of strength
(“very biased” vs. “biased”) or specificity (“biased but I
am not sure” vs. “moderately liberal”). The direction of
bias, if specified, was very rarely inconsistent.
Along with the annotation tasks, we asked workers
how we could improve our HITs. Some comments were
</bodyText>
<footnote confidence="0.919823142857143">
16258 workers participated; only 50 of them completed more than 10
annotations.
17Unlike traditional subjects for a user-annotation study, our annota-
tors have not judged all the sentences considered in the study. There-
fore, to compute the agreement, we considered only the case where two
annotators share 20 or more sentences.
18We consider only those with 10 or more annotations.
</footnote>
<page confidence="0.998928">
155
</page>
<bodyText confidence="0.999985714285714">
insightful for our study (as well as for the interface de-
sign). A few pointed out that an impolite statement or
a statement of negative fact is not the same as bias, and
therefore should be marked separately from bias. Others
mentioned that some sentences are difficult to judge out
of context. These comments will be taken into account in
future research.
</bodyText>
<sectionHeader confidence="0.769086" genericHeader="method">
4 Analysis and Significance
</sectionHeader>
<bodyText confidence="0.99978025">
In the following section we report some of the interesting
trends we found in our annotation results. We consider a
few questions and report the answers the data provide for
each.
</bodyText>
<subsectionHeader confidence="0.9260535">
4.1 Is a sentence from a liberal blog more likely be
seen as liberal?
</subsectionHeader>
<bodyText confidence="0.9998954">
In our sample sentence pool, conservatives and liberals
are equally represented, though each blog site has a dif-
ferent representation.19 We grouped sentences by source
site, then computed the percentage representation of each
site within each bias label; see Table 5. In the top row,
we show the percentage representation of each group in
overall judgements.
In general, a site yields more sentences that match its
known political leanings. Note that in our annotation
task, we did not disclose the sentence’s source to the
workers. The annotators formed their judgements solely
based on the content of the sentence. This result can
be taken as confirming people’s ability to perceive bias
within a sentence, or, conversely, as confirming our a pri-
ori categorizations of the blogs.
</bodyText>
<table confidence="0.9980495">
at ha mm db tp tpm
Overall 22.3 23.6 7.0 16.2 15.9 14.9
NB 23.7 22.3 6.1 15.7 17.0 15.3
VC 24.8 32.3 19.3 6.9 7.5 9.2
MC 24.4 33.6 8.0 8.2 13.6 12.2
ML 16.6 15.2 3.4 21.1 22.9 20.9
VL 16.7 9.0 4.3 31.0 22.4 16.7
B 20.1 25.4 7.2 19.5 12.3 13.7
</table>
<tableCaption confidence="0.993604">
Table 5: Percentage representation of each site within bias label
</tableCaption>
<bodyText confidence="0.782675">
pools from question 2 (direction of perceived bias): very liberal,
moderately liberal, no bias, moderately conservative, very con-
servative, biased but not sure which. Rows sum to 100. Bold-
face indicates rates higher than the site’s overall representation
in the pool.
</bodyText>
<subsectionHeader confidence="0.9852225">
4.2 Does a liberal leaning annotator see more
conservative bias?
</subsectionHeader>
<bodyText confidence="0.9758305">
In Table 5, we see that blogs are very different from each
other in terms of the bias annotators perceive in their lan-
</bodyText>
<footnote confidence="0.58524">
19Posts appear on different sites at different rates.
</footnote>
<figureCaption confidence="0.9967942">
Figure 2: Distribution of bias labels (by judgment) for social
and economic liberals (LL), social and economic moderates
(MM), and social and economic conservatives (CC), and over-
all. Note that this plot uses a logarithmic scale, to tease apart
the differences among groups.
</figureCaption>
<bodyText confidence="0.998674363636364">
guage. In general, conservative sites seemingly produced
much more identifiable partisan bias than liberal sites.20
This impression, however, might be an artifact of the
distribution of the annotators’ own bias. As seen in Ta-
ble 4, a large portion of our annotators identified them-
selves as liberal in some way. People might call a state-
ment biased if they disagree with it, while showing le-
niency toward hyperbole more consistent with their opin-
ions.
To answer this question, we break down the judgement
labels by the annotators’ self-identification, and check
the percentage of each bias type within key groups (see
Figure 2). In general, moderates perceive less bias than
partisans (another useful reality check, in the sense that
this is to be expected), but conservatives show a much
stronger tendency to label sentences as biased, in both
directions. (We caution that the underrepresentation of
self-identifying conservatives in our worker pool means
that only 608 judgments from 48 distinct workers were
used to estimate these statistics.) Liberals in this sample
are less balanced, perceiving conservative bias at double
the rate of liberal bias.
</bodyText>
<subsectionHeader confidence="0.8453365">
4.3 What are the lexical indicators of perceived
bias?
</subsectionHeader>
<bodyText confidence="0.970924363636364">
For a given word type w, we calculate the frequency that
it was marked as indicating bias, normalized by its total
number of occurrences. To combine the judgments of dif-
ferent annotators, we increment w’s count by k/n when-
ever k judgments out of n marked the word as showing
bias. We perform similar calculations with a restriction
to liberal and conservative judgments on the sentence as a
20Liberal sites cumulatively produced 64.9% of the moderately lib-
eral bias label and 70.1 % of very liberal, while conservative sites pro-
duced 66.0% of moderately conservative and 76.4% of very conserva-
tive, respectively.
</bodyText>
<figure confidence="0.9960011">
100
32
10
3
1
very conservative no bias very liberal not sure
LL
MM
CC
Overall
</figure>
<page confidence="0.988412">
156
</page>
<table confidence="0.999681923076923">
Overall Liberal Conservative Not Sure Which
bad 0.60 Administration 0.28 illegal 0.40 pass 0.32
personally 0.56 Americans 0.24 Obama’s 0.38 bad 0.32
illegal 0.53 woman 0.24 corruption 0.32 sure 0.28
woman 0.52 single 0.24 rich 0.28 blame 0.28
single 0.52 personally 0.24 stop 0.26 they’re 0.24
rich 0.52 lobbyists 0.23 tax 0.25 happen 0.24
corruption 0.52 Republican 0.22 claimed 0.25 doubt 0.24
Administration 0.52 union 0.20 human 0.24 doing 0.24
Americans 0.51 torture 0.20 doesn’t 0.24 death 0.24
conservative 0.50 rich 0.20 difficult 0.24 actually 0.24
doubt 0.48 interests 0.20 Democrats 0.24 exactly 0.22
torture 0.47 doing 0.20 less 0.23 wrong 0.22
</table>
<tableCaption confidence="0.9838235">
Table 6: Most strongly biased words, ranked by relative frequency of receiving a bias mark, normalized by total frequency. Only
words appearing five times or more in our annotation set are ranked.
</tableCaption>
<bodyText confidence="0.9998816">
whole. Top-ranked words for each calculation are shown
in Table 6.
Some of the patterns we see are consistent with what
we found in our automatic method for proposing biased
bigrams. For example, the bigrams tended to include
terms that refer to members or groups on the opposing
side. Here we find that Republican and Administration
(referring in 2008 to the Bush administration) tends to
show liberal bias, while Obama’s and Democrats show
conservative bias.
</bodyText>
<sectionHeader confidence="0.996959" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99999484375">
The study we have conducted here represents an initial
pass at empirical, corpus-driven analysis of bias using the
methods of computational linguistics. The results thus far
suggest that it is possible to automatically extract a sam-
ple that is rich in examples that annotators would con-
sider biased; that naive annotators can achieve reason-
able agreement with minimal instructions and no train-
ing; and that basic exploratory analysis of results yields
interpretable patterns that comport with prior expecta-
tions, as well as interesting observations that merit further
investigation.
In future work, enabled by annotations of biased and
non-biased material, we plan to delve more deeply into
the linguistic characteristics associated with biased ex-
pression. These will include, for example, an analysis
of the extent to which explicit “lexical framing” (use of
partisan terms, e.g., Monroe et al., 2008) is used to con-
vey bias, versus use of more subtle cues such as syntactic
framing (Greene and Resnik, 2009). We will also explore
the extent to which idiomatic usages are connected with
bias, with the prediction that partisan “memes” tend to be
more idiomatic than compositional in nature.
In our current analysis, the issue of subjectivity was not
directly addressed. Previous work has shown that opin-
ions are closely related to subjective language (Pang and
Lee, 2008). It is possible that asking annotators about
sentiment while asking about bias would provide a deeper
understanding of the latter. Interestingly, annotator feed-
back included remarks that mere negative “facts” do not
convey an author’s opinion or bias. The nature of subjec-
tivity as a factor in bias perception is an important issue
for future investigation.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992">
This paper considered the linguistic indicators of bias in
political text. We used Amazon Mechanical Turk judg-
ments about sentences from American political blogs,
asking annotators to indicate whether a sentence showed
bias, and if so, in which political direction and through
which word tokens; these data were augmented by a po-
litical questionnaire for each annotator. Our preliminary
analysis suggests that bias can be annotated reasonably
consistently, that bias perception varies based on personal
views, and that there are some consistent lexical cues for
bias in political blog data.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998905">
The authors acknowledge research support from HP
Labs, help with data from Jacob Eisenstein, and help-
ful comments from the reviewers, Olivia Buzek, Michael
Heilman, and Brendan O’Connor.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996481375">
Satanjeev Banerjee and Ted Pedersen. 2003. The design, implementa-
tion and use of the ngram statistics package. In the Fourth Interna-
tional Conference on Intelligent Text Processing and Computational
Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):37–46.
David Dowty. 1991. Thematic Proto-Roles and Argument Selection.
Language, 67:547–619.
</reference>
<page confidence="0.975218">
157
</page>
<reference confidence="0.99289612">
Ted Dunning. 1993. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61–74.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog
corpus. Technical report CMU-ML-10-101.
Robert M. Entman. 2007. Framing bias: Media in the distribution of
power. Journal of Communication, 57(1):163–173.
Stephan Greene and Philip Resnik. 2009. More than words: Syntactic
packaging and implicit sentiment. In NAACL, pages 503–511, June.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn. 2008.
Fightin’ words: Lexical feature selection and evaluation for identi-
fying the content of political conflict. Political Analysis, 16(4):372–
403, October.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis.
Foundations and Trends in Information Retrieval, 2(1-2):1–135.
J.W. Pennebaker, C.K Chung, M. Ireland, A Gonzales, and R J. Booth,
2007. The development and psychometric properties of LIWC2007.
Dietram A. Scheufele. 1999. Framing as a theory of media effects.
Journal of Communication, 49(1):103–122.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O’Hara. 1999.
Development and use of a gold standard data set for subjectivity
classifications. In Proceedings of the Association for Computational
Linguistics (ACL), pages 246–253.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “anno-
tator rationales” to improve machine learning for text categorization.
In NAACL, pages 260–267, April.
</reference>
<page confidence="0.997019">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946270">
<title confidence="0.999741">Shedding (a Thousand Points of) Light on Biased Language</title>
<author confidence="0.979927">Tae Yano</author>
<affiliation confidence="0.999894">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.998027">Pittsburgh, PA 15213, USA</address>
<email confidence="0.99982">taey@cs.cmu.edu</email>
<author confidence="0.988889">Philip</author>
<affiliation confidence="0.999911">Department of Linguistics and University of</affiliation>
<address confidence="0.996805">College Park, MD 20742,</address>
<email confidence="0.99984">resnik@umiacs.umd.edu</email>
<author confidence="0.99995">Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997887">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999656">nasmith@cs.cmu.edu</email>
<abstract confidence="0.998692083333333">This paper considers the linguistic indicators of bias in political text. We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>The design, implementation and use of the ngram statistics package.</title>
<date>2003</date>
<booktitle>In the Fourth International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="8101" citStr="Banerjee and Pedersen, 2003" startWordPosition="1214" endWordPosition="1217">e definition yielded the “conservative” bigrams. The resulting liberal list contained 495 bigrams, and the conservative list contained 539. We then manually filtered cases that were clearly remnant HTML tags and other markup, arriving at lists of 433 and 535, respectively. Table 1 shows the strongest weighted bigrams. As an example, consider this sentence (with a preceding sentence of context), which contains gi bill. There is no reason to think the bigram itself is inherently biased (in contrast to, for example, death tax, which we would 11We made use of Pedersen’s N-gram Statistics Package (Banerjee and Pedersen, 2003). 153 perceive as biased in virtually any unquoted context), but we do perceive bias in the full sentence. Their hard fiscal line softens in the face of American imperialist adventures. According to CongressDaily the Bush dogs are also whining because one of their members, Stephanie Herseth Sandlin, didn’t get HER GI Bill to the floor in favor of Jim Webb’s . Emotional lexical categories. Emotional words might be another indicator of bias. We extracted four categories of words from Pennebaker’s LIWC dictionary: Negative Emotion, Positive Emotion, Causation, and Anger.12 The following is one ex</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. The design, implementation and use of the ngram statistics package. In the Fourth International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="14359" citStr="Cohen, 1960" startWordPosition="2218" endWordPosition="2219">n social issues (row) and fiscal issue (column); {L, C, M, NA} denote liberal, conservative, moderate, and decline to answer, respectively. 3.1.2 Annotation Quality In this study, we are interested in where the wisdom of the crowd will take us, or where the majority consensus on bias may emerge. For this reason we did not contrive a gold standard for “correct” annotation. We are, however, mindful of its overall quality—whether annotations have reasonable agreement, and whether there are fraudulent responses tainting the results. To validate our data, we measured the pair-wise Kappa statistic (Cohen, 1960) among the 50 most frequent workers16 and took the average over all the scores.17. The average of the agreement score for the first question is 0.55, and the second 0.50. Those are within the range of reasonable agreement for moderately difficult task. We also inspected per worker average scores for frequent workers18 and found one with consistently low agreement scores. We discarded all the HITs by this worker from our results. We also manually inspected the first 200 HITs for apparent frauds. The annotations appeared to be consistent. Often annotators agreed (many “no bias” cases were unanim</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<date>1991</date>
<booktitle>Thematic Proto-Roles and Argument Selection. Language,</booktitle>
<pages>67--547</pages>
<contexts>
<context position="9863" citStr="Dowty, 1991" startWordPosition="1496" endWordPosition="1497">on of sentiment. For example, their psycholinguistic experiments would predict that when comparing Millions of people starved under Stalin (inchoative) with Stalin starved millions ofpeople (transitive), the latter will be perceived as more negative toward Stalin, because the transitive syntactic frame tends to be connected with semantic properties such as intended action by the subject and change of state in the object. “Kill verbs” provide particularly strong examples of such phenomena, because they exhibit a large set of semantic properties canonically associated with the transitive frame (Dowty, 1991). The study by Greene and Resnik used 11 verbs of killing and similar action to study the effect of syntactic “packaging” on perceptions of sentiment.14 We included membership on this list (in any morphological form) as a selection criterion, both because these verbs may be likely 12http://www.liwc.net. See Pennebaker et al. (2007) for detailed description of background theory, and how these lexicons were constructed. Our gratitude to Jamie Pennebaker for the use of this dictionary. 13Note that some LIWC lexical entries are specified as prefixes/stems, e.g. ugl*, which matches ugly uglier, etc</context>
</contexts>
<marker>Dowty, 1991</marker>
<rawString>David Dowty. 1991. Thematic Proto-Roles and Argument Selection. Language, 67:547–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="7208" citStr="Dunning, 1993" startWordPosition="1071" endWordPosition="1072">entences this way so that the collection has variety, while including enough examples for individual categories. Our goal was to gather at least 1,000 annotated sentences; ultimately we collected 1,041. The categories are as follows. “Sticky” partisan bigrams. One likely indicator of bias is the use of terms that are particular to one side or the other in a debate (Monroe et al., 2008). In order to identify such terms, we independently created two lists of “sticky” (i.e., strongly associated) bigrams in liberal and conservative subcorpora, measuring association using the log-likelihood ratio (Dunning, 1993) and omitting bigrams containing stopwords.11 We identified a bigram as “liberal” if it was among the top 1,000 bigrams from the liberal blogs, as measured by strength of association, and was also not among the top 1,000 bigrams on the conservative side. The reverse definition yielded the “conservative” bigrams. The resulting liberal list contained 495 bigrams, and the conservative list contained 539. We then manually filtered cases that were clearly remnant HTML tags and other markup, arriving at lists of 433 and 535, respectively. Table 1 shows the strongest weighted bigrams. As an example, </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Eric Xing</author>
</authors>
<title>political blog corpus.</title>
<date>2010</date>
<booktitle>The CMU</booktitle>
<tech>Technical report CMU-ML-10-101.</tech>
<contexts>
<context position="4871" citStr="Eisenstein and Xing (2010)" startWordPosition="740" endWordPosition="743">, other, decline to answer); 3. Which side of political spectrum he/she identified with for social issues (liberal, conservative, decline to answer); and 4. Which side of political spectrum he/she identified with for fiscal/economic issues (liberal, conservative, decline to answer). This information was gathered to allow us to measure variation in bias perception as it relates to the stance of the annotator, e.g., whether people who view themselves as liberal perceive more bias in conservative sources, and vice versa. 2.1 Dataset We extracted our sentences from the collection of blog posts in Eisenstein and Xing (2010). The corpus consists of 2008 blog posts gathered from six sites focused on American politics: • American Thinker (conservative),3 • Digby (liberal),4 • Hot Air (conservative),5 • Michelle Malkin (conservative),6 • Think Progress (liberal),7 and • Talking Points Memo (liberal).8 13,246 posts were gathered in total, and 261,073 sentences were extracted using WebHarvest9 and OpenNLP 1.3.0.10 Conservative and liberal sites are evenly represented (130,980 sentences from conservative sites, 130,093 from liberal sites). OpenNLP was also used for tokenization. 2Sarah Palin, http://www.facebook.com/no</context>
</contexts>
<marker>Eisenstein, Xing, 2010</marker>
<rawString>Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog corpus. Technical report CMU-ML-10-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Entman</author>
</authors>
<title>Framing bias: Media in the distribution of power.</title>
<date>2007</date>
<journal>Journal of Communication,</journal>
<volume>57</volume>
<issue>1</issue>
<contexts>
<context position="1107" citStr="Entman, 2007" startWordPosition="158" endWordPosition="159">on Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias. 1 Introduction Bias and framing are central topics in the study of communications, media, and political discourse (Scheufele, 1999; Entman, 2007), but they have received relatively little attention in computational linguistics. What are the linguistic indicators of bias? Are there lexical, syntactic, topical, or other clues that can be computationally modeled and automatically detected? Here we use Amazon Mechanical Turk (MTurk) to engage in a systematic, empirical study of linguistic indicators of bias in the political domain, using text drawn from political blogs. Using the MTurk framework, we collected judgments connected with the two dominant schools of thought in American politics, as exhibited in single sentences. Since no one pe</context>
</contexts>
<marker>Entman, 2007</marker>
<rawString>Robert M. Entman. 2007. Framing bias: Media in the distribution of power. Journal of Communication, 57(1):163–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>503--511</pages>
<contexts>
<context position="9190" citStr="Greene and Resnik (2009)" startWordPosition="1391" endWordPosition="1394">egories of words from Pennebaker’s LIWC dictionary: Negative Emotion, Positive Emotion, Causation, and Anger.12 The following is one example of a biased sentence in our dataset that matched these lexicons, in this case the Anger category; the match is in bold. A bunch of ugly facts are nailing the biggest scare story in history. The five most frequent matches in the corpus for each category are as follows.13 Negative Emotion: war attack* problem* numb* argu* Positive Emotion: like well good party* secur* Causation: how because lead* make why Anger: war attack* argu* fight* threat* Kill verbs. Greene and Resnik (2009) discuss the relevance of syntactic structure to the perception of sentiment. For example, their psycholinguistic experiments would predict that when comparing Millions of people starved under Stalin (inchoative) with Stalin starved millions ofpeople (transitive), the latter will be perceived as more negative toward Stalin, because the transitive syntactic frame tends to be connected with semantic properties such as intended action by the subject and change of state in the object. “Kill verbs” provide particularly strong examples of such phenomena, because they exhibit a large set of semantic </context>
<context position="22424" citStr="Greene and Resnik, 2009" startWordPosition="3535" endWordPosition="3538">no training; and that basic exploratory analysis of results yields interpretable patterns that comport with prior expectations, as well as interesting observations that merit further investigation. In future work, enabled by annotations of biased and non-biased material, we plan to delve more deeply into the linguistic characteristics associated with biased expression. These will include, for example, an analysis of the extent to which explicit “lexical framing” (use of partisan terms, e.g., Monroe et al., 2008) is used to convey bias, versus use of more subtle cues such as syntactic framing (Greene and Resnik, 2009). We will also explore the extent to which idiomatic usages are connected with bias, with the prediction that partisan “memes” tend to be more idiomatic than compositional in nature. In our current analysis, the issue of subjectivity was not directly addressed. Previous work has shown that opinions are closely related to subjective language (Pang and Lee, 2008). It is possible that asking annotators about sentiment while asking about bias would provide a deeper understanding of the latter. Interestingly, annotator feedback included remarks that mere negative “facts” do not convey an author’s o</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In NAACL, pages 503–511, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burt L Monroe</author>
<author>Michael P Colaresi</author>
<author>Kevin M Quinn</author>
</authors>
<title>Fightin’ words: Lexical feature selection and evaluation for identifying the content of political conflict.</title>
<date>2008</date>
<journal>Political Analysis,</journal>
<volume>16</volume>
<issue>4</issue>
<pages>403</pages>
<contexts>
<context position="6982" citStr="Monroe et al., 2008" startWordPosition="1038" endWordPosition="1041">low, filtering to keep those of length between 8 and 40 tokens. Then, for each category, we first sampled 100 sentences without replacement. We then randomly extracted sentences up to 1,100 from the remaining pool. We selected the sentences this way so that the collection has variety, while including enough examples for individual categories. Our goal was to gather at least 1,000 annotated sentences; ultimately we collected 1,041. The categories are as follows. “Sticky” partisan bigrams. One likely indicator of bias is the use of terms that are particular to one side or the other in a debate (Monroe et al., 2008). In order to identify such terms, we independently created two lists of “sticky” (i.e., strongly associated) bigrams in liberal and conservative subcorpora, measuring association using the log-likelihood ratio (Dunning, 1993) and omitting bigrams containing stopwords.11 We identified a bigram as “liberal” if it was among the top 1,000 bigrams from the liberal blogs, as measured by strength of association, and was also not among the top 1,000 bigrams on the conservative side. The reverse definition yielded the “conservative” bigrams. The resulting liberal list contained 495 bigrams, and the co</context>
<context position="22317" citStr="Monroe et al., 2008" startWordPosition="3515" endWordPosition="3518"> consider biased; that naive annotators can achieve reasonable agreement with minimal instructions and no training; and that basic exploratory analysis of results yields interpretable patterns that comport with prior expectations, as well as interesting observations that merit further investigation. In future work, enabled by annotations of biased and non-biased material, we plan to delve more deeply into the linguistic characteristics associated with biased expression. These will include, for example, an analysis of the extent to which explicit “lexical framing” (use of partisan terms, e.g., Monroe et al., 2008) is used to convey bias, versus use of more subtle cues such as syntactic framing (Greene and Resnik, 2009). We will also explore the extent to which idiomatic usages are connected with bias, with the prediction that partisan “memes” tend to be more idiomatic than compositional in nature. In our current analysis, the issue of subjectivity was not directly addressed. Previous work has shown that opinions are closely related to subjective language (Pang and Lee, 2008). It is possible that asking annotators about sentiment while asking about bias would provide a deeper understanding of the latter</context>
</contexts>
<marker>Monroe, Colaresi, Quinn, 2008</marker>
<rawString>Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn. 2008. Fightin’ words: Lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis, 16(4):372– 403, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="22787" citStr="Pang and Lee, 2008" startWordPosition="3593" endWordPosition="3596">on. These will include, for example, an analysis of the extent to which explicit “lexical framing” (use of partisan terms, e.g., Monroe et al., 2008) is used to convey bias, versus use of more subtle cues such as syntactic framing (Greene and Resnik, 2009). We will also explore the extent to which idiomatic usages are connected with bias, with the prediction that partisan “memes” tend to be more idiomatic than compositional in nature. In our current analysis, the issue of subjectivity was not directly addressed. Previous work has shown that opinions are closely related to subjective language (Pang and Lee, 2008). It is possible that asking annotators about sentiment while asking about bias would provide a deeper understanding of the latter. Interestingly, annotator feedback included remarks that mere negative “facts” do not convey an author’s opinion or bias. The nature of subjectivity as a factor in bias perception is an important issue for future investigation. 6 Conclusion This paper considered the linguistic indicators of bias in political text. We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>C K Chung</author>
<author>M Ireland</author>
<author>A Gonzales</author>
<author>R J Booth</author>
</authors>
<title>The development and psychometric properties of LIWC2007.</title>
<date>2007</date>
<contexts>
<context position="10196" citStr="Pennebaker et al. (2007)" startWordPosition="1549" endWordPosition="1552">cted with semantic properties such as intended action by the subject and change of state in the object. “Kill verbs” provide particularly strong examples of such phenomena, because they exhibit a large set of semantic properties canonically associated with the transitive frame (Dowty, 1991). The study by Greene and Resnik used 11 verbs of killing and similar action to study the effect of syntactic “packaging” on perceptions of sentiment.14 We included membership on this list (in any morphological form) as a selection criterion, both because these verbs may be likely 12http://www.liwc.net. See Pennebaker et al. (2007) for detailed description of background theory, and how these lexicons were constructed. Our gratitude to Jamie Pennebaker for the use of this dictionary. 13Note that some LIWC lexical entries are specified as prefixes/stems, e.g. ugl*, which matches ugly uglier, etc. 14The verbs are: kill, slaughter, assassinate, shoot, poison, strangle, smother, choke, drown, suffocate, and starve. to appear in sentences containing bias (they overlap significantly with Pennebaker’s Negative Emotion list), and because annotation of bias will provide further data relevant to Greene and Resnik’s hypothesis abou</context>
</contexts>
<marker>Pennebaker, Chung, Ireland, Gonzales, Booth, 2007</marker>
<rawString>J.W. Pennebaker, C.K Chung, M. Ireland, A Gonzales, and R J. Booth, 2007. The development and psychometric properties of LIWC2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietram A Scheufele</author>
</authors>
<title>Framing as a theory of media effects.</title>
<date>1999</date>
<journal>Journal of Communication,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="1092" citStr="Scheufele, 1999" startWordPosition="156" endWordPosition="157">ext. We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias. 1 Introduction Bias and framing are central topics in the study of communications, media, and political discourse (Scheufele, 1999; Entman, 2007), but they have received relatively little attention in computational linguistics. What are the linguistic indicators of bias? Are there lexical, syntactic, topical, or other clues that can be computationally modeled and automatically detected? Here we use Amazon Mechanical Turk (MTurk) to engage in a systematic, empirical study of linguistic indicators of bias in the political domain, using text drawn from political blogs. Using the MTurk framework, we collected judgments connected with the two dominant schools of thought in American politics, as exhibited in single sentences. </context>
</contexts>
<marker>Scheufele, 1999</marker>
<rawString>Dietram A. Scheufele. 1999. Framing as a theory of media effects. Journal of Communication, 49(1):103–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O’Hara</author>
</authors>
<title>Development and use of a gold standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>246--253</pages>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O’Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. In Proceedings of the Association for Computational Linguistics (ACL), pages 246–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using “annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In NAACL,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="2483" citStr="Zaidan et al. (2007)" startWordPosition="376" endWordPosition="379">lation. 2 Annotation Task We drew sentences from a corpus of American political blog posts from 2008. (Details in Section 2.1.) Sentences were presented to participants one at a time, without context. Participants were asked to judge the following (see Figure 1 for interface design): • To what extent a sentence or clause is biased (none, somewhat, very); • The nature of the bias (very liberal, moderately liberal, moderately conservative, very conservative, biased but not sure which direction); and • Which words in the sentence give away the author’s bias, similar to “rationale” annotations in Zaidan et al. (2007). For example, a participant might identify a moderate liberal bias in this sentence, Without Sestak’s challenge, we would have Specter, comfortably ensconced as a Democrat in name only. adding checkmarks on the underlined words. A more neutral paraphrase is: Without Sestak’s challenge, Specter would have no incentive to side more frequently with Democrats. It is worth noting that “bias,” in the sense we are using it here, is distinct from “subjectivity” as that topic has been studied in computational linguistics. Wiebe et al. (1999) characterize subjective sentences as those that “are used to</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In NAACL, pages 260–267, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>