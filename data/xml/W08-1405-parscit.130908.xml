<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.841295">
MultiSum
Query-Based Multi-Document Summarisation
</title>
<author confidence="0.982935">
Michael Rosner Carl Camilleri
</author>
<affiliation confidence="0.976203">
Dept. Artificial Intelligence Dept. Artificial Intelligence
University of Malta University of Malta
</affiliation>
<email confidence="0.996241">
mike.rosner@um.edu.mt ccam0002@um.edu.mt
</email>
<sectionHeader confidence="0.993797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764636363636">
This paper describes a generic, open-
domain multi-document summarisation
system which combines new and exist-
ing techniques in a novel way. The sys-
tem is capable of automatically identify-
ing query-related online documents and
compiling a report from the most use-
ful sources, whilst presenting the result in
such a way as to make it easy for the re-
searcher to look up the information in its
original context.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895321428571">
Although electronic resources have several in-
herent advantages over traditional research me-
dia, they also introduce several drawbacks, such
as Information Overload (Edmunds and Morris,
2000),which has become synonymous with the in-
formation retrieval phase of any research-related
task. Another problem which is directly related
to the one just described is that of Source Iden-
tification (Eppler and Mengis, 2004). This refers
to the problem of having relevant results intermin-
gled with results that are less relevant, or actually
irrelevant.
Lastly, the researcher usually has to also manu-
ally traverse the relevant sources of information in
order to form an answer to the research query.
These problems have led to the study of vari-
ous areas in computing, all of which aim to try and
minimise the manual effort of information retrieval
and extraction, one of which is Multi-Document
Summarisation (MDS).
The core aim of any MDS system is that of pro-
cessing multiple sources of information and out-
putting a relatively brief but broad report or sum-
mary. Uses of MDS systems vary widely, from
summarisation of closed-domain documents, such
as news documents (Evans et al., 2004), to aggre-
gation of information from several sources in an
open domain.
</bodyText>
<sectionHeader confidence="0.948446" genericHeader="introduction">
2 Aims and Objectives
</sectionHeader>
<bodyText confidence="0.999761555555556">
MDS techniques can be used in various tools that
may help addressing the problems described in
Section 1. On the other hand, a brief study of the
relevant literature indicates that the majority of the
work done in this area concerns closed-domains
such as news summarisation, which is perhaps the
reason why such tools have not yet become more
popular. The objectives of this study are thus
twofold.
</bodyText>
<listItem confidence="0.959105428571429">
• The primary objective is that of design-
ing, implementing and evaluating an open-
domain, query-based MDS system which is
capable of compiling an acceptably-coherent
report from the most relevant online sources
of information, whilst making it easy for the
reader to access the full source of information
in its original context.
• A secondary objective of this study is Search
Engine Optimisation (SEO): We require the
system to produce summaries which, if pub-
lished on the Internet, would be deemed rel-
evant to the original query by search en-
gine ranking algorithms. This is measured
by keyoword density in the summary. Suc-
cess on this objective addresses the problem
of Source Identification since the summary
would at the very least serve as a gateway to
the other relevant sources from which it was
formed.
© 2008. Licensed under the Creative Commons 25 Unsurprisingly, one of the problems that has to
</listItem>
<bodyText confidence="0.50628725">
C6U1#,26®8:�Vdk9Wi0JA&apos; ap�ir A0MI e Multilingual I &apos;i ii tE 1 i��r on s 25–32
(http://creativecommons.org/licenses/by-M&amp;tqLWier, August ��� par-
cense
Some rights reserved. ticular y in an open-domain system such as ours is
</bodyText>
<page confidence="0.993354">
26
</page>
<bodyText confidence="0.99834275">
the quality of output, as measured by a number of
different linguistic and non-linguistic criteria (see
Section 5). We have adopted a number of novel
techniques to address this such as
</bodyText>
<listItem confidence="0.99989225">
• Multi-Layered Architecture
• Sentence Ordering Model
• Heuristic Sentence Filtering
• Paragraph Clustering
</listItem>
<sectionHeader confidence="0.997224" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.999942">
3.1 Search Engine Ranking Criteria
</subsectionHeader>
<bodyText confidence="0.999968285714286">
Search engine ranking algorithms vary, and are
continuously being optimised in order to provide
better and more accurate results. However, some
guidelines that outline factors which web masters
need to take into account have been established (cf.
Google (2007), Vaughn (2007)).
When ranking documents for a particular search
query, ranking algorithms take into account both
on-page and off-page factors. Off-page factors
comprise mainly the number and quality of in-
bound links to a particular page, whilst on-page
factors comprise various criteria, most important
of which is the relevance of the content to the
search query.
</bodyText>
<subsectionHeader confidence="0.998275">
3.2 Multi-Document Summarisation
</subsectionHeader>
<bodyText confidence="0.978477666666667">
Several different approaches and processes have
been developed in automatic MDS systems. These
vary according to the problem domain, which usu-
ally defines particular formats for both input and
output. However, five basic sub-systems of any
MDS system can be identified (Mani, 2001).
</bodyText>
<listItem confidence="0.998752214285714">
1. Unit Identification During this first phase,
input documents are parsed and tokenised
into “units”, which can vary from single
words to whole documents, according to the
application problem.
2. Unit Matching (Clustering) The second
stage involves grouping similar units together.
In the context of MDS, similar units usu-
ally mean either identical or informationally-
equivalent strings (Clarke, 2004), with the
purpose of discovering the main themes in the
different units and identify the most salient
ones.
3. Unit Filtering The filtering stage eliminates
units residing in clusters which are deemed to
be non-salient.
4. Compacting During this phase, it is often as-
sumed that different clusters contain similar
units. Thus, a sample of units from different
clusters is chosen.
5. Presentation/Summary Generation The
last phase of the MDS process involves using
the output from the Compacting stage, and
generating a summary. Usually, naive string
concatenation does not produce coherent
summaries and thus, techniques such as
named entity normalisation and sentence
ordering criteria are used at this stage.
</listItem>
<subsectionHeader confidence="0.999676">
3.3 Clustering Techniques
</subsectionHeader>
<bodyText confidence="0.9998215">
As outlined in Section 3.2, MDS often makes use
of clustering techniques in order to group together
similar units. Clustering can be defined as a pro-
cess which performs “unsupervised classification
of patterns into groups based on their similarity”
(Clarke, 2004).
A particular clustering technique typically con-
sists of three main components:
</bodyText>
<listItem confidence="0.987533333333333">
1. Pattern Representation
2. Similarity Measure
3. Clustering Algorithm
</listItem>
<bodyText confidence="0.932905235294118">
The very generic nature of our problem domain
requires a clustering technique which is both suit-
able and without scenario-dependant parameters.
Fung’s algorithm (Fung et al., 2003), comprising a
pre-processing stage and a further three-phase core
process, uses the following concepts, and is briefly
described in Figure 1.
ItemSet A set of words occurring together
within a document. An ItemSet composed of k
words is called a k-ItemSet.
Global Support The Global Support of a word
item is the number of documents from the docu-
ment collection it appears in (cf. document fre-
quency).
Cluster Support The Cluster Support of a word
item is the number of documents within a cluster it
appears in.
</bodyText>
<listItem confidence="0.997935181818182">
1. Pre-Processing - stem, remove stop
words and convert to TFxIDF represen-
tation
2. Discover Global Frequent ItemSets
3. For each Global Frequent ItemSet (GFI)
create a corresponding cluster, contain-
ing all documents that contain all items
found within the GFI associated with
each cluster. This GFI will act as a “la-
bel” to the cluster.
4. Make Clusters Disjoint
</listItem>
<figureCaption confidence="0.928007">
Figure 1: Hierarchical Document Clustering Using
Frequent Itemsets
</figureCaption>
<bodyText confidence="0.991286888888889">
Frequent ItemSet An ItemSet occurring in a
pre-determined minimum portion of a document
collection. The pre-defined minimum is referred
to as the Minimum Support, and is usually deter-
mined empirically according to the application.
Global Frequent ItemSet An ItemSet which is
frequent within the whole document collection.
The words within a Global Frequent ItemSet are
referred to as Global Frequent Items, whilst the
minimum support is referred to as the Minimum
Global Support.
Cluster Frequent ItemSet An ItemSet which
is frequent within a cluster. In this context, the
minimum support is referred to as the Minimum
Cluster Support.
With these definitions, it is now possible to de-
scribe into more detail the core non-trivial phases
of the algorithm.
</bodyText>
<subsectionHeader confidence="0.985611">
3.3.1 Discovering Global Frequent ItemSets
</subsectionHeader>
<bodyText confidence="0.974902615384615">
From the definition of an ItemSet, it can be con-
cluded that the set of ItemSets is the power set of
all features1 within the document collection. Given
even a small document collection, enumerating all
the possible ItemSets and checking which of them
are Global Frequent would be intractable. In order
to discover Global Frequent ItemSets, the authors
recommend the use of the Apriori Candidate Gen-
eration algorithm, a data mining algorithm pro-
posed by Agrawal and Srikant (1994). This algo-
1Features here constitute distinct, single words found in
the whole document collection. In practice, stemming is ap-
plied before feature extraction.
rithm defines a way to reduce the number of candi-
date frequent ItemSets generated. The generation
algorithm basically operates on the principle that,
given a set of frequent k-1-ItemSets, a set of can-
didate frequent k-ItemSets can be generated such
that each candidate is composed of frequent k-1-
ItemSets.
Agrawal and Srikant (1994) also mention a sim-
ilar algorithm proposed by Mannila et al. (1994).
As illustrated in Figure 2, this algorithm consists
of first generating candidates, and then pruning the
result based on a principle similar to that men-
tioned.
</bodyText>
<subsectionHeader confidence="0.969191">
3.3.2 Making Clusters Disjoint
</subsectionHeader>
<bodyText confidence="0.999991846153846">
The purpose of the last phase of the algorithm is
converting a fuzzy cluster result to its crisp equiva-
lent. In order to identify the best cluster for a doc-
ument contained in multiple clusters, the authors
define the scoring function illustrated in the equa-
tion of Figure 3, where x is a global and cluster-
frequent item in dock, x&apos; a global frequent but not
cluster frequent item in dock, and n(x) a weighted
frequency (TF.IDF) of feature x in dock.
Using this function, the best cluster for a partic-
ular document is that which maximises the score.
In case of a tie, the most specific cluster (having
the largest number of labels) is chosen.
</bodyText>
<sectionHeader confidence="0.99414" genericHeader="method">
4 Procedure
</sectionHeader>
<bodyText confidence="0.999989571428571">
The system was designed in two parts, namely a
simple web-based user interface and a server pro-
cess responsible for iterating sequentially over user
queries and performing the content retrieval and
summarisation tasks. The following sections de-
scribe the various sub-systems that compose the
server process.
</bodyText>
<subsectionHeader confidence="0.999672">
4.1 Content Retrieval
</subsectionHeader>
<bodyText confidence="0.998350875">
The Content Retrieval sub-system is responsible
for retrieving web documents related to a user’s
query. This is done simply by querying a search
engine and retrieving the top ranked documents2.
Although throughout the course of this study the
system was configured to use only Google as its
document source, the number of search engines
that can be queried is arbitrary, and the system can
</bodyText>
<footnote confidence="0.394219">
2It was empirically determined that retrieving the top 30
27 ranked documents achieved the best results. Considering
less documents meant that, in most scenarios, main relevant
sources were missed, whilst considering more documents
caused the infiltration of irrelevant information
</footnote>
<figure confidence="0.78713425">
1. Join
Ck = {X U X0 |X,X0 E Lk−1, |X n X0 |= k − 2}
2. Prune
Ck = {X E C0k  |X contains k members of Lk−1}
</figure>
<figureCaption confidence="0.911057">
Figure 2: Candidate Generation Algorithm by Mannila et al. (1994)
</figureCaption>
<figure confidence="0.481762">
EScore(Ci +— docj) = En(x) x cluster support(x) − n(x0) x global support(x 0)
x x1
</figure>
<figureCaption confidence="0.998916">
Figure 3: Definition of Scoring Function
</figureCaption>
<page confidence="0.990724">
28
</page>
<bodyText confidence="0.954014">
be given a set of parameters to query a particular
search engine.
</bodyText>
<subsectionHeader confidence="0.993745">
4.2 Content Extraction
</subsectionHeader>
<bodyText confidence="0.99994378125">
The Content Extraction module is responsible for
transforming the retrieved HTML documents into
raw text. However, a simple de-tagging process
is not sufficient. This module was designed so as
to be able to identify the main content of a web
document, and leave out other clutter such as nav-
igation menus and headings. Finn et al. (2001) in-
troduce a generic method to achieve this, by trans-
lating the content extraction problem to an optimi-
sation problem. The authors observe that, essen-
tially, an HTML document consists of two types of
elements, that is, actual text and HTML tags. Thus,
such a document can easily be encoded as a binary
string B, where 0 represents a natural word, whilst
1 represents and HTML tag. Figure 4 shows a typ-
ical graphical representation obtained when cumu-
lative HTML tag tokens are graphed against the
cumulative number of tokens in a typical HTML
document.
Finn et al. (2001) suggest that, typically, the
plateau that can be discerned in such a graph con-
tains the actual document content. Therefore, in
order to extract the content, the start and end point
of the plateau (marked with black boxes in Figure
, and referred to hereafter as i and j respectively)
must be identified.
The optimisation problem now becomes max-
imisation of the number of HTML tags below i
and above j, in parallel with maximisation of the
number of natural language words between i and
j. The maximisation formula proposed by the au-
thors is given by Equation 1.
</bodyText>
<figureCaption confidence="0.9961445">
Figure 4: Total HTML Tokens VS Total Tokens
(Finn et al., 2001)
</figureCaption>
<equation confidence="0.96744">
(1−Bn) (1)
</equation>
<bodyText confidence="0.999772647058824">
Our Content Extraction module is further de-
composed into three sub-modules. The first is a
pre-processing module, which parses out the body
of the HTML document, and removes superfluous
content such as scripts and styling sections. The
second and core sub-module consists namely of an
implementation of the content extraction method
introduced by Finn et al. (2001), which is pri-
marily responsible for identifying the main con-
tent section of the input document. The last post-
processing module then ensures that the output
from the previous sub-modules is converted to raw
text, by performing an HTML detagging process-
ing and also inserting paragraph marks in places
where they are explicit cf. HTML &lt;p&gt; tag) or
where an element from a predefined set of HTML
text break delimiters occurs.
</bodyText>
<table confidence="0.2931025">
Ti,j = Ei− 1 Bn+ E j (1−Bn)+ N−1E
n=0 n=i n=j+1
</table>
<subsectionHeader confidence="0.971558">
4.3 Summarisation
</subsectionHeader>
<bodyText confidence="0.999924833333333">
The overall design of the core summarisation mod-
ule is loosely based upon the two-tiered MDS
architecture introduced by Torralbo et al. (2005)
The following sections map our system to a sim-
ilar two-tiered architecture, and explain how each
module operates.
</bodyText>
<subsectionHeader confidence="0.889237">
Document Identification
</subsectionHeader>
<bodyText confidence="0.99974075">
Document Identification is trivial, since docu-
ments are explicitly defined by the content retrieval
module, the output of which is basically a set of
query-related text documents.
</bodyText>
<subsectionHeader confidence="0.930848">
Document Filtering
</subsectionHeader>
<bodyText confidence="0.9999845">
The job of Document Filtering is partially done
at the very beginning by the search engine. How-
ever, our system further refines the document col-
lection by pre-processing each document, apply-
ing a noise3 removal procedure, stemming and stop
word and rare word removal. Each document is
then converted to a bag of words, or the Vector
Space Model, where each word is associated with
its corresponding TF•IDF measure. Any docu-
ment which, after pre-processing, ends up with an
empty bag of words, is filtered out from the doc-
ument collection. Furthermore, in order to ensure
the robustness of the system especially in subse-
quent intensive processing, documents which are
longer than 5 times the average document length
are truncated.
</bodyText>
<subsectionHeader confidence="0.81245">
Paragraph Identification
</subsectionHeader>
<bodyText confidence="0.9999705">
As outlined in Section 4.2, the Content Extrac-
tion sub-system inserts paragraph indicators in the
text wherever appropriate. Thus, the paragraph
identification phase is trivial, and entails only split-
ting the content of a document at the indicated po-
sitions.
</bodyText>
<subsectionHeader confidence="0.865781">
Paragraph Clustering and Filtering
</subsectionHeader>
<bodyText confidence="0.944557909090909">
In contrast to the technique of Torralbo et al.
(2005), a paragraph filtering module was intro-
duced in order to select only the most informa-
tive, query-related paragraphs. To achieve this,
we implemented the clustering technique out-
lined in Section 3.3 in order to obtain clusters of
thematically-similar paragraphs, using the Global
Frequent ItemSet generation technique from Man-
nila et al. (1994) and setting the Minimum Global 29
3“Noise” refers to any character which is not in the En-
glish alphabet.
</bodyText>
<listItem confidence="0.995904277777778">
1. For each paragraph pk
(a) Initialise the target summary 5umk
as an empty text
(b) Let p = pk
(c) Remove the first sentence s from p,
and add it at the end of 5umk.
(d) Calculate the similarity between s
and the first sentence of all the para-
graphs, using the size of the inter-
section of the two vectors of words
as a similarity metric.
(e) Let p be the paragraph whose first
sentence maximises the similarity,
and go back to step (c) with that
paragraph. If the best similarity is
0, stop.
2. Choose the longest one of the k different
summaries.
</listItem>
<figureCaption confidence="0.999707">
Figure 5: Summary Generation Algorithm (Tor-
ralbo et al., 2005)
</figureCaption>
<bodyText confidence="0.998344466666667">
Support and Minimum Cluster Support parameters
to 35 and 50 respectively.
The filtering technique then consists of simply
choosing the largest cluster. This is based on the
intuition that most of the paragraphs having the
central theme as their main theme will get clus-
tered together. Therefore, choosing the largest
cluster of paragraphs would filter out irrelevant
paragraphs. This paragraph filtering method may
filter out paragraphs which are actually relevant,
however, we rely on the redundancy of informa-
tion usually found in information obtained from
the web. Thus, the paragraph filtering gives more
importance to filtering out all the irrelevant para-
graphs.
</bodyText>
<sectionHeader confidence="0.754023" genericHeader="method">
Summary Generation
</sectionHeader>
<bodyText confidence="0.998065">
The role of the summary generation module is
to generate a report from a cluster of paragraphs.
We based our summary generation method on that
used by Torralbo et al. (2005), which is illustrated
in Figure 5. However, in order to make it more
applicable to our problem domain and increase the
output quality, we introduced some improvements.
Sentence Ordering Model We introduced a
probabilistic sentence ordering model which en-
ables the algorithm to choose the sentence that
</bodyText>
<page confidence="0.996143">
30
</page>
<bodyText confidence="0.999768631578947">
maximises the probability given the previous sen-
tence. The sentence ordering model, based on a
method of probabilistic text structuring introduced
by Lapata (2003), is trained upon the whole doc-
ument collection. We used Minipar (Lin, 1993),
a dependency-based parser, in order to identify
verbs, nouns, verb dependencies and noun depen-
dencies. Using counts of these features and Sim-
ple Good-Turing smoothing (Gale and Sampson,
1995), we were able to construct a probabilistic
sentence ordering model such that, during sum-
mary generation, given the previous sentence, we
are able to identify the sentence which is the most
likely to occur from the pool of sentences appear-
ing at the beginning of the remaining paragraphs.
Sentence Filtering We also introduced at this
stage a method to filter out sentences that decrease
the coherency and fluency of the resultant sum-
mary. This is based on two criteria:
</bodyText>
<listItem confidence="0.744788">
1. Very low probability of occurrence
</listItem>
<bodyText confidence="0.99025475">
If the most likely next-occurring sentence that
is chosen and removed from a paragraph still
has a very low probabilistic score, it is not
added to the output summary.
</bodyText>
<sectionHeader confidence="0.947214" genericHeader="method">
2. Heuristics
</sectionHeader>
<bodyText confidence="0.9995545">
We also introduce a set of heuristics to filter
out sentences having a wrong construction or
sentences which would not make sense in a
given context. These heuristics include:
</bodyText>
<listItem confidence="0.994977090909091">
(a) Sentences with no verbs
(b) Sentences starting with an adverb and
occurring at a paragraph transition
within the summary
(c) Sentences occurring at a context switch4
within the summary and starting with a
word matched with a select list of words
that usually occur as anaphora
(d) Malformed sentences (including sen-
tences not starting with a capital letter
and very short sentences)
</listItem>
<sectionHeader confidence="0.996928" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.928795">
5.1 Automatic Evaluation
5.1.1 Coherence Evaluation
</subsectionHeader>
<bodyText confidence="0.998973">
In order to evaluate the local coherence of the re-
ports generated by the system, we employed an au-
</bodyText>
<footnote confidence="0.952394">
4Context Switch refers to scenarios where a candidate sen-
tence comes from a different document than that of the last
sentence in the summary.
</footnote>
<bodyText confidence="0.999133">
tomatic coherence evaluation method introduced
by Barzilay and Lapata (2005)5. The main objec-
tive of this part of the evaluation phase was to de-
termine the effect on output quality when param-
eters are varied, namely the minimum cluster sup-
port parameter for the clustering algorithm, and the
key phrase popularity.
From this evaluation, we empirically deter-
mined that the optimum minimum cluster support
threshold for this application is 50, whilst the qual-
ity of the output is directly proportional to the key-
word popularity.
</bodyText>
<subsubsectionHeader confidence="0.797408">
5.1.2 Keyword Density Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999971888888889">
Here we focused on determining whether the
secondary objective was achieved (cf. section 2).
We measured the frequency of occurrence of the
keyword phrase within the output, or more specifi-
cally, the keyword density. The average key phrase
density achieved by the system was 1.32%, when
taking into account (i) the original keyword phrase
and its constituent keywords, and (ii) secondary
keyword phrases and their constituents.
</bodyText>
<subsectionHeader confidence="0.999713">
5.2 Manual Quality Evaluation
</subsectionHeader>
<bodyText confidence="0.999962166666667">
In order to measure the quality of the output and
determine whether the objectives of the study was
achieved, three users were introduced to the sys-
tem and asked to grade the system, on a scale of
1-5, on several criteria. Table 1 illustrates the re-
sults obtained from this evaluation.
</bodyText>
<sectionHeader confidence="0.999573" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<subsectionHeader confidence="0.999963">
6.1 Interpretation of Results
</subsectionHeader>
<bodyText confidence="0.9994355">
In this section we will identify some conclusions
elicited from the results obtained from the evalua-
tion phase and illustrated in Section 5.
Automatic Coherence Evaluation The auto-
matic coherence evaluation tests, although, in this
application, the level of “coherence” indicated did
not match that of manual evaluation, provided
nonetheless a standard by which different outputs
from the system using different parameters and ap-
plication scenarios could be compared. From the
results, we could empirically determine that the
optimal value for the cluster support parameter was
around 50%. Furthermore, unsurprisingly, the sys-
tem tends to produce output of a higher quality
</bodyText>
<footnote confidence="0.991219666666667">
5Data required to set up the automatic coherence eval-
uation model was available from the author’s website
http://people.csail.mit.edu/regina/coherence/.
</footnote>
<table confidence="0.9859215">
Grammaticality Non-Redundancy Referential Clarity Focus Structure Naturalness Usefulness
Average 3.62 2.21 4.03 4.28 3.27 2.76 4.78
</table>
<tableCaption confidence="0.999871">
Table 1: Results of Manual Evaluation
</tableCaption>
<page confidence="0.999858">
31
</page>
<bodyText confidence="0.996655619047619">
in scenarios where the keyword phrase is popular,
and thus more data is available.
SEO Evaluation From an SEO perspective, it
was predictable that the system would produce
query-related text, since its data source is ob-
tained from query-related search engine results.
However, the resulting average keyword density
achieved is significant, and is at a level which is
totally acceptable by most search engine ranking
algorithms6.
Manual Quality Evaluation Due to limited re-
sources, the results of the manual evaluation pro-
cedure were not statistically significant since only
three users were involved in evaluating six sum-
maries. However, allowing for a factor of sub-
jectivity, some conclusions could still be elicited,
namely:
1. The system did not perform well enough to
have its output rated as high as a manual sum-
marisation procedure. This can be concluded
from the low rating on the output Naturalness
criterion, as well as from the presence of re-
peated and irrelevant content in some of the
output summaries.
2. The system performed acceptably well in
generating reports that were adequately co-
herent and high-level enough to give an
overview of concepts represented by users’
queries. This can be concluded from the av-
erage scores achieved in the Focus and Refer-
ential Clarity criteria.
3. The evaluators were also asked to give a grade
indicating whether this system and similar
tools would actually be useful. A positive
grade was obtained on this criterion, indicat-
ing that the system achieved the MDS objec-
tive, enabling users to get a brief overview
of the topic as well as facilitating document
identification.
When comparing these results to those achieved
by Torralbo et al. (2005), we can elicit two main
conclusions:
</bodyText>
<footnote confidence="0.976908">
6Very high keyword density (more than a threshold of 2%
- 5%) is usually considered as a spammy technique known as
keyword stuffing.
</footnote>
<bodyText confidence="0.910389607142857">
1. Although our system achieved lower rank-
ings on the Non-Redundancy, Structure and
Grammaticality criteria, these rankings were
not unacceptable. We could attributed this to
the more generic domain in which our sys-
tem operates, where it is not possible to in-
troduce fixed heuristics such as those used by
Torralbo et al. (2005) for avoiding repeated
information by replacing a term definition by
its corresponding acronym. Such heuristics
tend to be relevant in the context of such a
term definition system.
2. Our system achieved higher grades on the
Referential Clarity and Focus criteria. Given
the fact that the system of Torralbo et al.
(2005) retrieves results from search engines
in a similar way used by our system, the im-
provement Focus might be attributed to the
fact that our paragraph filtering methodol-
ogy tends to perform well in selecting only
the most relevant parts of the document base.
Furthermore, the improved grade achieved in
the Referential Clarity criterion might arise
from the more advanced sentence ordering
methodology used, as well as to the different
heuristic-based sentence filtering techniques
employed by our summary generation mod-
ule.
</bodyText>
<subsectionHeader confidence="0.99625">
6.2 Limitations
</subsectionHeader>
<bodyText confidence="0.999984388888889">
The main limitation is that the quality of the output
is very susceptible to the quality and amount of re-
sources available. However, we also noticed a se-
vere fall in quality where results were largely com-
posed of business-oriented portals, which tend to
lack textual information. Furthermore, the output
summary is largely dictated by the results of search
engines. Therefore, the queries submitted to the
system must be formulated similarly to those sub-
mitted to search engines, since the system would
fail to generate a focused summary for queries
which, when submitted to traditional search en-
gines, return irrelevant results.
The system performance is also limited by the
quality and number of external components being
referenced, which are not state of the art and which
introduce performance bottlenecks by imposing a
batch-processing regime.
</bodyText>
<subsectionHeader confidence="0.989922">
6.3 Final Conclusions
</subsectionHeader>
<bodyText confidence="0.999973105263158">
Our system combines several existing techniques
in a novel way. New techniques, such as our
Heuristic-Based Sentence Filtering algorithm, are
also introduced.
The primary objective of creating an MDS was
achieved albeit with limited “coherency”. How-
ever, our system was considered a useful research
tool - supporting the hypothesis that a partially co-
herent but understandable report with minimum
effort is arguably better than a perfectly coherent
one, if the latter is unrealistically laborious to pro-
duce.
The secondary SEO objective was also
achieved, to the extent that the system generated
query-related content that has a natural level of
key phrase density. Such content has the potential
of being considered query-related also by search
engine ranking algorithms, if published within the
right context.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="acknowledgments">
7 Future Work
</sectionHeader>
<bodyText confidence="0.997902">
There remains much is to be done. We propose:
</bodyText>
<listItem confidence="0.866594">
• To increase the output quality and natural-
ness by focusing on an a sub-system for
anaphora identification and resolution which
would complement our probabilistic sentence
ordering model.
• To widen the scope by applying the system to
sources of information other than web docu-
ments.
• To convert our batch-processing system to an
</listItem>
<bodyText confidence="0.607904">
interactive one by incorporating all the re-
quired tools within the same environment.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99978745">
Agrawal, R. and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules in large
databases. In VLDB’94, Proc. of 20th International
Conference on Very Large Data Bases, Sept. 1994,
Santiago de Chile, Chile, pages 487–499. Morgan
Kaufmann.
Clarke, J. 2004. Clustering techniques for multi-
document summarisation. Master’s thesis, Univer-
sity of Sheffield.
Edmunds, A. and A. Morris. 2000. The problem of
information overload in business organisations: a re-
view of the literature. Int. Journal of Information
Management, 20(1):17–28.
Eppler, M.J. and J. Mengis. 2004. The Concept of
Information Overload: A Review of Literature from
Organization Science, Accounting, Marketing, MIS,
and Related Disciplines. The Information Society,
20(5):325–344.
Evans, D.K., J.L. Klavans, and K.R. McKeown. 2004.
Columbia Newsblaster: Multilingual News Summa-
rization on the Web. Proc. HLT Conference and the
NAACL Annual Meeting.
Finn, A., N. Kushmerick, and B. Smyth. 2001. Fact
or fiction: Content classification for digital libraries.
In DELOS Workshop: Personalisation and Recom-
mender Systems in Digital Libraries.
Fung, B.C.M., K. Wang, and M. Ester. 2003. Hier-
archical Document Clustering Using Frequent Item-
sets. Proc. of the SIAM International Conference on
Data Mining, 30.
Gale, W.A. and G. Sampson. 1995. Good-Turing Fre-
quency Estimation Without Tears. Journal of Quan-
titative Linguistics, 2(3):217–237.
Google. 2007. Google Webmaster Guidelines.
http://www.google.com/support/webmasters
/bin/answer.py?answer=35769.
Lapata, M. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. Proc. of the 41st
Meeting of the ACL, pages 545–552.
Lin, Dekang. 1993. Principle-based parsing without
overgeneration. In Meeting of the ACL, pages 112–
120.
Mani, I. 2001. Automatic Summarization. Computa-
tional Linguistics, 28(2).
Mannila, Heikki, Hannu Toivonen, and A. Inkeri
Verkamo. 1994. Efficient algorithms for discov-
ering association rules. In Fayyad, Usama M. and
Ramasamy Uthurusamy, editors, AAAI Workshop
on Knowledge Discovery in Databases (KDD-94),
pages 181–192, Seattle, Washington. AAAI Press.
Torralbo, R., E. Alfonseca, A. Moreno-Sandoval, and
J.M. Guirao. 2005. Automatic generation of
term definitions using multidocument summarisa-
tion from the Web. In Proc. Workshop on Crossing
Barriers in Text Summarisation Research, RANLP
Borovets.
Barzilay, R. and M. Lapata. 2005. Modeling local 32 Vaughn. 2007. Google Ranking Factors
coherence: an entity-based approach. In ACL ’05: - SEO Checklist. http://www.vaughns-1-
Proc. 43rd Annual Meeting of the ACL, pages 141– pagers.com/internet/google-ranking-factors.htm.
148, Morristown, NJ, USA. ACL.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510992">
<title confidence="0.999269">MultiSum Query-Based Multi-Document Summarisation</title>
<author confidence="0.999982">Michael Rosner Carl Camilleri</author>
<affiliation confidence="0.9980525">Dept. Artificial Intelligence Dept. Artificial Intelligence University of Malta University of Malta</affiliation>
<email confidence="0.590128">mike.rosner@um.edu.mtccam0002@um.edu.mt</email>
<abstract confidence="0.988408583333333">This paper describes a generic, opendomain multi-document summarisation system which combines new and existing techniques in a novel way. The system is capable of automatically identifying query-related online documents and compiling a report from the most useful sources, whilst presenting the result in such a way as to make it easy for the researcher to look up the information in its original context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Fast algorithms for mining association rules in large databases.</title>
<date>1994</date>
<booktitle>In VLDB’94, Proc. of 20th International Conference on Very Large Data Bases,</booktitle>
<pages>487--499</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Santiago de Chile, Chile,</location>
<contexts>
<context position="8690" citStr="Agrawal and Srikant (1994)" startWordPosition="1355" endWordPosition="1358">initions, it is now possible to describe into more detail the core non-trivial phases of the algorithm. 3.3.1 Discovering Global Frequent ItemSets From the definition of an ItemSet, it can be concluded that the set of ItemSets is the power set of all features1 within the document collection. Given even a small document collection, enumerating all the possible ItemSets and checking which of them are Global Frequent would be intractable. In order to discover Global Frequent ItemSets, the authors recommend the use of the Apriori Candidate Generation algorithm, a data mining algorithm proposed by Agrawal and Srikant (1994). This algo1Features here constitute distinct, single words found in the whole document collection. In practice, stemming is applied before feature extraction. rithm defines a way to reduce the number of candidate frequent ItemSets generated. The generation algorithm basically operates on the principle that, given a set of frequent k-1-ItemSets, a set of candidate frequent k-ItemSets can be generated such that each candidate is composed of frequent k-1- ItemSets. Agrawal and Srikant (1994) also mention a similar algorithm proposed by Mannila et al. (1994). As illustrated in Figure 2, this algo</context>
</contexts>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>Agrawal, R. and Ramakrishnan Srikant. 1994. Fast algorithms for mining association rules in large databases. In VLDB’94, Proc. of 20th International Conference on Very Large Data Bases, Sept. 1994, Santiago de Chile, Chile, pages 487–499. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
</authors>
<title>Clustering techniques for multidocument summarisation. Master’s thesis,</title>
<date>2004</date>
<institution>University of Sheffield.</institution>
<contexts>
<context position="5137" citStr="Clarke, 2004" startWordPosition="800" endWordPosition="801">atic MDS systems. These vary according to the problem domain, which usually defines particular formats for both input and output. However, five basic sub-systems of any MDS system can be identified (Mani, 2001). 1. Unit Identification During this first phase, input documents are parsed and tokenised into “units”, which can vary from single words to whole documents, according to the application problem. 2. Unit Matching (Clustering) The second stage involves grouping similar units together. In the context of MDS, similar units usually mean either identical or informationallyequivalent strings (Clarke, 2004), with the purpose of discovering the main themes in the different units and identify the most salient ones. 3. Unit Filtering The filtering stage eliminates units residing in clusters which are deemed to be non-salient. 4. Compacting During this phase, it is often assumed that different clusters contain similar units. Thus, a sample of units from different clusters is chosen. 5. Presentation/Summary Generation The last phase of the MDS process involves using the output from the Compacting stage, and generating a summary. Usually, naive string concatenation does not produce coherent summaries </context>
</contexts>
<marker>Clarke, 2004</marker>
<rawString>Clarke, J. 2004. Clustering techniques for multidocument summarisation. Master’s thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Edmunds</author>
<author>A Morris</author>
</authors>
<title>The problem of information overload in business organisations: a review of the literature.</title>
<date>2000</date>
<journal>Int. Journal of Information Management,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="841" citStr="Edmunds and Morris, 2000" startWordPosition="116" endWordPosition="119">u.mt Abstract This paper describes a generic, opendomain multi-document summarisation system which combines new and existing techniques in a novel way. The system is capable of automatically identifying query-related online documents and compiling a report from the most useful sources, whilst presenting the result in such a way as to make it easy for the researcher to look up the information in its original context. 1 Introduction Although electronic resources have several inherent advantages over traditional research media, they also introduce several drawbacks, such as Information Overload (Edmunds and Morris, 2000),which has become synonymous with the information retrieval phase of any research-related task. Another problem which is directly related to the one just described is that of Source Identification (Eppler and Mengis, 2004). This refers to the problem of having relevant results intermingled with results that are less relevant, or actually irrelevant. Lastly, the researcher usually has to also manually traverse the relevant sources of information in order to form an answer to the research query. These problems have led to the study of various areas in computing, all of which aim to try and minim</context>
</contexts>
<marker>Edmunds, Morris, 2000</marker>
<rawString>Edmunds, A. and A. Morris. 2000. The problem of information overload in business organisations: a review of the literature. Int. Journal of Information Management, 20(1):17–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Eppler</author>
<author>J Mengis</author>
</authors>
<title>The Concept of Information Overload: A Review of Literature from Organization Science, Accounting, Marketing, MIS, and Related Disciplines. The Information Society,</title>
<date>2004</date>
<contexts>
<context position="1063" citStr="Eppler and Mengis, 2004" startWordPosition="150" endWordPosition="153"> documents and compiling a report from the most useful sources, whilst presenting the result in such a way as to make it easy for the researcher to look up the information in its original context. 1 Introduction Although electronic resources have several inherent advantages over traditional research media, they also introduce several drawbacks, such as Information Overload (Edmunds and Morris, 2000),which has become synonymous with the information retrieval phase of any research-related task. Another problem which is directly related to the one just described is that of Source Identification (Eppler and Mengis, 2004). This refers to the problem of having relevant results intermingled with results that are less relevant, or actually irrelevant. Lastly, the researcher usually has to also manually traverse the relevant sources of information in order to form an answer to the research query. These problems have led to the study of various areas in computing, all of which aim to try and minimise the manual effort of information retrieval and extraction, one of which is Multi-Document Summarisation (MDS). The core aim of any MDS system is that of processing multiple sources of information and outputting a relat</context>
</contexts>
<marker>Eppler, Mengis, 2004</marker>
<rawString>Eppler, M.J. and J. Mengis. 2004. The Concept of Information Overload: A Review of Literature from Organization Science, Accounting, Marketing, MIS, and Related Disciplines. The Information Society, 20(5):325–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Evans</author>
<author>J L Klavans</author>
<author>K R McKeown</author>
</authors>
<title>Columbia Newsblaster: Multilingual News Summarization on the Web.</title>
<date>2004</date>
<booktitle>Proc. HLT Conference and the NAACL Annual Meeting.</booktitle>
<contexts>
<context position="1827" citStr="Evans et al., 2004" startWordPosition="278" endWordPosition="281">earcher usually has to also manually traverse the relevant sources of information in order to form an answer to the research query. These problems have led to the study of various areas in computing, all of which aim to try and minimise the manual effort of information retrieval and extraction, one of which is Multi-Document Summarisation (MDS). The core aim of any MDS system is that of processing multiple sources of information and outputting a relatively brief but broad report or summary. Uses of MDS systems vary widely, from summarisation of closed-domain documents, such as news documents (Evans et al., 2004), to aggregation of information from several sources in an open domain. 2 Aims and Objectives MDS techniques can be used in various tools that may help addressing the problems described in Section 1. On the other hand, a brief study of the relevant literature indicates that the majority of the work done in this area concerns closed-domains such as news summarisation, which is perhaps the reason why such tools have not yet become more popular. The objectives of this study are thus twofold. • The primary objective is that of designing, implementing and evaluating an opendomain, query-based MDS s</context>
</contexts>
<marker>Evans, Klavans, McKeown, 2004</marker>
<rawString>Evans, D.K., J.L. Klavans, and K.R. McKeown. 2004. Columbia Newsblaster: Multilingual News Summarization on the Web. Proc. HLT Conference and the NAACL Annual Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finn</author>
<author>N Kushmerick</author>
<author>B Smyth</author>
</authors>
<title>Fact or fiction: Content classification for digital libraries.</title>
<date>2001</date>
<booktitle>In DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries.</booktitle>
<contexts>
<context position="11826" citStr="Finn et al. (2001)" startWordPosition="1878" endWordPosition="1881">igure 2: Candidate Generation Algorithm by Mannila et al. (1994) EScore(Ci +— docj) = En(x) x cluster support(x) − n(x0) x global support(x 0) x x1 Figure 3: Definition of Scoring Function 28 be given a set of parameters to query a particular search engine. 4.2 Content Extraction The Content Extraction module is responsible for transforming the retrieved HTML documents into raw text. However, a simple de-tagging process is not sufficient. This module was designed so as to be able to identify the main content of a web document, and leave out other clutter such as navigation menus and headings. Finn et al. (2001) introduce a generic method to achieve this, by translating the content extraction problem to an optimisation problem. The authors observe that, essentially, an HTML document consists of two types of elements, that is, actual text and HTML tags. Thus, such a document can easily be encoded as a binary string B, where 0 represents a natural word, whilst 1 represents and HTML tag. Figure 4 shows a typical graphical representation obtained when cumulative HTML tag tokens are graphed against the cumulative number of tokens in a typical HTML document. Finn et al. (2001) suggest that, typically, the </context>
<context position="13399" citStr="Finn et al. (2001)" startWordPosition="2147" endWordPosition="2150">ow i and above j, in parallel with maximisation of the number of natural language words between i and j. The maximisation formula proposed by the authors is given by Equation 1. Figure 4: Total HTML Tokens VS Total Tokens (Finn et al., 2001) (1−Bn) (1) Our Content Extraction module is further decomposed into three sub-modules. The first is a pre-processing module, which parses out the body of the HTML document, and removes superfluous content such as scripts and styling sections. The second and core sub-module consists namely of an implementation of the content extraction method introduced by Finn et al. (2001), which is primarily responsible for identifying the main content section of the input document. The last postprocessing module then ensures that the output from the previous sub-modules is converted to raw text, by performing an HTML detagging processing and also inserting paragraph marks in places where they are explicit cf. HTML &lt;p&gt; tag) or where an element from a predefined set of HTML text break delimiters occurs. Ti,j = Ei− 1 Bn+ E j (1−Bn)+ N−1E n=0 n=i n=j+1 4.3 Summarisation The overall design of the core summarisation module is loosely based upon the two-tiered MDS architecture intro</context>
</contexts>
<marker>Finn, Kushmerick, Smyth, 2001</marker>
<rawString>Finn, A., N. Kushmerick, and B. Smyth. 2001. Fact or fiction: Content classification for digital libraries. In DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C M Fung</author>
<author>K Wang</author>
<author>M Ester</author>
</authors>
<title>Hierarchical Document Clustering Using Frequent Itemsets.</title>
<date>2003</date>
<booktitle>Proc. of the SIAM International Conference on Data Mining,</booktitle>
<pages>30</pages>
<contexts>
<context position="6470" citStr="Fung et al., 2003" startWordPosition="998" endWordPosition="1001">Clustering Techniques As outlined in Section 3.2, MDS often makes use of clustering techniques in order to group together similar units. Clustering can be defined as a process which performs “unsupervised classification of patterns into groups based on their similarity” (Clarke, 2004). A particular clustering technique typically consists of three main components: 1. Pattern Representation 2. Similarity Measure 3. Clustering Algorithm The very generic nature of our problem domain requires a clustering technique which is both suitable and without scenario-dependant parameters. Fung’s algorithm (Fung et al., 2003), comprising a pre-processing stage and a further three-phase core process, uses the following concepts, and is briefly described in Figure 1. ItemSet A set of words occurring together within a document. An ItemSet composed of k words is called a k-ItemSet. Global Support The Global Support of a word item is the number of documents from the document collection it appears in (cf. document frequency). Cluster Support The Cluster Support of a word item is the number of documents within a cluster it appears in. 1. Pre-Processing - stem, remove stop words and convert to TFxIDF representation 2. Dis</context>
</contexts>
<marker>Fung, Wang, Ester, 2003</marker>
<rawString>Fung, B.C.M., K. Wang, and M. Ester. 2003. Hierarchical Document Clustering Using Frequent Itemsets. Proc. of the SIAM International Conference on Data Mining, 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>G Sampson</author>
</authors>
<title>Good-Turing Frequency Estimation Without Tears.</title>
<date>1995</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="18131" citStr="Gale and Sampson, 1995" startWordPosition="2916" endWordPosition="2919"> the output quality, we introduced some improvements. Sentence Ordering Model We introduced a probabilistic sentence ordering model which enables the algorithm to choose the sentence that 30 maximises the probability given the previous sentence. The sentence ordering model, based on a method of probabilistic text structuring introduced by Lapata (2003), is trained upon the whole document collection. We used Minipar (Lin, 1993), a dependency-based parser, in order to identify verbs, nouns, verb dependencies and noun dependencies. Using counts of these features and Simple Good-Turing smoothing (Gale and Sampson, 1995), we were able to construct a probabilistic sentence ordering model such that, during summary generation, given the previous sentence, we are able to identify the sentence which is the most likely to occur from the pool of sentences appearing at the beginning of the remaining paragraphs. Sentence Filtering We also introduced at this stage a method to filter out sentences that decrease the coherency and fluency of the resultant summary. This is based on two criteria: 1. Very low probability of occurrence If the most likely next-occurring sentence that is chosen and removed from a paragraph stil</context>
</contexts>
<marker>Gale, Sampson, 1995</marker>
<rawString>Gale, W.A. and G. Sampson. 1995. Good-Turing Frequency Estimation Without Tears. Journal of Quantitative Linguistics, 2(3):217–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Google Webmaster Guidelines.</title>
<date>2007</date>
<note>http://www.google.com/support/webmasters /bin/answer.py?answer=35769.</note>
<contexts>
<context position="4054" citStr="Google (2007)" startWordPosition="639" endWordPosition="640">ours is 26 the quality of output, as measured by a number of different linguistic and non-linguistic criteria (see Section 5). We have adopted a number of novel techniques to address this such as • Multi-Layered Architecture • Sentence Ordering Model • Heuristic Sentence Filtering • Paragraph Clustering 3 Background 3.1 Search Engine Ranking Criteria Search engine ranking algorithms vary, and are continuously being optimised in order to provide better and more accurate results. However, some guidelines that outline factors which web masters need to take into account have been established (cf. Google (2007), Vaughn (2007)). When ranking documents for a particular search query, ranking algorithms take into account both on-page and off-page factors. Off-page factors comprise mainly the number and quality of inbound links to a particular page, whilst on-page factors comprise various criteria, most important of which is the relevance of the content to the search query. 3.2 Multi-Document Summarisation Several different approaches and processes have been developed in automatic MDS systems. These vary according to the problem domain, which usually defines particular formats for both input and output. </context>
</contexts>
<marker>Google, 2007</marker>
<rawString>Google. 2007. Google Webmaster Guidelines. http://www.google.com/support/webmasters /bin/answer.py?answer=35769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>Proc. of the 41st Meeting of the ACL,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="17862" citStr="Lapata (2003)" startWordPosition="2876" endWordPosition="2877">ation module is to generate a report from a cluster of paragraphs. We based our summary generation method on that used by Torralbo et al. (2005), which is illustrated in Figure 5. However, in order to make it more applicable to our problem domain and increase the output quality, we introduced some improvements. Sentence Ordering Model We introduced a probabilistic sentence ordering model which enables the algorithm to choose the sentence that 30 maximises the probability given the previous sentence. The sentence ordering model, based on a method of probabilistic text structuring introduced by Lapata (2003), is trained upon the whole document collection. We used Minipar (Lin, 1993), a dependency-based parser, in order to identify verbs, nouns, verb dependencies and noun dependencies. Using counts of these features and Simple Good-Turing smoothing (Gale and Sampson, 1995), we were able to construct a probabilistic sentence ordering model such that, during summary generation, given the previous sentence, we are able to identify the sentence which is the most likely to occur from the pool of sentences appearing at the beginning of the remaining paragraphs. Sentence Filtering We also introduced at t</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Lapata, M. 2003. Probabilistic text structuring: Experiments with sentence ordering. Proc. of the 41st Meeting of the ACL, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Meeting of the ACL,</booktitle>
<pages>112--120</pages>
<contexts>
<context position="17938" citStr="Lin, 1993" startWordPosition="2889" endWordPosition="2890"> summary generation method on that used by Torralbo et al. (2005), which is illustrated in Figure 5. However, in order to make it more applicable to our problem domain and increase the output quality, we introduced some improvements. Sentence Ordering Model We introduced a probabilistic sentence ordering model which enables the algorithm to choose the sentence that 30 maximises the probability given the previous sentence. The sentence ordering model, based on a method of probabilistic text structuring introduced by Lapata (2003), is trained upon the whole document collection. We used Minipar (Lin, 1993), a dependency-based parser, in order to identify verbs, nouns, verb dependencies and noun dependencies. Using counts of these features and Simple Good-Turing smoothing (Gale and Sampson, 1995), we were able to construct a probabilistic sentence ordering model such that, during summary generation, given the previous sentence, we are able to identify the sentence which is the most likely to occur from the pool of sentences appearing at the beginning of the remaining paragraphs. Sentence Filtering We also introduced at this stage a method to filter out sentences that decrease the coherency and f</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>Lin, Dekang. 1993. Principle-based parsing without overgeneration. In Meeting of the ACL, pages 112– 120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="4734" citStr="Mani, 2001" startWordPosition="741" endWordPosition="742">y, ranking algorithms take into account both on-page and off-page factors. Off-page factors comprise mainly the number and quality of inbound links to a particular page, whilst on-page factors comprise various criteria, most important of which is the relevance of the content to the search query. 3.2 Multi-Document Summarisation Several different approaches and processes have been developed in automatic MDS systems. These vary according to the problem domain, which usually defines particular formats for both input and output. However, five basic sub-systems of any MDS system can be identified (Mani, 2001). 1. Unit Identification During this first phase, input documents are parsed and tokenised into “units”, which can vary from single words to whole documents, according to the application problem. 2. Unit Matching (Clustering) The second stage involves grouping similar units together. In the context of MDS, similar units usually mean either identical or informationallyequivalent strings (Clarke, 2004), with the purpose of discovering the main themes in the different units and identify the most salient ones. 3. Unit Filtering The filtering stage eliminates units residing in clusters which are de</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, I. 2001. Automatic Summarization. Computational Linguistics, 28(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heikki Mannila</author>
<author>Hannu Toivonen</author>
<author>A Inkeri Verkamo</author>
</authors>
<title>Efficient algorithms for discovering association rules.</title>
<date>1994</date>
<booktitle>AAAI Workshop on Knowledge Discovery in Databases (KDD-94),</booktitle>
<pages>181--192</pages>
<editor>In Fayyad, Usama M. and Ramasamy Uthurusamy, editors,</editor>
<publisher>AAAI Press.</publisher>
<location>Seattle, Washington.</location>
<contexts>
<context position="9251" citStr="Mannila et al. (1994)" startWordPosition="1443" endWordPosition="1446">data mining algorithm proposed by Agrawal and Srikant (1994). This algo1Features here constitute distinct, single words found in the whole document collection. In practice, stemming is applied before feature extraction. rithm defines a way to reduce the number of candidate frequent ItemSets generated. The generation algorithm basically operates on the principle that, given a set of frequent k-1-ItemSets, a set of candidate frequent k-ItemSets can be generated such that each candidate is composed of frequent k-1- ItemSets. Agrawal and Srikant (1994) also mention a similar algorithm proposed by Mannila et al. (1994). As illustrated in Figure 2, this algorithm consists of first generating candidates, and then pruning the result based on a principle similar to that mentioned. 3.3.2 Making Clusters Disjoint The purpose of the last phase of the algorithm is converting a fuzzy cluster result to its crisp equivalent. In order to identify the best cluster for a document contained in multiple clusters, the authors define the scoring function illustrated in the equation of Figure 3, where x is a global and clusterfrequent item in dock, x&apos; a global frequent but not cluster frequent item in dock, and n(x) a weighte</context>
<context position="11272" citStr="Mannila et al. (1994)" startWordPosition="1783" endWordPosition="1786">e of this study the system was configured to use only Google as its document source, the number of search engines that can be queried is arbitrary, and the system can 2It was empirically determined that retrieving the top 30 27 ranked documents achieved the best results. Considering less documents meant that, in most scenarios, main relevant sources were missed, whilst considering more documents caused the infiltration of irrelevant information 1. Join Ck = {X U X0 |X,X0 E Lk−1, |X n X0 |= k − 2} 2. Prune Ck = {X E C0k |X contains k members of Lk−1} Figure 2: Candidate Generation Algorithm by Mannila et al. (1994) EScore(Ci +— docj) = En(x) x cluster support(x) − n(x0) x global support(x 0) x x1 Figure 3: Definition of Scoring Function 28 be given a set of parameters to query a particular search engine. 4.2 Content Extraction The Content Extraction module is responsible for transforming the retrieved HTML documents into raw text. However, a simple de-tagging process is not sufficient. This module was designed so as to be able to identify the main content of a web document, and leave out other clutter such as navigation menus and headings. Finn et al. (2001) introduce a generic method to achieve this, b</context>
<context position="15815" citStr="Mannila et al. (1994)" startWordPosition="2531" endWordPosition="2535">aragraph indicators in the text wherever appropriate. Thus, the paragraph identification phase is trivial, and entails only splitting the content of a document at the indicated positions. Paragraph Clustering and Filtering In contrast to the technique of Torralbo et al. (2005), a paragraph filtering module was introduced in order to select only the most informative, query-related paragraphs. To achieve this, we implemented the clustering technique outlined in Section 3.3 in order to obtain clusters of thematically-similar paragraphs, using the Global Frequent ItemSet generation technique from Mannila et al. (1994) and setting the Minimum Global 29 3“Noise” refers to any character which is not in the English alphabet. 1. For each paragraph pk (a) Initialise the target summary 5umk as an empty text (b) Let p = pk (c) Remove the first sentence s from p, and add it at the end of 5umk. (d) Calculate the similarity between s and the first sentence of all the paragraphs, using the size of the intersection of the two vectors of words as a similarity metric. (e) Let p be the paragraph whose first sentence maximises the similarity, and go back to step (c) with that paragraph. If the best similarity is 0, stop. 2</context>
</contexts>
<marker>Mannila, Toivonen, Verkamo, 1994</marker>
<rawString>Mannila, Heikki, Hannu Toivonen, and A. Inkeri Verkamo. 1994. Efficient algorithms for discovering association rules. In Fayyad, Usama M. and Ramasamy Uthurusamy, editors, AAAI Workshop on Knowledge Discovery in Databases (KDD-94), pages 181–192, Seattle, Washington. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Torralbo</author>
<author>E Alfonseca</author>
<author>A Moreno-Sandoval</author>
<author>J M Guirao</author>
</authors>
<title>Automatic generation of term definitions using multidocument summarisation from the Web. In</title>
<date>2005</date>
<booktitle>Proc. Workshop on Crossing Barriers in Text Summarisation Research, RANLP Borovets.</booktitle>
<contexts>
<context position="14030" citStr="Torralbo et al. (2005)" startWordPosition="2255" endWordPosition="2258">s primarily responsible for identifying the main content section of the input document. The last postprocessing module then ensures that the output from the previous sub-modules is converted to raw text, by performing an HTML detagging processing and also inserting paragraph marks in places where they are explicit cf. HTML &lt;p&gt; tag) or where an element from a predefined set of HTML text break delimiters occurs. Ti,j = Ei− 1 Bn+ E j (1−Bn)+ N−1E n=0 n=i n=j+1 4.3 Summarisation The overall design of the core summarisation module is loosely based upon the two-tiered MDS architecture introduced by Torralbo et al. (2005) The following sections map our system to a similar two-tiered architecture, and explain how each module operates. Document Identification Document Identification is trivial, since documents are explicitly defined by the content retrieval module, the output of which is basically a set of query-related text documents. Document Filtering The job of Document Filtering is partially done at the very beginning by the search engine. However, our system further refines the document collection by pre-processing each document, applying a noise3 removal procedure, stemming and stop word and rare word rem</context>
<context position="15471" citStr="Torralbo et al. (2005)" startWordPosition="2480" endWordPosition="2483">g of words, is filtered out from the document collection. Furthermore, in order to ensure the robustness of the system especially in subsequent intensive processing, documents which are longer than 5 times the average document length are truncated. Paragraph Identification As outlined in Section 4.2, the Content Extraction sub-system inserts paragraph indicators in the text wherever appropriate. Thus, the paragraph identification phase is trivial, and entails only splitting the content of a document at the indicated positions. Paragraph Clustering and Filtering In contrast to the technique of Torralbo et al. (2005), a paragraph filtering module was introduced in order to select only the most informative, query-related paragraphs. To achieve this, we implemented the clustering technique outlined in Section 3.3 in order to obtain clusters of thematically-similar paragraphs, using the Global Frequent ItemSet generation technique from Mannila et al. (1994) and setting the Minimum Global 29 3“Noise” refers to any character which is not in the English alphabet. 1. For each paragraph pk (a) Initialise the target summary 5umk as an empty text (b) Let p = pk (c) Remove the first sentence s from p, and add it at </context>
<context position="17393" citStr="Torralbo et al. (2005)" startWordPosition="2802" endWordPosition="2805"> their main theme will get clustered together. Therefore, choosing the largest cluster of paragraphs would filter out irrelevant paragraphs. This paragraph filtering method may filter out paragraphs which are actually relevant, however, we rely on the redundancy of information usually found in information obtained from the web. Thus, the paragraph filtering gives more importance to filtering out all the irrelevant paragraphs. Summary Generation The role of the summary generation module is to generate a report from a cluster of paragraphs. We based our summary generation method on that used by Torralbo et al. (2005), which is illustrated in Figure 5. However, in order to make it more applicable to our problem domain and increase the output quality, we introduced some improvements. Sentence Ordering Model We introduced a probabilistic sentence ordering model which enables the algorithm to choose the sentence that 30 maximises the probability given the previous sentence. The sentence ordering model, based on a method of probabilistic text structuring introduced by Lapata (2003), is trained upon the whole document collection. We used Minipar (Lin, 1993), a dependency-based parser, in order to identify verbs</context>
<context position="23734" citStr="Torralbo et al. (2005)" startWordPosition="3809" endWordPosition="3812">ts that were adequately coherent and high-level enough to give an overview of concepts represented by users’ queries. This can be concluded from the average scores achieved in the Focus and Referential Clarity criteria. 3. The evaluators were also asked to give a grade indicating whether this system and similar tools would actually be useful. A positive grade was obtained on this criterion, indicating that the system achieved the MDS objective, enabling users to get a brief overview of the topic as well as facilitating document identification. When comparing these results to those achieved by Torralbo et al. (2005), we can elicit two main conclusions: 6Very high keyword density (more than a threshold of 2% - 5%) is usually considered as a spammy technique known as keyword stuffing. 1. Although our system achieved lower rankings on the Non-Redundancy, Structure and Grammaticality criteria, these rankings were not unacceptable. We could attributed this to the more generic domain in which our system operates, where it is not possible to introduce fixed heuristics such as those used by Torralbo et al. (2005) for avoiding repeated information by replacing a term definition by its corresponding acronym. Such </context>
</contexts>
<marker>Torralbo, Alfonseca, Moreno-Sandoval, Guirao, 2005</marker>
<rawString>Torralbo, R., E. Alfonseca, A. Moreno-Sandoval, and J.M. Guirao. 2005. Automatic generation of term definitions using multidocument summarisation from the Web. In Proc. Workshop on Crossing Barriers in Text Summarisation Research, RANLP Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling local 32 Vaughn.</title>
<date>2005</date>
<booktitle>In ACL ’05: - SEO Checklist. http://www.vaughns-1-Proc. 43rd Annual Meeting of the ACL,</booktitle>
<pages>141--148</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19766" citStr="Barzilay and Lapata (2005)" startWordPosition="3185" endWordPosition="3188">urring at a context switch4 within the summary and starting with a word matched with a select list of words that usually occur as anaphora (d) Malformed sentences (including sentences not starting with a capital letter and very short sentences) 5 Evaluation 5.1 Automatic Evaluation 5.1.1 Coherence Evaluation In order to evaluate the local coherence of the reports generated by the system, we employed an au4Context Switch refers to scenarios where a candidate sentence comes from a different document than that of the last sentence in the summary. tomatic coherence evaluation method introduced by Barzilay and Lapata (2005)5. The main objective of this part of the evaluation phase was to determine the effect on output quality when parameters are varied, namely the minimum cluster support parameter for the clustering algorithm, and the key phrase popularity. From this evaluation, we empirically determined that the optimum minimum cluster support threshold for this application is 50, whilst the quality of the output is directly proportional to the keyword popularity. 5.1.2 Keyword Density Evaluation Here we focused on determining whether the secondary objective was achieved (cf. section 2). We measured the frequen</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Barzilay, R. and M. Lapata. 2005. Modeling local 32 Vaughn. 2007. Google Ranking Factors coherence: an entity-based approach. In ACL ’05: - SEO Checklist. http://www.vaughns-1-Proc. 43rd Annual Meeting of the ACL, pages 141– pagers.com/internet/google-ranking-factors.htm. 148, Morristown, NJ, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>