<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.847917">
Generating Vague Descriptions
</title>
<author confidence="0.729075">
Kees van Deemter
</author>
<affiliation confidence="0.7614875">
ITRI, University of Brighton
Lewes Road, Watts Building
</affiliation>
<address confidence="0.991213">
Brighton BN2 4W, United Kingdom
</address>
<email confidence="0.99877">
Kees.van.Deemter@itri.brighton.ac.uk
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840764705882">
This paper deals with the generation of definite
(i.e., uniquely referring) descriptions containing se-
mantically vague expressions (&apos;large&apos;, &apos;small&apos;, etc.).
Firstly, the paper proposes a semantic analysis of
vague descriptions that does justice to the context-
dependent meaning of the vague expressions in
them. Secondly, the paper shows how this semantic
analysis can be implemented using a modification of
the Dale and Reiter (1995) algorithm for the gener-
ation of referring expressions. A notable feature of
the new algorithm is that, unlike Dale and Reiter
(1995), it covers plural as well as singular NPs. This
algorithm has been implemented in an experimental
NLG program using PronT. The paper concludes by
formulating some pragmatic constraints that could
allow a generator to choose between different seman-
tically correct descriptions.
</bodyText>
<sectionHeader confidence="0.874442" genericHeader="method">
1 Introduction: Vague properties
</sectionHeader>
<bodyText confidence="0.979525764705882">
and Gradable Adjectives
Some properties can apply to an object to a greater
or lesser degree. Such continuous, or vague proper-
ties, which can be expressed by, among other pos-
sibilities, gradable adjectives (e_g., &apos;small&apos;, &apos;large&apos;,
e.g. Quirk et al. 1972 sections 5.5 and 5.39), pose a
difficult challenge to existing semantic theories, the-
oretical as well as computational. The problems are
caused partly by the extreme context-dependence of
the expressions involved, and partly by the resis-
tance of vague properties to discrete mathematical
modeling (e.g., Synthese 1973, Pirtkal 1995). The
weight of these problems is increased by fact that
vague expressions are ubiquitous in many domains.
The present paper demonstrates how a Natural Lan-
guage Generation (NLG) program can be enabled to
- generate uniquely referring descriptions containing
one gradable adjective, despite the vagueness of the
adjective. Having presented a semantic analysis for
such vague descriptions, we describe the semantic
core of an N 1,C; algorithm that has numerical data as
input and vague (uniquely referring) descriptions as
output.
One property setting our treatment of vagueness
apart from that in other NLC programs-(e.g. Gold-
berg 1994) is that it uses -vague properties for an
exact task, namely the ruling out of distractors in
referring expressions (Dale and Reiter 1995). An-
other distinctive property is that our account allows
the &apos;meaning&apos; of vague expressions to be determined
by a combination of linguistic context (i.e., the Com-
mon Noun following the adjective) and nonlinguistic
context (i.e., the properties of the elements in the
domain).
</bodyText>
<sectionHeader confidence="0.987609" genericHeader="method">
2 The Meaning of Vague
, Descriptions
</sectionHeader>
<bodyText confidence="0.929698590909091">
Several different analyses are possible of what it
means to be, for example, large&apos;: larger than aver-
age, larger than most, etc. But there is not necess-
rily just one correct analysis. Consider a domain of
four mice, sized 2,5,7, and 10cm.&apos; In this case, for
example, one can speak of
1. The large mouse
(= the one whose size is 10cm), and of
2. The two large mice
(= the two whose sizes are 7 and 10cm).
Clearly, what it takes to be large has not been writ-
ten in stone: the speaker may decide that 7cm is
enough (as in (2)), or she may set the standards
higher (as in (1)). A numeral (explicit, or implicit
as in (1)), allows the reader to make inferences about
the standards employed by the speaker.2 More pre-
cisely, it appears that in a definite description, the
absolute form of the adjective is semantically emu,:
alent with the superlative form:
The n large mice - The largest Ti mice
The large mice - The largest mice
The large mouse - The largest mouse.
</bodyText>
<footnote confidence="0.981262285714286">
1For simplicity, the adjectives involved will be assumed
to be one-dimensional. Note that the degree of precision re-
flected by the units of measurement affects the descriptions
generated, and even the objects or sets) that can be de-
scribed, since it determines which nhjects count as having
the same size.
=Thanks are due to Matthew Stone for this observation,
</footnote>
<page confidence="0.998522">
179
</page>
<bodyText confidence="0.999945647058824">
This claim, which has been underpinned by a small
experiment with human subjects (see Appendix),
means that if a sentence containing one element of
a pair is true then so is the corresponding sentence
containing the other. There are bound to be differ-
ences between the two forms, but these will be taken
to be of a pragmatic nature, having to do with felic-
ity rather than truth (see section 5.2).
An important qualification must be made with re-
spect to the analysis that we propose: to simplify
matters, we assume that the entire domain of rele-
vant individuals- is&apos; available &amp;quot;WI dlhat it is-this do&apos;
main alone which is taken into account when the ad-
jective is applied. In the case of the example above,
this means that all mice are irrelevant except the
four that are mentioned: no other knowledge about
the size of mice is assumed to be available.3
</bodyText>
<subsectionHeader confidence="0.699536">
2.1 A Formal Semantics for Vague
Descriptions
</subsectionHeader>
<bodyText confidence="0.99940375">
Let us be more precise. In our presentation, we will
focus on the adjective &apos;large&apos;, without intended loss
of generality. For simplicity, &apos;large&apos; will be treated
as semantically one-dimensional.
</bodyText>
<listItem confidence="0.687558">
i. &apos;The largest n mouse/mice&apos;. Imagine a set
</listItem>
<bodyText confidence="0.907185666666667">
C.; of contextually relevant animals. Then the NP
&apos;The largest n mouse/mice&apos; (n &gt; 0) presupposes
that there is an S C C that contains n elements,
all of which are mice, and such that (1) C — S /
and (2) every mouse in C — S is smaller than ev-
ery mouse in S such a set S exists then the NP
denotes S. The case where a = 1, realized as &apos;The
[Add-est [CN]&apos; (sg = singular), falls out automat-
ically.
</bodyText>
<listItem confidence="0.977915666666667">
ii. &apos;The largest mice&apos;. This account can be
extended to cover cases of the form &apos;The [Add-est
[CN,/j&apos; (pl = plural), where the numeral a is sup-
pressed: these will be taken to be ambiguous be-
tween all expressions of the form &amp;quot;Ile [Add-est n
[C1\1]&apos; where n &gt; L Thus, in a domain where there
are live mice, of sizes 4,4,4,5,6 cm, the only possible
value of a is 2. causing the NP to denote the two
mice of 5 and 6 cm size.
iii. &apos;The n large mouse/mice&apos;. We analyse &apos;The
n [Add [CN]&apos; (n &gt; 11) as semantically equivalent with
the corresponding NP of the form The Adjl-est a
[CN]&apos;. &apos;The two large mice&apos;, for example, denotes a
set of two mice, each of which is bigger than all other
contextually relevant mice.
iv. &apos;The large mice&apos;. Expressions of this form can
be analysed as being of the form &apos;The a [Add (CN]&apos;
for some value of n. In other words, we will take
</listItem>
<bodyText confidence="0.934845166666667">
In other words, only perceptual coritext-dependQuce is
laken intr■ account. as opposed to riorinature or functional
context-dependence [beling and Gelman (1q94).
them to be ambiguous or unspecific the difference
will not matter for present purposes - between &apos;The
2 large mice&apos;, &apos;The 3 large mice&apos;, etc.
</bodyText>
<sectionHeader confidence="0.893226" genericHeader="method">
3 Generation of Crisp Descriptions
</sectionHeader>
<bodyText confidence="0.999862684210526">
Generation of descriptions covers a number of tasks,
one of which consists of finding a set L of properties
which allows a reader to pick out a given unique in-
dividual or set of individuals. The state of the art
is discussed in Dale and Reiter (1995), who present
a computationally tractable algorithm for character-
izin duals.% This, algorithm - (henceforth .DSLR),
deals with vague properties, such as size, to some
extent, but these are treated as if they were context-
independent: always applying to the same sets of
objects.
In many cases, generating vague descriptions in-
volves generating a plural and no generally accepted
account of the generation of plural descriptions has
been advanced so far. In the following section, there-
fore, a generalization or D&amp;R will be offered, called
D&amp;Rpi,,,-, which focuses on sets of individuals. Char-
acterization of an individual will fall out as a special
case of the algorithm.
</bodyText>
<subsectionHeader confidence="0.9523055">
3.1 Plural Descriptions: Dale and Reiter
generalized
</subsectionHeader>
<bodyText confidence="0.985113566666667">
The properties which form the basis of D8zapi„r are
modeled as pairs of the form (Attribute,Valtie). In
our presentation of the algorithm, we will focus on
complete properties (i.e., (Attribute,Value) pairs)
rather than attributes, as in Dale and Reiter (1995),
since this facilitates the use of set-theoretic termi-
nology. Suppose S is the &apos;target&apos; set of individu-
als (i.e., the set of individuals to be characterized)
and C (where S C C) is the set of individuals from
which S is to be selected.4 Informally - and for-
getting about the special treatment of head nouns -
what happens is the following: The algorithm iter-
ates through a list P in which the properties appear
in order of &apos;preference&apos;; for each attribute, it checks
whether specifying a value for that attribute would
rule out at least one additional member of C: if so,
the attribute is added to I:, with a suitable value.
(The value can be optimized using some further con-
straints but these will be disregarded here.) Individ-
uals that are ruled out. by a property are removed
from C. The process of expanding L and contracting
C continues until C = S. The properties in L can
be used by a linguistic realization module to pro-
duce NPs such as &apos;The white mice&apos;, &apos;The white mice
that are.pregnant&apos;, etc. Schematically, the algorithm
goes as follows: (Notation: Given a property Q, the
set of objects that have the property Q is denoted
&amp;quot;lNote that C unutains r, unlike Dale and Reiter&apos;s &apos;contra.st
set&apos; C, which consists of those elements of the domain from
which r is set apari.
</bodyText>
<page confidence="0.974331">
180
</page>
<bodyText confidence="0.630051">
L 0 1# L is initialized to the empty set #)
For each e P do
</bodyText>
<figure confidence="0.868594272727273">
If S C [[Pg]] gr-C g [it Adding Pi
would remove distractors from C #1
then do
L L U {Pi} {# Property Pi. is added
to I, #1
C := C fl [(Pi]] 1# All elements outside
[[Pi]] are removed from C #1
If C = S then Return L (# Success #1
Return Failure{t&amp; All prop-erties in P-have been
tested, yet C S #1
&apos;Success&apos; means that the properties- in L are suffi-
</figure>
<bodyText confidence="0.999858352941177">
cient to characterize S. Thus, nil[p]i : E LI S.
The case in which S is a singleton set. amounts to
the generation of a singular description: D&amp;Ftpna-
becomes equivalent to D&amp;R (describing the individ-
ual r) when S in D&amp;Rpi, is replaced by {r}.
D&amp;RFaur uses hill climbing: an increasingly good
approximation of S is achieved with every contrac-
tion of C. Provided the initial C is finite, p■SERPtur
finds a suitable L if there exists one. Each property
is considered at most once, in order of &apos;preference&apos;.
As a consequence, L can contain semantically redun-
dant properties - causing the descriptions to become
more natural, cf. Dale and Reiter 1995 - and the al-
gorithm is polynomial in the cardinality of P.
Caveats. D&amp;R.Ptur does not allow a generator to in-
clude collective properties in a description, as in &apos;the
two neighbouring houses&apos;, for example. Furthermore,
D&amp;R,ra, cannot be employed to generate conjoined
NPs: It generates NPs like &apos;the large white mouse&apos;
but not &apos;the black cat and the large white mouse&apos;.
From a general viewpoint of generating descriptions,
this is an important limitation which is, moreover,
difficult to overcome in a computationally tractable
account. In the present context, however, the lim-
itation is inessential, since what is crucial here is
the interaction between an Adjective and a (possibly
complex) Common Noun following it: in more com-
plex constructs of the form `NP and the Adj CN&apos;,
only CN affects the meaning of Adj. 5 There is no
need for us to solve the harder problem of finding an
efficient algorithm for generating NPs uniquely de-
scribing arbitrary sets of objects, but only the easier
problem of doing this whenever a (nonconjunctive)
NP of the form the Adj CN&apos; is possible.
</bodyText>
<sectionHeader confidence="0.963626" genericHeader="method">
4 Generation of Vague Descriptions
</sectionHeader>
<bodyText confidence="0.949885226415094">
We now turn our attention to extensions Of D&amp;Rpi„,
that generate descriptions containing the expression
5111 &apos;The oiephant and the big rnouse•, for example, the
11101150 does not hwve to be bigger than any elephant.
of one vague property. Case i of section 2.1, &apos;The
largest n chihuahuas&apos; will be discussed in some de-
tail. All the others are minor variations.
Superlative adjectives. First, `The largest chi-
huahua&apos;. We will assume that size is stored (in the
KB that forms the input to the generator) as an at-
tribute with exact numerical values. We will take
them to be of the form n cm, where n is a positive
natural number. For example,
type = dog, chihuahua
colour black, blue, yellow
size = lcm, 2crzz, Wan.
With this KB as input, Dezn. allows us to generate
NPs based on L {yellow,chihuahua,9crn}, for ex-
ample, exploiting the number-valued attribute size.
The result could be the NP &apos;The 9cm yellow chi-
huahua&apos;, for example. The challenge, however, is
to generate superlatives like &apos;The largest yellow chi-
huahua&apos; instead.
There are several ways in which this challenge
may he answered. One possibility is to replace
an exact value like 9crn, in L, by a superlative
value whenever all distractors happen to have a
smaller size. The result would be a new list L -=-
{yellow,chiltuahua,largesti where `largestk is the
property &apos;being the unique largest element of C&apos;.
This list can then be realized as a superlative NP.
We will present a different approach that is more
easily extended to plurals, given that a plural de-
scription like &apos;the 2 large mice&apos; does not require the
two mice to have the same size.
Suppose size is the only vague property in the KB.
Vague properties are less &apos;preferred&apos; (in the sense
of section 3.1) than others (Krahmer and Theune
1999).6 As a result, when they are taken into consid-
eration, all the other relevant properties are already
in L. For instance, assume that this is the K13, and
that the object to be described is c4:
type(ci (13, c,1)=chillualma
type(m),--poodle
size(ci )=3cm
Size(c.))=5Crn
size(c3)=Sem
size(c4)--7size(p5)=9cm
At this point, inequalities of the forni size(x) &gt;
in cm are added to the 1.&lt;0. For every value of
the form n cm occuring in.. the .old 10, all-inequal-
ities of the form size(x) &gt; n cm are added whose
truth follows from the old K13. Inequalities are more
</bodyText>
<footnote confidence="0.828272">
6Note, by contrast, that vague properties tend tcl be real-
ized first (Greenbaum et al. 1985, Shaw and Hatzivassiloglou
1999). Surface realization. however, is not the topic of this
paper.
</footnote>
<page confidence="0.99546">
181
</page>
<bodyText confidence="0.9623">
preferred than equalities, while logically stronger in-
equalities are more preferred than logically weaker
ones.7 Thus, in order of preference,
</bodyText>
<equation confidence="0.998776333333333">
size(c4),size(p5) &gt; 8cm.
size(c3),size(c.4),size(p5) &gt; 5em
size(c2),size(c3),size(c4),size(p5) &gt; 3cm.
</equation>
<bodyText confidence="0.998151104166667">
The first property that makes it into L is &apos;chi-
huahua&apos;, which removes p5 but not 04 from the con-
text set. (Result: C = cll.) Now size is
taken into account, and the property size(r) &gt; 8cm
singles out at. The resuIting.list.is L =-{chihuabu4,
&gt; 8cm}. This implies that c4 is the only chihuahua
in the KB that is greater than 8cm and consequently,
the property size(x) &gt; 8cm can be replaced, in L, by
the property of &apos;being larger than all other elements
of C&apos;. The result is a list that may be written as
L = {chihuahua, largesti }, which can be employed
to generate the description &apos;the largest chihuahua&apos;.
Plurals can be treated along analogous lines. Sup-
pose, for example, the facts in the KB are the same
as above and the target set S is {c3, c4}. Its two ele-
ments share the property size(x) &gt; 5cm. This prop-
erty is exploited by DSznpi,„ to construct the list
L = {chihuahua,&gt;5cm}. Analogous to the singular
case, the inequality can be replaced by the property
&apos;being a set all of whose elements are larger than
all other elements of C&apos; (largest,, for short), leading
to NPs such as &apos;the largest chihualmas&apos;. Optionally,
the numeral may be included in the NP (`the two
largest chihuahuas&apos;).
— &apos;Absolute&apos; adjectives. The step from the su-
perlative descriptions of case i to the analogous &apos;ab-
solute&apos; descriptions is a small one. Let us first turn
to case iii, &apos;The n large mouse/mice&apos;. Assuming the
correctness of the semantic analysis in section 2, the
NP &apos;The n large mouse/mice&apos; is semantically equiv-
alent to the one discussed under i. Consequently,
an obvious variant of the algorithm that was just
described can be used for generating it. (For prag-
matic. issues, see section 5.2)
Filially, case iv. The large mice&apos;. Semantically,
this does not, introduce any new problems, since
it is to case iii what case ii is to case i. Accord-
ing to the semantic analysis of section 2.1, &apos;The
large mice&apos; should be analysed just like &apos;The n large
mouse/mice&apos;, except that the numeral rt is sup-
pressed. This means that a simplified version (i.e.,
without a cardinality check) of the algorithm that
Lakes care of case iii will be sufficient to generate
descriptions of this kind.
&apos; E.g., szze(x) &gt; in 15 preferred over- se(r) &gt;
The preference for inequalities causes the generator to avoid
the mentioning of measurements unless they are nettled for
he identification of thi target object.
</bodyText>
<sectionHeader confidence="0.887101" genericHeader="method">
5 Conclusions and loose ends
</sectionHeader>
<bodyText confidence="0.949915083333333">
We have shown how vague descriptions can be gen-
-erated that-make-Ilse-of .one vague propeitii. We be-
lieve our account to be an instructive model of how
the &apos;raw data&apos; in a standard knowledge base can be
presented in English expressions that have a very dif-
ferent structure. The numerical data that are the in-
put to our algorithm, for example, take a very differ-
ent form in the descriptions generated, and yet there
is, in an interesting sense, no loss of information: a
description has the same reference, whether it uses
...2,exacti.informationJ(.The,3ananot4se!).or ...Atague!
formation (`The large mouse&apos;) .8
</bodyText>
<subsectionHeader confidence="0.999166">
5.1 Limitations of the semantic analysis
</subsectionHeader>
<bodyText confidence="0.9996297">
Our proposal covers the generation of vague descrip-
tions &apos;from absolute values&apos;, which is argued in Dale
and Reiter (1995, section 5.1.2) to be most practi-
cally useful. When vague input is available (e.g., in
the generation component of a Machine Translation
System, or in wYSTWYM-style generation (Power and
Scott 1998)), simpler methods can be used. Our own
account is limited to the generation of definite de-
scriptions and no obvious generalization to indefinite
or quantified NPs exists. Other limitations include
</bodyText>
<listItem confidence="0.961074653846154">
a. Descriptions that contain properties for other
than individuating reasons (as when someone
asks you to clean &apos;the dirty table cloth&apos; when
only one table cloth is in sight). This limitation
is inherited directly from the D&amp;R algorithm
that our own algorithm extends.
b. Descriptions containing more than one vague
property, such as &apos;The fat tall bookcase&apos;, whose
meaning is more radically unclear than that of
definite descriptions containing only one vague
term. (The bookcase may be neither the fattest
nor the tallest, and it is not clear how the two
dimensions are weighed.)
c. Descriptions that rely on the salience of con-
textually available objects. Krahmer and The-
une (1998) have shown that a contextually
more adequate version of D8zR can be obtained
when degrees of salience are taken into account.
Their account can be summarized as analysing
&apos;the black dog&apos; as denoting the unique most
salient object in the domain that is both black
and a dog. (Generalizations of this idea to
1)&amp;ill&apos;itir are conceivable but nontrivial since
not all elements of the set S have to be equally
salient.) Our own extensions of Dk R (and per-
haps D&amp;Rpft,,..) could be `contextualized&apos; if the
</listItem>
<footnote confidence="0.9866424">
5This may be contrasted with the vague expressions gen-
erated in (Goldberg et at. 1994), where there is a real -and
intended -- loss of information. (E.g., &apos;Heavy rain fell on Tilos
clay&apos;, based on the information that the rainfall on Tnesday
equalled itrimm.)
</footnote>
<page confidence="0.99577">
182
</page>
<bodyText confidence="0.9999439">
role of salience is changed slightly: focusing on
the singular case, the algorithm can, for exam-
ple, be adapted to, legislate that . &apos;the. large(est)
mouse&apos; denotes the largest of all those mice
that are salient (according to some standard of
salience). Note that this analysis predicts am-
biguity when the largest mouse that is salient
according to one standard is smaller than the
largest mouse that is salient according to a more
relaxed standard. Suppose, for example,
</bodyText>
<subsectionHeader confidence="0.961484">
Salient (strict):
</subsectionHeader>
<bodyText confidence="0.904429">
ml (2cm); -m2.(5cm).
</bodyText>
<subsectionHeader confidence="0.929254">
Salient (relaxed):
</subsectionHeader>
<bodyText confidence="0.988186285714286">
ml (2cm), m2 (5cm), m3 (7cm);
then &apos;the large(est) mouse&apos; may designate ei-
ther m2 or m3 depending on the standards
of salience used. What this illustrates is that
salience and size are both vague properties, and
that — as we have seen under point b — combin-
ing vague properties is a tricky business.
</bodyText>
<subsectionHeader confidence="0.998425">
5.2 Pragmatics
</subsectionHeader>
<bodyText confidence="0.918543621212121">
An experimental ProFIT (Erbach 1995) program has
implemented the algorithms described so far, gen-
erating different descriptions, each of which would
allow a reader/hearer to identify an object or a set
of objects. But of course, an NLG program has to do
more than determine under what circumstances the
use of a description leads to a true statement: an
additional problem is to choose the most appropri-
ate description from those that are semantically cor-
rect. This makes NLC an ideal setting for exploring
issues that have plagued semanticists and philoso-
phers when they studied the meaning of vague ex-
pressions, such as whether it can be true for two
objects a: and y which are indistinguishable in size
that x is large and y is not (e.g. Synthese 1975).
The present setting allows us to say that a statement
of this kind may be true yet infelicitous (because
they conflict with certain pragmatic constraints),
and consequently to be avoided by a generator.
As for the choice between the `absolute/superlative
forms of the gradable adjective, we conjecture that
the following constraints apply:
Cl. Distinguishability. Expressions of the form
&apos;The (n ) large [CN&apos; are infelicitous when the
smallest element of the designated set S (named
x) and the largest CN smaller than all elements
of S (named y) are perceptually indistinguish-
able.
C2. Natural Grouping. Expressions of the form
&apos;The (71) large [CN] are better avoided when the
difference in size between x and y is &apos;compara-
tively small. One way of making this precise is
by requiring that the difference between x and
y cannot be smaller than that between either
x or y and one of their neighbouring elements.
Consider, for. example,. a domain consisting of
mice that are lcm, lcm, 2cm, 7cm, 9cm and
9cm large; then C2 predicts that the only felic-
itous use of &apos;the large mice&apos; refers to the largest
three of the group.
C3. Minimality. Otherwise, preference is given to
the absolute form. This implies that when ob-
jects of only two sizes are present, and the differ-
ence is perceptually distinguishable, the abso-
,5 .lute iornuiwprderred overthe superiative.form.
(For example, in a domain where there are two
sizes of pills, we are much more likely to speak
of &apos;the large pills&apos; than of the largest pills&apos;.)
In languages in which the superlative form is
morphologically more complex than the abso-
lute form, constraint C3 can be argued to follow
from general Gricean principles (Grice 1975)).
As for the presence/absence of the numeral, we
conjecture that the disambiguating numeral (as
in the n large mice&apos; or &apos;the n largest mice&apos;) can
be omitted under two types of circumstances: (1)
when any ambiguity resulting from different values
of n is likely to be inconsequential (see Van Deemter
and Peters (1996) for various perspectives); (2)
when the domain allows only one &apos;natural grouping&apos;
(in the sense of C2). Before and until a more
accurate version of the notion of a natural grouping
is available (perhaps using fuzzy logic as in Zim-
mermann 1985), generators could be forbidden to
omit the numeral, except in the case of a definite
description in the singular.
</bodyText>
<sectionHeader confidence="0.995221" genericHeader="method">
Appendix: A Supporting Experiment
</sectionHeader>
<bodyText confidence="0.995414388888889">
Human subjects were asked to judge the correctness
of an utterance in a variety of situations. The ex-
periment was set up to make plausible that, in a sit-
uation in which only perceptual context-dependence
(see section 1) is relevant, expressions of the form
the 71 large CN&apos; can be used whenever certain situ-
pie conditions are fullfilled. Note that this ( ) di-
rection of the hypothesis is most directly relevant
to the design of a generator, since we expect a gen-
erator to avoid mistakes rather than always use an
expression whenever it is legitimate.
Hypothesis (): In a situation in which
the domain D represents the set. of percep-
tually relevant objects, an expression of the
form n the n large CN&apos; (where n &gt; 1), can
be used to refer to a set S of cardinality
if all objects in D — S are smaller than any
of the n.
</bodyText>
<page confidence="0.997301">
183
</page>
<bodyText confidence="0.9972931">
The experiment explores whether &apos;the n large CN&apos;
can refer to the n largest objects in the domain,
whether or not this set of objects is held together by
spatial position or other factors. Subjects were pre-
sented with 26 different situations, in each of which
they had to say whether the sentence
The two high numbers appear in brackets
would constitute a correct utterance. The literal text
of our question was:
Suppose you want to inform a hearer
*which -numbers. irra.,giv.erylist-appear in-
brackets*, where the hearer knows what
the numbers are, but not which of them ap-
pear in brackets. For example, the hearer
knows that the list is 1 2 1 7 7 1 1 3 1.
You, as a speaker, know that only the
two occurrences of the number 7 appear
in brackets: 1 2 1 (7) (7) 1 1 3 1. Our
question to you is: Would it be *correct*
to convey this information by saying &amp;quot;The
two high numbers appear in brackets&amp;quot;?
(---)-
All subjects were shown the 26 situations in the
same, arbitrary, order. Each situation presented to
the subjects contained a list of nine numbers. In 24
cases, the lists had the following form:
111xyz111,
where each of x, y, z equalled either 6 or 9, and where
there were always two numbers among x,y, z that
appear in brackets. In 16 out of 24 cases, the two
bracketed positions are right next to each other, al-
lowing US to test whether spatial contiguity plays
any role. Subjects were presented with two addi-
tional situations, namely 1 1 1 (6) 1 (7) 1 1 1 and
11 1 (7) 1 (6) 1 1 1 in which, unlike the other 24
situations, the two largest numbers are not equally
large, to make sure that the descriptions do not re-
quire the elements in their denotation to be similar
in that respect. Our questions were presented via
email to 30 third-year psychology/cognitive science
students at the University of Durham. UK. all of
whom were native speakers of English and ten of
which responded.
Results: Eight subjects responded in exact confor-
mance with the analysis of section 2.1, marking all
and only those five sequences in which the highest
numbers appeared in brackets. Only two subjects
deviated slightly from this analysis: one of the two
(subject 9) described all the expected situations as
&apos;correct&apos; plus the two cases in which two coutiguous
6-es appeared in brackets: the other subject. (subject
10) appears to havt. made 0 typhig error. confusing
two subsequent situations in the experiment.9 All
other responses of subjects 9 and 10 were as pre-
dicted. This means. that all subjects except subject
10 were consistent with our hypothesis. The ex-
periment suggests that the converse of the hypoth-
esis might also be true, in which it is claimed that
expressions of the form the n large CN&apos; cannot be
employed to refer to the set S unless S consists of
the n largest objects in D:
Hypothesis (-): In a situation in which
the domain D represents the set of percep-
tually, relevant. objects;:amexpre.ssion -of the
form &apos;the n large CN&apos; (where n &gt; 1), can
only be used to refer to a set S of cardi-
nality n if all objects in D S are smaller
than any of the n.
Again disregarding subject 10, eight out of nine
subjects act in accordance with Hypothesis
while only one appears to follow a somewhat more
liberal rule. Given these findings, it appears to
be safe to build a generator that implements both
hypotheses, since none of our subjects would be
likely to disagree with any of the descriptions
generated by it.
This experiment has evident limitations. In partic-
ular, it has no bearing on the pragmatic constraints
suggested in section 5.2, which might be tested in a
follow-up experiment.
</bodyText>
<sectionHeader confidence="0.997393" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998796142857143">
Thanks are due to: Richard Power for discussions
and implementation; Erniel Krahmer, Ehud Reiter
and Matthew Stone for comments on an earlier
draft; Hua Cheng for observations on linguistic
realization; Rosemary Stevenson and Paul Piwek
for their help with the experiment described in the
Appendix.
</bodyText>
<sectionHeader confidence="0.999537" genericHeader="references">
6 References
</sectionHeader>
<reference confidence="0.857378666666667">
- Dale arid Reiter 199.5. R. Dale and E. Reiter. Com-
putational Interpretations of the Grice.an Maximes
in the Generation of Referring Expressions. Cogni-
tive Science 18: 233-263.
- Ebeling and Gelman 1991. Ebeling, K.S. Gelman
S.A. 1994. Children&apos;s use of context in interpreting
&amp;quot;big&amp;quot; and &amp;quot;little&apos;&apos;. Child Development 65(4): 1175-
1192.
- Erbach 1995. G. Erbach. Web page on the ProFrr
</reference>
<footnote confidence="0.799481">
9Th situations that we suspect to have been confused ate
</footnote>
<bodyText confidence="0.986274666666667">
I 1 1 (9) (9) 9 I I, which was marked as corrcct (although.
remarkably, none of the other &apos;three RI neS&apos; situations were
marked as correct) and 1 1 1 (9) (9) 6 1 1 I.
</bodyText>
<page confidence="0.997163">
184
</page>
<reference confidence="0.914005486486486">
programming language, http://coli.uni-sb.de/ er-
bach/formal/profit/profit.btml.
— -Goldberg et al. 1994.- E. -Goldberg,.N.•Driedger,
and R. Kitteridge. Using Natural-Language Pro-
cessing to Produce Weather Forecasts. IEEE Expert
9 no.2: 45-53.
- Greenbaum et al. 1985. &amp;quot;A Comprehensive Gram-
mar of the English Language&amp;quot;. Longman, Harlow,
Essex.
- Grice 1975. P. Once. Logic and Conversation.
In P. Cole and J. Morgan (Eds.), &amp;quot;Syntax and Se-
mantics: Vol 3, Speech Acts&amp;quot;: 43158. New York,
Academic Press.
- Krahmer and Theune 1999. E. Krahmer and M.
Theune. Generating Descriptions in Context. In
R. Ribble and K. van Deemter (Eds.), Procs. of
workshop The Generation of Nominal Expressions,
associated with the 11th European Summer School
in Logic, Language, and Information (ESSLLC99).
- Pinkal 1995. M. Pinkal. &amp;quot;Logic and Lexicon&amp;quot;. Ox-
ford University Press.
- Power and Scott 1998. R. Power and D. Scott.
Multilingual Authoring using Feedback Texts. In
Proc. COLING/ACL, Montreal.
- Quirk et al. 1972. Ft. Quirk, S. Greenbaum, and
G. Leech. &amp;quot;A Grammar of Contemporary English&amp;quot;.
Longman, Harlow, Essex.
- Shaw and Hatzivassiloglou 1999. Ordering Among
Prernodifiers. In Procs. of AcL99, Univ. Maryland.
- Synthese 1975. Special issue of the journal Syn-
these on semantic vagueness. Synthese 30.
- Van Deemter and Peters 1996. K. van Deemter
and S. Peters (Eds.) &amp;quot;Semantic Ambiguity and Un-
derspecification&amp;quot;. CSLI Publications. Stanford.
- Zirrunermann 1985. H. J. Zimmermann. &amp;quot;Fuzzy
Set Theory - and its Applications&amp;quot;. Kluwer Aca-
demic Publishers, Boston/Dordrecht/Lancaster
</reference>
<page confidence="0.998846">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.716207">
<title confidence="0.987206">Generating Vague Descriptions</title>
<affiliation confidence="0.982382">ITRI, University of</affiliation>
<address confidence="0.8819025">Lewes Road, Watts Brighton BN2 4W, United Kingdom</address>
<email confidence="0.978587">Kees.van.Deemter@itri.brighton.ac.uk</email>
<abstract confidence="0.998608555555555">This paper deals with the generation of definite (i.e., uniquely referring) descriptions containing semantically vague expressions (&apos;large&apos;, &apos;small&apos;, etc.). Firstly, the paper proposes a semantic analysis of vague descriptions that does justice to the contextdependent meaning of the vague expressions in them. Secondly, the paper shows how this semantic analysis can be implemented using a modification of the Dale and Reiter (1995) algorithm for the generation of referring expressions. A notable feature of the new algorithm is that, unlike Dale and Reiter (1995), it covers plural as well as singular NPs. This algorithm has been implemented in an experimental NLG program using PronT. The paper concludes by formulating some pragmatic constraints that could allow a generator to choose between different semantically correct descriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<journal>Computational Interpretations of the Grice.an Maximes in the Generation of Referring Expressions. Cognitive Science</journal>
<volume>18</volume>
<pages>233--263</pages>
<marker>Dale, Reiter, </marker>
<rawString>- Dale arid Reiter 199.5. R. Dale and E. Reiter. Computational Interpretations of the Grice.an Maximes in the Generation of Referring Expressions. Cognitive Science 18: 233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebeling</author>
<author>Gelman</author>
</authors>
<title>Children&apos;s use of context in interpreting &amp;quot;big&amp;quot; and &amp;quot;little&apos;&apos;.</title>
<date>1994</date>
<journal>Child Development</journal>
<volume>65</volume>
<issue>4</issue>
<pages>1175--1192</pages>
<marker>Ebeling, Gelman, 1994</marker>
<rawString>- Ebeling and Gelman 1991. Ebeling, K.S. Gelman S.A. 1994. Children&apos;s use of context in interpreting &amp;quot;big&amp;quot; and &amp;quot;little&apos;&apos;. Child Development 65(4): 1175-1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erbach</author>
</authors>
<title>Web page on the ProFrr programming language,</title>
<date>1995</date>
<note>http://coli.uni-sb.de/ erbach/formal/profit/profit.btml.</note>
<contexts>
<context position="20144" citStr="Erbach 1995" startWordPosition="3430" endWordPosition="3431">analysis predicts ambiguity when the largest mouse that is salient according to one standard is smaller than the largest mouse that is salient according to a more relaxed standard. Suppose, for example, Salient (strict): ml (2cm); -m2.(5cm). Salient (relaxed): ml (2cm), m2 (5cm), m3 (7cm); then &apos;the large(est) mouse&apos; may designate either m2 or m3 depending on the standards of salience used. What this illustrates is that salience and size are both vague properties, and that — as we have seen under point b — combining vague properties is a tricky business. 5.2 Pragmatics An experimental ProFIT (Erbach 1995) program has implemented the algorithms described so far, generating different descriptions, each of which would allow a reader/hearer to identify an object or a set of objects. But of course, an NLG program has to do more than determine under what circumstances the use of a description leads to a true statement: an additional problem is to choose the most appropriate description from those that are semantically correct. This makes NLC an ideal setting for exploring issues that have plagued semanticists and philosophers when they studied the meaning of vague expressions, such as whether it can</context>
</contexts>
<marker>Erbach, 1995</marker>
<rawString>- Erbach 1995. G. Erbach. Web page on the ProFrr programming language, http://coli.uni-sb.de/ erbach/formal/profit/profit.btml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>-Goldberg</author>
</authors>
<title>Using Natural-Language Processing to Produce Weather Forecasts.</title>
<date>1994</date>
<journal>IEEE Expert</journal>
<volume>9</volume>
<pages>45--53</pages>
<marker>-Goldberg, 1994</marker>
<rawString>— -Goldberg et al. 1994.- E. -Goldberg,.N.•Driedger, and R. Kitteridge. Using Natural-Language Processing to Produce Weather Forecasts. IEEE Expert 9 no.2: 45-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greenbaum</author>
</authors>
<title>A Comprehensive Grammar of the English Language&amp;quot;. Longman,</title>
<date>1985</date>
<location>Harlow, Essex.</location>
<marker>Greenbaum, 1985</marker>
<rawString>- Greenbaum et al. 1985. &amp;quot;A Comprehensive Grammar of the English Language&amp;quot;. Longman, Harlow, Essex.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Logic</author>
<author>Conversation In P Cole</author>
<author>J Morgan</author>
</authors>
<title>Syntax and Semantics: Vol 3, Speech Acts&amp;quot;: 43158.</title>
<publisher>Academic Press.</publisher>
<location>New York,</location>
<marker>Logic, Cole, Morgan, </marker>
<rawString>- Grice 1975. P. Once. Logic and Conversation. In P. Cole and J. Morgan (Eds.), &amp;quot;Syntax and Semantics: Vol 3, Speech Acts&amp;quot;: 43158. New York, Academic Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Krahmer</author>
<author>M Theune</author>
</authors>
<title>Generating Descriptions in Context. In</title>
<booktitle>Deemter (Eds.), Procs. of workshop The Generation of Nominal Expressions, associated with the 11th European Summer School in Logic, Language, and Information (ESSLLC99).</booktitle>
<marker>Krahmer, Theune, </marker>
<rawString>- Krahmer and Theune 1999. E. Krahmer and M. Theune. Generating Descriptions in Context. In R. Ribble and K. van Deemter (Eds.), Procs. of workshop The Generation of Nominal Expressions, associated with the 11th European Summer School in Logic, Language, and Information (ESSLLC99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinkal</author>
</authors>
<title>Logic and Lexicon&amp;quot;.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<marker>Pinkal, 1995</marker>
<rawString>- Pinkal 1995. M. Pinkal. &amp;quot;Logic and Lexicon&amp;quot;. Oxford University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Power</author>
<author>D Scott</author>
</authors>
<title>Multilingual Authoring using Feedback Texts.</title>
<booktitle>In Proc. COLING/ACL,</booktitle>
<location>Montreal.</location>
<marker>Power, Scott, </marker>
<rawString>- Power and Scott 1998. R. Power and D. Scott. Multilingual Authoring using Feedback Texts. In Proc. COLING/ACL, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quirk</author>
</authors>
<title>A Grammar of Contemporary English&amp;quot;.</title>
<date>1972</date>
<publisher>Longman,</publisher>
<location>Harlow, Essex.</location>
<marker>Quirk, 1972</marker>
<rawString>- Quirk et al. 1972. Ft. Quirk, S. Greenbaum, and G. Leech. &amp;quot;A Grammar of Contemporary English&amp;quot;. Longman, Harlow, Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaw</author>
<author>Hatzivassiloglou</author>
</authors>
<title>Ordering Among Prernodifiers.</title>
<date>1999</date>
<booktitle>In Procs. of AcL99,</booktitle>
<location>Univ. Maryland.</location>
<contexts>
<context position="13923" citStr="Shaw and Hatzivassiloglou 1999" startWordPosition="2388" endWordPosition="2391">n, all the other relevant properties are already in L. For instance, assume that this is the K13, and that the object to be described is c4: type(ci (13, c,1)=chillualma type(m),--poodle size(ci )=3cm Size(c.))=5Crn size(c3)=Sem size(c4)--7size(p5)=9cm At this point, inequalities of the forni size(x) &gt; in cm are added to the 1.&lt;0. For every value of the form n cm occuring in.. the .old 10, all-inequalities of the form size(x) &gt; n cm are added whose truth follows from the old K13. Inequalities are more 6Note, by contrast, that vague properties tend tcl be realized first (Greenbaum et al. 1985, Shaw and Hatzivassiloglou 1999). Surface realization. however, is not the topic of this paper. 181 preferred than equalities, while logically stronger inequalities are more preferred than logically weaker ones.7 Thus, in order of preference, size(c4),size(p5) &gt; 8cm. size(c3),size(c.4),size(p5) &gt; 5em size(c2),size(c3),size(c4),size(p5) &gt; 3cm. The first property that makes it into L is &apos;chihuahua&apos;, which removes p5 but not 04 from the context set. (Result: C = cll.) Now size is taken into account, and the property size(r) &gt; 8cm singles out at. The resuIting.list.is L =-{chihuabu4, &gt; 8cm}. This implies that c4 is the only chih</context>
</contexts>
<marker>Shaw, Hatzivassiloglou, 1999</marker>
<rawString>- Shaw and Hatzivassiloglou 1999. Ordering Among Prernodifiers. In Procs. of AcL99, Univ. Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Synthese</author>
</authors>
<title>Special issue of the journal Synthese on semantic vagueness.</title>
<date>1975</date>
<journal>Synthese</journal>
<volume>30</volume>
<contexts>
<context position="20863" citStr="Synthese 1975" startWordPosition="3555" endWordPosition="3556">h would allow a reader/hearer to identify an object or a set of objects. But of course, an NLG program has to do more than determine under what circumstances the use of a description leads to a true statement: an additional problem is to choose the most appropriate description from those that are semantically correct. This makes NLC an ideal setting for exploring issues that have plagued semanticists and philosophers when they studied the meaning of vague expressions, such as whether it can be true for two objects a: and y which are indistinguishable in size that x is large and y is not (e.g. Synthese 1975). The present setting allows us to say that a statement of this kind may be true yet infelicitous (because they conflict with certain pragmatic constraints), and consequently to be avoided by a generator. As for the choice between the `absolute/superlative forms of the gradable adjective, we conjecture that the following constraints apply: Cl. Distinguishability. Expressions of the form &apos;The (n ) large [CN&apos; are infelicitous when the smallest element of the designated set S (named x) and the largest CN smaller than all elements of S (named y) are perceptually indistinguishable. C2. Natural Grou</context>
</contexts>
<marker>Synthese, 1975</marker>
<rawString>- Synthese 1975. Special issue of the journal Synthese on semantic vagueness. Synthese 30.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K van Deemter</author>
<author>S Peters</author>
</authors>
<title>(Eds.) &amp;quot;Semantic Ambiguity and Underspecification&amp;quot;.</title>
<publisher>CSLI Publications. Stanford.</publisher>
<marker>van Deemter, Peters, </marker>
<rawString>- Van Deemter and Peters 1996. K. van Deemter and S. Peters (Eds.) &amp;quot;Semantic Ambiguity and Underspecification&amp;quot;. CSLI Publications. Stanford.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H J Zimmermann</author>
</authors>
<title>Fuzzy Set Theory - and its Applications&amp;quot;.</title>
<publisher>Kluwer Academic Publishers, Boston/Dordrecht/Lancaster</publisher>
<marker>Zimmermann, </marker>
<rawString>- Zirrunermann 1985. H. J. Zimmermann. &amp;quot;Fuzzy Set Theory - and its Applications&amp;quot;. Kluwer Academic Publishers, Boston/Dordrecht/Lancaster</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>