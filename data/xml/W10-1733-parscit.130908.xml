<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004760">
<title confidence="0.996518">
Incremental Decoding for Phrase-based Statistical Machine Translation
</title>
<author confidence="0.995364">
Baskaran Sankaran, Ajeet Grewal and Anoop Sarkar
</author>
<affiliation confidence="0.981367666666667">
School of Computing Science
Simon Fraser University
8888 University Drive
</affiliation>
<address confidence="0.580867">
Burnaby BC. V5A 2Y1. Canada
</address>
<email confidence="0.944177">
{baskaran, asg10, anoop}@cs.sfu.ca
</email>
<sectionHeader confidence="0.996909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999151">
In this paper we focus on the incremental
decoding for a statistical phrase-based ma-
chine translation system. In incremental
decoding, translations are generated incre-
mentally for every word typed by a user,
instead of waiting for the entire sentence
as input. We introduce a novel modifi-
cation to the beam-search decoding algo-
rithm for phrase-based MT to address this
issue, aimed at efficient computation of fu-
ture costs and avoiding search errors. Our
objective is to do a faster translation dur-
ing incremental decoding without signifi-
cant reduction in the translation quality.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953666666667">
Statistical Machine Translation has matured sig-
nificantly in the past decade and half, resulting in
the proliferation of several web-based and com-
mercial translation services. Most of these ser-
vices work on sentence or document level, where
a user enters a sentence or chooses a document
for translation, which are then translated by the
servers. Translation in such typical scenarios is
still offline in the sense that the user input and
translation happen sequentially without any inter-
action between the two phases.
In this paper we study decoding for SMT with
the constraint that translations are to be gener-
ated incrementally for every word typed in by the
user. Such a translation service can be used for
language learning, where the user is fluent in the
target language and experiments with many differ-
ent source language sentences interactively, or in
real-time translation environments such as speech-
speech translation or translation during interactive
chats.
We use a phrase-based decoder similar to
Moses (Koehn et al., 2007) and propose novel
modifications in the decoding algorithm to tackle
incremental decoding. Our system maintains a
partial decoder state at every stage and uses it
while decoding for each newly added word. As
the decoder has access only to the partial sentence
at every stage, the future costs change with ev-
ery additional word and this has to be taken into
account while continuing from an existing partial
decoder state. Another major issue is that as incre-
mental decoding is provided new input one word
at at time, some of the entries that were pruned out
at an earlier decoder state might later turn out to
better candidates resulting in search errors com-
pared to decoding the entire sentence at once. It
is to be noted that, the search error problem is re-
lated to the inability to compute full future cost
in incremental decoding. Our proposed modifica-
tions address these twin challenges and allow for
efficient incremental decoding.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="method">
2 Incremental Decoding
</sectionHeader>
<subsectionHeader confidence="0.987933">
2.1 Beam Search for Phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.999990173913044">
In this section we review the usual beam search de-
coder for phrase-based MT because we present our
modifications for incremental decoding using the
same notation. Beam search decoding for phrase-
based SMT (Koehn, 2004) begins by collecting
the translation options from the phrase table for all
possible phrases of a given input sentence and pre-
computes the future cost for all possible contigu-
ous sequences in the sentence. The pseudo-code
for the usual beam-search decoding algorithm is
illustrated in Algorithm 1.
The decoder creates n bins for storing hypothe-
ses grouped by the number of source words cov-
ered. Starting from a null hypothesis in bin 0, the
decoder iterates through bins 1 though n filling
them with new hypotheses by extending the en-
tries in the earlier bins.
A hypothesis contains the target words gener-
ated (e), the source positions translated so far (f)
commonly known as coverage set and the score
of the current translation (p) computed by the
weighted log-linear combination of different fea-
ture functions. It also contains a back-pointer to
</bodyText>
<page confidence="0.98534">
216
</page>
<note confidence="0.974575">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 216–223,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<construct confidence="0.3011075">
Algorithm 1 Phrase-based Decoder pseudocode
(Koehn, 2004)
</construct>
<listItem confidence="0.9996594">
1: Given: sentence Sn: s1s2...sn of length n
2: Pre-compute future costs for all contiguous
sequences
3: Initialize bins bi where i = 1... n
4: Create initial hypothesis: {e : (), f : (), p :
1.0}
5: for i = 1 to n do
6: for hyp E bi do
7: for newHyp that extends hyp do
8: nf := num src words covered by
newHyp
9: Add newHyp to bin bnf
10: Prune bin bnf using future costs
11: Find best hypothesis in bn
12: Output best path that leads to best hypothesis
</listItem>
<bodyText confidence="0.999731757575757">
its parent hypothesis in the previous state and other
information used for pruning and computing cost
in later iterations.
As a new hypothesis is generated by extending
an existing hypothesis with a new phrase pair, de-
coder updates the associated information such as
coverage set, the target words generated, future
cost (for translating rest of the source words) and
its translation score. For example, consider Span-
ish to English translation: for the source sentence
Maria no daba una bofetada, the hypothesis {e :
(Mary), f : (1),p : 0.534} which is the hypoth-
esis that covers Maria can be extended to a new
hypothesis {e : (Mary, slap), f : (1, 3, 4, 5),p :
0.043} by choosing a new phrase pair (daba una
bofetada, slap) covering the source phrases Maria
and daba una bofetada. The probability score is
obtained by weighted log-linear sum of the fea-
tures of the phrases contained in the derivation so
far.
An important aspect of beam search decoding
is the pruning away of low-scoring hypotheses in
each bin to reduce the search space and thus mak-
ing the decoding faster. To do this effectively,
beam search decoding uses the future cost of a hy-
pothesis together with its current cost. The future
cost is an estimate of the translation cost of the
input words that are yet to be translated, and is
typically pre-computed for all possible contiguous
sequences in the input sentence before the decod-
ing step. The future cost prevents the any hypothe-
ses that are low-scoring, but potentially promising,
from being pruned.
</bodyText>
<subsectionHeader confidence="0.994985">
2.2 Incremental Decoder - Challenges
</subsectionHeader>
<bodyText confidence="0.999977666666667">
Our goal for the incremental decoder (ID) is to
generate output translations incrementally for par-
tial phrases as the source sentence is being input
by the user. We assume white-space to be the word
delimiter and the partial sentence is decoded for
every encounter of the space character. We further
assume the return key to mark end-of-sentence
(EOS) and use it to compute language model score
for the entire sentence.
As we noted above, future costs cannot be pre-
computed as in regular decoding because the com-
plete input sentence is not known while decod-
ing incrementally. Thus the incremental decoder
can only use a partial future cost until the EOS
is reached. The partial future cost could result
in some of the potentially better candidates being
pruned away in earlier stages. This leads to search
errors and result in lower translation quality.
</bodyText>
<subsectionHeader confidence="0.99528">
2.3 Approach
</subsectionHeader>
<bodyText confidence="0.999966714285714">
We use a modified beam search for incremental
decoding (ID) and the two key modifications are
aimed at addressing the issues of future cost and
search errors. Beam search for ID begins with
a single bin for the first word and more bins are
added as the sentence is completed by the user.
Our approach requires that the decoder states for
the partial source sentence can be stored in a way
that allows efficient retrieval. It also maintains a
current decoder state, which includes all the bins
and the hypotheses contained in them, all pertain-
ing to the present sentence.
At each step ID goes through a pre-process
phase, where it recomputes the partial future costs
for all the spans accounting for the new word and
updates the current decoder state with new partial
future costs. It then generates new hypotheses into
all the earlier bins and in the newly created us-
ing any new phrases (resulting from the new word
added by the user) not used earlier.
Algorithm 2 shows the pseudocode of our incre-
mental decoder. Given a partial sentence Si1 ID
starts with the pre-process phase illustrated sepa-
rately in algorithm 3. We use Ptyp,(l) to denote
phrases of length l words and Htyp, to denote the
set of hypotheses; in both cases type correspond to
either old or new, indicating if it was not known in
the previous decoding state or not.
</bodyText>
<subsectionHeader confidence="0.5272">
1we use Si and si to denote a i word partial sentence and
</subsectionHeader>
<bodyText confidence="0.8516">
ith word in a (partial) sentence respectively
</bodyText>
<page confidence="0.989237">
217
</page>
<bodyText confidence="0.331792">
Algorithm 2 Incremental Decoder pseudocode
</bodyText>
<listItem confidence="0.995777625">
1: Input: (partial) sentence Sp: s1s2...si−1si
with ls words where si is the new word
2: PreProcess(Sp) (Algorithm 3)
3: for every bin bj in (1... i) do
4: Update future cost and cover set b Hold
5: Add any new phrase of length bj (subject to
d)
6: for bin bk in (bj−MaxPhrLen . . . bj−1) do
7: Generate Hnew for bj by extending:
8: every Hold with every other Pnew(bj −
bk)
9: every Hnew with every other Pany(bj −
bk)
10: Prune bin bj
Algorithm 3 PreProcess subroutine
1: Input: partial sentence Sp of length ls
2: Retrieve partial decoder object for Sp−1
3: Identify possible Pnew (subject to Max-
PhrLen)
4: Recompute fe for all spans in 1...ls
5: for every Pnew in local phrase table do
6: Load translation options to table
7: for every Pold in local phrase table do
8: Update fe with the recomputed cost
</listItem>
<bodyText confidence="0.99537925">
Given Si, the pre-process phase extracts the new
set of phrases (Pnew) for the ith word and adds
them to the existing phrases (Pold). It then recom-
putes the future-cost (fe) for all the contiguous se-
quences in the partial input and updates existing
entries in the local copy of phrase table with new
f�.
In decoding phase, ID generates new hypothe-
ses in two ways: i) by extending the existing hy-
potheses Hold in the previous decoder state Si−1
with new phrases Pnew and ii) by generating new
hypotheses Hnew that are unknown in the previous
state.
The main difference between incremental de-
coding and regular beam-search decoding is inside
the two ’for’ loops corresponding to lines 3 − 9 in
algorithm 2. In the outer loop each of the existing
hypotheses are updated to reflect the recomputed
fe and coverage set. Any new phrases belonging
to the current bin are also added to it2.
2Based on our implementation of lazier cube pruning they
are added to a priority queue, the contents of which are
flushed into the bin at the end of inner for-loop and before
the pruning step
</bodyText>
<subsectionHeader confidence="0.914536">
Hypothesis surfaces
</subsectionHeader>
<figureCaption confidence="0.999363">
Figure 1: Illustration of Lazier Cube Pruning
</figureCaption>
<bodyText confidence="0.999903333333333">
The inner for-loop corresponds to the extension
of hypotheses sets (grouped by same coverage set)
to generate new hypotheses. Here a distinction is
made between hypotheses Hold corresponding to
previous decoder state Sp−1 and hypotheses Hnew
resulting from the addition of word si. Hold is ex-
tended only using the newly found phrases Pnew,
whereas the newer hypotheses are processed as in
regular beam-search.
</bodyText>
<subsectionHeader confidence="0.996866">
2.4 Lazier Cube Pruning
</subsectionHeader>
<bodyText confidence="0.999994666666667">
We have adapted the pervasive lazy algorithm
(or ’lazier cube pruning’) proposed originally for
Hiero-style systems by (Pust and Knight, 2009)
for our phrase-based system. This step corre-
sponds to the lines 5 − 9 of algorithm 2 and allows
us to only generate as many hypotheses as speci-
fied by the configurable parameters, beam size and
beam threshold. Figure 1 illustrates the process of
lazier cube pruning for a single bin.
At the highest level it uses a priority queue,
which is populated by the different hyper-edges
or surfaces3, each corresponding to a pair of hy-
potheses that are being merged to create a new
hypothesis. New hypotheses are generated iter-
atively, such that the hypothesis with the highest
score is chosen in each iteration from among dif-
ferent hyper-edges bundles.
However, this will lead to search errors as have
been observed earlier. Any hyper-edge that has
been discarded due to poor score in an early stage
might later become a better candidate. The prob-
lem worsens further when using smaller beam
sizes (for interactive decoding in real-time set-
tings, we even consider a beam size of 3). In
</bodyText>
<footnote confidence="0.9440445">
3Unlike Hiero-style systems, only two hypotheses are
merged in a phrase-based system and hence the term surface
</footnote>
<figure confidence="0.956919666666667">
hyp 1
A single surface
hyp 2
IN
�����
�����
P. Queue
Hyp
stack
</figure>
<page confidence="0.991316">
218
</page>
<bodyText confidence="0.9776095">
the next section, we introduce the idea of delayed
pruning to reduce search errors.
</bodyText>
<sectionHeader confidence="0.996701" genericHeader="method">
3 Delayed Pruning
</sectionHeader>
<bodyText confidence="0.999985837209303">
Delayed pruning (DP) in our decoder was inspired
by the well known fable about the race between
a tortoise and a hare. If the decoding is consid-
ered to be a race between competing candidate hy-
potheses with the winner being the best hypothe-
sis for Viterbi decoding or among the top-n candi-
dates for n-best decoding.4
In this analogy, a hypothesis having a poor
score, might just be a tortoise having a slow start
(due to a bad estimate of the true future cost for
what the user intends to type in the future) as op-
posed to a high scoring hare in the same state.
Pruning such hypotheses early on is not risk-free
and might result in search errors. We hypothe-
size that, given enough chance it might improve its
score and move ahead of a hare in terms of trans-
lation score.
We implement DP by relaxing the lazier cube
pruning step to generate a small, fixed number
of hypotheses for coverage sets that are not rep-
resented in the priority queue and place them in
the bin. These hypotheses are distinct from the
usual top-k derivations. Thus, the resulting bin
will have entries from all possible hyper-edge bun-
dles. Though this reduces the search error prob-
lem, it leads to increasing number of possibilities
to be explored at later stages with vast majority
of them being worse hypotheses that should be
pruned away.
We use a two level strategy of delay and then
prune, to avoid such exponentially increasing
search space and at the same time to reduce search
error. At the delay level, the idea is to delay the
pruning for few promising tortoises, instead of re-
taining a fixed number of hypotheses from all un-
represented hyper-edges. We use the normalized
language model scores of the top-hypotheses in
each hyper-edge that is not represented in cube
pruning and based on a threshold (which is ob-
tained using a development test set), we selec-
tively choose few hyper-edge bundles and gen-
erate a small number (typically 1-3) of hypothe-
ses from each of them and flag them as tortoises.
</bodyText>
<footnote confidence="0.98473475">
4The analogy is used to compare two or more hypotheses
in terms of their translation scores and not speed. Though our
objective is faster incremental decoding, we use the analogy
here to compare the scores.
</footnote>
<bodyText confidence="0.999857875">
These tortoises are extended minimally at each it-
eration subject to their normalized LM score.
While this significantly reduces the total num-
ber of hypotheses at initial bins, many of these
tortoises might not show improvement even after
several bins. Thus at the prune level, we prune out
tortoises that does not improve beyond a threshold
number of bins called race course limit. The race
course limit signifies the number of steps a tortoise
has in order to get into the decoder beam.
When a tortoise improves in score and breaks
into the beam during cube pruning, it is de-
flagged as a tortoise and enters the regular decod-
ing stream. We found DP to be effective in reduc-
ing the search error for incremental decoder in our
experiments.
</bodyText>
<sectionHeader confidence="0.997905" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999992575757576">
The evaluation was performed using our own im-
plementation of the beam-search decoding algo-
rithms. The architecture of our system is similar
to Moses, which we also use for training and for
minimum error rate training (MERT) of the log-
linear model for translation (Och, 2003; Koehn et
al., 2007). Our features include 7 standard phrase-
based features: 4 translation model features, i.e.
p(f|e), p(elf), plex(fle) and plex(e|f), where e
and f are target and source phrases respectively;
features for phrase penalty, word penalty and lan-
guage model, and we do not include the reorder-
ing feature. We used Giza++ and Moses respec-
tively for aligning the sentences and training the
system. The decoder was written in Java and in-
cludes cube pruning (Huang and Chiang, 2007)
and lazier cube pruning (Pust and Knight, 2009)
functionalities as part of the decoder. Our de-
coder supports both regular beam search (similar
to Moses) and incremental decoding.
In our experiments we experimented various ap-
proaches for storing partial decoder states includ-
ing memcache and transactional persistence using
JDBM but found that the serialization and deseri-
alization of decoder objects directly into and from
the memory to work better in terms of speed and
memory requirements. The partial object is re-
trieved and deserialized from the memory when
required by the incremental decoder.
We evaluated the incremental decoder for trans-
lations between French and English (in both direc-
tions). We used the Workshop on Machine Trans-
lation shared task (WMT07) dataset for training,
</bodyText>
<page confidence="0.997766">
219
</page>
<bodyText confidence="0.99994765">
optimizing and testing. The system was trained us-
ing Moses and the feature weights were optimized
using MERT. To benchmark our Java decoder, we
compare it with Moses by running it in regular
beam search mode. The Moses systems were also
optimized separately on the WMT07 devsets.
Apart from comparing our decoder with Moses
in regular beam search, we also compared the in-
cremental decoding with regular regular beam us-
ing our decoder. To make it comparable with
incremental decoding, we used the regular beam
search to re-decode the sentence fragments for ev-
ery additional word in the input sentence. We
measured the following parameters in our empir-
ical analysis: translation quality (as measured by
BLEU (Papineni et al., 2002) and TER (Snover et
al., 2006)), search errors and translation speed. Fi-
nally, we also measured the effect of different race
course limits on BLEU and decoding speed for in-
cremental decoding.
</bodyText>
<subsectionHeader confidence="0.998239">
4.1 Benchmarking our decoder
</subsectionHeader>
<bodyText confidence="0.99998125">
In this section we compare our decoder with
Moses for regular beam search decoding. Table 1
gives the BLEU and TER for the two language
pairs. Our decoder implementation compares
favourably with Moses for Fr-En: the slightly bet-
ter BLEU and TER for our decoder in Fr-En is
possibly due to the minor differences in the con-
figuration settings. For En-Fr translation, Moses
performs better in both metrics. There are differ-
ences in the beam size between the two decoders,
in our system the beam size is set to 100 compared
to the default value of 1000 (the cube pruning pop
limit) in Moses; we are planning to explore this
and remove any other differences between them.
However based on our understanding of the Moses
implementation and our experiments, we believe
our decoder to be comparable in accuracy with the
Moses implementation. The numbers in the bold-
face are statistically significant at 95% confidence
interval.
</bodyText>
<subsectionHeader confidence="0.998429">
4.2 Re-decoding v.s. Incremental decoding
</subsectionHeader>
<bodyText confidence="0.999901355932203">
We test our hypothesis that incremental decod-
ing can benefit by using partial decoder states for
decoding every additional word in the input sen-
tence. In order to do this, we run our incremen-
tal decoder in both regular beam search mode and
in incremental decoding mode. In regular beam
search mode, we forced the beam search decoder
to re-decode the sentence fragments for every ad-
ditional word and in incremental decoding mode,
we used the partial decoding states to incremen-
tally decode lastly added word. We then compare
the BLEU and TER scores between them to vali-
date our hypothesis.
We further test effectiveness of delayed prun-
ing (DP) in incremental decoding by comparing
it to the case where we turn off the DP. For in-
cremental decoding, we set the beam size and the
race course limit (for DP) to be 3. Additionally,
we used a threshold of −2.0 (in log-scale) for nor-
malized LM in the delay phase of DP, which was
obtained by testing on a separate development test
set.
We would like to highlight two observations
from the results in Table 2. First the regular beam
search indicate possible search errors due to the
small beam size (cube pruning pop limit) and the
BLEU scores has decreased by 0.56 for Fr-En
and by over 2.5 for En-Fr, than the scores cor-
responding to a beam size of 100 shown in Ta-
ble 1. Secondly, we find the incremental decoding
to perform better for the same beam size. How-
ever, incremental decoding without delay pruning
still seems to incur search errors when compared
with the regular decoding with a larger beam. De-
layed pruning alleviates this issue and improves
the BLEU and TER significantly. This we believe,
is mainly because the strategy to delay the pruning
retains the potentially better partial hypotheses for
every coverage set. It should be noted that results
in Table 2 pertain only to our decoder implemen-
tation and not with Moses.
We now give a comparative note between our
approach and the pruning strategy in regular beam
search. Delaying the hypothesis pruning is the im-
portant aspect in our approach to incremental de-
coding. In the case of regular beam search, the
hypotheses are pruned when they fall out of the
beam and the idea is to have a larger beam size
to avoid the early pruning of potentially good can-
didates. With the advent of cube pruning (Huang
and Chiang, 2007), the ’cube pruning pop limit’
(in Moses) determines the number of hypotheses
retained in each stack. In both the cases, it is pos-
sible that some of the coverage sets go unrepre-
sented in the stack due to poor candidate scores.
This is not desirable in the incremental decoding
setting as this might lead to search errors while
decoding a partial sentence.
Additionally, Moses offers an option (cube
</bodyText>
<page confidence="0.989605">
220
</page>
<table confidence="0.9991085">
Decoder Fr-En En-Fr
BLEU TER BLEU TER
Moses 26.98 0.551 27.24 0.610
Our decoder 27.53 0.541 26.96 0.657
</table>
<tableCaption confidence="0.997631">
Table 1: Regular beam search: Moses v.s. Our decoder
</tableCaption>
<table confidence="0.9998124">
Decoder Fr-En En-Fr
BLEU TER BLEU TER
Re-decode w/ beam search 26.96 0.548 24.33 0.635
ID w/o delay pruning 27.01 0.547 25.00 0.618
ID w/ delay pruning 27.62 0.545 25.45 0.616
</table>
<tableCaption confidence="0.998972">
Table 2: BLEU and TER: Re-decoding v.s. Incremental Decoding (ID)
</tableCaption>
<bodyText confidence="0.99990855">
pruning diversity) to control the number of hy-
potheses generated for each coverage set (though
set to ’0’ by default). It might be possible to use
this in conjunction with cube pruning pop limit as
an alternative to our delayed pruning in the incre-
mental decoding setting (with the risk of combina-
torial explosion in the search space).
In contrast, the delayed pruning not only avoids
search errors but also provides a dynamically man-
ageable search space (refer section 4.2.2) by re-
taining the best of the potential candidates. In a
practical scenario like real-time translation of in-
ternet chat, translation speed is an important con-
sideration. Furthermore, it is better to avoid large
number of candidates and generate only few best
ones, as only the top few translations will be used
by the system. Thus we believe our delayed prun-
ing approach to be a principled pruning strategy
that combines the different factors in an elegant
framework.
</bodyText>
<subsectionHeader confidence="0.900674">
4.2.1 Search Errors
</subsectionHeader>
<bodyText confidence="0.999852647058824">
As BLEU only indirectly indicates the number
of search errors made by algorithm, we used a
more direct way of quantifying the search errors
incurred by the ID in comparison to regular beam
search. We define the search error to be the differ-
ence between the translation scores of the best hy-
potheses produced by the ID and the regular beam
search and then compute the mean squared error
(MSE) for the entire test set. We use this method
to compare ID in the two settings of delayed prun-
ing being turned off (using a smaller beam size
of 3 to simulate the requirements of near instanta-
neous translations in real-time environments) and
delayed pruning turned on. We compare the model
score in these cases with the model score for the
best result obtained from the regular beam search
decoder (using a larger beam of size 100).
</bodyText>
<table confidence="0.9998094">
Direction Beam search against
Incremental Decoding
w/o DP w/DP
Fr-En 0.3823 0.3235
En-Fr 1.1559 0.6755
</table>
<tableCaption confidence="0.999514">
Table 3: Search Errors in Incremental Decoding
</tableCaption>
<bodyText confidence="0.9999898">
The results are shown in Table 3 and as can be
clearly seen, ID shows much lesser mean square
error with the DP turned on than when it is turned
off. Together the BLEU and TER numbers and
the mean square search error show that delayed
pruning is useful in the incremental decoding set-
ting. Comparing the En-Fr and Fr-En results show
that the two language pairs show slightly different
characteristics but the experiments in both direc-
tions support our overall conclusions.
</bodyText>
<subsectionHeader confidence="0.79968">
4.2.2 Speed
</subsectionHeader>
<bodyText confidence="0.999990461538462">
In this experiment, we set out to evaluate the
ID against the regular beam-search in which sen-
tence fragments are incrementally decoded for ad-
ditional words. In order compare with the in-
cremental decoder, we modified the regular de-
coder to decode the partial phrases, so that it re-
decodes the partial phrase from the scratch instead
of reusing the earlier state.
We ran the timing experiments on a Dell ma-
chine with an Intel Core i7 processor and 12 GB
memory, clocking 2.67 GHz and running Linux
(CentOS 5.3). We measured the time taken for de-
coding the fragment with every word added and
</bodyText>
<page confidence="0.992172">
221
</page>
<bodyText confidence="0.999976518518518">
averaged it first over the sentence and then the en-
tire test set. The average time (in msecs) includes
the future cost computation for both. We also mea-
sured the average number of hypotheses for every
bin at the end of decoding a complete sentence,
which was also averaged over the test set.
The results in Table 4 show that the incremen-
tal decoder was significantly faster than the beam
search in re-decoding mode almost by a factor of
9 in the best case (for Fr-En). The speedup is pri-
marily due to two factors, i) computing the future
cost for the new phrases as opposed to computing
it for all the phrases and ii) using partial decoder
states without having to re-generate hypotheses
through the cube pruning step and the latencies
associated with computing LM scores for them.
The addition of delayed pruning slowed down the
speed at most by 7 msecs (for En-Fr). In addition,
delayed pruning can be seen generating far more
hypotheses than the other two cases. Clearly, this
is because of the delay in pruning the tortoises un-
til the race course limit. Even with such signifi-
cantly large number of hypotheses being retained
for every bin, DP results in improved speed (over
re-decoding from scratch) and better performance
by avoiding search errors (compared to the incre-
mental decoder that does not use DP).
</bodyText>
<subsectionHeader confidence="0.999603">
4.3 Effect of Race course limit
</subsectionHeader>
<bodyText confidence="0.999980882352942">
Table 5 shows the effect of different race course
limits on translation quality measured using
BLEU. We generally expect the race course limit
to behave similar to the beam size as they both al-
low more hypotheses in the bin thereby reducing
search error although at the expense of increasing
decoding time.
However, in our experiments for Fr-En, we did
not find significant variations in BLEU for differ-
ent race course limits. This could be due to the
absence of long distance re-orderings between En-
glish and French and that the smallest race course
limit of 3 is sufficient for capturing all cases of lo-
cal re-ordering. As expected, we find the decoding
speed to slightly decrease and the average number
of hypotheses per bin to increase with the increas-
ing race course limit.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.97637264">
Google5 does seem to perform incremental decod-
ing, but the underlying algorithms are not public
5translate.google.com
knowledge. They may be simply re-translating the
input each time using a fast decoder or re-using
prior decoder states as we do here.
Intereactive translation using text prediction
strategies have been studied well (Foster et al.,
1997; Foster et al., 2002; Och et al., 2003). They
all attempt to interactively help the human user in
the postediting process, by suggesting completion
of the word/phrase based on the user accepted pre-
fix and the source sentece. Incremental feedback
is part of Caitra (Koehn, 2009) an interactive tool
for human-aided MT and works on a similar set-
ting to interactive MT. In Caitra, the source text
is pre-translated first and during the interactions it
dynamically generates user suggestions.
Our incremental decoder work differs from
these text prediction based approaches, in the
sense that the input text is not available to the de-
coder beforehand and the decoding is being done
dynamically for every source word as opposed to
generating suggestions dynamically for complet-
ing target sentece.
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975761904762">
We presented a modified beam search algorithm
for an efficient incremental decoder (ID), which
will allow translations to be generated incremen-
tally for every word typed by a user, instead of
waiting for the entire sentence as input by reusing
the partial decoder state. Our proposed modifica-
tions help us to efficiently compute partial future
costs in the incremental setting. We introduced the
notion of delayed pruning (DP) to avoid search
errors in incremental decoding. We showed that
reusing the partial decoder states is faster than re-
decoding the input from the scratch every time a
new word is typed by the user. Our exhaustive ex-
periments further demonstrated DP to be highly
effective in avoiding search errors under the in-
cremental decoding setting. In our experiments in
this paper we used a very tight beam size; in fu-
ture work, we would like to explore the tradeoff
between speed, accuracy and the utility of delayed
pruning by varying the beam size in our experi-
ments.
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999085333333333">
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text mediated interactive machine
translation. Machine Translation, 12(1/2):175–194.
</reference>
<page confidence="0.99638">
222
</page>
<table confidence="0.9987092">
Decoder Fr-En En-Fr
Avg time Avg Hyp/ bin Avg time Avg Hyp/ bin
Re-decode 724.46 2.21 130.29 2.32
ID w/o DP 84.85 2.89 27.58 2.89
ID w/DP 87.01 85.11 34.35 60.46
</table>
<tableCaption confidence="0.999032">
Table 4: Speed: Re-decoding v.s. Incremental Decoding (ID)
</tableCaption>
<figure confidence="0.99354375">
Race Fr-En En-Fr
Course BLEU Avg time Avg Hyp/ bin BLEU Avg time Avg Hyp/ bin
Limit
3 26.75 87.83 85.11 25.39 36.15 75.03
4 26.77 91.14 86.35 25.37 36.21 77.69
5 26.77 90.81 86.52 25.37 36.25 78.47
6 26.77 95.91 86.56 25.37 37.34 78.71
7 26.77 91.67 86.57 25.37 36.26 78.81
</figure>
<tableCaption confidence="0.934829">
Table 5: Effect of different race course limits
</tableCaption>
<reference confidence="0.999636306451613">
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In EMNLP ’02: Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing, pages 148–155, Morristown, NJ, USA.
Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144–151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn Taylor, editors, AMTA, volume 3265 of Lec-
ture Notes in Computer Science, pages 115–124.
Springer.
Philipp Koehn. 2009. A web-based interactive com-
puter aided translation tool. In In Proceedings of
ACL-IJCNLP 2009: Software Demonstrations, Sun-
tec, Singapore, August.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In EACL ’03: Proceedings of the
tenth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 387–
393, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL.
Michael Pust and Kevin Knight. 2009. Faster mt
decoding through pervasive laziness. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 141–144,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas: AMTA 2006.
</reference>
<page confidence="0.999162">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.299860">
<title confidence="0.592572">Incremental Decoding for Phrase-based Statistical Machine Translation Baskaran Sankaran, Ajeet Grewal and Anoop School of Computing</title>
<author confidence="0.573562">Simon Fraser</author>
<affiliation confidence="0.8112">8888 University</affiliation>
<address confidence="0.86784">Burnaby BC. V5A 2Y1.</address>
<email confidence="0.901318">asg10,</email>
<abstract confidence="0.998760266666667">this paper we focus on the a statistical phrase-based machine translation system. In incremental decoding, translations are generated incrementally for every word typed by a user, instead of waiting for the entire sentence as input. We introduce a novel modification to the beam-search decoding algorithm for phrase-based MT to address this issue, aimed at efficient computation of future costs and avoiding search errors. Our objective is to do a faster translation during incremental decoding without significant reduction in the translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Pierre Plamondon</author>
</authors>
<title>Target-text mediated interactive machine translation.</title>
<date>1997</date>
<booktitle>Machine Translation,</booktitle>
<pages>12--1</pages>
<contexts>
<context position="27429" citStr="Foster et al., 1997" startWordPosition="4676" endWordPosition="4679">e smallest race course limit of 3 is sufficient for capturing all cases of local re-ordering. As expected, we find the decoding speed to slightly decrease and the average number of hypotheses per bin to increase with the increasing race course limit. 5 Related Work Google5 does seem to perform incremental decoding, but the underlying algorithms are not public 5translate.google.com knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input </context>
</contexts>
<marker>Foster, Isabelle, Plamondon, 1997</marker>
<rawString>George Foster, Pierre Isabelle, and Pierre Plamondon. 1997. Target-text mediated interactive machine translation. Machine Translation, 12(1/2):175–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>User-friendly text prediction for translators.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>148--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27450" citStr="Foster et al., 2002" startWordPosition="4680" endWordPosition="4683">e limit of 3 is sufficient for capturing all cases of local re-ordering. As expected, we find the decoding speed to slightly decrease and the average number of hypotheses per bin to increase with the increasing race course limit. 5 Related Work Google5 does seem to perform incremental decoding, but the underlying algorithms are not public 5translate.google.com knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input text is not available</context>
</contexts>
<marker>Foster, Langlais, Lapalme, 2002</marker>
<rawString>George Foster, Philippe Langlais, and Guy Lapalme. 2002. User-friendly text prediction for translators. In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 148–155, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="16084" citStr="Huang and Chiang, 2007" startWordPosition="2743" endWordPosition="2746">o Moses, which we also use for training and for minimum error rate training (MERT) of the loglinear model for translation (Och, 2003; Koehn et al., 2007). Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f|e), p(elf), plex(fle) and plex(e|f), where e and f are target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature. We used Giza++ and Moses respectively for aligning the sentences and training the system. The decoder was written in Java and includes cube pruning (Huang and Chiang, 2007) and lazier cube pruning (Pust and Knight, 2009) functionalities as part of the decoder. Our decoder supports both regular beam search (similar to Moses) and incremental decoding. In our experiments we experimented various approaches for storing partial decoder states including memcache and transactional persistence using JDBM but found that the serialization and deserialization of decoder objects directly into and from the memory to work better in terms of speed and memory requirements. The partial object is retrieved and deserialized from the memory when required by the incremental decoder. </context>
<context position="21117" citStr="Huang and Chiang, 2007" startWordPosition="3594" endWordPosition="3597">s the potentially better partial hypotheses for every coverage set. It should be noted that results in Table 2 pertain only to our decoder implementation and not with Moses. We now give a comparative note between our approach and the pruning strategy in regular beam search. Delaying the hypothesis pruning is the important aspect in our approach to incremental decoding. In the case of regular beam search, the hypotheses are pruned when they fall out of the beam and the idea is to have a larger beam size to avoid the early pruning of potentially good candidates. With the advent of cube pruning (Huang and Chiang, 2007), the ’cube pruning pop limit’ (in Moses) determines the number of hypotheses retained in each stack. In both the cases, it is possible that some of the coverage sets go unrepresented in the stack due to poor candidate scores. This is not desirable in the incremental decoding setting as this might lead to search errors while decoding a partial sentence. Additionally, Moses offers an option (cube 220 Decoder Fr-En En-Fr BLEU TER BLEU TER Moses 26.98 0.551 27.24 0.610 Our decoder 27.53 0.541 26.96 0.657 Table 1: Regular beam search: Moses v.s. Our decoder Decoder Fr-En En-Fr BLEU TER BLEU TER Re</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1898" citStr="Koehn et al., 2007" startWordPosition="288" endWordPosition="291">t the user input and translation happen sequentially without any interaction between the two phases. In this paper we study decoding for SMT with the constraint that translations are to be generated incrementally for every word typed in by the user. Such a translation service can be used for language learning, where the user is fluent in the target language and experiments with many different source language sentences interactively, or in real-time translation environments such as speechspeech translation or translation during interactive chats. We use a phrase-based decoder similar to Moses (Koehn et al., 2007) and propose novel modifications in the decoding algorithm to tackle incremental decoding. Our system maintains a partial decoder state at every stage and uses it while decoding for each newly added word. As the decoder has access only to the partial sentence at every stage, the future costs change with every additional word and this has to be taken into account while continuing from an existing partial decoder state. Another major issue is that as incremental decoding is provided new input one word at at time, some of the entries that were pruned out at an earlier decoder state might later tu</context>
<context position="15614" citStr="Koehn et al., 2007" startWordPosition="2666" endWordPosition="2669">se has in order to get into the decoder beam. When a tortoise improves in score and breaks into the beam during cube pruning, it is deflagged as a tortoise and enters the regular decoding stream. We found DP to be effective in reducing the search error for incremental decoder in our experiments. 4 Evaluation and Discussion The evaluation was performed using our own implementation of the beam-search decoding algorithms. The architecture of our system is similar to Moses, which we also use for training and for minimum error rate training (MERT) of the loglinear model for translation (Och, 2003; Koehn et al., 2007). Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f|e), p(elf), plex(fle) and plex(e|f), where e and f are target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature. We used Giza++ and Moses respectively for aligning the sentences and training the system. The decoder was written in Java and includes cube pruning (Huang and Chiang, 2007) and lazier cube pruning (Pust and Knight, 2009) functionalities as part of the decoder. Our decoder supports both regular beam se</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3265</volume>
<pages>115--124</pages>
<editor>In Robert E. Frederking and Kathryn Taylor, editors, AMTA,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3113" citStr="Koehn, 2004" startWordPosition="493" endWordPosition="494">out to better candidates resulting in search errors compared to decoding the entire sentence at once. It is to be noted that, the search error problem is related to the inability to compute full future cost in incremental decoding. Our proposed modifications address these twin challenges and allow for efficient incremental decoding. 2 Incremental Decoding 2.1 Beam Search for Phrase-based SMT In this section we review the usual beam search decoder for phrase-based MT because we present our modifications for incremental decoding using the same notation. Beam search decoding for phrasebased SMT (Koehn, 2004) begins by collecting the translation options from the phrase table for all possible phrases of a given input sentence and precomputes the future cost for all possible contiguous sequences in the sentence. The pseudo-code for the usual beam-search decoding algorithm is illustrated in Algorithm 1. The decoder creates n bins for storing hypotheses grouped by the number of source words covered. Starting from a null hypothesis in bin 0, the decoder iterates through bins 1 though n filling them with new hypotheses by extending the entries in the earlier bins. A hypothesis contains the target words </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Robert E. Frederking and Kathryn Taylor, editors, AMTA, volume 3265 of Lecture Notes in Computer Science, pages 115–124. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A web-based interactive computer aided translation tool. In</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009: Software Demonstrations,</booktitle>
<location>Suntec, Singapore,</location>
<contexts>
<context position="27707" citStr="Koehn, 2009" startWordPosition="4724" endWordPosition="4725">o perform incremental decoding, but the underlying algorithms are not public 5translate.google.com knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input text is not available to the decoder beforehand and the decoding is being done dynamically for every source word as opposed to generating suggestions dynamically for completing target sentece. 6 Conclusion and Future Work We presented a modified beam search algorithm for an eff</context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009. A web-based interactive computer aided translation tool. In In Proceedings of ACL-IJCNLP 2009: Software Demonstrations, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Efficient search for interactive statistical machine translation.</title>
<date>2003</date>
<booktitle>In EACL ’03: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>387--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27469" citStr="Och et al., 2003" startWordPosition="4684" endWordPosition="4687">cient for capturing all cases of local re-ordering. As expected, we find the decoding speed to slightly decrease and the average number of hypotheses per bin to increase with the increasing race course limit. 5 Related Work Google5 does seem to perform incremental decoding, but the underlying algorithms are not public 5translate.google.com knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input text is not available to the decoder bef</context>
</contexts>
<marker>Och, Zens, Ney, 2003</marker>
<rawString>Franz Josef Och, Richard Zens, and Hermann Ney. 2003. Efficient search for interactive statistical machine translation. In EACL ’03: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics, pages 387– 393, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="15593" citStr="Och, 2003" startWordPosition="2664" endWordPosition="2665">ps a tortoise has in order to get into the decoder beam. When a tortoise improves in score and breaks into the beam during cube pruning, it is deflagged as a tortoise and enters the regular decoding stream. We found DP to be effective in reducing the search error for incremental decoder in our experiments. 4 Evaluation and Discussion The evaluation was performed using our own implementation of the beam-search decoding algorithms. The architecture of our system is similar to Moses, which we also use for training and for minimum error rate training (MERT) of the loglinear model for translation (Och, 2003; Koehn et al., 2007). Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f|e), p(elf), plex(fle) and plex(e|f), where e and f are target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature. We used Giza++ and Moses respectively for aligning the sentences and training the system. The decoder was written in Java and includes cube pruning (Huang and Chiang, 2007) and lazier cube pruning (Pust and Knight, 2009) functionalities as part of the decoder. Our decoder supports</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WieJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="17606" citStr="Papineni et al., 2002" startWordPosition="2987" endWordPosition="2990">T. To benchmark our Java decoder, we compare it with Moses by running it in regular beam search mode. The Moses systems were also optimized separately on the WMT07 devsets. Apart from comparing our decoder with Moses in regular beam search, we also compared the incremental decoding with regular regular beam using our decoder. To make it comparable with incremental decoding, we used the regular beam search to re-decode the sentence fragments for every additional word in the input sentence. We measured the following parameters in our empirical analysis: translation quality (as measured by BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), search errors and translation speed. Finally, we also measured the effect of different race course limits on BLEU and decoding speed for incremental decoding. 4.1 Benchmarking our decoder In this section we compare our decoder with Moses for regular beam search decoding. Table 1 gives the BLEU and TER for the two language pairs. Our decoder implementation compares favourably with Moses for Fr-En: the slightly better BLEU and TER for our decoder in Fr-En is possibly due to the minor differences in the configuration settings. For En-Fr translation, Moses performs</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WieJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pust</author>
<author>Kevin Knight</author>
</authors>
<title>Faster mt decoding through pervasive laziness.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>141--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="11106" citStr="Pust and Knight, 2009" startWordPosition="1876" endWordPosition="1879">Figure 1: Illustration of Lazier Cube Pruning The inner for-loop corresponds to the extension of hypotheses sets (grouped by same coverage set) to generate new hypotheses. Here a distinction is made between hypotheses Hold corresponding to previous decoder state Sp−1 and hypotheses Hnew resulting from the addition of word si. Hold is extended only using the newly found phrases Pnew, whereas the newer hypotheses are processed as in regular beam-search. 2.4 Lazier Cube Pruning We have adapted the pervasive lazy algorithm (or ’lazier cube pruning’) proposed originally for Hiero-style systems by (Pust and Knight, 2009) for our phrase-based system. This step corresponds to the lines 5 − 9 of algorithm 2 and allows us to only generate as many hypotheses as specified by the configurable parameters, beam size and beam threshold. Figure 1 illustrates the process of lazier cube pruning for a single bin. At the highest level it uses a priority queue, which is populated by the different hyper-edges or surfaces3, each corresponding to a pair of hypotheses that are being merged to create a new hypothesis. New hypotheses are generated iteratively, such that the hypothesis with the highest score is chosen in each itera</context>
<context position="16132" citStr="Pust and Knight, 2009" startWordPosition="2751" endWordPosition="2754">inimum error rate training (MERT) of the loglinear model for translation (Och, 2003; Koehn et al., 2007). Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f|e), p(elf), plex(fle) and plex(e|f), where e and f are target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature. We used Giza++ and Moses respectively for aligning the sentences and training the system. The decoder was written in Java and includes cube pruning (Huang and Chiang, 2007) and lazier cube pruning (Pust and Knight, 2009) functionalities as part of the decoder. Our decoder supports both regular beam search (similar to Moses) and incremental decoding. In our experiments we experimented various approaches for storing partial decoder states including memcache and transactional persistence using JDBM but found that the serialization and deserialization of decoder objects directly into and from the memory to work better in terms of speed and memory requirements. The partial object is retrieved and deserialized from the memory when required by the incremental decoder. We evaluated the incremental decoder for transla</context>
</contexts>
<marker>Pust, Knight, 2009</marker>
<rawString>Michael Pust and Kevin Knight. 2009. Faster mt decoding through pervasive laziness. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 141–144, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas: AMTA</booktitle>
<contexts>
<context position="17636" citStr="Snover et al., 2006" startWordPosition="2993" endWordPosition="2996">, we compare it with Moses by running it in regular beam search mode. The Moses systems were also optimized separately on the WMT07 devsets. Apart from comparing our decoder with Moses in regular beam search, we also compared the incremental decoding with regular regular beam using our decoder. To make it comparable with incremental decoding, we used the regular beam search to re-decode the sentence fragments for every additional word in the input sentence. We measured the following parameters in our empirical analysis: translation quality (as measured by BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), search errors and translation speed. Finally, we also measured the effect of different race course limits on BLEU and decoding speed for incremental decoding. 4.1 Benchmarking our decoder In this section we compare our decoder with Moses for regular beam search decoding. Table 1 gives the BLEU and TER for the two language pairs. Our decoder implementation compares favourably with Moses for Fr-En: the slightly better BLEU and TER for our decoder in Fr-En is possibly due to the minor differences in the configuration settings. For En-Fr translation, Moses performs better in both metrics. There</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas: AMTA 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>