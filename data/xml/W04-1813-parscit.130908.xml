<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000534">
<note confidence="0.962117">
CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 87
</note>
<title confidence="0.9772695">
Determining the Specificity of Terms based on Information Theoretic
Measures
</title>
<author confidence="0.606538">
Pum-Mo Ryu and Key-Sun Choi
</author>
<sectionHeader confidence="0.3678375" genericHeader="abstract">
Dept. EECS/KORTERM KAIST
373-1 Guseong-dong Yuseong-gu
305-701 Daejeon
Korea
</sectionHeader>
<email confidence="0.853706">
pmryu@world.kaist.ac.kr, kschoi@world.kaist.ac.kr
</email>
<sectionHeader confidence="0.992398" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.999765076923077">
This paper introduces new specificity
determining methods for terms based on
information theoretic measures. The
specificity of terms represents the quantity of
domain specific information that is contained
in the terms. Compositional and contextual
information of terms are used in proposed
methods. As the methods don’t rely on domain
dependent information, they can be applied to
other domains without extra processes.
Experiments showed very promising results
with the precision 82.0% when applied to the
terms in MeSH thesaurus.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961166666667">
The specificity of terms represents the quantity of
domain specific information contained in the terms.
If a term has large quantity of domain specific
information, the specificity of the term is high. The
specificity of a term X is quantified to positive real
number as equation (1).
</bodyText>
<equation confidence="0.9985075">
Spec(X) R+
∈ (1)
</equation>
<bodyText confidence="0.999462038461538">
The specificity is a kind of necessary condition
for term hierarchy, i.e., if X1 is one of ancestors of
X2, then Spec(X1) is less than Spec(X2). Thus this
condition can be applied to automatic construction
or evaluation of term hierarchy. The specificity
also can be applied to automatic term recognition.
Many domain specific terms are multiword
terms. When domain specific concepts are
represented as multiword terms, the terms are
classified into two categories based on composition
of unit words. In the first category, new terms are
created by adding modifiers to existing terms. For
example “insulin-dependent diabetes mellitus” was
created by adding modifier “insulin-dependent” to
its hypernym “diabetes mellitus” as in Table 1. In
English, the specific level terms are very
commonly compounds of the generic level term
and some modifier (Croft, 2004). In this case,
compositional information is important to get
meaning of the terms. In the second category, new
terms are independent of existing terms. For
example, “wolfram syndrome” is semantically
related to its ancestor terms as in Table 1. But it
shares no common words with its ancestor terms.
In this case, contextual information is important to
get meaning of the terms.
</bodyText>
<table confidence="0.8624612">
Node Number Terms
C18.452.297 diabetes mellitus
C18.452.297.267 insulin-dependent diabet
es mellitus
C18.452.297.267.960 wolfram syndrome
</table>
<tableCaption confidence="0.9410525">
Table 1 Subtree of MeSH1 thesaurus. Node
numbers represent hierarchical structure of terms
</tableCaption>
<bodyText confidence="0.999875294117647">
Contextual information has been mainly used to
represent the meaning of terms in previous works.
(Grefenstette, 1994) (Pereira, 1993) and
(Sanderson, 1999) used contextual information to
find hyponymy relation between terms. (Caraballo,
1999) also used contextual information to
determine the specificity of nouns. Contrary,
compositional information of terms has not been
commonly discussed. We propose new specificity
measuring methods based on both compositional
and contextual information. The methods are
formulated as information theory like measures.
This paper consists as follow; new specificity
measuring methods are introduced in section 2, and
the experiments and evaluation on the methods are
discussed in section 3, finally conclusions are
drawn in section 4.
</bodyText>
<sectionHeader confidence="0.676169" genericHeader="method">
2 Specificity Measuring Methods
</sectionHeader>
<footnote confidence="0.840463166666667">
In this section, we describe information theory like
methods to measure the specificity of terms. Here,
we call information theory like methods, because
some probability values used in these methods are
1 MeSH is available at http://www.nlm.nih.gov/mesh.
MeSH 2003 was used in this research.
</footnote>
<note confidence="0.590695">
88 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology
</note>
<bodyText confidence="0.99901425">
not real probability, rather they are relative weight
of terms or words.
In information theory, when a low probability
message occurs on channel output, the quantity of
surprise is large, and the length of bits to represent
the message becomes long. Thus the large quantity
of information is gained by the message (Haykin,
1994). If we regard the terms in corpus as the
messages of channel output, the information
quantity of the terms can be measured using
information theory. A set of target terms is defined
as equation (2) for further explanation.
</bodyText>
<equation confidence="0.981050833333333">
T={tk|1≤k≤n} (2)
where tk is a term. In next step, a discrete random
variable X is defined as equation (3).
X = x ≤ k≤ n p x =
{ k |1 } ( k) Prob(X = xk (3)
)
</equation>
<bodyText confidence="0.9579675">
where xk is an event of tk is observed in corpus,
p(xk) is the probability of xk. The information
quantity, I(xk), gained after observing xk, is used as
the specificity of tk as equation (4).
Spec(tk) ≈ I(xk) = −log p(xk) (4)
By equation (4), we can measure the specificity
of tk, by estimating p(xk). We describe three
estimating methods for p(xk) in following sections.
</bodyText>
<subsectionHeader confidence="0.6941425">
2.1 Compositional Information based Method
(Method 1)
</subsectionHeader>
<bodyText confidence="0.977303">
By compositionality, the meaning of a term can be
strictly predicted from the meaning of the
individual words (Manning, 1999). This method is
divided into two steps: In the first step, the
specificity of each word is measured independently.
In the second step, the specificity of composite
words is summed up. For detail description, we
assume that tk consists of one or more words as
equation (5).
tk = w1w2... wm (5)
where wi is i-th word in tk. In next step, a discrete
random variable Y is defined as equation (6).
</bodyText>
<equation confidence="0.998569666666667">
Y = y ≤ i≤ m p y =
{ i |1 } ( i ) Prob( )
Y = yi (6)
</equation>
<bodyText confidence="0.99971825">
where yi is an event of wi occurs in term tk, p(yi) is
the probability of yi. Information quantity, I(xk), in
equation (4) is redefined as equation (7) based on
previous assumption.
</bodyText>
<equation confidence="0.967395571428571">
m
I x
( ) = −∑ p y
( )log ( )
p y
k i i
1
</equation>
<bodyText confidence="0.998198">
where I(xk) is average information quantity of all
words in tk. In this mechanism, p(yi) of informative
words should be smaller than that of non
informative words. Two information sources, word
frequency, tf.idf are used to estimate p(yi)
independently.
We assume that if a term is composed of low
frequency words, the term have large quantity of
domain information. Because low frequency words
appear in limited number of terms, they have high
discriminating ability. On this assumption, p(yi) in
equation (7) is estimated as relative frequency of wi
in corpus. In this estimation, P(yi) for low
frequency words becomes small.
tf.idf is widely used term weighting scheme in
information retrieval (Manning, 1999). We assume
that if a term is composed of high tf.idf words, the
term have domain specific information. On this
assumption, p(yi) in equation (7) is estimated as
equation (8).
</bodyText>
<equation confidence="0.98869975">
Ayi) ≈ pMLE(w) =1− tf ⋅ idf (w)
i (8)
∑ tf ⋅ idf (wi )
j
</equation>
<bodyText confidence="0.999761380952381">
where tf·idf(w) is tf.idf value of w. In this equation,
p(yi) of high tf.idf words becomes small.
If the modifier-head structure is known, the
specificity of the term is calculated incrementally
starting from head noun. In this manner, the
specificity of the term is always larger than that of
the head term. This result answers to the
assumption that more specific term has higher
specificity. We use simple nesting relations
between terms to analyze modifier-head structure
as follows (Frantzi, 2000):
Definition 1 If two terms X and Y are terms in
same semantic category and X is nested in Y as
W1XW2, then X is head term, and W1 and W2 are
modifiers of X.
For example, because “diabetes mellitus” is
nested in “insulin dependent diabetes mellitus” and
two terms are all disease names, “diabetes
mellitus” is head term and “insulin dependent” is
modifier. The specificity of Y is measured as
equation (9).
</bodyText>
<equation confidence="0.994082">
Spec(Y) = Spec(X)+α ⋅ Spec(W1)+β ⋅ Spec(W2) (9)
</equation>
<bodyText confidence="0.9999575">
where Spec(X), Spec(W1), and Spec(W2) are the
specificity of X, W1, W2 respectively. α and β are
weighting schemes for the specificity of modifiers.
They are found by experimentally.
</bodyText>
<subsectionHeader confidence="0.597371">
2.2 Contextual Information based Method
(Method 2)
</subsectionHeader>
<bodyText confidence="0.99998575">
There are some problems that are hard to address
using compositional information alone. Firstly,
although two disease names, “wolfram syndrome”
and “insulin-dependent diabetes mellitus”, share
</bodyText>
<equation confidence="0.73306525">
(7)
i
=
CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 89
</equation>
<bodyText confidence="0.999739413793103">
many common features in semantic level, they
don’t share any common words in lexical level. In
this case, it is unreasonable to compare two
specificity values based on compositional
information. Secondly, when several words are
combined into one term, there are additional
semantic components that are not predicted by unit
words. For example, “wolfram syndrome” is a kind
of “diabetes mellitus”. We can not predict the
meaning of “diabetes mellitus” from two separate
words “wolfram” and “syndrome”. Thus we use
contextual information to address these problems.
General terms are frequently modified by other
words in corpus. Because domain specific terms
have sufficient information in themselves, they are
rarely modified by other words, (Caraballo, 1999).
Under this assumption, we use probability
distribution of modifiers as contextual information.
Collecting sufficient modifiers from given corpus
is very important in this method. To this end, we
use Conexor functional dependency parser
(Conexor, 2004) to analyze the structure of
sentences. Among many dependency functions
defined in the parser, “attr” and “mod” functions
are used to extract modifiers from analyzed
structures. This method can be applied the terms
that are modified by other words in corpus.
Entropy of modifiers for a term is defined as
equation (10).
</bodyText>
<equation confidence="0.9940426">
H t = −∑ p mod t
mod ( k ) ( , )log ( , )
p mod t (10)
i k i k
i
</equation>
<bodyText confidence="0.999986222222222">
where p(modi,tk) is the probability of modi modifies
tk and it is estimated as relative frequency of modi
in all modifiers of tk. The entropy calculated by
equation (10) is the average information quantity
of all (modi,tk) pairs. Because domain specific
terms have simple modifier distributions, the
entropy of the terms is low. Therefore inversed
entropy is assigned to I(xk) in equation (4) to make
specific terms get large quantity of information.
</bodyText>
<equation confidence="0.990926">
I (x k) max( mod ( i )) mod ( k )
≈ H t H t
−
1 ≤ ≤
i n
</equation>
<bodyText confidence="0.999934">
where the first term of approximation is the
maximum modifier entropy of all terms.
</bodyText>
<subsectionHeader confidence="0.992802">
2.3 Hybrid Method (Method 3)
</subsectionHeader>
<bodyText confidence="0.99900475">
In this section, we describe hybrid method to
overcome shortcomings of previous two methods.
In this method the specificity is measured as
equation (12).
</bodyText>
<equation confidence="0.960051666666667">
I(xk) ≈ 1 1 1
γ (Imp
(xk))+(1−γ)(ICtx(xk))
</equation>
<bodyText confidence="0.999872857142857">
where ICmp(xk) and ICtx(xk) are information quantity
measured by method1 and method 2 respectively.
They are normalized value between 0 and 1.
γ(0≤ γ ≤ 1) is weight of two values. If γ = 0.5 , the
equation is harmonic mean of two values.
Therefore I(xk) becomes large when two values are
equally large.
</bodyText>
<sectionHeader confidence="0.993446" genericHeader="evaluation">
3 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999844736842105">
In this section, we describe our experiments and
evaluate proposed methods.
We select a subtree of MeSH thesaurus for the
experiment. “metabolic diseases(C18.452)” node is
root of the subtree, and the subtree consists of 436
disease names which are target terms for
specificity measuring. We used MEDLINE 2
database corpus (170,000 abstracts, 20,000,000
words) to extract statistical information.
Each method was evaluated by two criteria,
coverage and precision. Coverage is the fraction of
the terms which have the specificity by given
method. Method 2 gets relatively lower coverage
than method 1, because method 2 can measure the
specificity only when both the terms and their
modifiers occur in corpus. Method 1 can measure
the specificity whenever parts of composite words
appear in corpus. Precision is the fraction of
correct specificity relations values as equation (13).
</bodyText>
<equation confidence="0.99494225">
# ( , )
of R p c with correct specificity
= (13)
# ( , )
</equation>
<bodyText confidence="0.971626333333333">
of all R p c
where R(p,c) is a parent-child relation in MeSH
thesaurus. If child term c has larger specificity than
that of parent term p, then the relation is said to
have correct specificity. We divided parent-child
relations into two types. Relations where parent
term is nested in child term are categorized as type
I. Other relations are categorized as type II. There
are 43 relations in type I and 393 relations in type
II. The relations in type I always have correct
specificity provided modifier-head information
method described in section 2.1 is applied.
We tested prior experiment for 10 human
subjects to find out the upper bound of precision.
The subjects are all medical doctors of internal
medicine, which is closely related division to
“metabolic diseases”. They were asked to identify
parent-child relationship for given term pairs. The
average precisions of type I and type II were
96.6% and 86.4% respectively. We set these values
as upper bound of precision for suggested methods.
</bodyText>
<footnote confidence="0.980484333333333">
2 MEDLINE is a database of biomedical articles
serviced by National Library of Medicine, USA.
(http://www.nlm.nih.gov)
</footnote>
<table confidence="0.9932846875">
p
90 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology
Methods Precision Coverage
Type I Type II Total
Human subjects(Average) 96.6 86.4 87.4
Term frequency 100.0 53.5 60.6 89.5
Term tf·idf 52.6 59.2 58.2 89.5
Compositional Word Freq. 37.2 72.5 69.0 100.0
Information
Method
(Method 1)
Word Freq.+Structure (α=0=0.2) 100.0 72.8 75.5 100.0
Word tf·idf 44.2 75.3 72.2 100.0
Word tf·idf +Structure (α=0=0.2) 100.0 76.6 78.9 100.0
Contextual Information Method (Method 2) (mod cnt&gt;1) 90.0 66.4 70.0 70.2
Hybrid Method (Method 3) (tf·idf + Struct, γ=0.8) 95.0 79.6 82.0 70.2
</table>
<tableCaption confidence="0.99978">
Table 2. Experimental results (%)
</tableCaption>
<bodyText confidence="0.999866428571429">
The specificity of terms was measured with
method 1, method 2, and method 3 as Table 2.
Two additional methods, based on term frequency
and term tf.idf, were experimented to compare
compositionality based methods and term based
methods.
Method 1 showed better performance than term
based methods. This result illustrate basic
assumption of this paper that specific concepts are
created by adding information to existing concepts,
and new concepts are expressed as new terms by
adding modifiers to existing terms. Word tf.idf
based method showed better precision than word
frequency based method. This result illustrate that
tf.idf of words is more informative than frequency
of words.
Method 3 showed the best precision, 82.0%,
because the two methods interacted
complementary. In hybrid method, the weight
value γ = 0.8 indicates that compositional
information is more informative than contextual
information for the specificity of domain specific
terms.
One reason of the errors is that the names of
some internal nodes in MeSH thesaurus are
category names rather disease names. For example,
as “acid-base imbalance (C18.452.076)” is name
of disease category, it doesn&apos;t occur as frequently
as other real disease names. Other predictable
reason is that we didn’t consider various surface
forms of same term. For example, although
“NIDDM” is acronym of “non insulin dependent
diabetes mellitus”, the system counted two terms
separately. Therefore the extracted statistics can’t
properly reflect semantic level information.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999929571428572">
This paper proposed specificity measuring
methods for terms based on information theory like
measures using compositional and contextual
information of terms. The methods are
experimented on the terms in MeSH thesaurus.
Hybrid method showed the best precision of 82.0%,
because two methods complemented each other.
As the proposed methods don&apos;t use domain
dependent information, they can be adapted to
other domains without extra processes.
In the future, we will modify the system to
handle various term formations such as abbreviated
form. Finally we will apply the proposed methods
to the terms of other specific domains.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99729725">
This work was supported in part by Ministry of
Science &amp; Technology, Ministry of Culture &amp;
Tourism of Korean government, and Korea
Science &amp; Engineering Foundation.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99937092">
Caraballo, S. A., Charniak, E. 1999. Determining
the Specificity of Nouns from Text. Proceedings
of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora
Conexor. 2004. Conexor Functional Dependency
Grammar Parser. http://www.conexor.com
Croft, W. 2004. Typology and Universals. 2nd ed.
Cambridge Textbooks in Linguistics, Cambridge
Univ. Press
Frantzi, K., et. al. 2000. Automatic recognition of
multi-word terms: the C-value/NC-value method.
Journal of Digital Libraries, vol. 3, num. 2
Grefenstette, G. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic
Publishers
Haykin, S. 1994. Neural Network. IEEE Press
Manning, C. D. and Schutze, H. 1999.
Foundations of Statistical Natural Language
Processing. The MIT Presss
Pereira, F., Tishby, N., and Lee, L. 1993.
Distributational clustering of English words.
Proceedings of ACL
Sanderson, M. 1999. Deriving concept hierarchies
from text. Proceedings of ACM S1GIR
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.260850">
<title confidence="0.808899">CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 87 Determining the Specificity of Terms based on Information Theoretic Measures</title>
<author confidence="0.970596">Pum-Mo Ryu</author>
<author confidence="0.970596">Key-Sun</author>
<affiliation confidence="0.975168">Dept. EECS/KORTERM</affiliation>
<address confidence="0.818666">373-1 Guseong-dong 305-701</address>
<email confidence="0.989431">pmryu@world.kaist.ac.kr,kschoi@world.kaist.ac.kr</email>
<abstract confidence="0.994752928571429">This paper introduces new specificity determining methods for terms based on information theoretic measures. The specificity of terms represents the quantity of domain specific information that is contained in the terms. Compositional and contextual information of terms are used in proposed methods. As the methods don’t rely on domain dependent information, they can be applied to other domains without extra processes. Experiments showed very promising results with the precision 82.0% when applied to the terms in MeSH thesaurus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S A Caraballo</author>
<author>E Charniak</author>
</authors>
<title>Determining the Specificity of Nouns from Text.</title>
<date>1999</date>
<booktitle>Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</booktitle>
<marker>Caraballo, Charniak, 1999</marker>
<rawString>Caraballo, S. A., Charniak, E. 1999. Determining the Specificity of Nouns from Text. Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conexor</author>
</authors>
<title>Conexor Functional Dependency Grammar Parser.</title>
<date>2004</date>
<note>http://www.conexor.com</note>
<contexts>
<context position="9212" citStr="Conexor, 2004" startWordPosition="1473" endWordPosition="1474">We can not predict the meaning of “diabetes mellitus” from two separate words “wolfram” and “syndrome”. Thus we use contextual information to address these problems. General terms are frequently modified by other words in corpus. Because domain specific terms have sufficient information in themselves, they are rarely modified by other words, (Caraballo, 1999). Under this assumption, we use probability distribution of modifiers as contextual information. Collecting sufficient modifiers from given corpus is very important in this method. To this end, we use Conexor functional dependency parser (Conexor, 2004) to analyze the structure of sentences. Among many dependency functions defined in the parser, “attr” and “mod” functions are used to extract modifiers from analyzed structures. This method can be applied the terms that are modified by other words in corpus. Entropy of modifiers for a term is defined as equation (10). H t = −∑ p mod t mod ( k ) ( , )log ( , ) p mod t (10) i k i k i where p(modi,tk) is the probability of modi modifies tk and it is estimated as relative frequency of modi in all modifiers of tk. The entropy calculated by equation (10) is the average information quantity of all (m</context>
</contexts>
<marker>Conexor, 2004</marker>
<rawString>Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
</authors>
<title>Typology and Universals. 2nd ed. Cambridge Textbooks in Linguistics,</title>
<date>2004</date>
<publisher>Univ. Press</publisher>
<location>Cambridge</location>
<contexts>
<context position="2042" citStr="Croft, 2004" startWordPosition="301" endWordPosition="302">ificity also can be applied to automatic term recognition. Many domain specific terms are multiword terms. When domain specific concepts are represented as multiword terms, the terms are classified into two categories based on composition of unit words. In the first category, new terms are created by adding modifiers to existing terms. For example “insulin-dependent diabetes mellitus” was created by adding modifier “insulin-dependent” to its hypernym “diabetes mellitus” as in Table 1. In English, the specific level terms are very commonly compounds of the generic level term and some modifier (Croft, 2004). In this case, compositional information is important to get meaning of the terms. In the second category, new terms are independent of existing terms. For example, “wolfram syndrome” is semantically related to its ancestor terms as in Table 1. But it shares no common words with its ancestor terms. In this case, contextual information is important to get meaning of the terms. Node Number Terms C18.452.297 diabetes mellitus C18.452.297.267 insulin-dependent diabet es mellitus C18.452.297.267.960 wolfram syndrome Table 1 Subtree of MeSH1 thesaurus. Node numbers represent hierarchical structure </context>
</contexts>
<marker>Croft, 2004</marker>
<rawString>Croft, W. 2004. Typology and Universals. 2nd ed. Cambridge Textbooks in Linguistics, Cambridge Univ. Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Frantzi</author>
</authors>
<title>Automatic recognition of multi-word terms: the C-value/NC-value method.</title>
<date>2000</date>
<journal>Journal of Digital Libraries,</journal>
<volume>3</volume>
<pages>2</pages>
<contexts>
<context position="7220" citStr="Frantzi, 2000" startWordPosition="1167" endWordPosition="1168">n, p(yi) in equation (7) is estimated as equation (8). Ayi) ≈ pMLE(w) =1− tf ⋅ idf (w) i (8) ∑ tf ⋅ idf (wi ) j where tf·idf(w) is tf.idf value of w. In this equation, p(yi) of high tf.idf words becomes small. If the modifier-head structure is known, the specificity of the term is calculated incrementally starting from head noun. In this manner, the specificity of the term is always larger than that of the head term. This result answers to the assumption that more specific term has higher specificity. We use simple nesting relations between terms to analyze modifier-head structure as follows (Frantzi, 2000): Definition 1 If two terms X and Y are terms in same semantic category and X is nested in Y as W1XW2, then X is head term, and W1 and W2 are modifiers of X. For example, because “diabetes mellitus” is nested in “insulin dependent diabetes mellitus” and two terms are all disease names, “diabetes mellitus” is head term and “insulin dependent” is modifier. The specificity of Y is measured as equation (9). Spec(Y) = Spec(X)+α ⋅ Spec(W1)+β ⋅ Spec(W2) (9) where Spec(X), Spec(W1), and Spec(W2) are the specificity of X, W1, W2 respectively. α and β are weighting schemes for the specificity of modifie</context>
</contexts>
<marker>Frantzi, 2000</marker>
<rawString>Frantzi, K., et. al. 2000. Automatic recognition of multi-word terms: the C-value/NC-value method. Journal of Digital Libraries, vol. 3, num. 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers</publisher>
<contexts>
<context position="2768" citStr="Grefenstette, 1994" startWordPosition="407" endWordPosition="408"> new terms are independent of existing terms. For example, “wolfram syndrome” is semantically related to its ancestor terms as in Table 1. But it shares no common words with its ancestor terms. In this case, contextual information is important to get meaning of the terms. Node Number Terms C18.452.297 diabetes mellitus C18.452.297.267 insulin-dependent diabet es mellitus C18.452.297.267.960 wolfram syndrome Table 1 Subtree of MeSH1 thesaurus. Node numbers represent hierarchical structure of terms Contextual information has been mainly used to represent the meaning of terms in previous works. (Grefenstette, 1994) (Pereira, 1993) and (Sanderson, 1999) used contextual information to find hyponymy relation between terms. (Caraballo, 1999) also used contextual information to determine the specificity of nouns. Contrary, compositional information of terms has not been commonly discussed. We propose new specificity measuring methods based on both compositional and contextual information. The methods are formulated as information theory like measures. This paper consists as follow; new specificity measuring methods are introduced in section 2, and the experiments and evaluation on the methods are discussed i</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, G. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Haykin</author>
</authors>
<title>Neural Network.</title>
<date>1994</date>
<publisher>IEEE Press</publisher>
<contexts>
<context position="4168" citStr="Haykin, 1994" startWordPosition="613" endWordPosition="614">Here, we call information theory like methods, because some probability values used in these methods are 1 MeSH is available at http://www.nlm.nih.gov/mesh. MeSH 2003 was used in this research. 88 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology not real probability, rather they are relative weight of terms or words. In information theory, when a low probability message occurs on channel output, the quantity of surprise is large, and the length of bits to represent the message becomes long. Thus the large quantity of information is gained by the message (Haykin, 1994). If we regard the terms in corpus as the messages of channel output, the information quantity of the terms can be measured using information theory. A set of target terms is defined as equation (2) for further explanation. T={tk|1≤k≤n} (2) where tk is a term. In next step, a discrete random variable X is defined as equation (3). X = x ≤ k≤ n p x = { k |1 } ( k) Prob(X = xk (3) ) where xk is an event of tk is observed in corpus, p(xk) is the probability of xk. The information quantity, I(xk), gained after observing xk, is used as the specificity of tk as equation (4). Spec(tk) ≈ I(xk) = −log p</context>
</contexts>
<marker>Haykin, 1994</marker>
<rawString>Haykin, S. 1994. Neural Network. IEEE Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Presss</publisher>
<marker>Manning, Schutze, 1999</marker>
<rawString>Manning, C. D. and Schutze, H. 1999. Foundations of Statistical Natural Language Processing. The MIT Presss</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributational clustering of English words.</title>
<date>1993</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, F., Tishby, N., and Lee, L. 1993. Distributational clustering of English words. Proceedings of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
</authors>
<title>Deriving concept hierarchies from text.</title>
<date>1999</date>
<booktitle>Proceedings of ACM S1GIR</booktitle>
<contexts>
<context position="2806" citStr="Sanderson, 1999" startWordPosition="412" endWordPosition="413">rms. For example, “wolfram syndrome” is semantically related to its ancestor terms as in Table 1. But it shares no common words with its ancestor terms. In this case, contextual information is important to get meaning of the terms. Node Number Terms C18.452.297 diabetes mellitus C18.452.297.267 insulin-dependent diabet es mellitus C18.452.297.267.960 wolfram syndrome Table 1 Subtree of MeSH1 thesaurus. Node numbers represent hierarchical structure of terms Contextual information has been mainly used to represent the meaning of terms in previous works. (Grefenstette, 1994) (Pereira, 1993) and (Sanderson, 1999) used contextual information to find hyponymy relation between terms. (Caraballo, 1999) also used contextual information to determine the specificity of nouns. Contrary, compositional information of terms has not been commonly discussed. We propose new specificity measuring methods based on both compositional and contextual information. The methods are formulated as information theory like measures. This paper consists as follow; new specificity measuring methods are introduced in section 2, and the experiments and evaluation on the methods are discussed in section 3, finally conclusions are d</context>
</contexts>
<marker>Sanderson, 1999</marker>
<rawString>Sanderson, M. 1999. Deriving concept hierarchies from text. Proceedings of ACM S1GIR</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>