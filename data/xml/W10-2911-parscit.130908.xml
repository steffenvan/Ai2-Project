<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000138">
<title confidence="0.998657">
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study
</title>
<author confidence="0.999479">
Roi Reichart1* Omri Abend2*† Ari Rappoport2
</author>
<affiliation confidence="0.9986945">
1ICNC 2Institute of Computer Science
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.996867">
{roiri|omria01|arir}@cs.huji.ac.il
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996288">
Clustering is a central technique in NLP.
Consequently, clustering evaluation is of
great importance. Many clustering algo-
rithms are evaluated by their success in
tagging corpus tokens. In this paper we
discuss type level evaluation, which re-
flects class membership only and is inde-
pendent of the token statistics of a partic-
ular reference corpus. Type level evalua-
tion casts light on the merits of algorithms,
and for some applications is a more natural
measure of the algorithm’s quality.
We propose new type level evaluation
measures that, contrary to existing mea-
sures, are applicable when items are pol-
ysemous, the common case in NLP. We
demonstrate the benefits of our measures
using a detailed case study, POS induc-
tion. We experiment with seven leading
algorithms, obtaining useful insights and
showing that token and type level mea-
sures can weakly or even negatively corre-
late, which underscores the fact that these
two approaches reveal different aspects of
clustering quality.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998971625">
Clustering is a central machine learning technique.
In NLP, clustering has been used for virtually ev-
ery semi- and unsupervised task, including POS
tagging (Clark, 2003), labeled parse tree induction
(Reichart and Rappoport, 2008), verb-type clas-
sification (Schulte im Walde, 2006), lexical ac-
quisition (Davidov and Rappoport, 2006; Davi-
dov and Rappoport, 2008), multilingual document
</bodyText>
<note confidence="0.643244666666667">
* Both authors equally contributed to this paper.
† Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
</note>
<bodyText confidence="0.999895146341463">
clustering (Montavlo et al., 2006), coreference res-
olution (Nicolae and Nicolae, 2006) and named
entity recognition (Elsner et al., 2009). Conse-
quently, the methodology of clustering evaluation
is of great importance. In this paper we focus
on external clustering evaluation, i.e., evaluation
against manually annotated gold standards, which
exist for almost all such NLP tasks. External eval-
uation is the dominant form of clustering evalu-
ation in NLP, although other methods have been
proposed (see e.g. (Frank et al., 2009)).
In this paper we discuss type level evaluation,
which evaluates the set membership structure cre-
ated by the clustering, independently of the token
statistics of the gold standard corpus. Many clus-
tering algorithms are evaluated by their success
in tagging corpus tokens (Clark, 2003; Nicolae
and Nicolae, 2006; Goldwater and Griffiths, 2007;
Gao and Johnson, 2008; Elsner et al., 2009). How-
ever, in many cases a type level evaluation is the
natural one. This is the case, for example, when
a POS induction algorithm is used to compute a
tag dictionary (the set of tags that each word can
take), or when a lexical acquisition algorithm is
used for constructing a lexicon containing the set
of frames that a verb can participate in, or when a
sense induction algorithm computes the set of pos-
sible senses of each word. In addition, even when
the goal is corpus tagging, a type level evaluation
is highly valuable, since it may cast light on the
relative or absolute merits of different algorithms
(as we show in this paper).
Clustering evaluation has been extensively in-
vestigated (Section 3). However, the discussion
centers around the monosemous case, where each
item belongs to exactly one cluster, although pol-
ysemy is the common case in NLP.
The contribution of the present paper is as fol-
lows. First, we discuss the issue of type level eval-
uation and explain why even in the monosemous
case a token level evaluation presents a skewed
</bodyText>
<page confidence="0.981257">
77
</page>
<note confidence="0.9553905">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999530625">
picture (Section 2). Second, we show for the
common polysemous case why adapting existing
information-theoretic measures to type level eval-
uation is not natural (Section 3). Third, we pro-
pose new mapping-based measures and algorithms
to compute them (Section 4). Finally, we perform
a detailed case study with part-of-speech (POS)
induction (Section 5). We compare seven lead-
ing algorithms, showing that token and type level
measures can weakly or even negatively correlate.
This shows that type level evaluation indeed re-
veals aspects of a clustering solution that are not
revealed by the common tagging-based evaluation.
Clustering is a vast research area. As far as we
know, this is the first NLP paper to propose type
level measures for the polysemous case.
</bodyText>
<sectionHeader confidence="0.994266" genericHeader="method">
2 Type Level Clustering Evaluation
</sectionHeader>
<bodyText confidence="0.999922154761905">
This section motivates why both type and token
level external evaluations should be done, even in
the monosemous case.
Clustering algorithms compute a set of induced
clusters (a clustering). Some algorithms directly
compute a clustering, while some others produce
a tagging of corpus tokens from which a clustering
can be easily derived. A clustering is monosemous
if each item is allowed to belong to a single cluster
only, and polysemous otherwise. An external eval-
uation is one which is based on a comparison of an
algorithm’s result to a gold standard. In this paper
we focus solely on external evaluation, which is
the most common evaluation approach in NLP.
Token and type level evaluations reflect differ-
ent aspects of a clustering. External token level
evaluation assesses clustering quality according to
the clustering’s accuracy on a given manually an-
notated corpus. This is certainly a useful evalua-
tion measure, e.g. when the purpose of the cluster-
ing algorithm is to annotate a corpus to serve as
input to another application.
External type level evaluation views the com-
puted clustering as a set membership structure and
evalutes it independently of the token statistics in
the gold standard corpus. There are two main
cases in which this is useful. First, a type level
evaluation can be the natural one in light of the
problem itself. For example, if the purpose of
the clustering algorithm is to automatically build
a lexicon (e.g., VerbNet (Kipper et al., 2000)),
then the lexicon structure itself should be evalu-
ated. Second, it may be valuable to decouple cor-
pus statistics from the induced clustering when the
latter is to be used for annotating corpora that ex-
hibit different statistics. In other words, if we eval-
uate an algorithm that will be invoked on a diverse
set of corpora having different token statistics, a
type level evaluation might provide a better picture
(or at least a complementary one) on the quality of
the clustering algorithm.
To motivate type level evaluation, consider POS
induction, which exemplifies both cases above.
Clearly, a word form may belong to several parts
of speech (e.g., ‘contrast’ is both a noun and a
verb, ‘fast’ is both an adjective and an adverb,
‘that’ can be a determiner, conjunction and adverb,
etc.). As an evaluation of a POS induction algo-
rithm, it is natural to evaluate the lexicon it gener-
ates, even if the main goal is to annotate a corpus.
The lexicon lists the possible POS tags for each
word, and thus its evaluation is a polysemous type
level one.
Even if we ignore polysemy, type level evalua-
tion is useful for a POS induction algorithm used
to tag a corpus. There are POS classes whose
members are very frequent, e.g., determiners and
prepositions. Here, a very small number of word
types usually accounts for a large portion of corpus
tokens. For example, in the WSJ Penn Treebank
(Marcus et al., 1993), there are 43,740 word types
and over 1M word tokens. Of the types, 88 are
tagged as prepositions. These types account for
only 0.2% of the types, but for as many as 11.9%
of the tokens. An algorithm which is accurate only
on prepositions would do much better in a token
level evaluation than in a type level one.
This phenomenon is not restricted to preposi-
tions or English. In the WSJ corpus, determiners
account for 0.05% of the types but for 9.8% of the
tokens. In the German NEGRA corpus (Brants,
1997), the article class (both definite and indefi-
nite) accounts for 0.04% of the word types and for
12.5% of the word tokens, and the coordinating
conjunctions class accounts for 0.05% of the word
types but for 3% of the tokens.
The type and token behavior differences result
from the Zipfian distribution of word tokens to
word types (Mitzenmacher, 2004). Since the word
frequency distribution is Zipfian, any clustering al-
gorithm that is accurate only on a small number of
frequent words (not necessarily members of a par-
ticular class) would perform well in a token level
evaluation but not in a type one. For example,
</bodyText>
<page confidence="0.996763">
78
</page>
<bodyText confidence="0.999461631578947">
the most frequent 100 word types (regardless of
POS class) in WSJ (NEGRA) account for 43.9%
(41.3%) of the tokens in the corpus. These words
appear in 32 out of the 34 non-punctuation POS
classes in WSJ and in 38 out of the 51 classes in
NEGRA.
Other natural language entities also demonstrate
Zipfian distribution of tokens to types. For exam-
ple, the distribution of syntactic categories in parse
tree constituents is Zipfian, as shown in (Reichart
and Rappoport, 2008) for English, German and
Chinese corpora. Thus, the distinction between to-
ken and type level evaluation is important also for
grammar induction algorithms.
It may be argued that a token level evaluation
is sufficient since it already reflects type informa-
tion. In this paper we demonstrate that this is not
the case, by showing that they correlate weakly or
even negatively in an important NLP task.
</bodyText>
<sectionHeader confidence="0.9041355" genericHeader="method">
3 Existing Clustering Evaluation
Measures
</sectionHeader>
<bodyText confidence="0.997725032786885">
Clustering evaluation is challenging. Many mea-
sures have been proposed in the past decades
(Pfitzner et al., 2008). In this section, we briefly
survey the three main types: mapping based,
counting pairs, and information theoretic mea-
sures, and motivate our decision to focus on the
first in this paper.
Mapping based measures are based on a post-
processing step in which each induced cluster is
mapped to a gold class (or vice versa). The stan-
dard mappings are greedy many-to-one (M-1) and
greedy one-to-one (1-1). Several measures which
rely on these mappings were proposed. The most
common and perhaps the simplest one is accu-
racy, which computes the fraction of items cor-
rectly clustered under the mapping. Other mea-
sures include: L (Larsen, 1999), D (Van Dongen,
2000), misclassification index (MI) (Zeng et al.,
2002), H (Meila and Heckerman, 2001), clustering
F-measure (Fung et al., 2003) and micro-averaged
precision and recall (Dhillon et al., 2003). In Sec-
tion 4 we show why existing mapping-based mea-
sures cannot be applied to the polysemous type
case and present new mapping-based measures for
this case.
Counting pairs measures are based on a com-
binatorial approach which examines the number
of data element pairs that are clustered similarly
in the reference and proposed clustering. Among
these are Rand Index (Rand, 1971), Adjusted Rand
Index (Hubert and Arabie, 1985), F statistic (Hu-
bert and Schultz, 1976), Jaccard (Milligan et al.,
1983), Fowlkes-Mallows (Fowlkes and Mallows,
1983) and Mirkin (Mirkin, 1996). Schulte im
Walde (2006) used such a measure for type level
evaluation of monosemous verb type clustering.
Meila (2007) described a few problems with
such measures. A serious one is that their values
are unbounded, making it hard to interpret their
results. This can be solved by adjusting their val-
ues to lie in [0, 1], but even adjusted measures suf-
fer from severe distributional problems, limiting
their usability in practice. We thus do not address
counting pairs measures in this paper.
Information-theoretic (IT) measures. IT
measures assume that the items in the dataset are
taken from a known distribution (usually the uni-
form distribution), and thus the gold and induced
clusters can be treated as random variables. These
measures utilize a co-occurrence matrix I between
the gold and induced clusters. We denote the in-
duced clustering by K and the gold clustering by
C. IZj contains the number of items in the in-
tersection of the i-th gold class and the j-th in-
duced cluster. When assuming the uniform dis-
tribution, the probability of an event (a gold class
c or an induced cluster k) is its relative size, so
p(c) _ E|k |1 Iiv and p(k) _ Eg|1 Iiv (N is the
total number of clustered items).
Under this assumption we define the entropies
and the conditional entropies:
</bodyText>
<equation confidence="0.992412875">
P|K |P|K|
H(C) = − P|C |k=1 Ick k=1 Ick
N log
c=1 N
H(C|K) = − P|K|P|C|Ick N log Ick
k=1 c=1 P|C|
c=1 Ick
H(K) and H(KIC) are defined similarly.
</equation>
<bodyText confidence="0.999938125">
In Section 5 we use two IT measures for token
level evaluation, V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009)
(a normalized version of VI (Meila, 2007)). The
appealing properties of these measures have been
extensively discussed in these references; see also
(Pfitzner et al., 2008). V and NVI are defined as
follows:
</bodyText>
<equation confidence="0.976443058823529">
1 (C|K) H(C) = 0
h =( H(C) =6 0
1 H(K) = 0
1 H(K) =6 0
c = 1 −
(
−
HH(
H(K|C)
H(K)
2hc
V =
h + c
79
H(CJK)}H(K1C) H C 0
NV I(C, K) = H(C) ( ) #
H(K) H(C) = 0
</equation>
<bodyText confidence="0.999594666666667">
In the monosemous case (type or token), the ap-
plication of the measures described in this section
to type level evaluation is straightforward. In the
polysemous case, however, they suffer from seri-
ous shortcomings.
Consider a case in which each item is assigned
exactly r gold clusters and each gold cluster has
the exact same number of items (i.e., each has a
size of ���
|C|, where l is the number of items). Now,
consider an induced clustering where there are |C|
induced clusters (|K |= |C|) and each item is as-
signed to all induced clusters. The co-occurrence
matrix in this case should have identical values in
all its entries. Even if we allow the weight each
item contributes to the matrix to depend on its gold
and induced entry sizes, the situation will remain
the same. This is because all items have the exact
same entry size and both gold and induced cluster-
ings have uniform cluster sizes.
In this case, the random variables defined by the
induced and gold clustering assignments are in-
dependent (this easily follows from the definition
of independent events, since the joint probability
is the multiplication of the marginals). Hence,
H(K|C) = H(K) and H(C|K) = H(C), and
both V and NVI obtain their worst possible val-
ues1. However, the score should surely depend on
r (the size of each word’s gold entry). Specifi-
cally, when r = |C |we get that the induced and
gold clusterings are identical. This case should not
get the worst score, and it should definitely score
higher than the case in which r = 1, where K is
dramatically different from C.
The problem can in theory be solved by pro-
viding the number of clusters per item as an input
to the algorithm. However, in NLP this is unre-
alistic (even if the total number of clusters can be
provided) and the number should be determined
by the algorithm. We therefore do not consider
IT-based measures in this paper, deferring them to
future work.
</bodyText>
<sectionHeader confidence="0.7470265" genericHeader="method">
4 Mapping Based Measures for
Polysemous Type Evaluation
</sectionHeader>
<bodyText confidence="0.994191">
In this section we present new type level evalu-
ation measures for the polysemous case. As we
</bodyText>
<footnote confidence="0.5049">
1V values are in [0, 1], 0 being the worst. NVI obtains its
</footnote>
<bodyText confidence="0.986372696969697">
highest and worst possible value, 1 + ����|�|)
���) .
show below, these measures do not suffer from the
problems discussed for IT measures in Section 3.
All measures are mapping-based: first, a map-
ping between the induced and gold clusters is per-
formed, and then a measure E is computed. As
is common in the clustering evaluation literature
(Section 3), we use M-1 and 1-1 greedy mappings,
defined to be those that maximize the correspond-
ing measure E.
Let C = {c1, ..., c�} be the set of gold classes
and K = {k1, ..., km} be the set of induced clus-
ters. Denote the number of words types by l. Let
Ai C C, Bi C K, i = 1...l be the set of gold
classes and set of induced clusters for each word.
The polysemous nature of task is reflected by the
fact that Ai and Bi are subsets, rather than mem-
bers, of C and K respectively.
Our measures address quality from two persec-
tives, that of the individual items clustered (Sec-
tion 4.1) and that of the clusters (Section 4.2).
Item-based measures especially suit evaluation of
clustering quality for the purpose of lexicon induc-
tion, and have no counterpart in the monosemous
case. Cluster-based measures are a direct general-
ization of existing mapping based measures to the
polysemous case.
The difficulty in designing item-based and
cluster-based measures is that the number of clus-
ters assigned to each item is determined by the
clustering algorithm. Below we show how to over-
come this.
</bodyText>
<subsectionHeader confidence="0.938702">
4.1 Item-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.998882833333333">
For a given mapping h : K —* C, denote
h(Bi) = {h(x) : x E Bi}. A fundamental quan-
tity for item-based evaluation is the number of cor-
rect clusters for each item (word type) under this
mapping, denoted by IMi (IM stands for ‘item
match’):
</bodyText>
<equation confidence="0.879291">
IMi = |Ai n h(Bi)|
</equation>
<bodyText confidence="0.9606253">
The total item match IM is defined to be:
IM = E&apos;=1 IMi = E&apos;=1 |Ai n h(Bi)|
In the monosemous case, IM is normalized by
the number of items, yielding an accuracy score.
Applying a similar definition in the polysemous
case, normalizing instead by the total number of
gold clusters assigned to the items, can be easily
manipulated. Even a clustering which has the cor-
rect number of induced clusters (equal to the num-
ber of gold classes) but which assigns each item to
</bodyText>
<page confidence="0.985509">
80
</page>
<bodyText confidence="0.999190923076923">
all induced clusters, receives a perfect score under
both greedy M-1 and 1-1 mappings. This holds for
any induced clustering for which Vi, Ai C h(Bi).
Note that using a mapping from C to K (or a
combination of both directions) would exhibit the
same problem.
To overcome the problem, we use the harmonic
average of two normalized terms (F-score). We
use two average variants, micro and macro. Macro
average computes the total number of matches
over all words and normalizes in the end. Recall
(R), Precision (P) and their harmonic average (F-
score) are accordingly defined:
</bodyText>
<equation confidence="0.999710125">
R = Im Im
�l P =
�l
i=1 |Ai |i=1 |h(Bi)|
2RP
MacroI = R + P
2IM == F(h)
Ei=1 |Ai |+ Ei=1 |h(Bi)|
</equation>
<bodyText confidence="0.979380375">
F(h) is a constant depending on h. As all items
are equally weighted, those with larger gold and
induced entries have more impact on the measure.
The micro average, aiming to give all items an
equal status, first computes an F-score for each
item and then averages over them. Hence, each
item contributes at most 1 to the measure. This
MicroI measure is given by:
</bodyText>
<equation confidence="0.9983788">
Ri = Imi Pi = I mi Fi= 2 Ri Pi = 2I mi
|Ai ||h(Bi) |Ri+Pi |Ai|+|h(Bi)|
2IMi
|Ai |+ |h(Bi)|
wi(h) · IMi
</equation>
<bodyText confidence="0.9939720625">
Where wi(h) is a weight depending on h but
also on i.
For both measures, the maximum score is 1. It
is obtained if and only if Ai = h(Bi) for every i.
In 1-1 mapping, when the number of induced
clusters is larger than the number of gold clus-
ters, some of the induced clusters are not mapped.
To preserve the nature of 1-1 mapping that pun-
ishes for excessive clusters2, we define |h(Bi) |to
be equal to |Bi |even for these unmapped clusters.
Recall that any induced clustering in which
Vi, Ai C h(Bi) gets the best score under a greedy
mapping with the accuracy measure. In MacroI
and MicroI the obtained recalls are perfect, but the
precision terms reflect deviation from the correct
solution.
</bodyText>
<footnote confidence="0.899281">
2And to allow us to compute it accurately, see below.
</footnote>
<bodyText confidence="0.97203238">
In the example in Section 3 showing an unrea-
sonable behavior of IT-based measures, the score
depends on r for both MacroI and MicroI. With
our new measures, recall is always 1, but precision
is n. This is true both for 1-1 and M-1 mappings.
Hence, the new measures show reasonable behav-
ior in this example for all r values.
MicroI was used in (Dasgupta and Ng, 2007)
with a manually compiled mapping. Their map-
ping was not based on a well-defined scheme but
on a heuristic. Moreover, providing a manual
mapping might be impractical when the number of
clusters is large, and can be inaccurate, especially
when the clustering is not of very high quality.
In the following we discuss how to compute the
1-1 and M-1 greedy mappings for each measure.
1-1 Mapping. We compute h by finding the
maximal weighted matching in a bipartite graph.
In this graph one side represents the induced clus-
ters, the other represents the gold classes and
the matchings correspond to 1-1 mappings. The
problem can be efficiently solved by the Kuhn-
Munkres algorithm (Kuhn, 1955; Munkres, 1957).
To be able to use this technique, edge weights
must not depend upon h. In 1-1 mapping,
|h(Bi) |= |Bi|, and therefore F(h) = F and
wi(h) = wi. That is, both quantities are inde-
pendent of h3. For MacroI, the weight on the edge
between the s-th gold class and the j-th induced
cluster is: W(esj) = E1=1 F · IsEAiIjEBi. For
MicroI it is: W(esj) = EZ=1 wi · IsEAiIjEBi.
IsEAi is 1 if s E Ai and 0 otherwise.
M-1 Mapping. There are two problems in ap-
plying the bipartite graph technique to finding an
M-1 mapping. First, under such mapping wi(h)
and F(h) do depend on h. The problem may
be solved by selecting some constant weighting
scheme. However, a more serious problem also
arises.
Consider a case in which an item x has a gold
entry {C11 and an induced entry {K1,K21. Say
the chosen mapping mapped both K1 and K2 to
C1. By summing over the graph’s edges selected
by the mapping, we add weight (F(h) for MacroI
and wi(h) for MicroI) both to the edge between
K1 and C1 and to the edge between K2 and C1.
However, the item’s IMi is only 1. This prohibits
3Consequently, the increase in MacroI and MicroI follow-
ing an increase of 1 in an item’s gold/induced intersection size
(IMi) is independent of h.
</bodyText>
<equation confidence="0.703831090909091">
=
��
i=1
IMi
1 �� 1 ��
MicroI = l i=1 Fi = l i=1
��
i=1
1
=
l
</equation>
<page confidence="0.973742">
81
</page>
<bodyText confidence="0.9996205">
the use of the bipartite graph method for the M-1
case.
Since we are not aware of any exact method for
solving this problem, we use a hill-climbing al-
gorithm. We start with a random mapping and a
random order on the induced clusters. Then we
iterate over the induced clusters and map each of
them to the gold class which maximizes the mea-
sure given that the rest of the mapping remains
constant. We repeat the process until no improve-
ment to the measure can be obtained by changing
the assignment of a single induced cluster. Since
the score depends on the initial random mapping
and random order, we repeat this process several
times and choose the maximum between the ob-
tained scores.
</bodyText>
<subsectionHeader confidence="0.892179">
4.2 Cluster-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.99305475">
The cluster-based evaluation measures we propose
are a direct generalization of existing monose-
mous mapping based measures to the polysemous
type case.
For a given mapping h : K —* C, we define h:
Kh —* C. Kh is defined to be a clustering which
is obtained by performing set union between every
two clusters in K that are mapped to the same gold
cluster. The resulting h is always 1-1. We denote
|Kh |= mh.
Our motivation for using h in the definition of
the measures instead of h is to stay as close as
possible to accuracy, the most common mapping-
based measure in the monosemous case. M-1
(monosemous) accuracy does not punish for split-
ing classes. For instance, in a case where there is
a gold cluster ci and two induced clusters k1 and
k2 such that ci = k1 U k2, the M-1 accuracy is the
same as in the case where there is one cluster k1
such that ci = k1. M-1 accuracy, despite its in-
difference to splitting, was shown to reflect better
than 1-1 accuracy the clustering’s applicability for
subsequent applications (at least in some contexts)
(Headden III et al., 2008).
Recall that in item-based evaluation, IMi mea-
sures the intersection between the induced and
gold entries of each item. Therefore, the set union
operation is not needed for that case, since when
an item appears in two induced clusters that are
mapped to the same gold cluster, its IMi is in-
creased only by 1.
A fundamental quantity for cluster-based eval-
uation is the intersection between each induced
cluster and the gold class to which it is mapped.
We denote this value by CMj (CM stands for
‘cluster match’):
</bodyText>
<equation confidence="0.8605844">
CMj = |kj n �h(kj)|
The total intersection (CM) is accordingly de-
fined to be:
CM = E�h
j�1 CMj = E hj�1 |kj n�h(kj)|
</equation>
<bodyText confidence="0.999884157894737">
As with the item-based evaluation (Section 4.1),
using CM or a derived accuracy as a measure is
problematic. A clustering that assigns n induced
classes to each word (n is the number of gold
classes) will get the highest possible score under
every greedy mapping (1-1 or M-1), as will any
clustering in which Vi, Ai C h(Bi).
As in the item-based evaluation, a possible so-
lution is based on defining recall, precision and F-
score measures, computed either in the micro or in
the macro level. The macro cluster-based measure
turns out to be identical to the macro item-based
measure MacroI4.
The following derivation shows the equivalence
for the 1-1 case. The M-1 case is similar. We note
that h = h in the 1-1 case and we therefore ex-
change them in the definition of CM. It is enough
to show that CM = IM, since the denominator is
the same in both cases:
</bodyText>
<equation confidence="0.9854252">
CM = Pmj=1 |kj ∩ h(kj) |=
Pl= = Pm Pl i=1 IiEkjIiEh(kj) =
j=1
Pm j=1 IiEkjIiEh(kj) =
i=1
= Pli=1 |Ai ∩ h(Bi) |= IM
The micro cluster-based measures are defined:
Rj = CMj
|�h(kj) |Pj = CMj
|kj |Fj = 2RjPj Rj+Pj
</equation>
<bodyText confidence="0.988982">
The micro cluster measure MicroC is obtained
by taking a weighted average over the Fj’s:
</bodyText>
<equation confidence="0.9957795">
MicroC = EkEKh |k|
�� Fk
</equation>
<bodyText confidence="0.99963575">
Where N* = &amp;;EKh |z |is the number of clus-
tered items after performing the set union and
including repetitions. If, in the 1-1 case where
m &gt; n, an induced cluster is not mapped, we de-
fine Fk = 0. A definition of the measure using
a reverse mapping (i.e., from C to K) would have
used a weighted average with weights proportional
to the gold classes’ sizes.
</bodyText>
<footnote confidence="0.518771">
4Hence, we have six type level measures: MacroI (which
is equal to MacroC), MicroI, and MicroC, each of which in
two versions, M-1 and 1-1.
</footnote>
<page confidence="0.998199">
82
</page>
<bodyText confidence="0.999949090909091">
The definition of h causes a similar computa-
tional difficulty as in the M-1 item-based mea-
sures. Consequently, we apply a hill climbing
algorithm similar to the one described in Sec-
tion 4.1.
The 1-1 mapping is computed using the same
bipartite graph method described in Section 4.1.
The graph’s vertices correspond to gold and in-
duced clusters and an edge’s weight is the F-score
between the class and cluster corresponding to its
vertices times the cluster’s weight (IkI/N*).
</bodyText>
<sectionHeader confidence="0.894282" genericHeader="evaluation">
5 Evaluation of POS Induction Models
</sectionHeader>
<bodyText confidence="0.999905">
As a detailed case study for the ideas presented
in this paper, we apply the various measures for
the POS induction task, using seven leading POS
induction algorithms.
</bodyText>
<subsectionHeader confidence="0.92221">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999847858974359">
POS Induction Algorithms. We experimented
with the following models: ARR10 (Abend et al.,
2010), Clark03 (Clark, 2003), GG07 (Goldwa-
ter and Griffiths, 2007), GJ08 (Gao and Johnson,
2008), and GVG09 (Van Gael et al., 2009) (three
models). Additional recent good results for vari-
ous variants of the POS induction problem are de-
scribed in e.g., (Smith and Eisner, 2004; Grac¸a et
al., 2009).
Clark03 and ARR10 are monosemous algo-
rithms, allowing a single cluster for each word
type. The other algorithms are polysemous. They
perform sequence labeling where each token is
tagged in its context, and different tokens (in-
stances) of the same type (word form) may receive
different tags.
Data Set. All models were tested on sections
2-21 of the PTB-WSJ, which consists of 39832
sentences, 950028 tokens and 39546 unique types.
Of the tokens, 832629 (87.6%) are not punctuation
marks.
Evaluation Measures. Type level evaluation
used the measures MacroI (which is equal to
MacroC), MicroI and MicroC both with greedy
1-1 and M-1 mappings as described in Section 4.
The type level gold (induced) entry is defined to
be the set of all gold (induced) clusters with which
it appears.
For the token level evaluation, six measures are
used (see Section 3): accuracy with M-1 and 1-1
mappings, NVI, V, H(CIK) and H(KIC), using e
as the logarithm’s base. We use the full WSJ POS
tags set excluding punctuation5.
Punctuation. Punctuation marks occupy a
large volume of the corpus tokens (12.4% in our
experimental corpus), and are easy to cluster.
Clustering punctuation marks thus greatly inflates
token level results. To study the relationship be-
tween type and token level evaluations in a fo-
cused manner, we excluded punctuation from the
evaluation (they are still used during training, so
algorithms that rely on them are not harmed).
Number of Induced Clusters. The number
of gold POS tags in WSJ is 45, of which 11 are
punctuation marks. Therefore, for the ARR10 and
Clark03 models, 34 clusters were induced. For
GJ08 we received the output with 45 clusters. The
iHMM models of GVG09 determine the number
of clusters automatically (resulting in 47, 91 and
192 clusters, see below). For GG07, our com-
puting resources did not enable us to induce 45
clusters and we therefore used 176. Our focus in
this paper is to study the type vs. token distinction
rather than to provide a full scope comparison be-
tween algorithms, for which more clustering sizes
would need to be examined.
Configurations. We ran the ARR10 tagger
with the configuration detailed in (Abend et al.,
2010). For Clark03, we ran his neyessenmorph
model7 10 times (using an unknown words thresh-
old of 5) and report the average score for each
measure. The models of GVG09 were run in the
three configurations reported in their paper: one
with a Dirichlet process prior and fixed parame-
ters, another with a Pittman-Yore prior with fixed
parameters, and a third with a Dirichlet process
prior with parameters learnt from the data. All five
models were run in an optimal configuration.
We obtained the code of Goldwater and Grif-
fiths’ BHMM model and ran it for 10K iterations
with an annealing technique for parameter estima-
tion. That was the best parameter estimation tech-
nique available to us. This is the first time that this
model is evaluated on such a large experimental
corpus, and it performed well under these condi-
tions.
The output of the model of GJ08 was sent to
us by the authors. The model was run on sec-
</bodyText>
<footnote confidence="0.999453666666667">
5We use all WSJ tokens in the training stage, but omit
punctuation marks during evaluation.
6The 17 most frequent tags cover 94% of the word in-
stances and more than 99% of the word types in the WSJ
gold standard tagging.
7www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
</footnote>
<page confidence="0.999163">
83
</page>
<bodyText confidence="0.99991325">
tions 2-21 of the WSJ-PTB using significantly
inferior computing resources compared to those
used for producing the results reported in their
paper. While this model cannot be compared to
the aforementioned six models due to the subopti-
mal configuration, we evaluate its output using our
measures to get a broader variety of experimental
results8.
</bodyText>
<subsectionHeader confidence="0.9817">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999939717948718">
Table 1 presents the scores of the compared mod-
els under all evaluation measures (six token level,
six type level). What is important here to note
are the differences between type and token level
evaluations for the algorithms. We are mainly
interested in two things: (1) seeing how relative
rankings change in the two evaluation types, thus
showing that the two types are not highly corre-
lated and are both useful; and (2) insights gained
by using a type level evaluation in addition to the
usual token level one.
Note that the table should not be used to deduce
which algorithm is the ‘best’ for the task, even ac-
cording to a single evaluation type. This is be-
cause, as explained above, the algorithms do not
induce the same number of clusters and this affects
their results.
Results indicate that type level evaluation re-
veals aspects of the clustering quality that are not
expressed in the token level. For the Clark03
model the disparity is most apparent. While in
the token level it performs very well (better than
the polysemous algorithms for the 1-1, V and NVI
token level measures), in the type level it is the
second worst in the item-based 1-1 scores and the
worst in the M-1 scores.
Here we have a clear demonstration of the value
of type level evaluation. The Clark03 algorithm
is assessed as excellent using token level evalua-
tion (second only to ARR10 in M-1, 1-1, V and
NVI), and only a type level one shows its rela-
tively poor type performance. Although readers
may think that this is natural due to the algorithm’s
monosemous nature, this is not the case, since the
monosemous ARR10 generally ranked first in the
type level measures (more on this below).
The disparity is also observed for polysemous
algorithms. The GG07 model’s token level scores
are mediocre, while in the type level MicroC 1-1
</bodyText>
<footnote confidence="0.5854615">
8We would like to thank all authors for sending us the
data.
</footnote>
<bodyText confidence="0.999739680851064">
measure this model is the best and in the type level
MicroI and MacroI 1-1 measures it is the second
best.
Monosemous vs. polysemous algorithms. The
table shows that the ARR10 model achieves the
best results in most type and token level evalua-
tion measures. The fact that this monosemous al-
gorithm outperforms the polysemous ones, even
in a type level evaluation, may seem strange at
first sight but can be explained as follows. Pol-
ysemous tokens account for almost 60% of the
corpus (565K out of 950K), so we could expect
that a monosemous algorithm should do badly in
a token-level evaluation. However, for most of the
polysemous tokens the polysemy is only weakly
present in the corpus9, so it is hard to detect even
for polysemous algorithms. Regarding types, pol-
ysemous types constitute only 16.6% of the cor-
pus types, so a monosemous algorithm which is
quite good in assigning types to clusters has a good
chance of beating polysemous algorithms in a type
level evaluation.
Hence, monosemous POS induction algorithms
are not at such a great disadvantage relative to pol-
ysemous ones. This observation, which was fully
motivated by our type level case study, might be
used to guide future work on POS induction, and
it thus serves as another demonstration for the util-
ity of type level evaluation.
Hill climbing algorithm. For the type level
measures with greedy M-1 mapping, we used the
hill-climbing algorithm described in Section 4.
Recall that the mapping to which our algorithm
converges depends on its random initialization.
We therefore ran the algorithm with 10 differ-
ent random initializations and report the obtained
maximum for MacroI, MicroI and MicroC in Ta-
ble 1. The different initializations caused very lit-
tle fluctuation: not more than 1% in the 9 (7) best
runs for the item-based (MicroC) measures. We
take this result as an indication that the obtained
maximum is a good approximation of the global
maximum.
We tried to improve the algorithm by selecting
an intelligent initialization heuristic. We used the
M-1 mapping obtained by mapping each induced
cluster to the gold class with which it has the high-
</bodyText>
<footnote confidence="0.96290425">
9Only about 27% of the tokens are instances of words that
are polysemous but not weakly polysemous (we call a word
weakly polysemous if more than 95% of its instances (tokens)
are tagged by the same tag).
</footnote>
<page confidence="0.995513">
84
</page>
<table confidence="0.9997091">
Token Level Evaluation Type Level Evaluation
MacroI MicroI MicroC
M-1 1-1 NVI V H(CIK) H(KjC) M-1 1-1 M-1 1-1 M-1 1-1
ARR10 0.675 0.588 0.809 0.608 1.041 1.22 0.579 0.444 0.596 0.455 0.624 0.403
Clark03 0.65 0.484 0.887 0.586 1.04 1.441 0.396 0.301 0.384 0.288 0.463 0.347
GG07 0.5 0.415 0.989 0.479 1.523 1.241 0.497 0.405 0.461 0.398 0.563 0.445
GVG09(1) 0.51 0.444 1.033 0.477 1.471 1.409 0.513 0.354 0.436 0.352 0.486 0.33
GVG09(2) 0.591 0.484 0.998 0.529 1.221 1.564 0.637 0.369 0.52 0.373 0.548 0.32
GVG09(3) 0.668 0.368 1.132 0.534 0.978 2.18 0.736 0.280 0.558 0.276 0.565 0.199
GJ08* 0.605 0.383 1.09 0.506 1.231 1.818 0.467 0.298 0.446 0.311 0.561 0.291
</table>
<tableCaption confidence="0.872324">
Table 1: Token level (left columns) and type level (right columns) results for seven POS induction
algorithms (rows) (see text for details). Token and type level performance are weakly correlated and
</tableCaption>
<bodyText confidence="0.994330767857143">
complement each other as evaluation measures. ARR10, a monosemous algorithm, yields the best results
in most measures. (GJ08* results are different from those reported in the original paper because it was
run with weaker computing resources than those used there.)
est weight edge in the bipartite graph. Recall from
Section 4.1 that this is a reasonable approximation
of the greedy M-1 mapping. Again, we ran it for
the three type level measures for 10 times with a
random update order on the induced clusters. This
had only a minor effect on the final scores.
Number of clusters. Previous work (Reichart
and Rappoport, 2009) demonstrated that in data
sets where a relatively small fraction of the gold
classes covers most of the items, it is reasonable
to choose this number to be the number of induced
clusters. In our experimental data set, this number
(the ‘prominent cluster number’) is around 17 (see
Section 5.1). Up to this number, increasing the
number of clusters is likely to have a positive ef-
fect on token level M-1, 1-1, H(C|K), and H(K|C)
scores. Inducing a larger number of clusters, how-
ever, is likely to positively affect M-1 and H(C|K)
but to have a negative effect on 1-1 and H(K|C).
This tendency is reflected in Table 1. For the
GG07 model the number of induced clusters, 17,
approximates the number of prominent clusters
and is lower than the number of induced clus-
ters of the other models. This is reflected by
its low token level M-1 and H(C|K) performance
and its high quality H(K|C) and NVI token level
scores. The GVG (1)-(3) models induced 47, 91
and 192 clusters respectively. This might explain
the high token level M-1 and H(C|K) performance
of GVG(3), as well as its high M-1 type level
performance, compared to its mediocre scores in
other measures.
The item based measures. The table indicates
that there is no substantial difference between the
two item based type level scores with 1-1 map-
ping. The definitions of MacroI and MicroI imply
that if |Ai|+|h(Bi) |(which equals |Ai|+|Bi |un-
der a 1-1 mapping) is constant for all word types,
then a clustering will score equally on both 1-1
type measures. Indeed, in our experimental cor-
pus 83.4% of the word types have one POS tag,
12.5% have 2, 3.1% have 3 and only 1% of the
words have more. Therefore, |Ai |is roughly con-
stant. The ARR10 and Clark03 models assign a
word type to a single cluster. For the other models,
the number of clusters per word type is generally
similar to that of the gold standard. Consequently,
|Bi |is roughly constant as well, which explains
the similar behavior of the two measures.
Note that for other clustering tasks |Ai |may not
necessarily be constant, so the MacroI and MicroI
scores are not likely to be as similar under the 1-1
mapping.
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.9999425">
We discussed type level evaluation for polysemous
clustering, presented new mapping-based evalu-
ation measures, and applied them to the evalua-
tion of POS induction algorithms, demonstrating
that type level measures provide value beyond the
common token level ones.
We hope that type level evaluation in general
and the proposed measures in particular will be
used in the future for evaluating clustering perfor-
mance in NLP tasks.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9893076">
Omri Abend, Roi Reichart and Ari Rappoport, 2010.
Improved Unsupervised POS Induction through Pro-
totype Discovery. ACL ’10.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
</reference>
<page confidence="0.99644">
85
</page>
<reference confidence="0.97328349">
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ’03.
Maurice Kandall and Jean Dickinson, 1990. Rank
Correlation Methods. Oxford University Press, New
York.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ’07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ’06.
Dmitry Davidov, Ari Rappoport. 2008. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ’08
I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. In-
formation Theoretic Co-clustering. KDD ’03
Micha Elsner, Eugene Charniak, and Mark Johnson,
2009. Structured Generative Models for Unsuper-
vised Named-Entity Clustering. NAACL ’09.
Stella Frank, Sharon Goldwater, and Frank Keller,
2009. Evaluating Models of Syntactic Category
Acquisition without Using a Gold Standard. Proc.
31st Annual Conf. of the Cognitive Science Society,
2576–2581.
E.B Fowlkes and C.L. Mallows, 1983. A Method for
Comparing Two Hierarchical Clusterings. Journal
ofAmerican statistical Association,78:553-569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester,
2003. Hierarchical Document Clustering using Fre-
quent Itemsets. SIAM International Conference on
Data Mining ’03.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ’08.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ’07.
Jo˜ao Grac¸a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ’09.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. COLING
’08.
L. Hubert and J. Schultz, 1976. Quadratic Assignment
as a General Data Analysis Strategy. British Journal
ofMathematical and Statistical Psychology, 29:190-
241.
L. Hubert and P. Arabie, 1985. Comparing Partitions.
Journal of Classification, 2:193-218.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ’00.
Harold W. Kuhn, 1955. The Hungarian Method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Bjornar Larsen and Chinatsu Aone, 1999. Fast and ef-
fective text mining using linear-time document clus-
tering. KDD ’99.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313-330.
Marina Meila and David Heckerman, 2001. An Ex-
perimental Comparison of Model-based Clustering
Methods. Machine Learning, 42(1/2):9-29.
Marina Meila, 2007. Comparing Clustering – an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
C.W Milligan, S.C Soon and L.M Sokol, 1983. The
Effect of Cluster Size, Dimensionality and the Num-
ber of Clusters on Recovery of True Cluster Struc-
ture. IEEE transactions on Pattern Analysis and
Machine Intelligence, 5:40-47.
Boris G. Mirkin, 1996. Mathematical Classification
and Clustering. Kluwer Academic Press.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions. Internet Mathematics, 1(2):226-251.
Soto Montalvo, Raquel Martnez, Arantza Casillas, and
Vctor Fresno, 2006. Multilingual Document Clus-
tering: an Heuristic Approach Based on Cognate
Named Entities. ACL ’06.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Cristina Nicolae and Gabriel Nicolae, 2006. BEST-
CUT: A Graph Algorithm for Coreference Resolu-
tion. EMNLP ’06.
Darius M. Pfitzner, Richard E. Leibbrandt and David
M.W Powers, 2008. Characterization and Evalua-
tion of Similarity Measures for Pairs of Clusterings.
Knowledge and Information Systems: An Interna-
tional Journal, DOI 10.1007/s10115-008-0150-6.
William Rand, 1971. Objective Criteria for the Evalu-
ation of Clustering Methods. Journal of the Ameri-
can Statstical Association, 66(336):846-850.
</reference>
<page confidence="0.969316">
86
</page>
<reference confidence="0.999578458333333">
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ’08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ’09.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-based External
Cluster Evaluation Measure. EMNLP ’07.
Sabine Schulte im Walde, 2006. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Computational Linguistics, 32(2):159-194.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ’04.
Stijn Van Dongen, 2000. Performance Criteria for
Graph Clustering and Markov Cluster Experiments.
Technical report CWI, Amsterdam
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ’09.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and
Guang R. Gao, 2002. An Adaptive Meta-clustering
Approach: Combining the Information from Differ-
ent Clustering Results. CSB 00:276
</reference>
<page confidence="0.99947">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927365">
<title confidence="0.981373">Type Level Clustering Evaluation: New and a POS Induction Case Study</title>
<author confidence="0.973359">Omri</author>
<affiliation confidence="0.9947035">of Computer Hebrew University of</affiliation>
<abstract confidence="0.999454730769231">Clustering is a central technique in NLP. Consequently, clustering evaluation is of great importance. Many clustering algorithms are evaluated by their success in tagging corpus tokens. In this paper we level which reflects class membership only and is independent of the token statistics of a particular reference corpus. Type level evaluation casts light on the merits of algorithms, and for some applications is a more natural measure of the algorithm’s quality. We propose new type level evaluation measures that, contrary to existing measures, are applicable when items are polysemous, the common case in NLP. We demonstrate the benefits of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<date>2010</date>
<booktitle>Improved Unsupervised POS Induction through Prototype Discovery. ACL ’10.</booktitle>
<contexts>
<context position="26511" citStr="Abend et al., 2010" startWordPosition="4622" endWordPosition="4625">tion 4.1. The 1-1 mapping is computed using the same bipartite graph method described in Section 4.1. The graph’s vertices correspond to gold and induced clusters and an edge’s weight is the F-score between the class and cluster corresponding to its vertices times the cluster’s weight (IkI/N*). 5 Evaluation of POS Induction Models As a detailed case study for the ideas presented in this paper, we apply the various measures for the POS induction task, using seven leading POS induction algorithms. 5.1 Experimental Setup POS Induction Algorithms. We experimented with the following models: ARR10 (Abend et al., 2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al., 2009) (three models). Additional recent good results for various variants of the POS induction problem are described in e.g., (Smith and Eisner, 2004; Grac¸a et al., 2009). Clark03 and ARR10 are monosemous algorithms, allowing a single cluster for each word type. The other algorithms are polysemous. They perform sequence labeling where each token is tagged in its context, and different tokens (instances) of the same type (word form) may receive different tags. Data Set. All </context>
<context position="28975" citStr="Abend et al., 2010" startWordPosition="5034" endWordPosition="5037"> the ARR10 and Clark03 models, 34 clusters were induced. For GJ08 we received the output with 45 clusters. The iHMM models of GVG09 determine the number of clusters automatically (resulting in 47, 91 and 192 clusters, see below). For GG07, our computing resources did not enable us to induce 45 clusters and we therefore used 176. Our focus in this paper is to study the type vs. token distinction rather than to provide a full scope comparison between algorithms, for which more clustering sizes would need to be examined. Configurations. We ran the ARR10 tagger with the configuration detailed in (Abend et al., 2010). For Clark03, we ran his neyessenmorph model7 10 times (using an unknown words threshold of 5) and report the average score for each measure. The models of GVG09 were run in the three configurations reported in their paper: one with a Dirichlet process prior and fixed parameters, another with a Pittman-Yore prior with fixed parameters, and a third with a Dirichlet process prior with parameters learnt from the data. All five models were run in an optimal configuration. We obtained the code of Goldwater and Griffiths’ BHMM model and ran it for 10K iterations with an annealing technique for para</context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2010</marker>
<rawString>Omri Abend, Roi Reichart and Ari Rappoport, 2010. Improved Unsupervised POS Induction through Prototype Discovery. ACL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>The NEGRA Export Format.</title>
<date>1997</date>
<tech>CLAUS Report,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="8103" citStr="Brants, 1997" startWordPosition="1319" endWordPosition="1320">ccounts for a large portion of corpus tokens. For example, in the WSJ Penn Treebank (Marcus et al., 1993), there are 43,740 word types and over 1M word tokens. Of the types, 88 are tagged as prepositions. These types account for only 0.2% of the types, but for as many as 11.9% of the tokens. An algorithm which is accurate only on prepositions would do much better in a token level evaluation than in a type level one. This phenomenon is not restricted to prepositions or English. In the WSJ corpus, determiners account for 0.05% of the types but for 9.8% of the tokens. In the German NEGRA corpus (Brants, 1997), the article class (both definite and indefinite) accounts for 0.04% of the word types and for 12.5% of the word tokens, and the coordinating conjunctions class accounts for 0.05% of the word types but for 3% of the tokens. The type and token behavior differences result from the Zipfian distribution of word tokens to word types (Mitzenmacher, 2004). Since the word frequency distribution is Zipfian, any clustering algorithm that is accurate only on a small number of frequent words (not necessarily members of a particular class) would perform well in a token level evaluation but not in a type o</context>
</contexts>
<marker>Brants, 1997</marker>
<rawString>Thorsten Brants, 1997. The NEGRA Export Format. CLAUS Report, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining Distributional and Morphological Information for Part of Speech Induction.</title>
<date>2003</date>
<journal>EACL</journal>
<volume>03</volume>
<contexts>
<context position="1406" citStr="Clark, 2003" startWordPosition="213" endWordPosition="214">ing measures, are applicable when items are polysemous, the common case in NLP. We demonstrate the benefits of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on</context>
<context position="26534" citStr="Clark, 2003" startWordPosition="4627" endWordPosition="4628">omputed using the same bipartite graph method described in Section 4.1. The graph’s vertices correspond to gold and induced clusters and an edge’s weight is the F-score between the class and cluster corresponding to its vertices times the cluster’s weight (IkI/N*). 5 Evaluation of POS Induction Models As a detailed case study for the ideas presented in this paper, we apply the various measures for the POS induction task, using seven leading POS induction algorithms. 5.1 Experimental Setup POS Induction Algorithms. We experimented with the following models: ARR10 (Abend et al., 2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al., 2009) (three models). Additional recent good results for various variants of the POS induction problem are described in e.g., (Smith and Eisner, 2004; Grac¸a et al., 2009). Clark03 and ARR10 are monosemous algorithms, allowing a single cluster for each word type. The other algorithms are polysemous. They perform sequence labeling where each token is tagged in its context, and different tokens (instances) of the same type (word form) may receive different tags. Data Set. All models were tested on s</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark, 2003. Combining Distributional and Morphological Information for Part of Speech Induction. EACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kandall</author>
<author>Jean Dickinson</author>
</authors>
<title>Rank Correlation Methods.</title>
<date>1990</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<marker>Kandall, Dickinson, 1990</marker>
<rawString>Maurice Kandall and Jean Dickinson, 1990. Rank Correlation Methods. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised Part-of-Speech Acquisition for ResourceScarce Languages. EMNLP-CoNLL ’07.</title>
<date>2007</date>
<contexts>
<context position="19644" citStr="Dasgupta and Ng, 2007" startWordPosition="3369" endWordPosition="3372">C h(Bi) gets the best score under a greedy mapping with the accuracy measure. In MacroI and MicroI the obtained recalls are perfect, but the precision terms reflect deviation from the correct solution. 2And to allow us to compute it accurately, see below. In the example in Section 3 showing an unreasonable behavior of IT-based measures, the score depends on r for both MacroI and MicroI. With our new measures, recall is always 1, but precision is n. This is true both for 1-1 and M-1 mappings. Hence, the new measures show reasonable behavior in this example for all r values. MicroI was used in (Dasgupta and Ng, 2007) with a manually compiled mapping. Their mapping was not based on a well-defined scheme but on a heuristic. Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality. In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure. 1-1 Mapping. We compute h by finding the maximal weighted matching in a bipartite graph. In this graph one side represents the induced clusters, the other represents the gold classes and the matchings correspond to 1-1 m</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng, 2007. Unsupervised Part-of-Speech Acquisition for ResourceScarce Languages. EMNLP-CoNLL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient Unsupervised Discovery of Word Categories using Symmetric Patterns and High Frequency Words.</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<contexts>
<context position="1568" citStr="Davidov and Rappoport, 2006" startWordPosition="233" endWordPosition="236">tudy, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov, Ari Rappoport, 2006. Efficient Unsupervised Discovery of Word Categories using Symmetric Patterns and High Frequency Words. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions.</title>
<date>2008</date>
<journal>ACL</journal>
<volume>08</volume>
<contexts>
<context position="1598" citStr="Davidov and Rappoport, 2008" startWordPosition="237" endWordPosition="241">iment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering e</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov, Ari Rappoport. 2008. Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions. ACL ’08</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>S Mallela</author>
<author>D S Modha</author>
</authors>
<title>Information Theoretic Co-clustering.</title>
<date>2003</date>
<journal>KDD</journal>
<volume>03</volume>
<contexts>
<context position="10592" citStr="Dhillon et al., 2003" startWordPosition="1729" endWordPosition="1732">on a postprocessing step in which each induced cluster is mapped to a gold class (or vice versa). The standard mappings are greedy many-to-one (M-1) and greedy one-to-one (1-1). Several measures which rely on these mappings were proposed. The most common and perhaps the simplest one is accuracy, which computes the fraction of items correctly clustered under the mapping. Other measures include: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used</context>
</contexts>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. Information Theoretic Co-clustering. KDD ’03</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Structured Generative Models for Unsupervised Named-Entity Clustering.</title>
<date>2009</date>
<journal>NAACL</journal>
<volume>09</volume>
<contexts>
<context position="1900" citStr="Elsner et al., 2009" startWordPosition="284" endWordPosition="287">chnique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algori</context>
</contexts>
<marker>Elsner, Charniak, Johnson, 2009</marker>
<rawString>Micha Elsner, Eugene Charniak, and Mark Johnson, 2009. Structured Generative Models for Unsupervised Named-Entity Clustering. NAACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stella Frank</author>
<author>Sharon Goldwater</author>
<author>Frank Keller</author>
</authors>
<title>Evaluating Models of Syntactic Category Acquisition without Using a Gold Standard.</title>
<date>2009</date>
<booktitle>Proc. 31st Annual Conf. of the Cognitive Science Society,</booktitle>
<pages>2576--2581</pages>
<contexts>
<context position="2288" citStr="Frank et al., 2009" startWordPosition="344" endWordPosition="347">r. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or</context>
</contexts>
<marker>Frank, Goldwater, Keller, 2009</marker>
<rawString>Stella Frank, Sharon Goldwater, and Frank Keller, 2009. Evaluating Models of Syntactic Category Acquisition without Using a Gold Standard. Proc. 31st Annual Conf. of the Cognitive Science Society, 2576–2581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E B Fowlkes</author>
<author>C L Mallows</author>
</authors>
<title>A Method for Comparing Two Hierarchical Clusterings. Journal ofAmerican statistical Association,78:553-569.</title>
<date>1983</date>
<contexts>
<context position="11136" citStr="Fowlkes and Mallows, 1983" startWordPosition="1815" endWordPosition="1818">(Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address counting pairs measures in this paper. Information-theoretic (IT) measures. IT measures assume that the items in the dat</context>
</contexts>
<marker>Fowlkes, Mallows, 1983</marker>
<rawString>E.B Fowlkes and C.L. Mallows, 1983. A Method for Comparing Two Hierarchical Clusterings. Journal ofAmerican statistical Association,78:553-569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin C M Fung</author>
<author>Ke Wang</author>
<author>Martin Ester</author>
</authors>
<title>Hierarchical Document Clustering using Frequent Itemsets.</title>
<date>2003</date>
<booktitle>SIAM International Conference on Data Mining ’03.</booktitle>
<contexts>
<context position="10529" citStr="Fung et al., 2003" startWordPosition="1720" endWordPosition="1723">n the first in this paper. Mapping based measures are based on a postprocessing step in which each induced cluster is mapped to a gold class (or vice versa). The standard mappings are greedy many-to-one (M-1) and greedy one-to-one (1-1). Several measures which rely on these mappings were proposed. The most common and perhaps the simplest one is accuracy, which computes the fraction of items correctly clustered under the mapping. Other measures include: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows</context>
</contexts>
<marker>Fung, Wang, Ester, 2003</marker>
<rawString>Benjamin C. M. Fung, Ke Wang, and Martin Ester, 2003. Hierarchical Document Clustering using Frequent Itemsets. SIAM International Conference on Data Mining ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A Comparison of Bayesian Estimators for Unsupervised Hidden Markov Model POS Taggers.</title>
<date>2008</date>
<journal>EMNLP</journal>
<volume>08</volume>
<contexts>
<context position="2654" citStr="Gao and Johnson, 2008" startWordPosition="400" endWordPosition="403">tion, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute</context>
<context position="26602" citStr="Gao and Johnson, 2008" startWordPosition="4636" endWordPosition="4639"> Section 4.1. The graph’s vertices correspond to gold and induced clusters and an edge’s weight is the F-score between the class and cluster corresponding to its vertices times the cluster’s weight (IkI/N*). 5 Evaluation of POS Induction Models As a detailed case study for the ideas presented in this paper, we apply the various measures for the POS induction task, using seven leading POS induction algorithms. 5.1 Experimental Setup POS Induction Algorithms. We experimented with the following models: ARR10 (Abend et al., 2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al., 2009) (three models). Additional recent good results for various variants of the POS induction problem are described in e.g., (Smith and Eisner, 2004; Grac¸a et al., 2009). Clark03 and ARR10 are monosemous algorithms, allowing a single cluster for each word type. The other algorithms are polysemous. They perform sequence labeling where each token is tagged in its context, and different tokens (instances) of the same type (word form) may receive different tags. Data Set. All models were tested on sections 2-21 of the PTB-WSJ, which consists of 39832 sentences, 9500</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson, 2008. A Comparison of Bayesian Estimators for Unsupervised Hidden Markov Model POS Taggers. EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="2631" citStr="Goldwater and Griffiths, 2007" startWordPosition="396" endWordPosition="399">s on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on t</context>
<context position="26572" citStr="Goldwater and Griffiths, 2007" startWordPosition="4630" endWordPosition="4634">me bipartite graph method described in Section 4.1. The graph’s vertices correspond to gold and induced clusters and an edge’s weight is the F-score between the class and cluster corresponding to its vertices times the cluster’s weight (IkI/N*). 5 Evaluation of POS Induction Models As a detailed case study for the ideas presented in this paper, we apply the various measures for the POS induction task, using seven leading POS induction algorithms. 5.1 Experimental Setup POS Induction Algorithms. We experimented with the following models: ARR10 (Abend et al., 2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al., 2009) (three models). Additional recent good results for various variants of the POS induction problem are described in e.g., (Smith and Eisner, 2004; Grac¸a et al., 2009). Clark03 and ARR10 are monosemous algorithms, allowing a single cluster for each word type. The other algorithms are polysemous. They perform sequence labeling where each token is tagged in its context, and different tokens (instances) of the same type (word form) may receive different tags. Data Set. All models were tested on sections 2-21 of the PTB-WSJ, which con</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths, 2007. Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao Grac¸a</author>
</authors>
<title>Kuzman Ganchev, Ben Taskar and Frenando Pereira,</title>
<date>2009</date>
<journal>NIPS</journal>
<volume>09</volume>
<marker>Grac¸a, 2009</marker>
<rawString>Jo˜ao Grac¸a, Kuzman Ganchev, Ben Taskar and Frenando Pereira, 2009. Posterior vs. Parameter Sparsity in Latent Variable Models. NIPS ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Evaluating Unsupervised Part-ofSpeech Tagging for Grammar Induction.</title>
<date>2008</date>
<journal>COLING</journal>
<volume>08</volume>
<marker>McClosky, Charniak, 2008</marker>
<rawString>William P. Headden III, David McClosky and Eugene Charniak, 2008. Evaluating Unsupervised Part-ofSpeech Tagging for Grammar Induction. COLING ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hubert</author>
<author>J Schultz</author>
</authors>
<title>Quadratic Assignment as a General Data Analysis Strategy.</title>
<date>1976</date>
<journal>British Journal ofMathematical and Statistical Psychology,</journal>
<pages>29--190</pages>
<contexts>
<context position="11058" citStr="Hubert and Schultz, 1976" startWordPosition="1804" endWordPosition="1808">MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address counting pairs measures in this paper. Inf</context>
</contexts>
<marker>Hubert, Schultz, 1976</marker>
<rawString>L. Hubert and J. Schultz, 1976. Quadratic Assignment as a General Data Analysis Strategy. British Journal ofMathematical and Statistical Psychology, 29:190-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hubert</author>
<author>P Arabie</author>
</authors>
<title>Comparing Partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<pages>2--193</pages>
<contexts>
<context position="11018" citStr="Hubert and Arabie, 1985" startWordPosition="1798" endWordPosition="1801">ongen, 2000), misclassification index (MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address co</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>L. Hubert and P. Arabie, 1985. Comparing Partitions. Journal of Classification, 2:193-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
</authors>
<title>Hoa Trang Dang and Martha Palmer,</title>
<date>2000</date>
<journal>AAAI</journal>
<volume>00</volume>
<marker>Kipper, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang and Martha Palmer, 2000. Class-Based Construction of a Verb Lexicon. AAAI ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian Method for the Assignment Problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="20331" citStr="Kuhn, 1955" startWordPosition="3487" endWordPosition="3488">ned scheme but on a heuristic. Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality. In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure. 1-1 Mapping. We compute h by finding the maximal weighted matching in a bipartite graph. In this graph one side represents the induced clusters, the other represents the gold classes and the matchings correspond to 1-1 mappings. The problem can be efficiently solved by the KuhnMunkres algorithm (Kuhn, 1955; Munkres, 1957). To be able to use this technique, edge weights must not depend upon h. In 1-1 mapping, |h(Bi) |= |Bi|, and therefore F(h) = F and wi(h) = wi. That is, both quantities are independent of h3. For MacroI, the weight on the edge between the s-th gold class and the j-th induced cluster is: W(esj) = E1=1 F · IsEAiIjEBi. For MicroI it is: W(esj) = EZ=1 wi · IsEAiIjEBi. IsEAi is 1 if s E Ai and 0 otherwise. M-1 Mapping. There are two problems in applying the bipartite graph technique to finding an M-1 mapping. First, under such mapping wi(h) and F(h) do depend on h. The problem may b</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn, 1955. The Hungarian Method for the Assignment Problem. Naval Research Logistics Quarterly, 2:83-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bjornar Larsen</author>
<author>Chinatsu Aone</author>
</authors>
<title>Fast and effective text mining using linear-time document clustering.</title>
<date>1999</date>
<journal>KDD</journal>
<volume>99</volume>
<marker>Larsen, Aone, 1999</marker>
<rawString>Bjornar Larsen and Chinatsu Aone, 1999. Fast and effective text mining using linear-time document clustering. KDD ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="7595" citStr="Marcus et al., 1993" startWordPosition="1224" endWordPosition="1227">tc.). As an evaluation of a POS induction algorithm, it is natural to evaluate the lexicon it generates, even if the main goal is to annotate a corpus. The lexicon lists the possible POS tags for each word, and thus its evaluation is a polysemous type level one. Even if we ignore polysemy, type level evaluation is useful for a POS induction algorithm used to tag a corpus. There are POS classes whose members are very frequent, e.g., determiners and prepositions. Here, a very small number of word types usually accounts for a large portion of corpus tokens. For example, in the WSJ Penn Treebank (Marcus et al., 1993), there are 43,740 word types and over 1M word tokens. Of the types, 88 are tagged as prepositions. These types account for only 0.2% of the types, but for as many as 11.9% of the tokens. An algorithm which is accurate only on prepositions would do much better in a token level evaluation than in a type level one. This phenomenon is not restricted to prepositions or English. In the WSJ corpus, determiners account for 0.05% of the types but for 9.8% of the tokens. In the German NEGRA corpus (Brants, 1997), the article class (both definite and indefinite) accounts for 0.04% of the word types and </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
<author>David Heckerman</author>
</authors>
<title>An Experimental Comparison of Model-based Clustering Methods.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--1</pages>
<contexts>
<context position="10487" citStr="Meila and Heckerman, 2001" startWordPosition="1714" endWordPosition="1717">tic measures, and motivate our decision to focus on the first in this paper. Mapping based measures are based on a postprocessing step in which each induced cluster is mapped to a gold class (or vice versa). The standard mappings are greedy many-to-one (M-1) and greedy one-to-one (1-1). Several measures which rely on these mappings were proposed. The most common and perhaps the simplest one is accuracy, which computes the fraction of items correctly clustered under the mapping. Other measures include: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1</context>
</contexts>
<marker>Meila, Heckerman, 2001</marker>
<rawString>Marina Meila and David Heckerman, 2001. An Experimental Comparison of Model-based Clustering Methods. Machine Learning, 42(1/2):9-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing Clustering – an Information Based Distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<pages>98--873</pages>
<contexts>
<context position="11282" citStr="Meila (2007)" startWordPosition="1840" endWordPosition="1841">o the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address counting pairs measures in this paper. Information-theoretic (IT) measures. IT measures assume that the items in the dataset are taken from a known distribution (usually the uniform distribution), and thus the gold and induced clusters can be treated as random varia</context>
<context position="12766" citStr="Meila, 2007" startWordPosition="2103" endWordPosition="2104">assuming the uniform distribution, the probability of an event (a gold class c or an induced cluster k) is its relative size, so p(c) _ E|k |1 Iiv and p(k) _ Eg|1 Iiv (N is the total number of clustered items). Under this assumption we define the entropies and the conditional entropies: P|K |P|K| H(C) = − P|C |k=1 Ick k=1 Ick N log c=1 N H(C|K) = − P|K|P|C|Ick N log Ick k=1 c=1 P|C| c=1 Ick H(K) and H(KIC) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: 1 (C|K) H(C) = 0 h =( H(C) =6 0 1 H(K) = 0 1 H(K) =6 0 c = 1 − ( − HH( H(K|C) H(K) 2hc V = h + c 79 H(CJK)}H(K1C) H C 0 NV I(C, K) = H(C) ( ) # H(K) H(C) = 0 In the monosemous case (type or token), the application of the measures described in this section to type level evaluation is straightforward. In the polysemous case, however, they suffer from serious shortcomings. Consider a case in which each item is assigned exactly r gol</context>
</contexts>
<marker>Meila, 2007</marker>
<rawString>Marina Meila, 2007. Comparing Clustering – an Information Based Distance. Journal of Multivariate Analysis, 98:873-895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Milligan</author>
<author>S C Soon</author>
<author>L M Sokol</author>
</authors>
<title>The Effect of Cluster Size, Dimensionality and the Number of Clusters on Recovery of True Cluster Structure.</title>
<date>1983</date>
<booktitle>IEEE transactions on Pattern Analysis and Machine Intelligence,</booktitle>
<pages>5--40</pages>
<contexts>
<context position="11091" citStr="Milligan et al., 1983" startWordPosition="1810" endWordPosition="1813">d Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address counting pairs measures in this paper. Information-theoretic (IT) measures.</context>
</contexts>
<marker>Milligan, Soon, Sokol, 1983</marker>
<rawString>C.W Milligan, S.C Soon and L.M Sokol, 1983. The Effect of Cluster Size, Dimensionality and the Number of Clusters on Recovery of True Cluster Structure. IEEE transactions on Pattern Analysis and Machine Intelligence, 5:40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris G Mirkin</author>
</authors>
<title>Mathematical Classification and Clustering.</title>
<date>1996</date>
<publisher>Kluwer Academic Press.</publisher>
<contexts>
<context position="11162" citStr="Mirkin, 1996" startWordPosition="1821" endWordPosition="1822">precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address counting pairs measures in this paper. Information-theoretic (IT) measures. IT measures assume that the items in the dataset are taken from a know</context>
</contexts>
<marker>Mirkin, 1996</marker>
<rawString>Boris G. Mirkin, 1996. Mathematical Classification and Clustering. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mitzenmacher</author>
</authors>
<title>A Brief History of Generative Models for Power Law and Lognormal Distributions.</title>
<date>2004</date>
<journal>Internet Mathematics,</journal>
<pages>1--2</pages>
<contexts>
<context position="8454" citStr="Mitzenmacher, 2004" startWordPosition="1378" endWordPosition="1379">would do much better in a token level evaluation than in a type level one. This phenomenon is not restricted to prepositions or English. In the WSJ corpus, determiners account for 0.05% of the types but for 9.8% of the tokens. In the German NEGRA corpus (Brants, 1997), the article class (both definite and indefinite) accounts for 0.04% of the word types and for 12.5% of the word tokens, and the coordinating conjunctions class accounts for 0.05% of the word types but for 3% of the tokens. The type and token behavior differences result from the Zipfian distribution of word tokens to word types (Mitzenmacher, 2004). Since the word frequency distribution is Zipfian, any clustering algorithm that is accurate only on a small number of frequent words (not necessarily members of a particular class) would perform well in a token level evaluation but not in a type one. For example, 78 the most frequent 100 word types (regardless of POS class) in WSJ (NEGRA) account for 43.9% (41.3%) of the tokens in the corpus. These words appear in 32 out of the 34 non-punctuation POS classes in WSJ and in 38 out of the 51 classes in NEGRA. Other natural language entities also demonstrate Zipfian distribution of tokens to typ</context>
</contexts>
<marker>Mitzenmacher, 2004</marker>
<rawString>Michael Mitzenmacher , 2004. A Brief History of Generative Models for Power Law and Lognormal Distributions. Internet Mathematics, 1(2):226-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soto Montalvo</author>
<author>Raquel Martnez</author>
<author>Arantza Casillas</author>
<author>Vctor Fresno</author>
</authors>
<title>Multilingual Document Clustering: an Heuristic Approach Based on Cognate Named Entities.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<marker>Montalvo, Martnez, Casillas, Fresno, 2006</marker>
<rawString>Soto Montalvo, Raquel Martnez, Arantza Casillas, and Vctor Fresno, 2006. Multilingual Document Clustering: an Heuristic Approach Based on Cognate Named Entities. ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the Assignment and Transportation Problems.</title>
<date>1957</date>
<journal>Journal of the SIAM,</journal>
<pages>5--1</pages>
<contexts>
<context position="20347" citStr="Munkres, 1957" startWordPosition="3489" endWordPosition="3490">ut on a heuristic. Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality. In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure. 1-1 Mapping. We compute h by finding the maximal weighted matching in a bipartite graph. In this graph one side represents the induced clusters, the other represents the gold classes and the matchings correspond to 1-1 mappings. The problem can be efficiently solved by the KuhnMunkres algorithm (Kuhn, 1955; Munkres, 1957). To be able to use this technique, edge weights must not depend upon h. In 1-1 mapping, |h(Bi) |= |Bi|, and therefore F(h) = F and wi(h) = wi. That is, both quantities are independent of h3. For MacroI, the weight on the edge between the s-th gold class and the j-th induced cluster is: W(esj) = E1=1 F · IsEAiIjEBi. For MicroI it is: W(esj) = EZ=1 wi · IsEAiIjEBi. IsEAi is 1 if s E Ai and 0 otherwise. M-1 Mapping. There are two problems in applying the bipartite graph technique to finding an M-1 mapping. First, under such mapping wi(h) and F(h) do depend on h. The problem may be solved by sele</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres, 1957. Algorithms for the Assignment and Transportation Problems. Journal of the SIAM, 5(1):32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Nicolae</author>
<author>Gabriel Nicolae</author>
</authors>
<title>BESTCUT: A Graph Algorithm for Coreference Resolution.</title>
<date>2006</date>
<journal>EMNLP</journal>
<volume>06</volume>
<contexts>
<context position="1849" citStr="Nicolae and Nicolae, 2006" startWordPosition="276" endWordPosition="279"> Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics </context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>Cristina Nicolae and Gabriel Nicolae, 2006. BESTCUT: A Graph Algorithm for Coreference Resolution. EMNLP ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darius M Pfitzner</author>
<author>Richard E Leibbrandt</author>
<author>David M W Powers</author>
</authors>
<title>Characterization and Evaluation of Similarity Measures for Pairs of Clusterings. Knowledge and Information Systems:</title>
<date>2008</date>
<journal>An International Journal, DOI</journal>
<pages>10--1007</pages>
<contexts>
<context position="9749" citStr="Pfitzner et al., 2008" startWordPosition="1592" endWordPosition="1595">onstituents is Zipfian, as shown in (Reichart and Rappoport, 2008) for English, German and Chinese corpora. Thus, the distinction between token and type level evaluation is important also for grammar induction algorithms. It may be argued that a token level evaluation is sufficient since it already reflects type information. In this paper we demonstrate that this is not the case, by showing that they correlate weakly or even negatively in an important NLP task. 3 Existing Clustering Evaluation Measures Clustering evaluation is challenging. Many measures have been proposed in the past decades (Pfitzner et al., 2008). In this section, we briefly survey the three main types: mapping based, counting pairs, and information theoretic measures, and motivate our decision to focus on the first in this paper. Mapping based measures are based on a postprocessing step in which each induced cluster is mapped to a gold class (or vice versa). The standard mappings are greedy many-to-one (M-1) and greedy one-to-one (1-1). Several measures which rely on these mappings were proposed. The most common and perhaps the simplest one is accuracy, which computes the fraction of items correctly clustered under the mapping. Other</context>
<context position="12897" citStr="Pfitzner et al., 2008" startWordPosition="2120" endWordPosition="2123">ize, so p(c) _ E|k |1 Iiv and p(k) _ Eg|1 Iiv (N is the total number of clustered items). Under this assumption we define the entropies and the conditional entropies: P|K |P|K| H(C) = − P|C |k=1 Ick k=1 Ick N log c=1 N H(C|K) = − P|K|P|C|Ick N log Ick k=1 c=1 P|C| c=1 Ick H(K) and H(KIC) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: 1 (C|K) H(C) = 0 h =( H(C) =6 0 1 H(K) = 0 1 H(K) =6 0 c = 1 − ( − HH( H(K|C) H(K) 2hc V = h + c 79 H(CJK)}H(K1C) H C 0 NV I(C, K) = H(C) ( ) # H(K) H(C) = 0 In the monosemous case (type or token), the application of the measures described in this section to type level evaluation is straightforward. In the polysemous case, however, they suffer from serious shortcomings. Consider a case in which each item is assigned exactly r gold clusters and each gold cluster has the exact same number of items (i.e., each has a size of ��� |C|, where l is the number of ite</context>
</contexts>
<marker>Pfitzner, Leibbrandt, Powers, 2008</marker>
<rawString>Darius M. Pfitzner, Richard E. Leibbrandt and David M.W Powers, 2008. Characterization and Evaluation of Similarity Measures for Pairs of Clusterings. Knowledge and Information Systems: An International Journal, DOI 10.1007/s10115-008-0150-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Rand</author>
</authors>
<title>Objective Criteria for the Evaluation of Clustering Methods.</title>
<date>1971</date>
<journal>Journal of the American Statstical Association,</journal>
<pages>66--336</pages>
<contexts>
<context position="10971" citStr="Rand, 1971" startWordPosition="1793" endWordPosition="1794">nclude: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their u</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William Rand, 1971. Objective Criteria for the Evaluation of Clustering Methods. Journal of the American Statstical Association, 66(336):846-850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised Induction of Labeled Parse Trees by Clustering with Syntactic Features.</title>
<date>2008</date>
<journal>COLING</journal>
<volume>08</volume>
<contexts>
<context position="1467" citStr="Reichart and Rappoport, 2008" startWordPosition="219" endWordPosition="222">polysemous, the common case in NLP. We demonstrate the benefits of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against man</context>
<context position="9193" citStr="Reichart and Rappoport, 2008" startWordPosition="1503" endWordPosition="1506">number of frequent words (not necessarily members of a particular class) would perform well in a token level evaluation but not in a type one. For example, 78 the most frequent 100 word types (regardless of POS class) in WSJ (NEGRA) account for 43.9% (41.3%) of the tokens in the corpus. These words appear in 32 out of the 34 non-punctuation POS classes in WSJ and in 38 out of the 51 classes in NEGRA. Other natural language entities also demonstrate Zipfian distribution of tokens to types. For example, the distribution of syntactic categories in parse tree constituents is Zipfian, as shown in (Reichart and Rappoport, 2008) for English, German and Chinese corpora. Thus, the distinction between token and type level evaluation is important also for grammar induction algorithms. It may be argued that a token level evaluation is sufficient since it already reflects type information. In this paper we demonstrate that this is not the case, by showing that they correlate weakly or even negatively in an important NLP task. 3 Existing Clustering Evaluation Measures Clustering evaluation is challenging. Many measures have been proposed in the past decades (Pfitzner et al., 2008). In this section, we briefly survey the thr</context>
</contexts>
<marker>Reichart, Rappoport, 2008</marker>
<rawString>Roi Reichart and Ari Rappoport, 2008. Unsupervised Induction of Labeled Parse Trees by Clustering with Syntactic Features. COLING ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>The NVI Clustering Evaluation Measure.</title>
<date>2009</date>
<journal>CoNLL</journal>
<volume>09</volume>
<contexts>
<context position="12724" citStr="Reichart and Rappoport, 2009" startWordPosition="2094" endWordPosition="2097"> of the i-th gold class and the j-th induced cluster. When assuming the uniform distribution, the probability of an event (a gold class c or an induced cluster k) is its relative size, so p(c) _ E|k |1 Iiv and p(k) _ Eg|1 Iiv (N is the total number of clustered items). Under this assumption we define the entropies and the conditional entropies: P|K |P|K| H(C) = − P|C |k=1 Ick k=1 Ick N log c=1 N H(C|K) = − P|K|P|C|Ick N log Ick k=1 c=1 P|C| c=1 Ick H(K) and H(KIC) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: 1 (C|K) H(C) = 0 h =( H(C) =6 0 1 H(K) = 0 1 H(K) =6 0 c = 1 − ( − HH( H(K|C) H(K) 2hc V = h + c 79 H(CJK)}H(K1C) H C 0 NV I(C, K) = H(C) ( ) # H(K) H(C) = 0 In the monosemous case (type or token), the application of the measures described in this section to type level evaluation is straightforward. In the polysemous case, however, they suffer from serious shortcomings. Consider a case in</context>
<context position="36233" citStr="Reichart and Rappoport, 2009" startWordPosition="6265" endWordPosition="6268">nd complement each other as evaluation measures. ARR10, a monosemous algorithm, yields the best results in most measures. (GJ08* results are different from those reported in the original paper because it was run with weaker computing resources than those used there.) est weight edge in the bipartite graph. Recall from Section 4.1 that this is a reasonable approximation of the greedy M-1 mapping. Again, we ran it for the three type level measures for 10 times with a random update order on the induced clusters. This had only a minor effect on the final scores. Number of clusters. Previous work (Reichart and Rappoport, 2009) demonstrated that in data sets where a relatively small fraction of the gold classes covers most of the items, it is reasonable to choose this number to be the number of induced clusters. In our experimental data set, this number (the ‘prominent cluster number’) is around 17 (see Section 5.1). Up to this number, increasing the number of clusters is likely to have a positive effect on token level M-1, 1-1, H(C|K), and H(K|C) scores. Inducing a larger number of clusters, however, is likely to positively affect M-1 and H(C|K) but to have a negative effect on 1-1 and H(K|C). This tendency is refl</context>
</contexts>
<marker>Reichart, Rappoport, 2009</marker>
<rawString>Roi Reichart and Ari Rappoport, 2009. The NVI Clustering Evaluation Measure. CoNLL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>VMeasure: A Conditional Entropy-based External Cluster Evaluation Measure.</title>
<date>2007</date>
<journal>EMNLP</journal>
<volume>07</volume>
<contexts>
<context position="12685" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2088" endWordPosition="2091">s the number of items in the intersection of the i-th gold class and the j-th induced cluster. When assuming the uniform distribution, the probability of an event (a gold class c or an induced cluster k) is its relative size, so p(c) _ E|k |1 Iiv and p(k) _ Eg|1 Iiv (N is the total number of clustered items). Under this assumption we define the entropies and the conditional entropies: P|K |P|K| H(C) = − P|C |k=1 Ick k=1 Ick N log c=1 N H(C|K) = − P|K|P|C|Ick N log Ick k=1 c=1 P|C| c=1 Ick H(K) and H(KIC) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: 1 (C|K) H(C) = 0 h =( H(C) =6 0 1 H(K) = 0 1 H(K) =6 0 c = 1 − ( − HH( H(K|C) H(K) 2hc V = h + c 79 H(CJK)}H(K1C) H C 0 NV I(C, K) = H(C) ( ) # H(K) H(C) = 0 In the monosemous case (type or token), the application of the measures described in this section to type level evaluation is straightforward. In the polysemous case, however, they suffer from s</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg, 2007. VMeasure: A Conditional Entropy-based External Cluster Evaluation Measure. EMNLP ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the Automatic Induction of German Semantic Verb Classes. Computational Linguistics,</title>
<date>2006</date>
<pages>32--2</pages>
<contexts>
<context position="1518" citStr="Walde, 2006" startWordPosition="228" endWordPosition="229">ur measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. clustering (Montavlo et al., 2006), coreference resolution (Nicolae and Nicolae, 2006) and named entity recognition (Elsner et al., 2009). Consequently, the methodology of clustering evaluation is of great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for alm</context>
<context position="11187" citStr="Walde (2006)" startWordPosition="1825" endWordPosition="1826">lon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 1976), Jaccard (Milligan et al., 1983), Fowlkes-Mallows (Fowlkes and Mallows, 1983) and Mirkin (Mirkin, 1996). Schulte im Walde (2006) used such a measure for type level evaluation of monosemous verb type clustering. Meila (2007) described a few problems with such measures. A serious one is that their values are unbounded, making it hard to interpret their results. This can be solved by adjusting their values to lie in [0, 1], but even adjusted measures suffer from severe distributional problems, limiting their usability in practice. We thus do not address counting pairs measures in this paper. Information-theoretic (IT) measures. IT measures assume that the items in the dataset are taken from a known distribution (usually t</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde, 2006. Experiments on the Automatic Induction of German Semantic Verb Classes. Computational Linguistics, 32(2):159-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing Techniques for Unsupervised Statistical Language Learning.</title>
<date>2004</date>
<journal>ACL</journal>
<volume>04</volume>
<contexts>
<context position="26781" citStr="Smith and Eisner, 2004" startWordPosition="4667" endWordPosition="4670">e cluster’s weight (IkI/N*). 5 Evaluation of POS Induction Models As a detailed case study for the ideas presented in this paper, we apply the various measures for the POS induction task, using seven leading POS induction algorithms. 5.1 Experimental Setup POS Induction Algorithms. We experimented with the following models: ARR10 (Abend et al., 2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al., 2009) (three models). Additional recent good results for various variants of the POS induction problem are described in e.g., (Smith and Eisner, 2004; Grac¸a et al., 2009). Clark03 and ARR10 are monosemous algorithms, allowing a single cluster for each word type. The other algorithms are polysemous. They perform sequence labeling where each token is tagged in its context, and different tokens (instances) of the same type (word form) may receive different tags. Data Set. All models were tested on sections 2-21 of the PTB-WSJ, which consists of 39832 sentences, 950028 tokens and 39546 unique types. Of the tokens, 832629 (87.6%) are not punctuation marks. Evaluation Measures. Type level evaluation used the measures MacroI (which is equal to M</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>Noah A. Smith and Jason Eisner, 2004. Annealing Techniques for Unsupervised Statistical Language Learning. ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn Van Dongen</author>
</authors>
<title>Performance Criteria for Graph Clustering and Markov Cluster Experiments. Technical report CWI,</title>
<date>2000</date>
<location>Amsterdam</location>
<marker>Van Dongen, 2000</marker>
<rawString>Stijn Van Dongen, 2000. Performance Criteria for Graph Clustering and Markov Cluster Experiments. Technical report CWI, Amsterdam</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<date>2009</date>
<booktitle>The Infinite HMM for Unsupervised POS Tagging. EMNLP ’09.</booktitle>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahramani, 2009. The Infinite HMM for Unsupervised POS Tagging. EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yujing Zeng</author>
<author>Jianshan Tang</author>
<author>Javier Garcia-Frias</author>
<author>Guang R Gao</author>
</authors>
<title>An Adaptive Meta-clustering Approach: Combining the Information from Different Clustering Results.</title>
<date>2002</date>
<tech>CSB 00:276</tech>
<contexts>
<context position="10456" citStr="Zeng et al., 2002" startWordPosition="1709" endWordPosition="1712"> and information theoretic measures, and motivate our decision to focus on the first in this paper. Mapping based measures are based on a postprocessing step in which each induced cluster is mapped to a gold class (or vice versa). The standard mappings are greedy many-to-one (M-1) and greedy one-to-one (1-1). Several measures which rely on these mappings were proposed. The most common and perhaps the simplest one is accuracy, which computes the fraction of items correctly clustered under the mapping. Other measures include: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al., 2002), H (Meila and Heckerman, 2001), clustering F-measure (Fung et al., 2003) and micro-averaged precision and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among these are Rand Index (Rand, 1971), Adjusted Rand Index (Hubert and Arabie, 1985), F statistic (Hubert and Schultz, 197</context>
</contexts>
<marker>Zeng, Tang, Garcia-Frias, Gao, 2002</marker>
<rawString>Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and Guang R. Gao, 2002. An Adaptive Meta-clustering Approach: Combining the Information from Different Clustering Results. CSB 00:276</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>