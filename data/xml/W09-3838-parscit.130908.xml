<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.996944">
Dependency Parsing with Energy-based Reinforcement Learning
</title>
<author confidence="0.997575">
Lidan Zhang
</author>
<affiliation confidence="0.9929785">
Department of Computer Science
The University of Hong Kong
</affiliation>
<address confidence="0.514807">
Pokfulam Road, Hong Kong
</address>
<email confidence="0.96401">
lzhang@cs.hku.hk
</email>
<author confidence="0.990838">
Kwok Ping Chan
</author>
<affiliation confidence="0.9931965">
Department of Computer Science
The University of Hong Kong
</affiliation>
<address confidence="0.523953">
Pokfulam Road, Hong Kong
</address>
<email confidence="0.982861">
kpchan@cs.hku.hk
</email>
<sectionHeader confidence="0.994745" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991725">
We present a model which integrates
dependency parsing with reinforcement
learning based on Markov decision pro-
cess. At each time step, a transition is
picked up to construct the dependency tree
in terms of the long-run reward. The op-
timal policy for choosing transitions can
be found with the SARSA algorithm. In
SARSA, an approximation of the state-
action function can be obtained by calcu-
lating the negative free energies for the
Restricted Boltzmann Machine. The ex-
perimental results on CoNLL-X multilin-
gual data show that the proposed model
achieves comparable results with the cur-
rent state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998098" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933152173913">
Dependency parsing, an important task, can be
used to facilitate some natural language applica-
tions. Given a sentence, dependency parsing is
to find an acyclic labeled directed tree, projective
or non-projective.The label of each edge gives the
syntactic relationship between two words.
Data-driven dependency parsers can be catego-
rized into graph-based and transition-based mod-
els. Both of these two models have their advan-
tages as well as drawbacks. As discussed in (Mc-
Donald and Satta, 2007), transition-based mod-
els use local training and greedy inference algo-
rithms, with a rich feature set, whereas they might
lead to error propagation. In contrast, graph-based
models are globally trained coupled with exact in-
ference algorithms, whereas their features are re-
stricted to a limited number of graph arcs. Nivre
and McDonald (2008) presented a successful at-
tempt to integrate these two models by exploiting
their complementary strengths.
There are other researches on improving the
individual model with a novel framework. For
example, Daum´e et al. (2006) applied a greedy
search to transition-based model, which was ad-
justed by the resulting errors. Motivated by his
work, our transition-based model is expected to
overcome local dependencies by using a long-term
desirability introduced by reinforcement learning
(RL). We rely on a “global” policy to guide each
action selection for a particular state during pars-
ing. This policy considers not only the current
configuration but also a few of look-ahead steps.
Thus it yields an optimal action from the long-
term goal. For example, an action might return
a high value even if it produces a low immediate
reward, because its following state-actions might
yield high rewards. The reverse also holds true.
Finally we formulate the parsing problem with the
Markov Decision Process (MDP) for the dynamic
settings.
The reminder of this paper is organized as fol-
lows: Section 2 describes the transition-based de-
pendency parsing. Section 3 presents the proposed
reinforcement learning model. Section 4 gives the
experimental results. Finally, Section 5 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.910636" genericHeader="method">
2 Transition-based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.9997922">
In this paper, we focus on the transition-based
dependency parsing in a shift-reduce frame-
work (K¨ubler et al., 2009). Given a sentence
x = w0, w1, ..., wn, its dependency tree is con-
structed by a sequence of transitions. The data
structures include a stack 5 to store partially pro-
cessed words and a queue I to record the remain-
ing input words and the partial labeled dependency
structure constructed by the previous transitions.
Four permissible transitions are considered: Re-
duce: pops word wi from the stack; Shift: pushes
the next input wj onto the stack; Left-Arc,: adds
a labeled dependency arc r from the next input wj
to the top of the stack wi, then pops word wi from
the stack; Right-Arc,: adds a dependency arc r
</bodyText>
<page confidence="0.513879">
234
</page>
<note confidence="0.8425015">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 234–237,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<figureCaption confidence="0.965654666666667">
Figure 1: The MDP with factored states and actions. Left: The general network. Right: Detailed network
with one hidden layer at time t. Visible variables (states and actions) are shaded. Clear circles represent
hidden variables.
</figureCaption>
<figure confidence="0.992342904761905">
&apos;
i�
ht&apos;
IV.�it
&apos;
S`
A�
a&apos;i
�� &apos;
2
�&apos;
1
A1
S1
S&apos;
A&apos;
A�
S�
A��1
S��1
���1
</figure>
<bodyText confidence="0.999571555555555">
from the top of the stack wi to the next input wj,
and pushes word wj onto the stack.
Starting from the empty stack and initializing
the queue I as the input words, the parser termi-
nates when the queue I is empty. The optimal
transition (or say, action/decision A) in each step
is conditioned on the current configuration c of
the parser. For non-projective cases, preprocess-
ing and postprocessing are applied.
</bodyText>
<sectionHeader confidence="0.965529" genericHeader="method">
3 Reinforcement Learning
</sectionHeader>
<subsectionHeader confidence="0.986275">
3.1 General Framework
</subsectionHeader>
<bodyText confidence="0.9958628">
We begin with looking at the general framework to
integrate RL into the transition-based dependency
model. In this paper, we reformulate the depen-
dency parsing as Markov Decision Process (MDP,
(S, A, T , r)) where:
</bodyText>
<listItem confidence="0.968292428571428">
• S is the set of states.
• A is the set of possible actions.
• T is the transition function, T : S x A —* S.
we denote the transition probability Pij(a) =
P(st+1 = j|st = i, At = a).
• r is the reward function by executing action
a in a certain state, which is denoted as ri(a).
</listItem>
<bodyText confidence="0.999886">
As aforesaid, the key task of dependency pars-
ing is to select the optimal action to be performed
based on the current state. Given the expected im-
mediate reward r, the optimal policy (7r : 5 H A)
is to maximize the long-term expected reward as
follows:
</bodyText>
<equation confidence="0.989441">
00
Rt = X lykrt+k (1)
k=0
</equation>
<bodyText confidence="0.998605727272727">
Given a policy 7r, state-action function Q&amp;quot;(i, a)
can be defined as the expected accumulative re-
ward received by taking action a in state s. It takes
the following form:
Here 7r(j, b) is the probability of picking up action
b in state j, -y E [0, 1] is a discount factor to con-
trol the involvement of further actions. According
to the Bellman equation, the state-action function
can be updated iteratively with equation( 2).
Given the state-action function, a greedy policy
can be found by maximizing over possible actions:
</bodyText>
<equation confidence="0.8061815">
= arg max Q&amp;quot;(i, a) (3)
�
</equation>
<bodyText confidence="0.999554">
In the following, we will discuss how to com-
pute the state-action function Q by investigating
the free energy in RBM.
</bodyText>
<subsectionHeader confidence="0.9967075">
3.2 Restricted Boltzmann Machine
3.2.1 Free Energy
</subsectionHeader>
<bodyText confidence="0.999714846153846">
Figure 1 shows the general framework of our
model. At each time step t, there is no connections
between nodes within the same layer. In the net-
work, “visible” variables include both states and
actions (V = 5UA). The visible layer is fully con-
nected to a “hidden” layer, which can be regarded
as a Restricted Boltzmann Machine (RBM).
In our model, both states and actions are fac-
tored. They are consisted of a sets of discrete vari-
ables (Sallans and Hinton, 2004). The stochas-
tic energy of the network can be computed by
the conductivities between visible and hidden vari-
ables.
</bodyText>
<equation confidence="0.653754">
XE(s,a,h) = − Xwiksihk − Pjkajhk (4)
i,k j,k
7r&apos;
Q&amp;quot;(i, a) = E&amp;quot;[ 00 7krt+k|st = i, at = a]
X
k=0
X= Pij(a)[ri(a) + &apos;Y X 7r(j, b)Q&amp;quot;(j, b)]
j b
(2)
235
</equation>
<bodyText confidence="0.986578">
The above energy determine their equilibrium
probabilities via the Boltzmann distribution:
</bodyText>
<equation confidence="0.975754">
exp(−E(s, a, h))
P(s, a, h) = (5)
Es/,a/,h/ exp(−E(s&apos;, a&apos;, h&apos;))
</equation>
<bodyText confidence="0.999634">
By marginalizing out the hidden variables, we
can obtain the “equilibrium free energy” of s and
a, which can be expressed as an expected energy
minus an entropy:
</bodyText>
<equation confidence="0.9886564">
F(s,a)=−Ek(Ei(wiksi(hk))+Ej(µjkaj(hk))) (6)
+Ek(hk) log(hk)+(1−(hk)) log(1−(hk))
where (hk) is the expected value of variable hk:
(hk) = �( 1: 1: wiksi + Pjkaj) (7)
i,k j,k
</equation>
<bodyText confidence="0.99913125">
where Q = 1/(1 + e−x) is a sigmoid function.
As is proved in (Sallans and Hinton, 2004), the
value of a state-action function can be approxi-
mated by the negative free energy of the network:
</bodyText>
<note confidence="0.503348">
Q(s, a) Pz −F(s, a) (8)
</note>
<subsubsectionHeader confidence="0.519957">
3.2.2 Parameter Learning
</subsubsectionHeader>
<bodyText confidence="0.9999868">
The parameters of the network can be updated by
the SARSA (State-Action-Reward-State-Action)
algorithm. The inputs of the SARSA algorithm
are the state-action pairs of the two neighboring
slices. Then the error can be computed as:
</bodyText>
<equation confidence="0.973521">
£(st, at) = [rt+&apos;YQ(st+1, tt+1)]−Q(st, at) (9)
</equation>
<bodyText confidence="0.998259666666667">
Suppose the state-action function is parameter-
ized by 0. The update equation for the parameter
is:
</bodyText>
<note confidence="0.34157">
AB OC £(st, at)VθQ(st, at) (10)
</note>
<bodyText confidence="0.992296">
Back to our model, the parameters 0 = (w, u)
are given by:
</bodyText>
<equation confidence="0.9962915">
Dwik OC(rt+γQ(st+1,at+1)−Q(st,at))sti(hk)
Dujk OC(rt+γQ(st+1,at+1)−Q(st,at))atj(hk) (11)
</equation>
<bodyText confidence="0.993370285714286">
Leemon (1993) showed that the above update
rules can work well in practice even though there
is no proof of convergence in theory. In addition,
in dependency parsing task, the possible action
number is small (=4). Our experimental results
also showed that the learning rule can converge in
practice.
</bodyText>
<subsectionHeader confidence="0.999617">
3.3 Action Selection
</subsectionHeader>
<bodyText confidence="0.999952333333333">
After training, we use the softmax rules to select
the optimal action for a given state. The probabil-
ity of an action is given by Boltzmann distribution:
</bodyText>
<equation confidence="0.959374333333333">
eQ(s,a)/τ
P(a|s) � (12)
Z
</equation>
<bodyText confidence="0.999123666666667">
Here Z is an normalization factor. T is a pos-
itive number called the temperature. High tem-
perature means the actions are evenly distributed.
Low temperature case a great variety in selection
probability. In the limit as T —* 0, softmax action
selection becomes greedy action selection.
</bodyText>
<sectionHeader confidence="0.998181" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996141">
4.1 Settings
</subsectionHeader>
<bodyText confidence="0.992425361111111">
We use the CoNLL-X (Buchholz and Marsi,
2006) distribution data from seven different lan-
guages (Arabic, Bulgarian, Dutch, Portuguese,
Slovene, Spanish and Swedish). These treebanks
varied in sizes from 29,000 to 207,000 tokens. The
cut-off frequency for training data is 20, which
means we ignores any attribute (FORM, LEMMA,
POS or FEATS) occurred less than 20. Further-
more we randomly selected 10 percent of train-
ing data to construct the validation set. Test sets
are about equal for all languages. Since our algo-
rithm only deals with projective cases, we use pro-
jectivization/deprojectivization method for train-
ing and testing data.
For fair comparison, we use the exactly same
feature set as Nivre et al. (2006), which is com-
prised of a variety of features extracted from the
stack, the queue and the partially built dependency
graph.
In our experiment, the immediate reward value
is defined as the Hamming Loss between partial
tree and expected tree, which counts the number
of places that the partial output y� differs from the
T
true output y: �i=1 1[yi =� yi].
As shown in Figure 1, we compute the state-
action function using a feed-forward neural net-
work with one hidden layer. The number of hid-
den variables is set to match the variable number in
the visible layer (i.e. total number of state and ac-
tion variables). The parameters of the network are
modified by SARSA algorithm according to equa-
tion 2. Finally, 10-width beam search is employed
for all languages, during testing.
There are other parameters in our experiments,
which can be tuned using search. For simplicity,
</bodyText>
<page confidence="0.552747">
236
</page>
<table confidence="0.9920838">
Ar Bu Du Po Sl Sp Sw
LAS Our 63.24 88.89 79.06 87.54 72.44 82.79 87.20
Nivre 66.71 87.41 78.59 87.60 70.30 81.29 84.58
UAS Our 75.30 92.88 83.14 91.34 80.06 86.18 91.84
Nivre 77.51 91.72 81.35 91.22 78.72 84.67 89.50
</table>
<tableCaption confidence="0.999971">
Table 1: Comparison of dependency accuracy with Nivre
</tableCaption>
<bodyText confidence="0.99999325">
the learning rate was exponentially decreased form
0.1 to 0.01 in the course of each epoch. In ideal
cases, the discount factor should be set to 1. In our
experiments, discount factor is fixed to 0.6 consid-
ering the computational burden in long sentence.
The study of the this parameter is still left for fu-
ture work. Finally, the inverse temperature linearly
increased from 0 to 2.
</bodyText>
<sectionHeader confidence="0.598737" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.99998476">
The performance of our model is evaluated by
the official attachment score, including labeled
(LAS=the percentage of tokens with the correct
head and label) and unlabeled (UAS=the percent-
age of tokens with the correct head). Punctuation
tokens were excluded from scoring.
The result comparison between our system and
Nivre’s transition-based system is shown in Ta-
ble 11. From the table, we can see that the pro-
posed model outperformed the Nivre’s score in all
languages except Arabic. In Arabic, our results are
worse than Nivre, with about 3.5% performance
reduction in LAS measure and 2.2% in UAS. Most
of our errors occur in POSTAGS with N (16%
head errors and 31% dep errors) and P (47% head
errors and 8% dep errors), which is probably due
to the flexible usage of those two tags in Ara-
bic. The best performance of our model happens
in Swedish. The LAS improves from 84.58% to
87.20%, whereas UAS improves from 89.5% to
91.84%. The reason might be that the long depen-
dent relationship is not popular in Swedish. Fi-
nally, we believe the performance will be further
improved by carefully tuning parameters or broad-
ening the beam search width.
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.939273">
In this paper we proposed a dependency parsing
based on reinforcement learning. The parser uses
a policy to select the optimal transition in each
parsing stage. The policy is learned from RL in
1The performance of other systems can be accessed from
http://nextens.uvt.nl/∼conll
terms of the long-term reward. Tentative experi-
mental evaluations show that the introduction of
RL is feasible for some NLP applications. Finally,
there are a lot of future work, including the hierar-
chical model and parameter selections.
</bodyText>
<sectionHeader confidence="0.998463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973197368421">
Sabine Buchholz and Erwin Marsi. 2006. Conll-
x shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149–164, New York City, June. Association
for Computational Linguistics.
Hal Daum´e III, John Langford, and Daniel Marcu.
2006. Searn in practice.
Leemon C. Baird III and A. Harry. Klopf. 1993. Rein-
forcement learning with high-dimensional, contin-
uous actions. Technical Report WL–TR-93-1147,
Wright-Patterson Air Force Base Ohio: Wright Lab-
oratory.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Calif, Morgan &amp; Clay-
pool publishers, US.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the Tenth International
Conference on Parsing Technologies, pages 121–
132, Prague, Czech Republic, June. Association for
Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950–958, Columbus, Ohio, June. Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en
Eryiˇgit, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-X), pages 221–225, New York City,
June. Association for Computational Linguistics.
Brian Sallans and Geoffrey E. Hinton. 2004. Rein-
forcement learning with factored states and actions.
Journal of Machine Learning Research, 5:1063–
1088.
</reference>
<page confidence="0.873828">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.669301">
<title confidence="0.998796">Dependency Parsing with Energy-based Reinforcement Learning</title>
<author confidence="0.934723">Lidan</author>
<affiliation confidence="0.9950765">Department of Computer The University of Hong</affiliation>
<address confidence="0.860262">Pokfulam Road, Hong</address>
<email confidence="0.981521">lzhang@cs.hku.hk</email>
<author confidence="0.998977">Kwok Ping</author>
<affiliation confidence="0.996622">Department of Computer The University of Hong</affiliation>
<address confidence="0.869726">Pokfulam Road, Hong</address>
<email confidence="0.989389">kpchan@cs.hku.hk</email>
<abstract confidence="0.998819">We present a model which integrates dependency parsing with reinforcement learning based on Markov decision process. At each time step, a transition is picked up to construct the dependency tree in terms of the long-run reward. The optimal policy for choosing transitions can be found with the SARSA algorithm. In SARSA, an approximation of the stateaction function can be obtained by calculating the negative free energies for the Restricted Boltzmann Machine. The experimental results on CoNLL-X multilingual data show that the proposed model achieves comparable results with the current state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conllx shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="9107" citStr="Buchholz and Marsi, 2006" startWordPosition="1513" endWordPosition="1516">ults also showed that the learning rule can converge in practice. 3.3 Action Selection After training, we use the softmax rules to select the optimal action for a given state. The probability of an action is given by Boltzmann distribution: eQ(s,a)/τ P(a|s) � (12) Z Here Z is an normalization factor. T is a positive number called the temperature. High temperature means the actions are evenly distributed. Low temperature case a great variety in selection probability. In the limit as T —* 0, softmax action selection becomes greedy action selection. 4 Experiments 4.1 Settings We use the CoNLL-X (Buchholz and Marsi, 2006) distribution data from seven different languages (Arabic, Bulgarian, Dutch, Portuguese, Slovene, Spanish and Swedish). These treebanks varied in sizes from 29,000 to 207,000 tokens. The cut-off frequency for training data is 20, which means we ignores any attribute (FORM, LEMMA, POS or FEATS) occurred less than 20. Furthermore we randomly selected 10 percent of training data to construct the validation set. Test sets are about equal for all languages. Since our algorithm only deals with projective cases, we use projectivization/deprojectivization method for training and testing data. For fair</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conllx shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<date>2006</date>
<note>Searn in practice.</note>
<marker>Langford, Marcu, 2006</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2006. Searn in practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klopf</author>
</authors>
<title>Reinforcement learning with high-dimensional, continuous actions.</title>
<date>1993</date>
<tech>Technical Report WL–TR-93-1147,</tech>
<institution>Wright-Patterson Air Force Base Ohio: Wright Laboratory.</institution>
<marker>Klopf, 1993</marker>
<rawString>Leemon C. Baird III and A. Harry. Klopf. 1993. Reinforcement learning with high-dimensional, continuous actions. Technical Report WL–TR-93-1147, Wright-Patterson Air Force Base Ohio: Wright Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency parsing.</title>
<date>2009</date>
<publisher>Calif, Morgan &amp; Claypool publishers, US.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency parsing. Calif, Morgan &amp; Claypool publishers, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Tenth International Conference on Parsing Technologies,</booktitle>
<pages>121--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1423" citStr="McDonald and Satta, 2007" startWordPosition="211" endWordPosition="215">ltilingual data show that the proposed model achieves comparable results with the current state-of-the-art methods. 1 Introduction Dependency parsing, an important task, can be used to facilitate some natural language applications. Given a sentence, dependency parsing is to find an acyclic labeled directed tree, projective or non-projective.The label of each edge gives the syntactic relationship between two words. Data-driven dependency parsers can be categorized into graph-based and transition-based models. Both of these two models have their advantages as well as drawbacks. As discussed in (McDonald and Satta, 2007), transition-based models use local training and greedy inference algorithms, with a rich feature set, whereas they might lead to error propagation. In contrast, graph-based models are globally trained coupled with exact inference algorithms, whereas their features are restricted to a limited number of graph arcs. Nivre and McDonald (2008) presented a successful attempt to integrate these two models by exploiting their complementary strengths. There are other researches on improving the individual model with a novel framework. For example, Daum´e et al. (2006) applied a greedy search to transi</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proceedings of the Tenth International Conference on Parsing Technologies, pages 121– 132, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1764" citStr="Nivre and McDonald (2008)" startWordPosition="265" endWordPosition="268">e.The label of each edge gives the syntactic relationship between two words. Data-driven dependency parsers can be categorized into graph-based and transition-based models. Both of these two models have their advantages as well as drawbacks. As discussed in (McDonald and Satta, 2007), transition-based models use local training and greedy inference algorithms, with a rich feature set, whereas they might lead to error propagation. In contrast, graph-based models are globally trained coupled with exact inference algorithms, whereas their features are restricted to a limited number of graph arcs. Nivre and McDonald (2008) presented a successful attempt to integrate these two models by exploiting their complementary strengths. There are other researches on improving the individual model with a novel framework. For example, Daum´e et al. (2006) applied a greedy search to transition-based model, which was adjusted by the resulting errors. Motivated by his work, our transition-based model is expected to overcome local dependencies by using a long-term desirability introduced by reinforcement learning (RL). We rely on a “global” policy to guide each action selection for a particular state during parsing. This polic</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨uls¸en Eryiˇgit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>221--225</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiˇgit, and Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 221–225, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Sallans</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Reinforcement learning with factored states and actions.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>5</volume>
<pages>1088</pages>
<contexts>
<context position="6698" citStr="Sallans and Hinton, 2004" startWordPosition="1113" endWordPosition="1116"> following, we will discuss how to compute the state-action function Q by investigating the free energy in RBM. 3.2 Restricted Boltzmann Machine 3.2.1 Free Energy Figure 1 shows the general framework of our model. At each time step t, there is no connections between nodes within the same layer. In the network, “visible” variables include both states and actions (V = 5UA). The visible layer is fully connected to a “hidden” layer, which can be regarded as a Restricted Boltzmann Machine (RBM). In our model, both states and actions are factored. They are consisted of a sets of discrete variables (Sallans and Hinton, 2004). The stochastic energy of the network can be computed by the conductivities between visible and hidden variables. XE(s,a,h) = − Xwiksihk − Pjkajhk (4) i,k j,k 7r&apos; Q&amp;quot;(i, a) = E&amp;quot;[ 00 7krt+k|st = i, at = a] X k=0 X= Pij(a)[ri(a) + &apos;Y X 7r(j, b)Q&amp;quot;(j, b)] j b (2) 235 The above energy determine their equilibrium probabilities via the Boltzmann distribution: exp(−E(s, a, h)) P(s, a, h) = (5) Es/,a/,h/ exp(−E(s&apos;, a&apos;, h&apos;)) By marginalizing out the hidden variables, we can obtain the “equilibrium free energy” of s and a, which can be expressed as an expected energy minus an entropy: F(s,a)=−Ek(Ei(wiksi</context>
</contexts>
<marker>Sallans, Hinton, 2004</marker>
<rawString>Brian Sallans and Geoffrey E. Hinton. 2004. Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5:1063– 1088.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>