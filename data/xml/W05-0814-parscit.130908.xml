<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.998693">
ISI’s Participation in the Romanian-English Alignment Task
</title>
<author confidence="0.985102">
Alexander Fraser
</author>
<affiliation confidence="0.896816666666667">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.622666">
Marina del Rey, CA 90292-6601
</address>
<email confidence="0.998162">
fraser@isi.edu
</email>
<author confidence="0.99626">
Daniel Marcu
</author>
<affiliation confidence="0.899258666666667">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.623313">
Marina del Rey, CA 90292-6601
</address>
<email confidence="0.999311">
marcu@isi.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973">
We discuss results on the shared task of
Romanian-English word alignment. The
baseline technique is that of symmetrizing
two word alignments automatically gener-
ated using IBM Model 4. A simple vo-
cabulary reduction technique results in an
improvement in performance. We also
report on a new alignment model and a
new training algorithm based on alternat-
ing maximization of likelihood with mini-
mization of error rate.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930888888889">
ISI participated in the WPT05 Romanian-English
word alignment task. The system used for baseline
experiments is two runs of IBM Model 4 (Brown et
al., 1993) in the GIZA++ (Och and Ney, 2003) im-
plementation, which includes smoothing extensions
to Model 4. For symmetrization, we found that Och
and Ney’s “refined” technique described in (Och and
Ney, 2003) produced the best AER for this data set
under all experimental conditions.
We experimented with a statistical model for in-
ducing a stemmer cross-lingually, but found that the
best performance was obtained by simply lower-
casing both the English and Romanian text and re-
moving all but the first four characters of each word.
We also tried a new model and a new training
criterion based on alternating the maximization of
likelihood and minimization of the alignment error
rate. For these experiments, we have implemented
</bodyText>
<page confidence="0.9826">
91
</page>
<bodyText confidence="0.999943888888889">
an alignment package for IBM Model 4 using a hill-
climbing search and Viterbi training as described in
(Brown et al., 1993), and extended this to use new
submodels. The starting point is the final alignment
generated using GIZA++’s implementation of IBM
Model 1 and the Aachen HMM model (Vogel et al.,
1996).
Paper organization: Section 2 is on the baseline,
Section 3 discusses vocabulary reduction, Section 4
introduces our new model and training method, Sec-
tion 5 describes experiments, Section 6 concludes.
We use the following notation: e refers to an En-
glish sentence composed of English words labeled
ez. f refers to a Romanian sentence composed of
Romanian words labeled fj. a is an alignment of e
to f. We use the term “Viterbi alignment” to denote
the most probable alignment we can find, rather than
the true Viterbi alignment.
</bodyText>
<sectionHeader confidence="0.990044" genericHeader="introduction">
2 Baseline
</sectionHeader>
<bodyText confidence="0.997767714285714">
To train our systems, Model 4 was trained two times,
first using Romanian as the source language and
then using English as the source language. For each
training, we ran 5 iterations of Model 1, 5 iterations
of the HMM model and 3 iterations of Model 4.
For the distortion calculations of Model 4, we re-
moved the dependencies on Romanian and English
word classes. We applied the “union”, “intersection”
and “refined” symmetrization metrics (Och and Ney,
2003) to the final alignments output from training, as
well as evaluating the two final alignments directly.
We tried to have a strong baseline. GIZA++ has
many free parameters which can not be estimated us-
ing Maximum Likelihood training. We did not use
</bodyText>
<note confidence="0.6719605">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91–94,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.999927">
the defaults, but instead used settings which produce
good AER results on French/English bitext. We
also optimized p0 on the 2003 test set (using AER),
rather than using likelihood training. Turning off the
extensions to GIZA++ and training p0 as in (Brown
et al., 1993) produces a substantial increase in AER.
</bodyText>
<sectionHeader confidence="0.978986" genericHeader="method">
3 Vocabulary Size Reduction
</sectionHeader>
<bodyText confidence="0.999773766666667">
Romanian is a Romance language which has a sys-
tem of suffixes for inflection which is richer than
English. Given the small amount of training data,
we decided that vocabulary size reduction was de-
sirable. As a baseline for vocabulary reduction, we
tried reducing words to prefixes of varying sizes
for both English and Romanian after lowercasing
the corpora. We also tried Porter stemming (Porter,
1997) for English.
(Rogati et al., 2003) extended Model 1 with an ad-
ditional hidden variable to represent the split points
in Arabic between the prefix, the stem and the suf-
fix to generate a stemming for use in Cross-Lingual
Information Retrieval. As in (Rogati et al., 2003),
we can find the most probable stemming given the
model, apply this stemming, and retrain our word
alignment system. However, we can also use the
modified model directly to find the best word align-
ment without converting the text to its stemmed
form.
We introduce a variable rj for the Romanian stem
and a variable sj for the Romanian suffix (which
when concatenated together give us the Romanian
word fj) into the formula for the probability of gen-
erating a Romanian word fj using an alignment aj
given only an English sentence e. We use the index
z to denote a particular stemming possibility. For a
given Romanian word the stemming possibilities are
simply every possible split point where the stem is at
least one character (this includes the null suffix).
</bodyText>
<equation confidence="0.947086">
p(fj, aj|e) _ � p(rj,z, sj,z, aj|e) (1)
z
</equation>
<bodyText confidence="0.826511">
If the assumption is made that the stem and the
suffix are generated independently from e, we can
assume conditional independence.
</bodyText>
<equation confidence="0.920257">
p(fj, aj|e) _ � p(rj,z, aj|e)p(sj,z, aj|e) (2)
z
</equation>
<bodyText confidence="0.999955571428571">
We performed two sets of experiments, one set
where the English was stemmed using the Porter
stemmer and one set where each English word was
stemmed down to its first four characters. We
tried the best performing scoring heuristic for Ara-
bic from (Rogati et al., 2003) where p(sj,z, aj|e) is
modeled using the heuristic p(sj,z|lj) where sj,z is
the Romanian suffix, and lj is the last letter of the
Romanian word fj; these adjustments are updated
during EM training. We also tried several other ap-
proximations of p(sj,z, aj|e) with and without up-
dates in EM training. We were unable to produce
better results and elected to use the baseline vocab-
ulary reduction technique for the shared task.
</bodyText>
<sectionHeader confidence="0.987568" genericHeader="method">
4 New Model and Training Algorithm
</sectionHeader>
<bodyText confidence="0.999955666666667">
Our motivation for a new model and a new training
approach which combines likelihood maximization
with error rate minimization is threefold:
</bodyText>
<listItem confidence="0.997203333333333">
• Maximum Likelihood training of Model 4 is
not sufficient to find good alignments
• We would like to model factors not captured by
IBM Model 4
• Using labeled data could help us produce better
alignments, but we have very few labels
</listItem>
<bodyText confidence="0.9669458">
We create a new model and train it using an al-
gorithm which has a step which increases likelihood
(like one iteration in the EM algorithm), alternating
with a step which decreases error. We accomplish
this by:
</bodyText>
<listItem confidence="0.998008">
• grouping the parameters of Model 4 into 5 sub-
models
• implementing 6 new submodels
• combining these into a single log-linear model
with 11 weights, Al to All, which we group
into the vector A
• defining a search algorithm for finding the
alignment of highest probability given the sub-
models and A
• devising a method for finding a A which min-
imizes alignment error given fixed submodels
and a set of gold standard alignments
• inventing a training method for alternating
</listItem>
<bodyText confidence="0.9941684">
steps which estimate the submodels by increas-
ing likelihood with steps which set A to de-
crease alignment error
The submodels in our new alignment model are
listed in table 1, where for ease of exposition we
</bodyText>
<page confidence="0.997578">
92
</page>
<tableCaption confidence="0.999909">
Table 1: Submodels used for alignment
</tableCaption>
<table confidence="0.9923214">
1 t(fj|ei) TRANSLATION PROBABILITIES
2 n(Oi|ei) FERTILITY PROBABILITIES, Oi IS THE NUMBER OF WORDS GENERATED BY THE ENGLISH WORD ei
3 null PARAMETERS USED IN GENERATING ROMANIAN WORDS FROM ENGLISH NULL WORD (INCLUDING p0, p1)
4 d1(Aj) MOVEMENT (DISTORTION) PROBABILITIES OF FIRST ROMANIAN WORD GENERATED FROM ENGLISH WORD
5 d&gt;1(Aj) MOVEMENT (DISTORTION) PROBABILITIES OF OTHER ROMANIAN WORDS GENERATED FROM ENGLISH WORD
</table>
<sectionHeader confidence="0.543225666666667" genericHeader="method">
6 TTABLE ESTIMATED FROM INTERSECTION OF TWO STARTING ALIGNMENTS FOR THIS ITERATION
7 TRANSLATION TABLE FROM ENGLISH TO ROMANIAN MODEL 1 ITERATION 5
8 TRANSLATION TABLE FROM ROMANIAN TO ENGLISH MODEL 1 ITERATION 5
9 BACKOFF FERTILITY (FERTILITY ESTIMATED OVER ALL ENGLISH WORDS)
10 ZERO FERTILITY ENGLISH WORD PENALTY
11 NON-ZERO FERTILITY ENGLISH WORD PENALTY
</sectionHeader>
<bodyText confidence="0.995683333333333">
consider English to be the source language and Ro-
manian the target language.
The log-linear alignment model is specified by
equation 3. The model assigns non-zero proba-
bilities only to 1-to-many alignments, like Model
4. (Cettolo and Federico, 2004) used a log-linear
model trained using error minimization for the trans-
lation task, 3 of the submodels were taken from
Model 4 in a similar way to our first 5 submodels.
</bodyText>
<equation confidence="0.980523">
exp(Em λmhm(f, a, e))
pλ(a, f|e) = (3)
E f e a exp(Em λmhm(f, a, e))
</equation>
<bodyText confidence="0.998962222222222">
Given λ, the alignment search problem is to find
the alignment a of highest probability according to
equation 3. We solve this using the local search de-
fined in (Brown et al., 1993).
We set λ as follows. Given a sequence A of align-
ments we can calculate an error function, E(A). For
these experiments average sentence AER was used.
We wish to minimize this error function, so we se-
lect λ accordingly:
</bodyText>
<equation confidence="0.5291535">
�argmin
λ a�
</equation>
<bodyText confidence="0.9530146">
Maximizing performance for all of the weights
at once is not computationally tractable, but (Och,
2003) has described an efficient one-dimensional
search for a similar problem. We search over each
λm (holding the others constant) using this tech-
nique to find the best λm to update and the best value
to update it to. We repeat the process until no further
gain can be found.
Our new training method is:
REPEAT
</bodyText>
<listItem confidence="0.9796694">
• Start with submodels and lambda from previ-
ous iteration
• Find Viterbi alignments on entire training cor-
pus using new model (similar to E-step of
Model 4 training)
• Reestimate submodel parameters from Viterbi
alignments (similar to M-step of Model 4
Viterbi training)
• Find a setting for λ that reduces AER on dis-
criminative training set (new D-step)
</listItem>
<bodyText confidence="0.999964142857143">
We use the first 148 sentences of the 2003 test set
for the discriminative training set. 10 settings for λ
are found, the hypothesis list is augmented using the
results of 10 searches using these settings, and then
another 10 settings for λ are found. We then select
the best λ. The discriminative training regimen is
otherwise similar to (Och, 2003).
</bodyText>
<sectionHeader confidence="0.998688" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.86012">
Table 2 provides a comparison of our baseline sys-
tems using the “refined” symmetrization metric with
the best limited resources track system from WPT03
(Dejean et al., 2003) on the 2003 test set. The best
results are obtained by stemming both English and
Romanian words to the first four letters, as described
in section 2.
Table 3 provides details on our shared task sub-
mission. RUN1 is the word-based baseline system.
RUN2 is the stem-based baseline system. RUN4
uses only the first 6 submodels, while RUN5 uses
all 11 submodels. RUN3 had errors in processing,
so we omit it.
Results:
• Our new 1-to-many alignment model and train-
ing method are successful, producing decreases
of 0.03 AER when the source is Romanian, and
0.01 AER when the source is English.
</bodyText>
<equation confidence="0.476222">
E(�a)δ(�a, (argmax pλ(a, f|e))) (4)
a
</equation>
<page confidence="0.996739">
93
</page>
<tableCaption confidence="0.998265">
Table 2: Summary of results for 2003 test set
</tableCaption>
<table confidence="0.9923346">
SYSTEM STEM SIZES AER
XEROX “NOLEM-ER-56K” 0.289
BASELINE NO PROCESSING 0.284
BASELINE ENG PORTER / ROM 4 0.251
BASELINE ENG 4 / ROM 4 0.248
</table>
<tableCaption confidence="0.966572">
Table 3: Full results on shared task submissions (blind test 2005)
</tableCaption>
<table confidence="0.9973186">
RUN NAMES STEM SIZES SOURCE ROM SOURCE ENG UNION INTERSECTION REFINED
ISI.RUN1 NO PROCESSING 0.3834 0.3589 0.3590 0.3891 0.3165
ISI.RUN2 ENG 4 / ROM 4 0.3056 0.2880 0.2912 0.3041 0.2675
ISI.RUN4 ENG 4 / ROM 4 0.2798 0.2833 0.2773 0.2862 0.2663
ISI.RUN5 ENG 4 / ROM 4 0.2761 0.2778 0.2736 0.2807 0.2655
</table>
<listItem confidence="0.788780833333333">
• These decreases do not translate to a large im-
provement in the end-to-end task of producing
many-to-many alignments with a balanced pre-
cision and recall. We had a very small decrease
of 0.002 AER using the “refined” heuristic.
• The many-to-many alignments produced using
</listItem>
<bodyText confidence="0.83254">
“union” and the 1-to-1 alignments produced us-
ing “intersection” were also improved.
• It may be a problem that we trained p0 using
likelihood (it is in submodel 3) rather than op-
timizing p0 discriminatively as we did for the
baseline.
</bodyText>
<sectionHeader confidence="0.976266" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<listItem confidence="0.8032870625">
• Considering multiple stemming possibilities
for each word seems important.
• Alternating between increasing likelihood and
decreasing error rate is a useful training ap-
proach which can be used for many problems.
• Our model and training method improve upon a
strong baseline for producing 1-to-many align-
ments.
• Our model and training method can be used
with the “intersection” heuristic to produce
higher quality 1-to-1 alignments
• Models which can directly model many-to-
many alignments and do not require heuristic
symmetrization are needed to produce higher
quality many-to-many alignments. Our train-
ing method can be used to train them.
</listItem>
<sectionHeader confidence="0.998901" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.7798335">
This work was supported by DARPA-ITO grant
NN66001-00-1-9814 and NSF grant IIS-0326276.
</bodyText>
<sectionHeader confidence="0.997769" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721419354839">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computational Lin-
guistics, 19(2):263–311.
Mauro Cettolo and Marcello Federico. 2004. Minimum er-
ror training of log-linear translation models. In Proc. of the
International Workshop on Spoken Language Translation,
pages 103–106, Kyoto, Japan.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada.
2003. Reducing parameter space for word alignment. In
HLT-NAACL 2003 Workshop on Building and Using Paral-
lel Texts: Data Driven Machine Translation and Beyond, Ed-
monton, Alberta, July.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL),
pages 160–167, Sapporo, Japan, July.
M. F. Porter. 1997. An algorithm for suffix stripping. In Read-
ings in information retrieval, pages 313–316, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Monica Rogati, Scott McCarley, and Yiming Yang. 2003. Un-
supervised learning of arabic stemming using a parallel cor-
pus. In Proc. of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), Sapporo, Japan, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ’96: The 16th Int. Conf. on Computational Lin-
guistics, pages 836–841, Copenhagen, Denmark, August.
</reference>
<page confidence="0.99955">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820961">
<title confidence="0.998358">ISI’s Participation in the Romanian-English Alignment Task</title>
<author confidence="0.995955">Alexander</author>
<affiliation confidence="0.9994155">Information Sciences University of Southern</affiliation>
<address confidence="0.994641">4676 Admiralty Way, Suite</address>
<author confidence="0.932591">Marina del Rey</author>
<author confidence="0.932591">CA</author>
<email confidence="0.999535">fraser@isi.edu</email>
<author confidence="0.958765">Daniel</author>
<affiliation confidence="0.999819">Information Sciences University of Southern</affiliation>
<address confidence="0.994503">4676 Admiralty Way, Suite</address>
<author confidence="0.963225">Marina del Rey</author>
<author confidence="0.963225">CA</author>
<email confidence="0.997904">marcu@isi.edu</email>
<abstract confidence="0.997750833333333">We discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="966" citStr="Brown et al., 1993" startWordPosition="138" endWordPosition="141">292-6601 marcu@isi.edu Abstract We discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate. 1 Introduction ISI participated in the WPT05 Romanian-English word alignment task. The system used for baseline experiments is two runs of IBM Model 4 (Brown et al., 1993) in the GIZA++ (Och and Ney, 2003) implementation, which includes smoothing extensions to Model 4. For symmetrization, we found that Och and Ney’s “refined” technique described in (Och and Ney, 2003) produced the best AER for this data set under all experimental conditions. We experimented with a statistical model for inducing a stemmer cross-lingually, but found that the best performance was obtained by simply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating t</context>
<context position="3670" citStr="Brown et al., 1993" startWordPosition="587" endWordPosition="590">as evaluating the two final alignments directly. We tried to have a strong baseline. GIZA++ has many free parameters which can not be estimated using Maximum Likelihood training. We did not use Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91–94, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 the defaults, but instead used settings which produce good AER results on French/English bitext. We also optimized p0 on the 2003 test set (using AER), rather than using likelihood training. Turning off the extensions to GIZA++ and training p0 as in (Brown et al., 1993) produces a substantial increase in AER. 3 Vocabulary Size Reduction Romanian is a Romance language which has a system of suffixes for inflection which is richer than English. Given the small amount of training data, we decided that vocabulary size reduction was desirable. As a baseline for vocabulary reduction, we tried reducing words to prefixes of varying sizes for both English and Romanian after lowercasing the corpora. We also tried Porter stemming (Porter, 1997) for English. (Rogati et al., 2003) extended Model 1 with an additional hidden variable to represent the split points in Arabic </context>
<context position="8880" citStr="Brown et al., 1993" startWordPosition="1470" endWordPosition="1473">anian the target language. The log-linear alignment model is specified by equation 3. The model assigns non-zero probabilities only to 1-to-many alignments, like Model 4. (Cettolo and Federico, 2004) used a log-linear model trained using error minimization for the translation task, 3 of the submodels were taken from Model 4 in a similar way to our first 5 submodels. exp(Em λmhm(f, a, e)) pλ(a, f|e) = (3) E f e a exp(Em λmhm(f, a, e)) Given λ, the alignment search problem is to find the alignment a of highest probability according to equation 3. We solve this using the local search defined in (Brown et al., 1993). We set λ as follows. Given a sequence A of alignments we can calculate an error function, E(A). For these experiments average sentence AER was used. We wish to minimize this error function, so we select λ accordingly: �argmin λ a� Maximizing performance for all of the weights at once is not computationally tractable, but (Och, 2003) has described an efficient one-dimensional search for a similar problem. We search over each λm (holding the others constant) using this technique to find the best λm to update and the best value to update it to. We repeat the process until no further gain can be</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Marcello Federico</author>
</authors>
<title>Minimum error training of log-linear translation models.</title>
<date>2004</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<pages>103--106</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="8460" citStr="Cettolo and Federico, 2004" startWordPosition="1390" endWordPosition="1393">FROM ENGLISH WORD 6 TTABLE ESTIMATED FROM INTERSECTION OF TWO STARTING ALIGNMENTS FOR THIS ITERATION 7 TRANSLATION TABLE FROM ENGLISH TO ROMANIAN MODEL 1 ITERATION 5 8 TRANSLATION TABLE FROM ROMANIAN TO ENGLISH MODEL 1 ITERATION 5 9 BACKOFF FERTILITY (FERTILITY ESTIMATED OVER ALL ENGLISH WORDS) 10 ZERO FERTILITY ENGLISH WORD PENALTY 11 NON-ZERO FERTILITY ENGLISH WORD PENALTY consider English to be the source language and Romanian the target language. The log-linear alignment model is specified by equation 3. The model assigns non-zero probabilities only to 1-to-many alignments, like Model 4. (Cettolo and Federico, 2004) used a log-linear model trained using error minimization for the translation task, 3 of the submodels were taken from Model 4 in a similar way to our first 5 submodels. exp(Em λmhm(f, a, e)) pλ(a, f|e) = (3) E f e a exp(Em λmhm(f, a, e)) Given λ, the alignment search problem is to find the alignment a of highest probability according to equation 3. We solve this using the local search defined in (Brown et al., 1993). We set λ as follows. Given a sequence A of alignments we can calculate an error function, E(A). For these experiments average sentence AER was used. We wish to minimize this erro</context>
</contexts>
<marker>Cettolo, Federico, 2004</marker>
<rawString>Mauro Cettolo and Marcello Federico. 2004. Minimum error training of log-linear translation models. In Proc. of the International Workshop on Spoken Language Translation, pages 103–106, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herve Dejean</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Kenji Yamada</author>
</authors>
<title>Reducing parameter space for word alignment.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<location>Edmonton, Alberta,</location>
<contexts>
<context position="10417" citStr="Dejean et al., 2003" startWordPosition="1735" endWordPosition="1738">ind a setting for λ that reduces AER on discriminative training set (new D-step) We use the first 148 sentences of the 2003 test set for the discriminative training set. 10 settings for λ are found, the hypothesis list is augmented using the results of 10 searches using these settings, and then another 10 settings for λ are found. We then select the best λ. The discriminative training regimen is otherwise similar to (Och, 2003). 5 Experiments Table 2 provides a comparison of our baseline systems using the “refined” symmetrization metric with the best limited resources track system from WPT03 (Dejean et al., 2003) on the 2003 test set. The best results are obtained by stemming both English and Romanian words to the first four letters, as described in section 2. Table 3 provides details on our shared task submission. RUN1 is the word-based baseline system. RUN2 is the stem-based baseline system. RUN4 uses only the first 6 submodels, while RUN5 uses all 11 submodels. RUN3 had errors in processing, so we omit it. Results: • Our new 1-to-many alignment model and training method are successful, producing decreases of 0.03 AER when the source is Romanian, and 0.01 AER when the source is English. E(�a)δ(�a, (</context>
</contexts>
<marker>Dejean, Gaussier, Goutte, Yamada, 2003</marker>
<rawString>Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada. 2003. Reducing parameter space for word alignment. In HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, Edmonton, Alberta, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1000" citStr="Och and Ney, 2003" startWordPosition="145" endWordPosition="148">discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate. 1 Introduction ISI participated in the WPT05 Romanian-English word alignment task. The system used for baseline experiments is two runs of IBM Model 4 (Brown et al., 1993) in the GIZA++ (Och and Ney, 2003) implementation, which includes smoothing extensions to Model 4. For symmetrization, we found that Och and Ney’s “refined” technique described in (Och and Ney, 2003) produced the best AER for this data set under all experimental conditions. We experimented with a statistical model for inducing a stemmer cross-lingually, but found that the best performance was obtained by simply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating the maximization of likelihood and </context>
<context position="2996" citStr="Och and Ney, 2003" startWordPosition="478" endWordPosition="481">alignment of e to f. We use the term “Viterbi alignment” to denote the most probable alignment we can find, rather than the true Viterbi alignment. 2 Baseline To train our systems, Model 4 was trained two times, first using Romanian as the source language and then using English as the source language. For each training, we ran 5 iterations of Model 1, 5 iterations of the HMM model and 3 iterations of Model 4. For the distortion calculations of Model 4, we removed the dependencies on Romanian and English word classes. We applied the “union”, “intersection” and “refined” symmetrization metrics (Och and Ney, 2003) to the final alignments output from training, as well as evaluating the two final alignments directly. We tried to have a strong baseline. GIZA++ has many free parameters which can not be estimated using Maximum Likelihood training. We did not use Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91–94, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 the defaults, but instead used settings which produce good AER results on French/English bitext. We also optimized p0 on the 2003 test set (using AER), rather than using likelihood training. Turni</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="9216" citStr="Och, 2003" startWordPosition="1531" endWordPosition="1532"> our first 5 submodels. exp(Em λmhm(f, a, e)) pλ(a, f|e) = (3) E f e a exp(Em λmhm(f, a, e)) Given λ, the alignment search problem is to find the alignment a of highest probability according to equation 3. We solve this using the local search defined in (Brown et al., 1993). We set λ as follows. Given a sequence A of alignments we can calculate an error function, E(A). For these experiments average sentence AER was used. We wish to minimize this error function, so we select λ accordingly: �argmin λ a� Maximizing performance for all of the weights at once is not computationally tractable, but (Och, 2003) has described an efficient one-dimensional search for a similar problem. We search over each λm (holding the others constant) using this technique to find the best λm to update and the best value to update it to. We repeat the process until no further gain can be found. Our new training method is: REPEAT • Start with submodels and lambda from previous iteration • Find Viterbi alignments on entire training corpus using new model (similar to E-step of Model 4 training) • Reestimate submodel parameters from Viterbi alignments (similar to M-step of Model 4 Viterbi training) • Find a setting for λ</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1997</date>
<booktitle>In Readings in information retrieval,</booktitle>
<pages>313--316</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4142" citStr="Porter, 1997" startWordPosition="665" endWordPosition="666"> test set (using AER), rather than using likelihood training. Turning off the extensions to GIZA++ and training p0 as in (Brown et al., 1993) produces a substantial increase in AER. 3 Vocabulary Size Reduction Romanian is a Romance language which has a system of suffixes for inflection which is richer than English. Given the small amount of training data, we decided that vocabulary size reduction was desirable. As a baseline for vocabulary reduction, we tried reducing words to prefixes of varying sizes for both English and Romanian after lowercasing the corpora. We also tried Porter stemming (Porter, 1997) for English. (Rogati et al., 2003) extended Model 1 with an additional hidden variable to represent the split points in Arabic between the prefix, the stem and the suffix to generate a stemming for use in Cross-Lingual Information Retrieval. As in (Rogati et al., 2003), we can find the most probable stemming given the model, apply this stemming, and retrain our word alignment system. However, we can also use the modified model directly to find the best word alignment without converting the text to its stemmed form. We introduce a variable rj for the Romanian stem and a variable sj for the Rom</context>
</contexts>
<marker>Porter, 1997</marker>
<rawString>M. F. Porter. 1997. An algorithm for suffix stripping. In Readings in information retrieval, pages 313–316, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Rogati</author>
<author>Scott McCarley</author>
<author>Yiming Yang</author>
</authors>
<title>Unsupervised learning of arabic stemming using a parallel corpus.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="4177" citStr="Rogati et al., 2003" startWordPosition="669" endWordPosition="672">r than using likelihood training. Turning off the extensions to GIZA++ and training p0 as in (Brown et al., 1993) produces a substantial increase in AER. 3 Vocabulary Size Reduction Romanian is a Romance language which has a system of suffixes for inflection which is richer than English. Given the small amount of training data, we decided that vocabulary size reduction was desirable. As a baseline for vocabulary reduction, we tried reducing words to prefixes of varying sizes for both English and Romanian after lowercasing the corpora. We also tried Porter stemming (Porter, 1997) for English. (Rogati et al., 2003) extended Model 1 with an additional hidden variable to represent the split points in Arabic between the prefix, the stem and the suffix to generate a stemming for use in Cross-Lingual Information Retrieval. As in (Rogati et al., 2003), we can find the most probable stemming given the model, apply this stemming, and retrain our word alignment system. However, we can also use the modified model directly to find the best word alignment without converting the text to its stemmed form. We introduce a variable rj for the Romanian stem and a variable sj for the Romanian suffix (which when concatenat</context>
<context position="5664" citStr="Rogati et al., 2003" startWordPosition="927" endWordPosition="930">ing possibilities are simply every possible split point where the stem is at least one character (this includes the null suffix). p(fj, aj|e) _ � p(rj,z, sj,z, aj|e) (1) z If the assumption is made that the stem and the suffix are generated independently from e, we can assume conditional independence. p(fj, aj|e) _ � p(rj,z, aj|e)p(sj,z, aj|e) (2) z We performed two sets of experiments, one set where the English was stemmed using the Porter stemmer and one set where each English word was stemmed down to its first four characters. We tried the best performing scoring heuristic for Arabic from (Rogati et al., 2003) where p(sj,z, aj|e) is modeled using the heuristic p(sj,z|lj) where sj,z is the Romanian suffix, and lj is the last letter of the Romanian word fj; these adjustments are updated during EM training. We also tried several other approximations of p(sj,z, aj|e) with and without updates in EM training. We were unable to produce better results and elected to use the baseline vocabulary reduction technique for the shared task. 4 New Model and Training Algorithm Our motivation for a new model and a new training approach which combines likelihood maximization with error rate minimization is threefold:</context>
</contexts>
<marker>Rogati, McCarley, Yang, 2003</marker>
<rawString>Monica Rogati, Scott McCarley, and Yiming Yang. 2003. Unsupervised learning of arabic stemming using a parallel corpus. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="1994" citStr="Vogel et al., 1996" startWordPosition="309" endWordPosition="312">imply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating the maximization of likelihood and minimization of the alignment error rate. For these experiments, we have implemented 91 an alignment package for IBM Model 4 using a hillclimbing search and Viterbi training as described in (Brown et al., 1993), and extended this to use new submodels. The starting point is the final alignment generated using GIZA++’s implementation of IBM Model 1 and the Aachen HMM model (Vogel et al., 1996). Paper organization: Section 2 is on the baseline, Section 3 discusses vocabulary reduction, Section 4 introduces our new model and training method, Section 5 describes experiments, Section 6 concludes. We use the following notation: e refers to an English sentence composed of English words labeled ez. f refers to a Romanian sentence composed of Romanian words labeled fj. a is an alignment of e to f. We use the term “Viterbi alignment” to denote the most probable alignment we can find, rather than the true Viterbi alignment. 2 Baseline To train our systems, Model 4 was trained two times, firs</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>