<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000451">
<title confidence="0.9964925">
Exploring Miscommunication and Collaborative Behaviour in Hu-
man-Robot Interaction
</title>
<author confidence="0.987527">
Theodora Koulouri
</author>
<affiliation confidence="0.977928">
Department of Information Systems and
</affiliation>
<author confidence="0.310001">
Computing
</author>
<affiliation confidence="0.824881">
Brunel University
</affiliation>
<address confidence="0.86283">
Middlesex UB8 3PH
</address>
<email confidence="0.99792">
theodora.koulouri@brunel.ac.uk
</email>
<author confidence="0.979365">
Stanislao Lauria
</author>
<affiliation confidence="0.980783">
Department of Information Systems
and Computing
Brunel University
</affiliation>
<address confidence="0.92046">
Middlesex UB8 3PH
</address>
<email confidence="0.999161">
stasha.lauria@brunel.ac.uk
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998525625">
This paper presents the first step in de-
signing a speech-enabled robot that is ca-
pable of natural management of mis-
communication. It describes the methods
and results of two WOz studies, in which
dyads of naïve participants interacted in a
collaborative task. The first WOz study
explored human miscommunication
management. The second study investi-
gated how shared visual space and moni-
toring shape the processes of feedback
and communication in task-oriented inte-
ractions. The results provide insights for
the development of human-inspired and
robust natural language interfaces in ro-
bots.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9978603">
Robots are now escaping laboratory and indus-
trial environments and moving into our homes
and offices. Research activities have focused on
offering richer and more intuitive interfaces,
leading to the development of several practical
systems with Natural Language Interfaces
(NLIs). However, there are numerous open chal-
lenges arising from the nature of the medium
itself as well as the unique characteristics of
Human-Robot Interaction (HRI).
</bodyText>
<subsectionHeader confidence="0.7367315">
1.1 Miscommunication in Human-Robot
Interaction
</subsectionHeader>
<bodyText confidence="0.999932619047619">
HRI involves embodied interaction, in which
humans and robots coordinate their actions shar-
ing time and space. As most speech-enabled ro-
bots remain in the labs, people are generally un-
aware of what robots can understand and do re-
sulting in utterances that are out of the functional
and linguistic domain of the robot. Physical co-
presence will lead people to make strong but
misplaced assumptions of mutual knowledge
(Clark, 1996), increasing the use of underspeci-
fied referents and deictic expressions. Robots
operate in and manipulate the same environment
as humans, so failure to prevent and rectify errors
has potentially severe consequences. Finally,
these issues are aggravated by unresolved chal-
lenges with automatic speech recognition (ASR)
technologies. In conclusion, miscommunication
in HRI grows in scope, frequency and costs, im-
pelling researchers to acknowledge the necessity
to integrate miscommunication in the design
process of speech-enabled robots.
</bodyText>
<subsectionHeader confidence="0.995014">
1.2 Aims of study
</subsectionHeader>
<bodyText confidence="0.999826307692308">
The goal of this study is two-fold; first, to incor-
porate “natural” and robust miscommunication
management mechanisms (namely, prevention
and repair) into a mobile personal robot, which is
capable of learning by means of natural language
instruction (Lauria et al., 2001). Secondly, it
aims to offer some insights that are relevant for
the development of NLIs in HRI in general. This
research is largely motivated by models of hu-
man communication. It is situated within the lan-
guage-as-action tradition and its approach is to
explore and build upon how humans manage
miscommunication.
</bodyText>
<sectionHeader confidence="0.986703" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999981125">
We designed and performed two rounds of Wiz-
ard of Oz (WOz) simulations. Given that the
general aim of the study is to determine how ro-
bots should initiate repair and provide feedback
in collaborative tasks, the simulations departed
from the typical WOz methodology in that the
wizards were also naive participants. The domain
of the task is navigation. In particular, the user
</bodyText>
<note confidence="0.709885">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 111–119,
</note>
<affiliation confidence="0.662782">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999202">
111
</page>
<bodyText confidence="0.99984">
guided the robot to six designated locations in a
simulated town. The user had full access to the
map whereas the wizard could only see the sur-
rounding area of the robot. Thus, the wizard re-
lied on the user’s instructions on how to reach
the destination. In this section we outline the aim
and approach of each WOz study, the materials
used and the experimental procedure. Sections 4
and 5 focus on each study individually and their
results.
</bodyText>
<subsectionHeader confidence="0.984079">
2.1 The first WOz study
</subsectionHeader>
<bodyText confidence="0.994750714285714">
This study is a continuation of previous work by
the authors (Koulouri and Lauria, 2009). In that
study, the communicative resources of the wizard
were incrementally restricted, from “normal”
dialogue capabilities towards the capabilities of a
dialogue system, in three experimental condi-
tions:
</bodyText>
<listItem confidence="0.952937181818182">
• The wizard simulates a super-intelligent
robot capable of using unconstrained,
natural language with the user (henceforth,
Unconstrained Condition).
• The wizard can select from a list of de-
fault responses but can also ask for clarifi-
cation or provide task-related information
(henceforth, Semi-Constrained condition).
• The wizard is restricted to choose from a
limited set of canned responses similar to
a typical spoken dialogue system (SDS).
</listItem>
<bodyText confidence="0.9987835">
The current study investigates the first two con-
ditions and presents new findings.
</bodyText>
<subsectionHeader confidence="0.99869">
2.2 The second WOz study
</subsectionHeader>
<bodyText confidence="0.999962666666667">
The second round of WOz experiments explored
the effects of monitoring and shared visual in-
formation on the dialogue.
</bodyText>
<subsectionHeader confidence="0.999659">
2.3 Set-up
</subsectionHeader>
<bodyText confidence="0.997506789473684">
A custom Java-based system was developed and
was designed to simulate the existing prototype
(the mobile robot). The system consisted of two
applications which sent and received coordinates
and dialogue and were connected using the
TCP/IP protocol over a LAN. The system kept a
log of the interaction and the robot’s coordinates.
The user’s interface displayed the full map of
the town (Figure 1). The dialogue box was below
the map. Similar to an instant messaging applica-
tion, the user could type his/her messages and see
the robot’s responses appearing on the lower part
of the box. In the first WOz study, the user’s in-
terface included a small “monitor” on the upper
right corner of the screen that displayed the cur-
rent surrounding area of the robot, but not the
robot itself. Then, for the purposes of the second
study, this feature was removed (see Figure 1 in
Appendix A).
</bodyText>
<figureCaption confidence="0.998732">
Figure 1. The user’s interface.
</figureCaption>
<bodyText confidence="0.980702">
The wizard’s interface was modified accord-
ing to the two experimental conditions. For both
conditions, the wizard could only see a fraction
of the map- the area around the robot’s current
position. The robot was operated by the wizard
using the arrow keys on the keyboard. The dialo-
gue box of the wizard displayed the most recent
messages of both participants as well as a history
of the user’s messages. The buttons on the right
side of the screen simulated the actual robot’s
ability to remember previous routes: the wizard
clicked on the button that corresponded to a
known route and the robot automatically ex-
ecuted. In the interface for the Unconstrained
condition, the wizard could freely type and send
messages (Figure 2).
</bodyText>
<figureCaption confidence="0.8828425">
Figure 2. The wizard’s interface in the Uncon-
strained condition.
</figureCaption>
<bodyText confidence="0.9999248">
In the version for the Semi-Constrained condi-
tion, the wizard could interact with the user in
two ways: first, they could click on the buttons,
situated on the upper part of the dialogue box, to
automatically send the canned responses, “Hel-
</bodyText>
<page confidence="0.997373">
112
</page>
<bodyText confidence="0.996443857142857">
lo”, “Goodbye”, “Yes”, “No”, “Ok” and the
problem-signalling responses, “What?”, “I don’t
understand” and “I cannot do that”. The second
way was to click on the “Robot Asks Question”
and “Robot Gives Info” buttons which allowed
the wizard to type his/her own responses (see
Figure 2 in Appendix A).
</bodyText>
<subsectionHeader confidence="0.994149">
2.4 Procedure
</subsectionHeader>
<bodyText confidence="0.999930148148148">
A total of 32 participants were recruited, 16 users
and 16 wizards. The participants were randomly
assigned to the studies, experimental conditions
and to the roles of wizard or user. The pairs were
seated in different rooms equipped with a desk-
top PC. The wizards were given a short demon-
stration and a trial period to familiarise with the
operation of the system and were also informed
about whether the users would be able to monitor
them. The users were told that they would inte-
ract with a robot via a computer interface; this
robot was very fluent in understanding spatial
language and could give appropriate responses, it
could learn routes but had limited vision. The
users were asked to begin each task whenever
they felt ready by clicking on the links on their
computer screen, start the interaction with “Hel-
lo”, which opened the wizard’s application, and
end it with “Goodbye” which closed both appli-
cations. The participants received verbal and
written descriptions of the experiment. They
were not given any specific guidelines on how to
interact or what routes to take. However, the us-
ers were asked not to use directions such as
“north”, “south”, “up”, “down” etc. The wizards
were also advised not to try to “sound like a ro-
bot”, but respond normally.
</bodyText>
<sectionHeader confidence="0.790576" genericHeader="method">
3 Data analysis
</sectionHeader>
<bodyText confidence="0.99980925">
The dialogues collected were annotated on the
basis of dialogue acts (DAs), miscommunication,
and task status. An example of an annotated di-
alogue is shown in Table 1.
</bodyText>
<table confidence="0.998659129032258">
ID (x,y@TIME) T.S. MISC HCRC TAG
MESSAGE
U1 (899,445@10:44:34) instruct
go straight on.
R2 (880,286@10:44:59) Rej explain Imp
There is a wall straight on.
U3 (884,286@10:45:28) acknowledge/
ok turn left here then take the instruct
third right
R4 (352,216@10:45:58) Non check Imp
Before the bridge?
U5 (351,68@10:46:17) WE Corr explain
this is the wrong place
R6 (351,68@10:46:39) acknowledge/
Wrong place. Should Robot go query-yn
back?
U7 (351,68@10:46:55) instruct
go forward and at the cros-
sroads keep going forward and
the tube is at the end of the
road
R8 (351,0@10:47:14) WE explain
Out of bounds.
R9 (351,608@10:47:47) query-w
Where to go?
U10 (364,608@10:48:12) instruct
the tube is in front of you
R11 (402,547@10:48:23) BOT query-yn
Is it this one?
U12 (402,547@10:49:7) SUC reply-y
yes it is.
</table>
<tableCaption confidence="0.991076">
Table 1. Example of an annotated dialogue. ID
</tableCaption>
<bodyText confidence="0.4998965">
denotes the speaker (User or Robot), T.S. stands
for task status and MISC for miscommunication.
</bodyText>
<subsectionHeader confidence="0.999869">
3.1 Annotation of dialogue acts
</subsectionHeader>
<bodyText confidence="0.9999919">
The DAs in the corpus were annotated following
the HCRC coding scheme (Carletta et al., 1996).
Motivated by Skantze (2005), the last column in
Table 1 contains information on the explicitness
of the response. This feature was only relevant
for repair initiations by the wizards. For instance,
responses like “What?” and the the ones in Table
3 were considered to be explicit (EX) signals of
miscommunication, whereas lines 2 and 4 in the
dialogue above were labelled as implicit (IMP).
</bodyText>
<subsectionHeader confidence="0.999969">
3.2 Annotation of task execution status
</subsectionHeader>
<bodyText confidence="0.999986461538462">
The coordinates (x,y) of the robot’s position re-
corded for every exchanged message were placed
on the map of the town (of dimensions 1024x600
pixels) allowing the analysts to retrace the
movements of the robot. Wrong executions (WE)
were determined by juxtaposing the user’s in-
struction with the robot’s execution, as indicated
by the coordinates. Back-on-Track (BOT) was
tagged when the first user instruction after a
wrong execution was executed correctly. Finally,
task success (SUC) was labelled when the robot
reached the destination and it was confirmed by
the user.
</bodyText>
<subsectionHeader confidence="0.999965">
3.3 Annotation of miscommunication
</subsectionHeader>
<bodyText confidence="0.9999642">
The annotation of instances of miscommunica-
tion in the dialogues is based on the definitions
given by Hirst et al. (1994). Miscommunication
includes three categories of problems: misunder-
standings, non-understandings and misconcep-
tions. First, misunderstandings occur when the
hearer obtains an interpretation which is not
aligned to what the speaker intended him/her to
obtain. In this study, without attempting to unveil
the intention of the user, misunderstandings were
</bodyText>
<page confidence="0.995647">
113
</page>
<bodyText confidence="0.99999305">
tagged when the user (who was monitoring the
understanding) signalled a wrong execution (see
line 5 in Table 1). These correction tags (Corr)
did not always coincide with wrong execution
tags, but were used when the user became aware
of the error (after receiving visual or verbal in-
formation). Following the same definition, mis-
understandings were also tagged as rejections
(tag: Rej) when the wizard expressed inability to
execute the instruction (for instance, given the
robot’s current location, as shown in line 2 in the
dialogue), although he/she was able to interpret
it. Secondly, non-understandings (tag: Non, line
4) occurred when the wizards obtained no inter-
pretation at all or too many. Non-understandings
also included cases in which wizards were uncer-
tain about their interpretation (as suggested by
Gabsdil, 2003). Lastly, misconceptions happen
when the beliefs of the interlocutors clash, and
are outside the scope of this study.
</bodyText>
<sectionHeader confidence="0.984387" genericHeader="method">
4 First WOz study
</sectionHeader>
<bodyText confidence="0.999931291666667">
Skantze (2005) and Williams and Young (2004)
performed variations of WOz studies to explore
how humans handle ASR errors, using a real or
simulated speech recogniser. They discovered
that even after highly inaccurate recognition out-
put, the participants rarely signalled non-
understanding explicitly. Accordingly, the expe-
rimental hypothesis of the present study is that
wizards in both conditions will not choose expli-
cit responses to signal miscommunication (such
as “I don’t understand” or “What?”) but res-
ponses that contribute with information.
ASR is a major source of errors in SDS. But as
miscommunication is ubiquitous in interaction,
there are many other sources of ambiguity that
give rise to problematic understanding. Thus, for
the current purposes of this work, it was decided
that ASR would have an overwhelming effect on
the interaction that might prevent the observation
of other interesting dialogue phenomena.
This section describes further work on the Un-
constrained and Semi-Constrained conditions
(see Section 2.1). Twenty participants were re-
cruited and randomly allocated to each condition.
</bodyText>
<sectionHeader confidence="0.699413" genericHeader="method">
4.1 Results
</sectionHeader>
<bodyText confidence="0.9996367">
Analysis of the dialogues of the Unconstrained
condition reinforced previous findings and con-
firmed the experimental hypothesis. In particular,
wizards never used explicit repairs, but preferred
to describe their location, request clarification
and further instructions. Integrating finer classi-
fication of clarification requests (CRs) and the
original dialogue act tagging, the DAs used by
the wizards to signal non-understandings and
rejections were categorised as shown in Table 2.
</bodyText>
<table confidence="0.7986557">
Dialogue Act Explanation
Explain The wizard gives description of robot’s location.
E.g., “I crossed the bridge.”, “I am at a cross-
road”.
Check This category covers CRs. The corpus contained
two types of CRs: first, task-level reformulations
(as in line 4 in Table 1), which reformulate the
utterance on the basis of its effects on the task,
showing the wizard’s subjective understanding
(Gabsdil, 2003). Second, alternative CRs which
occur when the wizard gives two alternative
interpretations, trying to resolve referential
ambiguity. For instance, “back to the bridge or
to the factory”, to resolve “go back to last loca-
tion”.
Query-w The wizard asks for further instructions. E.g.,
“Please give me further instructions.”
Explain+Query- A combo of actions; the wizard provides infor-
w mation on location and asks for further instruc-
tions. E.g., “crossroads, now where?”
</table>
<tableCaption confidence="0.992719">
Table 2.Wizard DAs after miscommunication.
</tableCaption>
<bodyText confidence="0.742050217391305">
Figure 3 illustrates the distribution of these re-
sponses to signal non-understandings and rejec-
tions (columns labelled “Uncons-NON” and
“Uncons-REJ”, respectively). Evidently, there is
a much greater variety of CRs than the two CR
types reported here, as described in the work of
Purver (2006) and Schlangen (2004). However,
for a navigation task and having excluded ASR
errors, problems occurred mainly in the meaning
recognition level (explained below) and aimed
for reference resolution.
Figure 3. Use of strategies to signal non-
understandings or rejections, for either condition.
In conclusion, wizards in the Unconstrained
condition did not directly signal problems in un-
derstanding but, instead, they attempted to ad-
vance the dialogue by providing task-related in-
formation in either the form of CRs or simple
statements. The study contributes to the findings
presented in Skantze (2005) and Williams and
Young (2004) in that it demonstrates the use of
similar strategies to deal with different sources of
problems.
</bodyText>
<page confidence="0.994591">
114
</page>
<bodyText confidence="0.999718615384615">
In the Semi-Constrained condition, a degree of
restrain and control over the error handling ca-
pacity of the wizards was introduced. In particu-
lar, the wizards could explicitly signal communi-
cation problems in the utterance, meaning and
action level using three predefined responses.
This is inspired by the models of Clark (1996)
and Allwood (1995), according to which, mis-
communication can occur in any of these levels
and people select repair initiations that point to
the source of the problem. The model (adapted
from Mills and Healey, 2006) and the responses
are schematically shown in Table 3 below.
</bodyText>
<table confidence="0.9975898">
Levels of Communication Wizard Responses
Level 1 Securing Attention -
Level 2 Utterance Recognition “What?”
Level 3 Meaning Recognition “Sorry, I don’t understand.”
Level 4 Action Recognition “I cannot do that.”
</table>
<tableCaption confidence="0.998973">
Table 3. Levels of communication.
</tableCaption>
<bodyText confidence="0.999568777777778">
Moreover, based on the classification of the
wizard’s error handling strategies in the Uncon-
strained condition (Table 2), we collapsed the
observed strategies in two categories of re-
sponses which resulted in adding two more error
handling buttons; namely, the button denoted as
“Robot Asks Question” corresponded to the
“Check” and “Query-w” strategies. The “Robot
Gives Info” was associated with “Explain”. This
clear labelling of error handling actions pre-
sented to the wizards of the Semi-Constrained
condition aimed to “coerce” them to use the
strategies in a more transparent way. This could
allow us a glimpse to the mechanisms and proc-
esses underlying human miscommunication
management.
Analysis of the dialogues revealed that in the
Semi-Constrained condition wizards employed
both explicit and implicit strategies. Figure 4
shows the distribution of explicit and implicit
responses to signal non-understandings and re-
jections. Figure 3 shows the frequency of each
implicit strategy to signal non-understandings
(Semi-NON) and rejections (Semi-REJ).
The initial prediction was that wizards will not
use explicit signals of problems in the dialogue.
This was contradicted by the results. It can be
argued that the physical presence of the buttons
and the less effort required account for this phe-
nomenon. On the other hand, it is also plausible
to assume that these strategies matched what the
wizards wanted to say. Finally, there were no
significant differences between conditions in
terms of user experience, task success and time
on task (as reported in Koulouri and Lauria,
2009).
</bodyText>
<figureCaption confidence="0.9023975">
Figure 4. Occurrence of implicit and explicit
miscommunication signals (Semi-Constrained).
</figureCaption>
<subsectionHeader confidence="0.963805">
4.2 Discussion and future work
</subsectionHeader>
<bodyText confidence="0.999991757575758">
The findings of this study could be extrapolated
to HRI. Classification of the responses of the
wizards resulted in a limited set of error signal-
ling strategies. Therefore, in the presence of mis-
communication the robot could use the static,
explicit strategies. But these strategies alone are
inadequate (as shown by Koulouri and Lauria,
2009). They need to be supplemented, but not
entirely replaced, with dynamic error handling
strategies; namely, posing relevant questions and
providing descriptions of location. Yet this en-
tails several challenges. Gabsdil (2003) identifies
the complexity of adding clarification requests to
systems with deep semantic processing. With
regard to alternative clarifications, systems
would need to generate two alternative interpre-
tations for one referent. Task-level reformula-
tions would also require the system to have the
capability to identify the effects of all possible
executions of the instruction. As a next step, we
will focus on issues concerning the implementa-
tion of such functionality.
Schlangen (2004) suggests that “general-
purpose” repair initiations, such as “What?”,
which request repetition of the whole utterance,
are more severe for the dialogue compared to
reprise fragments (e.g., “Turn where?”) that ac-
cept part of the utterance. Mills and Healey
(2006) also found that “What’s” were more dis-
ruptive to the dialogue than reprise fragments.
Guided by these insights, our current work looks
at how each error strategy affects the subsequent
unfolding of the dialogue.
</bodyText>
<sectionHeader confidence="0.973606" genericHeader="method">
5 The second WOz study
</sectionHeader>
<bodyText confidence="0.9988958">
Research in human communication has shown
that in task-oriented interactions visual informa-
tion has a great impact on dialogue patterns and
improves performance in the task. In particular,
Gergle at al. (2004), Clark and Krych (2004) and
</bodyText>
<page confidence="0.998028">
115
</page>
<bodyText confidence="0.999861052631579">
Brennan (2005) explored different communica-
tion tasks and compared a condition, in which
visual and verbal information was available, with
a speech-only condition. In their experiments, a
person gave instructions to another participant on
how to complete a task. Their findings seem to
resonate. In terms of time for task completion
and number of words per turn, the interactions in
the visual information condition were more effi-
cient. The physical actions of the person follow-
ing the instructions functioned as confirmations
and substituted for verbal grounding. Regarding
errors, no significant differences were observed
between visual and speech-only conditions. Mo-
tivated by these findings in human-human inte-
raction, the second study aims to identify the dif-
ferences in the processes of communication de-
pending on whether the user can or cannot moni-
tor the actions of the robot.
</bodyText>
<subsectionHeader confidence="0.946282">
5.1 Experimental design
</subsectionHeader>
<bodyText confidence="0.99981875">
The study followed a between-subjects factorial
design. Experiments were performed for four
different conditions, as illustrated in Table 4. The
conditions “Monitor, Unconstrained” and “Moni-
tor, Semi-Constrained” were the same as in the
first study. Five pairs of participants were re-
cruited to each of the Monitor Conditions and
three pairs to each of the No Monitor Conditions.
</bodyText>
<tableCaption confidence="0.980028">
Table 4. The design of the 2nd study.
</tableCaption>
<subsectionHeader confidence="0.862403">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.998086020833333">
The data collection resulted in 96 dialogues, 93
of which were used in the analysis. The data
were analysed using a two-way ANOVA. All
effects that were found to be significant were
verified by T-tests. The efficiency of interaction
was determined using the following measures:
time per task, number of turns, words, miscom-
munication-tagged turns, wrong executions and
task success.
Time per task: The second column of Table 5
displays the average completion time per task in
the four conditions. As expected, a main effect of
the Monitor factor was found (F=4.879, df=1,11,
p&lt;0.05). Namely, when the user could monitor
the robot’s area the routes were completed faster.
The interaction effect between factors was also
marginally significant (F=4.225, df=1,11, p&lt;0.1);
pairs in the No Monitor, Semi-Constrained con-
dition could not compensate for the lack of visual
information and took longer for each task.
Number of turns and words: The aforemen-
tioned studies correlate task efficiency with
number of turns and words. In terms of the mean
number of turns per interaction, no significant
differences were found across the groups. Nev-
ertheless, we measured the number of words
used per task and in accordance with previous
research, we observed that pairs in the No Moni-
tor conditions used more words (F=4.602,
df=1,11, p=0.05). However, it was the wizards
under the No Monitor conditions that had to be
more “talkative” and descriptive (F=10.324,
df=1,11, p&lt;0.01). Figure 5 shows the “word-
possession” rates attributed to wizards in the four
conditions. Moreover, there seems to be a differ-
ence (F=4.397, df=1,11, p=0.05) in the mean
number of words per turn. In particular, when the
wizards’ actions were visible to the users, the
wizards required fewer words per turn. There is
also an interaction effect showing more signifi-
cant differences between the Monitor, Semi-
Constrained condition and the No Monitor,
Semi-Constrained condition (F=5.970, df=1,11,
p&lt;0.05); in the former, wizards managed with
less than 2 words per utterance, taking full ad-
vantage of the luxury of the buttons and the fact
that they were supervised. In the latter, wizards
used more than 6 words per turn.
</bodyText>
<figureCaption confidence="0.920739">
Figure 5. Words used by wizards over total.
</figureCaption>
<bodyText confidence="0.9997148">
Frequency of miscommunication: We meas-
ured the number of turns that were tagged as con-
taining miscommunication. Surprisingly, mis-
communication rates were much lower in the No
Monitor conditions (F=13.316, df=1,11, p&lt;0.01)
and not in the conditions in which the user could
check at all times the actions and understanding
of the robot. The same pattern was found for us-
er-initiated and robot-initiated miscommunica-
tion. The rates of miscommunication are in-
cluded in the third column of Table 5.
Wrong executions: Analysis of number of
wrong executions per task reveals a similar ef-
fect; wrong executions occurred much less fre-
quently when the wizards were not supervised by
</bodyText>
<table confidence="0.6664906">
Unconstrained Semi-Constrained
Monitor Monitor, Uncon- Monitor, Semi-
strained Constrained
No Monitor No Monitor, Uncon- No Monitor, Semi-
strained Constrained
</table>
<page confidence="0.998048">
116
</page>
<bodyText confidence="0.999791428571429">
the users (F=6.046, df=1,11, p&lt;0.05). They
made on average 1 mistake per task, whereas the
average number of wrong executions for the
pairs in the Monitor conditions was 5 (fourth
column in Table 5).
Task success rates: There were no differences
in the number of interrupted or aborted tasks.
</bodyText>
<table confidence="0.999755285714286">
Condition Time per Miscommunication #Wrong
Task Turns/Total Turns Executions
(min) per Task
Mon, Uncons 4.57 8.21% 4.2
Mon, Semi 4.63 8.82% 5.8
No Mon, Uncons 5.67 2.55% 1.0
No Mon, Semi 7.41 1.71% 0.7
</table>
<tableCaption confidence="0.999637">
Table 5. Summary of results (mean values).
</tableCaption>
<subsectionHeader confidence="0.978862">
5.3 Discussion and future work
</subsectionHeader>
<bodyText confidence="0.999992695652174">
These results are consistent with previous re-
search. The conditions in which the user could
see exactly what the robot saw and did resulted
in faster task completion and shorter dialogues.
However, a finding emerged which was not ex-
pected based on the aforementioned studies: in
the conditions in which users could not monitor
the robot’s actions, the wizards were more accu-
rate, leading to low occurrence of wrong execu-
tions and miscommunication (see column 3 and 4
in Table 5). The “least collaborative effort” is
balanced and compromised against the need to
ensure understanding. Thus, wizards provided
rich and timely feedback to the users in order to
compensate for the lack of visual information.
This feedback acted in a proactive way and pre-
vented miscommunication and wrong executions.
In the Monitor conditions, asymmetries in per-
ceived responsibility and knowledge between the
participants could have encouraged wizards to be
less cautious to act. In other words, as the user
had access to the full map and the location of the
wizard, the wizard felt less “obliged” to contri-
bute to the interaction. However, due to the com-
plex nature of the task, unless the wizard could
sufficiently communicate the relevant position of
the robot, the directions of the user would more
likely be incorrect. It could also be assumed that
since visual feedback is instant, the users were
also more inclined to issue commands in a “trial
and error” process. Irrespectively to the underly-
ing motives, these findings show that despite
higher costs in time and word count, linguistic
resources were adequate for completing complex
tasks successfully. The findings also resonate
with the collaborative view of communication.
The wizards adapted their behaviour in response
to variations in the knowledge state of their part-
ners and made up for the lack of visual informa-
tion with rich verbal descriptions of their loca-
tions.
We are currently performing more experi-
ments to balance the data sets of the study and
validate the initial results. Moreover, a fine-
grained analysis of the dialogues is under way
and focuses on the linguistic content of the inte-
ractions. The aim is identical to the first WOz
study, that is, to identify the strategies of the wi-
zards in the presence and absence of visual in-
formation.
These results have important implications for
HRI. As in human collaborative interaction, the
robot’s communicative actions have direct im-
pact on the actions of the users. In real-world
settings, there will be situations in which the us-
ers cannot monitor the robot’s activities or their
information and knowledge are either con-
strained or outdated. Robots that can dynamically
determine and provide appropriate feedback
could help the users avoid serious errors. Never-
theless, this is not a straightforward process; pro-
viding excessive, untimely feedback compromis-
es the “naturalness” and efficiency of the interac-
tion. The amount and placement of feedback
should be decided upon several knowledge
sources, combined in a single criterion that is
adaptive within and between interactions. These
issues are the object of our future work and im-
plementation.
</bodyText>
<sectionHeader confidence="0.9877" genericHeader="method">
6 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999995045454545">
One of the most valuable but complex processes
in the design of a NLI for a robot is enacting a
HRI scenario to obtain naturally-occurring data
which is yet generalisable and relevant for the
future implementation of the system. The present
study recreated a navigation scenario in which
non-experienced users interacted with and taught
a mobile robot. It also simulated two different
setups which corresponded to the realistic situa-
tions of supervised and unsupervised interaction.
The current trend in the fields of linguistics and
robotics is the unified investigation of spatial
language and dialogue (Coventry et al., 2009).
Exploring dialogue-based navigation of a robot,
our study aimed to contribute to this body of re-
search. It can be argued that there were limita-
tions in the simulation as compared to the expe-
rimental testing of a real system and, thus, the
study was primarily explorative. However, it
yielded natural dialogues given that naive “con-
federates” and no dialogue script were used. The
data analysis was more qualitative than quantita-
</bodyText>
<page confidence="0.992758">
117
</page>
<bodyText confidence="0.99998362962963">
tive and followed established methods from pre-
vious research. Finally, the results of the study
matched and extended these findings and pro-
vided useful information for the next version of
the system as well as some insight into the
processes of conversation and social psychology.
The next step in our research is to develop the
dialogue manager of the robot to incorporate the
feedback and miscommunication management
strategies, as observed in the collected data. This
holds the promise for a robust NLI that can han-
dle uncertainties arising from language and the
environment. However, miscommunication in
HRI reaches beyond preventing and repairing
recognition errors. Mills and Healey (2008)
demonstrate that miscommunication does not
inhibit but, on the contrary, it facilitates semantic
coordination. Martinovsky and Traum (2003)
suggest that through miscommunication, people
gain awareness of the state and capabilities of
each other. Miscommunication, thus, is seen as
an opportunity for communication. Under this
light, natural miscommunication management is
not only the end, but also the means to shape and
advance HRI, so that robots are not tools but
partners that play a positive, practical and long-
lasting role in human life.
</bodyText>
<sectionHeader confidence="0.999096" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999766743589744">
Bilyana Martinovsky and David Traum. 2003.The
Error Is the Clue: Breakdown in Human-Machine
Interaction. In Proceedings of the ISCA Workshop
on Error Handling in Dialogue Systems.
Dan Bohus and Alexander I. Rudnicky. 2005. Sorry, I
Didn’t Catch That! – An Investigation of Non-
understanding Errors and Recovery Strategies. In
Proceedings of SIGdial2005. Lisbon, Portugal.
Darren Gergle, Robert E. Kraut and Susan E. Fussell.
2004. Language Efficiency and Visual Technolo-
gy: Minimizing Collaborative Effort with Visual
Information. Journal of Language and Social Psy-
chology, 23(4):491-517. Sage Publications, CA.
David Schlangen. 2004. Causes and Strategies for
Requesting Clarification in dialogue. In Proceed-
ings of the 5th Workshop of the ACL SIG on Dis-
course and Dialogue (SIGdial04), Boston, USA.
Gabriel Skantze. 2005. Exploring Human Error Re-
covery Strategies: Implications for Spoken Dialo-
gue Systems. Speech Communication, 45(3):207-
359.
Graeme Hirst, Susan McRoy, Peter Heeman, Philip
Edmonds, Diane Horton.1994. Repairing Conver-
sational Misunderstandings and Nonunderstand-
ings. Speech Communication 15:213–230.
Gregory Mills and Patrick G. T. Healey. 2008. Nego-
tiation in Dialogue: Mechanisms of Alignment. In
Proceedings of the 8th SIGdial workshop on Dis-
course and Dialogue, Columbus, OH, USA.
Gregory Mills and Patrick G. T. Healey. 2006. Clari-
fying Spatial Descriptions: Local and Global Ef-
fects on Semantic Co-ordination. In Procs. of the
10th Workshop on the Semantics and Pragmatics of
Dialogue.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge, UK.
Herbert H. Clark and Meredyth A. Krych. 2004.
Speaking While Monitoring Addressees for Under-
standing. Journal of Memory and Language,
50:62-81.
Jason D. Williams and Steve Young. 2004. Characte-
rizing Task-Oriented Dialog Using a Simulated
ASR Channel. ICSLP. Jeju, South Korea.
Jean Carletta, Amy Isard, Stephen Isard, Jacqueline
Kowtko, Gwyneth Doherty-Sneddon and Anne H.
Anderson.1996. HCRC Dialogue Structure Coding
Manual(HCRC/TR-82). Human Communication
Research Centre, University of Edinburgh.
Jens Allwood. 1995. An Activity based Approach to
Pragmatics. Gothenburg Papers in Theoretical Lin-
guistics, 76, Göteborg University, Sweden.
Kenny Coventry, Thora Tenbrink and John Bateman,
2009. Spatial Language and Dialogue: Navigating
the Domain. In K. Coventry, T. Tenbrink, and J.
Bateman (Eds.) Spatial Language and Dialogue. 1-
8. Oxford University Press. Oxford, UK.
Malte Gabsdil. 2003. Clarification in Spoken Dialo-
gue Systems. In: Proceedings of 2003 AAAI Spring
Symposium on Natural Language Generation in
Spoken and Written Dialogue, Stanford, USA.
Matthew Purver. 2006. CLARIE: Handling Clarifica-
tion Requests in a Dialogue System. Research on
Language and Computation. 4(2-3):259-288.
Stanislao Lauria, Guido Bugmann, Theocharis Kyria-
cou, Johan Bos and Ewan Klein. 2001. Training
Personal Robots Using Natural Language Instruc-
tion. IEEE Intelligent Systems. 38–45.
Susan E. Brennan . 2005. How Conversation is
Shaped by Visual and Spoken Evidence. In J.
Trueswell &amp; M. Tanenhaus (Eds.) Approaches to
Studying World-situated Language Use: Bridging
the Language-as-product and Language-action
Traditions. 95-129. MIT Press, Cambridge, MA.
Theodora Koulouri and Stanislao Lauria. 2009. A
WOz Framework for Exploring Miscommunication
in HRI, In Procs. of the AISB Symposium on New
Frontiers in Human-Robot Interaction. Edinburgh,
UK.
</reference>
<page confidence="0.99822">
118
</page>
<sectionHeader confidence="0.8443425" genericHeader="method">
Appendix A. Screenshot images of the in-
terface
</sectionHeader>
<figureCaption confidence="0.992885">
Figure 1. The interface of the user without the
monitor (as used in the second WOz study).
Figure 2. The interface of the wizard in the Semi-
Constrained condition.
</figureCaption>
<page confidence="0.996841">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.287154">
<title confidence="0.9982945">Miscommunication and Collaborative Behaviour in man-Robot Interaction</title>
<author confidence="0.984524">Theodora</author>
<affiliation confidence="0.9">Department of Information Systems Brunel</affiliation>
<address confidence="0.944513">Middlesex UB8 3PH</address>
<email confidence="0.994844">theodora.koulouri@brunel.ac.uk</email>
<author confidence="0.625426">Stanislao</author>
<affiliation confidence="0.892587">Department of Information and Brunel</affiliation>
<address confidence="0.866959">Middlesex UB8 3PH</address>
<email confidence="0.99557">stasha.lauria@brunel.ac.uk</email>
<abstract confidence="0.992382470588235">This paper presents the first step in designing a speech-enabled robot that is capable of natural management of miscommunication. It describes the methods and results of two WOz studies, in which dyads of naïve participants interacted in a collaborative task. The first WOz study explored human miscommunication management. The second study investigated how shared visual space and monitoring shape the processes of feedback and communication in task-oriented interactions. The results provide insights for the development of human-inspired and robust natural language interfaces in robots.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bilyana Martinovsky</author>
<author>David Traum</author>
</authors>
<title>Error Is the Clue: Breakdown in Human-Machine Interaction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ISCA Workshop on Error Handling in Dialogue Systems.</booktitle>
<marker>Martinovsky, Traum, 2003</marker>
<rawString>Bilyana Martinovsky and David Traum. 2003.The Error Is the Clue: Breakdown in Human-Machine Interaction. In Proceedings of the ISCA Workshop on Error Handling in Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Sorry, I Didn’t Catch That! – An Investigation of Nonunderstanding Errors and Recovery Strategies.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGdial2005.</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Bohus, Rudnicky, 2005</marker>
<rawString>Dan Bohus and Alexander I. Rudnicky. 2005. Sorry, I Didn’t Catch That! – An Investigation of Nonunderstanding Errors and Recovery Strategies. In Proceedings of SIGdial2005. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Gergle</author>
<author>Robert E Kraut</author>
<author>Susan E Fussell</author>
</authors>
<title>Language Efficiency and Visual Technology: Minimizing Collaborative Effort with Visual Information.</title>
<date>2004</date>
<journal>Journal of Language and Social Psychology,</journal>
<pages>23--4</pages>
<publisher>Sage Publications, CA.</publisher>
<marker>Gergle, Kraut, Fussell, 2004</marker>
<rawString>Darren Gergle, Robert E. Kraut and Susan E. Fussell. 2004. Language Efficiency and Visual Technology: Minimizing Collaborative Effort with Visual Information. Journal of Language and Social Psychology, 23(4):491-517. Sage Publications, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
</authors>
<title>Causes and Strategies for Requesting Clarification in dialogue.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th Workshop of the ACL SIG on Discourse and Dialogue (SIGdial04),</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="15188" citStr="Schlangen (2004)" startWordPosition="2379" endWordPosition="2380"> last location”. Query-w The wizard asks for further instructions. E.g., “Please give me further instructions.” Explain+Query- A combo of actions; the wizard provides inforw mation on location and asks for further instructions. E.g., “crossroads, now where?” Table 2.Wizard DAs after miscommunication. Figure 3 illustrates the distribution of these responses to signal non-understandings and rejections (columns labelled “Uncons-NON” and “Uncons-REJ”, respectively). Evidently, there is a much greater variety of CRs than the two CR types reported here, as described in the work of Purver (2006) and Schlangen (2004). However, for a navigation task and having excluded ASR errors, problems occurred mainly in the meaning recognition level (explained below) and aimed for reference resolution. Figure 3. Use of strategies to signal nonunderstandings or rejections, for either condition. In conclusion, wizards in the Unconstrained condition did not directly signal problems in understanding but, instead, they attempted to advance the dialogue by providing task-related information in either the form of CRs or simple statements. The study contributes to the findings presented in Skantze (2005) and Williams and Youn</context>
<context position="19518" citStr="Schlangen (2004)" startWordPosition="3038" endWordPosition="3039">gies; namely, posing relevant questions and providing descriptions of location. Yet this entails several challenges. Gabsdil (2003) identifies the complexity of adding clarification requests to systems with deep semantic processing. With regard to alternative clarifications, systems would need to generate two alternative interpretations for one referent. Task-level reformulations would also require the system to have the capability to identify the effects of all possible executions of the instruction. As a next step, we will focus on issues concerning the implementation of such functionality. Schlangen (2004) suggests that “generalpurpose” repair initiations, such as “What?”, which request repetition of the whole utterance, are more severe for the dialogue compared to reprise fragments (e.g., “Turn where?”) that accept part of the utterance. Mills and Healey (2006) also found that “What’s” were more disruptive to the dialogue than reprise fragments. Guided by these insights, our current work looks at how each error strategy affects the subsequent unfolding of the dialogue. 5 The second WOz study Research in human communication has shown that in task-oriented interactions visual information has a g</context>
</contexts>
<marker>Schlangen, 2004</marker>
<rawString>David Schlangen. 2004. Causes and Strategies for Requesting Clarification in dialogue. In Proceedings of the 5th Workshop of the ACL SIG on Discourse and Dialogue (SIGdial04), Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
</authors>
<title>Exploring Human Error Recovery Strategies: Implications for Spoken Dialogue Systems. Speech Communication,</title>
<date>2005</date>
<pages>45--3</pages>
<contexts>
<context position="9896" citStr="Skantze (2005)" startWordPosition="1574" endWordPosition="1575">he crossroads keep going forward and the tube is at the end of the road R8 (351,0@10:47:14) WE explain Out of bounds. R9 (351,608@10:47:47) query-w Where to go? U10 (364,608@10:48:12) instruct the tube is in front of you R11 (402,547@10:48:23) BOT query-yn Is it this one? U12 (402,547@10:49:7) SUC reply-y yes it is. Table 1. Example of an annotated dialogue. ID denotes the speaker (User or Robot), T.S. stands for task status and MISC for miscommunication. 3.1 Annotation of dialogue acts The DAs in the corpus were annotated following the HCRC coding scheme (Carletta et al., 1996). Motivated by Skantze (2005), the last column in Table 1 contains information on the explicitness of the response. This feature was only relevant for repair initiations by the wizards. For instance, responses like “What?” and the the ones in Table 3 were considered to be explicit (EX) signals of miscommunication, whereas lines 2 and 4 in the dialogue above were labelled as implicit (IMP). 3.2 Annotation of task execution status The coordinates (x,y) of the robot’s position recorded for every exchanged message were placed on the map of the town (of dimensions 1024x600 pixels) allowing the analysts to retrace the movements</context>
<context position="12364" citStr="Skantze (2005)" startWordPosition="1957" endWordPosition="1958">ejections (tag: Rej) when the wizard expressed inability to execute the instruction (for instance, given the robot’s current location, as shown in line 2 in the dialogue), although he/she was able to interpret it. Secondly, non-understandings (tag: Non, line 4) occurred when the wizards obtained no interpretation at all or too many. Non-understandings also included cases in which wizards were uncertain about their interpretation (as suggested by Gabsdil, 2003). Lastly, misconceptions happen when the beliefs of the interlocutors clash, and are outside the scope of this study. 4 First WOz study Skantze (2005) and Williams and Young (2004) performed variations of WOz studies to explore how humans handle ASR errors, using a real or simulated speech recogniser. They discovered that even after highly inaccurate recognition output, the participants rarely signalled nonunderstanding explicitly. Accordingly, the experimental hypothesis of the present study is that wizards in both conditions will not choose explicit responses to signal miscommunication (such as “I don’t understand” or “What?”) but responses that contribute with information. ASR is a major source of errors in SDS. But as miscommunication i</context>
<context position="15766" citStr="Skantze (2005)" startWordPosition="2466" endWordPosition="2467">f Purver (2006) and Schlangen (2004). However, for a navigation task and having excluded ASR errors, problems occurred mainly in the meaning recognition level (explained below) and aimed for reference resolution. Figure 3. Use of strategies to signal nonunderstandings or rejections, for either condition. In conclusion, wizards in the Unconstrained condition did not directly signal problems in understanding but, instead, they attempted to advance the dialogue by providing task-related information in either the form of CRs or simple statements. The study contributes to the findings presented in Skantze (2005) and Williams and Young (2004) in that it demonstrates the use of similar strategies to deal with different sources of problems. 114 In the Semi-Constrained condition, a degree of restrain and control over the error handling capacity of the wizards was introduced. In particular, the wizards could explicitly signal communication problems in the utterance, meaning and action level using three predefined responses. This is inspired by the models of Clark (1996) and Allwood (1995), according to which, miscommunication can occur in any of these levels and people select repair initiations that point</context>
</contexts>
<marker>Skantze, 2005</marker>
<rawString>Gabriel Skantze. 2005. Exploring Human Error Recovery Strategies: Implications for Spoken Dialogue Systems. Speech Communication, 45(3):207-359.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Graeme Hirst</author>
<author>Susan McRoy</author>
<author>Peter Heeman</author>
<author>Philip Edmonds</author>
</authors>
<title>Diane Horton.1994. Repairing Conversational Misunderstandings and Nonunderstandings. Speech Communication 15:213–230.</title>
<marker>Hirst, McRoy, Heeman, Edmonds, </marker>
<rawString>Graeme Hirst, Susan McRoy, Peter Heeman, Philip Edmonds, Diane Horton.1994. Repairing Conversational Misunderstandings and Nonunderstandings. Speech Communication 15:213–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Mills</author>
<author>Patrick G T Healey</author>
</authors>
<title>Negotiation in Dialogue: Mechanisms of Alignment.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th SIGdial workshop on Discourse and Dialogue,</booktitle>
<location>Columbus, OH, USA.</location>
<marker>Mills, Healey, 2008</marker>
<rawString>Gregory Mills and Patrick G. T. Healey. 2008. Negotiation in Dialogue: Mechanisms of Alignment. In Proceedings of the 8th SIGdial workshop on Discourse and Dialogue, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Mills</author>
<author>Patrick G T Healey</author>
</authors>
<title>Clarifying Spatial Descriptions: Local and Global Effects on Semantic Co-ordination.</title>
<date>2006</date>
<booktitle>In Procs. of the 10th Workshop on the Semantics and Pragmatics of Dialogue.</booktitle>
<contexts>
<context position="16444" citStr="Mills and Healey, 2006" startWordPosition="2574" endWordPosition="2577">the use of similar strategies to deal with different sources of problems. 114 In the Semi-Constrained condition, a degree of restrain and control over the error handling capacity of the wizards was introduced. In particular, the wizards could explicitly signal communication problems in the utterance, meaning and action level using three predefined responses. This is inspired by the models of Clark (1996) and Allwood (1995), according to which, miscommunication can occur in any of these levels and people select repair initiations that point to the source of the problem. The model (adapted from Mills and Healey, 2006) and the responses are schematically shown in Table 3 below. Levels of Communication Wizard Responses Level 1 Securing Attention - Level 2 Utterance Recognition “What?” Level 3 Meaning Recognition “Sorry, I don’t understand.” Level 4 Action Recognition “I cannot do that.” Table 3. Levels of communication. Moreover, based on the classification of the wizard’s error handling strategies in the Unconstrained condition (Table 2), we collapsed the observed strategies in two categories of responses which resulted in adding two more error handling buttons; namely, the button denoted as “Robot Asks Que</context>
<context position="19779" citStr="Mills and Healey (2006)" startWordPosition="3076" endWordPosition="3079">ative clarifications, systems would need to generate two alternative interpretations for one referent. Task-level reformulations would also require the system to have the capability to identify the effects of all possible executions of the instruction. As a next step, we will focus on issues concerning the implementation of such functionality. Schlangen (2004) suggests that “generalpurpose” repair initiations, such as “What?”, which request repetition of the whole utterance, are more severe for the dialogue compared to reprise fragments (e.g., “Turn where?”) that accept part of the utterance. Mills and Healey (2006) also found that “What’s” were more disruptive to the dialogue than reprise fragments. Guided by these insights, our current work looks at how each error strategy affects the subsequent unfolding of the dialogue. 5 The second WOz study Research in human communication has shown that in task-oriented interactions visual information has a great impact on dialogue patterns and improves performance in the task. In particular, Gergle at al. (2004), Clark and Krych (2004) and 115 Brennan (2005) explored different communication tasks and compared a condition, in which visual and verbal information was</context>
</contexts>
<marker>Mills, Healey, 2006</marker>
<rawString>Gregory Mills and Patrick G. T. Healey. 2006. Clarifying Spatial Descriptions: Local and Global Effects on Semantic Co-ordination. In Procs. of the 10th Workshop on the Semantics and Pragmatics of Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="1878" citStr="Clark, 1996" startWordPosition="270" endWordPosition="271">s open challenges arising from the nature of the medium itself as well as the unique characteristics of Human-Robot Interaction (HRI). 1.1 Miscommunication in Human-Robot Interaction HRI involves embodied interaction, in which humans and robots coordinate their actions sharing time and space. As most speech-enabled robots remain in the labs, people are generally unaware of what robots can understand and do resulting in utterances that are out of the functional and linguistic domain of the robot. Physical copresence will lead people to make strong but misplaced assumptions of mutual knowledge (Clark, 1996), increasing the use of underspecified referents and deictic expressions. Robots operate in and manipulate the same environment as humans, so failure to prevent and rectify errors has potentially severe consequences. Finally, these issues are aggravated by unresolved challenges with automatic speech recognition (ASR) technologies. In conclusion, miscommunication in HRI grows in scope, frequency and costs, impelling researchers to acknowledge the necessity to integrate miscommunication in the design process of speech-enabled robots. 1.2 Aims of study The goal of this study is two-fold; first, t</context>
<context position="16228" citStr="Clark (1996)" startWordPosition="2540" endWordPosition="2541">y providing task-related information in either the form of CRs or simple statements. The study contributes to the findings presented in Skantze (2005) and Williams and Young (2004) in that it demonstrates the use of similar strategies to deal with different sources of problems. 114 In the Semi-Constrained condition, a degree of restrain and control over the error handling capacity of the wizards was introduced. In particular, the wizards could explicitly signal communication problems in the utterance, meaning and action level using three predefined responses. This is inspired by the models of Clark (1996) and Allwood (1995), according to which, miscommunication can occur in any of these levels and people select repair initiations that point to the source of the problem. The model (adapted from Mills and Healey, 2006) and the responses are schematically shown in Table 3 below. Levels of Communication Wizard Responses Level 1 Securing Attention - Level 2 Utterance Recognition “What?” Level 3 Meaning Recognition “Sorry, I don’t understand.” Level 4 Action Recognition “I cannot do that.” Table 3. Levels of communication. Moreover, based on the classification of the wizard’s error handling strategi</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Meredyth A Krych</author>
</authors>
<title>Speaking While Monitoring Addressees for Understanding.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<pages>50--62</pages>
<contexts>
<context position="20248" citStr="Clark and Krych (2004)" startWordPosition="3151" endWordPosition="3154">tterance, are more severe for the dialogue compared to reprise fragments (e.g., “Turn where?”) that accept part of the utterance. Mills and Healey (2006) also found that “What’s” were more disruptive to the dialogue than reprise fragments. Guided by these insights, our current work looks at how each error strategy affects the subsequent unfolding of the dialogue. 5 The second WOz study Research in human communication has shown that in task-oriented interactions visual information has a great impact on dialogue patterns and improves performance in the task. In particular, Gergle at al. (2004), Clark and Krych (2004) and 115 Brennan (2005) explored different communication tasks and compared a condition, in which visual and verbal information was available, with a speech-only condition. In their experiments, a person gave instructions to another participant on how to complete a task. Their findings seem to resonate. In terms of time for task completion and number of words per turn, the interactions in the visual information condition were more efficient. The physical actions of the person following the instructions functioned as confirmations and substituted for verbal grounding. Regarding errors, no signi</context>
</contexts>
<marker>Clark, Krych, 2004</marker>
<rawString>Herbert H. Clark and Meredyth A. Krych. 2004. Speaking While Monitoring Addressees for Understanding. Journal of Memory and Language, 50:62-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Characterizing Task-Oriented Dialog Using a Simulated ASR</title>
<date>2004</date>
<location>South</location>
<contexts>
<context position="12394" citStr="Williams and Young (2004)" startWordPosition="1960" endWordPosition="1963">) when the wizard expressed inability to execute the instruction (for instance, given the robot’s current location, as shown in line 2 in the dialogue), although he/she was able to interpret it. Secondly, non-understandings (tag: Non, line 4) occurred when the wizards obtained no interpretation at all or too many. Non-understandings also included cases in which wizards were uncertain about their interpretation (as suggested by Gabsdil, 2003). Lastly, misconceptions happen when the beliefs of the interlocutors clash, and are outside the scope of this study. 4 First WOz study Skantze (2005) and Williams and Young (2004) performed variations of WOz studies to explore how humans handle ASR errors, using a real or simulated speech recogniser. They discovered that even after highly inaccurate recognition output, the participants rarely signalled nonunderstanding explicitly. Accordingly, the experimental hypothesis of the present study is that wizards in both conditions will not choose explicit responses to signal miscommunication (such as “I don’t understand” or “What?”) but responses that contribute with information. ASR is a major source of errors in SDS. But as miscommunication is ubiquitous in interaction, t</context>
<context position="15796" citStr="Williams and Young (2004)" startWordPosition="2469" endWordPosition="2472"> Schlangen (2004). However, for a navigation task and having excluded ASR errors, problems occurred mainly in the meaning recognition level (explained below) and aimed for reference resolution. Figure 3. Use of strategies to signal nonunderstandings or rejections, for either condition. In conclusion, wizards in the Unconstrained condition did not directly signal problems in understanding but, instead, they attempted to advance the dialogue by providing task-related information in either the form of CRs or simple statements. The study contributes to the findings presented in Skantze (2005) and Williams and Young (2004) in that it demonstrates the use of similar strategies to deal with different sources of problems. 114 In the Semi-Constrained condition, a degree of restrain and control over the error handling capacity of the wizards was introduced. In particular, the wizards could explicitly signal communication problems in the utterance, meaning and action level using three predefined responses. This is inspired by the models of Clark (1996) and Allwood (1995), according to which, miscommunication can occur in any of these levels and people select repair initiations that point to the source of the problem.</context>
</contexts>
<marker>Williams, Young, 2004</marker>
<rawString>Jason D. Williams and Steve Young. 2004. Characterizing Task-Oriented Dialog Using a Simulated ASR Channel. ICSLP. Jeju, South Korea.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jean Carletta</author>
<author>Amy Isard</author>
<author>Stephen Isard</author>
<author>Jacqueline Kowtko</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>H Anne</author>
</authors>
<booktitle>Anderson.1996. HCRC Dialogue Structure Coding Manual(HCRC/TR-82). Human Communication</booktitle>
<institution>Research Centre, University of Edinburgh.</institution>
<marker>Carletta, Isard, Isard, Kowtko, Doherty-Sneddon, Anne, </marker>
<rawString>Jean Carletta, Amy Isard, Stephen Isard, Jacqueline Kowtko, Gwyneth Doherty-Sneddon and Anne H. Anderson.1996. HCRC Dialogue Structure Coding Manual(HCRC/TR-82). Human Communication Research Centre, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Allwood</author>
</authors>
<title>An Activity based Approach to Pragmatics. Gothenburg Papers in Theoretical Linguistics, 76,</title>
<date>1995</date>
<institution>Göteborg University, Sweden.</institution>
<contexts>
<context position="16247" citStr="Allwood (1995)" startWordPosition="2543" endWordPosition="2544">related information in either the form of CRs or simple statements. The study contributes to the findings presented in Skantze (2005) and Williams and Young (2004) in that it demonstrates the use of similar strategies to deal with different sources of problems. 114 In the Semi-Constrained condition, a degree of restrain and control over the error handling capacity of the wizards was introduced. In particular, the wizards could explicitly signal communication problems in the utterance, meaning and action level using three predefined responses. This is inspired by the models of Clark (1996) and Allwood (1995), according to which, miscommunication can occur in any of these levels and people select repair initiations that point to the source of the problem. The model (adapted from Mills and Healey, 2006) and the responses are schematically shown in Table 3 below. Levels of Communication Wizard Responses Level 1 Securing Attention - Level 2 Utterance Recognition “What?” Level 3 Meaning Recognition “Sorry, I don’t understand.” Level 4 Action Recognition “I cannot do that.” Table 3. Levels of communication. Moreover, based on the classification of the wizard’s error handling strategies in the Unconstra</context>
</contexts>
<marker>Allwood, 1995</marker>
<rawString>Jens Allwood. 1995. An Activity based Approach to Pragmatics. Gothenburg Papers in Theoretical Linguistics, 76, Göteborg University, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenny Coventry</author>
<author>Thora Tenbrink</author>
<author>John Bateman</author>
</authors>
<title>Spatial Language and Dialogue: Navigating the Domain. In</title>
<date>2009</date>
<pages>1--8</pages>
<publisher>Oxford University Press. Oxford, UK.</publisher>
<contexts>
<context position="29028" citStr="Coventry et al., 2009" startWordPosition="4554" endWordPosition="4557"> the most valuable but complex processes in the design of a NLI for a robot is enacting a HRI scenario to obtain naturally-occurring data which is yet generalisable and relevant for the future implementation of the system. The present study recreated a navigation scenario in which non-experienced users interacted with and taught a mobile robot. It also simulated two different setups which corresponded to the realistic situations of supervised and unsupervised interaction. The current trend in the fields of linguistics and robotics is the unified investigation of spatial language and dialogue (Coventry et al., 2009). Exploring dialogue-based navigation of a robot, our study aimed to contribute to this body of research. It can be argued that there were limitations in the simulation as compared to the experimental testing of a real system and, thus, the study was primarily explorative. However, it yielded natural dialogues given that naive “confederates” and no dialogue script were used. The data analysis was more qualitative than quantita117 tive and followed established methods from previous research. Finally, the results of the study matched and extended these findings and provided useful information fo</context>
</contexts>
<marker>Coventry, Tenbrink, Bateman, 2009</marker>
<rawString>Kenny Coventry, Thora Tenbrink and John Bateman, 2009. Spatial Language and Dialogue: Navigating the Domain. In K. Coventry, T. Tenbrink, and J. Bateman (Eds.) Spatial Language and Dialogue. 1-8. Oxford University Press. Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
</authors>
<title>Clarification in Spoken Dialogue Systems. In:</title>
<date>2003</date>
<booktitle>Proceedings of 2003 AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue,</booktitle>
<location>Stanford, USA.</location>
<contexts>
<context position="12214" citStr="Gabsdil, 2003" startWordPosition="1933" endWordPosition="1934"> user became aware of the error (after receiving visual or verbal information). Following the same definition, misunderstandings were also tagged as rejections (tag: Rej) when the wizard expressed inability to execute the instruction (for instance, given the robot’s current location, as shown in line 2 in the dialogue), although he/she was able to interpret it. Secondly, non-understandings (tag: Non, line 4) occurred when the wizards obtained no interpretation at all or too many. Non-understandings also included cases in which wizards were uncertain about their interpretation (as suggested by Gabsdil, 2003). Lastly, misconceptions happen when the beliefs of the interlocutors clash, and are outside the scope of this study. 4 First WOz study Skantze (2005) and Williams and Young (2004) performed variations of WOz studies to explore how humans handle ASR errors, using a real or simulated speech recogniser. They discovered that even after highly inaccurate recognition output, the participants rarely signalled nonunderstanding explicitly. Accordingly, the experimental hypothesis of the present study is that wizards in both conditions will not choose explicit responses to signal miscommunication (such</context>
<context position="14362" citStr="Gabsdil, 2003" startWordPosition="2255" endWordPosition="2256">tegrating finer classification of clarification requests (CRs) and the original dialogue act tagging, the DAs used by the wizards to signal non-understandings and rejections were categorised as shown in Table 2. Dialogue Act Explanation Explain The wizard gives description of robot’s location. E.g., “I crossed the bridge.”, “I am at a crossroad”. Check This category covers CRs. The corpus contained two types of CRs: first, task-level reformulations (as in line 4 in Table 1), which reformulate the utterance on the basis of its effects on the task, showing the wizard’s subjective understanding (Gabsdil, 2003). Second, alternative CRs which occur when the wizard gives two alternative interpretations, trying to resolve referential ambiguity. For instance, “back to the bridge or to the factory”, to resolve “go back to last location”. Query-w The wizard asks for further instructions. E.g., “Please give me further instructions.” Explain+Query- A combo of actions; the wizard provides inforw mation on location and asks for further instructions. E.g., “crossroads, now where?” Table 2.Wizard DAs after miscommunication. Figure 3 illustrates the distribution of these responses to signal non-understandings an</context>
<context position="19033" citStr="Gabsdil (2003)" startWordPosition="2967" endWordPosition="2968">i-Constrained). 4.2 Discussion and future work The findings of this study could be extrapolated to HRI. Classification of the responses of the wizards resulted in a limited set of error signalling strategies. Therefore, in the presence of miscommunication the robot could use the static, explicit strategies. But these strategies alone are inadequate (as shown by Koulouri and Lauria, 2009). They need to be supplemented, but not entirely replaced, with dynamic error handling strategies; namely, posing relevant questions and providing descriptions of location. Yet this entails several challenges. Gabsdil (2003) identifies the complexity of adding clarification requests to systems with deep semantic processing. With regard to alternative clarifications, systems would need to generate two alternative interpretations for one referent. Task-level reformulations would also require the system to have the capability to identify the effects of all possible executions of the instruction. As a next step, we will focus on issues concerning the implementation of such functionality. Schlangen (2004) suggests that “generalpurpose” repair initiations, such as “What?”, which request repetition of the whole utteranc</context>
</contexts>
<marker>Gabsdil, 2003</marker>
<rawString>Malte Gabsdil. 2003. Clarification in Spoken Dialogue Systems. In: Proceedings of 2003 AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>CLARIE: Handling Clarification Requests in a Dialogue System. Research on Language and Computation.</title>
<date>2006</date>
<pages>4--2</pages>
<contexts>
<context position="15167" citStr="Purver (2006)" startWordPosition="2376" endWordPosition="2377">esolve “go back to last location”. Query-w The wizard asks for further instructions. E.g., “Please give me further instructions.” Explain+Query- A combo of actions; the wizard provides inforw mation on location and asks for further instructions. E.g., “crossroads, now where?” Table 2.Wizard DAs after miscommunication. Figure 3 illustrates the distribution of these responses to signal non-understandings and rejections (columns labelled “Uncons-NON” and “Uncons-REJ”, respectively). Evidently, there is a much greater variety of CRs than the two CR types reported here, as described in the work of Purver (2006) and Schlangen (2004). However, for a navigation task and having excluded ASR errors, problems occurred mainly in the meaning recognition level (explained below) and aimed for reference resolution. Figure 3. Use of strategies to signal nonunderstandings or rejections, for either condition. In conclusion, wizards in the Unconstrained condition did not directly signal problems in understanding but, instead, they attempted to advance the dialogue by providing task-related information in either the form of CRs or simple statements. The study contributes to the findings presented in Skantze (2005) </context>
</contexts>
<marker>Purver, 2006</marker>
<rawString>Matthew Purver. 2006. CLARIE: Handling Clarification Requests in a Dialogue System. Research on Language and Computation. 4(2-3):259-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislao Lauria</author>
<author>Guido Bugmann</author>
<author>Theocharis Kyriacou</author>
<author>Johan Bos</author>
<author>Ewan Klein</author>
</authors>
<title>Training Personal Robots Using Natural Language Instruction.</title>
<date>2001</date>
<journal>IEEE Intelligent Systems.</journal>
<pages>38--45</pages>
<contexts>
<context position="2705" citStr="Lauria et al., 2001" startWordPosition="387" endWordPosition="390">equences. Finally, these issues are aggravated by unresolved challenges with automatic speech recognition (ASR) technologies. In conclusion, miscommunication in HRI grows in scope, frequency and costs, impelling researchers to acknowledge the necessity to integrate miscommunication in the design process of speech-enabled robots. 1.2 Aims of study The goal of this study is two-fold; first, to incorporate “natural” and robust miscommunication management mechanisms (namely, prevention and repair) into a mobile personal robot, which is capable of learning by means of natural language instruction (Lauria et al., 2001). Secondly, it aims to offer some insights that are relevant for the development of NLIs in HRI in general. This research is largely motivated by models of human communication. It is situated within the language-as-action tradition and its approach is to explore and build upon how humans manage miscommunication. 2 Method We designed and performed two rounds of Wizard of Oz (WOz) simulations. Given that the general aim of the study is to determine how robots should initiate repair and provide feedback in collaborative tasks, the simulations departed from the typical WOz methodology in that the </context>
</contexts>
<marker>Lauria, Bugmann, Kyriacou, Bos, Klein, 2001</marker>
<rawString>Stanislao Lauria, Guido Bugmann, Theocharis Kyriacou, Johan Bos and Ewan Klein. 2001. Training Personal Robots Using Natural Language Instruction. IEEE Intelligent Systems. 38–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
</authors>
<title>How Conversation is Shaped by Visual and Spoken Evidence. In</title>
<date>2005</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="20271" citStr="Brennan (2005)" startWordPosition="3157" endWordPosition="3158">he dialogue compared to reprise fragments (e.g., “Turn where?”) that accept part of the utterance. Mills and Healey (2006) also found that “What’s” were more disruptive to the dialogue than reprise fragments. Guided by these insights, our current work looks at how each error strategy affects the subsequent unfolding of the dialogue. 5 The second WOz study Research in human communication has shown that in task-oriented interactions visual information has a great impact on dialogue patterns and improves performance in the task. In particular, Gergle at al. (2004), Clark and Krych (2004) and 115 Brennan (2005) explored different communication tasks and compared a condition, in which visual and verbal information was available, with a speech-only condition. In their experiments, a person gave instructions to another participant on how to complete a task. Their findings seem to resonate. In terms of time for task completion and number of words per turn, the interactions in the visual information condition were more efficient. The physical actions of the person following the instructions functioned as confirmations and substituted for verbal grounding. Regarding errors, no significant differences were</context>
</contexts>
<marker>Brennan, 2005</marker>
<rawString>Susan E. Brennan . 2005. How Conversation is Shaped by Visual and Spoken Evidence. In J. Trueswell &amp; M. Tanenhaus (Eds.) Approaches to Studying World-situated Language Use: Bridging the Language-as-product and Language-action Traditions. 95-129. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodora Koulouri</author>
<author>Stanislao Lauria</author>
</authors>
<title>A WOz Framework for Exploring Miscommunication in HRI,</title>
<date>2009</date>
<booktitle>In Procs. of the AISB Symposium on New Frontiers in Human-Robot Interaction.</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="4186" citStr="Koulouri and Lauria, 2009" startWordPosition="634" endWordPosition="637">n, September 2009. c�2009 Association for Computational Linguistics 111 guided the robot to six designated locations in a simulated town. The user had full access to the map whereas the wizard could only see the surrounding area of the robot. Thus, the wizard relied on the user’s instructions on how to reach the destination. In this section we outline the aim and approach of each WOz study, the materials used and the experimental procedure. Sections 4 and 5 focus on each study individually and their results. 2.1 The first WOz study This study is a continuation of previous work by the authors (Koulouri and Lauria, 2009). In that study, the communicative resources of the wizard were incrementally restricted, from “normal” dialogue capabilities towards the capabilities of a dialogue system, in three experimental conditions: • The wizard simulates a super-intelligent robot capable of using unconstrained, natural language with the user (henceforth, Unconstrained Condition). • The wizard can select from a list of default responses but can also ask for clarification or provide task-related information (henceforth, Semi-Constrained condition). • The wizard is restricted to choose from a limited set of canned respon</context>
<context position="18342" citStr="Koulouri and Lauria, 2009" startWordPosition="2864" endWordPosition="2867">of each implicit strategy to signal non-understandings (Semi-NON) and rejections (Semi-REJ). The initial prediction was that wizards will not use explicit signals of problems in the dialogue. This was contradicted by the results. It can be argued that the physical presence of the buttons and the less effort required account for this phenomenon. On the other hand, it is also plausible to assume that these strategies matched what the wizards wanted to say. Finally, there were no significant differences between conditions in terms of user experience, task success and time on task (as reported in Koulouri and Lauria, 2009). Figure 4. Occurrence of implicit and explicit miscommunication signals (Semi-Constrained). 4.2 Discussion and future work The findings of this study could be extrapolated to HRI. Classification of the responses of the wizards resulted in a limited set of error signalling strategies. Therefore, in the presence of miscommunication the robot could use the static, explicit strategies. But these strategies alone are inadequate (as shown by Koulouri and Lauria, 2009). They need to be supplemented, but not entirely replaced, with dynamic error handling strategies; namely, posing relevant questions </context>
</contexts>
<marker>Koulouri, Lauria, 2009</marker>
<rawString>Theodora Koulouri and Stanislao Lauria. 2009. A WOz Framework for Exploring Miscommunication in HRI, In Procs. of the AISB Symposium on New Frontiers in Human-Robot Interaction. Edinburgh, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>