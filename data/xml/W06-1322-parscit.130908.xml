<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000965">
<title confidence="0.9965935">
A computational model of multi-modal grounding
for human robot interaction
</title>
<author confidence="0.994136">
Shuyin Li, Britta Wrede, and Gerhard Sagerer
</author>
<affiliation confidence="0.7993175">
Applied Computer Science, Faculty of Technology
Bielefeld University, 33594 Bielefeld, Germany
</affiliation>
<email confidence="0.96814">
shuyinli, bwrede, sagerer@techfak.uni-bielefeld.de
</email>
<sectionHeader confidence="0.98247" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941076923077">
Dialog systems for mobile robots operat-
ing in the real world should enable mixed-
initiative dialog style, handle multi-modal
information involved in the communica-
tion and be relatively independent of the
domain knowledge. Most dialog systems
developed for mobile robots today, how-
ever, are often system-oriented and have
limited capabilities. We present an agent-
based dialog model that are specially de-
signed for human-robot interaction and
provide evidence for its efficiency with our
implemented system.
</bodyText>
<sectionHeader confidence="0.995169" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943966666667">
Natural language is the most intuitive way to com-
municate for human beings (Allen et al., 2001). It
is, therefore, very important to enable dialog capa-
bility for personal service robots that should help
people in their everyday life. However, the inter-
action with a robot as a mobile, autonomous de-
vice is different than with many other computer
controlled devices which affects the dialog model-
ing. Here we want to first clarify the most essen-
tial requirements for dialog management systems
for human-robot interaction (HRI) and then out-
line state-of-the-art dialog modeling approaches to
position ourselves.
The first requirement results from the situated-
ness (Brooks, 1986) of HRI. A mobile robot is
situated “here and now” and cohabits the same
physical world as the user. Environmental changes
can have massive influence on the task execution.
For example, a robot should fetch a cup from the
kitchen but the door is locked. Under this cir-
cumstance the dialog system must support mixed-
initiative dialog style to receive user commands on
the one side and to report on the perceived envi-
ronmental changes on the other side. Otherwise
the robot had to break up the task execution and
there is no way for the user to find out the reason.
The second challenge for HRI dialog manage-
ment is the embodiment of a robot which changes
the way of interaction. Empirical studies show that
the visual access to the interlocutor’s body affects
the conversation in the way that non-verbal behav-
iors are used as communicative signals (Nakano et
al., 2003). For example, to refer to a cup that is
visible to both dialog partners, the speaker tends
to say “this cup” while pointing to it. The same
strategy is considerably ineffective during a phone
call. This example shows, an HRI dialog system
must account for multi-modal communication.
The third, probably the unique challenge for
HRI dialog management is the implication of the
learning ability of such a robot. Since a personal
service robot is intended to help human in their
individual household it is impossible to hard-code
all the knowledge it will need into the system, e.g.,
where the cup is and what should be served for
lunch. Thus, it is essential for such a robot to
be able to learn new knowledge and tasks. This
ability, however, has the implication for the dia-
log system that it can not rely on comprehensive,
hard-coded knowledge to do dialog planning. In-
stead, it must be designed in a way that it has a
loose relationship with the domain knowledge.
Many dialog modeling approaches already ex-
ist. McTear (2002) classified them into three main
types: finite state-based, frame-based, and agent-
based. In the first two approaches the dialog struc-
ture is closely coupled with pre-defined task steps
and can therefore only handle well-structured
tasks for which one-side led dialog styles are suf-
ficient. In the agent-based approach, the com-
</bodyText>
<page confidence="0.609546">
153
</page>
<note confidence="0.8495475">
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 153–160,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999927702702703">
munication is viewed as a collaboration between
two intelligent agents. Different approaches in-
spired by psychology and linguistics are in use
within this category. For example, within the
TRAINS/TRIPS project several complex dialog
systems for collaborative problem solving have
been developed (Allen et al., 2001). Here the dia-
log system is viewed as a conversational agent that
performs communicative acts. During a conver-
sation, the dialog system selects the communica-
tive goal based on its current belief about the do-
main and general conversational obligations. Such
systems make use of communication and domain
model to enable mixed-initiative dialog style and
to handle more complex tasks. In the HRI field,
due to the complexity of the overall systems, usu-
ally the finite-state-based strategy is employed
(Matsui et al., 1999; Bischoff and Graefe, 2002;
Aoyama and Shimomura, 2005). As to the is-
sue of multi-modality, one strand of the research
concerns the fusion and representation of multi-
modal information such as (Pfleger et al., 2003)
and the other strand focuses on the generalisation
of human-like conversational behaviors for virtual
agents. In this strand, Cassell (2000) proposes a
general architecture for multi-modal conversation
and Traum (2002) extends his information-state
based dialog model by adding more conversational
layers to account for multi-modality.
In this paper we present an agent-based dialog
model for HRI. As described in section 2, the two
main contributions of this model are the new mod-
eling approach of Clark’s grounding mechanism
and the extension of this model to handle multi-
modal grounding. In section 3 we outline the ca-
pabilities of the implemented system and present
some quantitative evaluation results.
</bodyText>
<sectionHeader confidence="0.994636" genericHeader="method">
2 Dialog Model
</sectionHeader>
<bodyText confidence="0.999908071428571">
We view a dialog as a collaboration between two
agents. Agents are subject to common conversa-
tional rules and participate in a conversation by
issuing multi-modal contributions (e.g., by say-
ing something or displaying a facial expression).
In subsection 2.1 we show how we handle con-
versational tasks by modeling the conversational
rules based on grounding and in subsection 2.2 we
present how we model individual contributions to
tackle the issue of multi-modality. In subsection
2.3 we put these two things together to complete
the model description. In this section, we also put
concrete examples from the robot domain to clar-
ify the relatively abstract model.
</bodyText>
<subsectionHeader confidence="0.992082">
2.1 Grounding
</subsectionHeader>
<bodyText confidence="0.99999255319149">
One of the most influential theories on the collab-
orative nature of dialog is the common ground the-
ory of Clark (1992). In his opinion, agents need
to coordinate their mental states based on their
mutual understanding about the current tasks, in-
tentions, and goals during a conversation. Clark
termed this process as grounding and proposed a
contribution model. In this model, “contributions”
from conversational agents are considered to be
the basic component of a conversation. Each con-
tribution has two phases: a Presentation phase and
an Acceptance phase. In the Presentation phase the
speaker presents an utterance to the listener, in the
Acceptance phase the listener issues an evidence
of understanding to the speaker. The speaker can
only be sure that the utterance she presented previ-
ously has become a part of their common ground
if this evidence is available.
Although this well established theory provides
comprehensive insight into human conversation
two issues in this theory remain critical when be-
ing applied to model dialog. The first one is the re-
cursivity of Acceptance. Clark claimed, since ev-
erything said by one agent needs to be understood
by her interlocutor, each Acceptance should also
play the role of Presentation which needs to be ac-
cepted, too. The contributions are thus to be or-
ganized as a graph. However, this implies that the
grounding process may never really end (Traum,
1994). The second critical issue is taking con-
tributions as the most basic grounding units. In
Clark’s view, the basic grounding unit, i.e., the unit
of conversation at which grounding takes place,
is the contribution. To provide Acceptance for a
contribution agents may need to issue clarification
questions or repair. But when modeling a dialog,
especially a task-oriented dialog, it is hard to map
one single contribution from one agent to a domain
task since tasks are always cooperately done by
the two agents (Cahn and Brennan, 1999). Traum
(1994) addressed the first issue by introducing a
finite-state based grounding mechanism and Cahn
and Brennan (1999) used “exchanges”’ as the ba-
sic grounding unit to tackle the second critical is-
sue. We combine the advantages of their work and
present a grounding mechanism based on an aug-
mented push-down automaton as described below.
</bodyText>
<page confidence="0.977323">
154
</page>
<bodyText confidence="0.999456078431374">
Basic grounding unit: As Cahn and Brennan
we take exchange as the most basic grounding
unit. An exchange is a pair of contributions ini-
tiated by the two conversational agents. They rep-
resent the idea of adjacency pairs (Schegloff and
Sacks, 1973). The first contribution of the ex-
change is the Presentation and the second contri-
bution is the Acceptance, e.g., if one asks a ques-
tion and the other answers it, then the question is
the Presentation and the answer is the Acceptance.
In our model, a contribution only represents one
speech act. For example, if an agent says “Hello,
my name is Tom, what is your name?” this ut-
terances is segmented into three Presentations (a
greeting, a statement, and a question) although
they occur in one turn. These three Presentations
initiate three exchanges and each of them needs to
be accepted by the interlocutor.
Changing status of grounding units: Also as
proposed by Cahn and Brennan, an exchange has
two states: not (yet) grounded and grounded. An
exchange is grounded if the Acceptance of the
Presentation is available. Note, the Acceptance
can be an implicit one, e.g, in form of “contin-
ued attention” in Clark’s term. Taking the exam-
ple above, the other agent would reply “Hello, my
name is Jane.” without explicitely commenting
Tom’s name, yet the three exchanges that Tom ini-
tiated were all accepted.
Organization of grounding units: In accor-
dance with Traum we do not think that the Pre-
sentation of one exchange should play the role
of the Acceptance of its previous exchange. In-
stead, we organize exchanges in a stack. The stack
represents the whole ungrounded discourse: un-
grounded exchanges are pushed onto it and the
grounded ones are popped out of it. One major
question of this representation is: What has the
grounding status of individual exchange to do with
the grounding status of the whole stack? Jane’s
Acceptance of Tom’s greeting has no apparent re-
lation to the remaining two still ungrounded ex-
changes initiated by Tom. But in the center em-
bedding example in Fig. 1, the Acceptance of B1
(utterance A2) contributes to the Acceptance of
A1 (utterance B2). These examples show that the
grounding status of the whole discourse depends
on (1) the grounding status of the individual ex-
changes and (2) the relationship between these ex-
changes, the grounding relation. These relations
are introduced by the Presentation of each ex-
change because they start an exchange. We identi-
fied 4 types of grounding relations: Default, Sup-
port, Correct, and Delete. In the following we
look at these relations in more detail and refer to
exchanges with relation x to its immediately pre-
ceding exchange (IPE) as “x exchange”, e.g., Sup-
port exchange:
Default: The current Presentation introduces a
new account that is independent of the previous
exchange in terms of grounding, e.g., what Tom
said to Jane constructs three Presentations that ini-
tiate three default exchanges. Such exchanges can
be grounded independently of each other.
Support: If an agent can not provide Accep-
tance for the given Presentation she will initiate
a new exchange to support the grounding process
of the ungrounded exchange. A typical exam-
ple of such an exchange is a clarification ques-
tion like “I beg your pardon?”. If a Support ex-
change is grounded its initiator will try to ground
the IPE again with the newly collected information
through the supporting exchange.
Correct: Some exchanges are created to correct
the content of the IPE, e.g., in case that the lis-
tener misunderstood the speaker and the speaker
corrects it. Similar to Support, after such an ex-
change is grounded its IPE is updated with new
information and has to be grounded again.
Delete: Agents can give up their effort to build a
common ground with her interlocutor, e.g., by say-
ing “Forget it.”. If the interlocutor agrees, such ex-
changes have the effect that all the ungrounded ex-
changes from the initial Default exchange up to the
current state are no longer relevant and the agents
do not need to ground them any more.
Note, once an exchange is grounded it is imme-
diately removed from the stack so that its IPE be-
comes the IPE of the next exchange. This model
is described as an augmented push-down automa-
ton (Fig. 2). It is augmented in so far that transi-
tions can trigger actions and a variable number of
exchanges can be popped or pushed in one step.
There are five states in this APDA and they rep-
resent the fact what kind of ungrounded exchange
is on the top of the stack. Along the arrows that
connect the states the input (denoted as I), the re-
sulting stack operation (denoted as S) and the pos-
sible action that is triggered (denoted as A) are
given. The input of this automaton includes Pre-
sentation (e.g., “defaultP” stands for “Default Pre-
sentation”) and Acceptance.
</bodyText>
<page confidence="0.737372">
155
</page>
<note confidence="0.51099725">
A1: What do you think about Mr. Watton?
B1: Mr. Watton? our music teacher?
A2: Yes. (accept B1)
B2: Well, he is OK. (accept A1)
</note>
<figureCaption confidence="0.993358333333333">
Figure 1: An example of center embedding
Figure 2: Augmented push-down automaton for
grounding (ex: exchange)
</figureCaption>
<bodyText confidence="0.999992386363636">
As long as there is an ungrounded exchange
at the top of the stack, the addressee will try to
ground it by providing Acceptance, unless its va-
lidity is deleted. For the reason of space, we only
explain the APDA with the center embedding ex-
ample in Fig. 1. Contribution A1 introduces a
question into the discourse which initiates a De-
fault exchange, say Ex1. This exchange is pushed
onto the stack. Instead of providing Acceptance
to A1, contribution B1 initiates a new exchange,
say Ex2, with grounding relation Support to Ex1
and is pushed onto the stack. Then contribution
A2 acknowledges B1 so that Ex2 is grounded and
popped out of the stack. The top element of the
stack is now the ungrounded Ex1. Since Ex2 sup-
ported Ex1, the Ex1 is updated with the infor-
mation contained in Ex2 (The music teacher was
meant) and B2 then successfully grounds this up-
dated Ex1.
In our model, every exchange can be individu-
ally grounded and contributes to the grounding of
the whole ungrounded discourse by acting on the
IPE according to their grounding relations. This
way we can organize the discourse in a sequence
without losing the local grounding flexibility. For
an implemented system, this means that both the
user and the system can easily take initiative or
issue clarification questions. To implement this
model, however, two points are crucial. The first
one is the recognition of the user’s contribution
type: for every user contribution, the dialog sys-
tem needs to decide whether it is a Presentation or
an Acceptance. If it is a Presentation, the system
needs further to decide whether it initiates a new
account, corrects or supports the current one, or
deletes it. This issue of intention recognition is a
classical challenge for dialog systems. We present
our solution in section 3. The second point is that
the dialog system needs to know when to create an
exchange of certain grounding relation by generat-
ing an appropriate Presentation and when to create
an Acceptance. For that we need to first look at the
structure of individual contributions more closely
in the next subsection.
</bodyText>
<subsectionHeader confidence="0.997517">
2.2 The structure of agents’ contributions
</subsectionHeader>
<bodyText confidence="0.999735085714286">
To represent the structure of the individual contri-
butions we take into account the whole language
generation process which enables us to come up
with a powerful solution as described below.
The layers of a contribution: What we can
observe in a conversation are only exchanges of
agents’ contributions in verbal or non-verbal form.
But in fact the contributions are the end-product
of a complex cognitive process: language produc-
tion. Levelt (1989) identified three phases of lan-
guage production: conceptualization, formulation,
and articulation. The production of an utterance
starts from the conception of a communicative in-
tention and the semantic organization in the con-
ceptualization phase before the utterance can be
formulated and articulated in the next two phases.
Intentions can arise from the previous discourse or
from other motivations such as needs for help or
information. This finding motivates us to set up a
two-layered structure of contributions. One layer
is the so-called intention layer where communi-
cation intentions are conceived. For a robot the
communication intentions come from the analysis
of the previous discourse or from the robot control
system. The other layer is the conversation layer.
The communication intentions are formulated and
articulated here1. These two layers represent the
intention conception and the language generation
process, respectively. We term this two-layered
structure of contribution interaction unit (IU).
The issue of multi-modality: Face-to-face
conversations are multi-modal. Speech and body
language (e.g., gesture) can happen simultane-
ously. McNeill (1992) stated that gesture and
speech arise from the same semantic source, the
</bodyText>
<tableCaption confidence="0.681894">
1Since most robot systems use speech synthesizer to gen-
erate acoustic output which replaces the articulation process,
only formulation is performed on this layer.
</tableCaption>
<figure confidence="0.990273222222222">
I:deleteP; S:push(ex)
I:defaultP
S:push(ex)
I:acc
IcorrectP; S:push(ex)
S:pop(ex)
Default
Top
I:acc; S:pop(ex); A:correct(IPE) I:acc; S:pop(ex)
I:defaultP; S:pop(all)&amp;push(ex)
Correct
Top
Start
Support
Top
I:supportP; S:push(ex)
I:acc; S:pop(ex)
Delete
Top
I[supportP|correctP|
deleteP; S:
I:acc
S:pop(ex)
S:push(ex)
I:supportP
I:deleteP; S:push(ex)
156
</figure>
<bodyText confidence="0.98827268">
so-called “idea unit” and are co-expressive. Since
semantic representation is created out of commu-
nicative intentions (Levelt, 1989) we assume the
communication intentions are the modality inde-
pendent base that governs the multi-modal lan-
guage production. We, therefore, extend our struc-
ture above by introducing two generators on the
conversation layer: one verbal and one non-verbal
generator that represent the verbal and non-verbal
language generation mechanism based on the
communication intentions created on the intention
layer. The relationship between these two genera-
tors is variable. For example, Iverson et al. (1999)
identified three types of informational relationship
between speech and gesture:
reinforcement (gesture rein-
forces the message conveyed
in speech, e.g., emphatic ges-
ture), disambiguation (ges-
ture serves as the precise ref-
erent of the speech, e.g., deic-
tic gesture accompanying the
utterance “this cup”), and adding-information
(e.g., saying “The ball is so big.” and shaping
the size with hands). In our work, when process-
ing users’ multi-modal contributions we focus on
the disambiguation relation; when creating multi-
modal contributions for the robot we are also inter-
ested in other informational relations 2. The struc-
ture of an IU is illustrated in Fig. 3.
Operation flow within an interaction unit:
During a conversation an agent either initiates
an account or replies to the interlocutor’s ac-
count. The communication intentions can thus be
self-motivated or other-motivated. For a robot,
self-motivated intentions can be triggered by the
robot control system, e.g., observed environmen-
tal changes. In this case, an IU is created with
its intention layer importing the message from the
robot control system and exporting an intention.
This intention is transfered to the conversation
layer which then formulates a verbal message with
the verbal generator and/or constructs a body lan-
guage expression with the non-verbal generator.
Other-motivated intentions can be triggered by the
needs of the on-going conversation, e.g., the need
to answer a question, or be triggered by robot’s ex-
ecution results of the tasks specified previously by
the user. The operation flow is similar to that of
- Conversation Layer -
2This policy has a practical reason: it is much more diffi-
cult in computer science to correctly recognize and interpret
human motion than to simulate it.
the self-motivation apart from the fact that, in case
of intentions motivated by conversational needs,
the intention layer of the IU does not import any
robot control system message but creates an inten-
tion directly. Note, the IUs that are initiated by the
robot and by the user have identical structure. But
in case of user initiated IUs we do not make any
assumption of their underlying intention building
process and the intention layer of their IUs are thus
always empty.
With the IUs, we can integrate the non-verbal
behavior systematically into the communication
process and model multi-modal dialog. Although
it is not the focus of our work, our model can also
handle purely non-verbal contributions, since the
verbal generator does not always need to be acti-
vated if the non-verbal generator already provides
enough information about the speaker’s intention.
Possible scenarios are: the user looks tired (pre-
sentation) and the robot offers “I can do that for
you.” (acceptance) or the user says something
(presentation) and robot nods (acceptance).
</bodyText>
<subsectionHeader confidence="0.998199">
2.3 Putting things together
</subsectionHeader>
<bodyText confidence="0.999527481481481">
Till now we have discussed our concept of using
a grounding mechanism to organize contributions
and of representing individual contributions as IU.
Now it is time to look at the still open point at the
end of the section 2.1: when to create an IU as
Presentation and when an IU as Acceptance.
Self-motivated intentions usually trigger the
creation of an IU as Presentation with Default re-
lation to its IPE. For example, if the robot needs
to report something to the user it can create a De-
fault exchange by generating an IU as its Presen-
tation. The user is then expected to signal her Ac-
ceptance. Other-motivated intentions can, accord-
ing to the context, result in either Presentation or
Acceptance. To make the correct decision we de-
veloped criteria based on the joint intention theory
of Levesque et al. (1990) which predicts that dur-
ing a collaboration the partners are committed to
a joint goal that they will always try to conform
till they reach the goal or give up. Note, this does
not mean that one will always agree with her inter-
locutor, but they will behave in the way that they
think is the best to achieve the goal. This theory
can be applied to human-robot dialog in a twofold
sense: Firstly, a dialog can be generally seen as
a collaboration as Clark proposed. Secondly, the
human-robot dialog is mostly task-oriented, i.e.,
</bodyText>
<figure confidence="0.859693888888889">
verbal
generator
intention conception
-Intention Layer-
non-verbal
generator
Figure 3: IU
157
push Exchange n with the interlocutor’s IU as presentation
</figure>
<bodyText confidence="0.999872509803922">
the human and the robot work towards the same
goal. With this theory in mind we describe how
we process other-motivated contributions below.
The precondition of language production based
on other-motivated intentions is language percep-
tion. Before reacting, i.e., before creating her own
IU, an agent first needs to understand the inten-
tion conveyed by her interlocutor’s IU by study-
ing its conversation layer. Since we focus on dis-
ambiguation function of non-verbal behavior we
assume that agents first study the generated ver-
bal information, if the intention can not be fully
recognized here, one will further study the infor-
mation provided by the non-verbal generator (e.g.,
a gesture) and fuse the verbal and non-verbal in-
formation. If the intention recognition is still un-
successful, the agent can not provide Acceptance
for the given IU. If she is still committed to the
dialog she will issue a clarification question, i.e.,
she generates an IU as Presentation that initiates
a Support exchange to the current ungrounded ex-
change. If the intention of her interlocutor is suc-
cessfully recognized the language perception pro-
cess ends and the agent tries to create her own IU.
As described in subsection 2.2 the creation of the
IU starts from the creation of an intention on the
intention layer. In case of a robot, the dialog sys-
tem accesses the robot control system and awaits
its reaction to the conveyed information (e.g., a
user instruction). Usually, a robot is designated
to do something for the user, i.e., the robot is com-
mitted to the goal proposed by the user, so we de-
fine the robot can only provide acceptance if the
task is successfully executed. In this case, the robot
completes the current IU with the filled intention
layer by generating an confirmation on its conver-
sation layer. Afterwards, this grounded exchange
can be popped from the stack. If the robot can not
execute the task for some reasons, then the current
exchange can not be grounded and the robot will
take the current IU with the filled intention layer
as another Presentation that initiates a Support or
Correct exchange to the current ungrounded ex-
change, similar as the case in Fig. 1. The conversa-
tion layer of this IU can thus formulate something
like “Sorry, I can’t do that because...” and present
a sorrowful face. This new Support or Correct ex-
change is pushed onto the stack. Figure 4 illus-
trates this process as a UML activity diagram.
In our model we only do general conversational
planning instead of domain specific task planning.
</bodyText>
<note confidence="0.554114">
study verbal info on the interlocutor’s CL
</note>
<figureCaption confidence="0.892907">
Figure 4: Handling other-motivated contribution
(CL: Conversation layer; IL: Intention Layer)
</figureCaption>
<bodyText confidence="0.999828666666667">
What the dialog system needs to know from the
robot control system is what processing results it
can produce. The association of these results with
robot intentions in terms of whether they start a
new account, support or correct one, or delete it,
can be configured externally and thus easily up-
dated or replaced. Based on this configuration IUs
are generated that operate according to the ground-
ing mechanism as described in section 2.1.
</bodyText>
<sectionHeader confidence="0.991625" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999965">
This dialog model was implemented for our robot
BIRON, a personal robot with learning abilities.
It can detect and follow persons, focus on objects
(according to human deictic gestures) and store
collected information into a memory. Our imple-
mentation scenario is the so-called home tour: a
user shows a new robot her home to prepare it for
future tasks. The robot should be able to learn and
remember features of objects that the user men-
tions and it “sees”, e.g., name, color, images etc.
Besides, our system was also successfully ported
to a humanoid robot BARTHOC for studies of
emotional and social factors of HRI (see. Fig. 5).
</bodyText>
<figureCaption confidence="0.973645">
Figure 5: Robots BIRON and BARTHOC
</figureCaption>
<bodyText confidence="0.966937">
The dialog manager is linked to a speech under-
standing system which transforms parts of speech
</bodyText>
<table confidence="0.8865815">
yEs no
intention recognized?
study non−verbal info on the CL
yEs
complete IU as Acceptance
ground exchange n
create IU as Presentation
create exchange n+1 with
Support or Correct relation
pop exchange n push exchange n+1
create one’s own IL
(access robot control system)
intention recognized?
no
yEs
no
intention conforms
the joint goal?
</table>
<page confidence="0.683072">
158
</page>
<bodyText confidence="0.999886636363636">
from a speech recognizer into a speech-act-based
form. To recognize the user’s intention, the dia-
log system classifies this input into 10 categories
of three groups according to heuristics, e.g., in-
struction, description, and query initiate new tasks
and thus a new Default exchange; deletion and
correction initiate Delete and Correct exchanges
that are related to early exchanges; and confirma-
tion, negation, etc. can only be responses and
are, therefore, viewed as user’s Acceptance of ex-
changes that the robot initiates. The main part of
the dialog system is the Dialog Manager that car-
ries out grounding analysis and stack operations.
The Robot Control Manager receives messages
from the robot control system and calls the Dia-
log Manager to do relevant operations. This dialog
system enables multi-modal, mixed-initiative dia-
log style and can handle complex repair behaviors.
With the example dialog in Fig. 6, which is tran-
scripted from a real interaction session between a
user and BIRON, we discuss the two most impor-
tant features of our system.
</bodyText>
<figureCaption confidence="0.9838385">
Figure 6: A dialog example with the extrovert
BIRON. (U: user, R: robot, Ex: Exchange)
</figureCaption>
<bodyText confidence="0.99965435483871">
Taking Initiative and robot personality: Ini-
tiatives that a dialog system can take often depends
on its back-end application. Since BIRON does
not have a task planner which would be ideal to
demonstrate this ability we implemented an extro-
vert personality for it (additionally to its basic per-
sonality) that takes communication-related initia-
tives. The basic BIRON behaves in a rather pas-
sive way and only says something when addressed
by the user. In contrast, the extrovert BIRON
greets persons actively (R1 in Table 6) and re-
marks on its own performance (R6). When the
robot control system detects a person the dialog
system initiates a Default exchange to greet her.
BIRON can also measure its own performance by
counting the number of Support exchanges it has
initiated for the current topic. Since the Support
exchanges are only created if BIRON can not pro-
vide Acceptance to the user’s Presentation (be-
cause it does not understand the user or it can
not execute a task), the amount of the Support ex-
changes thus has direct correlation to the robot’s
overall performance. On the other hand, the more
Default exchanges there are, the better is the per-
formance because the agents can proceed to an-
other topic only if the current one is grounded (or
deleted). Based on this performance indication
BIRON does remarks to motivate users.
Resolving multi-modal object references: It
happens quite frequently in the home tour scenario
that the user points to some objects and says “This
is a z”. BIRON needs to associate its symbolic
name (and eventually other features) mentioned by
the user with the image of the object. The reso-
lution of such multi-modal object references (U4-
R7 in Table 6) is solved as following: the Dialog
Manager creates an IU for the user-initiated utter-
ance (e.g., “this is a cup”) and studies the verbal
and non-verbal generator on its conversation layer.
In the verbal generator, what the pronoun “this”
refers to is unclear, but it indicates that the user
might be using a gesture. Therefore, the Dialog
Manager further studies the non-verbal generator.
The responsible robot vision module is activated
here to search for a gesture and to identify the ob-
ject cup. If the cup is found in the scene, this mod-
ule assigns an ID to the image and stores it in the
memory. After the Dialog Manager receives this
ID, the processing of the conversation layer of the
user IU ends, the Dialog Manager proceeds to cre-
ate its own IU to react to the user’s IU. Problems
with the object identification indicate failure of the
intention recognition process on the user conversa-
tion layer. In this case, the Dialog Manager creates
a Support exchange to ask the user which object
she refers to and retries it if she does not oppose
(R5-R7). This process and the associated multi-
modality fusion and representation are described
in (Li et al., 2005) in detail.
The evaluation of dialog systems for human
robot interaction is still an open issue. A robot
system is usually a complex system including a
</bodyText>
<figure confidence="0.9599102">
(looking sad)
to it with your finger?
U5: Yes!
R6: I know it is sometimes difficult with me,
Ex7
U6: (laugh)
(looking ashamed)
but don’t feel discouraged!
(looking happily)
R7: OK, I’ve found it, it is really nice!
R1: Hello human! (looking friendly)
Ex1
U1: Hello!
Who are you?
R5: Sorry, I can’t find it. Are you pointing
Ex2 R2: My name is BIRON, if you show me
something I can remember it.
U2: OK, um, look, this is a mug.
R3: I beg your pardon?
U3: This is a mug, umm, here.
R4: Sorry, can you use some other words?
U4: This is a mug!
(looking puzzled)
(looking friendly)
(looking sad)
Ex4
Ex5
Ex3
Ex6
159
</figure>
<bodyText confidence="0.999797481481482">
large number of modules that claim plenty of pro-
cessing time or are subject to environmental con-
ditions. For the dialog system, this means that the
correct interpretation and transaction of user utter-
ances is by no means a guaranty for a prompt re-
sponse or successful task execution. Thus, the per-
formance of the dialog system can not be directly
measured with the performance of the overall sys-
tem like most desktop dialog applications. We are
still working at evaluation metrics for HRI dialog
systems (Green et al., 2006). But the efficiency
of our system is already visible in the small ef-
fort associated with the porting of this system to
another robot platform and in the pilot user study
with BIRON. In this study, each of the 14 users in-
teracted with BIRON twice. In the total 28 runs
the dialog system generated 903 exchanges for
the 813 user utterances. Among these exchanges,
34% initiated clarification questions. This result
correlated with the evaluation result of our speech
understanding system which fully understood 65%
of all the user utterances. 18.6% of the exchanges
were Support exchanges created due to execution
failure of the robot control system which corre-
sponds to the performance of the robot control sys-
tem. The average processing time of the dialog
system was 11 msec.
</bodyText>
<sectionHeader confidence="0.999079" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99969625">
In this paper we presented an agent-based dialog
model for HRI. The implemented system enables
multi-modal, mixed-initiative dialog style and is
relatively domain independent. The real-time test-
ing of the system proves its efficiency. We will
work out detailed evaluation metrics for our sys-
tem to be able to draw more general conclusion
about the strength and weakness of our model.
</bodyText>
<sectionHeader confidence="0.996428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955552238806">
J. Allen, D. K. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2001. Towards conversational
human-computer interaction. AI Magazine, 22(4).
K. Aoyama and H. Shimomura. 2005. Real world speech
interaction with a humanoid robot on a layered robot be-
havior control architecture. In Proc. Int. Conf. on Robotics
and Automation.
R. Bischoff and V. Graefe. 2002. Dependable multimodal
communication and interaction with robotic assistants. In
Proc. Int. Workshop on Robot-Human Interactive Commu-
nication (ROMAN).
R. A. Brooks. 1986. A robust layered control system for a
mobile robot. IEEE Journal of Robotics and Automation,
2(1):14–23.
J. E. Cahn and S. E. Brennan. 1999. A psychological model
of grounding and repair in dialog. In Proc. Fall 1999 AAAI
Symposium on Psychological Models of Communication
in Collaborative Systems.
J. Cassell, T. Bickmore, L. Campbell, and H. Vilhjalmsson.
2000. Human conversation as a system framework: De-
signing embodied conversational agents. In J. Cassell,
J. Sullivan, S. Prevost, and E. Churchill, editors, Embod-
ied conversational agents. MIT Press.
H. H. Clark, editor. 1992. Arenas ofLanguage Use. Univer-
sity of Chicago Press.
A. Green, K. Severinson-Eklundh, B. Wrede, and S. Li.
2006. Integrating miscommunication analysis in natural
language interface design for a service robot. In Proc. Int.
Conf. on Intelligent Robots and Systems. submitted.
J. M. Iverson, O. Capirci, E. Longobardi, and M. C. Caselli.
1999. Gesturing in mother-child interactions. Cognitive
Develpment, 14(1):57–75.
W. Levelt. 1989. Speaking: From intention to articulation.
Cambridge, MA: MIT Press.
H. J. Levesque, P. R. Cohen, and J. H. T. Nunnes. 1990. On
acting together. In Proc. Nat. Conf. on Artificial Intelli-
gence (AAAI).
S. Li, A. Haasch, B. Wrede, J. Fritsch, and G. Sagerer.
2005. Human-style interaction with a robot for coopera-
tive learning of scene objects. In Proc. Int. Conf. on Mul-
timodal Interfaces.
T. Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T. Kurita,
I. Hara, and N. Otsu. 1999. Integrated natural spoken
dialogue system of jijo-2 mobile robot for office services,.
In Proc. AAAI Nat. Conf. and Innovative Applications of
Artificial Intelligence Conf.
D. McNeill. 1992. Hand and Mind: What Gesture Reveal
about Thought. University of Chicago Press.
M. F. McTear. 2002. Spoken dialogue technology: enabling
the conversational interface. ACM Computing Surveys,
34(1).
Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 2003.
Towards a model of face-to-face grounding. In Proc. An-
nual Meeting of the Association for Computational Lin-
guistics.
N. Pfleger, J. Alexandersson, and T. Becker. 2003. A ro-
bust and generic discourse model for multimodal dialogue.
In Proc. 3rd Workshop on Knowledge and Reasoning in
Practical Dialogue Systems.
E. A. Schegloff and H. Sacks. 1973. Opening up closings.
Semiotica, pages 289–327.
D. Traum and J. Rickel. 2002. Embodied agents for multi-
party dialogue in immersive virtual world. In Proc. 1st Int.
Conf on Autonomous Agents and Multi-agent Systems.
D. Traum. 1994. A Computational Theory of Grounding in
Natural Language Conversation. Ph.D. thesis, University
of Rochester.
</reference>
<page confidence="0.888695">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864822">
<title confidence="0.975623">A computational model of multi-modal for human robot interaction</title>
<author confidence="0.972852">Shuyin Li</author>
<author confidence="0.972852">Britta Wrede</author>
<author confidence="0.972852">Gerhard</author>
<affiliation confidence="0.9684205">Applied Computer Science, Faculty of Bielefeld University, 33594 Bielefeld,</affiliation>
<email confidence="0.981838">shuyinli,bwrede,sagerer@techfak.uni-bielefeld.de</email>
<abstract confidence="0.998963071428572">Dialog systems for mobile robots operating in the real world should enable mixedinitiative dialog style, handle multi-modal information involved in the communication and be relatively independent of the domain knowledge. Most dialog systems developed for mobile robots today, however, are often system-oriented and have limited capabilities. We present an agentbased dialog model that are specially designed for human-robot interaction and provide evidence for its efficiency with our implemented system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>D K Byron</author>
<author>M Dzikovska</author>
<author>G Ferguson</author>
<author>L Galescu</author>
<author>A Stent</author>
</authors>
<title>Towards conversational human-computer interaction.</title>
<date>2001</date>
<journal>AI Magazine,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="890" citStr="Allen et al., 2001" startWordPosition="122" endWordPosition="125">act Dialog systems for mobile robots operating in the real world should enable mixedinitiative dialog style, handle multi-modal information involved in the communication and be relatively independent of the domain knowledge. Most dialog systems developed for mobile robots today, however, are often system-oriented and have limited capabilities. We present an agentbased dialog model that are specially designed for human-robot interaction and provide evidence for its efficiency with our implemented system. 1 Introduction Natural language is the most intuitive way to communicate for human beings (Allen et al., 2001). It is, therefore, very important to enable dialog capability for personal service robots that should help people in their everyday life. However, the interaction with a robot as a mobile, autonomous device is different than with many other computer controlled devices which affects the dialog modeling. Here we want to first clarify the most essential requirements for dialog management systems for human-robot interaction (HRI) and then outline state-of-the-art dialog modeling approaches to position ourselves. The first requirement results from the situatedness (Brooks, 1986) of HRI. A mobile r</context>
<context position="4159" citStr="Allen et al., 2001" startWordPosition="657" endWordPosition="660">sk steps and can therefore only handle well-structured tasks for which one-side led dialog styles are sufficient. In the agent-based approach, the com153 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 153–160, Sydney, July 2006. c�2006 Association for Computational Linguistics munication is viewed as a collaboration between two intelligent agents. Different approaches inspired by psychology and linguistics are in use within this category. For example, within the TRAINS/TRIPS project several complex dialog systems for collaborative problem solving have been developed (Allen et al., 2001). Here the dialog system is viewed as a conversational agent that performs communicative acts. During a conversation, the dialog system selects the communicative goal based on its current belief about the domain and general conversational obligations. Such systems make use of communication and domain model to enable mixed-initiative dialog style and to handle more complex tasks. In the HRI field, due to the complexity of the overall systems, usually the finite-state-based strategy is employed (Matsui et al., 1999; Bischoff and Graefe, 2002; Aoyama and Shimomura, 2005). As to the issue of multi</context>
</contexts>
<marker>Allen, Byron, Dzikovska, Ferguson, Galescu, Stent, 2001</marker>
<rawString>J. Allen, D. K. Byron, M. Dzikovska, G. Ferguson, L. Galescu, and A. Stent. 2001. Towards conversational human-computer interaction. AI Magazine, 22(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Aoyama</author>
<author>H Shimomura</author>
</authors>
<title>Real world speech interaction with a humanoid robot on a layered robot behavior control architecture.</title>
<date>2005</date>
<booktitle>In Proc. Int. Conf. on Robotics and Automation.</booktitle>
<contexts>
<context position="4733" citStr="Aoyama and Shimomura, 2005" startWordPosition="748" endWordPosition="751">blem solving have been developed (Allen et al., 2001). Here the dialog system is viewed as a conversational agent that performs communicative acts. During a conversation, the dialog system selects the communicative goal based on its current belief about the domain and general conversational obligations. Such systems make use of communication and domain model to enable mixed-initiative dialog style and to handle more complex tasks. In the HRI field, due to the complexity of the overall systems, usually the finite-state-based strategy is employed (Matsui et al., 1999; Bischoff and Graefe, 2002; Aoyama and Shimomura, 2005). As to the issue of multi-modality, one strand of the research concerns the fusion and representation of multimodal information such as (Pfleger et al., 2003) and the other strand focuses on the generalisation of human-like conversational behaviors for virtual agents. In this strand, Cassell (2000) proposes a general architecture for multi-modal conversation and Traum (2002) extends his information-state based dialog model by adding more conversational layers to account for multi-modality. In this paper we present an agent-based dialog model for HRI. As described in section 2, the two main co</context>
</contexts>
<marker>Aoyama, Shimomura, 2005</marker>
<rawString>K. Aoyama and H. Shimomura. 2005. Real world speech interaction with a humanoid robot on a layered robot behavior control architecture. In Proc. Int. Conf. on Robotics and Automation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bischoff</author>
<author>V Graefe</author>
</authors>
<title>Dependable multimodal communication and interaction with robotic assistants.</title>
<date>2002</date>
<booktitle>In Proc. Int. Workshop on Robot-Human Interactive Communication</booktitle>
<location>(ROMAN).</location>
<contexts>
<context position="4704" citStr="Bischoff and Graefe, 2002" startWordPosition="744" endWordPosition="747">stems for collaborative problem solving have been developed (Allen et al., 2001). Here the dialog system is viewed as a conversational agent that performs communicative acts. During a conversation, the dialog system selects the communicative goal based on its current belief about the domain and general conversational obligations. Such systems make use of communication and domain model to enable mixed-initiative dialog style and to handle more complex tasks. In the HRI field, due to the complexity of the overall systems, usually the finite-state-based strategy is employed (Matsui et al., 1999; Bischoff and Graefe, 2002; Aoyama and Shimomura, 2005). As to the issue of multi-modality, one strand of the research concerns the fusion and representation of multimodal information such as (Pfleger et al., 2003) and the other strand focuses on the generalisation of human-like conversational behaviors for virtual agents. In this strand, Cassell (2000) proposes a general architecture for multi-modal conversation and Traum (2002) extends his information-state based dialog model by adding more conversational layers to account for multi-modality. In this paper we present an agent-based dialog model for HRI. As described </context>
</contexts>
<marker>Bischoff, Graefe, 2002</marker>
<rawString>R. Bischoff and V. Graefe. 2002. Dependable multimodal communication and interaction with robotic assistants. In Proc. Int. Workshop on Robot-Human Interactive Communication (ROMAN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Brooks</author>
</authors>
<title>A robust layered control system for a mobile robot.</title>
<date>1986</date>
<journal>IEEE Journal of Robotics and Automation,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1471" citStr="Brooks, 1986" startWordPosition="214" endWordPosition="215">uman beings (Allen et al., 2001). It is, therefore, very important to enable dialog capability for personal service robots that should help people in their everyday life. However, the interaction with a robot as a mobile, autonomous device is different than with many other computer controlled devices which affects the dialog modeling. Here we want to first clarify the most essential requirements for dialog management systems for human-robot interaction (HRI) and then outline state-of-the-art dialog modeling approaches to position ourselves. The first requirement results from the situatedness (Brooks, 1986) of HRI. A mobile robot is situated “here and now” and cohabits the same physical world as the user. Environmental changes can have massive influence on the task execution. For example, a robot should fetch a cup from the kitchen but the door is locked. Under this circumstance the dialog system must support mixedinitiative dialog style to receive user commands on the one side and to report on the perceived environmental changes on the other side. Otherwise the robot had to break up the task execution and there is no way for the user to find out the reason. The second challenge for HRI dialog m</context>
</contexts>
<marker>Brooks, 1986</marker>
<rawString>R. A. Brooks. 1986. A robust layered control system for a mobile robot. IEEE Journal of Robotics and Automation, 2(1):14–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Cahn</author>
<author>S E Brennan</author>
</authors>
<title>A psychological model of grounding and repair in dialog.</title>
<date>1999</date>
<booktitle>In Proc. Fall 1999 AAAI Symposium on Psychological Models of Communication in Collaborative Systems.</booktitle>
<contexts>
<context position="8241" citStr="Cahn and Brennan, 1999" startWordPosition="1311" endWordPosition="1314">raph. However, this implies that the grounding process may never really end (Traum, 1994). The second critical issue is taking contributions as the most basic grounding units. In Clark’s view, the basic grounding unit, i.e., the unit of conversation at which grounding takes place, is the contribution. To provide Acceptance for a contribution agents may need to issue clarification questions or repair. But when modeling a dialog, especially a task-oriented dialog, it is hard to map one single contribution from one agent to a domain task since tasks are always cooperately done by the two agents (Cahn and Brennan, 1999). Traum (1994) addressed the first issue by introducing a finite-state based grounding mechanism and Cahn and Brennan (1999) used “exchanges”’ as the basic grounding unit to tackle the second critical issue. We combine the advantages of their work and present a grounding mechanism based on an augmented push-down automaton as described below. 154 Basic grounding unit: As Cahn and Brennan we take exchange as the most basic grounding unit. An exchange is a pair of contributions initiated by the two conversational agents. They represent the idea of adjacency pairs (Schegloff and Sacks, 1973). The </context>
</contexts>
<marker>Cahn, Brennan, 1999</marker>
<rawString>J. E. Cahn and S. E. Brennan. 1999. A psychological model of grounding and repair in dialog. In Proc. Fall 1999 AAAI Symposium on Psychological Models of Communication in Collaborative Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>T Bickmore</author>
<author>L Campbell</author>
<author>H Vilhjalmsson</author>
</authors>
<title>Human conversation as a system framework: Designing embodied conversational agents.</title>
<date>2000</date>
<editor>In J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors, Embodied conversational agents.</editor>
<publisher>MIT Press.</publisher>
<marker>Cassell, Bickmore, Campbell, Vilhjalmsson, 2000</marker>
<rawString>J. Cassell, T. Bickmore, L. Campbell, and H. Vilhjalmsson. 2000. Human conversation as a system framework: Designing embodied conversational agents. In J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors, Embodied conversational agents. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>editor</author>
</authors>
<title>Arenas ofLanguage Use.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<marker>Clark, editor, 1992</marker>
<rawString>H. H. Clark, editor. 1992. Arenas ofLanguage Use. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Green</author>
<author>K Severinson-Eklundh</author>
<author>B Wrede</author>
<author>S Li</author>
</authors>
<title>Integrating miscommunication analysis in natural language interface design for a service robot.</title>
<date>2006</date>
<booktitle>In Proc. Int. Conf. on Intelligent Robots and Systems. submitted.</booktitle>
<contexts>
<context position="32450" citStr="Green et al., 2006" startWordPosition="5320" endWordPosition="5323">rds? U4: This is a mug! (looking puzzled) (looking friendly) (looking sad) Ex4 Ex5 Ex3 Ex6 159 large number of modules that claim plenty of processing time or are subject to environmental conditions. For the dialog system, this means that the correct interpretation and transaction of user utterances is by no means a guaranty for a prompt response or successful task execution. Thus, the performance of the dialog system can not be directly measured with the performance of the overall system like most desktop dialog applications. We are still working at evaluation metrics for HRI dialog systems (Green et al., 2006). But the efficiency of our system is already visible in the small effort associated with the porting of this system to another robot platform and in the pilot user study with BIRON. In this study, each of the 14 users interacted with BIRON twice. In the total 28 runs the dialog system generated 903 exchanges for the 813 user utterances. Among these exchanges, 34% initiated clarification questions. This result correlated with the evaluation result of our speech understanding system which fully understood 65% of all the user utterances. 18.6% of the exchanges were Support exchanges created due </context>
</contexts>
<marker>Green, Severinson-Eklundh, Wrede, Li, 2006</marker>
<rawString>A. Green, K. Severinson-Eklundh, B. Wrede, and S. Li. 2006. Integrating miscommunication analysis in natural language interface design for a service robot. In Proc. Int. Conf. on Intelligent Robots and Systems. submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Iverson</author>
<author>O Capirci</author>
<author>E Longobardi</author>
<author>M C Caselli</author>
</authors>
<title>Gesturing in mother-child interactions.</title>
<date>1999</date>
<journal>Cognitive Develpment,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="18585" citStr="Iverson et al. (1999)" startWordPosition="2999" endWordPosition="3002">d “idea unit” and are co-expressive. Since semantic representation is created out of communicative intentions (Levelt, 1989) we assume the communication intentions are the modality independent base that governs the multi-modal language production. We, therefore, extend our structure above by introducing two generators on the conversation layer: one verbal and one non-verbal generator that represent the verbal and non-verbal language generation mechanism based on the communication intentions created on the intention layer. The relationship between these two generators is variable. For example, Iverson et al. (1999) identified three types of informational relationship between speech and gesture: reinforcement (gesture reinforces the message conveyed in speech, e.g., emphatic gesture), disambiguation (gesture serves as the precise referent of the speech, e.g., deictic gesture accompanying the utterance “this cup”), and adding-information (e.g., saying “The ball is so big.” and shaping the size with hands). In our work, when processing users’ multi-modal contributions we focus on the disambiguation relation; when creating multimodal contributions for the robot we are also interested in other informational </context>
</contexts>
<marker>Iverson, Capirci, Longobardi, Caselli, 1999</marker>
<rawString>J. M. Iverson, O. Capirci, E. Longobardi, and M. C. Caselli. 1999. Gesturing in mother-child interactions. Cognitive Develpment, 14(1):57–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Levelt</author>
</authors>
<title>Speaking: From intention to articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="16193" citStr="Levelt (1989)" startWordPosition="2668" endWordPosition="2669">ptance. For that we need to first look at the structure of individual contributions more closely in the next subsection. 2.2 The structure of agents’ contributions To represent the structure of the individual contributions we take into account the whole language generation process which enables us to come up with a powerful solution as described below. The layers of a contribution: What we can observe in a conversation are only exchanges of agents’ contributions in verbal or non-verbal form. But in fact the contributions are the end-product of a complex cognitive process: language production. Levelt (1989) identified three phases of language production: conceptualization, formulation, and articulation. The production of an utterance starts from the conception of a communicative intention and the semantic organization in the conceptualization phase before the utterance can be formulated and articulated in the next two phases. Intentions can arise from the previous discourse or from other motivations such as needs for help or information. This finding motivates us to set up a two-layered structure of contributions. One layer is the so-called intention layer where communication intentions are conc</context>
<context position="18088" citStr="Levelt, 1989" startWordPosition="2928" endWordPosition="2929"> synthesizer to generate acoustic output which replaces the articulation process, only formulation is performed on this layer. I:deleteP; S:push(ex) I:defaultP S:push(ex) I:acc IcorrectP; S:push(ex) S:pop(ex) Default Top I:acc; S:pop(ex); A:correct(IPE) I:acc; S:pop(ex) I:defaultP; S:pop(all)&amp;push(ex) Correct Top Start Support Top I:supportP; S:push(ex) I:acc; S:pop(ex) Delete Top I[supportP|correctP| deleteP; S: I:acc S:pop(ex) S:push(ex) I:supportP I:deleteP; S:push(ex) 156 so-called “idea unit” and are co-expressive. Since semantic representation is created out of communicative intentions (Levelt, 1989) we assume the communication intentions are the modality independent base that governs the multi-modal language production. We, therefore, extend our structure above by introducing two generators on the conversation layer: one verbal and one non-verbal generator that represent the verbal and non-verbal language generation mechanism based on the communication intentions created on the intention layer. The relationship between these two generators is variable. For example, Iverson et al. (1999) identified three types of informational relationship between speech and gesture: reinforcement (gestur</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>W. Levelt. 1989. Speaking: From intention to articulation. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Levesque</author>
<author>P R Cohen</author>
<author>J H T Nunnes</author>
</authors>
<title>On acting together.</title>
<date>1990</date>
<booktitle>In Proc. Nat. Conf. on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="22253" citStr="Levesque et al. (1990)" startWordPosition="3592" endWordPosition="3595">oint at the end of the section 2.1: when to create an IU as Presentation and when an IU as Acceptance. Self-motivated intentions usually trigger the creation of an IU as Presentation with Default relation to its IPE. For example, if the robot needs to report something to the user it can create a Default exchange by generating an IU as its Presentation. The user is then expected to signal her Acceptance. Other-motivated intentions can, according to the context, result in either Presentation or Acceptance. To make the correct decision we developed criteria based on the joint intention theory of Levesque et al. (1990) which predicts that during a collaboration the partners are committed to a joint goal that they will always try to conform till they reach the goal or give up. Note, this does not mean that one will always agree with her interlocutor, but they will behave in the way that they think is the best to achieve the goal. This theory can be applied to human-robot dialog in a twofold sense: Firstly, a dialog can be generally seen as a collaboration as Clark proposed. Secondly, the human-robot dialog is mostly task-oriented, i.e., verbal generator intention conception -Intention Layernon-verbal generat</context>
</contexts>
<marker>Levesque, Cohen, Nunnes, 1990</marker>
<rawString>H. J. Levesque, P. R. Cohen, and J. H. T. Nunnes. 1990. On acting together. In Proc. Nat. Conf. on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>A Haasch</author>
<author>B Wrede</author>
<author>J Fritsch</author>
<author>G Sagerer</author>
</authors>
<title>Human-style interaction with a robot for cooperative learning of scene objects.</title>
<date>2005</date>
<booktitle>In Proc. Int. Conf. on Multimodal Interfaces.</booktitle>
<contexts>
<context position="31163" citStr="Li et al., 2005" startWordPosition="5087" endWordPosition="5090">igns an ID to the image and stores it in the memory. After the Dialog Manager receives this ID, the processing of the conversation layer of the user IU ends, the Dialog Manager proceeds to create its own IU to react to the user’s IU. Problems with the object identification indicate failure of the intention recognition process on the user conversation layer. In this case, the Dialog Manager creates a Support exchange to ask the user which object she refers to and retries it if she does not oppose (R5-R7). This process and the associated multimodality fusion and representation are described in (Li et al., 2005) in detail. The evaluation of dialog systems for human robot interaction is still an open issue. A robot system is usually a complex system including a (looking sad) to it with your finger? U5: Yes! R6: I know it is sometimes difficult with me, Ex7 U6: (laugh) (looking ashamed) but don’t feel discouraged! (looking happily) R7: OK, I’ve found it, it is really nice! R1: Hello human! (looking friendly) Ex1 U1: Hello! Who are you? R5: Sorry, I can’t find it. Are you pointing Ex2 R2: My name is BIRON, if you show me something I can remember it. U2: OK, um, look, this is a mug. R3: I beg your pardon</context>
</contexts>
<marker>Li, Haasch, Wrede, Fritsch, Sagerer, 2005</marker>
<rawString>S. Li, A. Haasch, B. Wrede, J. Fritsch, and G. Sagerer. 2005. Human-style interaction with a robot for cooperative learning of scene objects. In Proc. Int. Conf. on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsui</author>
<author>H Asoh</author>
<author>J Fry</author>
<author>Y Motomura</author>
<author>F Asano</author>
<author>T Kurita</author>
<author>I Hara</author>
<author>N Otsu</author>
</authors>
<title>Integrated natural spoken dialogue system of jijo-2 mobile robot for office services,.</title>
<date>1999</date>
<booktitle>In Proc. AAAI Nat. Conf. and Innovative Applications of Artificial Intelligence Conf.</booktitle>
<contexts>
<context position="4677" citStr="Matsui et al., 1999" startWordPosition="740" endWordPosition="743">ral complex dialog systems for collaborative problem solving have been developed (Allen et al., 2001). Here the dialog system is viewed as a conversational agent that performs communicative acts. During a conversation, the dialog system selects the communicative goal based on its current belief about the domain and general conversational obligations. Such systems make use of communication and domain model to enable mixed-initiative dialog style and to handle more complex tasks. In the HRI field, due to the complexity of the overall systems, usually the finite-state-based strategy is employed (Matsui et al., 1999; Bischoff and Graefe, 2002; Aoyama and Shimomura, 2005). As to the issue of multi-modality, one strand of the research concerns the fusion and representation of multimodal information such as (Pfleger et al., 2003) and the other strand focuses on the generalisation of human-like conversational behaviors for virtual agents. In this strand, Cassell (2000) proposes a general architecture for multi-modal conversation and Traum (2002) extends his information-state based dialog model by adding more conversational layers to account for multi-modality. In this paper we present an agent-based dialog m</context>
</contexts>
<marker>Matsui, Asoh, Fry, Motomura, Asano, Kurita, Hara, Otsu, 1999</marker>
<rawString>T. Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T. Kurita, I. Hara, and N. Otsu. 1999. Integrated natural spoken dialogue system of jijo-2 mobile robot for office services,. In Proc. AAAI Nat. Conf. and Innovative Applications of Artificial Intelligence Conf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and Mind: What Gesture Reveal about Thought.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="17366" citStr="McNeill (1992)" startWordPosition="2836" endWordPosition="2837">r where communication intentions are conceived. For a robot the communication intentions come from the analysis of the previous discourse or from the robot control system. The other layer is the conversation layer. The communication intentions are formulated and articulated here1. These two layers represent the intention conception and the language generation process, respectively. We term this two-layered structure of contribution interaction unit (IU). The issue of multi-modality: Face-to-face conversations are multi-modal. Speech and body language (e.g., gesture) can happen simultaneously. McNeill (1992) stated that gesture and speech arise from the same semantic source, the 1Since most robot systems use speech synthesizer to generate acoustic output which replaces the articulation process, only formulation is performed on this layer. I:deleteP; S:push(ex) I:defaultP S:push(ex) I:acc IcorrectP; S:push(ex) S:pop(ex) Default Top I:acc; S:pop(ex); A:correct(IPE) I:acc; S:pop(ex) I:defaultP; S:pop(all)&amp;push(ex) Correct Top Start Support Top I:supportP; S:push(ex) I:acc; S:pop(ex) Delete Top I[supportP|correctP| deleteP; S: I:acc S:pop(ex) S:push(ex) I:supportP I:deleteP; S:push(ex) 156 so-called </context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>D. McNeill. 1992. Hand and Mind: What Gesture Reveal about Thought. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F McTear</author>
</authors>
<title>Spoken dialogue technology: enabling the conversational interface.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="3364" citStr="McTear (2002)" startWordPosition="543" endWordPosition="544">nce a personal service robot is intended to help human in their individual household it is impossible to hard-code all the knowledge it will need into the system, e.g., where the cup is and what should be served for lunch. Thus, it is essential for such a robot to be able to learn new knowledge and tasks. This ability, however, has the implication for the dialog system that it can not rely on comprehensive, hard-coded knowledge to do dialog planning. Instead, it must be designed in a way that it has a loose relationship with the domain knowledge. Many dialog modeling approaches already exist. McTear (2002) classified them into three main types: finite state-based, frame-based, and agentbased. In the first two approaches the dialog structure is closely coupled with pre-defined task steps and can therefore only handle well-structured tasks for which one-side led dialog styles are sufficient. In the agent-based approach, the com153 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 153–160, Sydney, July 2006. c�2006 Association for Computational Linguistics munication is viewed as a collaboration between two intelligent agents. Different approaches inspired by psychology and </context>
</contexts>
<marker>McTear, 2002</marker>
<rawString>M. F. McTear. 2002. Spoken dialogue technology: enabling the conversational interface. ACM Computing Surveys, 34(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y I Nakano</author>
<author>G Reinstein</author>
<author>T Stocky</author>
<author>J Cassell</author>
</authors>
<title>Towards a model of face-to-face grounding.</title>
<date>2003</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2338" citStr="Nakano et al., 2003" startWordPosition="364" endWordPosition="367">ocked. Under this circumstance the dialog system must support mixedinitiative dialog style to receive user commands on the one side and to report on the perceived environmental changes on the other side. Otherwise the robot had to break up the task execution and there is no way for the user to find out the reason. The second challenge for HRI dialog management is the embodiment of a robot which changes the way of interaction. Empirical studies show that the visual access to the interlocutor’s body affects the conversation in the way that non-verbal behaviors are used as communicative signals (Nakano et al., 2003). For example, to refer to a cup that is visible to both dialog partners, the speaker tends to say “this cup” while pointing to it. The same strategy is considerably ineffective during a phone call. This example shows, an HRI dialog system must account for multi-modal communication. The third, probably the unique challenge for HRI dialog management is the implication of the learning ability of such a robot. Since a personal service robot is intended to help human in their individual household it is impossible to hard-code all the knowledge it will need into the system, e.g., where the cup is a</context>
</contexts>
<marker>Nakano, Reinstein, Stocky, Cassell, 2003</marker>
<rawString>Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 2003. Towards a model of face-to-face grounding. In Proc. Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pfleger</author>
<author>J Alexandersson</author>
<author>T Becker</author>
</authors>
<title>A robust and generic discourse model for multimodal dialogue.</title>
<date>2003</date>
<booktitle>In Proc. 3rd Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="4892" citStr="Pfleger et al., 2003" startWordPosition="775" endWordPosition="778">tion, the dialog system selects the communicative goal based on its current belief about the domain and general conversational obligations. Such systems make use of communication and domain model to enable mixed-initiative dialog style and to handle more complex tasks. In the HRI field, due to the complexity of the overall systems, usually the finite-state-based strategy is employed (Matsui et al., 1999; Bischoff and Graefe, 2002; Aoyama and Shimomura, 2005). As to the issue of multi-modality, one strand of the research concerns the fusion and representation of multimodal information such as (Pfleger et al., 2003) and the other strand focuses on the generalisation of human-like conversational behaviors for virtual agents. In this strand, Cassell (2000) proposes a general architecture for multi-modal conversation and Traum (2002) extends his information-state based dialog model by adding more conversational layers to account for multi-modality. In this paper we present an agent-based dialog model for HRI. As described in section 2, the two main contributions of this model are the new modeling approach of Clark’s grounding mechanism and the extension of this model to handle multimodal grounding. In secti</context>
</contexts>
<marker>Pfleger, Alexandersson, Becker, 2003</marker>
<rawString>N. Pfleger, J. Alexandersson, and T. Becker. 2003. A robust and generic discourse model for multimodal dialogue. In Proc. 3rd Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Schegloff</author>
<author>H Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<pages>289--327</pages>
<contexts>
<context position="8835" citStr="Schegloff and Sacks, 1973" startWordPosition="1409" endWordPosition="1412"> agents (Cahn and Brennan, 1999). Traum (1994) addressed the first issue by introducing a finite-state based grounding mechanism and Cahn and Brennan (1999) used “exchanges”’ as the basic grounding unit to tackle the second critical issue. We combine the advantages of their work and present a grounding mechanism based on an augmented push-down automaton as described below. 154 Basic grounding unit: As Cahn and Brennan we take exchange as the most basic grounding unit. An exchange is a pair of contributions initiated by the two conversational agents. They represent the idea of adjacency pairs (Schegloff and Sacks, 1973). The first contribution of the exchange is the Presentation and the second contribution is the Acceptance, e.g., if one asks a question and the other answers it, then the question is the Presentation and the answer is the Acceptance. In our model, a contribution only represents one speech act. For example, if an agent says “Hello, my name is Tom, what is your name?” this utterances is segmented into three Presentations (a greeting, a statement, and a question) although they occur in one turn. These three Presentations initiate three exchanges and each of them needs to be accepted by the inter</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>E. A. Schegloff and H. Sacks. 1973. Opening up closings. Semiotica, pages 289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>J Rickel</author>
</authors>
<title>Embodied agents for multiparty dialogue in immersive virtual world.</title>
<date>2002</date>
<booktitle>In Proc. 1st Int. Conf on Autonomous Agents and Multi-agent Systems.</booktitle>
<marker>Traum, Rickel, 2002</marker>
<rawString>D. Traum and J. Rickel. 2002. Embodied agents for multiparty dialogue in immersive virtual world. In Proc. 1st Int. Conf on Autonomous Agents and Multi-agent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
</authors>
<title>A Computational Theory of Grounding in Natural Language Conversation.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="7707" citStr="Traum, 1994" startWordPosition="1226" endWordPosition="1227">s become a part of their common ground if this evidence is available. Although this well established theory provides comprehensive insight into human conversation two issues in this theory remain critical when being applied to model dialog. The first one is the recursivity of Acceptance. Clark claimed, since everything said by one agent needs to be understood by her interlocutor, each Acceptance should also play the role of Presentation which needs to be accepted, too. The contributions are thus to be organized as a graph. However, this implies that the grounding process may never really end (Traum, 1994). The second critical issue is taking contributions as the most basic grounding units. In Clark’s view, the basic grounding unit, i.e., the unit of conversation at which grounding takes place, is the contribution. To provide Acceptance for a contribution agents may need to issue clarification questions or repair. But when modeling a dialog, especially a task-oriented dialog, it is hard to map one single contribution from one agent to a domain task since tasks are always cooperately done by the two agents (Cahn and Brennan, 1999). Traum (1994) addressed the first issue by introducing a finite-s</context>
</contexts>
<marker>Traum, 1994</marker>
<rawString>D. Traum. 1994. A Computational Theory of Grounding in Natural Language Conversation. Ph.D. thesis, University of Rochester.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>