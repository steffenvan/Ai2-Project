<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000797">
<title confidence="0.9986335">
Sample Selection for Statistical Parsers: Cognitively Driven
Algorithms and Evaluation Measures
</title>
<author confidence="0.956838">
Roi Reichart
</author>
<affiliation confidence="0.912859">
ICNC
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.993642">
roiri@cs.huji.ac.il
</email>
<author confidence="0.992116">
Ari Rappoport
</author>
<affiliation confidence="0.998897">
Institute of computer science
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.992303">
arir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.993742" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999491111111111">
Creating large amounts of manually annotated
training data for statistical parsers imposes
heavy cognitive load on the human annota-
tor and is thus costly and error prone. It
is hence of high importance to decrease the
human efforts involved in creating training
data without harming parser performance. For
constituency parsers, these efforts are tradi-
tionally evaluated using the total number of
constituents (TC) measure, assuming uniform
cost for each annotated item. In this paper, we
introduce novel measures that quantify aspects
of the cognitive efforts of the human annota-
tor that are not reflected by the TC measure,
and show that they are well established in the
psycholinguistic literature. We present a novel
parameter based sample selection approach
for creating good samples in terms of these
measures. We describe methods for global op-
timisation of lexical parameters of the sam-
ple based on a novel optimisation problem, the
constrained multiset multicover problem, and
for cluster-based sampling according to syn-
tactic parameters. Our methods outperform
previously suggested methods in terms of the
new measures, while maintaining similar TC
performance.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944023255814">
State of the art statistical parsers require large
amounts of manually annotated data to achieve good
performance. Creating such data imposes heavy
cognitive load on the human annotator and is thus
costly and error prone. Statistical parsers are ma-
jor components in NLP applications such as QA
(Kwok et al., 2001), MT (Marcu et al., 2006) and
SRL (Toutanova et al., 2005). These often oper-
ate over the highly variable Web, which consists of
texts written in many languages and genres. Since
the performance of parsers markedly degrades when
training and test data come from different domains
(Lease and Charniak, 2005), large amounts of train-
ing data from each domain are required for using
them effectively. Thus, decreasing the human efforts
involved in creating training data for parsers without
harming their performance is of high importance.
In this paper we address this problem through
sample selection: given a parsing algorithm and a
large pool of unannotated sentences S, select a sub-
set S1 ⊂ S for human annotation such that the hu-
man efforts in annotating S1 are minimized while
the parser performance when trained with this sam-
ple is maximized.
Previous works addressing training sample size
vs. parser performance for constituency parsers
(Section 2) evaluated training sample size using the
total number of constituents (TC). Sentences differ
in length and therefore in annotation efforts, and it
has been argued (see, e.g, (Hwa, 2004)) that TC re-
flects the number of decisions the human annotator
makes when syntactically annotating the sample, as-
suming uniform cost for each decision.
In this paper we posit that important aspects of
the efforts involved in annotating a sample are not
reflected by the TC measure. Since annotators ana-
lyze sentences rather than a bag of constituents, sen-
tence structure has a major impact on their cognitive
efforts. Sizeable psycholinguistic literature points
to the connection between nested structures in the
syntactic structure of a sentence and its annotation
efforts. This has motivated us to introduce (Sec-
tion 3) three sample size measures, the total and av-
</bodyText>
<page confidence="0.992927">
3
</page>
<note confidence="0.98986">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 3–11,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999400235294117">
erage number of nested structures of degree k in the
sample, and the average number of constituents per
sentence in the sample.
Active learning algorithms for sample selection
focus on sentences that are difficult for the parsing
algorithm when trained with the available training
data (Section 2). In Section 5 we show that active
learning samples contain a high number of complex
structures, much higher than their number in a ran-
domly selected sample that achieves the same parser
performance level. To avoid that, we introduce (Sec-
tion 4) a novel parameter based sample selection
(PBS) approach which aims to select a sample that
enables good estimation of the model parameters,
without focusing on difficult sentences. In Section 5
we show that the methods derived from our approach
select substantially fewer complex structures than
active learning methods and the random baseline.
We propose two different methods. In cluster
based sampling (CBS), we aim to select a sample
in which the distribution of the model parameters is
similar to their distribution in the whole unlabelled
pool. To do that we build a vector representation for
each sentence in the unlabelled pool reflecting the
distribution of the model parameters in this sentence,
and use a clustering algorithm to divide these vectors
into clusters. In the second method we use the fact
that a sample containing many examples of a certain
parameter yields better estimation of this parameter.
If this parameter is crucial for model performance
and the selection process does not harm the distri-
bution of other parameters, then the selected sam-
ple is of high quality. To select such a sample we
introduce a reduction between this selection prob-
lem and a variant of the NP-hard multiset-multicover
problem (Hochbaum, 1997). We call this problem
the constrained multiset multicover (CMM) problem,
and present an algorithm to approximate it.
We experiment (Section 5) with the WSJ Pen-
nTreebank (Marcus et al., 1994) and Collins’ gen-
erative parser (Collins, 1999), as in previous work.
We show that PBS algorithms achieve good results
in terms of both the traditional TC measure (signifi-
cantly better than the random selection baseline and
similar to the results of the state of the art tree en-
tropy (TE) method of (Hwa, 2004)) and our novel
cognitively driven measures (where PBS algorithms
significantly outperform both TE and the random
baseline). We thus argue that PBS provides a way to
select a sample that imposes reduced cognitive load
on the human annotator.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99986195">
Previous work on sample selection for statistical
parsers applied active learning (AL) (Cohn and Lad-
ner, 1994) to corpora of various languages and syn-
tactic annotation schemes and to parsers of different
performance levels. In order to be able to compare
our results to previous work targeting high parser
performance, we selected the corpus and parser
used by the method reporting the best results (Hwa,
2004), WSJ and Collins’ parser.
Hwa (2004) used uncertainty sampling with the
tree entropy (TE) selection function1 to select train-
ing samples for the Collins parser. In each it-
eration, each of the unlabelled pool sentences is
parsed by the parsing model, which outputs a list
of trees ranked by their probabilities. The scored
list is treated as a random variable and the sentences
whose variable has the highest entropy are selected
for human annotation. Sample size was measured
in TC and ranged from 100K to 700K WSJ con-
stituents. The initial size of the unlabelled pool was
800K constituents (the 40K sentences of sections 2-
21 of WSJ). A detailed comparison between the re-
sults of TE and our methods is given in Section 5.
The following works addressed the task of sam-
ple selection for statistical parsers, but in signifi-
cantly different experimental setups. Becker and
Osborne (2005) addressed lower performance lev-
els of the Collins parser. Their uncertainty sam-
pling protocol combined bagging with the TE func-
tion, achieving a 32% TC reduction for reaching a
parser f-score level of 85.5%. The target sample size
set contained a much smaller number of sentences
(∼5K) than ours. Baldridge and Osborne (2004) ad-
dressed HPSG parse selection using a feature based
log-linear parser, the Redwoods corpus and commit-
tee based active learning, obtaining 80% reduction
in annotation cost. Their annotation cost measure
was related to the number of possible parses of the
sentence. Tang et al. (2002) addressed a shallow
parser trained on a semantically annotated corpus.
</bodyText>
<footnote confidence="0.989515">
1Hwa explored several functions in the experimental setup
used in the present work, and TE gave the best results.
</footnote>
<page confidence="0.996926">
4
</page>
<bodyText confidence="0.998905884615385">
They used an uncertainty sampling protocol, where
in each iteration the sentences of the unlabelled pool
are clustered using a distance measure defined on
parse trees to a predefined number of clusters. The
most uncertain sentences are selected from the clus-
ters, the training taking into account the densities of
the clusters. They reduced the number of training
sentences required for their parser to achieve its best
performance from 1300 to 400.
The importance of cognitively driven measures of
sentences’ syntactic complexity has been recognized
by Roark et al. (2007) who demonstrated their utility
for mild cognitive impairment diagnosis. Zhu et al.
(2008) used a clustering algorithm for sampling the
initial labeled set in an AL algorithm for word sense
disambiguation and text classification. In contrast to
our CBS method, they proceeded with iterative un-
certainty AL selection. Melville et al. (2005) used
parameter-based sample selection for a classifier in
a classic active learning setting, for a task very dif-
ferent from ours.
Sample selection has been applied to many NLP
applications. Examples include base noun phrase
chunking (Ngai, 2000), named entity recognition
(Tomanek et al., 2007) and multi–task annotation
(Reichart et al., 2008).
</bodyText>
<sectionHeader confidence="0.989177" genericHeader="method">
3 Cognitively Driven Evaluation Measures
</sectionHeader>
<bodyText confidence="0.999927277777778">
While the resources, capabilities and constraints of
the human parser have been the subject of extensive
research, different theories predict different aspects
of its observed performance. We focus on struc-
tures that are widely agreed to impose a high cog-
nitive load on the human annotator and on theories
considering the cognitive resources required in pars-
ing a complete sentence. Based on these, we derive
measures for the cognitive load on the human parser
when syntactically annotating a set of sentences.
Nested structures. A nested structure is a parse
tree node representing a constituent created while
another constituent is still being processed (‘open’).
The degree K of a nested structure is the number of
such open constituents. In this paper, we enumer-
ate the constituents in a top-down left-right order,
and thus when a constituent is created, only its an-
cestors are processed2. A constituent is processed
</bodyText>
<footnote confidence="0.569461">
2A good review on node enumeration of the human parser
in given in (Abney and Johnson, 1991).
</footnote>
<note confidence="0.342455">
Lotus
</note>
<figureCaption confidence="0.999577">
Figure 1: An example parse tree.
</figureCaption>
<bodyText confidence="0.999962948717949">
until the processing of its children is completed. For
example, in Figure 1, when the constituent NP3 is
created, it starts a nested structure of degree 2, since
two levels of its ancestors (VP, S) are still processed.
Its parent (VP) starts a nested structure of degree 1.
The difficulty of deeply nested structures for the
human parser is well established in the psycholin-
guistics literature. We review here some of the vari-
ous explanations of this phenomenon; for a compre-
hensive review see (Gibson, 1998).
According to the classical stack overflow theory
(Chomsky and Miller, 1963) and its extension, the
incomplete syntactic/thematic dependencies theory
(Gibson, 1991), the human parser should track the
open structures in its short term memory. When the
number of these structures is too large or when the
structures are nested too deeply, the short term mem-
ory fails to hold them and the sentence becomes un-
interpretable.
According to the perspective shifts theory
(MacWhinney, 1982), processing deeply nested
structures requires multiple shifts of the annotator
perspective and is thus more difficult than process-
ing shallow structures. The difficulty of deeply
nested structured has been demonstrated for many
languages (Gibson, 1998).
We thus propose the total number of nested struc-
tures of degree K in a sample (TNSK) as a measure
of the cognitive efforts that its annotation requires.
The higher K is, the more demanding the structure.
Sentence level resources. In the psycholinguis-
tic literature of sentence processing there are many
theories describing the cognitive resources required
during a complete sentence processing. These re-
sources might be allocated during the processing of
a certain word and are needed long after its con-
stituent is closed. We briefly discuss two lines of
theory, focusing on their predictions that sentences
consisting of a large number of structures (e.g., con-
</bodyText>
<figure confidence="0.996986285714286">
NP1
NP2
VP
JJ
NNP
NN
NP3
VBD
Last
week
IBM
bought
NNP
S
</figure>
<page confidence="0.901072">
5
</page>
<bodyText confidence="0.999618395348837">
stituents or nested structures) require more cognitive
resources for longer periods.
Levelt (2001) suggested a layered model of the
mental lexicon organization, arguing that when one
hears or reads a sentence s/he activates word forms
(lexemes) that in turn activate lemma information.
The lemma information contains information about
syntactic properties of the word (e.g., whether it is
a noun or a verb) and about the possible sentence
structures that can be generated given that word. The
process of reading words and retrieving their lemma
information is incremental and the lemma informa-
tion for a given word is used until its syntactic struc-
ture is completed. The information about a word in-
clude all syntactic predictions, obligatory (e.g., the
prediction of a noun following a determiner) and op-
tional (e.g., optional arguments of a verb, modifier
relationships). This information might be relevant
long after the constituents containing the word are
closed, sometimes till the end of the sentence.
Another line of research focuses on working
memory, emphasizing the activation decay princi-
ple. It stresses that words and structures perceived
during sentence processing are forgotten over time.
As the distance between two related structures in a
sentence grows, it is more demanding to reactivate
one when seeing the other. Indeed, supported by
a variety of observations, many of the theories of
the human parser (see (Lewis et al., 2006) for a sur-
vey) predict that processing items towards the end of
longer sentences should be harder, since they most
often have to be integrated with items further back.
Thus, sentences with a large number of structures
impose a special cognitive load on the annotator.
We thus propose to use the number of structures
(constituents or nested structures) in a sentence as a
measure of its difficulty for human annotation. The
measures we use for a sample (a sentence set) are the
average number of constituents (AC) and the aver-
age number of nested structures of degree k (ANSK)
per sentence in the set. Higher AC or ANSK values
of a set imply higher annotation requirements3.
Pschycolinguistics research makes finer observa-
</bodyText>
<footnote confidence="0.9980228">
3The correlation between the number of constituents and
sentence length is very strong (e.g., correlation coefficient of
0.93 in WSJ section 0). We could use the number of words, but
we prefer the number of structures since the latter better reflects
the arguments made in the literature.
</footnote>
<bodyText confidence="0.9968254">
tions about the human parser than those described
here. A complete survey of that literature is beyond
the scope of this paper. We consider the proposed
measures a good approximation of some of the hu-
man parser characteristics.
</bodyText>
<sectionHeader confidence="0.984773" genericHeader="method">
4 Parameter Based Sampling (PBS)
</sectionHeader>
<bodyText confidence="0.996508818181818">
Our approach is to sample the unannotated pool with
respect to the distribution of the model parameters
in its sentences. In this paper, in order to compare to
previous works, we apply our methods to the Collins
generative parser (Collins, 1999). For any sentence
s and parse tree t it assigns a probability p(s, t),
and finds the tree for which this probability is maxi-
mized. To do that, it writes p(t, s) as a product of the
probabilities of the constituents in t and decomposes
the latter using the chain rule. In simplified notation,
it uses:
</bodyText>
<equation confidence="0.9984175">
p(t, s) = P(S1 → S2 . . . Sn) = 1 1 P(S1)·. . .· P(Sn  |O(S1 . . . Sn))
11 (1)
</equation>
<bodyText confidence="0.999895407407407">
We refer to the conditional probabilities as the model
parameters.
Cluster Based Sampling (CBS). We describe
here a method for sampling subsets that leads to a
parameter estimation that is similar to the parame-
ter estimation we would get if annotating the whole
unannotated set.
To do that, we randomly select M sentences from
the unlabelled pool N, manually annotate them,
train the parser with these sentences and parse the
rest of the unlabelled pool (G = N − M). Using
this annotation we build a syntactic vector repre-
sentation for each sentence in G. We then cluster
these sentences and sample the clusters with respect
to their weights to preserve the distribution of the
syntactic features. The selected sentences are man-
ually annotated and combined with the group of M
sentences to train the final parser. The size of this
combined sample is measured when the annotation
efforts are evaluated.
Denote the left hand side nonterminal of a con-
stituent by P and the unlexicalized head of the con-
stituent by H. The domain of P is the set of non-
terminals (excluding POS tags) and the domain of H
is the set of nonterminals and POS tags of WSJ. In
all the parameters of the Collins parser P and H are
conditioned upon. We thus use (P, H) pairs as the
</bodyText>
<page confidence="0.995881">
6
</page>
<bodyText confidence="0.996212">
features in the vector representation of each sentence
in G. The i-th coordinate is given by the equation:
</bodyText>
<equation confidence="0.9996035">
X X Fi(Q(c) == i) · L(c) (2)
c∈t(s) i
</equation>
<bodyText confidence="0.998115216216216">
Where c are the constituents of the sentence parse
t(s), Q is a function that returns the (P, H) pair
of the constituent c, Fi is a predicate that returns 1
iff it is given pair number i as an argument and 0
otherwise, and L is the number of modifying non-
terminals in the constituent plus 1 (for the head),
counting the number of parameters that condition
on (P, H). Following equation (2), the ith coordi-
nate of the vector representation of a sentence in G
contains the number of parameters that will be cal-
culated conditioned on the ith (P, H) pair.
We use the k-means clustering algorithm, with the
L2 norm as a distance metric (MacKay, 2002), to di-
vide vectors into clusters. Clusters created by this
algorithm contain adjacent vectors in a Euclidean
space. Clusters represent sentences with similar fea-
tures values. To initialize k-means, we sample the
initial centers values from a uniform distribution
over the data points.
We do not decide on the number of clusters in ad-
vance but try to find inherent structure in the data.
Several methods for estimating the ‘correct’ num-
ber of clusters are known (Milligan and Cooper,
1985). We used a statistical heuristic called the
elbow test. We define the ‘within cluster disper-
sion’ Wk as follows. Suppose that the data is di-
vided into k clusters C1 ... Ck with |Cj |points in
the jth cluster. Let Dt = Pi,jcCt di,j where
di,j is the squared Euclidean distance, then Wk :=
Pt= 1 2|Ct |Dt. Wk tends to decrease monotonically
as k increases. In many cases, from some k this de-
crease flattens markedly. The heuristic is that the
location of such an ‘elbow’ indicates the appropriate
number of clusters. In our experiments, an obvious
elbow occurred for 15 clusters.
ki sentences are randomly sampled from each
cluster, ki = D  |Ci  |where D is the number
</bodyText>
<subsectionHeader confidence="0.517721">
Pj |Cj|
</subsectionHeader>
<bodyText confidence="0.980891924528302">
of sentences to be sampled from G. That way we
ensure that in the final sample each cluster is repre-
sented according to its size.
CMM Sampling. All of the parameters in the
Collins parser are conditioned on the constituent’s
head word. Since word statistics are sparse, sam-
pling from clusters created according to a lexical
vector representation of the sentences does not seem
promising4.
Another way to create a sample from which the
parser can extract robust head word statistics is to
select a sample containing many examples of each
word. More formally, we denote the words that oc-
cur in the unlabelled pool at least t times by t-words,
where t is a parameter of the algorithm. We want to
select a sample containing at least t examples of as
many t-words as possible.
To select such a sample we introduce a novel op-
timisation problem. Our problem is a variant of the
multiset multicover (MM) problem, which we call
the constrained multiset multicover (CMM) prob-
lem. The setting of the MM problem is as fol-
lows (Hochbaum, 1997): Given a set I of m ele-
ments to be covered each bi times, a collection of
multisets Sj C I, j E J = 11, ... , n} (a multiset is
a set in which members’ multiplicity may be greater
than 1), and weights wj, find a subcollection C of
multisets that covers each i E I at least bi times, and
such that PjcC wj is minimized.
CMM differs from MM in that in CMM the sum
of the weights (representing the desired number of
sentences to annotate) is bounded, while the num-
ber of covered elements (representing the t-words)
should be maximized. In our case, I is the set of
words that occur at least t times in the unlabelled
pool, bi = t, bi E I, the multisets are the sentences
in that pool and wj = 1, bj E J.
Multiset multicover is NP-hard. However, there is
a good greedy approximation algorithm for it. De-
fine a(sj, i) = min(R(sj, i), di), where di is the
difference between bi and the number of instances
of item i that are present in our current sample, and
R(sj, i) is the multiplicity of the i-th element in the
multiset sj. Define A(sj) to be the multiset contain-
ing exactly a(sj, i) copies of any element i if sj is
not already in the set cover and the empty set if it
is. The greedy algorithm repeatedly adds a set mini-
mizing wj
|A(sj)|. This algorithm provenly achieves an
approximation ratio between ln(m) and ln(m) + 1.
In our case all weights are 1, so the algorithm would
4We explored CBS with several lexical features schemes and
got only marginal improvement over random selection.
</bodyText>
<page confidence="0.997654">
7
</page>
<figure confidence="0.996796">
10000
9000
8000
number of t−words
7000
6000
5000
4000
3000
2000
1000
</figure>
<bodyText confidence="0.996783511111111">
simply add the sentence that maximizes A(sj) to the
set cover.
The problem in directly applying the algorithm to
our case is that it does not take into account the de-
sired sample size. We devised a variant of the algo-
rithm where we use a binary tree to ‘push’ upwards
the number of t-words in the whole batch of unan-
notated sentences that occurs at least t times in the
selected one. Below is a detailed description. D de-
notes the desired number of items to sample.
The algorithm has two steps. First, we iter-
atively sample (without replacement) D multisets
(sentences) from a uniform distribution over the
multisets. In each iteration we calculate for the se-
lected multiset its ‘contribution’ – the number of
items that cross the threshold of t occurrences with
this multiset minus the number of items that cross
the t threshold without this multiset (i.e. the contri-
bution of the first multiset is the number of t-words
occurring more than t times in it). For each multiset
we build a node with a key that holds its contribu-
tion, and insert these nodes in a binary tree. Inser-
tion is done such that all downward paths are sorted
in decreasing order of key values.
Second, we iteratively sample (from a uniform
distribution, without replacement) the rest of the
multisets pool. For each multiset we perform two
steps. First, we prepare a node with a key as de-
scribed above. We then randomly choose Z leaves5
in the binary tree (if the number of leaves is smaller
than Z all of the leaves are chosen). For each leaf we
find the place of the new node in the path from the
root to the leaf (paths are sorted in decreasing order
of key values). We insert the new node to the high-
est such place found (if the new key is not smaller
than the existing paths), add its multiset to the set of
selected multisets, and remove the multiset that cor-
responds to the leaf of this path from the batch and
the leaf itself from the binary tree. We finally choose
the multisets that correspond to the highest D nodes
in the tree.
An empirical demonstration of the quality of ap-
proximation that the algorithm provides is given in
Figure 2. We ran our algorithm with the threshold
parameter set to t E [2,14] and counted the num-
</bodyText>
<footnote confidence="0.9711415">
5We tried Z values from 10 to 100 in steps of 10 and ob-
served very similar results. We report results for Z = 100.
</footnote>
<figure confidence="0.940207888888889">
t=2
t=8
t=11
t=14
t=5
random
0
0 100 200 300 400 500 600 700 800
number of training constituents (thousands)
</figure>
<figureCaption confidence="0.976188166666667">
Figure 2: Number of t-words for t = 5 in samples selected
by CMM runs with different values of the threshold pa-
rameter t and in a randomly selected sample. CMM with
t = 5 is significantly higher. All the lines except for the
line for t = 5 are unified. For clarity, we do not show all t
values: their curves are also similar to the t =� 5 lines.
</figureCaption>
<table confidence="0.999763714285714">
Method 86% 86.5% 87% 87.5% 88%
TE 16.9% 27.1% 26.9% 14.8% 15.8%
(152K) (183K) (258K) (414K) (563 K)
CBS 19.6% 16.8% 19% 21.1% 9%
(147K) (210K) (286K) (382K) (610K)
CMM 9% 10.4% 8.9% 10.3% 14%
(167K) (226K) (312K) (433K) (574K)
</table>
<tableCaption confidence="0.99968">
Table 1: Reduction in annotation cost in TC terms com-
</tableCaption>
<bodyText confidence="0.9454695">
pared to the random baseline for tree entropy (TE), syn-
tactic clustering (CBS) and CMM. The compared samples
are the smallest samples selected by each of the methods
that achieve certain f-score levels. Reduction is calcu-
latedby: 100 − 100 x (TCmethod/TCrandom).
ber of words occurring at least 5 times in the se-
lected sample. We followed the same experimen-
tal protocol as in Section 5. The graph shows that
the number of words occurring at least 5 times in a
sample selected by our algorithm when t = 5 is sig-
nificantly higher (by about a 1000) than the number
of such words in a randomly selected sample and in
samples selected by our algorithm with other t pa-
rameter values. We got the same pattern of results
when counting words occurring at least t times for
the other values of the t parameter – only the run of
the algorithm with the corresponding t value created
a sample with significantly higher number of words
not below threshold. The other runs and random se-
lection resulted in samples containing significantly
lower number of words not below threshold.
In Section 5 we show that the parser performance
when it is trained with a sample selected by CMM
is significantly better than when it is trained with a
randomly selected sample. Improvement is similar
across the t parameter values.
</bodyText>
<page confidence="0.99491">
8
</page>
<table confidence="0.998507">
86% 87% 88%
Method TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK
(1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22)
TE 34.9% 3.6% - 8.9% - 61.3% 42.2% 14.4% - 9.9% - 62.7% 25% 8.1% - 6.3% - 30%
CBS 21.3% 18.6% - 0.5% - 3.5% 19.6% 24.2% - 0.3% - 1.8% 8.9% 8.6 % 0% - 0.3%
CMM 10.18% 8.87% -0.82% -3.39% 11% 16.22% -0.34% -1.8% 14.65% 14.11% -0.02% - 0.08%
</table>
<tableCaption confidence="0.9754994">
Table 2: Annotation cost reduction in TNSK and ANSK compared to the random baseline for tree entropy (TE), syntactic
clustering (CBS) and CMM. The compared samples are the smallest selected by each of the methods that achieve certain
f-score levels. Each column represents the reduction in total or average number of structures of degree 1–6 or 7–22.
Reduction for each measure is calculated by: 100 − 100 × (measuremethod/measurerandom). Negative reduction
is an addition. Samples with a higher reduction in a certain measure are better in terms of that measure.
</tableCaption>
<figure confidence="0.992057854166667">
28
Average number of constituents
1.6
TE
CMM,t=8
CBS
26
ANSK method/ANSK random
1.7
1.5
TE
CMM, t = 8
CBS
24
1.4
1.3
22
1.2
20
1.1
18
86 86.5 87 87.5 88
1
0 5 10 15 20 25
F score
K
x 104
0 1 2 3 3.5
Number of sentences
x 104
3
CMM(t=8) − TE
CBS − TE
0 line
0
−1
0 5 10 15 20 25
K
2
1
TNSK (K)
1.5
TE
CMM, t=8
CBS
1.25
AC method/AC random
1
</figure>
<figureCaption confidence="0.787449333333333">
Figure 3: Left to right: First: The difference between the number of nested structures of degree K of CMM and TE and
of CBS and TE. The curves are unified. The 0 curve is given for reference. Samples selected by CMM and CBS have
more nested structures of degrees 1–6 and less nested structures of degrees 7–22. Results are presented for the smallest
</figureCaption>
<bodyText confidence="0.791608777777778">
samples required for achieving f-score of 88. Similar patterns are observed for other f-score values. Second: Average
number of nested structures of degree K as a function of K for the smallest sample required for achieving f-score of
88. Results for each of the methods are normalized by the average number of nested structures of degree K in the
smallest randomly selected sample required for achieving f-score of 88. The sentences in CMM and CBS samples are
not more complex than sentences in a randomly selected sample. In TE samples sentences are more complex. Third:
Average number of constituents (AC) for the smallest sample of each of the methods that is required for achieving a
given f-score. CMM and CBS samples contain sentences with a smaller number of constituents. Fourth: AC values for
the samples created by the methods (normalized by AC values of a randomly selected sample). The sentences in TE
samples, but not in CMM and CBS samples, are more complex than sentences in a randomly selected sample.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999675625">
Experimental setup. We used Bikel’s reimplemen-
tation of Collins’ parsing model 2 (Bikel, 2004).
Sections 02-21 and 23 of the WSJ were stripped
from their annotation. Sections 2-21 (39832 sen-
tences, about 800K constituents) were used for train-
ing, Section 23 (2416 sentences) for testing. No
development set was used. We used the gold stan-
dard POS tags in two cases: in the test section (23)
in all experiments, and in Sections 02-21 in the
CBS method when these sections are to be parsed
in the process of vector creation. In active learn-
ing methods the unlabelled pool is parsed in each
iteration and thus should be tagged with POS tags.
Hwa (2004) (to whom we compare our results) used
the gold standard POS tags for the same sections
in her work6. We implemented a random baseline
</bodyText>
<subsectionHeader confidence="0.513865">
6Personal communication with Hwa. Collins’ parser uses an
</subsectionHeader>
<bodyText confidence="0.965821210526316">
where sentences are uniformly selected from the un-
labelled pool for annotation. For reliability we re-
peated each experiment with the algorithms and the
random baseline 10 times, each time with different
random selections (M sentences for creating syntac-
tic tagging and k-means initialization for CBS, sen-
tence order in CMM), and averaged the results.
Each experiment contained 38 runs. In each run
a different desired sample size was selected, from
1700 onwards, in steps of 1000. Parsing perfor-
mance is measured in terms of f-score
Results. We compare the performance of our
CBS and CMM algorithms to the TE method (Hwa,
2004)7, which is the only sample selection work ad-
input POS tag only if it cannot tag its word using the statistics
learned from the training set.
7Hwa has kindly sent us the samples selected by her TE. We
evaluated these samples with TC and the new measures. The TC
of the minimal sample she sent us needed for achieving f-score
</bodyText>
<page confidence="0.993236">
9
</page>
<bodyText confidence="0.99343119117647">
dressing our experimental setup. Unless otherwise
stated, we report the reduction in annotation cost:
100 − 100 x (measuremethod/measurerandom).
CMM results are very similar for t E {2, 3, ... ,14},
and presented for t = 8.
Table 1 presents reduction in annotation cost in
TC terms. CBS achieves greater reduction for f =
86, 87.5, TE for f = 86.5, 87, 88. For f = 88, TE
and CMM performance are almost similar. Examin-
ing the f-score vs. TC sample size over the whole
constituents range (not shown due to space con-
straints) reveals that CBS, CMM and TE outperform
random selection over the whole range. CBS and
TE performance are quite similar with TE being bet-
ter in the ranges of 170–300K and 520–650K con-
stituents (42% of the 620K constituents compared)
and CBS being better in the ranges of 130–170K and
300–520K constituents (44% of the range). CMM
performance is worse than CBS and TE until 540K
constituents. From 650K constituents on, where
the parser achieves its best performance, the perfor-
mance of CMM and TE methods are similar, outper-
forming CBS.
Table 2 shows the annotation cost reduction in
ANSK and TNSK terms. TE achieves remarkable
reduction in the total number of relatively shallow
structures (TNSK K = 1–6). Our methods, in con-
trast, achieve remarkable reduction in the number of
deep structures (TNSK K = 7–22)8. This is true for
all f-score values. Moreover, the average number of
nested structures per sentence, for every degree K
(ANSK for every K) in TE sentences is much higher
than in sentences of a randomly selected sample. In
samples selected by our methods, the ANSK values
are very close to the ANSK values of randomly se-
lected samples. Thus, sentences in TE samples are
much more complex than in CBS and CMM samples.
The two leftmost graphs in Figure 3 demonstrate
(for the minimal samples required for f-score of 88)
that these reductions hold for each K value (ANSK)
and for each K E [7, 22] (TNSK) not just on the av-
of 88 is different from the number reported in (Hwa, 2004). We
compare our TC results with the TC result in the sample sent us
by Hwa.
8We present results where the border between shallow and
deep structures is set to be Kborder = 6. For every Kborder E
{7, 8, ... , 22} TNSK reductions with CBS and CMM are much
more impressive than with TE for structures whose degree is
K E [Kborder, 22].
erage over these K values. We observed similar re-
sults for other f-score values.
The two rightmost graphs of Figure 3 demon-
strates AC results. The left of them shows that for
every f-score value, the AC measure of the minimal
TE sample required to achieve that f-score is higher
than the AC value of PBS samples (which are very
similar to the AC values of randomly selected sam-
ples). The right graph demonstrates that for every
sample size, the AC value of TE samples is higher
than that of PBS samples.
All AL based previous work (including TE) is it-
erative. In each iteration thousands of sentences
are parsed, while PBS algorithms perform a single
iteration. Consequently, PBS computational com-
plexity is dramatically lower. Empirically, using a
Pentium 4 2.4GHz machine, CMM requires about an
hour and CBS about 16.5 hours, while the TE parsing
steps alone take 662 hours (27.58 days).
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999989458333333">
We introduced novel evaluation measures: AC,
TNSK and ANSK for the task of sample selection
for statistical parsers. Based on the psycholinguis-
tic literature we argue that these measures reflect as-
pects of the cognitive efforts of the human annota-
tor that are not reflected by the traditional TC mea-
sure. We introduced the parameter based sample se-
lection (PBS) approach and its CMM and CBS algo-
rithms that do not deliberately select difficult sen-
tences. Therefore, our intuition was that they should
select a sample that leads to an accurate parameter
estimation but does not contain a high number of
complex structures. We demonstrated that CMM and
CBS achieve results that are similar to the state of the
art TE method in TC terms and outperform it when
the cognitively driven measures are considered.
The measures we suggest do not provide a full
and accurate description of human annotator efforts.
In future work we intend to extend and refine our
measures and to revise our algorithms accordingly.
We also intend to design stopping criteria for the
PBS methods. These are criteria that decide when
the selected sample suffices for the parser best per-
formance and further annotation is not needed.
</bodyText>
<page confidence="0.99796">
10
</page>
<sectionHeader confidence="0.99585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998746534090909">
Steven Abney and Mark Johnson, 1991. Memory re-
quirements and local ambiguities of parsing strategies.
Psycholinguistic Research, 20(3):233–250.
Daniel M. Biken, 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Jason Baldridge and Miles Osborne, 2004. Active learn-
ing and the total cost of annotation. EMNLP ’04.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI 05.
Markus Becker, 2008. Active learning – an explicit treat-
ment of unreliable parameters. Ph.D. thesis, The Uni-
versity of Edinburgh.
Noam Chomsky and George A. Miller, 1963. Fini-
tary models of language users. In R. Duncan Luce,
Robert R. Bush, and Eugene Galanter, editors, Hand-
book of Mathematical Psychology, volume II. John
Wiley, New York, 419–491.
David Cohn, Les Atlas and Richard E. Ladner, 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201–221.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Edward Gibson, 1991. A computational theory of hu-
man linguistic processing: memory limitations and
processing breakdowns. Ph.D. thesis, Carnegie Mel-
lon University, Pittsburg, PA.
Edward Gibson, 1998. Linguistic complexity: locality
of syntactic dependencies. Cognition, 68:1–76.
Dorit Hochbaum (ed), 1997. Approximation algorithms
for NP-hard problems. PWS Publishing, Boston.
Rebecca Hwa, 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
Cody Kwok, Oren Etzioni, Daniel S. Weld, 2001. Scal-
ing question answering to the Web. WWW ’01.
Matthew Lease and Eugene Charniak, 2005. Parsing
biomedical literature. IJCNLP ’05.
Willem J.M. Levelt, 2001. Spoken word production: A
theory of lexical access. PNAS, 98(23):13464–13471.
Richard L. Lewis, Shravan Vasishth and Julie Van Dyke,
2006. Computational principles of working memory
in sentence comprehension. Trends in Cognitive Sci-
ence, October:447-454.
David MacKay, 2002. Information theory, inference and
learning algorithms. Cambridge University Press.
Brian MacWhinney, 1982. The competition model. In
B. MacWhinney, editor, Mechanisms of language ac-
quisition. Hillsdale, NJ: Lawrence Erlbaum, 249–308.
Daniel Marcu, Wei Wang, Abdessamabad Echihabi, and
Kevin Knight, 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
EMNLP ’06.
P. Melville, M. Saar-Tsechansky, F. Provost and R.J.
Mooney, 2005. An expected utility approach to ac-
tive feature-value acquisition. 5th IEEE Intl. Conf. on
Data Mining ’05.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz, 1994. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313–330.
G.W. Milligan and M.C Cooper, 1985. An examination
of procedures for determining the number of clusters
in a data set. Psychometrika, 58(2):159–157.
Grace Ngai and David Yarowski, 2000. Rule writing
or annotation: cost–efficient resource usage for base
noun phrase chunking. ACL ’00.
Roi Reichart, Katrin Tomanek, Udo Hahn and Ari Rap-
poport, 2008. Multi-task active learning for linguistic
annotations. ACL ’08.
Brian Roark, Margaret Mitchell and Kristy Hollingshead,
2007. Syntactic complexity measures for detecting
mild cognitive impairment. BioNLP workshop, ACL
’07.
Min Tang, Xiaoqiang Luo, and Salim Roukos, 2002. Ac-
tive learning for statistical natural language parsing.
ACL ’02.
Katrin Tomanek, Joachim Wermtre, and Udo Hahn,
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. EMNLP ’07.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning, 2005. Joint learning improves semantic role
labeling. ACL ’05.
Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin
K. Tsou, 2008. Active learning with sampling by un-
certainty and density for word sense disambiguation
and text classification. COLING ’08.
</reference>
<page confidence="0.999457">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720211">
<title confidence="0.998521">Sample Selection for Statistical Parsers: Cognitively Algorithms and Evaluation Measures</title>
<author confidence="0.992777">Roi</author>
<affiliation confidence="0.998409">Hebrew University of</affiliation>
<email confidence="0.965366">roiri@cs.huji.ac.il</email>
<author confidence="0.847594">Ari</author>
<affiliation confidence="0.997486">Institute of computer Hebrew University of</affiliation>
<email confidence="0.966117">arir@cs.huji.ac.il</email>
<abstract confidence="0.997147571428571">Creating large amounts of manually annotated training data for statistical parsers imposes heavy cognitive load on the human annotator and is thus costly and error prone. It is hence of high importance to decrease the human efforts involved in creating training data without harming parser performance. For constituency parsers, these efforts are traditionally evaluated using the total number of constituents (TC) measure, assuming uniform cost for each annotated item. In this paper, we introduce novel measures that quantify aspects of the cognitive efforts of the human annotator that are not reflected by the TC measure, and show that they are well established in the psycholinguistic literature. We present a novel based sample selection for creating good samples in terms of these measures. We describe methods for global optimisation of lexical parameters of the sample based on a novel optimisation problem, the multiset multicover and sampling to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="10711" citStr="Abney and Johnson, 1991" startWordPosition="1691" endWordPosition="1694">on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentences. Nested structures. A nested structure is a parse tree node representing a constituent created while another constituent is still being processed (‘open’). The degree K of a nested structure is the number of such open constituents. In this paper, we enumerate the constituents in a top-down left-right order, and thus when a constituent is created, only its ancestors are processed2. A constituent is processed 2A good review on node enumeration of the human parser in given in (Abney and Johnson, 1991). Lotus Figure 1: An example parse tree. until the processing of its children is completed. For example, in Figure 1, when the constituent NP3 is created, it starts a nested structure of degree 2, since two levels of its ancestors (VP, S) are still processed. Its parent (VP) starts a nested structure of degree 1. The difficulty of deeply nested structures for the human parser is well established in the psycholinguistics literature. We review here some of the various explanations of this phenomenon; for a comprehensive review see (Gibson, 1998). According to the classical stack overflow theory </context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Steven Abney and Mark Johnson, 1991. Memory requirements and local ambiguities of parsing strategies. Psycholinguistic Research, 20(3):233–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Biken</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<marker>Biken, 2004</marker>
<rawString>Daniel M. Biken, 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<journal>EMNLP</journal>
<volume>04</volume>
<contexts>
<context position="7926" citStr="Baldridge and Osborne (2004)" startWordPosition="1253" endWordPosition="1256">ituents (the 40K sentences of sections 2- 21 of WSJ). A detailed comparison between the results of TE and our methods is given in Section 5. The following works addressed the task of sample selection for statistical parsers, but in significantly different experimental setups. Becker and Osborne (2005) addressed lower performance levels of the Collins parser. Their uncertainty sampling protocol combined bagging with the TE function, achieving a 32% TC reduction for reaching a parser f-score level of 85.5%. The target sample size set contained a much smaller number of sentences (∼5K) than ours. Baldridge and Osborne (2004) addressed HPSG parse selection using a feature based log-linear parser, the Redwoods corpus and committee based active learning, obtaining 80% reduction in annotation cost. Their annotation cost measure was related to the number of possible parses of the sentence. Tang et al. (2002) addressed a shallow parser trained on a semantically annotated corpus. 1Hwa explored several functions in the experimental setup used in the present work, and TE gave the best results. 4 They used an uncertainty sampling protocol, where in each iteration the sentences of the unlabelled pool are clustered using a d</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>Jason Baldridge and Miles Osborne, 2004. Active learning and the total cost of annotation. EMNLP ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Miles Osborne</author>
</authors>
<title>A two-stage method for active learning of statistical grammars.</title>
<date>2005</date>
<journal>IJCAI</journal>
<volume>05</volume>
<contexts>
<context position="7600" citStr="Becker and Osborne (2005)" startWordPosition="1200" endWordPosition="1203">uts a list of trees ranked by their probabilities. The scored list is treated as a random variable and the sentences whose variable has the highest entropy are selected for human annotation. Sample size was measured in TC and ranged from 100K to 700K WSJ constituents. The initial size of the unlabelled pool was 800K constituents (the 40K sentences of sections 2- 21 of WSJ). A detailed comparison between the results of TE and our methods is given in Section 5. The following works addressed the task of sample selection for statistical parsers, but in significantly different experimental setups. Becker and Osborne (2005) addressed lower performance levels of the Collins parser. Their uncertainty sampling protocol combined bagging with the TE function, achieving a 32% TC reduction for reaching a parser f-score level of 85.5%. The target sample size set contained a much smaller number of sentences (∼5K) than ours. Baldridge and Osborne (2004) addressed HPSG parse selection using a feature based log-linear parser, the Redwoods corpus and committee based active learning, obtaining 80% reduction in annotation cost. Their annotation cost measure was related to the number of possible parses of the sentence. Tang et </context>
</contexts>
<marker>Becker, Osborne, 2005</marker>
<rawString>Markus Becker and Miles Osborne, 2005. A two-stage method for active learning of statistical grammars. IJCAI 05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
</authors>
<title>Active learning – an explicit treatment of unreliable parameters.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Edinburgh.</institution>
<marker>Becker, 2008</marker>
<rawString>Markus Becker, 2008. Active learning – an explicit treatment of unreliable parameters. Ph.D. thesis, The University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>George A Miller</author>
</authors>
<title>Finitary models of language users.</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology,</booktitle>
<volume>volume</volume>
<pages>419--491</pages>
<editor>In R. Duncan Luce, Robert R. Bush, and Eugene Galanter, editors,</editor>
<publisher>II. John Wiley,</publisher>
<location>New York,</location>
<contexts>
<context position="11337" citStr="Chomsky and Miller, 1963" startWordPosition="1795" endWordPosition="1798"> Lotus Figure 1: An example parse tree. until the processing of its children is completed. For example, in Figure 1, when the constituent NP3 is created, it starts a nested structure of degree 2, since two levels of its ancestors (VP, S) are still processed. Its parent (VP) starts a nested structure of degree 1. The difficulty of deeply nested structures for the human parser is well established in the psycholinguistics literature. We review here some of the various explanations of this phenomenon; for a comprehensive review see (Gibson, 1998). According to the classical stack overflow theory (Chomsky and Miller, 1963) and its extension, the incomplete syntactic/thematic dependencies theory (Gibson, 1991), the human parser should track the open structures in its short term memory. When the number of these structures is too large or when the structures are nested too deeply, the short term memory fails to hold them and the sentence becomes uninterpretable. According to the perspective shifts theory (MacWhinney, 1982), processing deeply nested structures requires multiple shifts of the annotator perspective and is thus more difficult than processing shallow structures. The difficulty of deeply nested structur</context>
</contexts>
<marker>Chomsky, Miller, 1963</marker>
<rawString>Noam Chomsky and George A. Miller, 1963. Finitary models of language users. In R. Duncan Luce, Robert R. Bush, and Eugene Galanter, editors, Handbook of Mathematical Psychology, volume II. John Wiley, New York, 419–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard E Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas and Richard E. Ladner, 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5784" citStr="Collins, 1999" startWordPosition="899" endWordPosition="900">eter yields better estimation of this parameter. If this parameter is crucial for model performance and the selection process does not harm the distribution of other parameters, then the selected sample is of high quality. To select such a sample we introduce a reduction between this selection problem and a variant of the NP-hard multiset-multicover problem (Hochbaum, 1997). We call this problem the constrained multiset multicover (CMM) problem, and present an algorithm to approximate it. We experiment (Section 5) with the WSJ PennTreebank (Marcus et al., 1994) and Collins’ generative parser (Collins, 1999), as in previous work. We show that PBS algorithms achieve good results in terms of both the traditional TC measure (significantly better than the random selection baseline and similar to the results of the state of the art tree entropy (TE) method of (Hwa, 2004)) and our novel cognitively driven measures (where PBS algorithms significantly outperform both TE and the random baseline). We thus argue that PBS provides a way to select a sample that imposes reduced cognitive load on the human annotator. 2 Related Work Previous work on sample selection for statistical parsers applied active learnin</context>
<context position="15680" citStr="Collins, 1999" startWordPosition="2496" endWordPosition="2497">t we prefer the number of structures since the latter better reflects the arguments made in the literature. tions about the human parser than those described here. A complete survey of that literature is beyond the scope of this paper. We consider the proposed measures a good approximation of some of the human parser characteristics. 4 Parameter Based Sampling (PBS) Our approach is to sample the unannotated pool with respect to the distribution of the model parameters in its sentences. In this paper, in order to compare to previous works, we apply our methods to the Collins generative parser (Collins, 1999). For any sentence s and parse tree t it assigns a probability p(s, t), and finds the tree for which this probability is maximized. To do that, it writes p(t, s) as a product of the probabilities of the constituents in t and decomposes the latter using the chain rule. In simplified notation, it uses: p(t, s) = P(S1 → S2 . . . Sn) = 1 1 P(S1)·. . .· P(Sn |O(S1 . . . Sn)) 11 (1) We refer to the conditional probabilities as the model parameters. Cluster Based Sampling (CBS). We describe here a method for sampling subsets that leads to a parameter estimation that is similar to the parameter estima</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins, 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>A computational theory of human linguistic processing: memory limitations and processing breakdowns.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburg, PA.</location>
<contexts>
<context position="11425" citStr="Gibson, 1991" startWordPosition="1807" endWordPosition="1808">le, in Figure 1, when the constituent NP3 is created, it starts a nested structure of degree 2, since two levels of its ancestors (VP, S) are still processed. Its parent (VP) starts a nested structure of degree 1. The difficulty of deeply nested structures for the human parser is well established in the psycholinguistics literature. We review here some of the various explanations of this phenomenon; for a comprehensive review see (Gibson, 1998). According to the classical stack overflow theory (Chomsky and Miller, 1963) and its extension, the incomplete syntactic/thematic dependencies theory (Gibson, 1991), the human parser should track the open structures in its short term memory. When the number of these structures is too large or when the structures are nested too deeply, the short term memory fails to hold them and the sentence becomes uninterpretable. According to the perspective shifts theory (MacWhinney, 1982), processing deeply nested structures requires multiple shifts of the annotator perspective and is thus more difficult than processing shallow structures. The difficulty of deeply nested structured has been demonstrated for many languages (Gibson, 1998). We thus propose the total nu</context>
</contexts>
<marker>Gibson, 1991</marker>
<rawString>Edward Gibson, 1991. A computational theory of human linguistic processing: memory limitations and processing breakdowns. Ph.D. thesis, Carnegie Mellon University, Pittsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>68--1</pages>
<contexts>
<context position="11260" citStr="Gibson, 1998" startWordPosition="1786" endWordPosition="1787">ration of the human parser in given in (Abney and Johnson, 1991). Lotus Figure 1: An example parse tree. until the processing of its children is completed. For example, in Figure 1, when the constituent NP3 is created, it starts a nested structure of degree 2, since two levels of its ancestors (VP, S) are still processed. Its parent (VP) starts a nested structure of degree 1. The difficulty of deeply nested structures for the human parser is well established in the psycholinguistics literature. We review here some of the various explanations of this phenomenon; for a comprehensive review see (Gibson, 1998). According to the classical stack overflow theory (Chomsky and Miller, 1963) and its extension, the incomplete syntactic/thematic dependencies theory (Gibson, 1991), the human parser should track the open structures in its short term memory. When the number of these structures is too large or when the structures are nested too deeply, the short term memory fails to hold them and the sentence becomes uninterpretable. According to the perspective shifts theory (MacWhinney, 1982), processing deeply nested structures requires multiple shifts of the annotator perspective and is thus more difficult</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson, 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dorit Hochbaum</author>
</authors>
<title>Approximation algorithms for NP-hard problems.</title>
<date>1997</date>
<publisher>PWS Publishing,</publisher>
<location>Boston.</location>
<contexts>
<context position="5546" citStr="Hochbaum, 1997" startWordPosition="862" endWordPosition="863">pool reflecting the distribution of the model parameters in this sentence, and use a clustering algorithm to divide these vectors into clusters. In the second method we use the fact that a sample containing many examples of a certain parameter yields better estimation of this parameter. If this parameter is crucial for model performance and the selection process does not harm the distribution of other parameters, then the selected sample is of high quality. To select such a sample we introduce a reduction between this selection problem and a variant of the NP-hard multiset-multicover problem (Hochbaum, 1997). We call this problem the constrained multiset multicover (CMM) problem, and present an algorithm to approximate it. We experiment (Section 5) with the WSJ PennTreebank (Marcus et al., 1994) and Collins’ generative parser (Collins, 1999), as in previous work. We show that PBS algorithms achieve good results in terms of both the traditional TC measure (significantly better than the random selection baseline and similar to the results of the state of the art tree entropy (TE) method of (Hwa, 2004)) and our novel cognitively driven measures (where PBS algorithms significantly outperform both TE </context>
<context position="20297" citStr="Hochbaum, 1997" startWordPosition="3331" endWordPosition="3332">mple from which the parser can extract robust head word statistics is to select a sample containing many examples of each word. More formally, we denote the words that occur in the unlabelled pool at least t times by t-words, where t is a parameter of the algorithm. We want to select a sample containing at least t examples of as many t-words as possible. To select such a sample we introduce a novel optimisation problem. Our problem is a variant of the multiset multicover (MM) problem, which we call the constrained multiset multicover (CMM) problem. The setting of the MM problem is as follows (Hochbaum, 1997): Given a set I of m elements to be covered each bi times, a collection of multisets Sj C I, j E J = 11, ... , n} (a multiset is a set in which members’ multiplicity may be greater than 1), and weights wj, find a subcollection C of multisets that covers each i E I at least bi times, and such that PjcC wj is minimized. CMM differs from MM in that in CMM the sum of the weights (representing the desired number of sentences to annotate) is bounded, while the number of covered elements (representing the t-words) should be maximized. In our case, I is the set of words that occur at least t times in </context>
</contexts>
<marker>Hochbaum, 1997</marker>
<rawString>Dorit Hochbaum (ed), 1997. Approximation algorithms for NP-hard problems. PWS Publishing, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="2905" citStr="Hwa, 2004" startWordPosition="443" endWordPosition="444">tance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S1 ⊂ S for human annotation such that the human efforts in annotating S1 are minimized while the parser performance when trained with this sample is maximized. Previous works addressing training sample size vs. parser performance for constituency parsers (Section 2) evaluated training sample size using the total number of constituents (TC). Sentences differ in length and therefore in annotation efforts, and it has been argued (see, e.g, (Hwa, 2004)) that TC reflects the number of decisions the human annotator makes when syntactically annotating the sample, assuming uniform cost for each decision. In this paper we posit that important aspects of the efforts involved in annotating a sample are not reflected by the TC measure. Since annotators analyze sentences rather than a bag of constituents, sentence structure has a major impact on their cognitive efforts. Sizeable psycholinguistic literature points to the connection between nested structures in the syntactic structure of a sentence and its annotation efforts. This has motivated us to </context>
<context position="6047" citStr="Hwa, 2004" startWordPosition="946" endWordPosition="947">tion between this selection problem and a variant of the NP-hard multiset-multicover problem (Hochbaum, 1997). We call this problem the constrained multiset multicover (CMM) problem, and present an algorithm to approximate it. We experiment (Section 5) with the WSJ PennTreebank (Marcus et al., 1994) and Collins’ generative parser (Collins, 1999), as in previous work. We show that PBS algorithms achieve good results in terms of both the traditional TC measure (significantly better than the random selection baseline and similar to the results of the state of the art tree entropy (TE) method of (Hwa, 2004)) and our novel cognitively driven measures (where PBS algorithms significantly outperform both TE and the random baseline). We thus argue that PBS provides a way to select a sample that imposes reduced cognitive load on the human annotator. 2 Related Work Previous work on sample selection for statistical parsers applied active learning (AL) (Cohn and Ladner, 1994) to corpora of various languages and syntactic annotation schemes and to parsers of different performance levels. In order to be able to compare our results to previous work targeting high parser performance, we selected the corpus a</context>
<context position="29547" citStr="Hwa (2004)" startWordPosition="5038" endWordPosition="5039">mplementation of Collins’ parsing model 2 (Bikel, 2004). Sections 02-21 and 23 of the WSJ were stripped from their annotation. Sections 2-21 (39832 sentences, about 800K constituents) were used for training, Section 23 (2416 sentences) for testing. No development set was used. We used the gold standard POS tags in two cases: in the test section (23) in all experiments, and in Sections 02-21 in the CBS method when these sections are to be parsed in the process of vector creation. In active learning methods the unlabelled pool is parsed in each iteration and thus should be tagged with POS tags. Hwa (2004) (to whom we compare our results) used the gold standard POS tags for the same sections in her work6. We implemented a random baseline 6Personal communication with Hwa. Collins’ parser uses an where sentences are uniformly selected from the unlabelled pool for annotation. For reliability we repeated each experiment with the algorithms and the random baseline 10 times, each time with different random selections (M sentences for creating syntactic tagging and k-means initialization for CBS, sentence order in CMM), and averaged the results. Each experiment contained 38 runs. In each run a differe</context>
<context position="32707" citStr="Hwa, 2004" startWordPosition="5589" endWordPosition="5590">of nested structures per sentence, for every degree K (ANSK for every K) in TE sentences is much higher than in sentences of a randomly selected sample. In samples selected by our methods, the ANSK values are very close to the ANSK values of randomly selected samples. Thus, sentences in TE samples are much more complex than in CBS and CMM samples. The two leftmost graphs in Figure 3 demonstrate (for the minimal samples required for f-score of 88) that these reductions hold for each K value (ANSK) and for each K E [7, 22] (TNSK) not just on the avof 88 is different from the number reported in (Hwa, 2004). We compare our TC results with the TC result in the sample sent us by Hwa. 8We present results where the border between shallow and deep structures is set to be Kborder = 6. For every Kborder E {7, 8, ... , 22} TNSK reductions with CBS and CMM are much more impressive than with TE for structures whose degree is K E [Kborder, 22]. erage over these K values. We observed similar results for other f-score values. The two rightmost graphs of Figure 3 demonstrates AC results. The left of them shows that for every f-score value, the AC measure of the minimal TE sample required to achieve that f-sco</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa, 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cody Kwok</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling question answering to the Web.</title>
<date>2001</date>
<journal>WWW</journal>
<volume>01</volume>
<contexts>
<context position="1767" citStr="Kwok et al., 2001" startWordPosition="258" endWordPosition="261">s of the sample based on a novel optimisation problem, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance. 1 Introduction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: g</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Cody Kwok, Oren Etzioni, Daniel S. Weld, 2001. Scaling question answering to the Web. WWW ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<journal>IJCNLP</journal>
<volume>05</volume>
<contexts>
<context position="2072" citStr="Lease and Charniak, 2005" startWordPosition="308" endWordPosition="311">uction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S1 ⊂ S for human annotation such that the human efforts in annotating S1 are minimized while the parser performance when trained with this sample is maximized. Previous works addressing training sample size vs. parser p</context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak, 2005. Parsing biomedical literature. IJCNLP ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem J M Levelt</author>
</authors>
<title>Spoken word production: A theory of lexical access.</title>
<date>2001</date>
<journal>PNAS,</journal>
<volume>98</volume>
<issue>23</issue>
<contexts>
<context position="12814" citStr="Levelt (2001)" startWordPosition="2030" endWordPosition="2031">Sentence level resources. In the psycholinguistic literature of sentence processing there are many theories describing the cognitive resources required during a complete sentence processing. These resources might be allocated during the processing of a certain word and are needed long after its constituent is closed. We briefly discuss two lines of theory, focusing on their predictions that sentences consisting of a large number of structures (e.g., conNP1 NP2 VP JJ NNP NN NP3 VBD Last week IBM bought NNP S 5 stituents or nested structures) require more cognitive resources for longer periods. Levelt (2001) suggested a layered model of the mental lexicon organization, arguing that when one hears or reads a sentence s/he activates word forms (lexemes) that in turn activate lemma information. The lemma information contains information about syntactic properties of the word (e.g., whether it is a noun or a verb) and about the possible sentence structures that can be generated given that word. The process of reading words and retrieving their lemma information is incremental and the lemma information for a given word is used until its syntactic structure is completed. The information about a word in</context>
</contexts>
<marker>Levelt, 2001</marker>
<rawString>Willem J.M. Levelt, 2001. Spoken word production: A theory of lexical access. PNAS, 98(23):13464–13471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
<author>Shravan Vasishth</author>
<author>Julie Van Dyke</author>
</authors>
<title>Computational principles of working memory in sentence comprehension.</title>
<date>2006</date>
<journal>Trends in Cognitive Science,</journal>
<pages>447--454</pages>
<marker>Lewis, Vasishth, Van Dyke, 2006</marker>
<rawString>Richard L. Lewis, Shravan Vasishth and Julie Van Dyke, 2006. Computational principles of working memory in sentence comprehension. Trends in Cognitive Science, October:447-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David MacKay</author>
</authors>
<title>Information theory, inference and learning algorithms.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18104" citStr="MacKay, 2002" startWordPosition="2948" endWordPosition="2949">the sentence parse t(s), Q is a function that returns the (P, H) pair of the constituent c, Fi is a predicate that returns 1 iff it is given pair number i as an argument and 0 otherwise, and L is the number of modifying nonterminals in the constituent plus 1 (for the head), counting the number of parameters that condition on (P, H). Following equation (2), the ith coordinate of the vector representation of a sentence in G contains the number of parameters that will be calculated conditioned on the ith (P, H) pair. We use the k-means clustering algorithm, with the L2 norm as a distance metric (MacKay, 2002), to divide vectors into clusters. Clusters created by this algorithm contain adjacent vectors in a Euclidean space. Clusters represent sentences with similar features values. To initialize k-means, we sample the initial centers values from a uniform distribution over the data points. We do not decide on the number of clusters in advance but try to find inherent structure in the data. Several methods for estimating the ‘correct’ number of clusters are known (Milligan and Cooper, 1985). We used a statistical heuristic called the elbow test. We define the ‘within cluster dispersion’ Wk as follow</context>
</contexts>
<marker>MacKay, 2002</marker>
<rawString>David MacKay, 2002. Information theory, inference and learning algorithms. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The competition model.</title>
<date>1982</date>
<booktitle>Mechanisms of language acquisition. Hillsdale, NJ: Lawrence Erlbaum,</booktitle>
<pages>249--308</pages>
<editor>In B. MacWhinney, editor,</editor>
<contexts>
<context position="11742" citStr="MacWhinney, 1982" startWordPosition="1860" endWordPosition="1861">nguistics literature. We review here some of the various explanations of this phenomenon; for a comprehensive review see (Gibson, 1998). According to the classical stack overflow theory (Chomsky and Miller, 1963) and its extension, the incomplete syntactic/thematic dependencies theory (Gibson, 1991), the human parser should track the open structures in its short term memory. When the number of these structures is too large or when the structures are nested too deeply, the short term memory fails to hold them and the sentence becomes uninterpretable. According to the perspective shifts theory (MacWhinney, 1982), processing deeply nested structures requires multiple shifts of the annotator perspective and is thus more difficult than processing shallow structures. The difficulty of deeply nested structured has been demonstrated for many languages (Gibson, 1998). We thus propose the total number of nested structures of degree K in a sample (TNSK) as a measure of the cognitive efforts that its annotation requires. The higher K is, the more demanding the structure. Sentence level resources. In the psycholinguistic literature of sentence processing there are many theories describing the cognitive resource</context>
</contexts>
<marker>MacWhinney, 1982</marker>
<rawString>Brian MacWhinney, 1982. The competition model. In B. MacWhinney, editor, Mechanisms of language acquisition. Hillsdale, NJ: Lawrence Erlbaum, 249–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamabad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<journal>EMNLP</journal>
<volume>06</volume>
<contexts>
<context position="1792" citStr="Marcu et al., 2006" startWordPosition="263" endWordPosition="266"> a novel optimisation problem, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance. 1 Introduction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamabad Echihabi, and Kevin Knight, 2006. SPMT: Statistical machine translation with syntactified target language phrases. EMNLP ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Melville</author>
<author>M Saar-Tsechansky</author>
<author>F Provost</author>
<author>R J Mooney</author>
</authors>
<title>An expected utility approach to active feature-value acquisition.</title>
<date>2005</date>
<booktitle>5th IEEE Intl. Conf. on Data Mining ’05.</booktitle>
<contexts>
<context position="9310" citStr="Melville et al. (2005)" startWordPosition="1470" endWordPosition="1473">ccount the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures </context>
</contexts>
<marker>Melville, Saar-Tsechansky, Provost, Mooney, 2005</marker>
<rawString>P. Melville, M. Saar-Tsechansky, F. Provost and R.J. Mooney, 2005. An expected utility approach to active feature-value acquisition. 5th IEEE Intl. Conf. on Data Mining ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5737" citStr="Marcus et al., 1994" startWordPosition="890" endWordPosition="893"> a sample containing many examples of a certain parameter yields better estimation of this parameter. If this parameter is crucial for model performance and the selection process does not harm the distribution of other parameters, then the selected sample is of high quality. To select such a sample we introduce a reduction between this selection problem and a variant of the NP-hard multiset-multicover problem (Hochbaum, 1997). We call this problem the constrained multiset multicover (CMM) problem, and present an algorithm to approximate it. We experiment (Section 5) with the WSJ PennTreebank (Marcus et al., 1994) and Collins’ generative parser (Collins, 1999), as in previous work. We show that PBS algorithms achieve good results in terms of both the traditional TC measure (significantly better than the random selection baseline and similar to the results of the state of the art tree entropy (TE) method of (Hwa, 2004)) and our novel cognitively driven measures (where PBS algorithms significantly outperform both TE and the random baseline). We thus argue that PBS provides a way to select a sample that imposes reduced cognitive load on the human annotator. 2 Related Work Previous work on sample selection</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz, 1994. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Milligan</author>
<author>M C Cooper</author>
</authors>
<title>An examination of procedures for determining the number of clusters in a data set.</title>
<date>1985</date>
<journal>Psychometrika,</journal>
<volume>58</volume>
<issue>2</issue>
<contexts>
<context position="18593" citStr="Milligan and Cooper, 1985" startWordPosition="3026" endWordPosition="3029">culated conditioned on the ith (P, H) pair. We use the k-means clustering algorithm, with the L2 norm as a distance metric (MacKay, 2002), to divide vectors into clusters. Clusters created by this algorithm contain adjacent vectors in a Euclidean space. Clusters represent sentences with similar features values. To initialize k-means, we sample the initial centers values from a uniform distribution over the data points. We do not decide on the number of clusters in advance but try to find inherent structure in the data. Several methods for estimating the ‘correct’ number of clusters are known (Milligan and Cooper, 1985). We used a statistical heuristic called the elbow test. We define the ‘within cluster dispersion’ Wk as follows. Suppose that the data is divided into k clusters C1 ... Ck with |Cj |points in the jth cluster. Let Dt = Pi,jcCt di,j where di,j is the squared Euclidean distance, then Wk := Pt= 1 2|Ct |Dt. Wk tends to decrease monotonically as k increases. In many cases, from some k this decrease flattens markedly. The heuristic is that the location of such an ‘elbow’ indicates the appropriate number of clusters. In our experiments, an obvious elbow occurred for 15 clusters. ki sentences are rand</context>
</contexts>
<marker>Milligan, Cooper, 1985</marker>
<rawString>G.W. Milligan and M.C Cooper, 1985. An examination of procedures for determining the number of clusters in a data set. Psychometrika, 58(2):159–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowski</author>
</authors>
<title>Rule writing or annotation: cost–efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<journal>ACL</journal>
<volume>00</volume>
<marker>Ngai, Yarowski, 2000</marker>
<rawString>Grace Ngai and David Yarowski, 2000. Rule writing or annotation: cost–efficient resource usage for base noun phrase chunking. ACL ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
<author>Ari Rappoport</author>
</authors>
<title>Multi-task active learning for linguistic annotations.</title>
<date>2008</date>
<journal>ACL</journal>
<volume>08</volume>
<contexts>
<context position="9655" citStr="Reichart et al., 2008" startWordPosition="1522" endWordPosition="1525">diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence. Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentences. Nested structures. A nested structure is a par</context>
</contexts>
<marker>Reichart, Tomanek, Hahn, Rappoport, 2008</marker>
<rawString>Roi Reichart, Katrin Tomanek, Udo Hahn and Ari Rappoport, 2008. Multi-task active learning for linguistic annotations. ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Margaret Mitchell</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Syntactic complexity measures for detecting mild cognitive impairment.</title>
<date>2007</date>
<journal>BioNLP workshop, ACL</journal>
<volume>07</volume>
<contexts>
<context position="8971" citStr="Roark et al. (2007)" startWordPosition="1418" endWordPosition="1421">present work, and TE gave the best results. 4 They used an uncertainty sampling protocol, where in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity </context>
</contexts>
<marker>Roark, Mitchell, Hollingshead, 2007</marker>
<rawString>Brian Roark, Margaret Mitchell and Kristy Hollingshead, 2007. Syntactic complexity measures for detecting mild cognitive impairment. BioNLP workshop, ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tang</author>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<journal>ACL</journal>
<volume>02</volume>
<contexts>
<context position="8210" citStr="Tang et al. (2002)" startWordPosition="1298" endWordPosition="1301">e (2005) addressed lower performance levels of the Collins parser. Their uncertainty sampling protocol combined bagging with the TE function, achieving a 32% TC reduction for reaching a parser f-score level of 85.5%. The target sample size set contained a much smaller number of sentences (∼5K) than ours. Baldridge and Osborne (2004) addressed HPSG parse selection using a feature based log-linear parser, the Redwoods corpus and committee based active learning, obtaining 80% reduction in annotation cost. Their annotation cost measure was related to the number of possible parses of the sentence. Tang et al. (2002) addressed a shallow parser trained on a semantically annotated corpus. 1Hwa explored several functions in the experimental setup used in the present work, and TE gave the best results. 4 They used an uncertainty sampling protocol, where in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve i</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>Min Tang, Xiaoqiang Luo, and Salim Roukos, 2002. Active learning for statistical natural language parsing. ACL ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermtre</author>
<author>Udo Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data.</title>
<date>2007</date>
<journal>EMNLP</journal>
<volume>07</volume>
<contexts>
<context position="9605" citStr="Tomanek et al., 2007" startWordPosition="1515" endWordPosition="1518">ated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence. Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentenc</context>
</contexts>
<marker>Tomanek, Wermtre, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermtre, and Udo Hahn, 2007. An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data. EMNLP ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<contexts>
<context position="1825" citStr="Toutanova et al., 2005" startWordPosition="269" endWordPosition="272">, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance. 1 Introduction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated s</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning, 2005. Joint learning improves semantic role labeling. ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
<author>Tianshun Yao</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Active learning with sampling by uncertainty and density for word sense disambiguation and text classification.</title>
<date>2008</date>
<journal>COLING</journal>
<volume>08</volume>
<contexts>
<context position="9061" citStr="Zhu et al. (2008)" startWordPosition="1431" endWordPosition="1434">ere in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Co</context>
</contexts>
<marker>Zhu, Wang, Yao, Tsou, 2008</marker>
<rawString>Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K. Tsou, 2008. Active learning with sampling by uncertainty and density for word sense disambiguation and text classification. COLING ’08.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>