<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001592">
<sectionHeader confidence="0.2990015" genericHeader="abstract">
A DISCOVERY PROCEDURE
FOR CERTAIN PHONOLOGICAL RULES
</sectionHeader>
<keyword confidence="0.445884">
Mark Johnson
Linguistics, UCSD.
</keyword>
<sectionHeader confidence="0.960108" genericHeader="keywords">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999379666666667">
Acquisition of phonological systems can be insightfully
studied in terms of discovery procedures. This paper describes
a discovery procedure, implemented in Lisp, capable of deter-
mining a set of ordered phonological rules, which may be in
opaque contexts, from a set of surface forms arranged in para-
digms.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9999508">
For generative grammarians, such as Chomsky (1965), a
primary problem of linguistics is to explain how the language
learner can acquire the grammar of his or her language on the
basis of the limited evidence available to him or her. Chomsky
introduced the idealization of instantaneous acquisition, which
I adopt here, in order to model the language acquisition device
as a function from primary linguistic data to possible gram-
mars, rather than as a process.
Assuming that the set of possible human languages is
small, rather than large, appears to make acquisition easier,
since there are fewer possible grammars to choose from, and
less data should be required to choose between them. Accord-
ingly, generative linguists are interested in delimiting the class
of possible human languages. This is done by looking for pro-
perties common to all human languages, or universals.
Together, these universals form universal grammar, a set of
principles that all human languages obey. Assuming that
universal grammar is innate, the language learner can use it to
restrict the number of possible grammars he or she must con-
sider when learning a language.
As part of universal grammar, the language learner is
supposed to innately possess an evaluation metric, which is
used to &amp;quot;decide&amp;quot; between two grammars when both are con-
sistent with other principles of universal grammar and the
available language data.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="method">
2. DISCOVERY PROCEDURES
</sectionHeader>
<bodyText confidence="0.999860791666667">
This approach deals with acquisition without reference to
a specific discovery procedure, and so in some sense the results
of such research are general, in that in principle they apply to
all discovery procedures. Still, I think that there is some util-
ity in considering the problem of acquisition in terms of actual
discovery procedures.
Firstly, we can identify the parts of a grammar that are
underspecified with respect to the available data. Parts of a
grammar or a rule are strongly data determined if they are
fixed or uniquely determined by the data, given the require-
ment that overall grammar be empirically correct.
By contrast, a part of a grammar or of a rule is weakly data
determined if there is a large class of grammar or rule parts
that are all consistent with the available data. For example, if
there are two possible analyses that equally well account for
the available data, then the choice of which of these analyses
should be incorporated in the final grammar is weakly data
determined. Strong or weak data determination is therefore a
property of the grammar formalism and the data combined,
and independent of the choice of discovery procedure.
Secondly, a discovery procedure may partition a phono-
logical system in an interesting way. For instance, in the
discovery procedure described here the evaluation metric is not
called apon to compare one grammar with another, but rather
to make smaller, more local, comparisons. This leads to a fac-
toring of the evaluation metric that may prove useful for its
further investigation.
Thirdly, focussing on discovery procedures forces us to
identify what the surface indications of the various construc-
tions in the grammar are. Of course, this does not mean one
should look for a one-to-one correspondence between individual
grammar constructions and the surface data; but rather com-
plexes of grammar constructions that interact to yield particu-
lar patterns on the surface. One is then investigating the logi-
cal implications of the existence of a particular constructions in
the data.
Following from the last point, I think a discovery pro-
cedure should have a deductive rather than enumerative struc-
ture. In particular, procedures that work essentially by
enumerating all possible (sub)grammars and seeing which ones
work are not only in general very inefficient, but also not very
insightful. These discovery by enumeration procedures simply
give us a list of all rule systems that are empirically adequate
as a result, but they give us no idea as to what properties of
these systems were crucial in their being empirically adequate.
This is because the structure imposed on the problem by a
simple recursive enumeration procedure is in general not
related to the intrinsic structure of the rule discovery problem.
</bodyText>
<sectionHeader confidence="0.996693" genericHeader="method">
3. A PHONOLOGICAL RULE DISCOVERY PRO-
CEDURE
</sectionHeader>
<bodyText confidence="0.999828">
Below and in Appendix A I outline a discovery pro-
cedure, which I have fully implemented in Franz Lisp on a
VAX 11/750 computer, for a restricted class of phonological
rules, namely rules of the type shown in (1).
</bodyText>
<equation confidence="0.631721666666667">
(1) a—,b/ C
Rule (1) means that any segment a that appears in Con-
text C in the input to the rule appears as a b in the rule&apos;s out-
</equation>
<bodyText confidence="0.399889">
put. Context C is a feature matrix, and to say that a appears
in context C means that C is a subset of the feature mat rix
</bodyText>
<page confidence="0.997845">
344
</page>
<bodyText confidence="0.999842083333333">
formed by the segments around al. A phonological system
consists of an ordered2 set of such rules, where the rules are
considered to apply in a cascaded fashion, that is, the output
of one rule is the input to the next.
The problem the discovery procedure must solve is, given
some data, to determine the set of rules. As an idealization, I
assume that the input to the discovery procedure is a set of
surface paradigms, a two dimensional array of words with all
words in the same row possessing the same stem and all words
in the same column the same affix. Moreover, I assume the
root and suffix morphemes are already identified, although I
admit this task may be non-trivial.
</bodyText>
<sectionHeader confidence="0.9989035" genericHeader="method">
4. DETERMINING THE CONTEXT THAT CONDI-
TIONS AN ALTERNATION
</sectionHeader>
<bodyText confidence="0.999615">
Consider the simplest phonological system; one in which
only one phonological rule is operative. In this system the
alternating segements a and b can be determined by inspec-
tion, since a and b will be the only alternating segments in the
data (although there will be a systematic ambiguity as to
which is a and which is 6). Thus a and b are strongly data
determined.
Given a and b. we can write a set of equations that the
rule context C that conditions this alternation must obey.
Our rule must apply in all contexts C5 where a b appears that
alternates with an a, since by hypothesis b was produced by
this rule. We can represent this by equation (2).
</bodyText>
<listItem confidence="0.642159">
(2) tiCb, C matches 6.5
</listItem>
<bodyText confidence="0.875382">
The second condition that our rule must obey is that it
doesn&apos;t apply in any context C. where an a appears. If it did,
of course, we would expect a b, not an a, in this position on
the surface. We can write this condition by equation (3).
</bodyText>
<listItem confidence="0.972636">
(3) teCa, C does not match C.
</listItem>
<bodyText confidence="0.999308030769231">
These two equations define the rule context C. Note that
in general these equations do not yield a unique value for C;
depending apon the data there may be no C that simultane-
ously satisfies (2) and (3). or there may he several different C
that simultaneously satisfies (2) and (3). We cannot appeal
further to the data to decide which C to use, since they all are
equally consistent with the data.
Let us call the set of C that simultaneously satisfies (2)
and (3) Sc. Then Sc is strongly data determined; in fact,
there is an efficient algorithm for computing Sc from the Ccs
and Cbs that does not involve enumerating and testing all ima-
ginable C (the algorithm is described in Appendix A).
However, if Sc contains more than one C, the choice of
which C from Sc to actually use as the rule&apos;s context is weakly
1 What is crucial for what follows is that saying context C
matches a portion of a word W is equivalent to saying that C
is a subset of W. Since both rule contexts and words can be
written as sets of features, I use &amp;quot;contexts&amp;quot; to refer both to
rule contexts and to words.
2 I make this assumption as a first approximation. In
fact, in real phonological systems phonological rules may be
unordered with respect to each other.
data determined. Moreover, the choice of %%hid] C from Sc 0
use does not affect any other decisions that the discovery pro-
cedure has to make - that is. nothing else in the complete
grammar must change if we decide to use one C instead of
another.
Plausibly, the evaluation metric and universal principles
decide which C to use in this situation. For example, if the
alternation involves nasalization of a vowel, something that
usually only occurs in the context. of a nasal, and one of the
contexts in Sc involves the feature nasal but the other C in Sc
do not, a reasonable requirement is that the discovery pro-
cedure should select the context involving the feature nasal as
the appropriate context C for the rule.
Another possibility is that Sc&apos;s containing more than one
member indicates to the discovery procedure that it simply has
too little data to determine the grammar. and it defers making
a decision on which C to use until it has the relevant data.
The decision as to which of these possibilities is correct is is
not unimportant, and may have interesting empirical conse-
quences regarding language acquisition.
McCarthy (1981) gives some data on a related issue.
Spanish does not tolerate word initial sC clusters, a fact which
might be accounted for in two ways; either with a rule that
inserts e before word initial sC clusters, or by a constraint on
well-formed underlying structures (a redundancy rule) barring
word initial sC. McCarthy reports that either constraint is
adequate to account for Spanish morphophonemics, and there
is no particular language internal evidence to prefer one over
the other.
The two accounts make differing predictions regarding
the treatment. of loan words. The e insertion rule predicts that
loan words beginning with sC should receive an initial e (as
they do: esnob, esmoking, esprey), while the well-formedness
constraint makes no such prediction.
McCarthy&apos;s evidence from Spanish therefore suggests that
the human acquisition procedure can adopt one potential
analysis and rejects an other without empirical evidence to dis-
tinguish between them. However, in the Spanish case, the two
potential analyses differ as to which components of the gram-
mar they involve (active phonological processes versus lexical
redundancy rules) which affects the overall structure of the
adopted grammar to a much greater degree than the choice of
one C from Sc over another.
</bodyText>
<sectionHeader confidence="0.991685" genericHeader="method">
5. RULE ORDERING
</sectionHeader>
<bodyText confidence="0.99997225">
In the last section I showed that a single phonological
rule can be determined from the surface data. In practice,
very few, if any, phonological systems involve only one rule.
Systems involving more than one rule show complexity that
single rule systems do not. In particular, a rules may be
ordered in such a fashion that one rule affects segments that
are part or the context that conditions the operation of
another rule. If a rule&apos;s context is visible on the surface (ie.
has not been destroyed by the operation of another rule) it is
said to be transparent, while if a rule&apos;s context is no longer
visible on the surface it is opaque. On the face of it, opaque
contexts could pose problems for discovery procedures.
</bodyText>
<page confidence="0.995925">
345
</page>
<bodyText confidence="0.982059443478261">
Ordering of rule, ha- beer&apos; a topic substantial research in
phonology. Nly main object ia in hi- section is to shrm that
extrinsically ordered rules in principle pose no problem for a
discovery procedure. even if later rules obscure the context of
earlier ones. I don&apos;t make any claim that the procedure
presented here is optimal - in fact I can think of at least two
ways to make it perform its job more efficiently&apos;. The output
of t his discovery procedure is t he set of all possible ordered
rule syst ems&apos; and I heir corresponding miderly ing forms that,
can produce the given surface forms.
As before. I assume I hat the data is in the form of sets of
paradigms. I also assume that for every rule changing an a to
a b. an all erniat ion bet ween a and b appears in I he data: thus
v‘e know by listing the alternations in the data just what the
possible as and Gs of the rule are&amp;quot;.
From t he assumption t hat rules are ext rinsically ordered
it follows that one of the rules must have applied last.; that is,
there is a unique &amp;quot;most surfacy&amp;quot; rule. The context of this rule
ill necessarily lw transparent (visible in the surface forms), as
there is no later rule to make its context opaque.
Of course. the discovery procedure has no a priori way of
v.hich alternation corresponds to the most surfacy rule.
Thus alt hough t he identity of the segments involved in the
most surfacy rule may be strictly data determined, at this
stage this information is not available to the discovery pro-
cedure.
So at this point, the discovery procedure proposed here
systematically investigates all of the surface alternations: for
each alternation it makes the hypothesis that it is the the
alternation of the roost surfacy- rule, checks that a context can
be found t hat conditions this all (this roust be so if
the hypothesis is correct) using the single rule algorithm
presented earlier, and then investigates if it is possible to con-
struct an empirically correct set of rules based on this
hypothesis.
Given that we have found a potential &amp;quot;most surfacy&amp;quot;
rule. all of the surface alternates are replaced by the putative
underlying segment to form a set of intermediate forms, in
which he rule just discovered has been undone. We can undo
this rule. because we previously identified the alternating seg-
ments. lint ),,rt an tly , undoing this rule means that all other
3 Thus if the n rules in the system are unordered, this
procedure returns n! solutions corresponding to the n ways of
ordering these rules.
4 The reason why the class of phonological rules con-
sidered in this paper was restricted to those mapping segments
into segments was so that. all alternations could be identified
by simply comparing surface forms segment by segment. Thus
in this discovery procedure the algorithm for identifying possi-
ble alternates can be of a particularly simple form. If we are
willing to complicate the machinery that determines the possi-
ble alternations in some data, we can relax the restriction
prohibiting epertthesis and deletion rules, and the requirement
that all alternations are visible on the surface. That is, if the
approach here is correct, the problem of identifying which seg-
ments alternate is a different problem to discovering the
context mat I otafflions II i .11111 hell &amp;quot;AI,.
rules whose contexts had been made opaque in the surface
data by the. operation of the most surfacy rule will now be
t ransparen t.
The hypothesis tester proceeds to look for another alter-
nation, this time in the intermediate forms, rather than in the
surface forms, and so on until all alternations have been
accounted for.
If at any stage the hypothesis tester fails to find a rule to
describe the alternation it is currently working with, that is,
the single-rule. algorithm determines that no rule context exists
that can capture this alternation, the hypothesis tester dis-
cards the current hypothesis. and tries another.
The hypothesis tester is responsible for proposing dif-
ferent rule orderings, which are tested by applying the rules in
reverse. to arrive. at progressively more removed representa-
tions. with the single- rule algorit hum being applied at each step
tel determine if a rule exists that relates one level of intermedi-
ate! representation with the next.. We can regard the
hypothesis tester as systematically searching through the space
of different rule orderings. seeking rule. orderings that success-
fully&apos; OCCOU nits for the observed data.
The output of this procedure is therefore a list of all pos-
sible rule orderings. As I mentioned before, I think that the
enumerative approach adopted here is basically flawed. So
although this procedure is relatively efficient in situations
where rule ordering is strictly data determined (that is, where
only one rule ordering is consistent with the data), in situa-
tions where the rules are unordered (any rule ordering will do),
the procedure will generate all possible n! orderings of the n
rules.
This was roost striking while working with SOIlle Japanese
data. with 6 distinct alternations, 4 of which were unordered
with respect to each other. The discovery procedure, as
presented above, required approximately 1 hour of CPU time
to completely analyse this data: it. found 4 different underlying
forms and 512 different rule systems that generate the
Japanese data, differing primarily in the ordering of the rules.
This demonstrates that a discovery procedure that simply
enumerates all possible rule ordering is failing to capture some
important insight regarding rule. ordering, since unordered
rules are much more difficult for this type of procedure to han-
dle, yet unordered rules are the most common situation in
natural language phonology.
This problem may be traced back to the assumption
made above that a phonological system consists of an ordered
set of rules. The Japanese example shows that in many real
phonological systems, the ordering of particular rules is simply
not strongly data determined. What we need is some way of
partitioning different rule orderings into equivalence classes, as
was done with this the different rule contexts in the single rule
algorithm, and then compute with these equivalence classes
rather than individual rule systems; that is. seek to localize the
weak data determinacy.
Looking at the problem in another way, we asked the
discovery procedure to find all sets of ordered rules that gen-
erate the surface data, which it did. However, it seems that
this simply was not right question, since the answer to this
(pleat ion, a set of 512 different systems, is virtually
</bodyText>
<page confidence="0.996723">
346
</page>
<bodyText confidence="0.999593923076923">
uninterpretable by human beings. Part of the problem is that
phonologists in general have not yet agreed what exactly the
principles of rule ordering are5.
Still, the present discovery procedure. whatever its defi-
ciencies, does demonstrate that rule ordering in phonology
does not pose any principled insurmountable problems for
discovery procedures (although the procedure presented here is
certainly practically lacking in certain situations), even if a
later rule is allowed to disturb the context of an earlier rule, so
that the rule&apos;s context is no longer &amp;quot;surface true&amp;quot;. None the
less, it is an empirical question as to whether phonology is best
described in terms of ordered interacting rules; all that I have
shown is that such systems are not in principle unlearnable.
</bodyText>
<sectionHeader confidence="0.997537" genericHeader="method">
6. CONCLUSION
</sectionHeader>
<bodyText confidence="0.999742863636364">
In this paper I have presented the details of a discovery
procedure that can determine a limited class of phonological
rules with arbitrary rule ordering. The procedure has the
interesting property that it can be separated into two separate
phases, the first. phase being superificial data analysis, that is,
collecting the sets C. and Cb of equations (2) and (3), and the
second phase being the application of the procedure proper,
which need never reference the data directly, but. can do all of
its calculations using C. and C. This property is interesting
because it is likely that C. and Cb have limiting values, as the
number of forms in the surface data increases. That is,
presumably the language only has a fixed number of alterna-
tions, and each of these only occurs in some fixed contexts,
and as soon as we have enough data to sec all of these con-
texts we will have determined C. and Cb, arid extra data will
not. make these sets larger. Thus the computational complex-
ity of the second phase of the discovery procedure is more or
less independent, of the size the lexicon, making the entire pro-
cedure require linear time with respect to the size of the data.
think this is a desirable result, since there is something coun-
terintuitive to a situation in which the difficulty of discovering
a grammar increases rapidly with the size of the lexicon.
</bodyText>
<sectionHeader confidence="0.9351135" genericHeader="method">
7. APPENDIX A: DETERMINING A RULE&apos;S CON-
TEXT
</sectionHeader>
<bodyText confidence="0.999907142857143">
In this appendix I describe an algorithm for calculating
the set of rule contexts Sc = { C } that satisify equations (2)
and (3) repeated below in set notation as (9) and (5). Recall
that Cb are the contexts in which the alternation did take
place, and C. are the contexts in which the alternations did
not take place. We want to find (the set of) contexts that
simultaneously match all the Cb, while riot matching any C..
</bodyText>
<sectionHeader confidence="0.647126" genericHeader="method">
(4) V Cb, C C C6
</sectionHeader>
<bodyText confidence="0.933588454545454">
5 In this paper 1 adopted strict ordering of all rules be-
cause it is one of the more stringent rule ordering hypotheses
available.
6 In fact, the sets C. and C, as defined above do not con-
tain quite enough information alone. We must also indicate
which segments in these contexts alternate, and what they al-
ternate to. This may form the basis of a very different rule
order discovery procedure.
(5) VC,. C C,
We can manipulate these into computationally more
tractable forms. Starting with (9), we have
</bodyText>
<equation confidence="0.971177090909091">
C6, Cc C6 (= (4))
V Cb, f E C, f E Cb
E C, 1€ n cb Cc 11 c6
Put C1 = n Cb. Then C C C1.
Now consider equation (5).
c„ f E C, f C.
VC.,3
f€ (C-C.)
But since C C C1, if f E ( C C.). then
1E ( C1— C.) n C. Then
V C., Elfc ( C1-- C.), f C
</equation>
<bodyText confidence="0.99951025">
This last equation says that every context that fulfills the
conditions above contains at least one feature that distin-
guishes it from each C., and that this feature must be in the
intersection of all the Cb. If for any C.. C1 - C, = 0 (the null
set of features), then there are no contexts C that simultane-
ously match all the Cb and none of the C., implying that no
rule exists that, accounts for the observed alternation.
We can construct the set Sc using this last formula by
first calculating Ci, the intersection of all the C6, and then for
each C., calculating C&apos;f= ( C C. ), a member of which
must be in every C. The idea is to keep a set of the minimal
C needed to account for the C. so far; if C contains a member
of Cf we don&apos;t need to modify it; if C does not contain a
member of C.( then we have to add a member of C1 to it in
order for it to satisfy the equations above. The algorithm
below acomplishes this.
</bodyText>
<figure confidence="0.82763425">
set C1 n Cb
set Sc {Q}
foreach C,
set Cf= C1 -
if Cf Q
return &amp;quot;No rule contexts&amp;quot;
foreach C in Sc
if C n C1sQ
remove C from Sc
foreach fin Cf
add CU {f}toSc
return Sc
</figure>
<bodyText confidence="0.999778">
where the subroutine &amp;quot;add&amp;quot; adds a set to Sc only if it or
its subset is not already present.
After this algorithm has applied, Sc will contain all the
minimal different C that satisfy equations (4) and (5) above.
</bodyText>
<page confidence="0.997466">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.231919">
<title confidence="0.999387">A DISCOVERY PROCEDURE FOR CERTAIN PHONOLOGICAL RULES</title>
<author confidence="0.999985">Mark Johnson</author>
<abstract confidence="0.883903125">Linguistics, UCSD. ABSTRACT Acquisition of phonological systems can be insightfully studied in terms of discovery procedures. This paper describes a discovery procedure, implemented in Lisp, capable of determining a set of ordered phonological rules, which may be in opaque contexts, from a set of surface forms arranged in paradigms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>