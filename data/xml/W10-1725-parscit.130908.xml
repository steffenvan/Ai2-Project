<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012527">
<title confidence="0.93713">
UPV-PRHLT English–Spanish system for WMT10
</title>
<author confidence="0.504478">
Germ´an Sanchis-Trilles and Jes´us Andr´es-Ferrer and Guillem Gasc´o
Jes´us Gonz´alez-Rubio and Pascual Martinez-G´omez and Martha-Alicia Rocha
Joan-Andreu S´anchez and Francisco Casacuberta
</author>
<affiliation confidence="0.3244105">
Instituto Tecnol´ogico de Inform´atica
Departamento de Sistemas Inform´aticos y Computaci´on
</affiliation>
<address confidence="0.306538">
Universidad Polit´ecnica de Valencia
</address>
<email confidence="0.953835">
{gsanchis|jandres|fcn}@dsic.upv.es
{ggasco|jegonzalez|pmartinez}@dsic.upv.es
{mrocha|jandreu}@dsic.upv.es
</email>
<bodyText confidence="0.9908776">
Abstract In the final evaluation, our system was ranked
In this paper, the system submitted by
the PRHLT group for the Fifth Work-
shop on Statistical Machine Translation of
ACL2010 is presented. On this evalua-
tion campaign, we have worked on the
English–Spanish language pair, putting
special emphasis on two problems derived
from the large amount of data available.
The first one, how to optimize the use of
the monolingual data within the language
model, and the second one, how to make
good use of all the bilingual data provided
without making use of unnecessary com-
putational resources.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9983758">
For this year’s translation shared task, the Pat-
tern Recognition and Human Language Technolo-
gies (PRHLT) research group of the Universidad
Polit´ecnica de Valencia submitted runs for the
English–Spanish translation task. In this paper, we
report the configuration of such a system, together
with preliminary experiments performed to estab-
lish the final setup.
As in 2009, the central focus of the Shared Task
is on Domain Adaptation, where a system typi-
cally trained using out-of-domain data is adjusted
to translate news commentaries.
For the preliminary experiments, we used only a
small amount of the largest available bilingual cor-
pus, i.e. the United Nations corpus, by including
into our system only those sentences which were
considered similar.
Language model interpolation using a develop-
ment set was explored in this work, together with
a technique to cope with the problem of ”out of
vocabulary words”.
Finally, a reordering constraint using walls and
zones was used in order to improve the perfor-
mance of the submitted system.
fifth, considering only primary runs.
</bodyText>
<sectionHeader confidence="0.991416" genericHeader="method">
2 Language Model interpolation
</sectionHeader>
<bodyText confidence="0.99987425">
Nowadays, it is quite common to have very large
amounts of monolingual data available from sev-
eral different domains. Despite of this fact, in
most of the cases we are only interested in trans-
lating from one specific domain, as is the case in
this year’s shared task, where the provided mono-
lingual training data belonged to European parlia-
mentary proceedings, news related domains, and
the United Nations corpus, which consists of data
crawled from the web.
Although the most obvious thing to do is to con-
catenate all the data available and train a single
language model on the whole data, we also inves-
tigated a “smarter” use of such data, by training
one language model for each of the available cor-
pora.
</bodyText>
<sectionHeader confidence="0.983538" genericHeader="method">
3 Similar sentences selection
</sectionHeader>
<bodyText confidence="0.999237666666667">
Currently, it is common to of huge bilingual cor-
pora for SMT. For some common language pairs,
corpora of millions of parallel sentences are avail-
able. In some of the cases big corpora are used
as out-of-domain corpora. For example, in the
case of the shared task, we try to translate a news
text using a small in-domain bilingual news corpus
(News Commentary) and two big out-of-domain
corpora: Europarl and United Nations.
Europarl is a medium size corpus and can be
completely incorporated to the training set. How-
ever, the use of the UN corpus requires a big com-
putational effort. In order to alleviate this prob-
lem, we have chosen only those bilingual sen-
tences from the United Nations that are similar to
the in-domain corpus sentences. As a similarity
measure, we have chosen the alignment score.
Alignment scores have already been used as a
</bodyText>
<page confidence="0.972097">
172
</page>
<note confidence="0.453282">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172–176,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999797">
filter for noisy corpora (Khadivi and Ney, 2005).
We trained an IBM model 4 using GIZA++ (Och
and Ney, 2003) with the in-domain corpus and
computed the alignment scores over the United
Nations sentences. We assume that the alignment
score is a good measure of similarity.
An important factor in the alignment score is
the length of the sentences, so we clustered the
bilingual sentences in groups with the same sum of
source and target language sentence sizes. In each
of the groups, the higher the alignment score is,
the more similar the sentence is to the in-domain
corpus sentences. Hence, we computed the aver-
age alignment score for each one of the clusters
obtained for the corpus considered in-domain (i.e.
the News-Commentary corpus). This being done,
we assessed the similarity of a given sentence by
computing the probability of such sentence with
respect to the alignment model of the in-domain
corpus, and established the following similarity
levels:
</bodyText>
<listItem confidence="0.999209833333333">
• Level 1: Sentences with an alignment score
equal or higher than the in-domain average.
• Level 2: Sentences with an alignment score
equal or higher than the in-domain average,
minus one standard deviation.
• Level 3: Sentences with an alignment score
</listItem>
<bodyText confidence="0.964521142857143">
equal or higher than the in-domain average,
minus two standard deviations.
Naturally, such similarity levels establish parti-
tions of the out-of-domain corpus. Then, such par-
titions were included into the training set used for
building the SMT system, and re-built the com-
plete system from scratch.
</bodyText>
<sectionHeader confidence="0.953966" genericHeader="method">
4 Out of Vocabulary Recovery
</sectionHeader>
<bodyText confidence="0.999983772727273">
As stated in the previous section, in order to avoid
a big computational effort, we do not use the
whole United Nations corpus to train the trans-
lation system. Out of vocabulary words are a
common problem for machine translation systems.
When translating the test set, there are test words
that are not in the reduced training set (out of vo-
cabulary words). Some of those out of vocabulary
words are present in the sentences discarded from
the United Nations Corpus. Thus, recovering the
discarded sentences with out of vocabulary words
is needed.
The out of vocabulary words recovery method
is simple: the out of vocabulary words from the
test, when taking into account the reduced training
set, are obtained and then discarded sentences that
contain at least one of them are retrieved. Then,
those sentences are added to the reduced training
set.
Finally, alignments with the resulting training
set were computed and the usual training proce-
dure for phrase-based systems was performed.
</bodyText>
<sectionHeader confidence="0.975229" genericHeader="method">
5 Walls and zones
</sectionHeader>
<bodyText confidence="0.999834">
In translation, as in other linguistics areas, punc-
tuation marks are essential as they help to un-
derstand the intention of a message and organise
the ideas to avoid ambiguity. They also indicate
pauses, hierarchies and emphasis.
In our system, punctuation marks have been
taken into account during decoding. Traditionally,
in SMT punctuation marks are treated as words
and this has undesirable effects (Koehn and Had-
dow, 2009). For example, commas have a high
probability of occurrence and many possible trans-
lations are generated. Most of them are not consis-
tent across languages. This introduces too much
noise to the phrase tables.
(Koehn and Haddow, 2009) established a
framework to specify reordering constraints with
walls and zones, where commas and end
of sentence are not mixed with various clauses.
Gains between 0.1 and 0.2 of BLEU are reported.
Specifying zones and walls with XML tags
in input sentences allows us to identify structured
fragments that the Moses decoder uses with the
following restrictions:
</bodyText>
<listItem confidence="0.999752769230769">
1. If a &lt;zone&gt; tag is detected, then a block
is identified and must be translated until a
&lt;/zone&gt; tag is found. The text between tags
&lt;zone&gt; and &lt;/zone&gt; is identified and trans-
lated as a block.
2. If the decoder detects a &lt;wall/&gt; tag, the text
is divided into a prefix and suffix and Moses
must translate all the words of the prefix be-
fore the suffix.
3. If both zones and walls are specified,
then local walls are considered where
the constraint 2 applies only to the area es-
tablished by zones.
</listItem>
<page confidence="0.991866">
173
</page>
<table confidence="0.9997356">
corpus Language |S ||W  ||V |
Europarl v5 Spanish 28M 154K
English 1272K 27M 106K
NC Spanish 1.8M 54K
English 81K 1.6M 39K
</table>
<tableCaption confidence="0.996286">
Table 1: Main figures of the Europarl v5 and
</tableCaption>
<bodyText confidence="0.965019230769231">
News-Commentary (NC) corpora. K/M stands
for thousands/millions. |S |is the number of sen-
tences, |W  |the number of running words, and |V |
the vocabulary size. Statistics are reported on the
tokenised and lowercased corpora.
We used quotation marks, parentheses, brackets
and dashes as zone delimiters. Quotation marks
(when appearing once in the sentence), com-
mas, colons, semicolons, exclamation and ques-
tion marks and periods are used as wall delimiters.
The use of zone delimiters do not alter the per-
formance. When using walls, a gain of 0.1
BLEU is obtained in our best model.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998575">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999769076923077">
For building our SMT systems, the open-source
SMT toolkit Moses (Koehn et al., 2007) was used
in its standard setup. The decoder includes a log-
linear model comprising a phrase-based transla-
tion model, a language model, a lexicalised dis-
tortion model and word and phrase penalties. The
weights of the log-linear interpolation were opti-
mised by means of MERT (Och, 2003). In addi-
tion, a 5-gram LM with Kneser-Ney (Kneser and
Ney, 1995) smoothing and interpolation was built
by means of the SRILM (Stolcke, 2002) toolkit.
For building our baseline system, the News-
Commentary and Europarl v5 (Koehn, 2005) data
were employed, with maximum sentence length
set to 40 in the case of the data used to build the
translation models, and without restriction in the
case of the LM. Statistics of the bilingual data can
be seen in Table 1.
In all the experiments reported, MERT was run
on the 2008 test set, whereas the test set 2009 was
considered as test set as such. In addition, all the
experiments described below were performed in
lowercase and tokenised conditions. For the fi-
nal run, the detokenisation and recasing was per-
formed according to the technique described in the
Workshop baseline description.
</bodyText>
<table confidence="0.9929336">
corpus |S ||W ||V |
Europarl 1822K 51M 172K
NC 108K 3M 68K
UN 6.2M 214M 411K
News 3.9M 107M 512K
</table>
<tableCaption confidence="0.958647666666667">
Table 2: Main figures of the Spanish resources
provided: Europarl v5, News-Commentary (NC),
United Nations (UN) and News-shuffled (News).
</tableCaption>
<subsectionHeader confidence="0.990507">
6.2 Language Model interpolation
</subsectionHeader>
<bodyText confidence="0.999964121212121">
The final system submitted to the shared task
included a linear interpolation of four language
models, one for each of the monolingual resources
available for Spanish (see Table 2). The results
can be seen in Table 3. As a first experiment, only
the in-domain corpus, i.e. the News-Commentary
data (NC data) was used for building the LM.
Then, all the available monolingual Spanish data
was included into a single LM, by concatenat-
ing all the data together (pooled). Next, in
interpolated, one LM for each one of the
provided monolingual resources was trained, and
then they were linearly interpolated so as to min-
imise the perplexity of the 2008 test set, and fed
such interpolation to the SMT system. We found
out that weights were distributed quite unevenly,
since the News-shuffled LM received a weight of
0.67, whereas the other three corpora received a
weight of 0.11 each. It must be noted that even
the in-domain LM received a weight of 0.11 (less
than the News-shuffled LM). The reason for this
might be that, although the in-domain LM should
be more appropriate and should receive a higher
weight, the News-shuffled corpus is also news re-
lated (hence not really out-of-domain), but much
larger. For this reason, the result of using only
such LM (News) was also analysed. As expected,
the translation quality dropped slightly. Never-
theless, since the differences are not statistically
significant, we used the News-shuffled LM for in-
ternal development purposes, and the interpolated
LM only whenever an improvement prooved to be
useful.
</bodyText>
<subsectionHeader confidence="0.999439">
6.3 Including UN data
</subsectionHeader>
<bodyText confidence="0.9993232">
We analysed the impact of the selection technique
detailed in Section 3. In this case, the LM used
was the interpolated LM described in the previous
section. The result can be seen in Table 4. As
it can be seen, translation quality as measured by
</bodyText>
<page confidence="0.999177">
174
</page>
<tableCaption confidence="0.999658">
Table 3: Effect of considering different LMs
</tableCaption>
<table confidence="0.9661516">
LM used BLEU
NC data 21.86
pooled 23.53
interpolated 24.97
news 24.79
</table>
<bodyText confidence="0.988961">
BLEU improves constantly as the number of sen-
tences selected increases. However, further sen-
tences were not included for computational rea-
sons.
In the same table, we also report the effect of
adding the UN sentences selected by our out-of-
vocabulary technique described in Section 4. In
this context, it should be noted that MERT was
not rerun once such sentences had been selected,
since such sentences are related with the test set,
and not with the development set on which MERT
is run.
</bodyText>
<tableCaption confidence="0.997921">
Table 4: Effect of including selected sentences
</tableCaption>
<table confidence="0.999649">
system BLEU
baseline 24.97
+ oovs 25.08
+ Level 1 24.98
+ Level 2 25.07
+ Level 3 25.13
</table>
<subsectionHeader confidence="0.990039">
6.4 Final system
</subsectionHeader>
<bodyText confidence="0.999826333333333">
Since the News-shuffled, UN and Europarl cor-
pora are large corpora, a new LM interpolation
was estimated by using a 6-gram LM on each one
of these corpora, obtaining a gain of 0.17 BLEU
points by doing so. Further increments in the n-
gram order did not show further improvements.
In addition, preliminary experimentation re-
vealed that the use of walls, as described in
Section 5, also provided slight improvements, al-
though using zones or combining both did not
prove to improve further. Hence, only walls
were included into the final system.
Lastly, the final system submitted to the Work-
shop was the result of combining all the techniques
described above. Such combination yielded a fi-
nal BLEU score of 25.31 on the 2009 test set, and
28.76 BLEU score on the 2010 test set, both in
tokenised and lowercased conditions.
</bodyText>
<sectionHeader confidence="0.990752" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999939333333333">
In this paper, the SMT system presented by the
UPV-PRHLT team for WMT 2010 has been de-
scribed. Specifically, preliminary results about
how to make use of larger data collections for
translating more focused test sets have been pre-
sented.
In this context, there are still some things which
need a deeper investigation, since the results pre-
sented here give only a small insight about the po-
tential of the similar sentence selection technique
described.
However, a deeper analysis is needed in order
to assess the potential of such technique and other
strategies should be implemented to explore new
kids of reordering constraints.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999822833333333">
This paper is based upon work supported by
the EC (FEDER/FSE) and the Spanish MICINN
under the MIPRCV “Consolider Ingenio 2010”
program (CSD2007-00018),iTrans2 (TIN2009-
14511) project, and the FPU scholarship AP2006-
00691. This work was also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and by the Generalitat Valen-
ciana under grant Prometeo/2009/014 and schol-
arships BFPI/2007/117 and ACIF/2010/226 and
by the Mexican government under the PROMEP-
DGEST program.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999063842105263">
Shahram Khadivi and Hermann Ney. 2005. Automatic
filtering of bilingual corpora for statistical machine
translation. In Natural Language Processing and In-
formation Systems, 10th Int. Conf. on Applications
of Natural Language to Information Systems, vol-
ume 3513 of Lecture Notes in Computer Science,
pages 263–274, Alicante, Spain, June. Springer.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, II:181–184, May.
Philipp Koehn and Barry Haddow. 2009. Edinburgh’s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In The 4th EACL Workshop on Statistical
Machine Translation, ACL, pages 160–164, Athens,
Greece, March. Springer.
P. Koehn et al. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of
</reference>
<page confidence="0.984971">
175
</page>
<reference confidence="0.996340230769231">
the ACL Demo and Poster Sessions, pages 177–180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
F.J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160–167, Sapporo, Japan.
A. Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proc. of ICSLP’02, pages 901–
904, September.
</reference>
<page confidence="0.99874">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.125289">
<title confidence="0.813821333333333">UPV-PRHLT English–Spanish system for WMT10 Sanchis-Trilles Andr´es-Ferrer Gonz´alez-Rubio Martinez-G´omez</title>
<author confidence="0.691998">S´anchez</author>
<affiliation confidence="0.623072666666667">Instituto Tecnol´ogico de Departamento de Sistemas Inform´aticos y Universidad Polit´ecnica de</affiliation>
<abstract confidence="0.991951266666667">the final evaluation, our system was ranked In this paper, the system submitted by the PRHLT group for the Fifth Workshop on Statistical Machine Translation of ACL2010 is presented. On this evaluation campaign, we have worked on the English–Spanish language pair, putting special emphasis on two problems derived from the large amount of data available. The first one, how to optimize the use of the monolingual data within the language model, and the second one, how to make good use of all the bilingual data provided without making use of unnecessary computational resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shahram Khadivi</author>
<author>Hermann Ney</author>
</authors>
<title>Automatic filtering of bilingual corpora for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Natural Language Processing and Information Systems, 10th Int. Conf. on Applications of Natural Language to Information Systems,</booktitle>
<volume>3513</volume>
<pages>263--274</pages>
<publisher>Springer.</publisher>
<location>Alicante, Spain,</location>
<contexts>
<context position="4010" citStr="Khadivi and Ney, 2005" startWordPosition="619" endWordPosition="622">e completely incorporated to the training set. However, the use of the UN corpus requires a big computational effort. In order to alleviate this problem, we have chosen only those bilingual sentences from the United Nations that are similar to the in-domain corpus sentences. As a similarity measure, we have chosen the alignment score. Alignment scores have already been used as a 172 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172–176, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics filter for noisy corpora (Khadivi and Ney, 2005). We trained an IBM model 4 using GIZA++ (Och and Ney, 2003) with the in-domain corpus and computed the alignment scores over the United Nations sentences. We assume that the alignment score is a good measure of similarity. An important factor in the alignment score is the length of the sentences, so we clustered the bilingual sentences in groups with the same sum of source and target language sentence sizes. In each of the groups, the higher the alignment score is, the more similar the sentence is to the in-domain corpus sentences. Hence, we computed the average alignment score for each one o</context>
</contexts>
<marker>Khadivi, Ney, 2005</marker>
<rawString>Shahram Khadivi and Hermann Ney. 2005. Automatic filtering of bilingual corpora for statistical machine translation. In Natural Language Processing and Information Systems, 10th Int. Conf. on Applications of Natural Language to Information Systems, volume 3513 of Lecture Notes in Computer Science, pages 263–274, Alicante, Spain, June. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing, II:181–184,</booktitle>
<contexts>
<context position="9252" citStr="Kneser and Ney, 1995" startWordPosition="1492" endWordPosition="1495"> used as wall delimiters. The use of zone delimiters do not alter the performance. When using walls, a gain of 0.1 BLEU is obtained in our best model. 6 Experiments 6.1 Experimental setup For building our SMT systems, the open-source SMT toolkit Moses (Koehn et al., 2007) was used in its standard setup. The decoder includes a loglinear model comprising a phrase-based translation model, a language model, a lexicalised distortion model and word and phrase penalties. The weights of the log-linear interpolation were optimised by means of MERT (Och, 2003). In addition, a 5-gram LM with Kneser-Ney (Kneser and Ney, 1995) smoothing and interpolation was built by means of the SRILM (Stolcke, 2002) toolkit. For building our baseline system, the NewsCommentary and Europarl v5 (Koehn, 2005) data were employed, with maximum sentence length set to 40 in the case of the data used to build the translation models, and without restriction in the case of the LM. Statistics of the bilingual data can be seen in Table 1. In all the experiments reported, MERT was run on the 2008 test set, whereas the test set 2009 was considered as test set as such. In addition, all the experiments described below were performed in lowercase</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. IEEE International Conference on Acoustics, Speech and Signal Processing, II:181–184, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Edinburgh’s submission to all tracks of the WMT2009 shared task with reordering and speed improvements to Moses.</title>
<date>2009</date>
<booktitle>In The 4th EACL Workshop on Statistical Machine Translation, ACL,</booktitle>
<pages>160--164</pages>
<publisher>Springer.</publisher>
<location>Athens, Greece,</location>
<contexts>
<context position="6937" citStr="Koehn and Haddow, 2009" startWordPosition="1096" endWordPosition="1100"> are added to the reduced training set. Finally, alignments with the resulting training set were computed and the usual training procedure for phrase-based systems was performed. 5 Walls and zones In translation, as in other linguistics areas, punctuation marks are essential as they help to understand the intention of a message and organise the ideas to avoid ambiguity. They also indicate pauses, hierarchies and emphasis. In our system, punctuation marks have been taken into account during decoding. Traditionally, in SMT punctuation marks are treated as words and this has undesirable effects (Koehn and Haddow, 2009). For example, commas have a high probability of occurrence and many possible translations are generated. Most of them are not consistent across languages. This introduces too much noise to the phrase tables. (Koehn and Haddow, 2009) established a framework to specify reordering constraints with walls and zones, where commas and end of sentence are not mixed with various clauses. Gains between 0.1 and 0.2 of BLEU are reported. Specifying zones and walls with XML tags in input sentences allows us to identify structured fragments that the Moses decoder uses with the following restrictions: 1. If</context>
</contexts>
<marker>Koehn, Haddow, 2009</marker>
<rawString>Philipp Koehn and Barry Haddow. 2009. Edinburgh’s submission to all tracks of the WMT2009 shared task with reordering and speed improvements to Moses. In The 4th EACL Workshop on Statistical Machine Translation, ACL, pages 160–164, Athens, Greece, March. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<marker>Koehn, 2007</marker>
<rawString>P. Koehn et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the ACL Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="9420" citStr="Koehn, 2005" startWordPosition="1520" endWordPosition="1521">ntal setup For building our SMT systems, the open-source SMT toolkit Moses (Koehn et al., 2007) was used in its standard setup. The decoder includes a loglinear model comprising a phrase-based translation model, a language model, a lexicalised distortion model and word and phrase penalties. The weights of the log-linear interpolation were optimised by means of MERT (Och, 2003). In addition, a 5-gram LM with Kneser-Ney (Kneser and Ney, 1995) smoothing and interpolation was built by means of the SRILM (Stolcke, 2002) toolkit. For building our baseline system, the NewsCommentary and Europarl v5 (Koehn, 2005) data were employed, with maximum sentence length set to 40 in the case of the data used to build the translation models, and without restriction in the case of the LM. Statistics of the bilingual data can be seen in Table 1. In all the experiments reported, MERT was run on the 2008 test set, whereas the test set 2009 was considered as test set as such. In addition, all the experiments described below were performed in lowercase and tokenised conditions. For the final run, the detokenisation and recasing was performed according to the technique described in the Workshop baseline description. c</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4070" citStr="Och and Ney, 2003" startWordPosition="631" endWordPosition="634">of the UN corpus requires a big computational effort. In order to alleviate this problem, we have chosen only those bilingual sentences from the United Nations that are similar to the in-domain corpus sentences. As a similarity measure, we have chosen the alignment score. Alignment scores have already been used as a 172 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172–176, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics filter for noisy corpora (Khadivi and Ney, 2005). We trained an IBM model 4 using GIZA++ (Och and Ney, 2003) with the in-domain corpus and computed the alignment scores over the United Nations sentences. We assume that the alignment score is a good measure of similarity. An important factor in the alignment score is the length of the sentences, so we clustered the bilingual sentences in groups with the same sum of source and target language sentence sizes. In each of the groups, the higher the alignment score is, the more similar the sentence is to the in-domain corpus sentences. Hence, we computed the average alignment score for each one of the clusters obtained for the corpus considered in-domain </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9187" citStr="Och, 2003" startWordPosition="1482" endWordPosition="1483">colons, exclamation and question marks and periods are used as wall delimiters. The use of zone delimiters do not alter the performance. When using walls, a gain of 0.1 BLEU is obtained in our best model. 6 Experiments 6.1 Experimental setup For building our SMT systems, the open-source SMT toolkit Moses (Koehn et al., 2007) was used in its standard setup. The decoder includes a loglinear model comprising a phrase-based translation model, a language model, a lexicalised distortion model and word and phrase penalties. The weights of the log-linear interpolation were optimised by means of MERT (Och, 2003). In addition, a 5-gram LM with Kneser-Ney (Kneser and Ney, 1995) smoothing and interpolation was built by means of the SRILM (Stolcke, 2002) toolkit. For building our baseline system, the NewsCommentary and Europarl v5 (Koehn, 2005) data were employed, with maximum sentence length set to 40 in the case of the data used to build the translation models, and without restriction in the case of the LM. Statistics of the bilingual data can be seen in Table 1. In all the experiments reported, MERT was run on the 2008 test set, whereas the test set 2009 was considered as test set as such. In addition</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP’02,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="9328" citStr="Stolcke, 2002" startWordPosition="1506" endWordPosition="1507">When using walls, a gain of 0.1 BLEU is obtained in our best model. 6 Experiments 6.1 Experimental setup For building our SMT systems, the open-source SMT toolkit Moses (Koehn et al., 2007) was used in its standard setup. The decoder includes a loglinear model comprising a phrase-based translation model, a language model, a lexicalised distortion model and word and phrase penalties. The weights of the log-linear interpolation were optimised by means of MERT (Och, 2003). In addition, a 5-gram LM with Kneser-Ney (Kneser and Ney, 1995) smoothing and interpolation was built by means of the SRILM (Stolcke, 2002) toolkit. For building our baseline system, the NewsCommentary and Europarl v5 (Koehn, 2005) data were employed, with maximum sentence length set to 40 in the case of the data used to build the translation models, and without restriction in the case of the LM. Statistics of the bilingual data can be seen in Table 1. In all the experiments reported, MERT was run on the 2008 test set, whereas the test set 2009 was considered as test set as such. In addition, all the experiments described below were performed in lowercase and tokenised conditions. For the final run, the detokenisation and recasin</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. of ICSLP’02, pages 901– 904, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>