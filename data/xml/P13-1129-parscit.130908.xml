<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001847">
<title confidence="0.989097">
Identification of Speakers in Novels
</title>
<author confidence="0.991258">
Hua He† Denilson Barbosa ‡ Grzegorz Kondrak‡
</author>
<affiliation confidence="0.9996645">
†Department of Computer Science ‡Department of Computing Science
University of Maryland University of Alberta
</affiliation>
<email confidence="0.998737">
huah@cs.umd.edu {denilson,gkondrak}@ualberta.ca
</email>
<sectionHeader confidence="0.993889" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961818181818">
Speaker identification is the task of at-
tributing utterances to characters in a lit-
erary narrative. It is challenging to auto-
mate because the speakers of the majority
of utterances are not explicitly identified in
novels. In this paper, we present a super-
vised machine learning approach for the
task that incorporates several novel fea-
tures. The experimental results show that
our method is more accurate and general
than previous approaches to the problem.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962179104478">
Novels are important as social communication
documents, in which novelists develop the plot
by means of discourse between various charac-
ters. In spite of a frequently expressed opinion
that all novels are simply variations of a certain
number of basic plots (Tobias, 2012), every novel
has a unique plot (or several plots) and a different
set of characters. The interactions among charac-
ters, especially in the form of conversations, help
the readers construct a mental model of the plot
and the changing relationships between charac-
ters. Many of the complexities of interpersonal re-
lationships, such as romantic interests, family ties,
and rivalries, are conveyed by utterances.
A precondition for understanding the relation-
ship between characters and plot development in
a novel is the identification of speakers behind all
utterances. However, the majority of utterances
are not explicitly tagged with speaker names, as
is the case in stage plays and film scripts. In most
cases, authors rely instead on the readers’ compre-
hension of the story and of the differences between
characters.
Since manual annotation of novels is costly, a
system for automatically determining speakers of
utterances would facilitate other tasks related to
the processing of literary texts. Speaker identifica-
tion could also be applied on its own, for instance
in generating high quality audio books without hu-
man lectors, where each character would be iden-
tifiable by a distinct way of speaking. In addi-
tion, research on spoken language processing for
broadcast and multi-party meetings (Salamin et
al., 2010; Favre et al., 2009) has demonstrated that
the analysis of dialogues is useful for the study of
social interactions.
In this paper, we investigate the task of speaker
identification in novels. Departing from previous
approaches, we develop a general system that can
be trained on relatively small annotated data sets,
and subsequently applied to other novels for which
no annotation is available. Since every novel has
its own set of characters, speaker identification
cannot be formulated as a straightforward tagging
problem with a universal set of fixed tags. Instead,
we adopt a ranking approach, which enables our
model to be applied to literary texts that are differ-
ent from the ones it has been trained on.
Our approach is grounded in a variety of fea-
tures that are easily generalizable across differ-
ent novels. Rather than attempt to construct com-
plete semantic models of the interactions, we ex-
ploit lexical and syntactic clues in the text itself.
We propose several novel features, including the
speaker alternation pattern, the presence of voca-
tives in utterances, and unsupervised actor-topic
features that associate speakers with utterances on
the basis of their content. Experimental evaluation
shows that our approach not only outperforms the
baseline, but also compares favorably to previous
approaches in terms of accuracy and generality,
even when tested on novels and authors that are
different from those used for training.
The paper is organized as follows. After dis-
cussing previous work, and defining the terminol-
ogy, we present our approach and the features that
it is based on. Next, we describe the data, the an-
</bodyText>
<page confidence="0.954043">
1312
</page>
<note confidence="0.913669">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1312–1320,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99969325">
notation details, and the results of our experimen-
tal evaluation. At the end, we discuss an applica-
tion to extracting a set of family relationships from
a novel.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999746909090909">
Previous work on speaker identification includes
both rule-based and machine-learning approaches.
Glass and Bangay (2007) propose a rule gener-
alization method with a scoring scheme that fo-
cuses on the speech verbs. The verbs, such as
said and cried, are extracted from the communi-
cation category of WordNet (Miller, 1995). The
speech-verb-actor pattern is applied to the utter-
ance, and the speaker is chosen from the avail-
able candidates on the basis of a scoring scheme.
Sarmento and Nunes (2009) present a similar ap-
proach for extracting speech quotes from online
news texts. They manually define 19 variations of
frequent speaker patterns, and identify a total of
35 candidate speech verbs. The rule-based meth-
ods are typically characterized by low coverage,
and are too brittle to be reliably applied to differ-
ent domains and changing styles.
Elson and McKeown (2010) (henceforth re-
ferred to as EM2010) apply the supervised ma-
chine learning paradigm to a corpus of utterances
extracted from novels. They construct a single
feature vector for each pair of an utterance and
a speaker candidate, and experiment with various
WEKA classifiers and score-combination meth-
ods. To identify the speaker of a given utterance,
they assume that all previous utterances are al-
ready correctly assigned to their speakers. Our
approach differs in considering the utterances in
a sequence, rather than independently from each
other, and in removing the unrealistic assumption
that the previous utterances are correctly identi-
fied.
The speaker identification task has also been in-
vestigated in other domains. Bethard et al. (2004)
identify opinion holders by using semantic pars-
ing techniques with additional linguistic features.
Pouliquen et al. (2007) aim at detecting direct
speech quotations in multilingual news. Krestel
et al. (2008) automatically tag speech sentences
in newspaper articles. Finally, Ruppenhofer et al.
(2010) implement a rule-based system to enrich
German cabinet protocols with automatic speaker
attribution.
</bodyText>
<sectionHeader confidence="0.980901" genericHeader="method">
3 Definitions and Conventions
</sectionHeader>
<bodyText confidence="0.99968193877551">
In this section, we introduce the terminology used
in the remainder of the paper. Our definitions are
different from those of EM2010 partly because we
developed our method independently, and partly
because we disagree with some of their choices.
The examples are from Jane Austen’s Pride and
Prejudice, which was the source of our develop-
ment set.
An utterance is a connected text that can be at-
tributed to a single speaker. Our task is to associate
each utterance with a single speaker. Utterances
that are attributable to more than one speaker are
rare; in such cases, we accept correctly identifying
one of the speakers as sufficient. In some cases, an
utterance may include more than one quotation-
delimited sequence of words, as in the following
example.
“Miss Bingley told me,” said Jane, “that
he never speaks much.”
In this case, the words said Jane are simply a
speaker tag inserted into the middle of the quoted
sentence. Unlike EM2010, we consider this a sin-
gle utterance, rather than two separate ones.
We assume that all utterances within a para-
graph can be attributed to a single speaker. This
“one speaker per paragraph” property is rarely vi-
olated in novels — we identified only five such
cases in Pride &amp; Prejudice, usually involving one
character citing another, or characters reading let-
ters containing quotations. We consider this an
acceptable simplification, much like assigning a
single part of speech to each word in a corpus.
We further assume that each utterance is contained
within a single paragraph. Exceptions to this rule
can be easily identified and resolved by detecting
quotation marks and other typographical conven-
tions.
The paragraphs without any quotations are re-
ferred to as narratives. The term dialogue denotes
a series of utterances together with related narra-
tives, which provide the context of conversations.
We define a dialogue as a series of utterances and
intervening narratives, with no more than three
continuous narratives. The rationale here is that
more than three narratives without any utterances
are likely to signal the end of a particular dialogue.
We distinguish three types of utterances, which
are listed with examples in Table 1: explicit
speaker (identified by name within the paragraph),
</bodyText>
<page confidence="0.959186">
1313
</page>
<table confidence="0.998927285714286">
Category Example
Implicit “Don’t keep coughing so, Kitty,
speaker for heaven’s sake!”
Explicit “I do not cough for my own
speaker amusement,” replied Kitty.
Anaphoric “Kitty has no discretion in her
speaker coughs,” said her father.
</table>
<tableCaption confidence="0.999892">
Table 1: Three types of utterances.
</tableCaption>
<bodyText confidence="0.999183">
anaphoric speaker (identified by an anaphoric ex-
pression), and implicit speaker (no speaker infor-
mation within the paragraph). Typically, the ma-
jority of utterances belong to the implicit-speaker
category. In Pride &amp; Prejudice only roughly 25%
of the utterances have explicit speakers, and an
even smaller 15% belong to the anaphoric-speaker
category. In modern fiction, the percentage of ex-
plicit attributions is even lower.
</bodyText>
<sectionHeader confidence="0.994342" genericHeader="method">
4 Speaker Identification
</sectionHeader>
<bodyText confidence="0.99990475">
In this section, we describe our method of extract-
ing explicit speakers, and our ranking approach,
which is designed to capture the speaker alterna-
tion pattern.
</bodyText>
<subsectionHeader confidence="0.999609">
4.1 Extracting Speakers
</subsectionHeader>
<bodyText confidence="0.99988809375">
We extract explicit speakers by focusing on the
speech verbs that appear before, after, or between
quotations. The following verbs cover most cases
in our development data: say, speak, talk, ask, re-
ply, answer, add, continue, go on, cry, sigh, and
think. If a verb from the above short list cannot be
found, any verb that is preceded by a name or a
personal pronoun in the vicinity of the utterance is
selected as the speech verb.
In order to locate the speaker’s name or
anaphoric expression, we apply a deterministic
method based on syntactic rules. First, all para-
graphs that include narrations are parsed with a
dependency parser. For example, consider the fol-
lowing paragraph:
As they went downstairs together, Char-
lotte said, “I shall depend on hearing
from you very often, Eliza.”
The parser identifies a number of dependency rela-
tions in the text, such as dobj(went-3, downstairs-
4) and advmod(went-3, together-5). Our method
extracts the speaker’s name from the dependency
relation nsubj(said-8, Charlotte-7), which links a
speech verb with a noun phrase that is the syntac-
tic subject of a clause.
Once an explicit speaker’s name or an anaphoric
expression is located, we determine the corre-
sponding gender information by referring to the
character list or by following straightforward rules
to handle the anaphora. For example, if the utter-
ance is followed by the phrase she said, we infer
that the gender of the speaker is female.
</bodyText>
<subsectionHeader confidence="0.980262">
4.2 Ranking Model
</subsectionHeader>
<bodyText confidence="0.999983388888889">
In spite of the highly sequential nature of the
chains of utterances, the speaker identification task
is difficult to model as sequential prediction. The
principal problem is that, unlike in many NLP
problems, a general fixed tag set cannot be de-
fined beyond the level of an individual novel.
Since we aim at a system that could be applied to
any novel with minimal pre-processing, sequential
prediction algorithms such as Conditional Ran-
dom Fields are not directly applicable.
We propose a more flexible approach that as-
signs scores to candidate speakers for each utter-
ance. Although the sequential information is not
directly modeled with tags, our system is able
to indirectly utilize the speaker alternation pat-
tern using the method described in the following
section. We implement our approach with SVM-
rank (Joachims, 2006).
</bodyText>
<subsectionHeader confidence="0.997035">
4.3 Speaker Alternation Pattern
</subsectionHeader>
<bodyText confidence="0.9990938">
The speaker alternation pattern is often employed
by authors in dialogues between two charac-
ters. After the speakers are identified explicitly at
the beginning of a dialogue, the remaining odd-
numbered and even-numbered utterances are at-
tributable to the first and second speaker, respec-
tively. If one of the speakers “misses their turn”, a
clue is provided in the text to reset the pattern.
Based on the speaker alternation pattern, we
make the following two observations:
</bodyText>
<listItem confidence="0.9547326">
1. The speakers of consecutive utterances are
usually different.
2. The speaker of the n-th utterance in a dia-
logue is likely to be the same as the speaker
of the (n − 2)-th utterance.
</listItem>
<bodyText confidence="0.9998795">
Our ranking model incorporates the speaker al-
ternation pattern by utilizing a feature expansion
scheme. For each utterance n, we first gener-
ate its own features (described in Section 5), and
</bodyText>
<page confidence="0.985971">
1314
</page>
<table confidence="0.973632">
Features Novelty
Distance to Utterance No
Speaker Appearance Count No
Speaker Name in Utterance No
Unsupervised Actor-Topic Model Yes
Vocative Speaker Name Yes
Neighboring Utterances Yes
Gender Matching Yes
Presence Matching Yes
</table>
<tableCaption confidence="0.972818">
Table 2: Principal feature sets.
</tableCaption>
<bodyText confidence="0.541755">
Feature Example
start of utterance “Kitty ...
</bodyText>
<construct confidence="0.46604925">
before period ... Jane.
between commas ..., Elizabeth, ...
between comma &amp; period ... , Mrs. Hurst.
before exclamation mark ... Mrs. Bennet!
before question mark ... Lizzy?.. .
vocative phrase Dear ...
after vocative phrase Oh! Lydia ...
2nd person pronoun ... you ...
</construct>
<tableCaption confidence="0.972761">
Table 3: Features for the vocative identification.
</tableCaption>
<bodyText confidence="0.9999812">
subsequently we add three more feature sets that
represent the following neighboring utterances:
n − 2, n − 1 and n + 1. Informally, the features of
the utterances n − 1 and n + 1 encode the first ob-
servation, while the features representing the utter-
ance n − 2 encode the second observation. In ad-
dition, we include a set of four binary features that
are set for the utterances in the range [n−2, n+1]
if the corresponding explicit speaker matches the
candidate speaker of the current utterance.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.9999584">
In this section, we describe the set of features used
in our ranking approach. The principal feature sets
are listed in Table 2, together with an indication
whether they are novel or have been used in previ-
ous work.
</bodyText>
<subsectionHeader confidence="0.990071">
5.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.999870111111111">
A subset of our features correspond to the features
that were proposed by EM2010. These are mostly
features related to speaker names. For example,
since names of speakers are often mentioned in
the vicinity of their utterances, we count the num-
ber of words separating the utterance and a name
mention. However, unlike EM2010, we consider
only the two nearest characters in each direction,
to reflect the observation that speakers tend to be
mentioned by name immediately before or after
their corresponding utterances. Another feature is
used to represent the number of appearances for
speaker candidates. This feature reflects the rela-
tive importance of a given character in the novel.
Finally, we use a feature to indicate the presence
or absence of a candidate speaker’s name within
the utterance. The intuition is that speakers are
unlikely to mention their own name.
</bodyText>
<subsectionHeader confidence="0.987179">
5.2 Vocatives
</subsectionHeader>
<bodyText confidence="0.990894862068965">
We propose a novel vocative feature, which en-
codes the character that is explicitly addressed in
an utterance. For example, consider the following
utterance:
“I hope Mr. Bingley will like it, Lizzy.”
Intuitively, the speaker of the utterance is neither
Mr. Bingley nor Lizzy; however, the speaker of the
next utterance is likely to be Lizzy. We aim at cap-
turing this intuition by identifying the addressee of
the utterance.
We manually annotated vocatives in about 900
utterances from the training set. About 25% of
the names within utterance were tagged as voca-
tives. A Logistic Regression classifier (Agresti,
2006) was trained to identify the vocatives. The
classifier features are shown in Table 3. The fea-
tures are designed to capture punctuation context,
as well as the presence of typical phrases that ac-
company vocatives. We also incorporate interjec-
tions like “oh!” and fixed phrases like “my dear”,
which are strong indicators of vocatives. Under
10-fold cross validation, the model achieved an F-
measure of 93.5% on the training set.
We incorporate vocatives in our speaker identi-
fication system by means of three binary features
that correspond to the utterances n −1, n − 2, and
n − 3. The features are set if the detected voca-
tive matches the candidate speaker of the current
utterance n.
</bodyText>
<subsectionHeader confidence="0.997906">
5.3 Matching Features
</subsectionHeader>
<bodyText confidence="0.999946333333333">
We incorporate two binary features for indicating
the gender and the presence of a candidate speaker.
The gender matching feature encodes the gender
agreement between a speaker candidate and the
speaker of the current utterance. The gender in-
formation extraction is applied to two utterance
</bodyText>
<page confidence="0.961752">
1315
</page>
<bodyText confidence="0.999796583333333">
groups: the anaphoric-speaker utterances, and the
explicit-speaker utterances. We use the technique
described in Section 4.1 to determine the gender
of a speaker of the current utterance. In contrast
with EM2010, this is not a hard constraint.
The presence matching feature indicates
whether a speaker candidate is a likely partic-
ipant in a dialogue. Each dialogue consists of
continuous utterance paragraphs together with
neighboring narration paragraphs as defined in
Section 3. The feature is set for a given character
if its name or alias appears within the dialogue.
</bodyText>
<subsectionHeader confidence="0.992773">
5.4 Unsupervised Actor-Topic Features
</subsectionHeader>
<bodyText confidence="0.999489461538462">
The final set of features is generated by the unsu-
pervised actor-topic model (ACTM) (Celikyilmaz
et al., 2010), which requires no annotated train-
ing data. The ACTM, as shown in Figure 1, ex-
tends the work of author-topic model in (Rosen-
Zvi et al., 2010). It can model dialogues in a lit-
erary text, which take place between two or more
speakers conversing on different topics, as distri-
butions over topics, which are also mixtures of the
term distributions associated with multiple speak-
ers. This follows the linguistic intuition that rich
contextual information can be useful in under-
standing dialogues.
</bodyText>
<figureCaption confidence="0.994468">
Figure 1: Graphical Representation of ACTM.
</figureCaption>
<bodyText confidence="0.999673">
The ACTM predicts the most likely speakers of
a given utterance by considering the content of an
utterance and its surrounding contexts. The Actor-
Topic-Term probabilities are calculated by using
both the relationship of utterances and the sur-
rounding textual clues. In our system, we utilize
four binary features that correspond to the four top
ranking positions from the ACTM model.
</bodyText>
<figureCaption confidence="0.995936">
Figure 2: Annotation Tool GUI.
</figureCaption>
<sectionHeader confidence="0.995767" genericHeader="method">
6 Data
</sectionHeader>
<bodyText confidence="0.999162545454545">
Our principal data set is derived from the text
of Pride and Prejudice, with chapters 19–26 as
the test set, chapters 27–33 as the development
set, and the remaining 46 chapters as the training
set. In order to ensure high-quality speaker anno-
tations, we developed a graphical interface (Fig-
ure 2), which displays the current utterance in con-
text, and a list of characters in the novel. After the
speaker is selected by clicking a button, the text
is scrolled automatically, with the next utterance
highlighted in yellow. The complete novel was
annotated by a student of English literature. The
annotations are publicly available1.
For the purpose of a generalization experiment,
we also utilize a corpus of utterances from the
19th and 20th century English novels compiled by
EM2010. The corpus differs from our data set in
three aspects. First, as discussed in Section 3, we
treat all quoted text within a single paragraph as
a single utterance, which reduces the total num-
ber of utterances, and results in a more realistic
reporting of accuracy. Second, our data set in-
cludes annotations for all utterances in the novel,
as opposed to only a subset of utterances from sev-
eral novels, which are not necessarily contiguous.
Lastly, our annotations come from a single expert,
while the annotations in the EM2010 corpus were
collected through Amazon’s Mechanical Turk, and
filtered by voting. For example, out of 308 utter-
ances from The Steppe, 244 are in fact annotated,
which raises the question whether the discarded
utterances tend to be more difficult to annotate.
Table 4 shows the number of utterances in all
</bodyText>
<footnote confidence="0.956402">
1www.cs.ualberta.ca/˜kondrak/austen
</footnote>
<page confidence="0.956501">
1316
</page>
<table confidence="0.9998452">
IS AS ES Total
Pride &amp; P. (all) 663 292 305 1260
Pride &amp; P. (test) 65 29 32 126
Emma 236 55 106 397
The Steppe 93 39 112 244
</table>
<tableCaption confidence="0.982771">
Table 4: The number of utterances in various
</tableCaption>
<bodyText confidence="0.988827333333333">
data sets by the type (IS - Implicit Speaker; AS
- Anaphoric Speaker; ES - Explicit Speaker).
data sets. We selected Jane Austen’s Emma as
a different novel by the same author, and Anton
Chekhov’s The Steppe as a novel by a different au-
thor for our generalization experiments.
Since our goal is to match utterances to charac-
ters rather than to name mentions, a preprocess-
ing step is performed to produce a list of char-
acters in the novel and their aliases. For exam-
ple, Elizabeth Bennet may be referred to as Liz,
Lizzy, Miss Lizzy, Miss Bennet, Miss Eliza, and
Miss Elizabeth Bennet. We apply a name entity
tagger, and then group the names into sets of char-
acter aliases, together with their gender informa-
tion. The sets of aliases are typically small, except
for major characters, and can be compiled with
the help of web resources, such as Wikipedia, or
study guides, such as CliffsNotesTM. This pre-
processing step could also be performed automati-
cally using a canonicalization method (Andrews et
al., 2012); however, since our focus is on speaker
identification, we decided to avoid introducing an-
notation errors at this stage.
Other preprocessing steps that are required for
processing a new novel include standarizing the
typographical conventions, and performing POS
tagging, NER tagging, and dependency parsing.
We utilize the Stanford tools (Toutanova et al.,
2003; Finkel et al., 2005; Marneffe et al., 2006).
</bodyText>
<sectionHeader confidence="0.99731" genericHeader="method">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999905363636364">
In this section, we describe experiments conducted
to evaluate our speaker identification approach.
We refer to our main model as NEIGHBORS, be-
cause it incorporates features from the neighbor-
ing utterances, as described in Section 4.3. In
contrast, the INDIVIDUAL model relies only on
features from the current utterance. In an at-
tempt to reproduce the evaluation methodology of
EM2010, we also test the ORACLE model, which
has access to the gold-standard information about
the speakers of eight neighboring utterances in the
</bodyText>
<table confidence="0.9990526">
Pride &amp; P. Emma Steppe
BASELINE 42.0 44.1 66.8
INDIVIDUAL 77.8 67.3 74.2
NEIGHBORS 82.5 74.8 80.3
ORACLE 86.5 80.1 83.6
</table>
<tableCaption confidence="0.956192">
Table 5: Speaker identification accuracy (in %) on
Pride &amp; Prejudice, Emma, and The Steppe.
</tableCaption>
<bodyText confidence="0.9988045">
range [n − 4, n + 4]. Lastly, the BASELINE ap-
proach selects the name that is the closest in the
narration, which is more accurate than the “most
recent name” baseline.
</bodyText>
<sectionHeader confidence="0.618695" genericHeader="method">
7.1 Results
</sectionHeader>
<bodyText confidence="0.999962916666667">
Table 5 shows the results of the models trained on
annotated utterances from Pride &amp; Prejudice on
three test sets. As expected, the accuracy of all
learning models on the test set that comes from
the same novel is higher than on unseen novels.
However, in both cases, the drop in accuracy for
the NEIGHBORS model is less than 10%.
Surprisingly, the accuracy is higher on The
Steppe than on Emma, even though the differ-
ent writing style of Chekhov should make the
task more difficult for models trained on Austen’s
prose. The protagonists of The Steppe are mostly
male, and the few female characters rarely speak
in the novel. This renders our gender feature
virtually useless, and results in lower accuracy
on anaphoric speakers than on explicit speakers.
On the other hand, Chekhov prefers to mention
speaker names in the dialogues (46% of utterances
are in the explicit-speaker category), which makes
his prose slightly easier in terms of speaker identi-
fication.
The relative order of the models is the same
on all three test sets, with the NEIGHBORS
model consistently outperforming the INDIVID-
UAL model, which indicates the importance of
capturing the speaker alternation pattern. The per-
formance of the NEIGHBORS model is actually
closer to the ORACLE model than to the INDIVID-
UAL model.
Table 6 shows the results on Emma broken
down according to the type of the utterance. Un-
surprisingly, the explicit speaker is the easiest cat-
egory, with nearly perfect accuracy. Both the IN-
DIVIDUAL and the NEIGHBORS models do better
on anaphoric speakers than on implicit speakers,
which is also expected. However, it is not the
</bodyText>
<page confidence="0.981511">
1317
</page>
<table confidence="0.99977525">
IS AS ES Total
INDIVIDUAL 52.5 67.3 100.0 67.3
NEIGHBORS 63.1 76.4 100.0 74.8
ORACLE 74.2 69.1 99.1 80.1
</table>
<tableCaption confidence="0.959392">
Table 6: Speaker identification accuracy (in %) on
</tableCaption>
<bodyText confidence="0.989101238095238">
Austen’s Emma by the type of utterance.
case for the ORACLE model. We conjecture that
the ORACLE model relies heavily on the neighbor-
hood features (which are rarely wrong), and con-
sequently tends to downplay the gender informa-
tion, which is the only information extracted from
the anaphora. In addition, anaphoric speaker is the
least frequent of the three categories.
Table 7 shows the results of an ablation study
performed to investigate the relative importance of
features. The INDIVIDUAL model serves as the
base model from which we remove specific fea-
tures. All tested features appear to contribute to
the overall performance, with the distance features
and the unsupervised actor-topic features having
the most pronounced impact. We conclude that the
incorporation of the neighboring features, which
is responsible for the difference between the IN-
DIVIDUAL and NEIGHBORS models, is similar in
terms of importance to our strongest textual fea-
tures.
</bodyText>
<table confidence="0.960142">
Feature Impact
Closest Mention -6.3
Unsupervised ACTM -5.6
Name within Utterance -4.8
Vocative -2.4
</table>
<tableCaption confidence="0.9897995">
Table 7: Results of feature ablation (in % accu-
racy) on Pride &amp; Prejudice.
</tableCaption>
<subsectionHeader confidence="0.995665">
7.2 Comparison to EM2010
</subsectionHeader>
<bodyText confidence="0.999821583333333">
In this section we analyze in more detail our re-
sults on Emma and The Steppe against the pub-
lished results of the state-of-the-art EM2010 sys-
tem. Recall that both novels form a part of the
corpus that was created by EM2010 for the devel-
opment of their system.
Direct comparison to EM2010 is difficult be-
cause they compute the accuracy separately for
seven different categories of utterances. For each
category, they experiment with all combinations
of three different classifiers and four score com-
bination methods, and report only the accuracy
</bodyText>
<table confidence="0.965201235294118">
Character
id name gender
. . .
9 Mr. Collins m
10 Charlotte f
11 Jane Bennet f
12 Elizabeth Bennet f
. . .
Relation
from to type mode
. . .
10 9 husband explicit
9 10 wife derived
10 12 friend explicit
12 10 friend derived
11 12 sister explicit
. . .
</table>
<figureCaption confidence="0.9719395">
Figure 3: Relational database with extracted social
network.
</figureCaption>
<bodyText confidence="0.997020176470588">
achieved by the best performing combination on
that category. In addition, they utilize the ground
truth speaker information of the preceding utter-
ances. Therefore, their results are best compared
against our ORACLE approach.
Unfortunately, EM2010 do not break down their
results by novel. They report the overall ac-
curacy of 63% on both “anaphora trigram” (our
anaphoric speaker), and “quote alone” (similar to
our implicit speaker). If we combine the two cate-
gories, the numbers corresponding to our NEIGH-
BORS model are 65.6% on Emma and 64.4% on
The Steppe, while ORACLE achieves 73.2% and
70.5%, respectively. Even though a direct com-
parison is not feasible, the numbers are remarkable
considering the context of the experiment, which
strongly favors the EM2010 system.
</bodyText>
<sectionHeader confidence="0.955588" genericHeader="method">
8 Extracting Family Relationships
</sectionHeader>
<bodyText confidence="0.998589">
In this section, we describe an application of
the speaker identification system to the extraction
of family relationships. Elson et al. (2010) ex-
tract unlabeled networks where the nodes repre-
sent characters and edges indicate their proxim-
ity, as indicated by their interactions. Our goal
is to construct networks in which edges are la-
beled by the mutual relationships between charac-
ters in a novel. We focus on family relationships,
but also include social relationships, such as friend
</bodyText>
<page confidence="0.969">
1318
</page>
<table confidence="0.534171166666667">
INSERT INTO Relation (id1, id2, t, m)
SELECT r.to AS id1, r.from AS id2 , ’wife’ AS t, ’derived’ AS m
FROM Relation r
WHERE r.type=’husband’ AND r.mode=’explicit’ AND
NOT EXISTS(SELECT * FROM Relation r2
WHERE r2.from=r.to AND r2.to=r.from AND r2.type=t)
</table>
<figureCaption confidence="0.996322">
Figure 4: An example inference rule.
</figureCaption>
<bodyText confidence="0.998540454545454">
and attracted-to.
Our approach to building a social network from
the novel is to build an active database of relation-
ships explicitly mentioned in the text, which is ex-
panded by triggering the execution of queries that
deduce implicit relations. This inference process
is repeated for every discovered relationship until
no new knowledge can be inferred.
The following example illustrates how speaker
identification helps in the extraction of social re-
lations among characters. Consider, the following
conversation:
“How so? how can it affect them?”
“My dear Mr. Bennet,” replied his wife,
“how can you be so tiresome!”
If the speakers are correctly identified, the utter-
ances are attributed to Mr. Bennet and Mrs. Ben-
net, respectively. Furthermore, the second utter-
ance implies that its speaker is the wife of the pre-
ceding speaker. This is an example of an explicit
relationship which is included in our database.
Several similar extraction rules are used to extract
explicit mentions indicating family and affective
relations, including mother, nephew, and fiancee.
We can also derive relationships that are not ex-
plicitly mentioned in the text; for example, that
Mr. Bennet is the husband of Mrs. Bennet.
Figure 3 shows a snippet of the relational
database of the network extracted from Pride &amp;
Prejudice. Table Character contains all characters
in the book, each with a unique identifier and gen-
der information, while Table Relation contains all
relationships that are explicitly mentioned in the
text or derived through reasoning.
Figure 4 shows an example of an inference rule
used in our system. The rule derives a new re-
lationship indicating that character c1 is the wife
of character c2 if it is known (through an explicit
mention in the text) that c2 is the husband of c1.
One condition for the rule to be applied is that the
database must not already contain a record indi-
cating the wife relationship. This inference rule
would derive the tuple in Figure 3 indicating that
the wife or Mr. Collins is Charlotte.
In our experiment with Pride &amp; Prejudice, a to-
tal of 55 explicitly indicated relationships were au-
tomatically identified once the utterances were at-
tributed to the characters. From those, another 57
implicit relationships were derived through infer-
ence. A preliminary manual inspection of the set
of relations extracted by this method (Makazhanov
et al., 2012) indicates that all of them are correct,
and include about 40% all personal relations that
can be inferred by a human reader from the text of
the novel.
</bodyText>
<sectionHeader confidence="0.978651" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9981725">
We have presented a novel approach to identifying
speakers of utterances in novels. Our system in-
corporates a variety of novel features which utilize
vocatives, unsupervised actor-topic models, and
the speaker alternation pattern. The results of our
evaluation experiments indicate a substantial im-
provement over the current state of the art.
There are several interesting directions for the
future work. Although the approach introduced
in this paper appears to be sufficiently general to
handle novels written in a different style and pe-
riod, more sophisticated statistical graphical mod-
els may achieve higher accuracy on this task. A re-
liable automatic generation of characters and their
aliases would remove the need for the preprocess-
ing step outlined in Section 6. The extraction of
social networks in novels that we discussed in Sec-
tion 8 would benefit from the introduction of ad-
ditional inference rules, and could be extended to
capture more subtle notions of sentiment or rela-
tionship among characters, as well as their devel-
opment over time.
We have demonstrated that speaker identifica-
tion can help extract family relationships, but the
converse is also true. Consider the following utter-
ance:
“Lizzy,” said her father, “I have given
him my consent.”
</bodyText>
<page confidence="0.983351">
1319
</page>
<bodyText confidence="0.999989">
In order to deduce the speaker of the utterance,
we need to combine the three pieces of informa-
tion: (a) the utterance is addressed to Lizzy (voca-
tive prediction), (b) the utterance is produced by
Lizzy’s father (pronoun resolution), and (c) Mr.
Bennet is the father of Lizzy (relationship ex-
traction). Similarly, in the task of compiling a
list of characters, which involves resolving aliases
such as Caroline, Caroline Bingley, and Miss Bin-
gley, simultaneous extraction of family relation-
ships would help detect the ambiguity of Miss
Benett, which can refer to any of several sis-
ters. A joint approach to resolving speaker attri-
bution, relationship extraction, co-reference reso-
lution, and alias-to-character mapping would not
only improve the accuracy on all these tasks, but
also represent a step towards deeper understanding
of complex plots and stories.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99988575">
We would like to thank Asli Celikyilmaz for col-
laboration in the early stages of this project, Su-
san Brown and Michelle Di Cintio for help with
data annotation, and David Elson for the attempt
to compute the accuracy of the EM2010 system
on Pride &amp; Prejudice. This research was partially
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999751583333333">
Alan Agresti. 2006. Building and applying logistic re-
gression models. In An Introduction to Categorical
Data Analysis. John Wiley &amp; Sons, Inc.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In EMNLP-CoNLL.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Auto-
matic extraction of opinion propositions and their
holders. In AAAI Spring Symposium on Exploring
Attitude and Affect in Text.
Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Grze-
gorz Kondrak, and Denilson Barbosa. 2010. The
actor-topic model for extracting social networks in
literary narrative. In Proceedings of the NIPS 2010
Workshop - Machine Learning for Social Comput-
ing.
David K. Elson and Kathleen McKeown. 2010. Auto-
matic attribution of quoted speech in literary narra-
tive. In AAAI.
David K. Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from literary
fiction. In ACL.
Sarah Favre, Alfred Dielmann, and Alessandro Vincia-
relli. 2009. Automatic role recognition in multi-
party recordings using social networks and proba-
bilistic sequential models. In ACM Multimedia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification in
fiction books. In Proceedings of the 18th Annual
Symposium of the Pattern Recognition.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In KDD.
Ralf Krestel, Sabine Bergler, and Ren´e Witte. 2008.
Minding the source: Automatic tagging of reported
speech in newspaper articles. In LREC.
Aibek Makazhanov, Denilson Barbosa, and Grzegorz
Kondrak. 2012. Extracting family relations from
literary fiction. Unpublished manuscript.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of theACM, 38:39–41.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In RANLP.
Michal Rosen-Zvi, Chaitanya Chemudugunta,
Thomas L. Griffiths, Padhraic Smyth, and Mark
Steyvers. 2010. Learning author-topic models from
text corpora. ACM Trans. Inf. Syst., 28(1).
Josef Ruppenhofer, Caroline Sporleder, and Fabian
Shirokov. 2010. Speaker attribution in cabinet pro-
tocols. In LREC.
Hugues Salamin, Alessandro Vinciarelli, Khiet Truong,
and Gelareh Mohammadi. 2010. Automatic role
recognition based on conversational and prosodic
behaviour. In ACM Multimedia.
Luis Sarmento and Sergio Nunes. 2009. Automatic ex-
traction of quotes and topics from news feeds. In 4th
Doctoral Symposium on Informatics Engineering.
Ronald B. Tobias. 2012. 20 Master Plots: And How to
Build Them. Writer’s Digest Books, 3rd edition.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT.
</reference>
<page confidence="0.990076">
1320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956764">
<title confidence="0.99565">Identification of Speakers in Novels</title>
<author confidence="0.963038">Barbosa</author>
<affiliation confidence="0.998933">of Computer Science of Computing Science University of Maryland University of Alberta</affiliation>
<abstract confidence="0.999692166666667">Speaker identification is the task of attributing utterances to characters in a literary narrative. It is challenging to automate because the speakers of the majority of utterances are not explicitly identified in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>Building and applying logistic regression models. In An Introduction to Categorical Data Analysis.</title>
<date>2006</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="15579" citStr="Agresti, 2006" startWordPosition="2489" endWordPosition="2490">propose a novel vocative feature, which encodes the character that is explicitly addressed in an utterance. For example, consider the following utterance: “I hope Mr. Bingley will like it, Lizzy.” Intuitively, the speaker of the utterance is neither Mr. Bingley nor Lizzy; however, the speaker of the next utterance is likely to be Lizzy. We aim at capturing this intuition by identifying the addressee of the utterance. We manually annotated vocatives in about 900 utterances from the training set. About 25% of the names within utterance were tagged as vocatives. A Logistic Regression classifier (Agresti, 2006) was trained to identify the vocatives. The classifier features are shown in Table 3. The features are designed to capture punctuation context, as well as the presence of typical phrases that accompany vocatives. We also incorporate interjections like “oh!” and fixed phrases like “my dear”, which are strong indicators of vocatives. Under 10-fold cross validation, the model achieved an Fmeasure of 93.5% on the training set. We incorporate vocatives in our speaker identification system by means of three binary features that correspond to the utterances n −1, n − 2, and n − 3. The features are se</context>
</contexts>
<marker>Agresti, 2006</marker>
<rawString>Alan Agresti. 2006. Building and applying logistic regression models. In An Introduction to Categorical Data Analysis. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Andrews</author>
<author>Jason Eisner</author>
<author>Mark Dredze</author>
</authors>
<title>Name phylogeny: A generative model of string variation.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="21097" citStr="Andrews et al., 2012" startWordPosition="3405" endWordPosition="3408">med to produce a list of characters in the novel and their aliases. For example, Elizabeth Bennet may be referred to as Liz, Lizzy, Miss Lizzy, Miss Bennet, Miss Eliza, and Miss Elizabeth Bennet. We apply a name entity tagger, and then group the names into sets of character aliases, together with their gender information. The sets of aliases are typically small, except for major characters, and can be compiled with the help of web resources, such as Wikipedia, or study guides, such as CliffsNotesTM. This preprocessing step could also be performed automatically using a canonicalization method (Andrews et al., 2012); however, since our focus is on speaker identification, we decided to avoid introducing annotation errors at this stage. Other preprocessing steps that are required for processing a new novel include standarizing the typographical conventions, and performing POS tagging, NER tagging, and dependency parsing. We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006). 7 Evaluation In this section, we describe experiments conducted to evaluate our speaker identification approach. We refer to our main model as NEIGHBORS, because it incorporates features fro</context>
</contexts>
<marker>Andrews, Eisner, Dredze, 2012</marker>
<rawString>Nicholas Andrews, Jason Eisner, and Mark Dredze. 2012. Name phylogeny: A generative model of string variation. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Hong Yu</author>
<author>Ashley Thornton</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Dan Jurafsky</author>
</authors>
<title>Automatic extraction of opinion propositions and their holders.</title>
<date>2004</date>
<booktitle>In AAAI Spring Symposium on Exploring Attitude and Affect in Text.</booktitle>
<contexts>
<context position="5935" citStr="Bethard et al. (2004)" startWordPosition="924" endWordPosition="927"> novels. They construct a single feature vector for each pair of an utterance and a speaker candidate, and experiment with various WEKA classifiers and score-combination methods. To identify the speaker of a given utterance, they assume that all previous utterances are already correctly assigned to their speakers. Our approach differs in considering the utterances in a sequence, rather than independently from each other, and in removing the unrealistic assumption that the previous utterances are correctly identified. The speaker identification task has also been investigated in other domains. Bethard et al. (2004) identify opinion holders by using semantic parsing techniques with additional linguistic features. Pouliquen et al. (2007) aim at detecting direct speech quotations in multilingual news. Krestel et al. (2008) automatically tag speech sentences in newspaper articles. Finally, Ruppenhofer et al. (2010) implement a rule-based system to enrich German cabinet protocols with automatic speaker attribution. 3 Definitions and Conventions In this section, we introduce the terminology used in the remainder of the paper. Our definitions are different from those of EM2010 partly because we developed our m</context>
</contexts>
<marker>Bethard, Yu, Thornton, Hatzivassiloglou, Jurafsky, 2004</marker>
<rawString>Steven Bethard, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic extraction of opinion propositions and their holders. In AAAI Spring Symposium on Exploring Attitude and Affect in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
<author>Hua He</author>
<author>Grzegorz Kondrak</author>
<author>Denilson Barbosa</author>
</authors>
<title>The actor-topic model for extracting social networks in literary narrative.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS 2010 Workshop - Machine Learning for Social Computing.</booktitle>
<contexts>
<context position="17302" citStr="Celikyilmaz et al., 2010" startWordPosition="2763" endWordPosition="2766">ique described in Section 4.1 to determine the gender of a speaker of the current utterance. In contrast with EM2010, this is not a hard constraint. The presence matching feature indicates whether a speaker candidate is a likely participant in a dialogue. Each dialogue consists of continuous utterance paragraphs together with neighboring narration paragraphs as defined in Section 3. The feature is set for a given character if its name or alias appears within the dialogue. 5.4 Unsupervised Actor-Topic Features The final set of features is generated by the unsupervised actor-topic model (ACTM) (Celikyilmaz et al., 2010), which requires no annotated training data. The ACTM, as shown in Figure 1, extends the work of author-topic model in (RosenZvi et al., 2010). It can model dialogues in a literary text, which take place between two or more speakers conversing on different topics, as distributions over topics, which are also mixtures of the term distributions associated with multiple speakers. This follows the linguistic intuition that rich contextual information can be useful in understanding dialogues. Figure 1: Graphical Representation of ACTM. The ACTM predicts the most likely speakers of a given utterance</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, He, Kondrak, Barbosa, 2010</marker>
<rawString>Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Grzegorz Kondrak, and Denilson Barbosa. 2010. The actor-topic model for extracting social networks in literary narrative. In Proceedings of the NIPS 2010 Workshop - Machine Learning for Social Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic attribution of quoted speech in literary narrative.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="5191" citStr="Elson and McKeown (2010)" startWordPosition="809" endWordPosition="812">d, are extracted from the communication category of WordNet (Miller, 1995). The speech-verb-actor pattern is applied to the utterance, and the speaker is chosen from the available candidates on the basis of a scoring scheme. Sarmento and Nunes (2009) present a similar approach for extracting speech quotes from online news texts. They manually define 19 variations of frequent speaker patterns, and identify a total of 35 candidate speech verbs. The rule-based methods are typically characterized by low coverage, and are too brittle to be reliably applied to different domains and changing styles. Elson and McKeown (2010) (henceforth referred to as EM2010) apply the supervised machine learning paradigm to a corpus of utterances extracted from novels. They construct a single feature vector for each pair of an utterance and a speaker candidate, and experiment with various WEKA classifiers and score-combination methods. To identify the speaker of a given utterance, they assume that all previous utterances are already correctly assigned to their speakers. Our approach differs in considering the utterances in a sequence, rather than independently from each other, and in removing the unrealistic assumption that the </context>
</contexts>
<marker>Elson, McKeown, 2010</marker>
<rawString>David K. Elson and Kathleen McKeown. 2010. Automatic attribution of quoted speech in literary narrative. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="27184" citStr="Elson et al. (2010)" startWordPosition="4405" endWordPosition="4408"> both “anaphora trigram” (our anaphoric speaker), and “quote alone” (similar to our implicit speaker). If we combine the two categories, the numbers corresponding to our NEIGHBORS model are 65.6% on Emma and 64.4% on The Steppe, while ORACLE achieves 73.2% and 70.5%, respectively. Even though a direct comparison is not feasible, the numbers are remarkable considering the context of the experiment, which strongly favors the EM2010 system. 8 Extracting Family Relationships In this section, we describe an application of the speaker identification system to the extraction of family relationships. Elson et al. (2010) extract unlabeled networks where the nodes represent characters and edges indicate their proximity, as indicated by their interactions. Our goal is to construct networks in which edges are labeled by the mutual relationships between characters in a novel. We focus on family relationships, but also include social relationships, such as friend 1318 INSERT INTO Relation (id1, id2, t, m) SELECT r.to AS id1, r.from AS id2 , ’wife’ AS t, ’derived’ AS m FROM Relation r WHERE r.type=’husband’ AND r.mode=’explicit’ AND NOT EXISTS(SELECT * FROM Relation r2 WHERE r2.from=r.to AND r2.to=r.from AND r2.typ</context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David K. Elson, Nicholas Dames, and Kathleen McKeown. 2010. Extracting social networks from literary fiction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Favre</author>
<author>Alfred Dielmann</author>
<author>Alessandro Vinciarelli</author>
</authors>
<title>Automatic role recognition in multiparty recordings using social networks and probabilistic sequential models.</title>
<date>2009</date>
<booktitle>In ACM Multimedia.</booktitle>
<contexts>
<context position="2332" citStr="Favre et al., 2009" startWordPosition="354" endWordPosition="357">rely instead on the readers’ comprehension of the story and of the differences between characters. Since manual annotation of novels is costly, a system for automatically determining speakers of utterances would facilitate other tasks related to the processing of literary texts. Speaker identification could also be applied on its own, for instance in generating high quality audio books without human lectors, where each character would be identifiable by a distinct way of speaking. In addition, research on spoken language processing for broadcast and multi-party meetings (Salamin et al., 2010; Favre et al., 2009) has demonstrated that the analysis of dialogues is useful for the study of social interactions. In this paper, we investigate the task of speaker identification in novels. Departing from previous approaches, we develop a general system that can be trained on relatively small annotated data sets, and subsequently applied to other novels for which no annotation is available. Since every novel has its own set of characters, speaker identification cannot be formulated as a straightforward tagging problem with a universal set of fixed tags. Instead, we adopt a ranking approach, which enables our m</context>
</contexts>
<marker>Favre, Dielmann, Vinciarelli, 2009</marker>
<rawString>Sarah Favre, Alfred Dielmann, and Alessandro Vinciarelli. 2009. Automatic role recognition in multiparty recordings using social networks and probabilistic sequential models. In ACM Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="21481" citStr="Finkel et al., 2005" startWordPosition="3462" endWordPosition="3465">acters, and can be compiled with the help of web resources, such as Wikipedia, or study guides, such as CliffsNotesTM. This preprocessing step could also be performed automatically using a canonicalization method (Andrews et al., 2012); however, since our focus is on speaker identification, we decided to avoid introducing annotation errors at this stage. Other preprocessing steps that are required for processing a new novel include standarizing the typographical conventions, and performing POS tagging, NER tagging, and dependency parsing. We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006). 7 Evaluation In this section, we describe experiments conducted to evaluate our speaker identification approach. We refer to our main model as NEIGHBORS, because it incorporates features from the neighboring utterances, as described in Section 4.3. In contrast, the INDIVIDUAL model relies only on features from the current utterance. In an attempt to reproduce the evaluation methodology of EM2010, we also test the ORACLE model, which has access to the gold-standard information about the speakers of eight neighboring utterances in the Pride &amp; P. Emma Steppe BASELINE 42.</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Glass</author>
<author>Shaun Bangay</author>
</authors>
<title>A naive salience-based method for speaker identification in fiction books.</title>
<date>2007</date>
<booktitle>In Proceedings of the 18th Annual Symposium of the Pattern Recognition.</booktitle>
<contexts>
<context position="4441" citStr="Glass and Bangay (2007)" startWordPosition="684" endWordPosition="687">ork, and defining the terminology, we present our approach and the features that it is based on. Next, we describe the data, the an1312 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1312–1320, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics notation details, and the results of our experimental evaluation. At the end, we discuss an application to extracting a set of family relationships from a novel. 2 Related Work Previous work on speaker identification includes both rule-based and machine-learning approaches. Glass and Bangay (2007) propose a rule generalization method with a scoring scheme that focuses on the speech verbs. The verbs, such as said and cried, are extracted from the communication category of WordNet (Miller, 1995). The speech-verb-actor pattern is applied to the utterance, and the speaker is chosen from the available candidates on the basis of a scoring scheme. Sarmento and Nunes (2009) present a similar approach for extracting speech quotes from online news texts. They manually define 19 variations of frequent speaker patterns, and identify a total of 35 candidate speech verbs. The rule-based methods are </context>
</contexts>
<marker>Glass, Bangay, 2007</marker>
<rawString>Kevin Glass and Shaun Bangay. 2007. A naive salience-based method for speaker identification in fiction books. In Proceedings of the 18th Annual Symposium of the Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="11817" citStr="Joachims, 2006" startWordPosition="1869" endWordPosition="1870">eneral fixed tag set cannot be defined beyond the level of an individual novel. Since we aim at a system that could be applied to any novel with minimal pre-processing, sequential prediction algorithms such as Conditional Random Fields are not directly applicable. We propose a more flexible approach that assigns scores to candidate speakers for each utterance. Although the sequential information is not directly modeled with tags, our system is able to indirectly utilize the speaker alternation pattern using the method described in the following section. We implement our approach with SVMrank (Joachims, 2006). 4.3 Speaker Alternation Pattern The speaker alternation pattern is often employed by authors in dialogues between two characters. After the speakers are identified explicitly at the beginning of a dialogue, the remaining oddnumbered and even-numbered utterances are attributable to the first and second speaker, respectively. If one of the speakers “misses their turn”, a clue is provided in the text to reset the pattern. Based on the speaker alternation pattern, we make the following two observations: 1. The speakers of consecutive utterances are usually different. 2. The speaker of the n-th u</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Krestel</author>
<author>Sabine Bergler</author>
<author>Ren´e Witte</author>
</authors>
<title>Minding the source: Automatic tagging of reported speech in newspaper articles.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="6144" citStr="Krestel et al. (2008)" startWordPosition="954" endWordPosition="957">iven utterance, they assume that all previous utterances are already correctly assigned to their speakers. Our approach differs in considering the utterances in a sequence, rather than independently from each other, and in removing the unrealistic assumption that the previous utterances are correctly identified. The speaker identification task has also been investigated in other domains. Bethard et al. (2004) identify opinion holders by using semantic parsing techniques with additional linguistic features. Pouliquen et al. (2007) aim at detecting direct speech quotations in multilingual news. Krestel et al. (2008) automatically tag speech sentences in newspaper articles. Finally, Ruppenhofer et al. (2010) implement a rule-based system to enrich German cabinet protocols with automatic speaker attribution. 3 Definitions and Conventions In this section, we introduce the terminology used in the remainder of the paper. Our definitions are different from those of EM2010 partly because we developed our method independently, and partly because we disagree with some of their choices. The examples are from Jane Austen’s Pride and Prejudice, which was the source of our development set. An utterance is a connected</context>
</contexts>
<marker>Krestel, Bergler, Witte, 2008</marker>
<rawString>Ralf Krestel, Sabine Bergler, and Ren´e Witte. 2008. Minding the source: Automatic tagging of reported speech in newspaper articles. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aibek Makazhanov</author>
<author>Denilson Barbosa</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Extracting family relations from literary fiction.</title>
<date>2012</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="30209" citStr="Makazhanov et al., 2012" startWordPosition="4898" endWordPosition="4901">c2 is the husband of c1. One condition for the rule to be applied is that the database must not already contain a record indicating the wife relationship. This inference rule would derive the tuple in Figure 3 indicating that the wife or Mr. Collins is Charlotte. In our experiment with Pride &amp; Prejudice, a total of 55 explicitly indicated relationships were automatically identified once the utterances were attributed to the characters. From those, another 57 implicit relationships were derived through inference. A preliminary manual inspection of the set of relations extracted by this method (Makazhanov et al., 2012) indicates that all of them are correct, and include about 40% all personal relations that can be inferred by a human reader from the text of the novel. 9 Conclusion and Future Work We have presented a novel approach to identifying speakers of utterances in novels. Our system incorporates a variety of novel features which utilize vocatives, unsupervised actor-topic models, and the speaker alternation pattern. The results of our evaluation experiments indicate a substantial improvement over the current state of the art. There are several interesting directions for the future work. Although the </context>
</contexts>
<marker>Makazhanov, Barbosa, Kondrak, 2012</marker>
<rawString>Aibek Makazhanov, Denilson Barbosa, and Grzegorz Kondrak. 2012. Extracting family relations from literary fiction. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of theACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="4641" citStr="Miller, 1995" startWordPosition="721" endWordPosition="722">l Linguistics, pages 1312–1320, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics notation details, and the results of our experimental evaluation. At the end, we discuss an application to extracting a set of family relationships from a novel. 2 Related Work Previous work on speaker identification includes both rule-based and machine-learning approaches. Glass and Bangay (2007) propose a rule generalization method with a scoring scheme that focuses on the speech verbs. The verbs, such as said and cried, are extracted from the communication category of WordNet (Miller, 1995). The speech-verb-actor pattern is applied to the utterance, and the speaker is chosen from the available candidates on the basis of a scoring scheme. Sarmento and Nunes (2009) present a similar approach for extracting speech quotes from online news texts. They manually define 19 variations of frequent speaker patterns, and identify a total of 35 candidate speech verbs. The rule-based methods are typically characterized by low coverage, and are too brittle to be reliably applied to different domains and changing styles. Elson and McKeown (2010) (henceforth referred to as EM2010) apply the supe</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications of theACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Clive Best</author>
</authors>
<title>Automatic detection of quotations in multilingual news.</title>
<date>2007</date>
<booktitle>In RANLP.</booktitle>
<contexts>
<context position="6058" citStr="Pouliquen et al. (2007)" startWordPosition="941" endWordPosition="944">h various WEKA classifiers and score-combination methods. To identify the speaker of a given utterance, they assume that all previous utterances are already correctly assigned to their speakers. Our approach differs in considering the utterances in a sequence, rather than independently from each other, and in removing the unrealistic assumption that the previous utterances are correctly identified. The speaker identification task has also been investigated in other domains. Bethard et al. (2004) identify opinion holders by using semantic parsing techniques with additional linguistic features. Pouliquen et al. (2007) aim at detecting direct speech quotations in multilingual news. Krestel et al. (2008) automatically tag speech sentences in newspaper articles. Finally, Ruppenhofer et al. (2010) implement a rule-based system to enrich German cabinet protocols with automatic speaker attribution. 3 Definitions and Conventions In this section, we introduce the terminology used in the remainder of the paper. Our definitions are different from those of EM2010 partly because we developed our method independently, and partly because we disagree with some of their choices. The examples are from Jane Austen’s Pride a</context>
</contexts>
<marker>Pouliquen, Steinberger, Best, 2007</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, and Clive Best. 2007. Automatic detection of quotations in multilingual news. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Chaitanya Chemudugunta</author>
<author>Thomas L Griffiths</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Learning author-topic models from text corpora.</title>
<date>2010</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>Rosen-Zvi, Chemudugunta, Griffiths, Smyth, Steyvers, 2010</marker>
<rawString>Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas L. Griffiths, Padhraic Smyth, and Mark Steyvers. 2010. Learning author-topic models from text corpora. ACM Trans. Inf. Syst., 28(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Fabian Shirokov</author>
</authors>
<title>Speaker attribution in cabinet protocols.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="6237" citStr="Ruppenhofer et al. (2010)" startWordPosition="966" endWordPosition="969">o their speakers. Our approach differs in considering the utterances in a sequence, rather than independently from each other, and in removing the unrealistic assumption that the previous utterances are correctly identified. The speaker identification task has also been investigated in other domains. Bethard et al. (2004) identify opinion holders by using semantic parsing techniques with additional linguistic features. Pouliquen et al. (2007) aim at detecting direct speech quotations in multilingual news. Krestel et al. (2008) automatically tag speech sentences in newspaper articles. Finally, Ruppenhofer et al. (2010) implement a rule-based system to enrich German cabinet protocols with automatic speaker attribution. 3 Definitions and Conventions In this section, we introduce the terminology used in the remainder of the paper. Our definitions are different from those of EM2010 partly because we developed our method independently, and partly because we disagree with some of their choices. The examples are from Jane Austen’s Pride and Prejudice, which was the source of our development set. An utterance is a connected text that can be attributed to a single speaker. Our task is to associate each utterance wit</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Shirokov, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, and Fabian Shirokov. 2010. Speaker attribution in cabinet protocols. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugues Salamin</author>
<author>Alessandro Vinciarelli</author>
<author>Khiet Truong</author>
<author>Gelareh Mohammadi</author>
</authors>
<title>Automatic role recognition based on conversational and prosodic behaviour.</title>
<date>2010</date>
<booktitle>In ACM Multimedia.</booktitle>
<contexts>
<context position="2311" citStr="Salamin et al., 2010" startWordPosition="350" endWordPosition="353">n most cases, authors rely instead on the readers’ comprehension of the story and of the differences between characters. Since manual annotation of novels is costly, a system for automatically determining speakers of utterances would facilitate other tasks related to the processing of literary texts. Speaker identification could also be applied on its own, for instance in generating high quality audio books without human lectors, where each character would be identifiable by a distinct way of speaking. In addition, research on spoken language processing for broadcast and multi-party meetings (Salamin et al., 2010; Favre et al., 2009) has demonstrated that the analysis of dialogues is useful for the study of social interactions. In this paper, we investigate the task of speaker identification in novels. Departing from previous approaches, we develop a general system that can be trained on relatively small annotated data sets, and subsequently applied to other novels for which no annotation is available. Since every novel has its own set of characters, speaker identification cannot be formulated as a straightforward tagging problem with a universal set of fixed tags. Instead, we adopt a ranking approach</context>
</contexts>
<marker>Salamin, Vinciarelli, Truong, Mohammadi, 2010</marker>
<rawString>Hugues Salamin, Alessandro Vinciarelli, Khiet Truong, and Gelareh Mohammadi. 2010. Automatic role recognition based on conversational and prosodic behaviour. In ACM Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Sarmento</author>
<author>Sergio Nunes</author>
</authors>
<title>Automatic extraction of quotes and topics from news feeds.</title>
<date>2009</date>
<booktitle>In 4th Doctoral Symposium on Informatics Engineering.</booktitle>
<contexts>
<context position="4817" citStr="Sarmento and Nunes (2009)" startWordPosition="749" endWordPosition="752">ntal evaluation. At the end, we discuss an application to extracting a set of family relationships from a novel. 2 Related Work Previous work on speaker identification includes both rule-based and machine-learning approaches. Glass and Bangay (2007) propose a rule generalization method with a scoring scheme that focuses on the speech verbs. The verbs, such as said and cried, are extracted from the communication category of WordNet (Miller, 1995). The speech-verb-actor pattern is applied to the utterance, and the speaker is chosen from the available candidates on the basis of a scoring scheme. Sarmento and Nunes (2009) present a similar approach for extracting speech quotes from online news texts. They manually define 19 variations of frequent speaker patterns, and identify a total of 35 candidate speech verbs. The rule-based methods are typically characterized by low coverage, and are too brittle to be reliably applied to different domains and changing styles. Elson and McKeown (2010) (henceforth referred to as EM2010) apply the supervised machine learning paradigm to a corpus of utterances extracted from novels. They construct a single feature vector for each pair of an utterance and a speaker candidate, </context>
</contexts>
<marker>Sarmento, Nunes, 2009</marker>
<rawString>Luis Sarmento and Sergio Nunes. 2009. Automatic extraction of quotes and topics from news feeds. In 4th Doctoral Symposium on Informatics Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald B Tobias</author>
</authors>
<title>20 Master Plots: And How to Build Them. Writer’s Digest Books, 3rd edition.</title>
<date>2012</date>
<contexts>
<context position="994" citStr="Tobias, 2012" startWordPosition="146" endWordPosition="147">cause the speakers of the majority of utterances are not explicitly identified in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem. 1 Introduction Novels are important as social communication documents, in which novelists develop the plot by means of discourse between various characters. In spite of a frequently expressed opinion that all novels are simply variations of a certain number of basic plots (Tobias, 2012), every novel has a unique plot (or several plots) and a different set of characters. The interactions among characters, especially in the form of conversations, help the readers construct a mental model of the plot and the changing relationships between characters. Many of the complexities of interpersonal relationships, such as romantic interests, family ties, and rivalries, are conveyed by utterances. A precondition for understanding the relationship between characters and plot development in a novel is the identification of speakers behind all utterances. However, the majority of utterance</context>
</contexts>
<marker>Tobias, 2012</marker>
<rawString>Ronald B. Tobias. 2012. 20 Master Plots: And How to Build Them. Writer’s Digest Books, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="21460" citStr="Toutanova et al., 2003" startWordPosition="3458" endWordPosition="3461">l, except for major characters, and can be compiled with the help of web resources, such as Wikipedia, or study guides, such as CliffsNotesTM. This preprocessing step could also be performed automatically using a canonicalization method (Andrews et al., 2012); however, since our focus is on speaker identification, we decided to avoid introducing annotation errors at this stage. Other preprocessing steps that are required for processing a new novel include standarizing the typographical conventions, and performing POS tagging, NER tagging, and dependency parsing. We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006). 7 Evaluation In this section, we describe experiments conducted to evaluate our speaker identification approach. We refer to our main model as NEIGHBORS, because it incorporates features from the neighboring utterances, as described in Section 4.3. In contrast, the INDIVIDUAL model relies only on features from the current utterance. In an attempt to reproduce the evaluation methodology of EM2010, we also test the ORACLE model, which has access to the gold-standard information about the speakers of eight neighboring utterances in the Pride &amp; P. Emm</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In NAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>