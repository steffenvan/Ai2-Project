<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.9978565">
Machine Learning Based Semantic Inference: Experiments and Ob-
servations at RTE-3
</title>
<author confidence="0.992947">
Baoli Li1, Joseph Irwin1, Ernest V. Garcia2, and Ashwin Ram1
</author>
<affiliation confidence="0.9919985">
1 College of Computing
Georgia Institute of Technology
</affiliation>
<address confidence="0.953662">
Atlanta, GA 30332, USA
</address>
<email confidence="0.991134666666667">
baoli@gatech.edu
gtg519g@mail.gatech.edu
ashwin@cc.gatech.edu
</email>
<sectionHeader confidence="0.994464" genericHeader="abstract">
Abstract
</sectionHeader>
<affiliation confidence="0.461718">
2 Department of Radiology
School of Medicine, Emory University
</affiliation>
<address confidence="0.559405">
Atlanta, GA 30322, USA
</address>
<email confidence="0.825177">
Ernest.Garcia@emoryhealthcare.org
</email>
<bodyText confidence="0.999955185185185">
pair (pair id 5) from the RTE-3 development
dataset is as follows:
Textual Entailment Recognition is a se-
mantic inference task that is required in
many natural language processing (NLP)
applications. In this paper, we present our
system for the third PASCAL recognizing
textual entailment (RTE-3) challenge. The
system is built on a machine learning
framework with the following features de-
rived by state-of-the-art NLP techniques:
lexical semantic similarity (LSS), named
entities (NE), dependent content word pairs
(DEP), average distance (DIST), negation
(NG), task (TK), and text length (LEN). On
the RTE-3 test dataset, our system achieves
the accuracy of 0.64 and 0.6488 for the two
official submissions, respectively. Experi-
mental results show that LSS and NE are
the most effective features. Further analy-
ses indicate that a baseline dummy system
can achieve accuracy 0.545 on the RTE-3
test dataset, which makes RTE-3 relatively
easier than RTE-2 and RTE-1. In addition,
we demonstrate with examples that the cur-
rent Average Precision measure and its
evaluation process need to be changed.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999460025641025">
Textual entailment is a relation between two text
snippets in which the meaning of one snippet,
called the hypothesis (H), can be inferred from the
other snippet, called the text (T). Textual
entailment recognition is the task of deciding
whether a given T entails a given H. An example
T: A bus collision with a truck in Uganda has resulted
in at least 30 fatalities and has left a further 21 injured.
H: 30 die in a bus collision in Uganda.
Given such a pair, a recognizing textual entail-
ment (RTE) system should output its judgement
about whether or not an entailment relation holds
between them. For the above example pair, H is
entailed by T.
The PASCAL Recognizing Textual Entailment
Challenge is an annual challenge on this task
which has been held since 2005 (Dagan et al.,
2006; Bar-Haim et al. 2006). As textual entailment
recognition is thought to be a common underlying
semantic inference task for many natural language
processing applications, such as Information Ex-
traction (IE), Information Retrieval (IR), Question
Answering (QA), and Document Summarization
(SUM), the PASCAL RTE Challenge has been
gaining more and more attention in the NLP com-
munity. In the past challenges, various approaches
to recognizing textual entailment have been pro-
posed, from syntactic analysis to logical inference
(Bar-Haim et al. 2006).
As a new participant, we have two goals by at-
tending the RTE-3 Challenge: first, we would like
to explore how state-of-the-art language techniques
help to deal with this semantic inference problem;
second, we try to obtain a more thorough knowl-
edge of this research and its state-of-the-art.
Inspired by the success of machine learning
techniques in RTE-2, we employ the same strategy
in our RTE-3 system. Several lexical, syntactical,
and semantical language analysis techniques are
</bodyText>
<page confidence="0.98403">
159
</page>
<bodyText confidence="0.953159642857143">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 159–164,
Prague, June 2007. c�2007 Association for Computational Linguistics
explored to derive effective features for determin-
ing textual entailment relation. Then, a general
machine learning algorithm is applied on the trans-
formed data for training and prediction. Our two
official submissions achieve accuracy 0.64 and
0.6488, respectively.
In the rest of this paper we describe the detail of
our system and analyze the results. Section 2 gives
the overview of our system, while Section 3 dis-
cusses the various features in-depth. We present
our experiments and discussions in Section 4, and
conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.976949" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9856955">
Figure 1 gives the architecture of our RTE-3 sys-
tem, which finishes the process of both training
and prediction in two stages. At the first stage, a T-
H pair goes through language processing and fea-
ture extraction modules, and is finally converted to
a set of feature-values. At the second stage, a ma-
chine learning algorithm is applied to obtain an
inference/prediction model when training or output
its decision when predicting.
In the language processing module, we try to
analyze T-H pairs with the state-of-the-art NLP
techniques, including lexical, syntactical, and se-
mantical analyses. We first split text into sentences,
and tag the Part of Speech (POS) of each word.
The text with POS information is then fed into
three separate modules: a named entities recog-
nizer, a word sense disambiguation (WSD) module,
and a dependency parser. These language analyz-
ers output their own intermediate representations
for the feature extraction module.
We produce seven features for each T-H pair:
lexical semantic similarity (LSS), named entities
(NE), dependent content word pairs (DEP), aver-
age distance (DIST), negation (NG), task (TK),
and text length (LEN). The last two features are
extracted from each pair itself, while others are
based on the results of language analyzers.
The resources that we used in our RTE-3 system
include:
OAK: a general English analysis tool (Sekine
2002). It is used for sentence splitting, POS tag-
ging, and named entities recognition.
WordNet::SenseRelate::Allwords package: a
word sense disambiguation (WSD) module for as-
signing each content word a sense from WordNet
(Pedersen et al., 2005). It is used in WSD module.
</bodyText>
<figureCaption confidence="0.996779">
Figure 1. System Architecture.
</figureCaption>
<bodyText confidence="0.991025642857143">
WordNet::Similarity package: a Perl module
that implements a variety of semantic similarity
and relatedness measures based on WordNet (Pe-
dersen et al., 2005). This package is used for deriv-
ing LSS and DIST features in feature extraction
module.
C&amp;C parser: a powerful CCG parser (Clark
and Curran 2004). We use C&amp;C parser to obtain
dependent content word pairs in dependency pars-
ing module.
WEKA: the widely used data mining software
(Witten&amp;Frank 2005). We have experimented with
several machine learning algorithms implemented
in WEKA at the second stage.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.993061">
In this section, we explain the seven features that
we employ in our RTE-3 system.
</bodyText>
<subsectionHeader confidence="0.999642">
3.1 Lexical Semantic Similarity (LSS)
</subsectionHeader>
<bodyText confidence="0.9967846">
Let H={HW 1, HW 2, ..., HW m} be the set of words in
a hypothesis, and T={TW 1, TW 2, ..., TW n} the set of
words in a text, then the lexical semantic similarity
feature LSS for a T-H pair is calculated as the fol-
lowing equation:
</bodyText>
<construct confidence="0.743299">
SSim (HWi, TW j
j
i
</construct>
<bodyText confidence="0.99993375">
where IDF(w) return the Inverse Document Fre-
quency (IDF) value of word w, and SSim is any
function for calculating the semantic relatedness
between two words. We use WordNet::Similarity
</bodyText>
<equation confidence="0.996596875">
LSS (H, T) = i
SSim (HWi,HWi)
IDF (HW )
i
(MAX (
. (1)
) * ( ))
IDF HW i
</equation>
<page confidence="0.954554">
160
</page>
<bodyText confidence="0.9999905">
package to calculate the semantic similarity of two
content words in WordNet (Fellbaum 1998). This
package provides many different semantic related-
ness measures. In our system, we use the Lesk re-
latedness measure for function SSim, as it can be
used to make comparisons between concepts of
different parts of speech (POS) (Baner-
jee&amp;Pedersen, 2002). Because the value of SSim
may be larger than 1, we normalize the original
value from the WordNet::Similarity package to
guarantee it fall between 0 and 1.
For the words out of WordNet, e.g. new proper
nouns, we use the following strategy: if two words
match exactly, the similarity between them is 1;
otherwise, the similarity is 0.
It needs to be pointed out that Equation (1) is a
variant of the text semantic similarity proposed in
(Mihalcea et al. 2006). However, in Equation (1),
we take into account out of vocabulary words and
normalization for some word-to-word similarity
metrics that may be larger than 1.
In addition, we use an IDF dictionary from
MEAD (Radev et al. 2001; http://www.summari-
zation.com/mead/) for retrieving the IDF value for
each word. For the words out of the IDF diction-
ary, we assign a default value 3.0.
</bodyText>
<subsectionHeader confidence="0.999008">
3.2 Named Entities (NE)
</subsectionHeader>
<bodyText confidence="0.999971294117647">
Named Entities are important semantic information
carriers, which convey more specific information
than individual component words. Intuitively, we
can assume that all named entities in a hypothesis
would appear in a textual snippet which entails the
hypothesis. Otherwise, it is very likely that the en-
tailment relation in a T-H pair doesn’t hold. Based
on this assumption, we derive a NE feature for
each T-H pair as follows:
Function NE_S derives the set of named entities
from a textual snippet. When we search in T the
counterpart of a named entity in H, we use a looser
matching strategy: if a named entity neA in H is
consumed by a named entity neB in T, neA and
neB are thought to be matched. We use the English
analysis tool OAK (Sekine 2002) to recognize
named entities in textual snippets.
</bodyText>
<subsectionHeader confidence="0.998548">
3.3 Dependent Content Word Pairs (DEP)
</subsectionHeader>
<bodyText confidence="0.999920714285714">
With the NE feature, we can capture some local
dependency relations between words, but we may
miss many dependency relations expressed in a
long distance. These missed long distance depend-
ency relations may be helpful for determining
whether entailment holds between H and T. So, we
design a DEP feature as follows:
</bodyText>
<equation confidence="0.686128">
�1 ,
DEP(H,T)=j |DEP _S(H)∩ DEP _S(T)  |if |DEP _S(H
 |DEP_ S(H) |
</equation>
<bodyText confidence="0.9999">
Function DEP_S derives the set of dependent
content word pairs from a textual snippet. We re-
quire that the two content words of each pair
should be dependent directly or linked with at most
one function word. We use C&amp;C parser (Clark and
Curran 2004) to parse the dependency structure of
a textual snippet and then derive the dependent
content word pairs. We don’t consider the type of
dependency relation between two linked words.
</bodyText>
<subsectionHeader confidence="0.976569">
3.4 Average Distance (DIST)
</subsectionHeader>
<bodyText confidence="0.999989352941177">
The DIST feature measures the distance between
unmapped tokens in the text. Adams (2006) uses a
simple count of the number of unmapped tokens in
the text that occur between two mapped tokens,
scaled to the length of the hypothesis. Our system
uses a different approach, i.e. measuring the aver-
age length of the gaps between mapped tokens.
The number of tokens in the text between each
consecutive pair of mapped tokens is summed up,
and this sum is divided by the number of gaps
(equivalent to the number of tokens – 1). In this
formula, consecutive mapped tokens in the text
count as gaps of 0, so a prevalence of consecutive
mapped tokens lowers the value for this feature.
The purpose of this approach is to reduce the effect
of long appositives, which may not be mapped to
the hypothesis but should not rule out entailment.
</bodyText>
<subsectionHeader confidence="0.886736">
3.5 Negation (NG)
</subsectionHeader>
<bodyText confidence="0.9969896">
The Negation feature is very simple. We simply
count the occurrences of negative words from a list
in both the hypothesis (nh) and the text (nt). The list
includes some common negating affixes. Then the
value is:
</bodyText>
<figure confidence="0.969453434782609">
1, if n and n have the
h t
NEG(H,T) =
1 ,  |_ ( )  |0,
if NE S H =
(H, T)= |NE_S(H)∩NE_S(T) |if|NE_S(H
 |_ ( ) |
NE S H
�
��
IL
NE
)  |0.
&gt;
|
if
DEP_S(H)|=0,
I�
)  |0 .
&gt;
���
~~0, otherwise
same parity
</figure>
<page confidence="0.926502">
161
</page>
<subsectionHeader confidence="0.950657">
3.6 Task (TK)
</subsectionHeader>
<bodyText confidence="0.9604032">
The Task feature is simply the task domain from
which the text-hypothesis pair was drawn. The
values are Question Answering (QA), Information
Retrieval (IR), Information Extraction (IE), and
Multi-Document Summarization (SUM).
</bodyText>
<subsectionHeader confidence="0.995953">
3.7 Text Length (LEN)
</subsectionHeader>
<bodyText confidence="0.999477666666667">
The Text Length feature is drawn directly from the
length attribute of each T-H pair. Based on the
length of T, its value is either “short” or “long”.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="method">
4 Experiments and Discussions
</sectionHeader>
<bodyText confidence="0.9999108">
We run several experiments using various datasets
to train and test models, as well as different com-
binations of features. We also experiment with
several different machine learning algorithms, in-
cluding support vector machine, decision tree, k-
nearest neighbor, naïve bayes, and so on. Decision
tree algorithm achieves the best results in all ex-
periments during development. Therefore, we
choose to use decision tree algorithm (J48 in
WEKA) at the machine learning stage.
</bodyText>
<subsectionHeader confidence="0.976155">
4.1 RTE-3 Datasets
</subsectionHeader>
<bodyText confidence="0.999190366666667">
RTE-3 organizers provide two datasets, i.e. a de-
velopment set and a test set, each consisting of 800
T-H pairs. In both sets pairs are annotated accord-
ing to the task the example was drawn from and its
length. The length annotation is introduced in this
year’s competition, and has a value of either
“long” or “short.” In addition, the development set
is annotated as to whether each pair is in an en-
tailment relation or not.
In order to aid our analysis, we compile some
statistics on the datasets of RTE-3. Statistics on the
development dataset are given in Table 1, while
those on the test dataset appear in Table 2.
From these two tables, we found the distribution
of different kinds of pairs is not balanced in both
the RTE-3 development dataset and the RTE-3 test
dataset. 412 entailed pairs appear in the develop-
ment dataset, where 410 pairs in the test dataset are
marked as “YES”. Thus, the first baseline system
that outputs all “YES” achieves accuracy 0.5125.
If we consider task information (IE, IR, QA, and
SUM) and assume the two datasets have the same
“YES” and “NO” distribution for each task, we
will derive the second baseline system, which can
get accuracy 0.5450. Similarly, if we further con-
sider length information (short and long) and as-
sume the two datasets have the same “YES” and
“NO” distribution for each task with length infor-
mation, we will derive the third baseline system,
which can also get accuracy 0.5450.
</bodyText>
<table confidence="0.999832833333334">
Long IE NO 11 1.38%
(135)
YES 17 2.13%
IR NO 22 2.75%
YES 21 2.63%
QA NO 20 2.50%
YES 27 3.38%
SUM NO 4 0.50%
YES 13 1.63%
Short IE NO 80 10.00%
(665)
YES 92 11.50%
IR NO 89 11.13%
YES 68 8.50%
QA NO 73 9.13%
YES 80 10.00%
SUM NO 89 11.13%
YES 94 11.75%
</table>
<tableCaption confidence="0.9860375">
Table 1. Statistical Information of the RTE-3 De-
velopment Dataset.
</tableCaption>
<table confidence="0.999958777777778">
Long IE NO 11 1.38%
(117)
YES 8 1.00%
IR NO 31 3.88%
YES 23 2.88%
QA NO 13 1.63%
YES 22 2.75%
SUM NO 4 0.50%
YES 5 0.63%
Short IE NO 84 10.50%
(683)
YES 97 12.13%
IR NO 82 10.25%
YES 64 8.00%
QA NO 81 10.13%
YES 84 10.50%
SUM NO 84 10.50%
YES 107 13.38%
</table>
<tableCaption confidence="0.912739">
Table 2. Statistical Information of the RTE-3 Test
Dataset.
</tableCaption>
<bodyText confidence="0.9993342">
As different kinds of pairs are evenly distributed
in RTE-1 and RTE-2 datasets, the baseline system
for RTE-1 and RTE-2 that assumes all “YES” or
all “NO” can only achieve accuracy 0.5. The rela-
tively higher baseline performance for RTE-3 data-
sets (0.545 vs. 0.5) makes us expect that the aver-
age accuracy may be higher than those in previous
RTE Challenges.
Another observation is that the numbers of long
pairs in both datasets are very limited. Only
</bodyText>
<page confidence="0.995191">
162
</page>
<bodyText confidence="0.9993855">
16.88% and 14.63% pairs are long in the develop-
ment dataset and the test dataset respectively.
</bodyText>
<subsectionHeader confidence="0.995746">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9990002">
Systems are evaluated by simple accuracy as in
Equation (2); that is, the number of pairs (C) clas-
sified correctly over the total number of pairs (N).
This score can be further broken down according
to task.
</bodyText>
<equation confidence="0.525684">
Accuracy= . (2)
C
N
</equation>
<bodyText confidence="0.928626125">
There is another scoring available for ranked re-
sults, Average Precision, which aims to evaluate
the ability of systems to rank all the T-H pairs in
the test set according to their entailment confi-
dence (in decreasing order from the most certain
entailment to the least certain). It is calculated as in
Equation (3).
rs.
</bodyText>
<table confidence="0.99937775">
RUN Accuracy Overall Accuracy by Task
IE IR QA SUM
1 0.6400 0.5100 0.6600 0.7950 0.5950
2 0.6488 0.5300 0.6350 0.8050 0.6250
RTE-3 Run Results.
cial RTE-3 Results
Accuracy by Task
Feature Set
IR QA SUM
Acc.IE
LSS 0.530 0.660 0.720 0.595 0.6263
NE 0.520 0.620 0.780 0.580 0.6250
DEP 0.495 0.625 0.575 0.570 0.5663
TK 0.525 0.565 0.530 0.560 0.5450
DIST 0.525 0.435 0.530 0.560 0.5125
NG 0.555 0.505 0.590 0.535 0.5463
LEN 0.525 0.435 0.530 0.560
0.5125LSS+NE0.525 0.645 0.805 0.585 0.6400
LSS+NE+DEP0.520 0.650 0.810
0.5800.6400LSS+NE+TK0.5300.625 0.805 0.595 0.6388
LSS+NE+TK+LEN 0.530 0.630 0.805 0.625 0.6475
0.530 0.625 0.805 0.620
LSS+NE+TK+DEP
0.460 0.625 0.785 0.655 0.6313
0.6450LSS+NE+TK+DEP+NG
LSS+NE+TK+LEN+DEP0.525 0.615 0.790 0.600 0.6325
LSS+NE+TK+LEN+DIST0.530 0.6250.6488(run2) 0.635 0.805
All Features 0.500 0.590 163 cannot beat this baseline system on IE task, while RUN2 only has a trivial advantage over it. In fur- 0.790 0.630 0.6275
</table>
<bodyText confidence="0.994847363636364">
ther analysis on the detailed results, we found that
our system tends to label all IE pairs as entailed
ones, because most of the IE pairs exhibit higher
lexical overlapping between T and H. In our opin-
ion, word order and long syntactic structures may
be helpful for dealing with IE pairs. We will ex-
plore this idea and other methods to improve RTE
on the RTE-3 Test dataset (Trai
ned on the RTE-3
development dataset).
systems on IE pai
</bodyText>
<subsectionHeader confidence="0.983646">
4.4 Discussions
</subsectionHeader>
<equation confidence="0.956137428571428">
= �
R i = 1 i
. (3)
AvgP
1
N E(i)*Nep
(i)
</equation>
<bodyText confidence="0.9513855">
Where R is the total number of positive pairs in
the test set, E(i) is 1 if the i-th pair is positive and 0
otherwise, and Nep(i) returns the number of posi-
tive pairs in the top i pai
</bodyText>
<tableCaption confidence="0.963247">
Table 3. Our Official
</tableCaption>
<subsectionHeader confidence="0.988178">
4.3 Offi
</subsectionHeader>
<bodyText confidence="0.987760684210526">
The official results for our system are shown in
Table 3. For our first run, the model was trained on
all the datasets from the two previous challenges as
well as the RTE-3 development set, using only the
LSS, NE, and TK features. This feature combina-
tion achieves the best performance on the RTE-3
development dataset in our experiments. For the
second run, the model was trained only on the
RTE-3 development dataset, but adding other two
features LEN and DIST. We hope these two fea-
tures may be helpful for differentiating pairs with
different length.
RUN2 with five features achieves better results
than RUN1. It performs better on IE, QA and SUM
tasks than RUN1, but poorer on IR task. Both runs
obtain the best performance on QA task, and per-
form very poor on IE task. For the IE task itself, a
baseline system can
rs in our future research.
</bodyText>
<tableCaption confidence="0.983626">
Table 4. Accuracy by task and selected feature set
</tableCaption>
<subsectionHeader confidence="0.488297">
4.4.1 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.963139529411765">
Table
out the results of using various feature
combinations to train the classifier. All of the
models were trained on the RTE 3 development
dataset only.
It is obvious that the LSS and NE features have
the most utility. The DIST and LEN features seem
useless for this dataset, as these features them-
selves can not beat the baseline system with accu-
racy 0.545. Systems with individual features per-
form similarly on SUM pairs except NG, and on IE
pairs except NG and DEP features. However, on
IR and QA pairs, they behave quite differently. For
example, system with NE feature achieves accu-
racy 0.78 on QA pairs, while system with DEP
feature obtains 0.575. NE and LSS features have
similar effects, but NE is more useful for QA pai
</bodyText>
<sectionHeader confidence="0.997245" genericHeader="evaluation">
4 lays
</sectionHeader>
<bodyText confidence="0.97463075">
rs.
get accuracy 0.525. RUN1
It is interesting to note that some features im-
prove the score in some combinations, but in oth-
ers they decrease it. For instance, although DEP
scores above the baseline at 0.5663, when added to
the combination of LSS, NE, TK, and LEN it low-
ers the overall accuracy by 1.5%.
</bodyText>
<subsectionHeader confidence="0.658306">
4.4.2 About Average Precision Measure
</subsectionHeader>
<bodyText confidence="0.999974875">
As we mentioned in section 4.2, Average Precision
(AvgP) is expected to evaluate the ranking ability
of a system according to confidence values. How-
ever, we found that the current evaluation process
and the measure itself have some problems and
need to be modified for RTE evaluation.
On one hand, the current evaluation process
doesn’t consider tied cases where many pairs may
have the same confidence value. It is reasonable to
assume that the order of tied pairs will be random.
Accordingly, the derived Average Precision will
vary.
Let’s look at a simple example: suppose we
have two pairs c and d, and c is the only one posi-
tive entailment pair. Here, R=1, N=2 for Equation
(3). Two systems X and Y output ranked results as
{c, d} and {d,c} respectively. According to Equa-
tion (3), the AvgP value of system X is 1, where
that of system Y is 0.5. If these two systems assign
same confidence value for both pairs, we can not
conclude that system X is better than system Y.
To avoid this problem, we suggest requiring that
each system for ranked submission output its con-
fidence for each pair. Then, when calculating Av-
erage Precision measure, we first re-rank the list
with these confidence values and true answers for
each pair. For tied pairs, we rank pairs with true
answer “NO” before those with positive entailment
relation. By this way, we can produce a stable and
more reasonable Average Precision value. For ex-
ample, in the above example, the modified average
precisions for both systems will be 0.5.
On the other hand, from the Equation (3), we
know that the upper bound of Average Precision is
1. At the same time, we can also derive a lower
bound for this measure as in Equation (4). It corre-
sponds to the worst system which places all the
negative pairs before all the positive pairs. The
lower bound of Average Precision for RTE-3 test
dataset is 0.3172.
</bodyText>
<equation confidence="0.648321666666667">
R−1 J
LB _ AvgP = 1 E R -− . (4)
R j N j
</equation>
<page confidence="0.661715">
=0
</page>
<bodyText confidence="0.999224571428571">
As the values of N and R change, the lower
bound of Average Precision will vary. Therefore,
the original Average Precision measure as in Equa-
tion (3) is not an ideal one for comparison across
datasets.
To solve this problem, we propose a normalized
Average Precision measure as in Equation (5).
</bodyText>
<sectionHeader confidence="0.99372" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99979">
In this paper, we report our RTE-3 system. The
system was built on a machine learning framework
with features produced by state-of-the-art NLP
techniques. Lexical semantic similarity and Named
entities are the two most effective features. Data
analysis shows a higher baseline performance for
RTE-3 than RTE-1 and RTE-2, and the current
Average Precision measure needs to be changed.
As T-H pairs from IE task are the most difficult
ones, we will focus on these pairs in our future re-
search.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999045785714286">
Rod Adams. 2006. Textual Entailment Through Extended Lexical
Overlap. In Proceedings of RTE-2 Workshop.
Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algo-
rithm for Word Sense Disambiguation Using WordNet. In Pro-
ceedings of CICLING-02.
Roy Bar-Haim et al. 2006. The Second PASCAL Recognising Textual
Entailment Challenge. In Proceedings of RTE-2 Workshop.
Stephen Clark and James R. Curran. 2004. Parsing the WSJ using
CCG and Log-Linear Models. In Proceedings of ACL-04.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PAS-
CAL Recognising Textual Entailment Challenge. In Quiñonero-
Candela et al. (editors.), MLCW 2005, LNAI Volume 3944.
Christiane Fellbaum. 1998. WordNet: an Electronic Lexical Database.
MIT Press.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Cor-
pus-based and Knowledge-based Measures of Text Semantic Simi-
larity. In Proceedings of AAAI-06.
Ted Pedersen et al. 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation. Research Report UMSI
2005/25, Supercomputing Institute, University of Minnesota.
Dragomir Radev, Sasha Blair-Goldensohn, and ZhuZhang. 2001.
Experiments in single and multidocument summarization using
MEAD. In Proceedings of DUC 2001.
Satoshi Sekine. 2002. Manual of Oak System (version 0.1). Computer
Science Department, New York University,
http://nlp.cs.nyu.edu/oak.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine
learning tools and techniques. Morgan Kaufmann, San Francisco.
</reference>
<figure confidence="0.99055">
Norm AvgP AvgP LB AvgP (5)
_ − _
= .
1−LB AvgP
_
</figure>
<page confidence="0.965302">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.117741">
<title confidence="0.971552">Learning Based Semantic Inference: Experiments and</title>
<note confidence="0.323723333333333">servations at RTE-3 Joseph Ernest V. and Ashwin of</note>
<affiliation confidence="0.981564">Georgia Institute of</affiliation>
<address confidence="0.997149">Atlanta, GA 30332, USA</address>
<email confidence="0.999672">ashwin@cc.gatech.edu</email>
<abstract confidence="0.981475">of</abstract>
<affiliation confidence="0.968741">School of Medicine, Emory</affiliation>
<address confidence="0.99795">Atlanta, GA 30322, USA</address>
<email confidence="0.741196">Ernest.Garcia@emoryhealthcare.org</email>
<abstract confidence="0.997135814814815">pair (pair id 5) from the RTE-3 development dataset is as follows: Textual Entailment Recognition is a semantic inference task that is required in many natural language processing (NLP) applications. In this paper, we present our system for the third PASCAL recognizing textual entailment (RTE-3) challenge. The system is built on a machine learning framework with the following features derived by state-of-the-art NLP techniques: lexical semantic similarity (LSS), named entities (NE), dependent content word pairs (DEP), average distance (DIST), negation (NG), task (TK), and text length (LEN). On the RTE-3 test dataset, our system achieves the accuracy of 0.64 and 0.6488 for the two official submissions, respectively. Experimental results show that LSS and NE are the most effective features. Further analyses indicate that a baseline dummy system can achieve accuracy 0.545 on the RTE-3 test dataset, which makes RTE-3 relatively easier than RTE-2 and RTE-1. In addition, we demonstrate with examples that the current Average Precision measure and its evaluation process need to be changed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rod Adams</author>
</authors>
<title>Textual Entailment Through Extended Lexical Overlap.</title>
<date>2006</date>
<booktitle>In Proceedings of RTE-2 Workshop.</booktitle>
<contexts>
<context position="9924" citStr="Adams (2006)" startWordPosition="1613" endWordPosition="1614">1 , DEP(H,T)=j |DEP _S(H)∩ DEP _S(T) |if |DEP _S(H |DEP_ S(H) | Function DEP_S derives the set of dependent content word pairs from a textual snippet. We require that the two content words of each pair should be dependent directly or linked with at most one function word. We use C&amp;C parser (Clark and Curran 2004) to parse the dependency structure of a textual snippet and then derive the dependent content word pairs. We don’t consider the type of dependency relation between two linked words. 3.4 Average Distance (DIST) The DIST feature measures the distance between unmapped tokens in the text. Adams (2006) uses a simple count of the number of unmapped tokens in the text that occur between two mapped tokens, scaled to the length of the hypothesis. Our system uses a different approach, i.e. measuring the average length of the gaps between mapped tokens. The number of tokens in the text between each consecutive pair of mapped tokens is summed up, and this sum is divided by the number of gaps (equivalent to the number of tokens – 1). In this formula, consecutive mapped tokens in the text count as gaps of 0, so a prevalence of consecutive mapped tokens lowers the value for this feature. The purpose </context>
</contexts>
<marker>Adams, 2006</marker>
<rawString>Rod Adams. 2006. Textual Entailment Through Extended Lexical Overlap. In Proceedings of RTE-2 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of CICLING-02.</booktitle>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. In Proceedings of CICLING-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of RTE-2 Workshop.</booktitle>
<marker>Bar-Haim, 2006</marker>
<rawString>Roy Bar-Haim et al. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of RTE-2 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and Log-Linear Models.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04.</booktitle>
<contexts>
<context position="6045" citStr="Clark and Curran 2004" startWordPosition="938" endWordPosition="941">sis tool (Sekine 2002). It is used for sentence splitting, POS tagging, and named entities recognition. WordNet::SenseRelate::Allwords package: a word sense disambiguation (WSD) module for assigning each content word a sense from WordNet (Pedersen et al., 2005). It is used in WSD module. Figure 1. System Architecture. WordNet::Similarity package: a Perl module that implements a variety of semantic similarity and relatedness measures based on WordNet (Pedersen et al., 2005). This package is used for deriving LSS and DIST features in feature extraction module. C&amp;C parser: a powerful CCG parser (Clark and Curran 2004). We use C&amp;C parser to obtain dependent content word pairs in dependency parsing module. WEKA: the widely used data mining software (Witten&amp;Frank 2005). We have experimented with several machine learning algorithms implemented in WEKA at the second stage. 3 Features In this section, we explain the seven features that we employ in our RTE-3 system. 3.1 Lexical Semantic Similarity (LSS) Let H={HW 1, HW 2, ..., HW m} be the set of words in a hypothesis, and T={TW 1, TW 2, ..., TW n} the set of words in a text, then the lexical semantic similarity feature LSS for a T-H pair is calculated as the fo</context>
<context position="9626" citStr="Clark and Curran 2004" startWordPosition="1564" endWordPosition="1567">NE feature, we can capture some local dependency relations between words, but we may miss many dependency relations expressed in a long distance. These missed long distance dependency relations may be helpful for determining whether entailment holds between H and T. So, we design a DEP feature as follows: �1 , DEP(H,T)=j |DEP _S(H)∩ DEP _S(T) |if |DEP _S(H |DEP_ S(H) | Function DEP_S derives the set of dependent content word pairs from a textual snippet. We require that the two content words of each pair should be dependent directly or linked with at most one function word. We use C&amp;C parser (Clark and Curran 2004) to parse the dependency structure of a textual snippet and then derive the dependent content word pairs. We don’t consider the type of dependency relation between two linked words. 3.4 Average Distance (DIST) The DIST feature measures the distance between unmapped tokens in the text. Adams (2006) uses a simple count of the number of unmapped tokens in the text that occur between two mapped tokens, scaled to the length of the hypothesis. Our system uses a different approach, i.e. measuring the average length of the gaps between mapped tokens. The number of tokens in the text between each conse</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and Log-Linear Models. In Proceedings of ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>MLCW 2005, LNAI Volume</booktitle>
<pages>3944</pages>
<editor>In QuiñoneroCandela et al. (editors.),</editor>
<contexts>
<context position="2311" citStr="Dagan et al., 2006" startWordPosition="358" endWordPosition="361">d the text (T). Textual entailment recognition is the task of deciding whether a given T entails a given H. An example T: A bus collision with a truck in Uganda has resulted in at least 30 fatalities and has left a further 21 injured. H: 30 die in a bus collision in Uganda. Given such a pair, a recognizing textual entailment (RTE) system should output its judgement about whether or not an entailment relation holds between them. For the above example pair, H is entailed by T. The PASCAL Recognizing Textual Entailment Challenge is an annual challenge on this task which has been held since 2005 (Dagan et al., 2006; Bar-Haim et al. 2006). As textual entailment recognition is thought to be a common underlying semantic inference task for many natural language processing applications, such as Information Extraction (IE), Information Retrieval (IR), Question Answering (QA), and Document Summarization (SUM), the PASCAL RTE Challenge has been gaining more and more attention in the NLP community. In the past challenges, various approaches to recognizing textual entailment have been proposed, from syntactic analysis to logical inference (Bar-Haim et al. 2006). As a new participant, we have two goals by attendin</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In QuiñoneroCandela et al. (editors.), MLCW 2005, LNAI Volume 3944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7038" citStr="Fellbaum 1998" startWordPosition="1122" endWordPosition="1123">ty (LSS) Let H={HW 1, HW 2, ..., HW m} be the set of words in a hypothesis, and T={TW 1, TW 2, ..., TW n} the set of words in a text, then the lexical semantic similarity feature LSS for a T-H pair is calculated as the following equation: SSim (HWi, TW j j i where IDF(w) return the Inverse Document Frequency (IDF) value of word w, and SSim is any function for calculating the semantic relatedness between two words. We use WordNet::Similarity LSS (H, T) = i SSim (HWi,HWi) IDF (HW ) i (MAX ( . (1) ) * ( )) IDF HW i 160 package to calculate the semantic similarity of two content words in WordNet (Fellbaum 1998). This package provides many different semantic relatedness measures. In our system, we use the Lesk relatedness measure for function SSim, as it can be used to make comparisons between concepts of different parts of speech (POS) (Banerjee&amp;Pedersen, 2002). Because the value of SSim may be larger than 1, we normalize the original value from the WordNet::Similarity package to guarantee it fall between 0 and 1. For the words out of WordNet, e.g. new proper nouns, we use the following strategy: if two words match exactly, the similarity between them is 1; otherwise, the similarity is 0. It needs t</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-06.</booktitle>
<contexts>
<context position="7752" citStr="Mihalcea et al. 2006" startWordPosition="1242" endWordPosition="1245">he Lesk relatedness measure for function SSim, as it can be used to make comparisons between concepts of different parts of speech (POS) (Banerjee&amp;Pedersen, 2002). Because the value of SSim may be larger than 1, we normalize the original value from the WordNet::Similarity package to guarantee it fall between 0 and 1. For the words out of WordNet, e.g. new proper nouns, we use the following strategy: if two words match exactly, the similarity between them is 1; otherwise, the similarity is 0. It needs to be pointed out that Equation (1) is a variant of the text semantic similarity proposed in (Mihalcea et al. 2006). However, in Equation (1), we take into account out of vocabulary words and normalization for some word-to-word similarity metrics that may be larger than 1. In addition, we use an IDF dictionary from MEAD (Radev et al. 2001; http://www.summarization.com/mead/) for retrieving the IDF value for each word. For the words out of the IDF dictionary, we assign a default value 3.0. 3.2 Named Entities (NE) Named Entities are important semantic information carriers, which convey more specific information than individual component words. Intuitively, we can assume that all named entities in a hypothesi</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of AAAI-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.</title>
<date>2005</date>
<tech>Research Report UMSI 2005/25,</tech>
<institution>Supercomputing Institute, University of Minnesota.</institution>
<marker>Pedersen, 2005</marker>
<rawString>Ted Pedersen et al. 2005. Maximizing Semantic Relatedness to Perform Word Sense Disambiguation. Research Report UMSI 2005/25, Supercomputing Institute, University of Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Sasha Blair-Goldensohn</author>
<author>ZhuZhang</author>
</authors>
<title>Experiments in single and multidocument summarization using MEAD.</title>
<date>2001</date>
<booktitle>In Proceedings of DUC</booktitle>
<contexts>
<context position="7977" citStr="Radev et al. 2001" startWordPosition="1280" endWordPosition="1283">original value from the WordNet::Similarity package to guarantee it fall between 0 and 1. For the words out of WordNet, e.g. new proper nouns, we use the following strategy: if two words match exactly, the similarity between them is 1; otherwise, the similarity is 0. It needs to be pointed out that Equation (1) is a variant of the text semantic similarity proposed in (Mihalcea et al. 2006). However, in Equation (1), we take into account out of vocabulary words and normalization for some word-to-word similarity metrics that may be larger than 1. In addition, we use an IDF dictionary from MEAD (Radev et al. 2001; http://www.summarization.com/mead/) for retrieving the IDF value for each word. For the words out of the IDF dictionary, we assign a default value 3.0. 3.2 Named Entities (NE) Named Entities are important semantic information carriers, which convey more specific information than individual component words. Intuitively, we can assume that all named entities in a hypothesis would appear in a textual snippet which entails the hypothesis. Otherwise, it is very likely that the entailment relation in a T-H pair doesn’t hold. Based on this assumption, we derive a NE feature for each T-H pair as fol</context>
</contexts>
<marker>Radev, Blair-Goldensohn, ZhuZhang, 2001</marker>
<rawString>Dragomir Radev, Sasha Blair-Goldensohn, and ZhuZhang. 2001. Experiments in single and multidocument summarization using MEAD. In Proceedings of DUC 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<date>2002</date>
<journal>Manual of Oak System (version</journal>
<volume>0</volume>
<institution>Computer Science Department, New York University,</institution>
<location>http://nlp.cs.nyu.edu/oak.</location>
<contexts>
<context position="5445" citStr="Sekine 2002" startWordPosition="847" endWordPosition="848">, a word sense disambiguation (WSD) module, and a dependency parser. These language analyzers output their own intermediate representations for the feature extraction module. We produce seven features for each T-H pair: lexical semantic similarity (LSS), named entities (NE), dependent content word pairs (DEP), average distance (DIST), negation (NG), task (TK), and text length (LEN). The last two features are extracted from each pair itself, while others are based on the results of language analyzers. The resources that we used in our RTE-3 system include: OAK: a general English analysis tool (Sekine 2002). It is used for sentence splitting, POS tagging, and named entities recognition. WordNet::SenseRelate::Allwords package: a word sense disambiguation (WSD) module for assigning each content word a sense from WordNet (Pedersen et al., 2005). It is used in WSD module. Figure 1. System Architecture. WordNet::Similarity package: a Perl module that implements a variety of semantic similarity and relatedness measures based on WordNet (Pedersen et al., 2005). This package is used for deriving LSS and DIST features in feature extraction module. C&amp;C parser: a powerful CCG parser (Clark and Curran 2004)</context>
<context position="8906" citStr="Sekine 2002" startWordPosition="1443" endWordPosition="1444">vely, we can assume that all named entities in a hypothesis would appear in a textual snippet which entails the hypothesis. Otherwise, it is very likely that the entailment relation in a T-H pair doesn’t hold. Based on this assumption, we derive a NE feature for each T-H pair as follows: Function NE_S derives the set of named entities from a textual snippet. When we search in T the counterpart of a named entity in H, we use a looser matching strategy: if a named entity neA in H is consumed by a named entity neB in T, neA and neB are thought to be matched. We use the English analysis tool OAK (Sekine 2002) to recognize named entities in textual snippets. 3.3 Dependent Content Word Pairs (DEP) With the NE feature, we can capture some local dependency relations between words, but we may miss many dependency relations expressed in a long distance. These missed long distance dependency relations may be helpful for determining whether entailment holds between H and T. So, we design a DEP feature as follows: �1 , DEP(H,T)=j |DEP _S(H)∩ DEP _S(T) |if |DEP _S(H |DEP_ S(H) | Function DEP_S derives the set of dependent content word pairs from a textual snippet. We require that the two content words of ea</context>
</contexts>
<marker>Sekine, 2002</marker>
<rawString>Satoshi Sekine. 2002. Manual of Oak System (version 0.1). Computer Science Department, New York University, http://nlp.cs.nyu.edu/oak.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>