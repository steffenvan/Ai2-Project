<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004903">
<title confidence="0.999479">
Identifying Sources of Inter-Annotator Variation:
Evaluating Two Models of Argument Analysis
</title>
<author confidence="0.996854">
Barbara White
</author>
<affiliation confidence="0.996072">
The University of Western Ontario
</affiliation>
<address confidence="0.932162">
London, ON, Canada N6A 3K7
</address>
<email confidence="0.999068">
bwhite6@uwo.ca
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999603307692308">
This paper reports on a pilot study where two
Models of argument were applied to the Dis-
cussion sections of a corpus of biomedical re-
search articles. The goal was to identify
sources of systematic inter-annotator variation
as diagnostics for improving the Models. In
addition to showing a need to revise both
Models, the results identified problems result-
ing from limitations in annotator expertise. In
future work two types of annotators are re-
quired: those with biomedical domain exper-
tise and those with an understanding of rhe-
torical structure.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980111111111">
Given the vast and growing body of biomedical
research literature being published there is a need
to develop automated text mining tools that will
assist in filtering out the information most useful
to researchers. Previous studies applying Argu-
mentative Zoning (AZ) (Teufel et al. 1999) and
Zone Analysis (ZA) (Mizuta et al. 2005) have
shown that an analysis of the argumentative
structure of a text can be of use in Information
Extraction (IE). As an alternative approach, it
was believed that Toulmin’s work on informal
logic and argument structure (1958/2003) could
reflect the rhetorical strategies used by the au-
thors of biomedical research articles.
In order to compare and evaluate these ap-
proaches two Models of argument were applied
to the same set of biomedical research articles.
Inter-annotator agreement/disagreement between
and within Models was examined. Given that
human-annotated data are ultimately to be used
for machine learning purposes, there is growing
recognition of the need to analyze coder dis-
agreements in order to differentiate between sys-
tematic variation and noise (e.g. Reidsma and
Carletta 2008). The goal of this study was to
identify systematic disagreements as diagnostics
for improving the Models of argument.
</bodyText>
<sectionHeader confidence="0.99647" genericHeader="method">
2 Annotation Project
</sectionHeader>
<bodyText confidence="0.999889473684211">
The two Models of rhetoric (argument) in Tables
1 and 2 were applied to a corpus of 12 articles
downloaded at random from the BMC-series
(BioMed Central) of journals. The corpus cov-
ered nine different domains, with a total of 400
sentences; the three annotators worked inde-
pendently. Although the entire articles were read
by the annotators, only the sentences in the Dis-
cussion section were argumentatively catego-
rized. The annotators were the study coordinator
(B, a PhD student in Computational Linguistics
and current author) and two fourth year under-
graduate students from the Bachelor of Medical
Sciences program at The University of Western
Ontario (J and K).
Coders annotated one article at a time, apply-
ing each of the two Models; no sentence was al-
lowed to be left unannotated. In cases where an
annotator was conflicted between categories
guidelines for ‘trumping’ were provided with the
Models. (For details on the Models, trumping
systems, instructions to annotators, corpus data
and a sample annotated article please see
www.csd.uwo.ca/~mercer/White_Thesis09.pdf.)
The first model (Model 1) of argumentation to
be applied stems from work in AZ and ZA and
was adapted by White. It focuses on the content
of a text, essentially differentiating ‘new’ from
‘old’ information, and results from analysis (Ta-
ble 1). The second model is based on the con-
cepts and language of Toulmin (1958/2003). Jen-
icek applied Toulmin to create a guide for writ-
ing medical research articles (2006) and Graves
(personal communications 2008, 2009) further
adapted these ideas to work with our corpus
(Model 2). Its main focus is to identify ‘Claims’
being made by the authors, but it also differenti-
ates between internal and external evidence, as
</bodyText>
<page confidence="0.972725">
132
</page>
<note confidence="0.5073455">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 132–136,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.6835965">
well as categories of explanation and implication
(Table 2).
</bodyText>
<table confidence="0.995789692307692">
Category Specifications
CONTEXT (1) Background, accepted facts,
previous work, motivation
METHOD (2) Methods, tools, processes,
experimental design
CURRENT Findings of current
RESULTS (3) experiment
RESULTS Current results support or
COMPARED (4) contradict previous work
ANALYSIS (5) Possible interpretations or
implications of current or
previous results, significance
or limitations of their study
</table>
<tableCaption confidence="0.990726">
Table 1: Model 1 categories (White 2009)
</tableCaption>
<table confidence="0.996341933333333">
Category Specifications
EXTRANEOUS (0) Statements extraneous to
authors’ argumentation,
not related to a CLAIM
CLAIM (1) Proposition put forward
based on analysis of results
GROUNDS (2) Internal evidence from
current study
WARRANT/ Understanding of theproblem, or data, from
BACKING (3) other studies
QUALIFIER (4) Possible explanations for
results, comparisons with
external evidence
PROBLEM IN Implications for the field,
CONTEXT (5) future research directions
</table>
<tableCaption confidence="0.905804">
Table 2: Model 2 categories (Toulmin 1958,
Jenicek 2006, Graves 2009)
</tableCaption>
<sectionHeader confidence="0.551016" genericHeader="method">
2.1 Results
</sectionHeader>
<bodyText confidence="0.998651277777778">
Data were compiled on individual annotator’s
argument category choices for each of the 400
sentences, for each Model of rhetoric. This al-
lowed comparisons to be made between the two
Models, within Model by category, and between
annotators. Although the coders had different
backgrounds, they were treated as equals i.e.
there was no ‘expert’ who served as a bench-
mark. There were three possible types of inter-
annotator agreement: we all agreed on a choice
of category, we all differed, or two annotators
agreed and the third disagreed. This latter group
of two-way agreement (also implying two-way
variation) was broken down into its three possi-
bilities: J and K agreed, and differed from B
(JK—B), J and B agreed, and differed from K
(JB—K), or B and K agreed, and differed from J
(BK—J) (Table 3).
</bodyText>
<table confidence="0.997305571428571">
Model 1 Model 2
All agree 242 60.50% 157 39.25%
All disagree 15 3.75% 33 8.25%
JK—B 32 8.00% 71 17.75%
JB—K 42 10.50% 68 17.00%
BK—J 69 17.25% 71 17.75%
Total 400 100% 400 100%
</table>
<tableCaption confidence="0.986377">
Table 3 Number of sentences in agreement
</tableCaption>
<bodyText confidence="0.98506512">
groups
The overall (three-way) inter-annotator agree-
ment was higher for Model 1 at 60.5%, with
Model 2 at 39.25%. All annotators were less fa-
miliar with Model 2 than Model 1, and the for-
mer had one more category, thus there was more
opportunity to disagree. Although there is no
guarantee that three-way agreement implies we
were all ‘right’, it does suggest a shared under-
standing of what the Model categories describe.
On the other hand, there were instances of sen-
tences under both Models where three different
categories had been chosen but they could all
seem to legitimately apply. In addition, in sen-
tences which are argumentatively and/or gram-
matically complex, where one is forced to choose
only one categorization, it is often difficult to
decide which is the most appropriate.
Given the difference in academic background
of the annotators, one hypothesis had been that J
and K would be more likely to agree with each
other and differ from B, the coder who was not
knowledgeable in the biomedical sciences. As
can be seen in Table 3, however, this did not turn
out to be the case.
</bodyText>
<sectionHeader confidence="0.920416" genericHeader="method">
3 Sources of Inter-Annotator Variation
</sectionHeader>
<bodyText confidence="0.9998736">
It was crucial to examine inter-annotator dis-
agreements within each Model in order to deter-
mine the categories that were particular sources
of variation. As a reference point for this, and for
looking at individual annotator preferences, I
present in Tables 4 and 5 the overall distribution
of argument categories within Model. These are
calculated on the basis of all 1200 annotation
tokens (400 sentences * 3 annotators) across the
corpus.
</bodyText>
<page confidence="0.9954">
133
</page>
<table confidence="0.9870668">
3.1 Model 1
Category Tokens Percent
CONTEXT (1) 337 28.0%
METHOD (2) 128 10.7%
CURRENT 189 15.8%
RESULTS (3)
RESULTS 114 9.5%
COMPARED (4)
ANALYSIS (5) 432 36.0%
Total 1200 100%
</table>
<tableCaption confidence="0.9535925">
Table 4 Overall distribution by category –
Model 1
</tableCaption>
<bodyText confidence="0.9998234">
The CONTEXT category was developed in order
to filter out background (‘old’) material. Al-
though this seemed straightforward, the results
showed that CONTEXT was the largest source
of inter-annotator variation under Model 1: of the
158 sentences that had some degree of inter-
annotator variation, almost two-thirds (100) in-
volved some variation between CONTEXT and
another category. The primary reason for this
was that frequently sentences in our corpus that
included category (1) material also included ma-
terial suited to other categories (typically
ANALYSIS or RESULTS COMPARED) i.e.
they were complex sentences. There was also
inter-annotator disagreement between CUR-
RENT RESULTS (3) and RESULTS COM-
PARED (4); this was to be expected given the
potential overlap of content when discussing the
authors’ current study, especially in complex
sentences.
</bodyText>
<subsectionHeader confidence="0.847759">
3.2 Model 2
</subsectionHeader>
<table confidence="0.9990631">
Category Tokens Percent
EXTRANEOUS (0) 250 20.8%
CLAIM (1) 185 15.4%
GROUNDS (2) 218 18.2%
WARRANT/ 215 18.0%
BACKING (3)
QUALIFIER (4) 256 21.3%
PROBLEM IN 76 6.3%
CONTEXT (5)
Total 1200 100%
</table>
<tableCaption confidence="0.815534">
Table 5 Overall distribution by category –
Model 2
</tableCaption>
<bodyText confidence="0.999891258064516">
The EXTRANEOUS category had been devel-
oped for sentences of a ‘background’ nature,
which did not fit into the Toulmin argument
structure i.e. they did not seem to relate directly
to any CLAIM. Of the 243 sentences with some
degree of inter-annotator variation under Model
2, 101 involved the EXTRANEOUS category.
This variation a) showed that there were prob-
lems in understanding argument structure, and b)
reflected the differences in annotator preferences
(Table 7).
Model 2 is crucially a CLAIMS-based sys-
tem, so variation between CLAIMS and other
categories is particularly significant, especially
since it is assumed that this might be the cate-
gory of greatest interest to biomedical research-
ers. There were 52 sentences which involved
some variation between CLAIM (1) and
QUALIFIER (4), a fact which revealed a need to
make clearer distinctions between these two
categories. Many sentences in our corpus seemed
to meet the specifications for both categories at
the same time i.e. they were both an explanation
and a conclusion. There were 46 sentences in-
volving some disagreement between (4) and
WARRANT/BACKING (3). The source of this
variation seemed to be the difficulty deciding
whether the ‘compare and contrast with external
evidence’ aspect of (4) or the straightforward
‘external evidence’ of (3) was more appropriate
for certain, especially complex, sentences.
</bodyText>
<subsectionHeader confidence="0.999104">
3.3 Annotators
</subsectionHeader>
<bodyText confidence="0.9941475">
Under Model 1 the three annotator columns
show a relatively similar distribution (Table 6).
The exception is that J was less inclined to select
the CONTEXT category, and more inclined to
select RESULTS COMPARED, than either B or
K.
</bodyText>
<table confidence="0.999298444444444">
Category B J K Total
CONTEXT (1) 121 92 124 337
METHOD (2) 39 43 46 128
CURRENT 59 67 63 189
RESULTS (3)
RESULTS 36 57 21 114
COMPARED (4)
ANALYSIS (5) 145 141 146 432
Total 400 400 400 1200
</table>
<tableCaption confidence="0.9977">
Table 6 Category distribution by annotator –
</tableCaption>
<bodyText confidence="0.912157833333333">
Model 1
Under Model 2 we see an extreme range
among annotators in the number of sentences
they identified as EXTRANEOUS with J having
more than twice as many as B (Table 7). This
degree of annotator bias guaranteed that category
</bodyText>
<page confidence="0.997164">
134
</page>
<bodyText confidence="0.999663285714286">
(0) would be involved in considerable inter-
annotator disagreement. The other notable skew-
ing occurred in categories (1) and (4) where B
and J shared similar numbers as opposed to K: K
had 91 sentences as CLAIM, almost twice as
many as B or J, and only 50 sentences as
QUALIFIER, roughly half as many as B or J.
</bodyText>
<table confidence="0.9995442">
Category B J K Total
EXTRANEOUS (0) 54 116 80 250
CLAIM (1) 45 49 91 185
GROUNDS (2) 86 61 71 218
WARRANT/ 81 49 85 215
BACKING (3)
QUALIFIER (4) 108 98 50 256
PROBLEM IN 26 27 23 76
CONTEXT (5)
Total 400 400 400 1200
</table>
<tableCaption confidence="0.998625">
Table 7 Category distribution by annotator –
</tableCaption>
<bodyText confidence="0.9486515">
Model 2
In addition to the systematic annotator prefer-
ences discussed above there were instances of
‘errors’, choices which appear to be violations of
category specifications. These may be the result
of haste or inattention, insufficient training or a
lack of understanding of the article’s content or
the Models.
</bodyText>
<subsectionHeader confidence="0.986282">
3.4 Corpus Data
</subsectionHeader>
<bodyText confidence="0.999982444444445">
It was assumed that longer sentences would be
more likely to be complex and thus more likely
to involve inter-annotator variation. The results
showed that the articles with the smallest (19)
and largest (31) average number of words per
sentence did exhibit this pattern: the former
ranked highly in three-way annotator agreement
(first under Model 1 and second under Model 2)
and the latter second lowest under both Models.
However, between these extremes there was no
clear relationship between sentence length and
overall coder agreement under either Model.
The most striking finding was the wide range of
three-way coder agreement among the twelve
articles in the corpus: from 36% to 81% under
Model 1 and 8% to 69% under Model 2. The av-
erages in Table 3 mask this source of inter-
annotator variation.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999983625">
The problem of choosing a single argument cate-
gory for a complex sentence was at the core of
much of the inter-annotator variation found un-
der both Models. The issue of sentences which
are rhetorically but not grammatically complex
e.g. those with a single tensed verb that seemed
to qualify as both a CLAIM and a QUALIFIER
under Model 2 should be dealt with where possi-
ble by revising the category specifications. How-
ever sentences that are grammatically complex
should be divided into clauses (one for each
tensed verb) as a pre-annotating process. Al-
though this creates more units and thus more op-
portunities for coders to disagree, it is believed
that reducing uncertainty by allowing a different
argument category for each clause would be
worth the trade-off.
Although Model 1 had higher average three-
way agreement at 60.5% than Model 2, this was
still relatively poor performance. As discussed
above the clear problem with this Model is the
CONTEXT (1) category. Research scientists are
always working within and building on previous
work – their own and others’; thus ‘old’ and
‘new’ information are inherently intertwined.
Therefore this category needs to be revised, pos-
sibly separating specific previous studies from
statements related to the motivation for or goals
of the current experiment. As discussed above,
the EXTRANEOUS category of Model 2 needs
to be redefined, and the CLAIM and QUALI-
FIER categories must be clearly distinguished.
Despite the relatively poor performance of
Model 2, with the above improvements it is be-
lieved that a CLAIMS-based Model is still a
good candidate for developing future IE tools.
Annotator bias reflects the fact that coders did
not have sufficient understanding of rhetorical
techniques and structure, but also the problems
with category specifications noted above. The
extreme ‘inter-article’ variation (Section 3.4)
indicates that when texts are not clearly written,
an annotator’s lack of knowledge of biomedicine
and/or argument are even more problematic.
Since the quality of writing in a corpus is a factor
that cannot be controlled ‘team’ annotations are
recommended: a biomedical domain expert
should work together with an expert in rhetoric.
It must be admitted, however, that even with
improvements to the Models of argument and
using annotators with more domain expertise,
some degree of inter-annotator disagreement will
inevitably occur as a result of individual differ-
ences. Ultimately annotators are making judg-
ments − about texts and arguments that were cre-
ated by others − that are somewhat subjective.
</bodyText>
<page confidence="0.998494">
135
</page>
<sectionHeader confidence="0.995874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99904996">
Milos Jenicek. 2006. How to read, understand, and
write ‘Discussion’ sections in medical articles: An
exercise in critical thinking. Med Sci Monitor,
12(6): SR28-SR36.
Yoko Mizuta, Anna Korhonen, Tony Mullen and
Nigel Collier. 2005. Zone Analysis in Biology Ar-
ticles as a Basis for Information Extraction. Inter-
national Journal of Medical Informatics, 75(6):
468-487.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Lin-
guistics, 34(3): 319-326.
Simone Teufel, Jean Carletta and Mark Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. Proceedings of the
Eighth Meeting of the European Chapter of the As-
sociation for Computational Linguistics: 110-117.
Stephen E. Toulmin. 1958/2003. The Uses of Argu-
ment. Cambridge University Press, Cambridge,
U.K.
Barbara White. 2009. Annotating a Corpus of Bio-
medical Research Texts: Two Models of Rhetorical
Analysis. PhD thesis, The University of Western
Ontario, Canada.
www.csd.uwo.ca/~mercer/White_Thesis09.pdf
</reference>
<page confidence="0.998714">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.905300">
<title confidence="0.999515">Identifying Sources of Inter-Annotator Evaluating Two Models of Argument Analysis</title>
<author confidence="0.999896">Barbara White</author>
<affiliation confidence="0.999829">The University of Western</affiliation>
<address confidence="0.995205">London, ON, Canada N6A 3K7</address>
<email confidence="0.981046">bwhite6@uwo.ca</email>
<abstract confidence="0.994741142857143">This paper reports on a pilot study where two Models of argument were applied to the Discussion sections of a corpus of biomedical research articles. The goal was to identify sources of systematic inter-annotator variation as diagnostics for improving the Models. In addition to showing a need to revise both Models, the results identified problems resulting from limitations in annotator expertise. In future work two types of annotators are required: those with biomedical domain expertise and those with an understanding of rhetorical structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Milos Jenicek</author>
</authors>
<title>How to read, understand, and write ‘Discussion’ sections in medical articles: An exercise in critical thinking.</title>
<date>2006</date>
<journal>Med Sci Monitor,</journal>
<volume>12</volume>
<issue>6</issue>
<pages>28--36</pages>
<contexts>
<context position="4952" citStr="Jenicek 2006" startWordPosition="751" endWordPosition="752"> results, significance or limitations of their study Table 1: Model 1 categories (White 2009) Category Specifications EXTRANEOUS (0) Statements extraneous to authors’ argumentation, not related to a CLAIM CLAIM (1) Proposition put forward based on analysis of results GROUNDS (2) Internal evidence from current study WARRANT/ Understanding of theproblem, or data, from BACKING (3) other studies QUALIFIER (4) Possible explanations for results, comparisons with external evidence PROBLEM IN Implications for the field, CONTEXT (5) future research directions Table 2: Model 2 categories (Toulmin 1958, Jenicek 2006, Graves 2009) 2.1 Results Data were compiled on individual annotator’s argument category choices for each of the 400 sentences, for each Model of rhetoric. This allowed comparisons to be made between the two Models, within Model by category, and between annotators. Although the coders had different backgrounds, they were treated as equals i.e. there was no ‘expert’ who served as a benchmark. There were three possible types of interannotator agreement: we all agreed on a choice of category, we all differed, or two annotators agreed and the third disagreed. This latter group of two-way agreemen</context>
</contexts>
<marker>Jenicek, 2006</marker>
<rawString>Milos Jenicek. 2006. How to read, understand, and write ‘Discussion’ sections in medical articles: An exercise in critical thinking. Med Sci Monitor, 12(6): SR28-SR36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoko Mizuta</author>
<author>Anna Korhonen</author>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Zone Analysis in Biology Articles as a Basis for Information Extraction.</title>
<date>2005</date>
<journal>International Journal of Medical Informatics,</journal>
<volume>75</volume>
<issue>6</issue>
<pages>468--487</pages>
<contexts>
<context position="1087" citStr="Mizuta et al. 2005" startWordPosition="165" endWordPosition="168">to showing a need to revise both Models, the results identified problems resulting from limitations in annotator expertise. In future work two types of annotators are required: those with biomedical domain expertise and those with an understanding of rhetorical structure. 1 Introduction Given the vast and growing body of biomedical research literature being published there is a need to develop automated text mining tools that will assist in filtering out the information most useful to researchers. Previous studies applying Argumentative Zoning (AZ) (Teufel et al. 1999) and Zone Analysis (ZA) (Mizuta et al. 2005) have shown that an analysis of the argumentative structure of a text can be of use in Information Extraction (IE). As an alternative approach, it was believed that Toulmin’s work on informal logic and argument structure (1958/2003) could reflect the rhetorical strategies used by the authors of biomedical research articles. In order to compare and evaluate these approaches two Models of argument were applied to the same set of biomedical research articles. Inter-annotator agreement/disagreement between and within Models was examined. Given that human-annotated data are ultimately to be used fo</context>
</contexts>
<marker>Mizuta, Korhonen, Mullen, Collier, 2005</marker>
<rawString>Yoko Mizuta, Anna Korhonen, Tony Mullen and Nigel Collier. 2005. Zone Analysis in Biology Articles as a Basis for Information Extraction. International Journal of Medical Informatics, 75(6): 468-487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability Measurement without Limits.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>319--326</pages>
<contexts>
<context position="1885" citStr="Reidsma and Carletta 2008" startWordPosition="287" endWordPosition="290">’s work on informal logic and argument structure (1958/2003) could reflect the rhetorical strategies used by the authors of biomedical research articles. In order to compare and evaluate these approaches two Models of argument were applied to the same set of biomedical research articles. Inter-annotator agreement/disagreement between and within Models was examined. Given that human-annotated data are ultimately to be used for machine learning purposes, there is growing recognition of the need to analyze coder disagreements in order to differentiate between systematic variation and noise (e.g. Reidsma and Carletta 2008). The goal of this study was to identify systematic disagreements as diagnostics for improving the Models of argument. 2 Annotation Project The two Models of rhetoric (argument) in Tables 1 and 2 were applied to a corpus of 12 articles downloaded at random from the BMC-series (BioMed Central) of journals. The corpus covered nine different domains, with a total of 400 sentences; the three annotators worked independently. Although the entire articles were read by the annotators, only the sentences in the Discussion section were argumentatively categorized. The annotators were the study coordinat</context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability Measurement without Limits. Computational Linguistics, 34(3): 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Jean Carletta</author>
<author>Mark Moens</author>
</authors>
<title>An annotation scheme for discourse-level argumentation in research articles.</title>
<date>1999</date>
<booktitle>Proceedings of the Eighth Meeting of the European Chapter of the Association for Computational Linguistics:</booktitle>
<pages>110--117</pages>
<contexts>
<context position="1043" citStr="Teufel et al. 1999" startWordPosition="157" endWordPosition="160">stics for improving the Models. In addition to showing a need to revise both Models, the results identified problems resulting from limitations in annotator expertise. In future work two types of annotators are required: those with biomedical domain expertise and those with an understanding of rhetorical structure. 1 Introduction Given the vast and growing body of biomedical research literature being published there is a need to develop automated text mining tools that will assist in filtering out the information most useful to researchers. Previous studies applying Argumentative Zoning (AZ) (Teufel et al. 1999) and Zone Analysis (ZA) (Mizuta et al. 2005) have shown that an analysis of the argumentative structure of a text can be of use in Information Extraction (IE). As an alternative approach, it was believed that Toulmin’s work on informal logic and argument structure (1958/2003) could reflect the rhetorical strategies used by the authors of biomedical research articles. In order to compare and evaluate these approaches two Models of argument were applied to the same set of biomedical research articles. Inter-annotator agreement/disagreement between and within Models was examined. Given that human</context>
</contexts>
<marker>Teufel, Carletta, Moens, 1999</marker>
<rawString>Simone Teufel, Jean Carletta and Mark Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. Proceedings of the Eighth Meeting of the European Chapter of the Association for Computational Linguistics: 110-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Toulmin</author>
</authors>
<title>The Uses of Argument.</title>
<date>1958</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.</location>
<contexts>
<context position="3419" citStr="Toulmin (1958" startWordPosition="535" endWordPosition="536">tated. In cases where an annotator was conflicted between categories guidelines for ‘trumping’ were provided with the Models. (For details on the Models, trumping systems, instructions to annotators, corpus data and a sample annotated article please see www.csd.uwo.ca/~mercer/White_Thesis09.pdf.) The first model (Model 1) of argumentation to be applied stems from work in AZ and ZA and was adapted by White. It focuses on the content of a text, essentially differentiating ‘new’ from ‘old’ information, and results from analysis (Table 1). The second model is based on the concepts and language of Toulmin (1958/2003). Jenicek applied Toulmin to create a guide for writing medical research articles (2006) and Graves (personal communications 2008, 2009) further adapted these ideas to work with our corpus (Model 2). Its main focus is to identify ‘Claims’ being made by the authors, but it also differentiates between internal and external evidence, as 132 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 132–136, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics well as categories of explanation and implication (Table 2). Category Specifications CON</context>
<context position="4938" citStr="Toulmin 1958" startWordPosition="749" endWordPosition="750">nt or previous results, significance or limitations of their study Table 1: Model 1 categories (White 2009) Category Specifications EXTRANEOUS (0) Statements extraneous to authors’ argumentation, not related to a CLAIM CLAIM (1) Proposition put forward based on analysis of results GROUNDS (2) Internal evidence from current study WARRANT/ Understanding of theproblem, or data, from BACKING (3) other studies QUALIFIER (4) Possible explanations for results, comparisons with external evidence PROBLEM IN Implications for the field, CONTEXT (5) future research directions Table 2: Model 2 categories (Toulmin 1958, Jenicek 2006, Graves 2009) 2.1 Results Data were compiled on individual annotator’s argument category choices for each of the 400 sentences, for each Model of rhetoric. This allowed comparisons to be made between the two Models, within Model by category, and between annotators. Although the coders had different backgrounds, they were treated as equals i.e. there was no ‘expert’ who served as a benchmark. There were three possible types of interannotator agreement: we all agreed on a choice of category, we all differed, or two annotators agreed and the third disagreed. This latter group of tw</context>
</contexts>
<marker>Toulmin, 1958</marker>
<rawString>Stephen E. Toulmin. 1958/2003. The Uses of Argument. Cambridge University Press, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara White</author>
</authors>
<title>Annotating a Corpus of Biomedical Research Texts: Two Models of Rhetorical Analysis.</title>
<date>2009</date>
<tech>PhD thesis,</tech>
<institution>The University of Western Ontario,</institution>
<note>www.csd.uwo.ca/~mercer/White_Thesis09.pdf</note>
<contexts>
<context position="4433" citStr="White 2009" startWordPosition="679" endWordPosition="680"> ACL 2010, pages 132–136, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics well as categories of explanation and implication (Table 2). Category Specifications CONTEXT (1) Background, accepted facts, previous work, motivation METHOD (2) Methods, tools, processes, experimental design CURRENT Findings of current RESULTS (3) experiment RESULTS Current results support or COMPARED (4) contradict previous work ANALYSIS (5) Possible interpretations or implications of current or previous results, significance or limitations of their study Table 1: Model 1 categories (White 2009) Category Specifications EXTRANEOUS (0) Statements extraneous to authors’ argumentation, not related to a CLAIM CLAIM (1) Proposition put forward based on analysis of results GROUNDS (2) Internal evidence from current study WARRANT/ Understanding of theproblem, or data, from BACKING (3) other studies QUALIFIER (4) Possible explanations for results, comparisons with external evidence PROBLEM IN Implications for the field, CONTEXT (5) future research directions Table 2: Model 2 categories (Toulmin 1958, Jenicek 2006, Graves 2009) 2.1 Results Data were compiled on individual annotator’s argument </context>
</contexts>
<marker>White, 2009</marker>
<rawString>Barbara White. 2009. Annotating a Corpus of Biomedical Research Texts: Two Models of Rhetorical Analysis. PhD thesis, The University of Western Ontario, Canada. www.csd.uwo.ca/~mercer/White_Thesis09.pdf</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>