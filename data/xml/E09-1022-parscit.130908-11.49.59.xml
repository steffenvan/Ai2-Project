<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001375">
<title confidence="0.96943">
Learning to Interpret Utterances Using Dialogue History
</title>
<author confidence="0.998148">
David DeVault
</author>
<affiliation confidence="0.9984785">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.616408">
Marina del Rey, CA 90292
</address>
<email confidence="0.998496">
devault@ict.usc.edu
</email>
<author confidence="0.984173">
Matthew Stone
</author>
<affiliation confidence="0.9703685">
Department of Computer Science
Rutgers University
</affiliation>
<address confidence="0.469799">
Piscataway, NJ 08845-8019
</address>
<email confidence="0.992963">
Matthew.Stone@rutgers.edu
</email>
<sectionHeader confidence="0.997278" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975166666667">
We describe a methodology for learning a
disambiguation model for deep pragmatic
interpretations in the context of situated
task-oriented dialogue. The system accu-
mulates training examples for ambiguity
resolution by tracking the fates of alter-
native interpretations across dialogue, in-
cluding subsequent clarificatory episodes
initiated by the system itself. We illus-
trate with a case study building maxi-
mum entropy models over abductive in-
terpretations in a referential communica-
tion task. The resulting model correctly re-
solves 81% of ambiguities left unresolved
by an initial handcrafted baseline. A key
innovation is that our method draws exclu-
sively on a system’s own skills and experi-
ence and requires no human annotation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981816666667">
In dialogue, the basic problem of interpretation is
to identify the contribution a speaker is making to
the conversation. There is much to recognize: the
domain objects and properties the speaker is refer-
ring to; the kind of action that the speaker is per-
forming; the presuppositions and implicatures that
relate that action to the ongoing task. Neverthe-
less, since the seminal work of Hobbs et al. (1993),
it has been possible to conceptualize pragmatic in-
terpretation as a unified reasoning process that se-
lects a representation of the speaker’s contribution
that is most preferred according to a background
model of how speakers tend to behave.
In principle, the problem of pragmatic interpre-
tation is qualitatively no different from the many
problems that have been tackled successfully by
data-driven models in NLP. However, while re-
searchers have shown that it is sometimes possi-
ble to annotate corpora that capture features of in-
terpretation, to provide empirical support for the-
ories, as in (Eugenio et al., 2000), or to build
classifiers that assist in dialogue reasoning, as in
(Jordan and Walker, 2005), it is rarely feasible
to fully annotate the interpretations themselves.
The distinctions that must be encoded are subtle,
theoretically-loaded and task-specific—and they
are not always signaled unambiguously by the
speaker. See (Poesio and Vieira, 1998; Poesio
and Artstein, 2005), for example, for an overview
of problems of vagueness, underspecification and
ambiguity in reference annotation.
As an alternative to annotation, we argue here
that dialogue systems can and should prepare
their own training data by inference from under-
specified models, which provide sets of candi-
date meanings, and from skilled engagement with
their interlocutors, who know which meanings are
right. Our specific approach is based on contribu-
tion tracking (DeVault, 2008), a framework which
casts linguistic inference in situated, task-oriented
dialogue in probabilistic terms. In contribution
tracking, ambiguous utterances may result in alter-
native possible contexts. As subsequent utterances
are interpreted in those contexts, ambiguities may
ramify, cascade, or disappear, giving new insight
into the pattern of activity that the interlocutor is
engaged in. For example, consider what happens
if the system initiates clarification. The interlocu-
tor’s answer may indicate not only what they mean
now but also what they must have meant earlier
when they used the original ambiguous utterance.
Contribution tracking allows a system to accu-
mulate training examples for ambiguity resolution
by tracking the fates of alternative interpretations
across dialogue. The system can use these ex-
amples to improve its models of pragmatic inter-
pretation. To demonstrate the feasibility of this
approach in realistic situations, we present a sys-
tem that tracks contributions to a referential com-
munication task using an abductive interpretation
</bodyText>
<note confidence="0.922986">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 184–192,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99799">
184
</page>
<bodyText confidence="0.999248272727273">
model: see Section 2. A user study with this sys-
tem, described in Section 3, shows that this sys-
tem can, in the course of interacting with its users,
discover the correct interpretations of many poten-
tially ambiguous utterances. The system thereby
automatically acquires a body of training data in
its native representations. We use this data to build
a maximum entropy model of pragmatic interpre-
tation in our referential communication task. After
training, we correctly resolve 81% of the ambigu-
ities left open in our handcrafted baseline.
</bodyText>
<sectionHeader confidence="0.828925" genericHeader="method">
2 Contribution tracking
</sectionHeader>
<bodyText confidence="0.999968157894737">
We continue a tradition of research that uses sim-
ple referential communication tasks to explore the
organization and processing of human–computer
and mediated human–human conversation, includ-
ing recently (DeVault and Stone, 2007; Gergle
et al., 2007; Healey and Mills, 2006; Schlangen
and Fern´andez, 2007). Our specific task is a two-
player object-identification game adapted from the
experiments of Clark and Wilkes-Gibbs (1986)
and Brennan and Clark (1996); see Section 2.1.
To play this game, our agent, COREF, inter-
prets utterances as performing sequences of task-
specific problem-solving acts using a combination
of grammar-based constraint inference and abduc-
tive plan recognition; see Section 2.2. Crucially,
COREF’s capabilities also include the ambiguity
management skills described in Section 2.3, in-
cluding policies for asking and answering clarifi-
cation questions.
</bodyText>
<subsectionHeader confidence="0.998045">
2.1 A referential communication task
</subsectionHeader>
<bodyText confidence="0.981150736842105">
The game plays out in a special-purpose graphical
interface, which can support either human–human
or human–agent interactions. Two players work
together to create a specific configuration of ob-
jects, or a scene, by adding objects into the scene
one at a time. Their interfaces display the same set
of candidate objects (geometric objects that differ
in shape, color and pattern), but their locations are
shuffled. The shuffling undermines the use of spa-
tial expressions such as “the object at bottom left”.
Figures 1 and 2 illustrate the different views.1
1Note that in a human–human game, there are literally
two versions of the graphical interface on the separate com-
puters the human participants are using. In a human–agent
interaction, COREF does not literally use the graphical inter-
face, but the information that COREF is provided is limited
to the information the graphical interface would provide to a
human participant. For example, COREF is not aware of the
locations of objects on its partner’s screen.
</bodyText>
<figureCaption confidence="0.9111005">
Figure 1: A human user plays an object identifi-
cation game with COREF. The figure shows the
perspective of the user (denoted c4). The user is
playing the role of director, and trying to identify
the diamond at upper right (indicated to the user
by the blue arrow) to COREF.
Figure 2: The conversation of Figure 1 from
COREF’s perspective. COREF is playing the role
of matcher, and trying to determine which object
the user wants COREF to identify.
</figureCaption>
<bodyText confidence="0.99980025">
As in the experiments of Clark and Wilkes-
Gibbs (1986) and Brennan and Clark (1996), one
of the players, who plays the role of director,
instructs the other player, who plays the role of
matcher, which object is to be added next to the
scene. As the game proceeds, the next target ob-
ject is automatically determined by the interface
and privately indicated to the director with a blue
arrow, as shown in Figure 1. (Note that the corre-
sponding matcher’s perspective, shown in Figure
2, does not include the blue arrow.) The director’s
job is then to get the matcher to click on (their ver-
sion of) this target object.
To achieve agreement about the target, the two
players can exchange text through an instant-
messaging modality. (This is the only communi-
</bodyText>
<figure confidence="0.991752470588235">
Candidate Objects Your scene
Present: [c4, Agent], Active: []
c4: brown diamond
c4: yes
or
Continue (next object)
Skip this object
History
You (c4:)
Candidate Objects Your scene
You (Agent:)
or
Present: [c4, Agent], Active: []
Skip this object
History
c4: brown diamond
c4: yes
</figure>
<page confidence="0.996515">
185
</page>
<bodyText confidence="0.999757875">
cation channel.) Each player’s interface provides
a real-time indication that their partner is “Active”
while their partner is composing an utterance, but
the interface does not show in real-time what is
being typed. Once the Enter key is pressed, the
utterance appears to both players at the bottom of
a scrollable display which provides full access to
all the previous utterances in the dialogue.
When the matcher clicks on an object they be-
lieve is the target, their version of that object is pri-
vately moved into their scene. The director has no
visible indication that the matcher has clicked on
an object. However, the director needs to click the
Continue (next object) button (see Fig-
ure 1) in order to move the current target into the
director’s scene, and move on to the next target
object. This means that the players need to discuss
not just what the target object is, but also whether
the matcher has added it, so that they can coordi-
nate on the right moment to move on to the next
object. If this coordination succeeds, then after
the director and matcher have completed a series
of objects, they will have created the exact same
scene in their separate interfaces.
</bodyText>
<subsectionHeader confidence="0.999903">
2.2 Interpreting user utterances
</subsectionHeader>
<bodyText confidence="0.999883789473684">
COREF treats interpretation broadly as a prob-
lem of abductive intention recognition (Hobbs et
al., 1993).2 We give a brief sketch here to high-
light the content of COREF’s representations, the
sources of information that COREF uses to con-
struct them, and the demands they place on disam-
biguation. See DeVault (2008) for full details.
COREF’s utterance interpretations take the
form of action sequences that it believes would
constitute coherent contributions to the dialogue
task in the current context. Interpretations are con-
structed abductively in that the initial actions in
the sequence need not be directly tied to observ-
able events; they may be tacit in the terminology
of Thomason et al. (2006). Examples of such tacit
actions include clicking an object, initiating a clar-
ification, or abandoning a previous question. As
a concrete example, consider utterance (1b) from
the dialogue of Figure 1, repeated here as (1):
</bodyText>
<listItem confidence="0.99319725">
(1) a. COREF: is the target round?
b. c4: brown diamond
c. COREF: do you mean dark brown?
d. c4: yes
</listItem>
<bodyText confidence="0.8796115">
2In fact, the same reasoning interprets utterances, button
presses and the other actions COREF observes!
In interpreting (1b), COREF hypothesizes that the
user has tacitly abandoned the agent’s question in
(1a). In fact, COREF identifies two possible inter-
pretations for (1b):
</bodyText>
<equation confidence="0.98961525">
i2,1= ( c4:tacitAbandonTasks[2],
c4:addcr[t7,rhombus(t7)],
c4:setPrag[inFocus(t7)],
c4:addcr[t7,saddlebrown(t7)])
i2,2= ( c4:tacitAbandonTasks[2],
c4:addcr[t7,rhombus(t7)],
c4:setPrag[inFocus(t7)],
c4:addcr[t7,sandybrown(t7)])
</equation>
<bodyText confidence="0.999874121951219">
Both interpretations begin by assuming that
user c4 has tacitly abandoned the previous ques-
tion, and then further analyze the utterance as per-
forming three additional dialogue acts. When a di-
alogue act is preceded by tacit actions in an inter-
pretation, the speaker of the utterance implicates
that the earlier tacit actions have taken place (De-
Vault, 2008). These implicatures are an important
part of the interlocutors’ coordination in COREF’s
dialogues, but they are a major obstacle to annotat-
ing interpretations by hand.
Action sequences such as i2,1 and i2,2 are coher-
ent only when they match the state of the ongoing
referential communication game and the seman-
tic and pragmatic status of information in the dia-
logue. COREF tracks these connections by main-
taining a probability distribution over a set of di-
alogue states, each of which represents a possi-
ble thread that resolves the ambiguities in the di-
alogue history. For performance reasons, COREF
entertains up to three alternative threads of inter-
pretation; COREF strategically drops down to the
single most probable thread at the moment each
object is completed. Each dialogue state repre-
sents the stack of processes underway in the ref-
erential communication game; constituent activi-
ties include problem-solving interactions such as
identifying an object, information-seeking interac-
tions such as question–answer pairs, and ground-
ing processes such as acknowledgment and clari-
fication. Dialogue states also represent pragmatic
information including recent utterances and refer-
ents which are salient or in focus.
COREF abductively recognizes the intention I
of an actor in three steps. First, for each dia-
logue state sk, COREF builds a horizon graph of
possible tacit action sequences that could be as-
sumed coherently, given the pending tasks (De-
Vault, 2008).
Second, COREF uses the horizon graph and
other resources to solve any constraints associ-
</bodyText>
<page confidence="0.994244">
186
</page>
<bodyText confidence="0.999985423076923">
ated with the observed action. This step instanti-
ates any free parameters associated with the action
to contextually relevant values. For utterances,
the relevant constraints are identified by parsing
the utterance using a hand-built, lexicalized tree-
adjoining grammar. In interpreting (1b), the parse
yields an ambiguity in the dialogue act associated
with the word “brown”, which may mean either
of the two shades of brown in Figure 1, which
COREF distinguishes using its saddlebrown
and sandybrown concepts.
Once COREF has identified a set of interpre-
tations {it,1, ..., it,n} for an utterance o at time t,
the last step is to assign a probability to each. In
general, we conceive of this following Hobbs et
al. (1993): the agent should weigh the different
assumptions that went into constructing each in-
terpretation.3 Ultimately, this process should be
made sensitive to the rich range of factors that
are available from COREF’s deep representation
of the dialogue state and the input utterance—this
is our project in this paper. However, in our initial
implemented prototype, COREF assigned these
probabilities using a simple hand-built model con-
sidering only NT, the number of tacit actions ab-
ductively assumed to occur in an interpretation:
</bodyText>
<equation confidence="0.9954035">
P(I = it,j  |o, St = sk) a Tit 1 + 1 (1)
N J)
</equation>
<bodyText confidence="0.999741">
In effect, this is a “null hypothesis” that assigns
relatively uniform weights to different abductive
hypotheses.
</bodyText>
<subsectionHeader confidence="0.983879">
2.3 Interactive disambiguation
</subsectionHeader>
<bodyText confidence="0.9999584375">
COREF uses its probabilistic model of context in
order to tolerate ambiguity as it moves forward
with its dialogues and to resolve ambiguity over
time. We have put particular effort into COREF’s
skills with three kinds of ambiguity: word-sense
ambiguities, where COREF finds multiple resolu-
tions for the domain concept evoked by the use
of a lexical item, as in the interaction (1) of Fig-
ure 1; referential ambiguities, where COREF takes
a noun phrase to be compatible with multiple ob-
jects from the display; and speech act ambiguities,
where alternative interpretations communicate or
implicate different kinds of contributions to the
ongoing task.
The resolution of ambiguity may involve some
combination of asking questions of the user, ag-
</bodyText>
<footnote confidence="0.999084">
3Though note that Hobbs et al. do not explicitly construe
their weights in terms of probabilities.
</footnote>
<figureCaption confidence="0.950147333333333">
Figure 3: A human user plays the director role in
an object identification game with COREF. The
figure shows the perspective of the user.
</figureCaption>
<bodyText confidence="0.9952434">
gregating information provided by the user across
multiple turns of dialogue, and strategically drop-
ping threads of interpretation. For example,
COREF represents the context resulting from (1b)
in terms of two states: one from interpretation i2,1
and one from i2,2. COREF asks a clarification
question (1c); the user’s explicit answer yes allows
COREF to discard one of the possible states and
allocate all its probability mass to the other one.
The dialogue in (2) shows an alternative case.
</bodyText>
<listItem confidence="0.994168333333333">
(2) a. c1: color is orange
b. c1: shape is diamond
c. COREF: ok
</listItem>
<bodyText confidence="0.998405866666667">
The example is taken from the setting shown in
Figure 3. In this case, COREF finds two colors on
the screen it thinks the user could intend to evoke
with the word orange; the peachy orange of the di-
amond and circle on the top row and the brighter
orange of the solid and empty squares in the mid-
dle column. COREF responds to the ambiguity by
introducing two states which track the alternative
colors. Immediately COREF gets an additional
description from the user, and adds the constraint
that the object is a diamond. As there is no bright
orange diamond, there is no way to interpret the
user’s utterance in the bright orange state; COREF
discards this state and allocates all its probability
mass to the other one.
</bodyText>
<sectionHeader confidence="0.994146" genericHeader="method">
3 Inferring the fates of interpretations
</sectionHeader>
<bodyText confidence="0.99441625">
Our approach is based on the observation that
COREF’s contribution tracking can be viewed as
assigning a fate to every dialogue state it enter-
tains as part of some thread of interpretation. In
</bodyText>
<page confidence="0.991528">
187
</page>
<bodyText confidence="0.999934117647059">
particular, if we consider the agent’s contribution
tracking retrospectively, every dialogue state can
be assigned a fate of correct or incorrect, where a
state is viewed as correct if it or some of its descen-
dants eventually capture all the probability mass
that COREF is distributing across the viable sur-
viving states, and incorrect otherwise.
In general, there are two ways that a state can
end up with fate incorrect. One way is that the
state and all of its descendants are eventually de-
nied any probability mass due to a failure to in-
terpret a subsequent utterance or action as a co-
herent contribution from any of those states. In
this case, we say that the incorrect state was elimi-
nated. The second way a state can end up incorrect
is if COREF makes a strategic decision to drop the
state, or all of its surviving descendants, at a time
when the state or its descendants were assigned
nonzero probability mass. In this case we say that
the incorrect state was dropped. Meanwhile, be-
cause COREF drops all states but one after each
object is completed, there is a single hypothesized
state at each time t whose descendants will ulti-
mately capture all of COREF’s probability mass.
Thus, for each time t, COREF will retrospectively
classify exactly one state as correct.
Of course, we really want to classify interpre-
tations. Because we seek to estimate P(I =
it,j|o, 5t = sk), which conditions the probability
assigned to I = it,j on the correctness of state
sk, we consider only those interpretations arising
in states that are retrospectively identified as cor-
rect. For each such interpretation, we start from
the state where that interpretation is adopted and
trace forward to a correct state or to its last surviv-
ing descendant. We classify the interpretation the
same way as that final state, either correct, elimi-
nated, or dropped.
We harvested a training set using this method-
ology from the transcripts of a previous evaluation
experiment designed to exercise COREF’s ambi-
guity management skills. The data comes from
20 subjects—most of them undergraduates par-
ticipating for course credit—who interacted with
COREF over the web in three rounds of the ref-
erential communication each. The number of ob-
jects increased from 4 to 9 to 16 across rounds;
the roles of director and matcher alternated in each
round, with the initial role assigned at random.
Of the 3275 sensory events that COREF in-
terpreted in these dialogues, from the (retrospec-
</bodyText>
<table confidence="0.991254333333333">
N Percentage N Percentage
0 10.53 5 0.21
1 79.76 6 0.12
2 7.79 7 0.09
3 0.85 8 0.06
4 0.58 9 0.0
</table>
<figureCaption confidence="0.93030425">
Figure 4: Distribution of degree of ambiguity in
training set. The table lists percentage of events
that had a specific number N of candidate inter-
pretations constructed from the correct state.
</figureCaption>
<bodyText confidence="0.999606">
tively) correct state, COREF hypothesized 0 inter-
pretations for 345 events, 1 interpretation for 2612
events, and more than one interpretation for 318
events. The overall distribution in the number of
interpretations hypothesized from the correct state
is given in Figure 4.
</bodyText>
<sectionHeader confidence="0.961928" genericHeader="method">
4 Learning pragmatic interpretation
</sectionHeader>
<bodyText confidence="0.9999916">
We capture the fate of each interpretation it,j in a
discrete variable F whose value is correct, elimi-
nated, or dropped. We also represent each inten-
tion it,j, observation o, and state sk in terms of
features. We seek to learn a function
</bodyText>
<equation confidence="0.970972">
P(F = correct I features(it,j),
features(o),
features(sk))
</equation>
<bodyText confidence="0.883203428571429">
from a set of training examples E = lei,..., enJ
where, for l = 1..n, we have:
el = ( F = fate(it,j), features(it,j),
features(o), features(sk)).
We chose to train maximum entropy models
(Berger et al., 1996). Our learning framework is
described in Section 4.1; the results in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.999043">
4.1 Learning setup
</subsectionHeader>
<bodyText confidence="0.999974307692308">
We defined a range of potentially useful features,
which we list in Figures 5, 6, and 7. These fea-
tures formalize pragmatic distinctions that plau-
sibly provide evidence of the correct interpreta-
tion for a user utterance or action. You might
annotate any of these features by hand, but com-
puting them automatically lets us easily explore a
much larger range of possibilities. To allow these
various kinds of features (integer-valued, binary-
valued, and string-valued) to interface to the max-
imum entropy model, these features were con-
verted into a much broader class of indicator fea-
tures taking on a value of either 0.0 or 1.0.
</bodyText>
<page confidence="0.98484">
188
</page>
<bodyText confidence="0.597261416666667">
feature set description
NumTacitActions The number of tacit actions in it,j.
TaskActions These features represent the action type (function symbol) of
each action ak in it,j = (A1 : a1, A2 : a2,..., An : an), as a
string.
ActorDoesTaskAction For each Ak : ak in it,j = (A1 : a1, A2 : a2,..., An : an), a
feature indicates that Ak (represented as string “Agent” or
“User”) has performed action ak (represented as a string
action type, as in the TaskActions features).
Presuppositions If o is an utterance, we include a string representation of each
presupposition assigned to o by it,j. The predicate/argument
structure is captured in the string, but any gensym identifiers
within the string (e.g. target12) are replaced with
exemplars for that identifier type (e.g. target).
Assertions If o is an utterance, we include a string representation of each
dialogue act assigned to o by it,j. Gensym identifiers are
filtered as in the Presuppositions features.
Syntax If o is an utterance, we include a string representation of the
bracketed phrase structure of the syntactic analysis assigned to
o by it,j. This includes the categories of all non-terminals in
the structure.
FlexiTaskIntentionActors Given it,j = (A1 : a1, A2 : a2,..., An : an), we include a single
string feature capturing the actor sequence (A1, A2,..., An) in
it,j (e.g. “User, Agent, Agent”).
</bodyText>
<figureCaption confidence="0.998653">
Figure 5: The interpretation features, features(it,j), available for selection in our learned model.
</figureCaption>
<bodyText confidence="0.949648">
feature set description
Words If o is an utterance, we include features that indicate the
presence of each word that occurs in the utterance.
</bodyText>
<figureCaption confidence="0.998748">
Figure 6: The observation features, features(o), available for selection in our learned model.
</figureCaption>
<bodyText confidence="0.7895988">
feature set description
NumTasksUnderway The number of tasks underway in sk.
TasksUnderway The name, stack depth, and current task state for each task
underway in sk.
NumRemainingReferents The number of objects yet to be identified in sk.
</bodyText>
<table confidence="0.987950111111111">
TabulatedFacts String features representing each proposition in the
conversational record in sk (with filtered gensym identifiers).
CurrentTargetConstraints String features for each positive and negative constraint on the
current target in sk (with filtered gensym identifiers). E.g.
“positive: squareFigureObject(target)” or
“negative: solidFigureObject(target)”.
UsefulProperties String features for each property instantiated in the experiment
interface in sk. E.g. “squareFigureObject”,
“solidFigureObject”, etc.
</table>
<figureCaption confidence="0.998352">
Figure 7: The dialogue state features, features(sk), available for selection in our learned model.
</figureCaption>
<page confidence="0.997002">
189
</page>
<bodyText confidence="0.9998704">
We used the MALLET maximum entropy clas-
sifier (McCallum, 2002) as an off-the-shelf, train-
able maximum entropy model. Each run involved
two steps. First, we applied MALLET’s feature
selection algorithm, which incrementally selects
features (as well as conjunctions of features) that
maximize an exponential gain function which rep-
resents the value of the feature in predicting in-
terpretation fates. Based on manual experimenta-
tion, we chose to have MALLET select about 300
features for each learned model. In the second
step, the selected features were used to train the
model to estimate probabilities. We used MAL-
LET’s implementation of Limited-Memory BFGS
(Nocedal, 1980).
</bodyText>
<subsectionHeader confidence="0.981216">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998948588235294">
We are generally interested in whether COREF’s
experience with previous subjects can be lever-
aged to improve its interactions with new sub-
jects. Therefore, to evaluate our approach, while
making maximal use of our available data set, we
performed a hold-one-subject-out cross-validation
using our 20 human subjects H = {h1, ..., h20}.
That is, for each subject hi, we trained a model
on the training examples associated with subjects
H \ {hi}, and then tested the model on the exam-
ples associated with subject hi.
To quantify the performance of the learned
model in comparison to our baseline, we adapt
the mean reciprocal rank statistic commonly used
for evaluation in information retrieval (Vorhees,
1999). We expect that a system will use the prob-
abilities calculated by a disambiguation model to
decide which interpretations to pursue and how to
follow them up through the most efficient interac-
tion. What matters is not the absolute probability
of the correct interpretation but its rank with re-
spect to competing interpretations. Thus, we con-
sider each utterance as a query; the disambigua-
tion model produces a ranked list of responses for
this query (candidate interpretations), ordered by
probability. We find the rank r of the correct in-
terpretation in this list and measure the outcome
of the query as r. Because of its weak assump-
tions, our baseline disambiguation model actually
leaves many ties. So in fact we must compute an
expected reciprocal rank (ERR) statistic that aver-
ages 1� over all ways of ordering the correct inter-
pretation against competitors of equal probability.
Figure 8 shows a histogram of ERR across
</bodyText>
<table confidence="0.9945185">
ERR range Hand-built Learned
model models
1 20.75% 81.76%
[12, 1) 74.21% 16.35%
[13, 12) 3.46% 1.26%
[0, 13) 1.57% 0.63%
mean(ERR) 0.77 0.92
var(ERR) 0.02 0.03
</table>
<figureCaption confidence="0.9156995">
Figure 8: For the 318 ambiguous sensory events,
the distribution of the expected reciprocal of rank
of the correct interpretation, for the initial, hand-
built model and the learned models in aggregate.
</figureCaption>
<bodyText confidence="0.997131333333333">
the ambiguous utterances from the corpus. The
learned models correctly resolve almost 82%,
while the baseline model correctly resolves about
21%. In fact, the learned models get much of this
improvement by learning weights to break the ties
in our baseline model. The overall performance
measure for a disambiguation model is the mean
expected reciprocal rank across all examples in the
corpus. The learned model improves this metric to
0.92 from a baseline of 0.77. The difference is un-
ambiguously significant (Wilcoxon rank sum test
W = 23743.5, p &lt; 10−15).
</bodyText>
<subsectionHeader confidence="0.999331">
4.3 Selected features
</subsectionHeader>
<bodyText confidence="0.9999638">
Feature selection during training identified a vari-
ety of syntactic, semantic, and pragmatic features
as useful in disambiguating correct interpretations.
Selections were made from every feature set in
Figures 5, 6, and 7. It was often possible to iden-
tify relevant features as playing a role in successful
disambiguation by the learned models. For exam-
ple, the learned model trained on H \ {c4} deliv-
ered the following probabilities for the two inter-
pretations COREF found for c4’s utterance (1b):
</bodyText>
<equation confidence="0.927948">
p(I = i2,11o, 52 = s8923) = 0.665
p(I = i2,21o, 52 = s8923) = 0.335
</equation>
<bodyText confidence="0.999922111111111">
The correct interpretation, i2,1, hypothesizes that
the user means saddlebrown, the darker of the
two shades of brown in the display. Among the
features selected in this model is a Presupposi-
tions feature (see Figure 5) which is present just
in case the word ‘brown’ is interpreted as mean-
ing saddlebrown rather than some other shade.
This feature allows the learned model to prefer
to interpret c4’s use of ‘brown’ as meaning this
</bodyText>
<page confidence="0.990714">
190
</page>
<bodyText confidence="0.9119495">
darker shade of brown, based on the observed lin-
guistic behavior of other users.
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="evaluation">
5 Results in context
</sectionHeader>
<bodyText confidence="0.999982396551724">
Our work adds to a body of research learning deep
models of language from evidence implicit in an
agent’s interactions with its environment. It shares
much of its motivation with co-training (Blum and
Mitchell, 1998) in improving initial models by
leveraging additional data that is easy to obtain.
However, as the examples of Section 2.3 illustrate,
COREF’s interactions with its users offer substan-
tially more information about interpretation than
the raw text generally used for co-training. Closer
in spirit is AI research on learning vocabulary
items by connecting user vocabulary to the agent’s
perceptual representations at the time of utterance
(Oates et al., 2000; Roy and Pentland, 2002; Co-
hen et al., 2002; Yu and Ballard, 2004; Steels
and Belpaeme, 2005). Our framework augments
this information about utterance context with ad-
ditional evidence about meaning from linguistic
interaction. In general, dialogue coherence is an
important source of evidence for all aspects of lan-
guage, for both human language learning (Saxton
et al., 2005) as well as machine models. For exam-
ple, Bohus et al. (2008) use users’ confirmations
of their spoken requests in a multi-modal interface
to tune the system’s ASR rankings for recognizing
subsequent utterances.
Our work to date has a number of limitations.
First, although 318 ambiguous interpretations did
occur, this user study provided a relatively small
number of ambiguous interpretations, in machine
learning terms; and most (80.2%) of those that did
occur were 2-way ambiguities. A richer domain
would require both more data and a generative ap-
proach to model-building and search.
Second, this learning experiment has been per-
formed after the fact, and we have not yet inves-
tigated the performance of the learned model in a
follow-up experiment in which COREF uses the
learned model in interactions with its users.
A third limitation lies in the detection of
‘correct’ interpretations. Our scheme some-
times conflates the user’s actual intentions with
COREF’s subsequent assumptions about them. If
COREF decides to strategically drop the user’s
actual intended interpretation, our scheme may
mark another interpretation as ‘correct’. Alterna-
tive approaches may do better at harvesting mean-
ingful examples of correct and incorrect interpre-
tations from an agent’s dialogue experience. Our
approach also depends on having clear evidence
about what an interlocutor has said and whether
the system has interpreted it correctly—evidence
that is often unavailable with spoken input or
information-seeking tasks. Thus, even when spo-
ken language interfaces use probabilistic inference
for dialogue management (Williams and Young,
2007), new techniques may be needed to mine
their experience for correct interpretations.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999996318181818">
We have implemented a system COREF that
makes productive use of its dialogue experience by
learning to rank new interpretations based on fea-
tures it has historically associated with correct ut-
terance interpretations. We present these results as
a proof-of-concept that contribution tracking pro-
vides a source of information that an agent can
use to improve its statistical interpretation process.
Further work is required to scale these techniques
to richer dialogue systems, and to understand the
best architecture for extracting evidence from an
agent’s interpretive experience and modeling that
evidence for future language use. Nevertheless,
we believe that these results showcase how judi-
cious system-building efforts can lead to dialogue
capabilities that defuse some of the bottlenecks to
learning rich pragmatic interpretation. In particu-
lar, a focus on improving our agents’ basic abilities
to tolerate and resolve ambiguities as a dialogue
proceeds may prove to be a valuable technique for
improving the overall dialogue competence of the
agents we build.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999634777777778">
This work was sponsored in part by NSF CCF-
0541185 and HSD-0624191, and by the U.S.
Army Research, Development, and Engineering
Command (RDECOM). Statements and opinions
expressed do not necessarily reflect the position or
the policy of the Government, and no official en-
dorsement should be inferred. Thanks to our re-
viewers, Rich Thomason, David Traum and Jason
Williams.
</bodyText>
<page confidence="0.995123">
191
</page>
<table confidence="0.629845">
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and
Paul Martin. 1993. Interpretation as abduction. Ar-
tificial Intelligence, 63:69–142.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. JAIR, 24:157–194.
</table>
<sectionHeader confidence="0.729574" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.72706">
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39–71.
</bodyText>
<reference confidence="0.998306463157895">
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory, pages 92–100.
Dan Bohus, Xiao Li, Patrick Nguyen, and Geoffrey
Zweig. 2008. Learning n-best correction models
from implicit user feedback in a multi-modal local
search application. In The 9th SIGdial Workshop on
Discourse and Dialogue.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology, 22(6):1482–
1493.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. In Philip R. Co-
hen, Jerry Morgan, and Martha E. Pollack, editors,
Intentions in Communication, pages 463–493. MIT
Press, Cambridge, Massachusetts, 1990.
Paul R. Cohen, Tim Oates, Carole R. Beal, and Niall
Adams. 2002. Contentful mental states for robot
baby. In Eighteenth national conference on Artifi-
cial intelligence, pages 126–131, Menlo Park, CA,
USA. American Association for Artificial Intelli-
gence.
David DeVault and Matthew Stone. 2007. Managing
ambiguities across utterances in dialogue. In Pro-
ceedings of the 11th Workshop on the Semantics and
Pragmatics of Dialogue (Decalog 2007), pages 49–
56.
David DeVault. 2008. Contribution Tracking: Par-
ticipating in Task-Oriented Dialogue under Uncer-
tainty. Ph.D. thesis, Department of Computer Sci-
ence, Rutgers, The State University of New Jersey,
New Brunswick, NJ.
Barbara Di Eugenio, Pamela W. Jordan, Richmond H.
Thomason, and Johanna D. Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogue. International Journal of Human-Computer
Studies, 53:1017–1076.
Darren Gergle, Carolyn P. Ros´e, and Robert E. Kraut.
2007. Modeling the impact of shared visual infor-
mation on collaborative reference. In CHI 2007 Pro-
ceedings, pages 1543–1552.
Patrick G. T. Healey and Greg J. Mills. 2006. Partic-
ipation, precedence and co-ordination in dialogue.
In Proceedings of Cognitive Science, pages 1470–
1475.
Andrew McCallum. 2002. MALLET: A
MAchine learning for LanguagE toolkit.
http://mallet.cs.umass.edu.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773–782.
Tim Oates, Zachary Eyler-Walker, and Paul R. Co-
hen. 2000. Toward natural language interfaces for
robotic agents. In Proc. Agents, pages 227–228.
Massimo Poesio and Ron Artstein. 2005. Annotating
(anaphoric) ambiguity. In Proceedings of the Cor-
pus Linguistics Conference.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183–216.
Deb Roy and Alex Pentland. 2002. Learning words
from sights and sounds: A computational model.
Cognitive Science, 26(1):113–146.
Matthew Saxton, Carmel Houston-Price, and Natasha
Dawson. 2005. The prompt hypothesis: clarifica-
tion requests as corrective input for grammatical er-
rors. Applied Psycholinguistics, 26(3):393–414.
David Schlangen and Raquel Fern´andez. 2007. Speak-
ing through a noisy channel: Experiments on in-
ducing clarification behaviour in human–human di-
alogue. In Proceedings of Interspeech 2007.
Luc Steels and Tony Belpaeme. 2005. Coordinating
perceptually grounded categories through language.
a case study for colour. Behavioral and Brain Sci-
ences, 28(4):469–529.
Richmond H. Thomason, Matthew Stone, and David
DeVault. 2006. Enlightened update: A
computational architecture for presupposition and
other pragmatic phenomena. For the Ohio
State Pragmatics Initiative, 2006, available at
http://www.research.rutgers.edu/˜ddevault/.
Ellen M. Vorhees. 1999. The TREC-8 question an-
swering track report. In Proceedings of the 8th Text
Retrieval Conference, pages 77–82.
Jason Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393–422.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1:57–80.
</reference>
<page confidence="0.998191">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942037">
<title confidence="0.999819">Learning to Interpret Utterances Using Dialogue History</title>
<author confidence="0.999925">David DeVault</author>
<affiliation confidence="0.99963">Institute for Creative Technologies University of Southern California</affiliation>
<address confidence="0.999349">Marina del Rey, CA 90292</address>
<email confidence="0.999625">devault@ict.usc.edu</email>
<author confidence="0.999634">Matthew Stone</author>
<affiliation confidence="0.9999625">Department of Computer Science Rutgers University</affiliation>
<address confidence="0.996956">Piscataway, NJ 08845-8019</address>
<email confidence="0.998588">Matthew.Stone@rutgers.edu</email>
<abstract confidence="0.997274368421053">We describe a methodology for learning a disambiguation model for deep pragmatic interpretations in the context of situated task-oriented dialogue. The system accumulates training examples for ambiguity resolution by tracking the fates of alternative interpretations across dialogue, including subsequent clarificatory episodes initiated by the system itself. We illustrate with a case study building maximum entropy models over abductive interpretations in a referential communication task. The resulting model correctly resolves 81% of ambiguities left unresolved by an initial handcrafted baseline. A key innovation is that our method draws exclusively on a system’s own skills and experience and requires no human annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="28160" citStr="Blum and Mitchell, 1998" startWordPosition="4506" endWordPosition="4509">ay. Among the features selected in this model is a Presuppositions feature (see Figure 5) which is present just in case the word ‘brown’ is interpreted as meaning saddlebrown rather than some other shade. This feature allows the learned model to prefer to interpret c4’s use of ‘brown’ as meaning this 190 darker shade of brown, based on the observed linguistic behavior of other users. 5 Results in context Our work adds to a body of research learning deep models of language from evidence implicit in an agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substantially more information about interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about ut</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Xiao Li</author>
<author>Patrick Nguyen</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Learning n-best correction models from implicit user feedback in a multi-modal local search application.</title>
<date>2008</date>
<booktitle>In The 9th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="29053" citStr="Bohus et al. (2008)" startWordPosition="4646" endWordPosition="4649">ing. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to date has a number of limitations. First, although 318 ambiguous interpretations did occur, this user study provided a relatively small number of ambiguous interpretations, in machine learning terms; and most (80.2%) of those that did occur were 2-way ambiguities. A richer domain would require both more data and a generative approach to model-building and search. Second, this learning experiment has been performed after the fact, and </context>
</contexts>
<marker>Bohus, Li, Nguyen, Zweig, 2008</marker>
<rawString>Dan Bohus, Xiao Li, Patrick Nguyen, and Geoffrey Zweig. 2008. Learning n-best correction models from implicit user feedback in a multi-modal local search application. In The 9th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Herbert H Clark</author>
</authors>
<title>Conceptual pacts and lexical choice in conversation.</title>
<date>1996</date>
<journal>Journal of Experimental Psychology,</journal>
<volume>22</volume>
<issue>6</issue>
<pages>1493</pages>
<contexts>
<context position="5191" citStr="Brennan and Clark (1996)" startWordPosition="776" endWordPosition="779">ion in our referential communication task. After training, we correctly resolve 81% of the ambiguities left open in our handcrafted baseline. 2 Contribution tracking We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human–computer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fern´andez, 2007). Our specific task is a twoplayer object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996); see Section 2.1. To play this game, our agent, COREF, interprets utterances as performing sequences of taskspecific problem-solving acts using a combination of grammar-based constraint inference and abductive plan recognition; see Section 2.2. Crucially, COREF’s capabilities also include the ambiguity management skills described in Section 2.3, including policies for asking and answering clarification questions. 2.1 A referential communication task The game plays out in a special-purpose graphical interface, which can support either human–human or human–agent interactions. Two players work t</context>
<context position="7190" citStr="Brennan and Clark (1996)" startWordPosition="1097" endWordPosition="1100">icipant. For example, COREF is not aware of the locations of objects on its partner’s screen. Figure 1: A human user plays an object identification game with COREF. The figure shows the perspective of the user (denoted c4). The user is playing the role of director, and trying to identify the diamond at upper right (indicated to the user by the blue arrow) to COREF. Figure 2: The conversation of Figure 1 from COREF’s perspective. COREF is playing the role of matcher, and trying to determine which object the user wants COREF to identify. As in the experiments of Clark and WilkesGibbs (1986) and Brennan and Clark (1996), one of the players, who plays the role of director, instructs the other player, who plays the role of matcher, which object is to be added next to the scene. As the game proceeds, the next target object is automatically determined by the interface and privately indicated to the director with a blue arrow, as shown in Figure 1. (Note that the corresponding matcher’s perspective, shown in Figure 2, does not include the blue arrow.) The director’s job is then to get the matcher to click on (their version of) this target object. To achieve agreement about the target, the two players can exchange</context>
</contexts>
<marker>Brennan, Clark, 1996</marker>
<rawString>Susan E. Brennan and Herbert H. Clark. 1996. Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology, 22(6):1482– 1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>463--493</pages>
<editor>In Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="5162" citStr="Clark and Wilkes-Gibbs (1986)" startWordPosition="771" endWordPosition="774">opy model of pragmatic interpretation in our referential communication task. After training, we correctly resolve 81% of the ambiguities left open in our handcrafted baseline. 2 Contribution tracking We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human–computer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fern´andez, 2007). Our specific task is a twoplayer object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996); see Section 2.1. To play this game, our agent, COREF, interprets utterances as performing sequences of taskspecific problem-solving acts using a combination of grammar-based constraint inference and abductive plan recognition; see Section 2.2. Crucially, COREF’s capabilities also include the ambiguity management skills described in Section 2.3, including policies for asking and answering clarification questions. 2.1 A referential communication task The game plays out in a special-purpose graphical interface, which can support either human–human or human–agent int</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as a collaborative process. In Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, editors, Intentions in Communication, pages 463–493. MIT Press, Cambridge, Massachusetts, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
<author>Tim Oates</author>
<author>Carole R Beal</author>
<author>Niall Adams</author>
</authors>
<title>Contentful mental states for robot baby.</title>
<date>2002</date>
<journal>Artificial Intelligence.</journal>
<booktitle>In Eighteenth national conference on Artificial intelligence,</booktitle>
<pages>126--131</pages>
<publisher>American Association for</publisher>
<location>Menlo Park, CA, USA.</location>
<contexts>
<context position="28660" citStr="Cohen et al., 2002" startWordPosition="4583" endWordPosition="4587">agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substantially more information about interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to date has a number of limitations. First, alt</context>
</contexts>
<marker>Cohen, Oates, Beal, Adams, 2002</marker>
<rawString>Paul R. Cohen, Tim Oates, Carole R. Beal, and Niall Adams. 2002. Contentful mental states for robot baby. In Eighteenth national conference on Artificial intelligence, pages 126–131, Menlo Park, CA, USA. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Matthew Stone</author>
</authors>
<title>Managing ambiguities across utterances in dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Workshop on the Semantics and Pragmatics of Dialogue (Decalog</booktitle>
<pages>49--56</pages>
<contexts>
<context position="4961" citStr="DeVault and Stone, 2007" startWordPosition="741" endWordPosition="744">interpretations of many potentially ambiguous utterances. The system thereby automatically acquires a body of training data in its native representations. We use this data to build a maximum entropy model of pragmatic interpretation in our referential communication task. After training, we correctly resolve 81% of the ambiguities left open in our handcrafted baseline. 2 Contribution tracking We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human–computer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fern´andez, 2007). Our specific task is a twoplayer object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996); see Section 2.1. To play this game, our agent, COREF, interprets utterances as performing sequences of taskspecific problem-solving acts using a combination of grammar-based constraint inference and abductive plan recognition; see Section 2.2. Crucially, COREF’s capabilities also include the ambiguity management skills described in Section 2.3, including policies fo</context>
</contexts>
<marker>DeVault, Stone, 2007</marker>
<rawString>David DeVault and Matthew Stone. 2007. Managing ambiguities across utterances in dialogue. In Proceedings of the 11th Workshop on the Semantics and Pragmatics of Dialogue (Decalog 2007), pages 49– 56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
</authors>
<title>Contribution Tracking: Participating in Task-Oriented Dialogue under Uncertainty.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Rutgers, The State University of New Jersey,</institution>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="2924" citStr="DeVault, 2008" startWordPosition="437" endWordPosition="438">ed and task-specific—and they are not always signaled unambiguously by the speaker. See (Poesio and Vieira, 1998; Poesio and Artstein, 2005), for example, for an overview of problems of vagueness, underspecification and ambiguity in reference annotation. As an alternative to annotation, we argue here that dialogue systems can and should prepare their own training data by inference from underspecified models, which provide sets of candidate meanings, and from skilled engagement with their interlocutors, who know which meanings are right. Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilistic terms. In contribution tracking, ambiguous utterances may result in alternative possible contexts. As subsequent utterances are interpreted in those contexts, ambiguities may ramify, cascade, or disappear, giving new insight into the pattern of activity that the interlocutor is engaged in. For example, consider what happens if the system initiates clarification. The interlocutor’s answer may indicate not only what they mean now but also what they must have meant earlier when they used the origin</context>
<context position="9667" citStr="DeVault (2008)" startWordPosition="1524" endWordPosition="1525"> added it, so that they can coordinate on the right moment to move on to the next object. If this coordination succeeds, then after the director and matcher have completed a series of objects, they will have created the exact same scene in their separate interfaces. 2.2 Interpreting user utterances COREF treats interpretation broadly as a problem of abductive intention recognition (Hobbs et al., 1993).2 We give a brief sketch here to highlight the content of COREF’s representations, the sources of information that COREF uses to construct them, and the demands they place on disambiguation. See DeVault (2008) for full details. COREF’s utterance interpretations take the form of action sequences that it believes would constitute coherent contributions to the dialogue task in the current context. Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events; they may be tacit in the terminology of Thomason et al. (2006). Examples of such tacit actions include clicking an object, initiating a clarification, or abandoning a previous question. As a concrete example, consider utterance (1b) from the dialogue of Figure 1, repeated he</context>
<context position="11239" citStr="DeVault, 2008" startWordPosition="1752" endWordPosition="1754">ible interpretations for (1b): i2,1= ( c4:tacitAbandonTasks[2], c4:addcr[t7,rhombus(t7)], c4:setPrag[inFocus(t7)], c4:addcr[t7,saddlebrown(t7)]) i2,2= ( c4:tacitAbandonTasks[2], c4:addcr[t7,rhombus(t7)], c4:setPrag[inFocus(t7)], c4:addcr[t7,sandybrown(t7)]) Both interpretations begin by assuming that user c4 has tacitly abandoned the previous question, and then further analyze the utterance as performing three additional dialogue acts. When a dialogue act is preceded by tacit actions in an interpretation, the speaker of the utterance implicates that the earlier tacit actions have taken place (DeVault, 2008). These implicatures are an important part of the interlocutors’ coordination in COREF’s dialogues, but they are a major obstacle to annotating interpretations by hand. Action sequences such as i2,1 and i2,2 are coherent only when they match the state of the ongoing referential communication game and the semantic and pragmatic status of information in the dialogue. COREF tracks these connections by maintaining a probability distribution over a set of dialogue states, each of which represents a possible thread that resolves the ambiguities in the dialogue history. For performance reasons, COREF</context>
<context position="12702" citStr="DeVault, 2008" startWordPosition="1980" endWordPosition="1982">l communication game; constituent activities include problem-solving interactions such as identifying an object, information-seeking interactions such as question–answer pairs, and grounding processes such as acknowledgment and clarification. Dialogue states also represent pragmatic information including recent utterances and referents which are salient or in focus. COREF abductively recognizes the intention I of an actor in three steps. First, for each dialogue state sk, COREF builds a horizon graph of possible tacit action sequences that could be assumed coherently, given the pending tasks (DeVault, 2008). Second, COREF uses the horizon graph and other resources to solve any constraints associ186 ated with the observed action. This step instantiates any free parameters associated with the action to contextually relevant values. For utterances, the relevant constraints are identified by parsing the utterance using a hand-built, lexicalized treeadjoining grammar. In interpreting (1b), the parse yields an ambiguity in the dialogue act associated with the word “brown”, which may mean either of the two shades of brown in Figure 1, which COREF distinguishes using its saddlebrown and sandybrown conce</context>
</contexts>
<marker>DeVault, 2008</marker>
<rawString>David DeVault. 2008. Contribution Tracking: Participating in Task-Oriented Dialogue under Uncertainty. Ph.D. thesis, Department of Computer Science, Rutgers, The State University of New Jersey, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Pamela W Jordan</author>
<author>Richmond H Thomason</author>
<author>Johanna D Moore</author>
</authors>
<title>The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogue.</title>
<date>2000</date>
<journal>International Journal of Human-Computer Studies,</journal>
<pages>53--1017</pages>
<marker>Di Eugenio, Jordan, Thomason, Moore, 2000</marker>
<rawString>Barbara Di Eugenio, Pamela W. Jordan, Richmond H. Thomason, and Johanna D. Moore. 2000. The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogue. International Journal of Human-Computer Studies, 53:1017–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Gergle</author>
<author>Carolyn P Ros´e</author>
<author>Robert E Kraut</author>
</authors>
<title>Modeling the impact of shared visual information on collaborative reference.</title>
<date>2007</date>
<booktitle>In CHI 2007 Proceedings,</booktitle>
<pages>1543--1552</pages>
<marker>Gergle, Ros´e, Kraut, 2007</marker>
<rawString>Darren Gergle, Carolyn P. Ros´e, and Robert E. Kraut. 2007. Modeling the impact of shared visual information on collaborative reference. In CHI 2007 Proceedings, pages 1543–1552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick G T Healey</author>
<author>Greg J Mills</author>
</authors>
<title>Participation, precedence and co-ordination in dialogue.</title>
<date>2006</date>
<booktitle>In Proceedings of Cognitive Science,</booktitle>
<pages>1470--1475</pages>
<contexts>
<context position="5006" citStr="Healey and Mills, 2006" startWordPosition="749" endWordPosition="752">utterances. The system thereby automatically acquires a body of training data in its native representations. We use this data to build a maximum entropy model of pragmatic interpretation in our referential communication task. After training, we correctly resolve 81% of the ambiguities left open in our handcrafted baseline. 2 Contribution tracking We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human–computer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fern´andez, 2007). Our specific task is a twoplayer object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996); see Section 2.1. To play this game, our agent, COREF, interprets utterances as performing sequences of taskspecific problem-solving acts using a combination of grammar-based constraint inference and abductive plan recognition; see Section 2.2. Crucially, COREF’s capabilities also include the ambiguity management skills described in Section 2.3, including policies for asking and answering clarification question</context>
</contexts>
<marker>Healey, Mills, 2006</marker>
<rawString>Patrick G. T. Healey and Greg J. Mills. 2006. Participation, precedence and co-ordination in dialogue. In Proceedings of Cognitive Science, pages 1470– 1475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>MALLET: A MAchine learning for LanguagE toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="23635" citStr="McCallum, 2002" startWordPosition="3769" endWordPosition="3770"> conversational record in sk (with filtered gensym identifiers). CurrentTargetConstraints String features for each positive and negative constraint on the current target in sk (with filtered gensym identifiers). E.g. “positive: squareFigureObject(target)” or “negative: solidFigureObject(target)”. UsefulProperties String features for each property instantiated in the experiment interface in sk. E.g. “squareFigureObject”, “solidFigureObject”, etc. Figure 7: The dialogue state features, features(sk), available for selection in our learned model. 189 We used the MALLET maximum entropy classifier (McCallum, 2002) as an off-the-shelf, trainable maximum entropy model. Each run involved two steps. First, we applied MALLET’s feature selection algorithm, which incrementally selects features (as well as conjunctions of features) that maximize an exponential gain function which represents the value of the feature in predicting interpretation fates. Based on manual experimentation, we chose to have MALLET select about 300 features for each learned model. In the second step, the selected features were used to train the model to estimate probabilities. We used MALLET’s implementation of Limited-Memory BFGS (Noc</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. MALLET: A MAchine learning for LanguagE toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating quasi-newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<volume>35</volume>
<issue>151</issue>
<contexts>
<context position="24246" citStr="Nocedal, 1980" startWordPosition="3862" endWordPosition="3863">02) as an off-the-shelf, trainable maximum entropy model. Each run involved two steps. First, we applied MALLET’s feature selection algorithm, which incrementally selects features (as well as conjunctions of features) that maximize an exponential gain function which represents the value of the feature in predicting interpretation fates. Based on manual experimentation, we chose to have MALLET select about 300 features for each learned model. In the second step, the selected features were used to train the model to estimate probabilities. We used MALLET’s implementation of Limited-Memory BFGS (Nocedal, 1980). 4.2 Evaluation We are generally interested in whether COREF’s experience with previous subjects can be leveraged to improve its interactions with new subjects. Therefore, to evaluate our approach, while making maximal use of our available data set, we performed a hold-one-subject-out cross-validation using our 20 human subjects H = {h1, ..., h20}. That is, for each subject hi, we trained a model on the training examples associated with subjects H \ {hi}, and then tested the model on the examples associated with subject hi. To quantify the performance of the learned model in comparison to our</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating quasi-newton matrices with limited storage. Mathematics of Computation, 35(151):773–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Oates</author>
<author>Zachary Eyler-Walker</author>
<author>Paul R Cohen</author>
</authors>
<title>Toward natural language interfaces for robotic agents.</title>
<date>2000</date>
<booktitle>In Proc. Agents,</booktitle>
<pages>227--228</pages>
<contexts>
<context position="28616" citStr="Oates et al., 2000" startWordPosition="4575" endWordPosition="4578">ls of language from evidence implicit in an agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substantially more information about interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to </context>
</contexts>
<marker>Oates, Eyler-Walker, Cohen, 2000</marker>
<rawString>Tim Oates, Zachary Eyler-Walker, and Paul R. Cohen. 2000. Toward natural language interfaces for robotic agents. In Proc. Agents, pages 227–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Ron Artstein</author>
</authors>
<title>Annotating (anaphoric) ambiguity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Corpus Linguistics Conference.</booktitle>
<contexts>
<context position="2450" citStr="Poesio and Artstein, 2005" startWordPosition="364" endWordPosition="367"> tackled successfully by data-driven models in NLP. However, while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation, to provide empirical support for theories, as in (Eugenio et al., 2000), or to build classifiers that assist in dialogue reasoning, as in (Jordan and Walker, 2005), it is rarely feasible to fully annotate the interpretations themselves. The distinctions that must be encoded are subtle, theoretically-loaded and task-specific—and they are not always signaled unambiguously by the speaker. See (Poesio and Vieira, 1998; Poesio and Artstein, 2005), for example, for an overview of problems of vagueness, underspecification and ambiguity in reference annotation. As an alternative to annotation, we argue here that dialogue systems can and should prepare their own training data by inference from underspecified models, which provide sets of candidate meanings, and from skilled engagement with their interlocutors, who know which meanings are right. Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilistic terms. In contribution tra</context>
</contexts>
<marker>Poesio, Artstein, 2005</marker>
<rawString>Massimo Poesio and Ron Artstein. 2005. Annotating (anaphoric) ambiguity. In Proceedings of the Corpus Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Renata Vieira</author>
</authors>
<title>A corpusbased investigation of definite description use.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="2422" citStr="Poesio and Vieira, 1998" startWordPosition="360" endWordPosition="363">y problems that have been tackled successfully by data-driven models in NLP. However, while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation, to provide empirical support for theories, as in (Eugenio et al., 2000), or to build classifiers that assist in dialogue reasoning, as in (Jordan and Walker, 2005), it is rarely feasible to fully annotate the interpretations themselves. The distinctions that must be encoded are subtle, theoretically-loaded and task-specific—and they are not always signaled unambiguously by the speaker. See (Poesio and Vieira, 1998; Poesio and Artstein, 2005), for example, for an overview of problems of vagueness, underspecification and ambiguity in reference annotation. As an alternative to annotation, we argue here that dialogue systems can and should prepare their own training data by inference from underspecified models, which provide sets of candidate meanings, and from skilled engagement with their interlocutors, who know which meanings are right. Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilisti</context>
</contexts>
<marker>Poesio, Vieira, 1998</marker>
<rawString>Massimo Poesio and Renata Vieira. 1998. A corpusbased investigation of definite description use. Computational Linguistics, 24(2):183–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb Roy</author>
<author>Alex Pentland</author>
</authors>
<title>Learning words from sights and sounds: A computational model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="28640" citStr="Roy and Pentland, 2002" startWordPosition="4579" endWordPosition="4582">evidence implicit in an agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substantially more information about interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to date has a number of lim</context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>Deb Roy and Alex Pentland. 2002. Learning words from sights and sounds: A computational model. Cognitive Science, 26(1):113–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Saxton</author>
<author>Carmel Houston-Price</author>
<author>Natasha Dawson</author>
</authors>
<title>The prompt hypothesis: clarification requests as corrective input for grammatical errors.</title>
<date>2005</date>
<journal>Applied Psycholinguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="28993" citStr="Saxton et al., 2005" startWordPosition="4634" endWordPosition="4637"> interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to date has a number of limitations. First, although 318 ambiguous interpretations did occur, this user study provided a relatively small number of ambiguous interpretations, in machine learning terms; and most (80.2%) of those that did occur were 2-way ambiguities. A richer domain would require both more data and a generative approach to model-building and search. Second, this</context>
</contexts>
<marker>Saxton, Houston-Price, Dawson, 2005</marker>
<rawString>Matthew Saxton, Carmel Houston-Price, and Natasha Dawson. 2005. The prompt hypothesis: clarification requests as corrective input for grammatical errors. Applied Psycholinguistics, 26(3):393–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Raquel Fern´andez</author>
</authors>
<title>Speaking through a noisy channel: Experiments on inducing clarification behaviour in human–human dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<marker>Schlangen, Fern´andez, 2007</marker>
<rawString>David Schlangen and Raquel Fern´andez. 2007. Speaking through a noisy channel: Experiments on inducing clarification behaviour in human–human dialogue. In Proceedings of Interspeech 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Steels</author>
<author>Tony Belpaeme</author>
</authors>
<title>Coordinating perceptually grounded categories through language. a case study for colour.</title>
<date>2005</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="28710" citStr="Steels and Belpaeme, 2005" startWordPosition="4592" endWordPosition="4595"> It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substantially more information about interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to date has a number of limitations. First, although 318 ambiguous interpretations did occur, thi</context>
</contexts>
<marker>Steels, Belpaeme, 2005</marker>
<rawString>Luc Steels and Tony Belpaeme. 2005. Coordinating perceptually grounded categories through language. a case study for colour. Behavioral and Brain Sciences, 28(4):469–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
<author>Matthew Stone</author>
<author>David DeVault</author>
</authors>
<title>Enlightened update: A computational architecture for presupposition and other pragmatic phenomena. For the Ohio State Pragmatics Initiative,</title>
<date>2006</date>
<contexts>
<context position="10054" citStr="Thomason et al. (2006)" startWordPosition="1583" endWordPosition="1586">tion (Hobbs et al., 1993).2 We give a brief sketch here to highlight the content of COREF’s representations, the sources of information that COREF uses to construct them, and the demands they place on disambiguation. See DeVault (2008) for full details. COREF’s utterance interpretations take the form of action sequences that it believes would constitute coherent contributions to the dialogue task in the current context. Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events; they may be tacit in the terminology of Thomason et al. (2006). Examples of such tacit actions include clicking an object, initiating a clarification, or abandoning a previous question. As a concrete example, consider utterance (1b) from the dialogue of Figure 1, repeated here as (1): (1) a. COREF: is the target round? b. c4: brown diamond c. COREF: do you mean dark brown? d. c4: yes 2In fact, the same reasoning interprets utterances, button presses and the other actions COREF observes! In interpreting (1b), COREF hypothesizes that the user has tacitly abandoned the agent’s question in (1a). In fact, COREF identifies two possible interpretations for (1b)</context>
</contexts>
<marker>Thomason, Stone, DeVault, 2006</marker>
<rawString>Richmond H. Thomason, Matthew Stone, and David DeVault. 2006. Enlightened update: A computational architecture for presupposition and other pragmatic phenomena. For the Ohio State Pragmatics Initiative, 2006, available at http://www.research.rutgers.edu/˜ddevault/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Vorhees</author>
</authors>
<title>The TREC-8 question answering track report.</title>
<date>1999</date>
<booktitle>In Proceedings of the 8th Text Retrieval Conference,</booktitle>
<pages>77--82</pages>
<contexts>
<context position="24970" citStr="Vorhees, 1999" startWordPosition="3978" endWordPosition="3979">aged to improve its interactions with new subjects. Therefore, to evaluate our approach, while making maximal use of our available data set, we performed a hold-one-subject-out cross-validation using our 20 human subjects H = {h1, ..., h20}. That is, for each subject hi, we trained a model on the training examples associated with subjects H \ {hi}, and then tested the model on the examples associated with subject hi. To quantify the performance of the learned model in comparison to our baseline, we adapt the mean reciprocal rank statistic commonly used for evaluation in information retrieval (Vorhees, 1999). We expect that a system will use the probabilities calculated by a disambiguation model to decide which interpretations to pursue and how to follow them up through the most efficient interaction. What matters is not the absolute probability of the correct interpretation but its rank with respect to competing interpretations. Thus, we consider each utterance as a query; the disambiguation model produces a ranked list of responses for this query (candidate interpretations), ordered by probability. We find the rank r of the correct interpretation in this list and measure the outcome of the quer</context>
</contexts>
<marker>Vorhees, 1999</marker>
<rawString>Ellen M. Vorhees. 1999. The TREC-8 question answering track report. In Proceedings of the 8th Text Retrieval Conference, pages 77–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="30625" citStr="Williams and Young, 2007" startWordPosition="4881" endWordPosition="4884">EF decides to strategically drop the user’s actual intended interpretation, our scheme may mark another interpretation as ‘correct’. Alternative approaches may do better at harvesting meaningful examples of correct and incorrect interpretations from an agent’s dialogue experience. Our approach also depends on having clear evidence about what an interlocutor has said and whether the system has interpreted it correctly—evidence that is often unavailable with spoken input or information-seeking tasks. Thus, even when spoken language interfaces use probabilistic inference for dialogue management (Williams and Young, 2007), new techniques may be needed to mine their experience for correct interpretations. 6 Conclusion We have implemented a system COREF that makes productive use of its dialogue experience by learning to rank new interpretations based on features it has historically associated with correct utterance interpretations. We present these results as a proof-of-concept that contribution tracking provides a source of information that an agent can use to improve its statistical interpretation process. Further work is required to scale these techniques to richer dialogue systems, and to understand the best</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>A multimodal learning interface for grounding spoken language in sensory perceptions.</title>
<date>2004</date>
<journal>ACM Transactions on Applied Perception,</journal>
<pages>1--57</pages>
<contexts>
<context position="28682" citStr="Yu and Ballard, 2004" startWordPosition="4588" endWordPosition="4591"> with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substantially more information about interpretation than the raw text generally used for co-training. Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005). Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction. In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. Our work to date has a number of limitations. First, although 318 ambiguous in</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>Chen Yu and Dana H. Ballard. 2004. A multimodal learning interface for grounding spoken language in sensory perceptions. ACM Transactions on Applied Perception, 1:57–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>