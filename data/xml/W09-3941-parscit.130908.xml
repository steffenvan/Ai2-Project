<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000505">
<title confidence="0.984449">
Evaluating automatic extraction of rules for sentence plan construction
</title>
<author confidence="0.906776">
Amanda Stent Martin Molina
</author>
<affiliation confidence="0.7013615">
AT&amp;T Labs – Research Department of Artificial Intelligence
Florham Park, NJ, USA Universidad Polit´ecnica de Madrid, Spain
</affiliation>
<email confidence="0.994505">
stent@research.att.com martin.molina@upm.es
</email>
<sectionHeader confidence="0.995554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999460625">
The freely available SPaRKy sentence
planner uses hand-written weighted rules
for sentence plan construction, and a user-
or domain-specific second-stage ranker for
sentence plan selection. However, coming
up with sentence plan construction rules
for a new domain can be difficult. In this
paper, we automatically extract sentence
plan construction rules from the RST-DT
corpus. In our rules, we use only domain-
independent features that are available to a
sentence planner at runtime. We evaluate
these rules, and outline ways in which they
can be used for sentence planning. We
have integrated them into a revised version
of SPaRKy.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996325">
Most natural language generation (NLG) systems
have a pipeline architecture consisting of four core
stages: content selection, discourse planning, sen-
tence planning, and surface realization (Reiter and
Dale, 2000; Rambow et al., 2001). A sentence
planner maps from an input discourse plan to an
output sentence plan. As part of this process it
performs several tasks, including sentence order-
ing, sentence aggregation, discourse cue insertion
and perhaps referring expression generation (Stent
et al., 2004; Walker et al., 2007; Williams and Re-
iter, 2003).
The developer of a sentence planner must typ-
ically write rules by hand (e.g. (Stent et al.,
2004; Walker et al., 2007)) or learn a domain-
specific model from a corpus of training data (e.g.
(Williams and Reiter, 2003)). Unfortunately, there
are very few corpora annotated with discourse
plans, and it is hard to automatically label a cor-
pus for discourse structure. It is also hard to
hand-write sentence planning rules starting from
a “blank slate”, as it were.
In this paper, we outline a method for ex-
tracting sentence plan construction rules from the
only publicly available corpus of discourse trees,
the RST Discourse Treebank (RST-DT) (Carl-
son et al., 2002). These rules use only domain-
independent information available to a sentence
planner at run-time. They have been integrated
into the freely-available SPaRKy sentence plan-
ner. They serve as a starting point for a user of
SPaRKy, who can add, remove or modify rules to
fit a particular domain.
We also describe a set of experiments in which
we look at each sentence plan construction task in
order, evaluating our rules for that task in terms
of coverage and discriminative power. We discuss
the implications of these experiments for sentence
planning.
The rest of this paper is structured as follows: In
Section 2 we describe the sentence planning pro-
cess using SPaRKy as an example. In Sections 3
through 5 we describe how we obtain sentence
plan construction rules. In Section 6, we evalu-
ate alternative rule sets. In Section 7, we describe
our modifications to the SPaRKy sentence planner
to use these rules. In Section 8 we conclude and
present future work.
</bodyText>
<sectionHeader confidence="0.890477" genericHeader="method">
2 Sentence Planning in SPaRKy
</sectionHeader>
<bodyText confidence="0.999932666666667">
The only publicly available sentence planner for
data-to-text generation is SPaRKy (Stent et al.,
2004). SPaRKy takes as input a discourse plan (a
tree with rhetorical relations on the internal nodes
and a proposition representing a text span on each
leaf), and outputs one or more sentence plans
</bodyText>
<note confidence="0.710024">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 290–297,
</note>
<affiliation confidence="0.66248">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998078">
290
</page>
<bodyText confidence="0.9991748">
(each a tree with discourse cues and/or punctua-
tion on the internal nodes). SPaRKy is a two-stage
sentence planner. First, possible sentence plans
are constructed through a sequence of decisions
made using only local information about single
nodes in the discourse plan. Second, the possible
sentence plans are ranked using a user- or domain-
specific sentence plan ranker that evaluates the
global quality of each sentence plan (Walker et al.,
2007).
Sentence plan construction in SPaRKy involves
three tasks: span ordering, sentence aggregation
(deciding whether to realize a pair of propositions
as a single clause, a single sentence, or two sen-
tences), and discourse cue selection1. SPaRKy
uses a single set of hand-written weighted rules
to perform these tasks. In the current distributed
version of SPaRKy, there are 20 rules covering
9 discourse cues (and, because, but, however, on
the other hand, since, while, with, and the default,
period). Each rule operates on the children of
one rhetorical relation, and may impose an order-
ing, insert punctuation or merge two propositions,
and/or insert a discourse cue. During sentence
plan construction, SPaRKy walks over the input
discourse plan, at each node finding all matching
rules and applying one which it selects probabilis-
tically according to the rule weights (with some
randomness to permit variation).
While the developer of a NLG system will al-
ways have to adapt the sentence planner to his or
her domain, it is often hard to come up with sen-
tence planning rules “from scratch”. As a result of
the work described here a SPaRKy user will have
a solid foundation for sentence plan construction.
</bodyText>
<sectionHeader confidence="0.996382" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999822166666667">
We use the Wall Street Journal Penn Treebank
corpus (Marcus et al., 1993), which is a corpus
of text annotated for syntactic structure. We also
use two additional annotations done on (parts of)
that corpus: PropBank (Kingsbury and Palmer,
2003), which consists of annotations for predicate-
argument structure; and the RST-DT (Carlson
et al., 2002), which consists of annotations for
rhetorical structure.
We had to process this data into a form suitable
for feature extraction. First, we produced a flat-
tened form of the syntactic annotations, in which
</bodyText>
<footnote confidence="0.975119">
1SPaRKy also does some referring expression generation,
in a single pass over each completed sentence plan.
</footnote>
<bodyText confidence="0.999906117647059">
each word was labeled with its part-of-speech tag
and the path to the root of the parse tree. Each
word was also assigned indices in the sentence (so
we could apply the PropBank annotations) and in
the document (so we could apply the RST-DT an-
notations)2.
Second, we attach to each word one or more
labels from the PropBank annotations (each label
consists of a predicate index, and either a predicate
name or a semantic role type and index).
Third, we extract relation information from the
RST-DT. For each relation, we extract the rela-
tion name, the types of each child (“Nucleus” or
“Satellite”), and the start and end word indices for
each child. Finally, we extract from the word-
level annotations the marked-up words for each
text span in each rhetorical relation.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999755592592593">
Features are individual rule conditions. In the
standard NLG pipeline, no information about the
realized text is available to the sentence planner.
However, existing sentence planners use lexical
and word sequence information to improve per-
formance for a particular domain. Williams and
Reiter (2003) appear to do surface realization be-
fore sentence planning, while Walker et al. (2007)
perform surface realization between sentence plan
construction and sentence plan ranking. We are
concerned with sentence plan construction only;
also, we want to produce sentence plan construc-
tion rules that are as domain-independent as pos-
sible. So we use no features that rely on having
realized text. However, we assume that the input
propositions have been fairly well fleshed-out, so
that one has information about predicate-argument
structure, tense, and the information status of enti-
ties to be realized.
A relation has a label as well as one or more
child text spans. The features we extract from our
data include both per-span and per-relation fea-
tures. In our experiments we use a subset of these
features which is fairly domain-independent and
does not overly partition our data. The complete
set of features (full) is as well as our reduced set
are given in Table 1.
</bodyText>
<footnote confidence="0.998757666666667">
2The Penn Treebank and the RST-DT segment words and
punctuation slightly differently, which makes it hard to align
the various annotations.
</footnote>
<page confidence="0.991959">
291
</page>
<table confidence="0.995257181818182">
Feature type Full feature set Reduced feature set
Per-relation relation, relation is leaf, parent relation, span coref, com- relation, relation is leaf, parent rela-
bined verb type class, combined verb type, identifier of tion, span coref, combined verb type
shortest span, temporal order of spans class, identifier of shortest span, tem-
poral order of spans
Per-span, span identifier span identifier span identifier
Per-span, span length number of NPs in span
Per-span, span verb verb type class, verb type, verb part of speech, verb is
negated, verb has modal
Per-span, arguments argument status for ARG0 to ARG5 plus ARGM-{EXT,
DIR, LOC, TMP, REC, PRD, ADV, MNR, CAU, PNC}
</table>
<tableCaption confidence="0.999412">
Table 1: Features used in evaluation
</tableCaption>
<subsectionHeader confidence="0.991239">
4.1 Per-Span Features
</subsectionHeader>
<bodyText confidence="0.999940056338028">
We extract per-span features from basic spans
(leaves of the RST tree) and from complex spans
(internal nodes of the RST tree). For each span we
compute: identifier, text, length, verb information,
span argument information, discourse cue infor-
mation, and span-final punctuation.
Identifier We need a way to refer to the child spans
in the rules. For relations having only one child
span of each type (Satellite or Nucleus), we order
the spans by type. Otherwise, we order the spans
alphabetically by span text. The span identifier for
each child span is the index of the span in the re-
sulting list.
Text We extract the text of the span, and the indices
of its first and last words in the Penn Treebank. We
only use this information during data extraction.
However, in a system like that of Williams and Re-
iter (Williams and Reiter, 2003), where sentence
planning is done after or with surface realization,
these features could be used. They could also be
used to train a sentence plan ranker for SPaRKy
specific to the news domain.
Length We use the number of base NPs in the span
(as we cannot rely on having the complete realiza-
tion during sentence planning).
Verb We extract verb type, which can be N/A (there
is no labeled predicate for the span), stat (the
span’s main verb is a form of “to be”), a single
PropBank predicate (e.g. create.01), or mixed (the
span contains more than one predicate). We then
abstract to get the verb type class: N/A, pb (a Prop-
Bank predicate), stat, or mixed.
If the span contains a single predicate or multi-
ple predicates all having the same part-of-speech
tag, we extract that (as an indicator of tense).
We also extract information about negation and
modals (using the PropBank tags ARGM-NEG
and ARGM-MOD).
Arguments We extract the text of the arguments
of the predicate(s) in the span: ARG0 to ARG5,
as well as ARGM-{EXT, DIR, LOC, TMP, REC,
PRD, ADV, MNR, CAU, PNC}. We then abstract
to get an approximation of information status.
An argument status feature covers zero or more
instantiations of the argument and can have the
value N/A (no instantiations), proper (proper noun
phrase(s)), pro (pronoun(s)), def (definite noun
phrase(s)), indef (indefinite noun phrase(s)), quant
(noun phrase (s) containing quantifiers), other (we
cannot determine a value), or mixed (the argument
instantiations are not all of the same type).
Discourse Cues We extract discourse cue informa-
tion from basic spans and from the first basic span
in complex spans. We identify discourse cue(s)
appearing at the start of the span, inside the span,
and at the end of the span. PropBank includes
the argument label ARGM-DIS for discourse cues;
however, we adopt a more expansive notion of dis-
course cue. We say that a discourse cue can be ei-
ther: any sequence of words all labeled ARGM-
DIS and belonging to the same predicate, any-
where in the span; or any cue from a (slightly
expanded version of) the set of cues studied by
Marcu (Marcu, 1997), if it appears at the start of a
span, at the end of a span, or immediately before or
after a comma, and if its lowest containing phrase
tag is one of {ADJP, ADVP, CONJP, FRAG, NP-
ADV, PP, UCP, SBAR, WH} or its part of speech
tag is one of {CC, WDT}3.
Punctuation We extract punctuation (N/A or . or ?
or ! or ; or : or ,) at the end of the span.
</bodyText>
<footnote confidence="0.9591272">
3We constructed these rules by extracting from the WSJ
Penn Treebank all instances of the cues in Marcu’s list, and
then examining instances where the word sequence was not
actually a discourse cue. Some mistakes still occur in cue
extraction.
</footnote>
<page confidence="0.990081">
292
</page>
<subsectionHeader confidence="0.973118">
4.2 Per-Relation Features
</subsectionHeader>
<bodyText confidence="0.999746846153846">
For each relation we compute: name, the com-
bined verb type and verb class of the child spans,
whether any argument instantiations in the child
spans are coreferential, and which child span is
shortest (or the temporal order of the child spans).
Relation, Parent Relation The core relation label
for the relation and its parent relation (e.g. attri-
bution for attribution-e and attribution-n).
Relation is Leaf True if child spans of the relation
are leaf spans (not themselves relations).
Combined Verb The shared verb for the relation:
the child spans’ verb type if there is only one non-
N/A verb type among the child spans; otherwise,
mixed. We then abstract from the shared verb type
to the shared verb type class.
Span Coreference We use the information Prop-
Bank gives about intra-sentential coreference. We
do not employ any algorithm or annotation to iden-
tify inter-sentential coreference.
Shortest Span The identifier of the child span with
the fewest base NPs.
Temporal Order of Spans For some relations (e.g.
sequence, temporal-before, temporal-after), the
temporal order is very important. For these rela-
tions we note the temporal order of the child spans
rather than the shortest span.
</bodyText>
<sectionHeader confidence="0.990978" genericHeader="method">
5 Rule Extraction
</sectionHeader>
<bodyText confidence="0.999685318181818">
Each rule we extract consists of a set of per-
relation and per-span features (the conditions), and
a pattern (the effects). The conditions contain
either: the relation only, features from the re-
duced feature set, or features from the full fea-
ture set. The pattern can be an ordering of child
spans, a set of between-span punctuation markers,
a set of discourse cues, or an ordering of child
spans mixed with punctuation markers and dis-
course cues. Each extracted rule is stored as XML.
We only extract rules for relations having two or
more children. We also exclude RST-DT’s span
and same-unit relations because they are not im-
portant for our task. Finally, because the accu-
racy of low-level (just above the span) rhetorical
relation annotation is greater than that of high-
level relation annotation, we extract rules from
two data sets: one only containing first-level re-
lations (those whose children are all basic spans),
and one containing all relations regardless of level
in the RST tree. The output from the rule ex-
traction process is six alternative rule sets for each
</bodyText>
<table confidence="0.925213741935484">
Concession rule:
conditions:
type child=”0”: nucleus, type child=”1”: satellite, shortest: 0,
isCoref: 0, isLeaf: 1, isSamePredClass: mixed,
numChildren: 2, relation: concession, parentRel: antithesis
effects:
order: 1 0, punc child=”1”: comma, cues child=”1”: while
example:
(1) While some automotive programs have been delayed,
(0) they have n’t been canceled
Sequence rule:
conditions:
type child=”0”: nucleus, type child=”1”: nucleus,
type child=”2”: nucleus, type child=”3”: nucleus,
isCoref: 1, isLeaf: 1, isSamePredClass: mixed,
numChildren: 4, relation: sequence, parentRel: circumstance,
temporalOrder: 0 1 2 3
effects:
order: 0 1 2 3, punc child=”0”: comma, punc child=”1”:
comma, punc child=”2”: n/a, cues child=”3”: and
example:
(0) when you can get pension fund money, (1) buy a portfolio,
(2) sell off pieces off it (3) and play your own game
Purpose rule:
conditions:
type child=”0”: nucleus, type child=”1”: satellite, shortest: 0,
isCoref: 0, isLeaf: 0, isSamePredClass: shared,
numChildren: 2, relation: purpose, parentRel: list
effects:
order: 0 1, punc child=”0”: n/a, cues child=”1”: so
example:
</table>
<listItem confidence="0.315101666666667">
(0) In a modern system the government ’s role is to give the
people as much choice as possible
(1) so they are capable of making a choice
</listItem>
<figureCaption confidence="0.8365015">
Figure 1: Glosses of extracted sentence planning
rules for three relations (reduced feature set)
</figureCaption>
<bodyText confidence="0.956000294117647">
sentence plan construction task: first-level or all
data, with either the relation condition alone, the
reduced feature set, or the full feature set.
The maximum number of patterns we could
have is 7680 per relation, if we limit ourselves
to condition sets, relation instances with only two
child spans, and a maximum of one discourse
cue to each span (two possible orderings for child
spans * four possible choices for punctuation *
480 choices for discourse cue on each span). By
contrast, for our all data set there are 5810 unique
rules conditioned on the reduced feature set (109.6
per relation) and 292 conditioned on just the rela-
tion (5.5 per relation). Example rules are given in
Figure 1. Even though the data constrains sentence
planning choices considerably, we still have many
rules (most differing only in discourse cues).
</bodyText>
<page confidence="0.996918">
293
</page>
<sectionHeader confidence="0.983345" genericHeader="method">
6 Rule Evaluation
</sectionHeader>
<subsectionHeader confidence="0.993493">
6.1 On Evaluation
</subsectionHeader>
<bodyText confidence="0.999992016129033">
There are two basic approaches to NLG, text-to-
text generation (in which a model learned from a
text corpus is applied to produce new texts from
text input) and data-to-text generation (in which
non-text input is converted into text output). In
text-to-text generation, there has been consider-
able work on sentence fusion and information or-
dering, which are partly sentence planning tasks.
For evaluation, researchers typically compare au-
tomatically produced text to the original human-
produced text, which is assumed to be “correct”
(e.g. (Karamanis, 2007; Barzilay and McKeown,
2005; Marsi and Krahmer, 2005)). However, an
evaluation that considers the only “correct” an-
swer for a sentence planning task to be the an-
swer in the original text is overly harsh. First, al-
though we assume that all the possibilities in the
human-produced text are “reasonable”, some may
be awkward or incorrect for particular domains,
while other less frequent ones in the newspaper
domain may be more “correct” in another domain.
Our purpose is to lay out sentence plan construc-
tion possibilities, not to reproduce the WSJ au-
thorial voice. Second, because SPaRKy is a two-
stage sentence planner and we are focusing here
on sentence plan construction, we can only evalu-
ate the local decisions made during that stage, not
the overall quality of SPaRKy’s output.
Evaluations of sentence planning tasks for data-
to-text generation have tended to focus solely
on discourse cues (e.g. (Eugenio et al., 1997;
Grote and Stede, 1998; Moser and Moore, 1995;
Nakatsu, 2008; Taboada, 2006)). By contrast, we
want good coverage for all core sentence planning
tasks. Although Walker et al. performed an eval-
uation of SPaRKy (Stent et al., 2004; Walker et
al., 2007), they evaluated the output from the sen-
tence planner as a whole, rather than evaluating
each stage separately. Williams and Reiter, in the
work most similar to ours, examined a subset of
the RST-DT corpus to see if they could use it to
perform span ordering, punctuation selection, and
discourse cue selection and placement. However,
they assumed that surface realization was already
complete, so they used lexical features. Their sen-
tence planner is not publicly available.
In the following sections, we evaluate the infor-
mation in our sentence plan construction rules in
terms of coverage and discriminative power. The
first type of evaluation allows us to assess the de-
gree to which our rules are general and provide
system developers with an adequate number of
choices for sentence planning. The second type
of evaluation allows us to evaluate whether our re-
duced feature set helps us choose from the avail-
able possibilities better than a feature set consist-
ing simply of the relation (i.e. is the complicated
feature extraction necessary). Because we include
the full feature set in this evaluation, it can also
be seen as a text-to-text generation type of evalua-
tion for readers who would like to use the sentence
planning rules for news-style text generation.
</bodyText>
<subsectionHeader confidence="0.995562">
6.2 Coverage
</subsectionHeader>
<bodyText confidence="0.99998075">
In our evaluation of coverage, we count the num-
ber of relations, discourse cues, and patterns we
have obtained, and compare against other data sets
described in the research literature.
</bodyText>
<subsectionHeader confidence="0.568747">
6.2.1 Relation Coverage
</subsectionHeader>
<bodyText confidence="0.998621133333333">
There are 57 unique core relation labels in
the RST-DT. We exclude span and same-unit.
Two others, elaboration-process-step and topic-
comment, never occur with two or more child
spans. Our first-level and all rules cover all of
the remaining 53. The most frequently occurring
relations are elaboration-additional, list, attribu-
tion, elaboration-object-attribute, contrast, cir-
cumstance and explanation-argumentative.
By contrast, the current version of SPaRKy cov-
ers only 4 relations (justify, contrast, sequence,
and infer)4.
Mann and Thompson originally defined 24 re-
lations (Mann and Thompson, 1987), while Hovy
and Maier listed about 70 (Hovy and Maier, 1992).
</bodyText>
<subsectionHeader confidence="0.853362">
6.2.2 Discourse Cue Coverage
</subsectionHeader>
<bodyText confidence="0.932874928571429">
Our first-level rules cover 92 discourse cues,
and our all rules cover 205 discourse cues. The
most commonly occurring discourse cues in both
cases are and, but, that, when, as, who and which.
By contrast, the current version of SPaRKy cov-
ers only about 9 discourse cues.
In his dissertation Marcu identified about 478
discourse cues. We used a modified version of
Marcu’s cue list to extract discourse cues from our
corpus, but some of Marcu’s discourse cues do not
occur in the RST-DT.
4Curiously, only two of these relations (contrast and se-
quence) appear in the RST-DT data (although infer may be
equivalent to span).
</bodyText>
<page confidence="0.993357">
294
</page>
<subsectionHeader confidence="0.942249">
6.2.3 Sentence Plan Pattern Coverage
</subsectionHeader>
<bodyText confidence="0.999954153846154">
For the first-level data we have 140 unique sen-
tence plan patterns using the relation condition
alone, and 1767 conditioning on the reduced fea-
ture set. For the all data we have 292 unique pat-
terns with relation condition alone and 5810 with
the reduced feature set. Most patterns differ only
in choice of discourse cue(s).
No system developer will want to examine all
5810 rules. However, she or he may wish to look
at the patterns for a particular relation. In our use
of SPaRKy, for example, we have extended the
patterns for the sequence relation by hand to cover
temporal sequences of up to seven steps.
</bodyText>
<subsectionHeader confidence="0.99858">
6.3 Discriminative Power
</subsectionHeader>
<bodyText confidence="0.999984157894737">
In this evaluation, we train decision tree classifiers
for each sentence plan construction task. We ex-
periment with both the first-level and all data sets
and with both the reduced and full feature sets.
For each experiment we perform ten-fold cross-
validation using the J48 decision tree implemen-
tation provided in Weka (Witten and Eibe, 2005)
with its default parameters. We also report perfor-
mance for a model that selects a pattern condition-
ing only on the relation. Finally, we report perfor-
mance of a baseline which always selects the most
frequent pattern.
We evaluate using 1-best classification accu-
racy, by comparing with the choice made in the
Penn Treebank for that task. We test for signifi-
cant differences between methods using Cochran’s
Q, followed by post-hoc McNemar tests if signif-
icant differences existed. We also report the fea-
tures with information gain greater than 0.1.
</bodyText>
<subsectionHeader confidence="0.963855">
6.3.1 Span Ordering
</subsectionHeader>
<bodyText confidence="0.999843272727273">
We have one input feature vector for each rela-
tion instance that has two children5. In the feature
vector, child spans are ordered by their identifiers,
and the pattern is either 0 1 (first child, then sec-
ond child) or 1 0 (second child, then first child).
Classification accuracy for all methods is re-
ported in Table 2. All methods perform signifi-
cantly better than baseline (p &lt; .001), and both
the reduced and full feature sets give results sig-
nificantly better than using the relation alone (p &lt;
.001). The full feature set performs significantly
</bodyText>
<footnote confidence="0.9918485">
5The number of relation instances with three or more child
spans is less than 2% of the data. Removing these relations
made it feasible for us to train classifiers without crashing
Weka.
</footnote>
<table confidence="0.909778">
First-level All
Baseline 71.8144 71.4356
Per-relation 84.2707 82.3894
Reduced 89.6092 90.3147
Full 90.2129 91.9666
Table 2: Span ordering classification accuracy.
For first-level data, n = 3147. For all data, n =
10170. Labels = {0 1, 1 0}.
First-level All
Baseline 74.5154 50.4425
Per-relation 74.5154 64.2773
Reduced 77.8201 72.1731
Full 74.3883 66.1357
</table>
<tableCaption confidence="0.995923">
Table 3: Between-span punctuation classification
</tableCaption>
<bodyText confidence="0.95628975">
accuracy. For first-level data, n = 3147. For all
data, n = 10170. Labels = {semicolon, comma,
full, N/A}.
better than the reduced feature set for the all data
set (p &lt; .001), but not for the first-level data set.
Most of the relations have a strong preference
for one ordering or the other. Most mistakes are
made on those that don’t (e.g. attribution, list).
</bodyText>
<subsectionHeader confidence="0.802074">
6.3.2 Punctuation Insertion
</subsectionHeader>
<bodyText confidence="0.99995624">
We have one input feature vector for each re-
lation instance that has two children. We assume
that span ordering is performed prior to punctu-
ation insertion, so the child spans are ordered as
they appear in the data. The pattern is the punc-
tuation mark that should appear between the two
child spans (one of N/A or comma or semicolon
or full6), which indicates whether the two children
should be realized as separate sentences, as sepa-
rate clauses, or merged.
Classification accuracy for all methods is re-
ported in Table 3. For the all data set, all meth-
ods perform significantly better than baseline (p &lt;
.001), and both the reduced and full feature sets
give results significantly better than using the re-
lation alone (p &lt; .001). Furthermore, the re-
duced feature set performs significantly better than
the full feature set (p &lt; .001). By contrast, for
the first-level data set, the reduced feature set per-
forms significantly better than all the other data
sets, while there are no statistically significant dif-
ferences in performance between the baseline, per-
relation and full feature sets.
The most common type of error was misclas-
sifying comma, semicolon or full as N/A: for the
</bodyText>
<footnote confidence="0.981071">
6full indicates a sentence boundary (. or ? or !).
</footnote>
<page confidence="0.98712">
295
</page>
<table confidence="0.9992162">
First-level All
Baseline 62.6629 68.4267
Per-relation 68.605 70.1377
Reduced 73.6257 73.9135
Full 74.3565 74.5919
</table>
<tableCaption confidence="0.963356">
Table 4: Discourse cue classification accuracy.
</tableCaption>
<bodyText confidence="0.993606166666667">
For first-level data, n = 3147 and no. labels = 92.
For all data, n = 10170 and no. labels = 203.
first-level data this is what the models trained on
the per-relation and full feature sets do most of the
time. The second most common type of error was
misclassifying comma, semicolon or N/A as full.
</bodyText>
<subsectionHeader confidence="0.931597">
6.3.3 Discourse cue selection
</subsectionHeader>
<bodyText confidence="0.9999775">
We have one input feature vector for each re-
lation instance having two children. We use the
same features as in the previous experiment, and
as in the previous experiment, we order the child
spans as they appear in the data. The pattern is the
first discourse cue appearing in the ordered child
spans7.
Classification accuracy for all methods is re-
ported in Table 4. All methods perform signifi-
cantly better than baseline (p &lt; .001), and both
the reduced and full feature sets give results sig-
nificantly better than using the relation alone (p &lt;
.001). The performance differences between the
reduced and full feature sets are not statistically
significant for either data set.
For this task, 44 of the 92 labels in the first-level
data, and 97 of the 203 labels in the all data, oc-
curred only once. These cues were typically misla-
beled. Commonly occurring labels were typically
labeled correctly.
</bodyText>
<subsectionHeader confidence="0.863168">
6.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9999539">
Our methods for rule extraction are not general in
the sense that they rely on having access to particu-
lar types of annotation which are not widely avail-
able nor readily obtainable by automatic means.
However, our extracted rules have quite broad cov-
erage and will give NLG system developers a jump
start when using and adapting SPaRKy.
Our reduced feature set compares favorably in
discriminative power to both our full feature set
and the per-relation feature set. It achieves a very
</bodyText>
<footnote confidence="0.944101833333333">
7Some relations have multiple cues, either independent
cues such as but and also, or cues that depend on each other
such as on the one hand and on the other hand. Using all
cues is infeasible, and there are too few span-internal and
span-final cues to break up the cue classification for this eval-
uation.
</footnote>
<bodyText confidence="0.9999415">
good fit to the input data for the span ordering task
and a good fit to the input data for the punctua-
tion and discourse cue insertion tasks, especially
for the first-level data set. Factors affect perfor-
mance include: the punctuation insertion data is
highly imbalanced (by far the most common label
is N/A), while for the discourse cue insertion task
there is a problem of data sparsity.
</bodyText>
<sectionHeader confidence="0.991623" genericHeader="method">
7 Revised SPaRKy
</sectionHeader>
<bodyText confidence="0.999568652173913">
One way to use these results would be to model the
sentence planning task as a cascade of classifiers,
but this method does not permit the system devel-
oper to add his or her own rules. So we continue to
use SPaRKy, which is rule-based. We have made
several changes to the Java version of SPaRKy to
support application of our sentence plan construc-
tion rules. We modified the classes for storing and
managing rules to read our XML rule format and
process rule conditions and patterns. We stripped
out the dependence on RealPro and added hooks
for SimpleNLG (Gatt and Reiter, 2009). We modi-
fied the rule application algorithm so that users can
choose to use a single rule set with patterns cov-
ering all three sentence planning tasks, or one rule
set for each sentence planning task. Also, since
there are now many rules, we give the user the
option to specify which relations jSPaRKy should
load rules for at each run.
Information about the revised jSparky, in-
cluding how to obtain it, is available at
http://www.research.att.com/˜stent/sparky2.0/
or by contacting the first author.
</bodyText>
<sectionHeader confidence="0.996228" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999271833333333">
In this paper we described how we extracted
less domain-dependent sentence plan construction
rules from the RST-DT corpus. We presented eval-
uations of our extracted rule sets and described
how we integrated them into the freely-available
SPaRKy sentence planner.
In future work, we will experiment with dis-
course cue clustering. We are also looking at alter-
native ways of doing sentence planning that permit
a tighter interleaving of sentence planning and sur-
face realization for improved efficiency and output
quality.
</bodyText>
<page confidence="0.997074">
296
</page>
<sectionHeader confidence="0.995893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860602564103">
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3):297–328.
L. Carlson, D. Marcu, and M. E. Okurowski. 2002.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of the SIGdial workshop on discourse and dialogue.
B. Di Eugenio, J. D. Moore, and M. Paolucci. 1997.
Learning features that predict cue usage. In Pro-
ceedings of the EACL.
A. Gatt and E. Reiter. 2009. SimpleNLG: A realisation
engine for practical applications. In Proceedings of
the European Workshop on Natural Language Gen-
eration.
B. Grote and M. Stede. 1998. Discourse marker choice
in sentence planning. In Proceedings of the 9th In-
ternational Workshop on Natural Language Gener-
ation.
E. Hovy and E. Maier. 1992. Parsimo-
nious or profligate: how many and which dis-
course structure relations? Available from
http://handle.dtic.mil/100.2/ADA278715.
N. Karamanis. 2007. Supplementing entity coherence
with local rhetorical relations for information order-
ing. Journal of Logic, Language and Information,
16(4):445–464.
P. Kingsbury and M. Palmer. 2003. PropBank: the
next level of TreeBank. In Proceedings of the Work-
shop on Treebanks and Lexical Theories.
B. Lavoi and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of ANLP.
W. Mann and S. Thompson. 1987. Rhetorical structure
theory: A theory of text organization. Technical Re-
port ISI/RS-87-190, Information Sciences Institute,
Los Angeles, CA.
D. Marcu. 1997. The rhetorical parsing, summa-
rization, and generation of natural language texts.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Toronto.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
E. Marsi and E. Krahmer. 2005. Explorations in sen-
tence fusion. In Proceedings of the European Work-
shop on Natural Language Generation.
M. Moser and J. D. Moore. 1995. Using discourse
analysis and automatic text generation to study dis-
course cue usage. In Proceedings of the AAAI 1995
Spring Symposium on Empirical Methods in Dis-
course Interpretation and Generation.
C. Nakatsu. 2008. Learning contrastive connective in
sentence realization ranking. In Proceedings of SIG-
dial 2008.
O. Rambow, S. Bangalore, and M. A. Walker. 2001.
Natural language generation in dialog systems. In
Proceedings of HLT.
E. Reiter and R. Dale. 2000. Building natural lan-
guage generation systems. Cambridge University
Press, Cambridge, UK.
A. Stent, R. Prasad, and M. A. Walker. 2004. Trainable
sentence planning for complex information presen-
tations in spoken dialog systems. In Proceedings of
the ACL.
M. Taboada. 2006. Discourse markers as signals (or
not) of rhetorical relations. Journal of Pragmatics,
38(4):567–592.
M. A. Walker, A. Stent, F. Mairesse, and R. Prasad.
2007. Individual and domain adaptation in sentence
planning for dialogue. Journal of Artificial Intelli-
gence Research, 30:413–456.
S. Williams and E. Reiter. 2003. A corpus analysis of
discourse relations for natural language generation.
In Proceedings of Corpus Linguistics.
I. Witten and F. Eibe. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
</reference>
<page confidence="0.997406">
297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798248">
<title confidence="0.999622">Evaluating automatic extraction of rules for sentence plan construction</title>
<author confidence="0.999873">Amanda Stent Martin Molina</author>
<affiliation confidence="0.980994">AT&amp;T Labs – Research Department of Artificial Intelligence</affiliation>
<address confidence="0.844609">Florham Park, NJ, USA Universidad Polit´ecnica de Madrid, Spain</address>
<email confidence="0.964306">stent@research.att.commartin.molina@upm.es</email>
<abstract confidence="0.998456">The freely available SPaRKy sentence planner uses hand-written weighted rules for sentence plan construction, and a useror domain-specific second-stage ranker for sentence plan selection. However, coming up with sentence plan construction rules for a new domain can be difficult. In this paper, we automatically extract sentence plan construction rules from the RST-DT corpus. In our rules, we use only domainindependent features that are available to a sentence planner at runtime. We evaluate these rules, and outline ways in which they can be used for sentence planning. We have integrated them into a revised version of SPaRKy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="17583" citStr="Barzilay and McKeown, 2005" startWordPosition="2865" endWordPosition="2868">s). 293 6 Rule Evaluation 6.1 On Evaluation There are two basic approaches to NLG, text-totext generation (in which a model learned from a text corpus is applied to produce new texts from text input) and data-to-text generation (in which non-text input is converted into text output). In text-to-text generation, there has been considerable work on sentence fusion and information ordering, which are partly sentence planning tasks. For evaluation, researchers typically compare automatically produced text to the original humanproduced text, which is assumed to be “correct” (e.g. (Karamanis, 2007; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005)). However, an evaluation that considers the only “correct” answer for a sentence planning task to be the answer in the original text is overly harsh. First, although we assume that all the possibilities in the human-produced text are “reasonable”, some may be awkward or incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we a</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>R. Barzilay and K. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2002</date>
<booktitle>In Proceedings of the SIGdial workshop on discourse and dialogue.</booktitle>
<contexts>
<context position="2142" citStr="Carlson et al., 2002" startWordPosition="323" endWordPosition="327">ypically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with discourse plans, and it is hard to automatically label a corpus for discourse structure. It is also hard to hand-write sentence planning rules starting from a “blank slate”, as it were. In this paper, we outline a method for extracting sentence plan construction rules from the only publicly available corpus of discourse trees, the RST Discourse Treebank (RST-DT) (Carlson et al., 2002). These rules use only domainindependent information available to a sentence planner at run-time. They have been integrated into the freely-available SPaRKy sentence planner. They serve as a starting point for a user of SPaRKy, who can add, remove or modify rules to fit a particular domain. We also describe a set of experiments in which we look at each sentence plan construction task in order, evaluating our rules for that task in terms of coverage and discriminative power. We discuss the implications of these experiments for sentence planning. The rest of this paper is structured as follows: </context>
<context position="5665" citStr="Carlson et al., 2002" startWordPosition="896" endWordPosition="899"> NLG system will always have to adapt the sentence planner to his or her domain, it is often hard to come up with sentence planning rules “from scratch”. As a result of the work described here a SPaRKy user will have a solid foundation for sentence plan construction. 3 Data We use the Wall Street Journal Penn Treebank corpus (Marcus et al., 1993), which is a corpus of text annotated for syntactic structure. We also use two additional annotations done on (parts of) that corpus: PropBank (Kingsbury and Palmer, 2003), which consists of annotations for predicateargument structure; and the RST-DT (Carlson et al., 2002), which consists of annotations for rhetorical structure. We had to process this data into a form suitable for feature extraction. First, we produced a flattened form of the syntactic annotations, in which 1SPaRKy also does some referring expression generation, in a single pass over each completed sentence plan. each word was labeled with its part-of-speech tag and the path to the root of the parse tree. Each word was also assigned indices in the sentence (so we could apply the PropBank annotations) and in the document (so we could apply the RST-DT annotations)2. Second, we attach to each word</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2002. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the SIGdial workshop on discourse and dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>J D Moore</author>
<author>M Paolucci</author>
</authors>
<title>Learning features that predict cue usage.</title>
<date>1997</date>
<booktitle>In Proceedings of the EACL.</booktitle>
<marker>Di Eugenio, Moore, Paolucci, 1997</marker>
<rawString>B. Di Eugenio, J. D. Moore, and M. Paolucci. 1997. Learning features that predict cue usage. In Proceedings of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>E Reiter</author>
</authors>
<title>SimpleNLG: A realisation engine for practical applications.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="29074" citStr="Gatt and Reiter, 2009" startWordPosition="4786" endWordPosition="4789">problem of data sparsity. 7 Revised SPaRKy One way to use these results would be to model the sentence planning task as a cascade of classifiers, but this method does not permit the system developer to add his or her own rules. So we continue to use SPaRKy, which is rule-based. We have made several changes to the Java version of SPaRKy to support application of our sentence plan construction rules. We modified the classes for storing and managing rules to read our XML rule format and process rule conditions and patterns. We stripped out the dependence on RealPro and added hooks for SimpleNLG (Gatt and Reiter, 2009). We modified the rule application algorithm so that users can choose to use a single rule set with patterns covering all three sentence planning tasks, or one rule set for each sentence planning task. Also, since there are now many rules, we give the user the option to specify which relations jSPaRKy should load rules for at each run. Information about the revised jSparky, including how to obtain it, is available at http://www.research.att.com/˜stent/sparky2.0/ or by contacting the first author. 8 Conclusions and Future Work In this paper we described how we extracted less domain-dependent se</context>
</contexts>
<marker>Gatt, Reiter, 2009</marker>
<rawString>A. Gatt and E. Reiter. 2009. SimpleNLG: A realisation engine for practical applications. In Proceedings of the European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grote</author>
<author>M Stede</author>
</authors>
<title>Discourse marker choice in sentence planning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="18502" citStr="Grote and Stede, 1998" startWordPosition="3018" endWordPosition="3021">incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization</context>
</contexts>
<marker>Grote, Stede, 1998</marker>
<rawString>B. Grote and M. Stede. 1998. Discourse marker choice in sentence planning. In Proceedings of the 9th International Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>E Maier</author>
</authors>
<title>Parsimonious or profligate: how many and which discourse structure relations? Available from http://handle.dtic.mil/100.2/ADA278715.</title>
<date>1992</date>
<contexts>
<context position="20883" citStr="Hovy and Maier, 1992" startWordPosition="3395" endWordPosition="3398">in the RST-DT. We exclude span and same-unit. Two others, elaboration-process-step and topiccomment, never occur with two or more child spans. Our first-level and all rules cover all of the remaining 53. The most frequently occurring relations are elaboration-additional, list, attribution, elaboration-object-attribute, contrast, circumstance and explanation-argumentative. By contrast, the current version of SPaRKy covers only 4 relations (justify, contrast, sequence, and infer)4. Mann and Thompson originally defined 24 relations (Mann and Thompson, 1987), while Hovy and Maier listed about 70 (Hovy and Maier, 1992). 6.2.2 Discourse Cue Coverage Our first-level rules cover 92 discourse cues, and our all rules cover 205 discourse cues. The most commonly occurring discourse cues in both cases are and, but, that, when, as, who and which. By contrast, the current version of SPaRKy covers only about 9 discourse cues. In his dissertation Marcu identified about 478 discourse cues. We used a modified version of Marcu’s cue list to extract discourse cues from our corpus, but some of Marcu’s discourse cues do not occur in the RST-DT. 4Curiously, only two of these relations (contrast and sequence) appear in the RST</context>
</contexts>
<marker>Hovy, Maier, 1992</marker>
<rawString>E. Hovy and E. Maier. 1992. Parsimonious or profligate: how many and which discourse structure relations? Available from http://handle.dtic.mil/100.2/ADA278715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
</authors>
<title>Supplementing entity coherence with local rhetorical relations for information ordering.</title>
<date>2007</date>
<journal>Journal of Logic, Language and Information,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="17555" citStr="Karamanis, 2007" startWordPosition="2863" endWordPosition="2864"> in discourse cues). 293 6 Rule Evaluation 6.1 On Evaluation There are two basic approaches to NLG, text-totext generation (in which a model learned from a text corpus is applied to produce new texts from text input) and data-to-text generation (in which non-text input is converted into text output). In text-to-text generation, there has been considerable work on sentence fusion and information ordering, which are partly sentence planning tasks. For evaluation, researchers typically compare automatically produced text to the original humanproduced text, which is assumed to be “correct” (e.g. (Karamanis, 2007; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005)). However, an evaluation that considers the only “correct” answer for a sentence planning task to be the answer in the original text is overly harsh. First, although we assume that all the possibilities in the human-produced text are “reasonable”, some may be awkward or incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twosta</context>
</contexts>
<marker>Karamanis, 2007</marker>
<rawString>N. Karamanis. 2007. Supplementing entity coherence with local rhetorical relations for information ordering. Journal of Logic, Language and Information, 16(4):445–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>PropBank: the next level of TreeBank.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Lexical Theories.</booktitle>
<contexts>
<context position="5563" citStr="Kingsbury and Palmer, 2003" startWordPosition="881" endWordPosition="884">istically according to the rule weights (with some randomness to permit variation). While the developer of a NLG system will always have to adapt the sentence planner to his or her domain, it is often hard to come up with sentence planning rules “from scratch”. As a result of the work described here a SPaRKy user will have a solid foundation for sentence plan construction. 3 Data We use the Wall Street Journal Penn Treebank corpus (Marcus et al., 1993), which is a corpus of text annotated for syntactic structure. We also use two additional annotations done on (parts of) that corpus: PropBank (Kingsbury and Palmer, 2003), which consists of annotations for predicateargument structure; and the RST-DT (Carlson et al., 2002), which consists of annotations for rhetorical structure. We had to process this data into a form suitable for feature extraction. First, we produced a flattened form of the syntactic annotations, in which 1SPaRKy also does some referring expression generation, in a single pass over each completed sentence plan. each word was labeled with its part-of-speech tag and the path to the root of the parse tree. Each word was also assigned indices in the sentence (so we could apply the PropBank annota</context>
</contexts>
<marker>Kingsbury, Palmer, 2003</marker>
<rawString>P. Kingsbury and M. Palmer. 2003. PropBank: the next level of TreeBank. In Proceedings of the Workshop on Treebanks and Lexical Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoi</author>
<author>O Rambow</author>
</authors>
<title>A fast and portable realizer for text generation systems.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP.</booktitle>
<marker>Lavoi, Rambow, 1997</marker>
<rawString>B. Lavoi and O. Rambow. 1997. A fast and portable realizer for text generation systems. In Proceedings of ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/RS-87-190,</tech>
<institution>Information Sciences Institute,</institution>
<location>Los Angeles, CA.</location>
<contexts>
<context position="20822" citStr="Mann and Thompson, 1987" startWordPosition="3384" endWordPosition="3387">.2.1 Relation Coverage There are 57 unique core relation labels in the RST-DT. We exclude span and same-unit. Two others, elaboration-process-step and topiccomment, never occur with two or more child spans. Our first-level and all rules cover all of the remaining 53. The most frequently occurring relations are elaboration-additional, list, attribution, elaboration-object-attribute, contrast, circumstance and explanation-argumentative. By contrast, the current version of SPaRKy covers only 4 relations (justify, contrast, sequence, and infer)4. Mann and Thompson originally defined 24 relations (Mann and Thompson, 1987), while Hovy and Maier listed about 70 (Hovy and Maier, 1992). 6.2.2 Discourse Cue Coverage Our first-level rules cover 92 discourse cues, and our all rules cover 205 discourse cues. The most commonly occurring discourse cues in both cases are and, but, that, when, as, who and which. By contrast, the current version of SPaRKy covers only about 9 discourse cues. In his dissertation Marcu identified about 478 discourse cues. We used a modified version of Marcu’s cue list to extract discourse cues from our corpus, but some of Marcu’s discourse cues do not occur in the RST-DT. 4Curiously, only two</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>W. Mann and S. Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report ISI/RS-87-190, Information Sciences Institute, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The rhetorical parsing, summarization, and generation of natural language texts.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="11863" citStr="Marcu, 1997" startWordPosition="1933" endWordPosition="1934"> of the same type). Discourse Cues We extract discourse cue information from basic spans and from the first basic span in complex spans. We identify discourse cue(s) appearing at the start of the span, inside the span, and at the end of the span. PropBank includes the argument label ARGM-DIS for discourse cues; however, we adopt a more expansive notion of discourse cue. We say that a discourse cue can be either: any sequence of words all labeled ARGMDIS and belonging to the same predicate, anywhere in the span; or any cue from a (slightly expanded version of) the set of cues studied by Marcu (Marcu, 1997), if it appears at the start of a span, at the end of a span, or immediately before or after a comma, and if its lowest containing phrase tag is one of {ADJP, ADVP, CONJP, FRAG, NPADV, PP, UCP, SBAR, WH} or its part of speech tag is one of {CC, WDT}3. Punctuation We extract punctuation (N/A or . or ? or ! or ; or : or ,) at the end of the span. 3We constructed these rules by extracting from the WSJ Penn Treebank all instances of the cues in Marcu’s list, and then examining instances where the word sequence was not actually a discourse cue. Some mistakes still occur in cue extraction. 292 4.2 P</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D. Marcu. 1997. The rhetorical parsing, summarization, and generation of natural language texts. Ph.D. thesis, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5392" citStr="Marcus et al., 1993" startWordPosition="854" endWordPosition="857">e. During sentence plan construction, SPaRKy walks over the input discourse plan, at each node finding all matching rules and applying one which it selects probabilistically according to the rule weights (with some randomness to permit variation). While the developer of a NLG system will always have to adapt the sentence planner to his or her domain, it is often hard to come up with sentence planning rules “from scratch”. As a result of the work described here a SPaRKy user will have a solid foundation for sentence plan construction. 3 Data We use the Wall Street Journal Penn Treebank corpus (Marcus et al., 1993), which is a corpus of text annotated for syntactic structure. We also use two additional annotations done on (parts of) that corpus: PropBank (Kingsbury and Palmer, 2003), which consists of annotations for predicateargument structure; and the RST-DT (Carlson et al., 2002), which consists of annotations for rhetorical structure. We had to process this data into a form suitable for feature extraction. First, we produced a flattened form of the syntactic annotations, in which 1SPaRKy also does some referring expression generation, in a single pass over each completed sentence plan. each word was</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>E Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>In Proceedings of the European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="17609" citStr="Marsi and Krahmer, 2005" startWordPosition="2869" endWordPosition="2872">1 On Evaluation There are two basic approaches to NLG, text-totext generation (in which a model learned from a text corpus is applied to produce new texts from text input) and data-to-text generation (in which non-text input is converted into text output). In text-to-text generation, there has been considerable work on sentence fusion and information ordering, which are partly sentence planning tasks. For evaluation, researchers typically compare automatically produced text to the original humanproduced text, which is assumed to be “correct” (e.g. (Karamanis, 2007; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005)). However, an evaluation that considers the only “correct” answer for a sentence planning task to be the answer in the original text is overly harsh. First, although we assume that all the possibilities in the human-produced text are “reasonable”, some may be awkward or incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on senten</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>E. Marsi and E. Krahmer. 2005. Explorations in sentence fusion. In Proceedings of the European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moser</author>
<author>J D Moore</author>
</authors>
<title>Using discourse analysis and automatic text generation to study discourse cue usage.</title>
<date>1995</date>
<booktitle>In Proceedings of the AAAI 1995 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.</booktitle>
<contexts>
<context position="18525" citStr="Moser and Moore, 1995" startWordPosition="3022" endWordPosition="3025">r domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization was already complete, </context>
</contexts>
<marker>Moser, Moore, 1995</marker>
<rawString>M. Moser and J. D. Moore. 1995. Using discourse analysis and automatic text generation to study discourse cue usage. In Proceedings of the AAAI 1995 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nakatsu</author>
</authors>
<title>Learning contrastive connective in sentence realization ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGdial</booktitle>
<contexts>
<context position="18540" citStr="Nakatsu, 2008" startWordPosition="3026" endWordPosition="3027">less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization was already complete, so they used le</context>
</contexts>
<marker>Nakatsu, 2008</marker>
<rawString>C. Nakatsu. 2008. Learning contrastive connective in sentence realization ranking. In Proceedings of SIGdial 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>S Bangalore</author>
<author>M A Walker</author>
</authors>
<title>Natural language generation in dialog systems.</title>
<date>2001</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="1156" citStr="Rambow et al., 2001" startWordPosition="162" endWordPosition="165"> can be difficult. In this paper, we automatically extract sentence plan construction rules from the RST-DT corpus. In our rules, we use only domainindependent features that are available to a sentence planner at runtime. We evaluate these rules, and outline ways in which they can be used for sentence planning. We have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with d</context>
</contexts>
<marker>Rambow, Bangalore, Walker, 2001</marker>
<rawString>O. Rambow, S. Bangalore, and M. A. Walker. 2001. Natural language generation in dialog systems. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="1134" citStr="Reiter and Dale, 2000" startWordPosition="158" endWordPosition="161"> rules for a new domain can be difficult. In this paper, we automatically extract sentence plan construction rules from the RST-DT corpus. In our rules, we use only domainindependent features that are available to a sentence planner at runtime. We evaluate these rules, and outline ways in which they can be used for sentence planning. We have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few co</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and R. Dale. 2000. Building natural language generation systems. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>R Prasad</author>
<author>M A Walker</author>
</authors>
<title>Trainable sentence planning for complex information presentations in spoken dialog systems.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1428" citStr="Stent et al., 2004" startWordPosition="203" endWordPosition="206">hey can be used for sentence planning. We have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with discourse plans, and it is hard to automatically label a corpus for discourse structure. It is also hard to hand-write sentence planning rules starting from a “blank slate”, as it were. In this paper, we outline a method for extracting sentence plan construction rules from</context>
<context position="3236" citStr="Stent et al., 2004" startWordPosition="506" endWordPosition="509">er. We discuss the implications of these experiments for sentence planning. The rest of this paper is structured as follows: In Section 2 we describe the sentence planning process using SPaRKy as an example. In Sections 3 through 5 we describe how we obtain sentence plan construction rules. In Section 6, we evaluate alternative rule sets. In Section 7, we describe our modifications to the SPaRKy sentence planner to use these rules. In Section 8 we conclude and present future work. 2 Sentence Planning in SPaRKy The only publicly available sentence planner for data-to-text generation is SPaRKy (Stent et al., 2004). SPaRKy takes as input a discourse plan (a tree with rhetorical relations on the internal nodes and a proposition representing a text span on each leaf), and outputs one or more sentence plans Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 290–297, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 290 (each a tree with discourse cues and/or punctuation on the internal nodes). SPaRKy is a two-stage sentence planner. First, possible sentence plans are constructed through a sequen</context>
<context position="18708" citStr="Stent et al., 2004" startWordPosition="3052" endWordPosition="3055">oduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization was already complete, so they used lexical features. Their sentence planner is not publicly available. In the following sections, we evaluate the information in our sentence plan construction rules in term</context>
</contexts>
<marker>Stent, Prasad, Walker, 2004</marker>
<rawString>A. Stent, R. Prasad, and M. A. Walker. 2004. Trainable sentence planning for complex information presentations in spoken dialog systems. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
</authors>
<title>Discourse markers as signals (or not) of rhetorical relations.</title>
<date>2006</date>
<journal>Journal of Pragmatics,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="18556" citStr="Taboada, 2006" startWordPosition="3028" endWordPosition="3029">nes in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization was already complete, so they used lexical features. </context>
</contexts>
<marker>Taboada, 2006</marker>
<rawString>M. Taboada. 2006. Discourse markers as signals (or not) of rhetorical relations. Journal of Pragmatics, 38(4):567–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>A Stent</author>
<author>F Mairesse</author>
<author>R Prasad</author>
</authors>
<title>Individual and domain adaptation in sentence planning for dialogue.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>30--413</pages>
<contexts>
<context position="1449" citStr="Walker et al., 2007" startWordPosition="207" endWordPosition="210">sentence planning. We have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with discourse plans, and it is hard to automatically label a corpus for discourse structure. It is also hard to hand-write sentence planning rules starting from a “blank slate”, as it were. In this paper, we outline a method for extracting sentence plan construction rules from the only publicly av</context>
<context position="4105" citStr="Walker et al., 2007" startWordPosition="642" endWordPosition="645"> the Special Interest Group in Discourse and Dialogue, pages 290–297, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 290 (each a tree with discourse cues and/or punctuation on the internal nodes). SPaRKy is a two-stage sentence planner. First, possible sentence plans are constructed through a sequence of decisions made using only local information about single nodes in the discourse plan. Second, the possible sentence plans are ranked using a user- or domainspecific sentence plan ranker that evaluates the global quality of each sentence plan (Walker et al., 2007). Sentence plan construction in SPaRKy involves three tasks: span ordering, sentence aggregation (deciding whether to realize a pair of propositions as a single clause, a single sentence, or two sentences), and discourse cue selection1. SPaRKy uses a single set of hand-written weighted rules to perform these tasks. In the current distributed version of SPaRKy, there are 20 rules covering 9 discourse cues (and, because, but, however, on the other hand, since, while, with, and the default, period). Each rule operates on the children of one rhetorical relation, and may impose an ordering, insert </context>
<context position="7146" citStr="Walker et al. (2007)" startWordPosition="1138" endWordPosition="1141"> the types of each child (“Nucleus” or “Satellite”), and the start and end word indices for each child. Finally, we extract from the wordlevel annotations the marked-up words for each text span in each rhetorical relation. 4 Features Features are individual rule conditions. In the standard NLG pipeline, no information about the realized text is available to the sentence planner. However, existing sentence planners use lexical and word sequence information to improve performance for a particular domain. Williams and Reiter (2003) appear to do surface realization before sentence planning, while Walker et al. (2007) perform surface realization between sentence plan construction and sentence plan ranking. We are concerned with sentence plan construction only; also, we want to produce sentence plan construction rules that are as domain-independent as possible. So we use no features that rely on having realized text. However, we assume that the input propositions have been fairly well fleshed-out, so that one has information about predicate-argument structure, tense, and the information status of entities to be realized. A relation has a label as well as one or more child text spans. The features we extract</context>
<context position="18730" citStr="Walker et al., 2007" startWordPosition="3056" endWordPosition="3059">ial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization was already complete, so they used lexical features. Their sentence planner is not publicly available. In the following sections, we evaluate the information in our sentence plan construction rules in terms of coverage and disc</context>
</contexts>
<marker>Walker, Stent, Mairesse, Prasad, 2007</marker>
<rawString>M. A. Walker, A. Stent, F. Mairesse, and R. Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research, 30:413–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>A corpus analysis of discourse relations for natural language generation.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<contexts>
<context position="1477" citStr="Williams and Reiter, 2003" startWordPosition="211" endWordPosition="215"> have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with discourse plans, and it is hard to automatically label a corpus for discourse structure. It is also hard to hand-write sentence planning rules starting from a “blank slate”, as it were. In this paper, we outline a method for extracting sentence plan construction rules from the only publicly available corpus of discourse </context>
<context position="7060" citStr="Williams and Reiter (2003)" startWordPosition="1124" endWordPosition="1127">tract relation information from the RST-DT. For each relation, we extract the relation name, the types of each child (“Nucleus” or “Satellite”), and the start and end word indices for each child. Finally, we extract from the wordlevel annotations the marked-up words for each text span in each rhetorical relation. 4 Features Features are individual rule conditions. In the standard NLG pipeline, no information about the realized text is available to the sentence planner. However, existing sentence planners use lexical and word sequence information to improve performance for a particular domain. Williams and Reiter (2003) appear to do surface realization before sentence planning, while Walker et al. (2007) perform surface realization between sentence plan construction and sentence plan ranking. We are concerned with sentence plan construction only; also, we want to produce sentence plan construction rules that are as domain-independent as possible. So we use no features that rely on having realized text. However, we assume that the input propositions have been fairly well fleshed-out, so that one has information about predicate-argument structure, tense, and the information status of entities to be realized. A</context>
<context position="9741" citStr="Williams and Reiter, 2003" startWordPosition="1567" endWordPosition="1570">information, discourse cue information, and span-final punctuation. Identifier We need a way to refer to the child spans in the rules. For relations having only one child span of each type (Satellite or Nucleus), we order the spans by type. Otherwise, we order the spans alphabetically by span text. The span identifier for each child span is the index of the span in the resulting list. Text We extract the text of the span, and the indices of its first and last words in the Penn Treebank. We only use this information during data extraction. However, in a system like that of Williams and Reiter (Williams and Reiter, 2003), where sentence planning is done after or with surface realization, these features could be used. They could also be used to train a sentence plan ranker for SPaRKy specific to the news domain. Length We use the number of base NPs in the span (as we cannot rely on having the complete realization during sentence planning). Verb We extract verb type, which can be N/A (there is no labeled predicate for the span), stat (the span’s main verb is a form of “to be”), a single PropBank predicate (e.g. create.01), or mixed (the span contains more than one predicate). We then abstract to get the verb ty</context>
</contexts>
<marker>Williams, Reiter, 2003</marker>
<rawString>S. Williams and E. Reiter. 2003. A corpus analysis of discourse relations for natural language generation. In Proceedings of Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>F Eibe</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="22554" citStr="Witten and Eibe, 2005" startWordPosition="3678" endWordPosition="3681">o examine all 5810 rules. However, she or he may wish to look at the patterns for a particular relation. In our use of SPaRKy, for example, we have extended the patterns for the sequence relation by hand to cover temporal sequences of up to seven steps. 6.3 Discriminative Power In this evaluation, we train decision tree classifiers for each sentence plan construction task. We experiment with both the first-level and all data sets and with both the reduced and full feature sets. For each experiment we perform ten-fold crossvalidation using the J48 decision tree implementation provided in Weka (Witten and Eibe, 2005) with its default parameters. We also report performance for a model that selects a pattern conditioning only on the relation. Finally, we report performance of a baseline which always selects the most frequent pattern. We evaluate using 1-best classification accuracy, by comparing with the choice made in the Penn Treebank for that task. We test for significant differences between methods using Cochran’s Q, followed by post-hoc McNemar tests if significant differences existed. We also report the features with information gain greater than 0.1. 6.3.1 Span Ordering We have one input feature vect</context>
</contexts>
<marker>Witten, Eibe, 2005</marker>
<rawString>I. Witten and F. Eibe. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>