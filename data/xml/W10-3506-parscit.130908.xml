<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998548">
Measuring Conceptual Similarity by Spreading Activation over
Wikipedia’s Hyperlink Structure
</title>
<author confidence="0.998034">
Stephan Gouws, G-J van Rooyen, and Herman A. Engelbrecht
</author>
<affiliation confidence="0.995399">
Stellenbosch University
</affiliation>
<email confidence="0.992307">
{stephan,gvrooyen,hebrecht}@ml.sun.ac.za
</email>
<sectionHeader confidence="0.993746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999654642857143">
Keyword-matching systems based on
simple models of semantic relatedness
are inadequate at modelling the ambigu-
ities in natural language text, and cannot
reliably address the increasingly com-
plex information needs of users. In
this paper we propose novel methods
for computing semantic relatedness by
spreading activation energy over the hy-
perlink structure of Wikipedia. We
demonstrate that our techniques can
approach state-of-the-art performance,
while requiring only a fraction of the
background data.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966981132075">
The volume of information available to users
on the World Wide Web is growing at an
exponential rate (Lyman and Varian, 2003).
Current keyword-matching information retrieval
(IR) systems suffer from several limitations,
most notably an inability to accurately model
the ambiguities in natural language, such as syn-
onymy (different words having the same mean-
ing) and polysemy (one word having multiple
different meanings), which is largely governed
by the context in which a word appears (Metzler
and Croft, 2006).
In recent years, much research attention has
therefore been given to semantic techniques of
information retrieval. Such systems allow for
sophisticated semantic search, however, require
the use of a more difficult-to-understand query-
syntax (Tran et al., 2008). Furthermore, these
methods require specially encoded (and thus
costly) ontologies to describe the particular do-
main knowledge in which the system operates,
and the specific interrelations of concepts within
that domain.
In this paper, we focus on the problem of
computationally estimating similarity or related-
ness between two natural-language documents.
A novel technique is proposed for comput-
ing semantic similarity by spreading activation
over the hyperlink structure of Wikipedia, the
largest free online encyclopaedia. New mea-
sures for computing similarity between individ-
ual concepts (inter-concept similarity, such as
“France” and “Great Britain”), as well as be-
tween documents (inter-document similarity)
are proposed and tested. It will be demonstrated
that the proposed techniques can achieve compa-
rable inter-concept and inter-document similar-
ity accuracy on similar datasets as compared to
the current state of the art Wikipedia Link-based
Measure (WLM) (Witten and Milne, 2008) and
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007) methods respectively.
Our methods outperform WLM in computing
inter-concept similarity, and match ESA for
inter-document similarity. Furthermore, we use
the same background data as for WLM, which is
less than 10% of the data required for ESA.
In the following sections we introduce work
related to our work and an overview of our
approach and the problems that have to be
solved. We then discuss our method in detail and
present several experiments to test and compare
it against other state-of-the-art methods.
</bodyText>
<page confidence="0.993907">
46
</page>
<note confidence="0.8186465">
Beijing, August 2010
Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 46–54,
</note>
<sectionHeader confidence="0.982982" genericHeader="introduction">
2 Related Work and Overview
</sectionHeader>
<bodyText confidence="0.999865123076923">
Although Spreading Activation (SA) is foremost
a cognitive theory modelling semantic mem-
ory (Collins and Loftus, 1975), it has been ap-
plied computationally to IR with various lev-
els of success (Preece, 1982), with the biggest
hurdle in this regard the cost of creating an as-
sociative network or knowledge base with ad-
equate conceptual coverage (Crestani, 1997).
Recent knowledge-based methods for comput-
ing semantic similarity between texts based on
Wikipedia, such as Wikipedia Link-based Mea-
sure (WLM) (Witten and Milne, 2008) and Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007), have been found to out-
perform earlier WordNet-based methods (Bu-
danitsky and Hirst, 2001), arguably due to
Wikipedia’s larger conceptual coverage.
WLM treats the anchor text in Wikipedia arti-
cles as links to other articles (all links are treated
equally), and compare concepts based on how
much overlap exists in the out-links of the arti-
cles representing them. ESA discards the link
structure and uses only the text in articles to de-
rive an explicit concept space in which each di-
mension represents one article/concept. Text is
categorised as vectors in this concept space and
similarity is computed as the cosine similarity of
their ESA vectors. The most similar work to ours
is Yeh (2009) in which the authors derive a graph
structure from the inter-article links in Wikipedia
pages, and then perform random walks over the
graph to compute relatedness.
In Wikipedia, users create links between arti-
cles which are seen to be related to some degree.
Since links relate one article to its neighbours,
and by extension to their neighbours, we ex-
tract and process this hyperlink structure (using
SA) as an Associative Network (AN) (Berger
et al., 2004) of concepts and links relating them
to one another. The SA algorithm can briefly
be described as an iterative process of propagat-
ing real-valued energy from one or more source
nodes, via weighted links over an associative net-
work (each such a propagation is called a pulse).
The algorithm consists of two steps: First, one
or more pulses are triggered, and second, ter-
mination checks determine whether the process
should continue or halt. This process of acti-
vating more and more nodes in the network and
checking for termination conditions are repeated
pulse after pulse, until all termination conditions
are met, which results in a final activation state
for the network. These final node activations
are then translated into a score of relatedness be-
tween the initial nodes.
Our work presents a computational imple-
mentation of SA over the Wikipedia graph.
We therefore overcome the cost of produc-
ing a knowledge base of adequate coverage by
utilising the collaboratively-created knowledge
source Wikipedia. However, additional strate-
gies are required for translating the hyperlink
structure of Wikipedia into a suitable associative
network format, and for this new techniques are
proposed and tested.
</bodyText>
<sectionHeader confidence="0.853288" genericHeader="method">
3 Extracting the Hyperlink Graph
Structure
</sectionHeader>
<bodyText confidence="0.99983">
One article in Wikipedia covers one specific
topic (concept) in detail. Hyperlinks link a page
A to a page B, and are thus directed. We
can model Wikipedia’s hyperlink structure us-
ing standard graph theory as a directed graph G,
consisting of a set of vertices V, and a set of
edges E. Each edge eij ∈ E connects two ver-
tices vi, vj ∈ V. For consistency, we use the
term node to refer to a vertex (Wikipedia article)
in the graph, and link to refer to an edge (hyper-
link) between such nodes.
In this model, each Wikipedia article is seen
to represent a single concept, and the hyperlink
structure relates these concepts to one another. In
order to compute relatedness between two con-
cepts vi and vj, we use spreading activation and
rely on the fundamental principle of an associa-
tive network, namely that it connects nodes that
are associated with one another via real-valued
links denoting how strongly the objects are re-
lated. Since Wikipedia was not created as an as-
sociative network, but primarily as an online en-
cyclopaedia, none of these weights exist, and we
will have to deduce these (see Fan-out constraint
in Section 4).
</bodyText>
<page confidence="0.998734">
47
</page>
<bodyText confidence="0.9999314">
Links into pages are used, since this leads to
better results (Witten and Milne, 2008). The
Wikipedia graph structure is represented in an
adjacency list structure, i.e. for each node vi we
store its list of neighbour nodes in a dictionary
using vi’s id as key. This approach is preferred
over an adjacency matrix structure, since most
articles are linked to by only 34 articles on aver-
age, which would lead to a very sparse adjacency
matrix structure.
</bodyText>
<subsectionHeader confidence="0.6924585">
4 Adapting Spreading Activation for
Wikipedia’s Hyperlink Structure
</subsectionHeader>
<bodyText confidence="0.951500833333333">
Each pulse in the Spreading Activation (SA) pro-
cess consists of three stages: 1) pre-adjustment,
2) spreading, and 3) post-adjustment (Crestani,
1997). During pre- and post-adjustment, some
form of activation decay is optionally applied to
the active nodes. This serves both to avoid re-
tention of activation from previous pulses, and,
from a connectionist point of view, models ‘loss
of interest’ when nodes are not continually acti-
vated.
Let ai,in denote the total energy input (acti-
vation) for node vi, and N(vi) the set of vi’s
neighbour nodes with incoming links to vi. Also,
let aj,out denote the output activation of a node
vj connected to node vi, and let wij denote the
weight of connection between node vi and vj.
For a node vi, we can then describe the pure
model of spreading activation as follows:
</bodyText>
<equation confidence="0.995617">
�ai,in = aj,outwij. (1)
vj∈N(vi)
</equation>
<bodyText confidence="0.983765818181818">
This pure model of SA has several significant
problems, the most notable being that activation
can saturate the entire network unless certain
constraints are imposed, namely limiting how
far activation can spread from the initially acti-
vated nodes (distance constraint), and limiting
the effect of very highly-connected nodes (fan-
out constraint) (Crestani, 1997). In the following
three sections we discuss how these constraints
were implemented in our model for SA.
Distance constraint
For every pulse in the spreading process, a
node’s activation value is multiplied by a global
network decay parameter 0 &lt; d &lt; 1. We
therefore substitute wij in Equation 1 for wijd.
This decays activation exponentially in the path
length. For a path length of one, activation is de-
cayed by d, for a path length of two, activation
is decays by dd = d2, etc. This penalises activa-
tion transfer over longer paths. We also include
a maximum path length parameter Lp,max which
limits how far activation can spread.
</bodyText>
<subsectionHeader confidence="0.70707">
Fan-out constraint
</subsectionHeader>
<bodyText confidence="0.99776296969697">
As noted above, in an associative network, links
have associated real-valued weights to denote the
strength of association between the two nodes
they connect (i.e. wij in Equation 1). These
weights have to be estimated for the Wikipedia
hyperlink graph, and for this purpose we propose
the use of three weighting schemes:
In pure Energy Distribution (ED), a node
vi’s weight w is made inversely proportional to
its in-degree (number of neighbours N(vi) ≥ 1
with incoming links to vi1). Thus ED(vi, vj) =
wij = 1
|N(vi)|. This reduces the effect of very
connected nodes on the spreading process (con-
straint 2 above).
For instance, we consider a path connecting
two nodes via a general article such as USA (con-
nected to 322,000 articles) not nearly as indica-
tive of a semantic relationship, as a path con-
necting them via a very specific concept, such
as Hair Pin (only connected to 20 articles).
Inverse Link-Frequency (ILF) is inspired by
the term-frequency inverse document-frequency
(tf-idf) heuristic (Salton and McGill, 1983) in
which a term’s weight is reduced as it is con-
tained in more documents in the corpus. It is
based on the idea that the more a term appears
in documents across the corpus, the less it can
discriminate any one of those documents.
We define a node vi’s link-frequency as the
number of nodes that vi is connected to |N(vi)|
divided by the number of possible nodes it could
be connected to in the entire Wikipedia graph
</bodyText>
<footnote confidence="0.463065">
1All orphan nodes are removed from the AN.
</footnote>
<page confidence="0.994831">
48
</page>
<bodyText confidence="0.996041">
|G|, and therefore give the log-smoothed inverse
link-frequency of node vi as:
</bodyText>
<equation confidence="0.763125">
ILF(vi) A log \ |N(I i)  |/ &gt; 0 (2)
</equation>
<bodyText confidence="0.988365">
As noted above for pure energy distribution, we
consider less connected nodes as more specific.
If one node connects to another via a very spe-
cific node with a low in-degree, |G|
</bodyText>
<subsectionHeader confidence="0.804831">
|N(vi) |is very
</subsectionHeader>
<bodyText confidence="0.9568732">
large and ILF(vi) &gt; 1, thus boosting that spe-
cific link’s weight. This has the effect of ‘boost-
ing’ paths (increasing their contribution) which
contain nodes that are less connected, and there-
fore more meaningful in our model.
To evaluate the effect of this boosting ef-
fect described above, we also define a third
normalised weighting scheme called the Nor-
malised Inverse Link-Frequency (NILF), 0 &lt;
NILF(vi) &lt; 1:
</bodyText>
<equation confidence="0.9986405">
NILF(vi) o ILF(vi) (3)
log |G|
</equation>
<bodyText confidence="0.999382">
ILF reaches a maximum of log |G |when
|N(vi) |= 1 (see Equation 2). We therefore di-
vide by log |G |to normalise its range to [0,1].
</bodyText>
<subsectionHeader confidence="0.502067">
Threshold constraint
</subsectionHeader>
<bodyText confidence="0.9999502">
Finally, the above-mentioned constraints are en-
forced through the use of a threshold parameter
0 &lt; T &lt; 1. Activation transfer to a next node
ceases when a node’s activation value drops be-
low a certain threshold T.
</bodyText>
<sectionHeader confidence="0.99391" genericHeader="method">
5 Strategies for Interpreting
Activations
</sectionHeader>
<bodyText confidence="0.9999629375">
After spreading has ceased, we are left with a
vector of nodes and their respective values of
activation (an activation vector). We wish to
translate this activation vector into a score re-
sembling strength of association or relatedness
between the two initial nodes.
We approach this problem using two differ-
ent approaches, the Target Activation Approach
(TAA) and the Agglomerative Approach (AA).
These approaches are based on two distinct hy-
potheses, namely: Relatedness between two
nodes can be measured as either 1) the ratio of
initial energy that reaches the target node, or 2)
the amount of overlap between their individual
activation vectors by spreading from both nodes
individually.
</bodyText>
<subsectionHeader confidence="0.842642">
Target Activation Approach (TAA)
</subsectionHeader>
<bodyText confidence="0.999808">
To measure the relatedness between vi and vj,
we set ai to some initial value Kinit (usually 1.0),
and all node activations including aj = 0. Af-
ter the SA process has terminated, vj is activated
with some aj,in. Relatedness is computed as the
</bodyText>
<equation confidence="0.6010375">
ratio sim — ° i
TAA(v v) ° z� ,y — Kinit.
</equation>
<sectionHeader confidence="0.51792" genericHeader="method">
Agglomerative Approach (AA)
</sectionHeader>
<bodyText confidence="0.999578818181818">
The second approach is called the Agglomera-
tive Approach since we agglomerate all activa-
tions into one score resembling relatedness. Af-
ter spreading has terminated, relatedness is com-
puted as the amount of overlap between the indi-
vidual nodes’ activation vectors, using either the
cosine similarity (AA-cos), or an adapted ver-
sion of the information theory based WLM (Wit-
ten and Milne, 2008) measure.
Assume the same set of initial nodes vi and
vj. Let Ak be the N-dimensional vector of real-
valued activation values obtained by spreading
over the N nodes in the graph from node vk
(called an activation vector). We use akx to de-
note the element at position x in Ak. Further-
more, let Vk = {vk1, ..., vkMI denote the set of
M nodes activated by spreading from vk, i.e. the
set of identifiers of nodes with non-zero activa-
tions in Ak after spreading has terminated (and
therefore M &lt; N).
We then define the cosine Agglomerative Ap-
proach (henceforth called AA-cos) as
</bodyText>
<equation confidence="0.9473665">
(4)
||Ai |Aj||
</equation>
<bodyText confidence="0.839639714285714">
For our adaptation of the Wikipedia Link-based
Measure (WLM) approach to spreading activa-
tion, we define the WLM Agglomerative Ap-
proach (henceforth called AA-wlm2) as
2AA-wlm is our adaptation of WLM (Witten and Milne,
2008) for SA, not to be confused with their method, which
we simply call WLM.
</bodyText>
<figure confidence="0.2888055">
simAA,cos(Ai, Aj)
A Ai · Aj
</figure>
<page confidence="0.793295">
49
</page>
<equation confidence="0.976457333333333">
simAA,wlm(Vi, Vj)
A log(max(JViJ,JVjJ))−log(JVinVjJ) (5)
log(JGJ)−log(min(JViJ,JVjJ))
</equation>
<bodyText confidence="0.9956255">
with |G |representing the number of nodes in the
entire Wikipedia hyperlink graph. Note that the
AA-wlm method does not take activations into
account, while the AA-cos method does.
</bodyText>
<sectionHeader confidence="0.952899" genericHeader="method">
6 Spreading Activation Algorithm
</sectionHeader>
<bodyText confidence="0.999959421052632">
Both the TAA and AA approaches described
above rely on a function to spread activation
from one node to all its neighbours, and itera-
tively to all their neighbours, subject to the con-
straints listed. TAA stops at this point and com-
putes relatedness as the ratio of energy received
to energy sent between the target and source
node respectively. However, AA repeats the pro-
cess from the target node and computes related-
ness as some function (cosine or information the-
ory based) of the two activation vectors, as given
by Equation 4 and Equation 5.
We therefore define SPREAD UNIDIR() as
shown in Algorithm 1. Prior to spreading from
some node vi, its activation value ai is set to
some initial activation value Kinit (usually 1.0).
The activation vector A is a dynamic node-
value-pair list, updated in-place. P is a dynamic
list of nodes in the path to vi to avoid cycles.
</bodyText>
<sectionHeader confidence="0.990528" genericHeader="method">
7 Parameter Optimisation:
Inter-concept Similarity
</sectionHeader>
<bodyText confidence="0.999987375">
The model for SA as introduced in this paper re-
lies on several important parameters, namely the
spreading strategy (TAA, AA-cos, or AA-wlm),
weighting scheme (pure ED, ILF, and NILF),
maximum path length Lp,max, network decay d,
and threshold T. These parameters have a large
influence on the accuracy of the proposed tech-
nique, and therefore need to be optimised.
</bodyText>
<subsectionHeader confidence="0.985109">
Experimental Method
</subsectionHeader>
<bodyText confidence="0.998284111111111">
In order to compare our method with results re-
ported by Gabrilovich and Markovitch (2007)
and Witten and Milne (2008), we followed
the same approach by randomly selecting
Algorithm 1 Pseudo code to spread activation
depth-first from node vi up to level Lp,max, us-
ing global decay d, and threshold T, given an
adjacency list graph structure G and a weighting
scheme W such that 0 &lt; wig E W &lt; 1.
</bodyText>
<equation confidence="0.696986944444445">
Require: G, Lp,—ax, d, T
function SPREAD UNIDIR(vi, A, P)
if (vi, ai) E/ A or ai &lt; T then &gt; Threshold
return
end if
Add vi to P &gt; To avoid cycles
for vj E N(vi) do &gt; Process neighbours
if (vj, aj) E/ A then
aj = 0
end if
if vj E/ P and JPJ &lt; Lp,—ax then
a∗j =aj+ai*wij*d
Replace (vj, aj) E A with (vj, a∗j )
SPREAD UNIDIR(vj, A, P)
end if
end for
return
end function
</equation>
<bodyText confidence="0.999638346153846">
50 word-pairs from the WordSimilarity-353
dataset (Gabrilovich, 2002) and correlating
our method’s scores with the human-assigned
scores. To reduce the possibility of overestimat-
ing the performance of our technique on a sam-
ple set that happens to be favourable to our tech-
nique, we furthermore implemented a technique
of repeated holdout (Witten and Frank, 2005):
Given a sample test set of N pairs of words
with human-assigned ratings of relatedness, ran-
domly divide this set into k parts of roughly
equal size3. Hold out one part of the data and
iteratively evaluate the performance of the algo-
rithm on the remaining k−1 parts until all k parts
have been held out once. Finally, average the al-
gorithm’s performance over all k runs into one
score resembling the performance for that set of
parameters.
Since there are five parameters (spreading
strategy, weighting scheme, path length, network
decay, and threshold), a grid search was imple-
mented by holding three of the five parameters
constant, and evaluating combinations of decay
and threshold by stepping over the possible pa-
rameter space using some step size. A coarse-
grained grid search was first conducted with step
</bodyText>
<footnote confidence="0.824711">
3k was chosen as 5.
</footnote>
<page confidence="0.99774">
50
</page>
<tableCaption confidence="0.734748">
Table 1: Spreading results by spreading
strategy (TAA=Target Activation Approach,
</tableCaption>
<table confidence="0.570274625">
AA=Agglomerative Approach, Lp,max = max-
imum path length used, ED=energy distri-
bution only, ILF=Inverse Link Frequency,
NILF=normalised ILF.) Best results in bold.
Strategy Pmax Parameters
TAA 0.56 ED, Lp,max=3, d=0.6, T=0.001
AA-wlm 0.60 NILF, Lp,max=3, d=0.1, T=10−s
AA-cos 0.70 ILF, Lp,max=3, d=0.5, T=0.1
</table>
<bodyText confidence="0.993504676470588">
size of 0.1 over d and a logarithmic scale over
T, thus T = {0, 0.1, 0.01, 0.001, ...,10−9}. The
best values for d and T were then chosen to con-
duct a finer-grained grid search.
Influence of the different Parameters
The spreading strategy determines how activa-
tions resulting from the spreading process are
converted into scores of relatedness or similar-
ity between two nodes. Table 1 summarises the
best results obtained for each of the three strate-
gies, with the specific set of parameters that were
used in each run.
Results are better using the AA (pmax =
0.70 for AA-cos) than using the TAA (pmax =
0.56). Secondly, the AA-cos spreading strat-
egy significantly outperforms the AA-wlm strat-
egy over this sample set (pmax,wlm = 0.60
vs pmax,cos = 0.70). These results compare
favourably to similar inter-concept results re-
ported for WLM (Witten and Milne, 2008) (p =
0.69) and ESA (Gabrilovich and Markovitch,
2007) (p = 0.75).
Maximum path length Lp,max is related to
how far one node can spread its activation in the
network. We extend the first-order link model
used by WLM, by approaching the link structure
as an associative network and by using spreading
activation.
To evaluate if this is a useful approach, tests
were conducted by using maximum path lengths
of one, two, and three. Table 2 summarises
the results for this experiment. Increasing path
length from one to two hops increases per-
formance from pmax = 0.47 to pmax =
</bodyText>
<tableCaption confidence="0.854407">
Table 2: Spreading results by maximum path
</tableCaption>
<table confidence="0.9003244">
length Lp,max. Best results in bold.
Lp,max pmax Parameters
1 0.47 TAA, ED/ILF/NILF
2 0.66 AA-cos, ILF, d=0.4, T=0.1
3 0.70 AA-cos, ILF, d=0.5, T=0.1
</table>
<tableCaption confidence="0.9910215">
Table 3: Spreading results by weighting scheme
w. Best results in bold.
</tableCaption>
<table confidence="0.99918175">
w Pmax Parameters
NILF 0.63 AA-cos, Lp,max = 3, d=0.9, T=0.01
ED 0.64 AA-cos, Lp,max = 3, d=0.9, T=0.01
ILF 0.70 AA-cos, Lp,max = 3, d=0.5, T=0.1
</table>
<bodyText confidence="0.9974323">
0.66. Moreover, increasing Lp,max from two to
three hops furthermore increases performance to
pmax = 0.70.
In an associative network, each link has a
real-valued weight denoting the strength of as-
sociation between the two nodes it connects.
The derived Wikipedia hyperlink graph lacks
these weights. We therefore proposed three new
weighting schemes (pure ED, ILF, and NILF) to
estimate these weights.
Table 3 summarises the best performances us-
ing the different weighting schemes. ILF outper-
forms both ED and NILF. Furthermore, both ED
and NILF perform best using higher decay val-
ues (both 0.9) and lower threshold values (both
0.01), compared to ILF (0.5 and 0.1 respectively
for d and T). We attribute this observation to
the boosting effect of the ILF weighting scheme
for less connected nodes, and offer the following
explanation:
Recall from the section on ILF that in our
model, strongly connected nodes are viewed as
more general, and nodes with low in-degrees
are seen as very specific concepts. We argued
that a path connecting two concepts via these
more specific concepts are more indicative of
a stronger semantic relationship than through
some very general concept. In the ILF weighting
scheme, paths containing these less connected
nodes are automatically boosted to be more im-
</bodyText>
<page confidence="0.996568">
51
</page>
<bodyText confidence="0.9999906">
portant. Therefore, by not boosting less mean-
ingful paths, a lower decay and higher threshold
effectively limits the amount of non-important
nodes that are activated, since their activations
are more quickly decayed, whilst at the same
time requiring a higher threshold to continue
spreading. Boosting more important nodes can
therefore lead to activation vectors which capture
the semantic context of the source nodes more
accurately, leading to higher performance.
</bodyText>
<sectionHeader confidence="0.73391" genericHeader="method">
8 Computing document similarity
</sectionHeader>
<bodyText confidence="0.999979818181818">
To compute document similarity, we first extract
key representative Wikipedia concepts from a
document to produce document concept vec-
tors4. This process is known as wikifica-
tion (Csomai and Mihalcea, 2008), and we used
an implementation of Milne and Witten (2008).
This produces document concept vectors of the
form Vi = {(id1, w1), (id2, w2), ...} with idi
some Wikipedia article identifier and wi a weight
denoting how strongly the concept relates to the
current document. We next present two algo-
rithms, MAXSIM and WIKISPREAD, for com-
puting document similarity, and test these over
the Lee (2005) document similarity dataset, a
set of 50 documents between 51 and 126 words
each, with the averaged gold standard similarity
ratings produced by 83 test subjects (see (Lee et
al., 2005)).
The first metric we propose is called
MAXSIM (see Algorithm 2) and is based on
the idea of measuring document similarity by
pairing up each Wikipedia concept in one docu-
ment’s concept vector with its most similar con-
cept in the other document. We average those
similarities to produce an inter-document simi-
larity score, weighted by how strongly each con-
cept is seen to represent a document (0 &lt; pi &lt;
1). The contribution of a concept is further
weighted by its ILF score, so that more specific
concepts contribute more to final relatedness.
The second document similarity metric we
propose is called the WIKISPREAD method and
is a natural extension of the inter-concept spread-
</bodyText>
<subsectionHeader confidence="0.8338735">
4Vectors of Wikipedia topics (concepts) and how
strongly they are seen to relate to the current document.
</subsectionHeader>
<bodyText confidence="0.99739125">
Algorithm 2 Pseudo code for the MaxSim al-
gorithm for computing inter-document similar-
ity. vi is a Wikipedia concept and 0 &lt; pi &lt; 1
how strongly it relates to the current document.
</bodyText>
<figure confidence="0.380413705882353">
Require: ILF lookup function
function MAXSIM(V1, V2)
num=0
den=0
for (vi, pi) E V1 do
sk = 0 &gt; sk = maxj sim(vi, vj)
for vj E V2 do &gt; Find most related topic
sj = sim(vi, vj)
if sj &gt; sk then
vk = vj&gt; Topic in V2 most related to vi
sk = sj
end if
end for
num += skpiILF(vk)
den += ILF(vk)
end for
return num / den
</figure>
<subsectionHeader confidence="0.615293">
end function
</subsectionHeader>
<bodyText confidence="0.965702666666667">
Algorithm 3 Pseudo code for the WikiSpread al-
gorithm for computing inter-document similar-
ity. Kirit = 1.0.
</bodyText>
<equation confidence="0.94191325">
function WIKISPREAD(V1, V2)
A1 = 0 &gt; Dynamic activation vectors.
A2 = 0
for (vi, pi) E V1 do &gt; Document 1
ai = Kirit · pi &gt; Update ai a pi
Add (vi, ai) to A1
SPREAD UNIDIR(vi, A1, 0)
end for
for (vj, pj) E V2 do &gt; Document 2
aj = Kirit · pj
Add (vj, aj) to A2
SPREAD UNIDIR(vj, A2, 0)
</equation>
<subsectionHeader confidence="0.545916">
end for
</subsectionHeader>
<bodyText confidence="0.54227">
Compute similarity using AA-cos or AA-wlm
</bodyText>
<subsectionHeader confidence="0.755496">
end function
</subsectionHeader>
<bodyText confidence="0.999817307692308">
ing activation work introduced in the previous
section. We view a document concept vector as
a cluster of concepts, and build a single docu-
ment activation vector (see Algorithm 3) – i.e. a
vector of article ids and their respective activa-
tions – for each document, by iteratively spread-
ing from each concept in the document concept
vector. Finally, similarity is computed using ei-
ther the AA-cos or AA-wlm methods given by
Equation 4 and Equation 5 respectively.
Knowledge-based approaches such as the
Wikipedia-based methods can capture more
complex lexical and semantic relationships than
</bodyText>
<page confidence="0.998842">
52
</page>
<tableCaption confidence="0.811156">
Table 4: Summary of final document similarity
correlations over the Lee &amp; Pincombe document
similarity dataset. ESA score from Gabrilovich
and Markovitch (2007).
</tableCaption>
<table confidence="0.998468833333333">
Pearson p
Cosine VSM (with tf-idf) only 0.56
MaxSim method 0.68
WikiSpread method 0.62
ESA 0.72
Combined (Cosine + MaxSim) 0.72
</table>
<bodyText confidence="0.999493083333333">
keyword-matching approaches, however, noth-
ing can be said about concepts not adequately
represented in the underlying knowledge base
(Wikipedia). We therefore hypothesise that com-
bining the two approaches will lead to more ro-
bust document similarity performance. There-
fore, the final document similarity metric we
evaluate (COMBINED) is a linear combination
of the best-performing Wikipedia-based meth-
ods described above, and the well-known Vector
Space Model (VSM) with cosine similarity and
tf-idf (Salton and McGill, 1983).
</bodyText>
<sectionHeader confidence="0.964841" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.995895473684211">
The results obtained on the Lee (2005) document
similarity dataset using the three document sim-
ilarity metrics (MAXSIM, WIKISPREAD, and
COMBINED) are summarised in Table 4. Of
the two Wikipedia-only methods, the MaxSim
method achieves the best correlation score of
p = 0.68. By combining the standard co-
sine VSM with tf-idf with the MaxSim metric
in the ratio A and (1 − A) for 0 &lt; A &lt; 1,
and performing a parameter sweep over A, we
can weight the contributions made by the indi-
vidual methods and observe the effect this has
on final performance. The results are shown
in Fig 1. Note that both methods contribute
equally (A = 0.5) to the final best correlation
score of p = 0.72. This suggests that selective
knowledge-based augmentation of simple VSM
methods can lead to more accurate document
similarity performance.
</bodyText>
<figureCaption confidence="0.9411465">
Figure 1: Parameter sweep over A showing con-
tributions from cosine (A) and Wikipedia-based
MAXSIM method (1 − A) to the final perfor-
mance over the Lee (2005) dataset.
</figureCaption>
<sectionHeader confidence="0.995435" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999968043478261">
In this paper, the problem of computing con-
ceptual similarity between concepts and docu-
ments are approached by spreading activation
over Wikipedia’s hyperlink graph. New strate-
gies are required to infer weights of associa-
tion between articles, and for this we introduce
and test three new weighting schemes and find
our Inverse Link-Frequency (ILF) to give best
results. Strategies are also required for trans-
lating resulting activations into scores of relat-
edness, and for this we propose and test three
new strategies, and find that our cosine Agglom-
erative Approach gives best results. For com-
puting document similarity, we propose and test
two new methods using only Wikipedia. Finally,
we show that using our best Wikipedia-based
method to augment the cosine VSM method us-
ing tf-idf, leads to the best results. The final
result of p = 0.72 is equal to that reported
for ESA (Gabrilovich and Markovitch, 2007),
while requiring less than 10% of the Wikipedia
database required for ESA. Table 4 summarises
the document-similarity results.
</bodyText>
<sectionHeader confidence="0.997521" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996423333333333">
We thank Michael D. Lee for his document simi-
larity data and MIH Holdings Ltd. for financially
supporting this research.
</bodyText>
<page confidence="0.998247">
53
</page>
<sectionHeader confidence="0.99587" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998025">
Berger, Helmut, Michael Dittenbach, and Dieter
Merkl. 2004. An adaptive information retrieval
system based on associative networks. APCCM
’04: Proceedings of the first Asian-Pacific confer-
ence on Conceptual Modelling, pages 27–36.
Budanitsky, A. and G. Hirst. 2001. Semantic dis-
tance in WordNet: An experimental, application-
oriented evaluation of five measures. In Work-
shop on WordNet and Other Lexical Resources,
volume 2. Citeseer.
Collins, A.M. and E.F. Loftus. 1975. A spreading-
activation theory of semantic processing. Psycho-
logical review, 82(6):407–428.
Crestani, F. 1997. Application of Spreading Activa-
tion Techniques in Information Retrieval. Artifi-
cial Intelligence Review, 11(6):453–482.
Csomai, A. and R. Mihalcea. 2008. Linking docu-
ments to encyclopedic knowledge. IEEE Intelli-
gent Systems, 23(5):34–41.
Gabrilovich, E. and S. Markovitch. 2007. Comput-
ing Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. Proceedings of the
20th International Joint Conference on Artificial
Intelligence, pages 6–12.
Gabrilovich, E. 2002. The WordSimilarity-353 Test
Collection. Using Information Content to Evalu-
ate Semantic Similarity in a Taxonomy.
Lee, M.D., B. Pincombe, and M. Welsh. 2005. A
Comparison of Machine Measures of Text Docu-
ment Similarity with Human Judgments. In 27th
Annual Meeting of the Cognitive Science Society
(CogSci2005), pages 1254–1259.
Lyman, P. and H.R. Varian. 2003. How much
information? http://www2.sims.
berkeley.edu/research/projects/
how-much-info-2003/index.htm. Ac-
cessed: May, 2010.
Metzler, Donald and W. Bruce Croft. 2006. Beyond
bags of words: Modeling implicit user preferences
in information retrieval. AAAI’06: Proceedings of
the 21st National Conference on Artificial Intelli-
gence, pages 1646–1649.
Milne, David and Ian H. Witten. 2008. Learning to
link with wikipedia. CIKM ’08: Proceeding of the
17th ACM Conference on Information and Knowl-
edge Management, pages 509–518.
Preece, SE. 1982. Spreading Activation Network
Model for Information Retrieval. Ph.D. thesis.
Salton, G. and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill New
York.
Tran, T., P. Cimiano, S. Rudolph, and R. Studer.
2008. Ontology-based Interpretation of Keywords
for Semantic Search. The Semantic Web, pages
523–536.
Witten, I.H. and E. Frank. 2005. Data Min-
ing: Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann.
Witten, I.H. and D. Milne. 2008. An Effective, Low-
Cost Measure of Semantic Relatedness Obtained
From Wikipedia Links. In Proceeding of AAAI
Workshop on Wikipedia and Artificial Intelligence:
an Evolving Synergy, AAAI Press, Chicago, USA,
pages 25–30.
Yeh, E., D. Ramage, C.D. Manning, E. Agirre, and
A. Soroa. 2009. WikiWalk: Random walks on
Wikipedia for semantic relatedness. In Proceed-
ings of the 2009 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 41–
49. Association for Computational Linguistics.
</reference>
<page confidence="0.999011">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954951">
<title confidence="0.9852655">Measuring Conceptual Similarity by Spreading Activation Wikipedia’s Hyperlink Structure</title>
<author confidence="0.987565">Stephan Gouws</author>
<author confidence="0.987565">G-J van_Rooyen</author>
<author confidence="0.987565">A Herman</author>
<affiliation confidence="0.999887">Stellenbosch University</affiliation>
<abstract confidence="0.9993586">Keyword-matching systems based on simple models of semantic relatedness are inadequate at modelling the ambiguities in natural language text, and cannot reliably address the increasingly complex information needs of users. In this paper we propose novel methods for computing semantic relatedness by spreading activation energy over the hyperlink structure of Wikipedia. We demonstrate that our techniques can approach state-of-the-art performance, while requiring only a fraction of the background data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Helmut Berger</author>
<author>Michael Dittenbach</author>
<author>Dieter Merkl</author>
</authors>
<title>An adaptive information retrieval system based on associative networks.</title>
<date>2004</date>
<booktitle>APCCM ’04: Proceedings of the first Asian-Pacific conference on Conceptual Modelling,</booktitle>
<pages>27--36</pages>
<contexts>
<context position="5009" citStr="Berger et al., 2004" startWordPosition="748" endWordPosition="751">ategorised as vectors in this concept space and similarity is computed as the cosine similarity of their ESA vectors. The most similar work to ours is Yeh (2009) in which the authors derive a graph structure from the inter-article links in Wikipedia pages, and then perform random walks over the graph to compute relatedness. In Wikipedia, users create links between articles which are seen to be related to some degree. Since links relate one article to its neighbours, and by extension to their neighbours, we extract and process this hyperlink structure (using SA) as an Associative Network (AN) (Berger et al., 2004) of concepts and links relating them to one another. The SA algorithm can briefly be described as an iterative process of propagating real-valued energy from one or more source nodes, via weighted links over an associative network (each such a propagation is called a pulse). The algorithm consists of two steps: First, one or more pulses are triggered, and second, termination checks determine whether the process should continue or halt. This process of activating more and more nodes in the network and checking for termination conditions are repeated pulse after pulse, until all termination cond</context>
</contexts>
<marker>Berger, Dittenbach, Merkl, 2004</marker>
<rawString>Berger, Helmut, Michael Dittenbach, and Dieter Merkl. 2004. An adaptive information retrieval system based on associative networks. APCCM ’04: Proceedings of the first Asian-Pacific conference on Conceptual Modelling, pages 27–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Semantic distance in WordNet: An experimental, applicationoriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In Workshop on WordNet and Other Lexical Resources,</booktitle>
<volume>2</volume>
<publisher>Citeseer.</publisher>
<contexts>
<context position="3948" citStr="Budanitsky and Hirst, 2001" startWordPosition="570" endWordPosition="574">ry modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997). Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia’s larger conceptual coverage. WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them. ESA discards the link structure and uses only the text in articles to derive an explicit concept space in which each dimension represents one article/concept. Text is categorised as vectors in this concept space and similarity is computed as the cosine similarity of their ESA vectors. The most similar work to ours is Yeh (200</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>Budanitsky, A. and G. Hirst. 2001. Semantic distance in WordNet: An experimental, applicationoriented evaluation of five measures. In Workshop on WordNet and Other Lexical Resources, volume 2. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>E F Loftus</author>
</authors>
<title>A spreadingactivation theory of semantic processing.</title>
<date>1975</date>
<journal>Psychological review,</journal>
<volume>82</volume>
<issue>6</issue>
<contexts>
<context position="3376" citStr="Collins and Loftus, 1975" startWordPosition="482" endWordPosition="485">ound data as for WLM, which is less than 10% of the data required for ESA. In the following sections we introduce work related to our work and an overview of our approach and the problems that have to be solved. We then discuss our method in detail and present several experiments to test and compare it against other state-of-the-art methods. 46 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 46–54, 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997). Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia’</context>
</contexts>
<marker>Collins, Loftus, 1975</marker>
<rawString>Collins, A.M. and E.F. Loftus. 1975. A spreadingactivation theory of semantic processing. Psychological review, 82(6):407–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Crestani</author>
</authors>
<title>Application of Spreading Activation Techniques in Information Retrieval.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<volume>11</volume>
<issue>6</issue>
<contexts>
<context position="3618" citStr="Crestani, 1997" startWordPosition="525" endWordPosition="526"> present several experiments to test and compare it against other state-of-the-art methods. 46 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 46–54, 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997). Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia’s larger conceptual coverage. WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing the</context>
<context position="8062" citStr="Crestani, 1997" startWordPosition="1257" endWordPosition="1258">sults (Witten and Milne, 2008). The Wikipedia graph structure is represented in an adjacency list structure, i.e. for each node vi we store its list of neighbour nodes in a dictionary using vi’s id as key. This approach is preferred over an adjacency matrix structure, since most articles are linked to by only 34 articles on average, which would lead to a very sparse adjacency matrix structure. 4 Adapting Spreading Activation for Wikipedia’s Hyperlink Structure Each pulse in the Spreading Activation (SA) process consists of three stages: 1) pre-adjustment, 2) spreading, and 3) post-adjustment (Crestani, 1997). During pre- and post-adjustment, some form of activation decay is optionally applied to the active nodes. This serves both to avoid retention of activation from previous pulses, and, from a connectionist point of view, models ‘loss of interest’ when nodes are not continually activated. Let ai,in denote the total energy input (activation) for node vi, and N(vi) the set of vi’s neighbour nodes with incoming links to vi. Also, let aj,out denote the output activation of a node vj connected to node vi, and let wij denote the weight of connection between node vi and vj. For a node vi, we can then </context>
</contexts>
<marker>Crestani, 1997</marker>
<rawString>Crestani, F. 1997. Application of Spreading Activation Techniques in Information Retrieval. Artificial Intelligence Review, 11(6):453–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Csomai</author>
<author>R Mihalcea</author>
</authors>
<title>Linking documents to encyclopedic knowledge.</title>
<date>2008</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="22574" citStr="Csomai and Mihalcea, 2008" startWordPosition="3698" endWordPosition="3701">gher threshold effectively limits the amount of non-important nodes that are activated, since their activations are more quickly decayed, whilst at the same time requiring a higher threshold to continue spreading. Boosting more important nodes can therefore lead to activation vectors which capture the semantic context of the source nodes more accurately, leading to higher performance. 8 Computing document similarity To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4. This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008). This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document. We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)). The first metric </context>
</contexts>
<marker>Csomai, Mihalcea, 2008</marker>
<rawString>Csomai, A. and R. Mihalcea. 2008. Linking documents to encyclopedic knowledge. IEEE Intelligent Systems, 23(5):34–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>6--12</pages>
<contexts>
<context position="2582" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="360" endWordPosition="363">g activation over the hyperlink structure of Wikipedia, the largest free online encyclopaedia. New measures for computing similarity between individual concepts (inter-concept similarity, such as “France” and “Great Britain”), as well as between documents (inter-document similarity) are proposed and tested. It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively. Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity. Furthermore, we use the same background data as for WLM, which is less than 10% of the data required for ESA. In the following sections we introduce work related to our work and an overview of our approach and the problems that have to be solved. We then discuss our method in detail and present several experiments to test and compare it against other state-of-the-art methods. 46 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed</context>
<context position="3858" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="557" endWordPosition="560">–54, 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997). Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia’s larger conceptual coverage. WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them. ESA discards the link structure and uses only the text in articles to derive an explicit concept space in which each dimension represents one article/concept. Text is categorised as vectors in this concept space and similarity is compute</context>
<context position="16466" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="2676" endWordPosition="2679">amic nodevalue-pair list, updated in-place. P is a dynamic list of nodes in the path to vi to avoid cycles. 7 Parameter Optimisation: Inter-concept Similarity The model for SA as introduced in this paper relies on several important parameters, namely the spreading strategy (TAA, AA-cos, or AA-wlm), weighting scheme (pure ED, ILF, and NILF), maximum path length Lp,max, network decay d, and threshold T. These parameters have a large influence on the accuracy of the proposed technique, and therefore need to be optimised. Experimental Method In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 &lt; wig E W &lt; 1. Require: G, Lp,—ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai &lt; T then &gt; Threshold return end if Add vi to P &gt; To avoid cycles for vj E N(vi) do &gt; Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ &lt; Lp,—ax then a∗j =aj+ai*wij*d Replace (vj, aj) E A wit</context>
<context position="19643" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3223" endWordPosition="3226"> spreading process are converted into scores of relatedness or similarity between two nodes. Table 1 summarises the best results obtained for each of the three strategies, with the specific set of parameters that were used in each run. Results are better using the AA (pmax = 0.70 for AA-cos) than using the TAA (pmax = 0.56). Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70). These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75). Maximum path length Lp,max is related to how far one node can spread its activation in the network. We extend the first-order link model used by WLM, by approaching the link structure as an associative network and by using spreading activation. To evaluate if this is a useful approach, tests were conducted by using maximum path lengths of one, two, and three. Table 2 summarises the results for this experiment. Increasing path length from one to two hops increases performance from pmax = 0.47 to pmax = Table 2: Spreading results by maximum path length Lp,max. Best results in bold. </context>
<context position="25655" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="4245" endWordPosition="4248">uild a single document activation vector (see Algorithm 3) – i.e. a vector of article ids and their respective activations – for each document, by iteratively spreading from each concept in the document concept vector. Finally, similarity is computed using either the AA-cos or AA-wlm methods given by Equation 4 and Equation 5 respectively. Knowledge-based approaches such as the Wikipedia-based methods can capture more complex lexical and semantic relationships than 52 Table 4: Summary of final document similarity correlations over the Lee &amp; Pincombe document similarity dataset. ESA score from Gabrilovich and Markovitch (2007). Pearson p Cosine VSM (with tf-idf) only 0.56 MaxSim method 0.68 WikiSpread method 0.62 ESA 0.72 Combined (Cosine + MaxSim) 0.72 keyword-matching approaches, however, nothing can be said about concepts not adequately represented in the underlying knowledge base (Wikipedia). We therefore hypothesise that combining the two approaches will lead to more robust document similarity performance. Therefore, the final document similarity metric we evaluate (COMBINED) is a linear combination of the best-performing Wikipedia-based methods described above, and the well-known Vector Space Model (VSM) with</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Gabrilovich, E. and S. Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
</authors>
<title>The WordSimilarity-353 Test Collection. Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>2002</date>
<contexts>
<context position="17207" citStr="Gabrilovich, 2002" startWordPosition="2826" endWordPosition="2827">ion depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 &lt; wig E W &lt; 1. Require: G, Lp,—ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai &lt; T then &gt; Threshold return end if Add vi to P &gt; To avoid cycles for vj E N(vi) do &gt; Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ &lt; Lp,—ax then a∗j =aj+ai*wij*d Replace (vj, aj) E A with (vj, a∗j ) SPREAD UNIDIR(vj, A, P) end if end for return end function 50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method’s scores with the human-assigned scores. To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3. Hold out one part of the data and iteratively evaluate the performance of the algorithm on the remaining k−1 parts until all k parts have been held out onc</context>
</contexts>
<marker>Gabrilovich, 2002</marker>
<rawString>Gabrilovich, E. 2002. The WordSimilarity-353 Test Collection. Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pincombe Lee</author>
<author>M Welsh</author>
</authors>
<title>A Comparison of Machine Measures of Text Document Similarity with Human Judgments.</title>
<date>2005</date>
<booktitle>In 27th Annual Meeting of the Cognitive Science Society (CogSci2005),</booktitle>
<pages>1254--1259</pages>
<marker>Lee, Welsh, 2005</marker>
<rawString>Lee, M.D., B. Pincombe, and M. Welsh. 2005. A Comparison of Machine Measures of Text Document Similarity with Human Judgments. In 27th Annual Meeting of the Cognitive Science Society (CogSci2005), pages 1254–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lyman</author>
<author>H R Varian</author>
</authors>
<title>How much information? http://www2.sims. berkeley.edu/research/projects/ how-much-info-2003/index.htm.</title>
<date>2003</date>
<location>Accessed:</location>
<contexts>
<context position="869" citStr="Lyman and Varian, 2003" startWordPosition="115" endWordPosition="118">ms based on simple models of semantic relatedness are inadequate at modelling the ambiguities in natural language text, and cannot reliably address the increasingly complex information needs of users. In this paper we propose novel methods for computing semantic relatedness by spreading activation energy over the hyperlink structure of Wikipedia. We demonstrate that our techniques can approach state-of-the-art performance, while requiring only a fraction of the background data. 1 Introduction The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003). Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006). In recent years, much research attention has therefore been given to semantic techniques of information retrieval. Such systems allow for sophisticated semantic search, however, require the use of a more difficu</context>
</contexts>
<marker>Lyman, Varian, 2003</marker>
<rawString>Lyman, P. and H.R. Varian. 2003. How much information? http://www2.sims. berkeley.edu/research/projects/ how-much-info-2003/index.htm. Accessed: May, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>Beyond bags of words: Modeling implicit user preferences in information retrieval.</title>
<date>2006</date>
<booktitle>AAAI’06: Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>1646--1649</pages>
<contexts>
<context position="1256" citStr="Metzler and Croft, 2006" startWordPosition="172" endWordPosition="175">n approach state-of-the-art performance, while requiring only a fraction of the background data. 1 Introduction The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003). Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006). In recent years, much research attention has therefore been given to semantic techniques of information retrieval. Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (Tran et al., 2008). Furthermore, these methods require specially encoded (and thus costly) ontologies to describe the particular domain knowledge in which the system operates, and the specific interrelations of concepts within that domain. In this paper, we focus on the problem of computationally estimating similarity or relatedness between two natural-la</context>
</contexts>
<marker>Metzler, Croft, 2006</marker>
<rawString>Metzler, Donald and W. Bruce Croft. 2006. Beyond bags of words: Modeling implicit user preferences in information retrieval. AAAI’06: Proceedings of the 21st National Conference on Artificial Intelligence, pages 1646–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>CIKM ’08: Proceeding of the 17th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>509--518</pages>
<contexts>
<context position="22632" citStr="Milne and Witten (2008)" startWordPosition="3708" endWordPosition="3711"> nodes that are activated, since their activations are more quickly decayed, whilst at the same time requiring a higher threshold to continue spreading. Boosting more important nodes can therefore lead to activation vectors which capture the semantic context of the source nodes more accurately, leading to higher performance. 8 Computing document similarity To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4. This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008). This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document. We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)). The first metric we propose is called MAXSIM (see Algorithm 2) and is based</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>Milne, David and Ian H. Witten. 2008. Learning to link with wikipedia. CIKM ’08: Proceeding of the 17th ACM Conference on Information and Knowledge Management, pages 509–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SE Preece</author>
</authors>
<title>Spreading Activation Network Model for Information Retrieval.</title>
<date>1982</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="3465" citStr="Preece, 1982" startWordPosition="500" endWordPosition="501">we introduce work related to our work and an overview of our approach and the problems that have to be solved. We then discuss our method in detail and present several experiments to test and compare it against other state-of-the-art methods. 46 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 46–54, 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997). Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia’s larger conceptual coverage. WLM treats the anchor text in Wikipedia articles as links t</context>
</contexts>
<marker>Preece, 1982</marker>
<rawString>Preece, SE. 1982. Spreading Activation Network Model for Information Retrieval. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill</publisher>
<location>New York.</location>
<contexts>
<context position="10798" citStr="Salton and McGill, 1983" startWordPosition="1710" endWordPosition="1713">nal to its in-degree (number of neighbours N(vi) ≥ 1 with incoming links to vi1). Thus ED(vi, vj) = wij = 1 |N(vi)|. This reduces the effect of very connected nodes on the spreading process (constraint 2 above). For instance, we consider a path connecting two nodes via a general article such as USA (connected to 322,000 articles) not nearly as indicative of a semantic relationship, as a path connecting them via a very specific concept, such as Hair Pin (only connected to 20 articles). Inverse Link-Frequency (ILF) is inspired by the term-frequency inverse document-frequency (tf-idf) heuristic (Salton and McGill, 1983) in which a term’s weight is reduced as it is contained in more documents in the corpus. It is based on the idea that the more a term appears in documents across the corpus, the less it can discriminate any one of those documents. We define a node vi’s link-frequency as the number of nodes that vi is connected to |N(vi)| divided by the number of possible nodes it could be connected to in the entire Wikipedia graph 1All orphan nodes are removed from the AN. 48 |G|, and therefore give the log-smoothed inverse link-frequency of node vi as: ILF(vi) A log \ |N(I i) |/ &gt; 0 (2) As noted above for pur</context>
<context position="26310" citStr="Salton and McGill, 1983" startWordPosition="4341" endWordPosition="4344">-idf) only 0.56 MaxSim method 0.68 WikiSpread method 0.62 ESA 0.72 Combined (Cosine + MaxSim) 0.72 keyword-matching approaches, however, nothing can be said about concepts not adequately represented in the underlying knowledge base (Wikipedia). We therefore hypothesise that combining the two approaches will lead to more robust document similarity performance. Therefore, the final document similarity metric we evaluate (COMBINED) is a linear combination of the best-performing Wikipedia-based methods described above, and the well-known Vector Space Model (VSM) with cosine similarity and tf-idf (Salton and McGill, 1983). Results The results obtained on the Lee (2005) document similarity dataset using the three document similarity metrics (MAXSIM, WIKISPREAD, and COMBINED) are summarised in Table 4. Of the two Wikipedia-only methods, the MaxSim method achieves the best correlation score of p = 0.68. By combining the standard cosine VSM with tf-idf with the MaxSim metric in the ratio A and (1 − A) for 0 &lt; A &lt; 1, and performing a parameter sweep over A, we can weight the contributions made by the individual methods and observe the effect this has on final performance. The results are shown in Fig 1. Note that b</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tran</author>
<author>P Cimiano</author>
<author>S Rudolph</author>
<author>R Studer</author>
</authors>
<title>Ontology-based Interpretation of Keywords for Semantic Search. The Semantic Web,</title>
<date>2008</date>
<pages>523--536</pages>
<contexts>
<context position="1517" citStr="Tran et al., 2008" startWordPosition="209" endWordPosition="212">ormation retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006). In recent years, much research attention has therefore been given to semantic techniques of information retrieval. Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (Tran et al., 2008). Furthermore, these methods require specially encoded (and thus costly) ontologies to describe the particular domain knowledge in which the system operates, and the specific interrelations of concepts within that domain. In this paper, we focus on the problem of computationally estimating similarity or relatedness between two natural-language documents. A novel technique is proposed for computing semantic similarity by spreading activation over the hyperlink structure of Wikipedia, the largest free online encyclopaedia. New measures for computing similarity between individual concepts (inter-</context>
</contexts>
<marker>Tran, Cimiano, Rudolph, Studer, 2008</marker>
<rawString>Tran, T., P. Cimiano, S. Rudolph, and R. Studer. 2008. Ontology-based Interpretation of Keywords for Semantic Search. The Semantic Web, pages 523–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="17500" citStr="Witten and Frank, 2005" startWordPosition="2871" endWordPosition="2874">rn end if Add vi to P &gt; To avoid cycles for vj E N(vi) do &gt; Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ &lt; Lp,—ax then a∗j =aj+ai*wij*d Replace (vj, aj) E A with (vj, a∗j ) SPREAD UNIDIR(vj, A, P) end if end for return end function 50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method’s scores with the human-assigned scores. To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3. Hold out one part of the data and iteratively evaluate the performance of the algorithm on the remaining k−1 parts until all k parts have been held out once. Finally, average the algorithm’s performance over all k runs into one score resembling the performance for that set of parameters. Since there are five parameters (spreading strategy, weighting scheme, path length, network decay, and threshold), a grid search was implemented by holding thr</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, I.H. and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>D Milne</author>
</authors>
<title>An Effective, LowCost Measure of Semantic Relatedness Obtained From Wikipedia Links.</title>
<date>2008</date>
<booktitle>In Proceeding of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy,</booktitle>
<pages>25--30</pages>
<publisher>AAAI Press,</publisher>
<location>Chicago, USA,</location>
<contexts>
<context position="2510" citStr="Witten and Milne, 2008" startWordPosition="351" endWordPosition="354">ique is proposed for computing semantic similarity by spreading activation over the hyperlink structure of Wikipedia, the largest free online encyclopaedia. New measures for computing similarity between individual concepts (inter-concept similarity, such as “France” and “Great Britain”), as well as between documents (inter-document similarity) are proposed and tested. It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively. Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity. Furthermore, we use the same background data as for WLM, which is less than 10% of the data required for ESA. In the following sections we introduce work related to our work and an overview of our approach and the problems that have to be solved. We then discuss our method in detail and present several experiments to test and compare it against other state-of-the-art methods. 46 Beijing, Aug</context>
<context position="3786" citStr="Witten and Milne, 2008" startWordPosition="547" endWordPosition="550">atively Constructed Semantic Resources”, Coling 2010, pages 46–54, 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997). Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia’s larger conceptual coverage. WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them. ESA discards the link structure and uses only the text in articles to derive an explicit concept space in which each dimension represents one article/concept. Text i</context>
<context position="7477" citStr="Witten and Milne, 2008" startWordPosition="1162" endWordPosition="1165">relates these concepts to one another. In order to compute relatedness between two concepts vi and vj, we use spreading activation and rely on the fundamental principle of an associative network, namely that it connects nodes that are associated with one another via real-valued links denoting how strongly the objects are related. Since Wikipedia was not created as an associative network, but primarily as an online encyclopaedia, none of these weights exist, and we will have to deduce these (see Fan-out constraint in Section 4). 47 Links into pages are used, since this leads to better results (Witten and Milne, 2008). The Wikipedia graph structure is represented in an adjacency list structure, i.e. for each node vi we store its list of neighbour nodes in a dictionary using vi’s id as key. This approach is preferred over an adjacency matrix structure, since most articles are linked to by only 34 articles on average, which would lead to a very sparse adjacency matrix structure. 4 Adapting Spreading Activation for Wikipedia’s Hyperlink Structure Each pulse in the Spreading Activation (SA) process consists of three stages: 1) pre-adjustment, 2) spreading, and 3) post-adjustment (Crestani, 1997). During pre- a</context>
<context position="13854" citStr="Witten and Milne, 2008" startWordPosition="2236" endWordPosition="2240">it (usually 1.0), and all node activations including aj = 0. After the SA process has terminated, vj is activated with some aj,in. Relatedness is computed as the ratio sim — ° i TAA(v v) ° z� ,y — Kinit. Agglomerative Approach (AA) The second approach is called the Agglomerative Approach since we agglomerate all activations into one score resembling relatedness. After spreading has terminated, relatedness is computed as the amount of overlap between the individual nodes’ activation vectors, using either the cosine similarity (AA-cos), or an adapted version of the information theory based WLM (Witten and Milne, 2008) measure. Assume the same set of initial nodes vi and vj. Let Ak be the N-dimensional vector of realvalued activation values obtained by spreading over the N nodes in the graph from node vk (called an activation vector). We use akx to denote the element at position x in Ak. Furthermore, let Vk = {vk1, ..., vkMI denote the set of M nodes activated by spreading from vk, i.e. the set of identifiers of nodes with non-zero activations in Ak after spreading has terminated (and therefore M &lt; N). We then define the cosine Agglomerative Approach (henceforth called AA-cos) as (4) ||Ai |Aj|| For our adap</context>
<context position="16494" citStr="Witten and Milne (2008)" startWordPosition="2681" endWordPosition="2684">lace. P is a dynamic list of nodes in the path to vi to avoid cycles. 7 Parameter Optimisation: Inter-concept Similarity The model for SA as introduced in this paper relies on several important parameters, namely the spreading strategy (TAA, AA-cos, or AA-wlm), weighting scheme (pure ED, ILF, and NILF), maximum path length Lp,max, network decay d, and threshold T. These parameters have a large influence on the accuracy of the proposed technique, and therefore need to be optimised. Experimental Method In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 &lt; wig E W &lt; 1. Require: G, Lp,—ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai &lt; T then &gt; Threshold return end if Add vi to P &gt; To avoid cycles for vj E N(vi) do &gt; Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ &lt; Lp,—ax then a∗j =aj+ai*wij*d Replace (vj, aj) E A with (vj, a∗j ) SPREAD UNIDIR(v</context>
<context position="19589" citStr="Witten and Milne, 2008" startWordPosition="3214" endWordPosition="3217">etermines how activations resulting from the spreading process are converted into scores of relatedness or similarity between two nodes. Table 1 summarises the best results obtained for each of the three strategies, with the specific set of parameters that were used in each run. Results are better using the AA (pmax = 0.70 for AA-cos) than using the TAA (pmax = 0.56). Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70). These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75). Maximum path length Lp,max is related to how far one node can spread its activation in the network. We extend the first-order link model used by WLM, by approaching the link structure as an associative network and by using spreading activation. To evaluate if this is a useful approach, tests were conducted by using maximum path lengths of one, two, and three. Table 2 summarises the results for this experiment. Increasing path length from one to two hops increases performance from pmax = 0.47 to pmax = Table 2: Spreading results</context>
</contexts>
<marker>Witten, Milne, 2008</marker>
<rawString>Witten, I.H. and D. Milne. 2008. An Effective, LowCost Measure of Semantic Relatedness Obtained From Wikipedia Links. In Proceeding of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy, AAAI Press, Chicago, USA, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Yeh</author>
<author>D Ramage</author>
<author>C D Manning</author>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>WikiWalk: Random walks on Wikipedia for semantic relatedness.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>pages</pages>
<marker>Yeh, Ramage, Manning, Agirre, Soroa, 2009</marker>
<rawString>Yeh, E., D. Ramage, C.D. Manning, E. Agirre, and A. Soroa. 2009. WikiWalk: Random walks on Wikipedia for semantic relatedness. In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, pages 41– 49. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>