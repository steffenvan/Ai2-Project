<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005760">
<title confidence="0.9960045">
Active Dual Supervision: Reducing the Cost
of Annotating Examples and Features
</title>
<note confidence="0.55265">
Prem Melville
IBM T.J. Watson Research Center
</note>
<address confidence="0.629874">
Yorktown Heights, NY 10598
</address>
<email confidence="0.989501">
pmelvil@us.ibm.com
</email>
<note confidence="0.8287325">
Vikas Sindhwani
IBM T.J. Watson Research Center
</note>
<address confidence="0.587941">
Yorktown Heights, NY 10598
</address>
<email confidence="0.994247">
vsindhw@us.ibm.com
</email>
<sectionHeader confidence="0.996592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867095238095">
When faced with the task of building machine
learning or NLP models, it is often worthwhile
to turn to active learning to obtain human an-
notations at minimal costs. Traditional active
learning schemes query a human for labels of
intelligently chosen examples. However, hu-
man effort can also be expended in collecting
alternative forms of annotations. For example,
one may attempt to learn a text classifier by
labeling class-indicating words, instead of, or
in addition to, documents. Learning from two
different kinds of supervision brings a new,
unexplored dimension to the problem of ac-
tive learning. In this paper, we demonstrate
the value of such active dual supervision in
the context of sentiment analysis. We show
how interleaving queries for both documents
and words significantly reduces human effort
– more than what is possible through tradi-
tional one-dimensional active learning, or by
passive combinations of supervisory inputs.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929454545455">
As a canonical running example for the theme of
this paper, consider the problem of sentiment anal-
ysis (Pang and Lee, 2008). Given a piece of text as
input, the desired output is a polarity score that indi-
cates whether this text expresses a positive or nega-
tive opinion towards a topic of interest. From a ma-
chine learning viewpoint, this problem may be posed
as a typical binary text classification task. Senti-
ment, however, is often conveyed with subtle lin-
guistic mechanisms such as sarcasm, negation and
the use of highly domain-specific and contextual
</bodyText>
<page confidence="0.991497">
49
</page>
<bodyText confidence="0.999865558823529">
cues. This brings a multi-disciplinary flavor to the
problem, drawing interest from both Natural Lan-
guage Processing and Machine Learning communi-
ties.
Many methodologies proposed in these disci-
plines share a common limitation that their perfor-
mance is bounded by the amount and quality of la-
beled data. However, they differ conceptually in
the type of human effort they require. On one
hand, supervised machine learning techniques re-
quire human effort in acquiring labeled examples,
which requires reading documents and annotating
them with their aggregate sentiment. On the other
hand, dictionary-based NLP systems require human
effort in collecting labeled features: for example,
in the domain of movie reviews, words that evoke
positive sentiment (e.g., “mesmerizing”, “thrilling”
etc) may be labeled positive, while words that evoke
negative sentiment (e.g., “boring”,“disappointing”)
may be labeled negative. This kind of annotation
requires a human to condense prior linguistic expe-
rience with a word into a sentiment label that reflects
the net emotion that the word evokes.
We refer to the general setting of learning from
both labels on examples and features as dual super-
vision. This setting arises more broadly in tasks
where in addition to labeled documents, it is fre-
quently possible to provide domain knowledge in the
form of words, or phrases (Zaidan and Eisner, 2008)
or even more sophisticated linguistic features, that
associate strongly with a class. Recent work (Druck
et al., 2008; Sindhwani and Melville, 2008) has
demonstrated that the presence of word supervision
can greatly reduce the number of labeled documents
</bodyText>
<note confidence="0.988875">
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 49–57,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.994172882352941">
required to build high quality text classifiers.
In general, these two sources of supervision are
not mutually redundant, and have different annota-
tion costs, human response quality, and degrees of
utility towards learning a dual supervision model.
This leads naturally to the problem of active dual
supervision, or, how to optimally query a human or-
acle to simultaneously collect document and feature
annotations, with the objective of building the high-
est quality model with the lowest cost. Much of the
machine learning literature on active learning has
focused on one-sided example-only annotation for
classification problems. Less attention has been de-
voted to simultaneously acquiring alternative forms
of supervisory domain knowledge, such as the kind
routinely encountered in NLP. Our contribution may
be viewed as a step in this direction.
</bodyText>
<sectionHeader confidence="0.991979" genericHeader="method">
2 Dual supervision
</sectionHeader>
<bodyText confidence="0.999889666666667">
Most work in supervised learning has focused on
learning from examples, each represented by a set
of feature values and a class label. In dual super-
vision we consider an additional aspect, by way of
labels of features, which convey prior knowledge on
associations of features to particular classes. Since
we deal only with text classification in this paper, all
features represent term-frequencies of words, and as
such we use feature and word interchangeably.
The active learning schemes we explore in this pa-
per are broadly applicable to any learner that can
support dual supervision, but here we focus on ac-
tive learning for the Pooling Multinomials classi-
fier (Melville et al., 2009) described below. In con-
current related work, we propose active dual su-
pervision schemes for a class of graph-based and
kernel-based dual supervision methods (Sindhwani
et al., 2009).
</bodyText>
<subsectionHeader confidence="0.99742">
2.1 Pooling Multinomials
</subsectionHeader>
<bodyText confidence="0.999905196428571">
The Pooling Multinomials classifier was introduced
by Melville et al. (2009) as an approach to incorpo-
rate prior lexical knowledge into supervised learn-
ing for better sentiment detection. In the context of
sentiment analysis, lexical knowledge is available in
terms of the prior sentiment-polarity of words. From
a dual supervision point of view, this knowledge can
be seen as labeled features, since the lexicon effec-
tively provides associations of a set of words with
the positive or negative class.
Pooling Multinomials classifies unlabeled exam-
ples just as in multinomial Naive Bayes classifica-
tion (McCallum and Nigam, 1998), by predicting
the class with the maximum likelihood, given by
argmaxcjP(cj) ni P(wi|cj); where P(cj) is the
prior probability of class cj, and P(wi|cj) is the
probability of word wi appearing in a document of
class cj. In the absence of background knowledge
about the class distribution, we estimate the class
priors P(cj) solely from the training data. However,
unlike regular Naive Bayes, the conditional prob-
abilities P(wi|cj) are computed using both the la-
beled examples and the labeled features.
Pooling distributions is a general approach for
combining information from multiple sources or ex-
perts; where experts are typically represented in
terms of probability distributions (Clemen and Win-
kler, 1999). Here, we only consider the special case
of combining multinomial distributions from two
sources – namely, the labeled examples and labeled
features. The multinomial parameters of such mod-
els can be easily combined using the linear opin-
ion pool (Clemen and Winkler, 1999), in which
the aggregate probability is given by P(wi|cj) =
αPe(wi|cj) + (1 − α)Pf(wi|cj); where Pe(wi|cj)
and Pf(wi|cj) represent the probability assigned by
using the example labels and feature labels respec-
tively, and α is the weight for combining these dis-
tributions. The weight indicates a level of confi-
dence in each source of information, and Melville
et al. (2009) explore ways of automatically selecting
this weight. However, in order to not confound our
results with the choice of weight-selection mecha-
nism, here we make the simplifying assumption that
the two experts based on instance and feature labels
are equally valuable, and as such set α to 0.5.
To learn a model from the labeled examples we
compute conditionals Pe(wi|cj) based on observed
term frequencies, as in standard Naive Bayes classi-
fication. In addition, for Pooling Multinomials we
need to construct a multinomial model represent-
ing the labeled features in the background knowl-
edge. For this, we assume that the feature-class as-
sociations provided by labeled features are implic-
itly arrived at by human experts by examining many
positive and negative sentiment documents. So we
</bodyText>
<page confidence="0.992393">
50
</page>
<bodyText confidence="0.948758857142857">
attempt to select the parameters Pf(wi|cj) of the
multinomial distributions that would generate such
documents. The exact values of these condition-
als are presented below. Their derivation is not di-
rectly pertinent to the subject of this paper, but can
be found in (Melville et al., 2009).
Given:
</bodyText>
<equation confidence="0.886029">
V – the vocabulary, i.e., set of words in our domain
P – set of words labeled as positive
N – set of words labeled as negative
U – set of unknown words, i.e. V − (N U P)
m – size of vocabulary, i.e. |V|
p – number of positive words, i.e. |P|
n – number of negative words, i.e. |N|
</equation>
<bodyText confidence="0.970182636363636">
All words in the vocabulary can be divided into
three categories – words with a positive label, nega-
tive label, and unknown label. We refer to the prob-
ability of any positive term appearing in a positive
document simply as Pf(w+|+). Similarly, we refer
to the probability of any negative term appearing in a
negative document as Pf(w−|−); and the probabil-
ity of an unknown word in a positive or negative con-
text as Pf(wu|+) and Pf(wu|−) respectively. The
generative model for labeled features can then be de-
fined by:
</bodyText>
<equation confidence="0.999306">
1
Pf(w+|+) = Pf(w−|−) = p + n
1
Pf(w+|−) = Pf(w−|+) = p + n
n(1 − 1/r)
Pf(wu|+) =
(p + n)(m − p − n)
p(1 − 1/r)
Pf(wu|−) =
(p + n)(m − p − n)
</equation>
<bodyText confidence="0.999985">
where, the polarity level, r, is a measure of how
much more likely it is for a positive term to occur
in a positive document compared to a negative term.
The value of r is set to 100 in our experiments, as
done in (Melville et al., 2009).
</bodyText>
<subsectionHeader confidence="0.999304">
2.2 Learning from example vs. feature labels
</subsectionHeader>
<bodyText confidence="0.999993660377359">
Dual supervision makes it possible to learn from la-
beled examples and labeled features simultaneously;
and, as in most supervised learning tasks, one would
expect more labeled data of either form to lead to
more accurate models. In this section we explore the
influence of increased number of instance labels and
feature labels independently, and also in tandem.
For these, and all subsequent experiments, we
use 10-fold cross-validation on the publicly avail-
able data of movie reviews provided by Pang et
al. (2002). This data consists of 1000 positive
and 1000 negative reviews from the Internet Movie
Database; where positive labels were assigned to re-
views that had a rating above 3.5 stars and negative
labels were assigned to ratings of 2 stars and below.
We use a bag-of-words representation of reviews,
where each review is represented by the term fre-
quencies of the 5000 most frequent words across all
reviews, excluding stop-words.
In order to study the effect of increasing number
of labels we need to simulate a human oracle label-
ing data. In the case of examples this is straight-
forward, since all examples in the Movies dataset
have labels. However, in the case of features, we
do not have a gold-standard set of feature labels. So
in order to simulate human responses to queries for
feature labels, we construct a feature oracle in the
following manner. The information gain of words
with respect to the known true class labels in the
dataset is computed using binary feature represen-
tations. Next, out of the 5000 total words, the top
1000 as ranked by information gain are assigned a
label. This label is the class in which the word ap-
pears more frequently. The oracle returns a “dont
know” response for the remaining words. Thus, this
oracle simulates a human domain expert who is able
to recognize and label the most relevant task-specific
words, and also reject a word that falls below the rel-
evance threshold. For instance, in sentiment classi-
fication, we would expect a “don’t know” response
for non-polar words.
We ran experiments beginning with a classifier
provided with labels for 10 randomly selected in-
stances and 10 randomly selected features. We then
compare three schemes - Instances-then-features,
Features-then-instances, and Passive Interleaving.
As the name suggests, Instances-then-features, is
provided labels for randomly selected instances until
all instances have been labeled, and then switches to
labeling features. Similarly, Features-then-instances
acquires labels for randomly selected features first
and then switches to getting instance labels. In
Passive Interleaving we probabilistically switch be-
</bodyText>
<equation confidence="0.863153666666667">
1
X
r
</equation>
<page confidence="0.935606">
51
</page>
<bodyText confidence="0.993816204081633">
Accuracy
tween issuing queries for randomly chosen instance
and feature labels. In particular, at each step we
choose to query for an instance with probability
0.36, otherwise we query for a feature label. The
instance-query rate of 0.36 is selected based on the
ratio of available instances (1800) to available fea-
tures (5000). The results of these learning curves are
presented in Fig. 1. Note that the x-axis in the figure
corresponds to the number of queries issued. As dis-
cussed earlier, in the case of features, the oracle may
respond to a query with a class label or may issue
a “don’t know” response, indicating that no label is
available. As such, the number of feature-queries
on the x-axis does not correspond to the number
of actual known feature labels. We would expect
that on average 1 in 5 feature-label queries prompts
a response from the feature oracle that results in a
known feature label being provided.
At the end of the learning curves, each method
has labels for all available instances and features;
and as such, the last points of all three curves are
identical. The results show that fixing the number
of labeled features, and increasing the number of la-
beled instances steadily improves classification ac-
curacy. This is what one would expect from tra-
ditional supervised learning curves. More interest-
ingly, the results also indicate that we can fix the
number of instances, and improve accuracy by la-
beling more features. Finally, results on Passive In-
terleaving show that, though both feature labels and
example labels are beneficial by themselves, dual su-
pervision which exploits the interaction of examples
and features does in fact benefit from acquiring both
types of labels concurrently.
For all results above, we are selecting instances
and/or features to be labeled uniformly at random.
Based on previous work in active learning one would
expect that we can select instances to be labeled
more efficiently, by having the learner decide which
instances it is most likely to benefit from. The results
in this section suggests that actively selecting fea-
tures to be labeled may also be beneficial. Further-
more, the Passive Interleaving results suggest that an
ideal active dual supervision scheme would actively
select both instances and features for labeling. We
begin by exploring active learning for feature labels
in the next section, and then consider the simultane-
ous selection of instances and features in Sec. 4.
</bodyText>
<figure confidence="0.9753345">
0 1000 2000 3000 4000 5000 6000 7000
Number of queries
</figure>
<figureCaption confidence="0.9732065">
Figure 1: Comparing the effect of instance and feature
label acquisition in dual supervision.
</figureCaption>
<sectionHeader confidence="0.955511" genericHeader="method">
3 Acquiring feature labels
</sectionHeader>
<bodyText confidence="0.9999691">
Traditional active learning has primarily focused on
selecting unlabeled instances to be labeled. The
dual-supervision setting now provides us with an ad-
ditional dimension to active learning, where labels
may also be acquired for features. In this section
we look at the novel task of active learning applied
only to feature-label acquisition. In Section 4 we
study the more general task of active dual supervi-
sion, where both instance and feature labels may be
acquired concurrently.
</bodyText>
<subsectionHeader confidence="0.995881">
3.1 Feature uncertainty vs. certainty
</subsectionHeader>
<bodyText confidence="0.999941666666667">
A very common approach to active learning for in-
stances is Uncertainty Sampling (Lewis and Catlett,
1994). In this approach we acquire labels for in-
stances that the current model is most uncertain
about. Uncertainty Sampling is founded on the
heuristic that uncertain instances are close to the cur-
rent classification boundary, and acquiring the cor-
rect labels for them are likely to help refine the loca-
tion of this boundary. Despite its simplicity, Uncer-
tainty Sampling is usually quite effective in practice;
which raises the question of whether one can apply
the same principle to feature-label acquisition. In
this case, we want to select unlabeled features that
the current model is most uncertain about.
Much like instance uncertainty, feature uncer-
tainty can be measured in different ways, depend-
ing on the underlying method used for dual super-
vision. For instance, if the learner produces a lin-
</bodyText>
<figure confidence="0.991361333333333">
90
85
80
75
70
65
60
55
50
Instances-then-features
Features-then-instances
Passive Interleaving
</figure>
<page confidence="0.987725">
52
</page>
<bodyText confidence="0.999930285714286">
ear classifier as in (Sindhwani and Melville, 2008),
we could use the magnitude of the weights on the
features as a measure of uncertainty – where lower
weights indicate less certainty. Since Pooling Multi-
nomials builds a multinomial Naive Bayes model,
we can directly use the model’s conditional proba-
bilities of each word (feature) given a class.
For ease of exposition we refer to the two classes
in binary classification as postive (+) and negative
(-), without loss of generality. Given the probabili-
ties of word f belonging to the positive and negative
class, P(f|+) and P(f|−), we can determine the
uncertainty of a feature using the absolute value of
the log-odds ratio, i.e.,
</bodyText>
<equation confidence="0.9841375">
abs I log( P(f |+)11 (1)
\P(f|−)))
</equation>
<bodyText confidence="0.99997352">
The smaller this value, the more uncertain the model
is about the feature’s class association. In every it-
eration of active learning we can select the features
with the lowest certainty scores. We refer to this ap-
proach as Feature Uncertainty.
Though Uncertainty Sampling for features seems
like an appealing notion, it may not lead to better
models. If a classifier is uncertain about a feature,
it may have insufficient information about this fea-
ture and may indeed benefit from learning its la-
bel. However, it is also quite likely that a feature
has a low certainty score because it does not carry
much discriminative information about the classes.
In the context of sentiment detection, one would ex-
pect that neutral/non-polar words will appear to be
uncertain words. For example, words such as “the”
which are unlikely to help in discriminating between
classes, are also likely to be considered the most un-
certain. As we shortly report, on the movies dataset,
Feature Uncertainty ends up wasting queries on such
words ending up with performance inferior to ran-
dom feature queries. What works significantly bet-
ter is an alternative strategy which acquires labels
for features in the descending order of the score in
Eq 1. We refer to this approach as Feature Certainty.
</bodyText>
<subsectionHeader confidence="0.990186">
3.2 Expected feature utility
</subsectionHeader>
<bodyText confidence="0.9999893">
The intuition underlying the feature certainty heuris-
tic is that it serves to confirm or correct the orienta-
tion of model probabilities on different words during
the active learning process. One can argue that fea-
ture certainty is also suboptimal in that queries may
be wasted simply confirming confident predictions,
which is of limited utility to the model. An alterna-
tive to using a certainty-based heuristic, is to directly
estimate the expected value of acquiring each fea-
ture label. Such Expected Utility (Estimated Risk
Minimization) approaches have been applied suc-
cessfully to traditional active learning (Roy and Mc-
Callum, 2001), and to active feature-value acquisi-
tion (Melville et al., 2005). In this section we de-
scribe how this Expected Utility framework can be
adapted for feature-label acquisition.
At every step of active learning for features, the
next best feature to label is one that will result in
the highest improvement in classifier performance.
Since the true label of the unlabeled features are
unknown prior to acquisition, it is necessary to es-
timate the potential impact of every feature query
for all possible outcomes.1 Hence, the decision-
theoretic optimal policy is to ask for feature labels
which, once incorporated into the data, will result in
the highest increase in classification performance in
expectation.
If fj is the label of the j-th feature, and qj is the
query for this feature’s label, then the Expected Util-
ity of a feature query qj can be computed as:
</bodyText>
<equation confidence="0.999247333333333">
K
EU(qj) = P(fj = ck)U(fj = ck) (2)
k=1
</equation>
<bodyText confidence="0.999723733333333">
Where P(fj = ck) is the probability that fj will be
labeled with class ck, and U(fj = ck) is the util-
ity to the model of knowing that fj has the label
ck. In practice, the true values of these two quan-
tities are unknown, and the main challenge of any
Expected Utility approach is to accurately estimate
these quantities from the data currently available.
A direct way to estimate the utility of a feature la-
bel to classification, is to measure classification ac-
curacy on the training set of a model built using this
feature label. However, small changes in the model
that may result from a acquiring a single additional
feature label may not be reflected by a change in ac-
curacy. As such, we use a more fine-grained mea-
sure of classifier performance, Log Gain, which is
</bodyText>
<footnote confidence="0.99577">
1In the case of binary polarity classification, the possible
outcomes are a positive or negative label for a queried feature.
</footnote>
<page confidence="0.999342">
53
</page>
<bodyText confidence="0.998172571428571">
computed as follows. For a model induced from a
training set T, let Pˆ(ck|xi) be the probability es-
timated by the model that instance xi belongs to
class ck; and ]l is an indicator function such that
]l(ck,xi) = 1 if ck is the correct class for xi and
]l(ck, xi) = 0, otherwise. Log Gain is then defined
as:
</bodyText>
<equation confidence="0.999387333333333">
K
LG(xi) = − ]l(ck) Pˆ(ck|xi) (3)
k=1
</equation>
<bodyText confidence="0.99994875">
Then the utility of a classifier, U, can be measured
by summing the Log Gain for all instances in the
training set T. A lower value of Log Gain indi-
cates a better classifier performance. For a deeper
discussion of this measure see (Saar-Tsechansky et
al., 2008).
In Eq. 2, apart from the measure of utility, we
also do not know the true probability distribution
of labels for the feature under consideration. This
too can be estimated from the training data, by see-
ing how frequently the word appears in documents
of each class. In a multinomial Naive Bayes model
we already collect these statistics in order to deter-
mine the conditional probability of a class given a
word, i.e. P(fj|ck). We can use these probabilities
to get an estimate of the feature label distribution,
</bodyText>
<equation confidence="0.868341">
Pˆ (fj = ck) = P(fj|ck) EKk=1 P(fj|ck).
</equation>
<bodyText confidence="0.999970740740741">
Given the estimated values of the feature-label
distribution and the utility of a particular feature
query outcome, we can now estimate the Expected
Utility of each unknown feature, and select the fea-
tures with the highest Expected Utility to modeling.
Though theoretically appealing, this approach is
quite computationally intensive if applied to evalu-
ate all unknown features. In the worst case it re-
quires building and evaluating models for each pos-
sible outcome of each unlabeled feature query. If
you have m features and K classes, this approach
requires training O(mK) classifiers. However, the
complexity of the approach can be significantly al-
leviated by only applying Expected Utility evalua-
tion to a sub-sample of all unlabeled features. Given
the large number of features with no true class la-
bels, selecting a sample of available features uni-
formly at random may be sub-optimal. Instead we
select a sample of features based on Feature Cer-
tainty. In particular we select the top 100 unknown
features that the current model is most certain about,
and identify the features in this pool with the highest
Expected Utility. We refer to this approach as Ex-
pected Feature Utility. We use Feature Certainty to
sub-sample the available feature queries, since this
approach is more likely to select features for which
the label is known by the Oracle.
</bodyText>
<subsectionHeader confidence="0.99429">
3.3 Active learning with feature labels
</subsectionHeader>
<bodyText confidence="0.999941466666667">
We ran experiments comparing the three different
active learning approaches described above. In
these, and all subsequent experiments, we begin
with a model trained on 10 labeled features and 100
labeled instances, which were randomly selected.
From our prior efforts of manually labeling such
data, we find this to be a reasonable initial setting.
The experiments in this section focus only on the
selection offeatures to be labeled. So, in each itera-
tion of active learning we select the next 10 feature-
label queries, based on Feature Uncertainty, Feature
Certainty, or Expected Feature Utility. As a baseline,
we also compare to the performance of a model that
selects features uniformly at random. Our results are
presented in Fig. 2.
</bodyText>
<figure confidence="0.8346345">
0 50 100 150 200 250 300 350 400
Number of queries
</figure>
<figureCaption confidence="0.997565">
Figure 2: Comparing different active learning approaches
for acquiring feature labels.
</figureCaption>
<bodyText confidence="0.999544428571429">
The results show that Feature Uncertainty, which
is a direct analog of Uncertainty Sampling, actu-
ally performs worse than random sampling. Many
uncertain features may actually not be very useful
in discriminating between the classes, and selecting
them can be systematically worse than selecting uni-
formly at random. However, the converse approach
</bodyText>
<figure confidence="0.987428818181818">
Accuracy
75
70
65
60
55
50
Expected Feature Utility
Feature Certainty
Random Feature
Feature Uncertainty
</figure>
<page confidence="0.995184">
54
</page>
<bodyText confidence="0.999763611111111">
of Feature Certainty does remarkably well. This
may be because polarized words are better for learn-
ing, but it is also likely that querying for such words
increases the likelihood of selecting one whose label
is known to the oracle.
The results on Expected Feature Utility show that
estimating the expected impact of potential labels
for features does in fact perform much better than
feature certainty. The results confirm that despite
our crude estimations in Eq. 2, Expected Feature
Utility is an effective approach to active learning of
feature labels. Furthermore, we demonstrate that by
applying the approach to only a small sub-sample of
certain features, we are able to make this method
computationally feasible to use in practice. In-
creasing the size of the sample of candidate feature
queries is likely to improve performance, at the cost
of increased time in selecting queries.
</bodyText>
<sectionHeader confidence="0.962545" genericHeader="method">
4 Active dual supervision
</sectionHeader>
<bodyText confidence="0.999904351851852">
In the previous section we demonstrated that ac-
tively selecting informative features to be labeled is
significantly better than random selection. In this
section, we look at the complementary task of se-
lecting instances to be labeled, and combined active
learning for both forms of supervision.
Selecting unlabeled examples for learning has
been a well-studied problem, and we use Uncer-
tainty Sampling (Lewis and Catlett, 1994), which
has been shown to be a computationally efficient
and effective approach in the literature. In particular
we select unlabeled examples to be labeled in order
of decreasing uncertainty, where uncertainty is mea-
sured in terms of the margin, as done in (Melville
and Mooney, 2004). The margin on an unlabeled ex-
ample is defined as the absolute difference between
the class probabilities predicted by the classifier for
the given example, i.e., |P(+|x) − P(−|x)|. We re-
fer to the selection of instances based on this uncer-
tainty as Instance Uncertainty, in order to distinguish
it from Feature Uncertainty.
We ran experiments as before, comparing selec-
tion of instances using Instance Uncertainty and se-
lection of features using Expected Feature Utility.
In addition, we also combine these to methods by
interleaving feature and instance selection. In par-
ticular, we first order instances in decreasing order
of uncertainty, and features in terms of decreasing
Expected Feature Utility. We then probabilistically
select instances or features from the top of these
lists, where, as before, the probability of selecting
an instance is 0.36. Recall that this probability cor-
responds to the ratio of available instances (1800)
and features (5000). We refer to this approach as Ac-
tive Interleaving, in contrast to Passive Interleaving,
which we also present as a baseline. Recall that Pas-
sive Interleaving corresponds to probabilistically in-
terleaving queries for randomly chosen, not actively
chosen, examples and features. Our results are pre-
sented in Fig. 3.
We observe that, Instance Uncertainty performs
better than Passive Interleaving, which in turn is bet-
ter than random selection of only instances or fea-
tures – as seen in Fig. 1. However, effectively se-
lecting features labels, via Expected Feature Util-
ity, does even better than actively selecting only in-
stances. Finally, selecting instance and features si-
multaneously via Active Interleaving performs bet-
ter than active learning of features or instances sep-
arately. Active Interleaving is indeed very effective,
reaching an accuracy of 77% with only 500 queries,
while Passive Interleaving requires more than 4000
queries to reach the same performance – as evi-
denced by Fig. 1
</bodyText>
<figure confidence="0.8528905">
0 50 100 150 200 250 300 350 400
Number of queries
</figure>
<figureCaption confidence="0.972177">
Figure 3: Comparing Active Interleaving to alternative
label acquisition strategies.
</figureCaption>
<sectionHeader confidence="0.999414" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.936297">
Active learning in the context of dual supervision
models is a new area of research with very little prior
</bodyText>
<figure confidence="0.994119333333333">
Accuracy
80
75
70
65
60
55
50
Active Interleaving
Expected Feature Utility
Instance Uncertainty
Passive Interleaving
</figure>
<page confidence="0.994015">
55
</page>
<bodyText confidence="0.999979">
work, to the best of our knowledge. Most prior work
has focused on pooled-based active learning, where
examples from an unlabeled pool are selected for la-
beling (Cohn et al., 1994; Tong and Koller, 2000). In
contrast, active feature-value acquisition (Melville
et al., 2005) and budgeted learning (Lizotte et al.,
2003) focus on estimating the value of acquiring
missing features, but do not deal with the task of
learning from feature labels. In contrast, Raghavan
and Allan (2007) and Raghavan et al. (2006) study
the problem of tandem learning where they combine
uncertainty sampling for instances along with co-
occurence based interactive feature selection. God-
bole et al. (2004) propose notions of feature uncer-
tainty and incorporate the acquired feature labels,
into learning by creating one-term mini-documents.
Learning from labeled examples and features via
dual supervision, is itself a new area of research.
Sindhwani et al. (2008) use a kernel-based frame-
work to build dual supervision into co-clustering
models. Sindhwani and Melville (2008) apply sim-
ilar ideas for graph-based sentiment analysis. There
have also been previous attempts at using only fea-
ture supervision, mostly along with unlabeled doc-
uments. Much of this work (Schapire et al., 2002;
Wu and Srihari, 2004; Liu et al., 2004; Dayanik
et al., 2006) has focused on using labeled features
to generate pseudo-labeled examples that are then
used with well-known models. In contrast, Druck
et al. (2008) constrain the outputs of a multinomial
logistic regression model to match certain reference
distributions associated with labeled features.
</bodyText>
<sectionHeader confidence="0.986942" genericHeader="evaluation">
6 Perspectives and future work
</sectionHeader>
<bodyText confidence="0.997633604166667">
Though Active Interleaving is a very effective ap-
proach to active dual supervision, there is still a lot
of room for improvement. Firstly, Active Interleav-
ing relies on Uncertainty Sampling for the selection
of instances. Though Uncertainty Sampling has the
advantage of being fast and effective, there exist ap-
proaches that lead to better models with fewer ex-
amples – usually at the cost of computation time.
One such method, estimating error reduction (Roy
and McCallum, 2001), is a direct analog of Ex-
pected Feature Utility applied to instance selection.
One would expect that an improvement in instance
selection, should directly improve any method that
combines instance and feature label selection. Sec-
ondly, Active Interleaving uses the simple approach
of probabilistically choosing to select an instance or
feature for each subsequent query. However, a more
intelligent active scheme should be able to assess if
an instance or feature would be more beneficial at
each step. Furthermore, we do not currently con-
sider the cost of acquiring labels. Presumably la-
beling a feature versus labeling an instance could
incur very different costs – which could be mone-
tary costs or time taken for each annotation. Fortu-
nately, the Expected Utility method is very flexible,
and allows us to address all these issues within a sin-
gle framework. We can specifically estimate the ex-
pected utility of different forms of annotation, per
unit cost. For instance, Provost et al. (2007) use
such an approach to estimate the utility of acquir-
ing class labels and feature values (not labels) per
unit cost, within one unified framework. A similar
method can be applied for a holistic approach to ac-
tive dual supervision, where the Expected Utility of
an instance or feature label query q, can be computed
as EU(q) = EKk=1 P(q = ck)U( q=ck); where ωq is
ωq
cost of the query q, and utility U can be computed as
in Eq. 3. By evaluating instances and features on the
same scale, and by measuring utility per unit cost of
acquisition, such a framework should enable us to
handle the trade-off between the costs and benefits
of the different types of acquisitions. The primary
challenge in the success of this approach is to accu-
rately and efficiently estimate the different quantities
in the equation above, using only the training data
currently available. These are directions for future
exploration.
</bodyText>
<sectionHeader confidence="0.999347" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9994985">
This paper is a preliminary foray into active dual su-
pervision. We have demonstrated that not only is
combining example and feature labels beneficial for
modeling, but that actively selecting the most infor-
mative examples and features for labeling can sig-
nificantly reduce the burden of annotating such data.
In future work, we would like to explore more effec-
tive solutions to the problem, and also to corroborate
our results on a larger number of datasets and under
different experimental settings.
</bodyText>
<page confidence="0.995408">
56
</page>
<sectionHeader confidence="0.993864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999787313253012">
R. T. Clemen and R. L. Winkler. 1999. Combining prob-
ability distributions from experts in risk analysis. Risk
Analysis, 19:187–203.
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201–221.
Aynur Dayanik, David D. Lewis, David Madigan,
Vladimir Menkov, and Alexander Genkin. 2006. Con-
structing informative prior distributions from domain
knowledge in text classification. In SIGIR.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In SIGIR.
S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.
2004. Document classification through interactive su-
pervision of document and term labels. In PKDD.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning. In
Proc. of 11th Intl. Conf. on Machine Learning (ICML-
94), pages 148–156, San Francisco, CA, July. Morgan
Kaufmann.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip Yu. 2004.
Text classification by labeling words. In AAAI.
Dan Lizotte, Omid Madani, and Russell Greiner. 2003.
Budgeted learning of naive-Bayes classifiers. In UAI.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In AAAI Workshop on Text Categorization.
Prem Melville and Raymond J. Mooney. 2004. Diverse
ensembles for active learning. In Proc. of 21st Intl.
Conf. on Machine Learning (ICML-2004), pages 584–
591, Banff, Canada, July.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond Mooney. 2005. An expected utility ap-
proach to active feature-value acquisition. In ICDM.
Prem Melville, Wojciech Gryc, and Richard Lawrence.
2009. Sentiment analysis of blogs by combining lexi-
cal knowledge with text classification. In KDD.
Bo Pang and Lilian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Informa-
tion Retrieval: Vol. 2: No 1, pp 1-135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In EMNLP.
Foster Provost, Prem Melville, and Maytal Saar-
Tsechansky. 2007. Data acquisition and cost-effective
predictive modeling: Targeting offers for electronic
commerce. In ICEC ’07: Proceedings of the ninth in-
ternational conference on Electronic commerce.
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning with feedback on features and in-
stances. J. Mach. Learn. Res., 7:1655–1686.
H. Raghavan, O. Madani, and R. Jones. 2007. An inter-
active algorithm for asking and incorporating feature
feedback into support vector machines. In SIGIR.
Nicholas Roy and Andrew McCallum. 2001. Toward
optimal active learning through sampling estimation of
error reduction. In ICML.
Maytal Saar-Tsechansky, Prem Melville, and Foster
Provost. 2008. Active feature-value acquisition. In
Management Science.
Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and
Narendra Gupta. 2002. Incorporating prior knowl-
edge into boosting. In ICML.
Vikas Sindhwani and Prem Melville. 2008. Document-
word co-regularization for semi-supervised sentiment
analysis. In ICDM.
Vikas Sindhwani, Jianying Hu, and Alexandra Mo-
jsilovic. 2008. Regularized co-clustering with dual
supervision. In NIPS.
Vikas Sindhwani, Prem Melville, and Richard Lawrence.
2009. Uncertainty sampling and transductive experi-
mental design for active dual supervision. In ICML.
Simon Tong and Daphne Koller. 2000. Support vec-
tor machine active learning with applications to text
classification. In Proc. of 17th Intl. Conf. on Machine
Learning (ICML-2000).
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledge with weighted margin support vector
machines. In KDD.
O. F. Zaidan and J. Eisner. 2008. Modeling annotators:
A generative approach to learning from annotator ra-
tionales. In EMNLP.
</reference>
<page confidence="0.99914">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.456442">
<title confidence="0.9963055">Active Dual Supervision: Reducing the of Annotating Examples and Features</title>
<author confidence="0.881863">Prem</author>
<affiliation confidence="0.870647">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.997617">pmelvil@us.ibm.com</email>
<author confidence="0.838353">Vikas</author>
<affiliation confidence="0.8943245">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.998564">vsindhw@us.ibm.com</email>
<abstract confidence="0.997646045454546">When faced with the task of building machine learning or NLP models, it is often worthwhile to turn to active learning to obtain human annotations at minimal costs. Traditional active learning schemes query a human for labels of intelligently chosen examples. However, human effort can also be expended in collecting alternative forms of annotations. For example, one may attempt to learn a text classifier by labeling class-indicating words, instead of, or in addition to, documents. Learning from two different kinds of supervision brings a new, unexplored dimension to the problem of active learning. In this paper, we demonstrate the value of such active dual supervision in the context of sentiment analysis. We show how interleaving queries for both documents and words significantly reduces human effort – more than what is possible through traditional one-dimensional active learning, or by passive combinations of supervisory inputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R T Clemen</author>
<author>R L Winkler</author>
</authors>
<title>Combining probability distributions from experts in risk analysis. Risk Analysis,</title>
<date>1999</date>
<pages>19--187</pages>
<contexts>
<context position="6717" citStr="Clemen and Winkler, 1999" startWordPosition="1035" endWordPosition="1039">cj); where P(cj) is the prior probability of class cj, and P(wi|cj) is the probability of word wi appearing in a document of class cj. In the absence of background knowledge about the class distribution, we estimate the class priors P(cj) solely from the training data. However, unlike regular Naive Bayes, the conditional probabilities P(wi|cj) are computed using both the labeled examples and the labeled features. Pooling distributions is a general approach for combining information from multiple sources or experts; where experts are typically represented in terms of probability distributions (Clemen and Winkler, 1999). Here, we only consider the special case of combining multinomial distributions from two sources – namely, the labeled examples and labeled features. The multinomial parameters of such models can be easily combined using the linear opinion pool (Clemen and Winkler, 1999), in which the aggregate probability is given by P(wi|cj) = αPe(wi|cj) + (1 − α)Pf(wi|cj); where Pe(wi|cj) and Pf(wi|cj) represent the probability assigned by using the example labels and feature labels respectively, and α is the weight for combining these distributions. The weight indicates a level of confidence in each sourc</context>
</contexts>
<marker>Clemen, Winkler, 1999</marker>
<rawString>R. T. Clemen and R. L. Winkler. 1999. Combining probability distributions from experts in risk analysis. Risk Analysis, 19:187–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>L Atlas</author>
<author>R Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="28969" citStr="Cohn et al., 1994" startWordPosition="4724" endWordPosition="4727">ries to reach the same performance – as evidenced by Fig. 1 0 50 100 150 200 250 300 350 400 Number of queries Figure 3: Comparing Active Interleaving to alternative label acquisition strategies. 5 Related work Active learning in the context of dual supervision models is a new area of research with very little prior Accuracy 80 75 70 65 60 55 50 Active Interleaving Expected Feature Utility Instance Uncertainty Passive Interleaving 55 work, to the best of our knowledge. Most prior work has focused on pooled-based active learning, where examples from an unlabeled pool are selected for labeling (Cohn et al., 1994; Tong and Koller, 2000). In contrast, active feature-value acquisition (Melville et al., 2005) and budgeted learning (Lizotte et al., 2003) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learnin</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>D. Cohn, L. Atlas, and R. Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aynur Dayanik</author>
<author>David D Lewis</author>
<author>David Madigan</author>
<author>Vladimir Menkov</author>
<author>Alexander Genkin</author>
</authors>
<title>Constructing informative prior distributions from domain knowledge in text classification.</title>
<date>2006</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="30116" citStr="Dayanik et al., 2006" startWordPosition="4902" endWordPosition="4905">ture uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Though Active Interleaving is a very effective approach to active dual supervision, there is still a lot of room for improvement. Firstly, Active Interleaving relies on Uncertainty Sampling for the selection of instances. Though Uncertainty Sampling has the advantage of being f</context>
</contexts>
<marker>Dayanik, Lewis, Madigan, Menkov, Genkin, 2006</marker>
<rawString>Aynur Dayanik, David D. Lewis, David Madigan, Vladimir Menkov, and Alexander Genkin. 2006. Constructing informative prior distributions from domain knowledge in text classification. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="3293" citStr="Druck et al., 2008" startWordPosition="509" endWordPosition="512">ing”) may be labeled negative. This kind of annotation requires a human to condense prior linguistic experience with a word into a sentiment label that reflects the net emotion that the word evokes. We refer to the general setting of learning from both labels on examples and features as dual supervision. This setting arises more broadly in tasks where in addition to labeled documents, it is frequently possible to provide domain knowledge in the form of words, or phrases (Zaidan and Eisner, 2008) or even more sophisticated linguistic features, that associate strongly with a class. Recent work (Druck et al., 2008; Sindhwani and Melville, 2008) has demonstrated that the presence of word supervision can greatly reduce the number of labeled documents Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics required to build high quality text classifiers. In general, these two sources of supervision are not mutually redundant, and have different annotation costs, human response quality, and degrees of utility towards learning a dual supervision model. This leads naturally to the pro</context>
<context position="30266" citStr="Druck et al. (2008)" startWordPosition="4925" endWordPosition="4928">ures via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Though Active Interleaving is a very effective approach to active dual supervision, there is still a lot of room for improvement. Firstly, Active Interleaving relies on Uncertainty Sampling for the selection of instances. Though Uncertainty Sampling has the advantage of being fast and effective, there exist approaches that lead to better models with fewer examples – usually at the cost of computation time. One such method, e</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>G. Druck, G. Mann, and A. McCallum. 2008. Learning from labeled features using generalized expectation criteria. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Godbole</author>
<author>A Harpale</author>
<author>S Sarawagi</author>
<author>S Chakrabarti</author>
</authors>
<title>Document classification through interactive supervision of document and term labels.</title>
<date>2004</date>
<booktitle>In PKDD.</booktitle>
<contexts>
<context position="29472" citStr="Godbole et al. (2004)" startWordPosition="4801" endWordPosition="4805">ed on pooled-based active learning, where examples from an unlabeled pool are selected for labeling (Cohn et al., 1994; Tong and Koller, 2000). In contrast, active feature-value acquisition (Melville et al., 2005) and budgeted learning (Lizotte et al., 2003) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2</context>
</contexts>
<marker>Godbole, Harpale, Sarawagi, Chakrabarti, 2004</marker>
<rawString>S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti. 2004. Document classification through interactive supervision of document and term labels. In PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proc. of 11th Intl. Conf. on Machine Learning (ICML94),</booktitle>
<pages>148--156</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA,</location>
<contexts>
<context position="15584" citStr="Lewis and Catlett, 1994" startWordPosition="2518" endWordPosition="2521">itional active learning has primarily focused on selecting unlabeled instances to be labeled. The dual-supervision setting now provides us with an additional dimension to active learning, where labels may also be acquired for features. In this section we look at the novel task of active learning applied only to feature-label acquisition. In Section 4 we study the more general task of active dual supervision, where both instance and feature labels may be acquired concurrently. 3.1 Feature uncertainty vs. certainty A very common approach to active learning for instances is Uncertainty Sampling (Lewis and Catlett, 1994). In this approach we acquire labels for instances that the current model is most uncertain about. Uncertainty Sampling is founded on the heuristic that uncertain instances are close to the current classification boundary, and acquiring the correct labels for them are likely to help refine the location of this boundary. Despite its simplicity, Uncertainty Sampling is usually quite effective in practice; which raises the question of whether one can apply the same principle to feature-label acquisition. In this case, we want to select unlabeled features that the current model is most uncertain a</context>
<context position="26174" citStr="Lewis and Catlett, 1994" startWordPosition="4278" endWordPosition="4281">e to use in practice. Increasing the size of the sample of candidate feature queries is likely to improve performance, at the cost of increased time in selecting queries. 4 Active dual supervision In the previous section we demonstrated that actively selecting informative features to be labeled is significantly better than random selection. In this section, we look at the complementary task of selecting instances to be labeled, and combined active learning for both forms of supervision. Selecting unlabeled examples for learning has been a well-studied problem, and we use Uncertainty Sampling (Lewis and Catlett, 1994), which has been shown to be a computationally efficient and effective approach in the literature. In particular we select unlabeled examples to be labeled in order of decreasing uncertainty, where uncertainty is measured in terms of the margin, as done in (Melville and Mooney, 2004). The margin on an unlabeled example is defined as the absolute difference between the class probabilities predicted by the classifier for the given example, i.e., |P(+|x) − P(−|x)|. We refer to the selection of instances based on this uncertainty as Instance Uncertainty, in order to distinguish it from Feature Unc</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David D. Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proc. of 11th Intl. Conf. on Machine Learning (ICML94), pages 148–156, San Francisco, CA, July. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Xiaoli Li</author>
<author>Wee Sun Lee</author>
<author>Philip Yu</author>
</authors>
<title>Text classification by labeling words.</title>
<date>2004</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="30093" citStr="Liu et al., 2004" startWordPosition="4898" endWordPosition="4901">ose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Though Active Interleaving is a very effective approach to active dual supervision, there is still a lot of room for improvement. Firstly, Active Interleaving relies on Uncertainty Sampling for the selection of instances. Though Uncertainty Sampling has t</context>
</contexts>
<marker>Liu, Li, Lee, Yu, 2004</marker>
<rawString>Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip Yu. 2004. Text classification by labeling words. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Lizotte</author>
<author>Omid Madani</author>
<author>Russell Greiner</author>
</authors>
<title>Budgeted learning of naive-Bayes classifiers.</title>
<date>2003</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="29109" citStr="Lizotte et al., 2003" startWordPosition="4744" endWordPosition="4747">e Interleaving to alternative label acquisition strategies. 5 Related work Active learning in the context of dual supervision models is a new area of research with very little prior Accuracy 80 75 70 65 60 55 50 Active Interleaving Expected Feature Utility Instance Uncertainty Passive Interleaving 55 work, to the best of our knowledge. Most prior work has focused on pooled-based active learning, where examples from an unlabeled pool are selected for labeling (Cohn et al., 1994; Tong and Koller, 2000). In contrast, active feature-value acquisition (Melville et al., 2005) and budgeted learning (Lizotte et al., 2003) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. S</context>
</contexts>
<marker>Lizotte, Madani, Greiner, 2003</marker>
<rawString>Dan Lizotte, Omid Madani, and Russell Greiner. 2003. Budgeted learning of naive-Bayes classifiers. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI Workshop on Text Categorization.</booktitle>
<contexts>
<context position="6006" citStr="McCallum and Nigam, 1998" startWordPosition="926" endWordPosition="929">Pooling Multinomials classifier was introduced by Melville et al. (2009) as an approach to incorporate prior lexical knowledge into supervised learning for better sentiment detection. In the context of sentiment analysis, lexical knowledge is available in terms of the prior sentiment-polarity of words. From a dual supervision point of view, this knowledge can be seen as labeled features, since the lexicon effectively provides associations of a set of words with the positive or negative class. Pooling Multinomials classifies unlabeled examples just as in multinomial Naive Bayes classification (McCallum and Nigam, 1998), by predicting the class with the maximum likelihood, given by argmaxcjP(cj) ni P(wi|cj); where P(cj) is the prior probability of class cj, and P(wi|cj) is the probability of word wi appearing in a document of class cj. In the absence of background knowledge about the class distribution, we estimate the class priors P(cj) solely from the training data. However, unlike regular Naive Bayes, the conditional probabilities P(wi|cj) are computed using both the labeled examples and the labeled features. Pooling distributions is a general approach for combining information from multiple sources or ex</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive Bayes text classification. In AAAI Workshop on Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Melville</author>
<author>Raymond J Mooney</author>
</authors>
<title>Diverse ensembles for active learning.</title>
<date>2004</date>
<booktitle>In Proc. of 21st Intl. Conf. on Machine Learning (ICML-2004),</booktitle>
<pages>584--591</pages>
<location>Banff, Canada,</location>
<contexts>
<context position="26458" citStr="Melville and Mooney, 2004" startWordPosition="4324" endWordPosition="4327">to be labeled is significantly better than random selection. In this section, we look at the complementary task of selecting instances to be labeled, and combined active learning for both forms of supervision. Selecting unlabeled examples for learning has been a well-studied problem, and we use Uncertainty Sampling (Lewis and Catlett, 1994), which has been shown to be a computationally efficient and effective approach in the literature. In particular we select unlabeled examples to be labeled in order of decreasing uncertainty, where uncertainty is measured in terms of the margin, as done in (Melville and Mooney, 2004). The margin on an unlabeled example is defined as the absolute difference between the class probabilities predicted by the classifier for the given example, i.e., |P(+|x) − P(−|x)|. We refer to the selection of instances based on this uncertainty as Instance Uncertainty, in order to distinguish it from Feature Uncertainty. We ran experiments as before, comparing selection of instances using Instance Uncertainty and selection of features using Expected Feature Utility. In addition, we also combine these to methods by interleaving feature and instance selection. In particular, we first order in</context>
</contexts>
<marker>Melville, Mooney, 2004</marker>
<rawString>Prem Melville and Raymond J. Mooney. 2004. Diverse ensembles for active learning. In Proc. of 21st Intl. Conf. on Machine Learning (ICML-2004), pages 584– 591, Banff, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Melville</author>
<author>Maytal Saar-Tsechansky</author>
<author>Foster Provost</author>
<author>Raymond Mooney</author>
</authors>
<title>An expected utility approach to active feature-value acquisition.</title>
<date>2005</date>
<booktitle>In ICDM.</booktitle>
<contexts>
<context position="19207" citStr="Melville et al., 2005" startWordPosition="3109" endWordPosition="3112">firm or correct the orientation of model probabilities on different words during the active learning process. One can argue that feature certainty is also suboptimal in that queries may be wasted simply confirming confident predictions, which is of limited utility to the model. An alternative to using a certainty-based heuristic, is to directly estimate the expected value of acquiring each feature label. Such Expected Utility (Estimated Risk Minimization) approaches have been applied successfully to traditional active learning (Roy and McCallum, 2001), and to active feature-value acquisition (Melville et al., 2005). In this section we describe how this Expected Utility framework can be adapted for feature-label acquisition. At every step of active learning for features, the next best feature to label is one that will result in the highest improvement in classifier performance. Since the true label of the unlabeled features are unknown prior to acquisition, it is necessary to estimate the potential impact of every feature query for all possible outcomes.1 Hence, the decisiontheoretic optimal policy is to ask for feature labels which, once incorporated into the data, will result in the highest increase in</context>
<context position="29064" citStr="Melville et al., 2005" startWordPosition="4737" endWordPosition="4740">00 Number of queries Figure 3: Comparing Active Interleaving to alternative label acquisition strategies. 5 Related work Active learning in the context of dual supervision models is a new area of research with very little prior Accuracy 80 75 70 65 60 55 50 Active Interleaving Expected Feature Utility Instance Uncertainty Passive Interleaving 55 work, to the best of our knowledge. Most prior work has focused on pooled-based active learning, where examples from an unlabeled pool are selected for labeling (Cohn et al., 1994; Tong and Koller, 2000). In contrast, active feature-value acquisition (Melville et al., 2005) and budgeted learning (Lizotte et al., 2003) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual sup</context>
</contexts>
<marker>Melville, Saar-Tsechansky, Provost, Mooney, 2005</marker>
<rawString>Prem Melville, Maytal Saar-Tsechansky, Foster Provost, and Raymond Mooney. 2005. An expected utility approach to active feature-value acquisition. In ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Melville</author>
<author>Wojciech Gryc</author>
<author>Richard Lawrence</author>
</authors>
<title>Sentiment analysis of blogs by combining lexical knowledge with text classification.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="5168" citStr="Melville et al., 2009" startWordPosition="798" endWordPosition="801">mples, each represented by a set of feature values and a class label. In dual supervision we consider an additional aspect, by way of labels of features, which convey prior knowledge on associations of features to particular classes. Since we deal only with text classification in this paper, all features represent term-frequencies of words, and as such we use feature and word interchangeably. The active learning schemes we explore in this paper are broadly applicable to any learner that can support dual supervision, but here we focus on active learning for the Pooling Multinomials classifier (Melville et al., 2009) described below. In concurrent related work, we propose active dual supervision schemes for a class of graph-based and kernel-based dual supervision methods (Sindhwani et al., 2009). 2.1 Pooling Multinomials The Pooling Multinomials classifier was introduced by Melville et al. (2009) as an approach to incorporate prior lexical knowledge into supervised learning for better sentiment detection. In the context of sentiment analysis, lexical knowledge is available in terms of the prior sentiment-polarity of words. From a dual supervision point of view, this knowledge can be seen as labeled featur</context>
<context position="7361" citStr="Melville et al. (2009)" startWordPosition="1140" endWordPosition="1143">er the special case of combining multinomial distributions from two sources – namely, the labeled examples and labeled features. The multinomial parameters of such models can be easily combined using the linear opinion pool (Clemen and Winkler, 1999), in which the aggregate probability is given by P(wi|cj) = αPe(wi|cj) + (1 − α)Pf(wi|cj); where Pe(wi|cj) and Pf(wi|cj) represent the probability assigned by using the example labels and feature labels respectively, and α is the weight for combining these distributions. The weight indicates a level of confidence in each source of information, and Melville et al. (2009) explore ways of automatically selecting this weight. However, in order to not confound our results with the choice of weight-selection mechanism, here we make the simplifying assumption that the two experts based on instance and feature labels are equally valuable, and as such set α to 0.5. To learn a model from the labeled examples we compute conditionals Pe(wi|cj) based on observed term frequencies, as in standard Naive Bayes classification. In addition, for Pooling Multinomials we need to construct a multinomial model representing the labeled features in the background knowledge. For this,</context>
<context position="9624" citStr="Melville et al., 2009" startWordPosition="1554" endWordPosition="1557"> any negative term appearing in a negative document as Pf(w−|−); and the probability of an unknown word in a positive or negative context as Pf(wu|+) and Pf(wu|−) respectively. The generative model for labeled features can then be defined by: 1 Pf(w+|+) = Pf(w−|−) = p + n 1 Pf(w+|−) = Pf(w−|+) = p + n n(1 − 1/r) Pf(wu|+) = (p + n)(m − p − n) p(1 − 1/r) Pf(wu|−) = (p + n)(m − p − n) where, the polarity level, r, is a measure of how much more likely it is for a positive term to occur in a positive document compared to a negative term. The value of r is set to 100 in our experiments, as done in (Melville et al., 2009). 2.2 Learning from example vs. feature labels Dual supervision makes it possible to learn from labeled examples and labeled features simultaneously; and, as in most supervised learning tasks, one would expect more labeled data of either form to lead to more accurate models. In this section we explore the influence of increased number of instance labels and feature labels independently, and also in tandem. For these, and all subsequent experiments, we use 10-fold cross-validation on the publicly available data of movie reviews provided by Pang et al. (2002). This data consists of 1000 positive</context>
</contexts>
<marker>Melville, Gryc, Lawrence, 2009</marker>
<rawString>Prem Melville, Wojciech Gryc, and Richard Lawrence. 2009. Sentiment analysis of blogs by combining lexical knowledge with text classification. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval:</booktitle>
<volume>2</volume>
<pages>1--135</pages>
<contexts>
<context position="1355" citStr="Pang and Lee, 2008" startWordPosition="203" endWordPosition="206">ddition to, documents. Learning from two different kinds of supervision brings a new, unexplored dimension to the problem of active learning. In this paper, we demonstrate the value of such active dual supervision in the context of sentiment analysis. We show how interleaving queries for both documents and words significantly reduces human effort – more than what is possible through traditional one-dimensional active learning, or by passive combinations of supervisory inputs. 1 Introduction As a canonical running example for the theme of this paper, consider the problem of sentiment analysis (Pang and Lee, 2008). Given a piece of text as input, the desired output is a polarity score that indicates whether this text expresses a positive or negative opinion towards a topic of interest. From a machine learning viewpoint, this problem may be posed as a typical binary text classification task. Sentiment, however, is often conveyed with subtle linguistic mechanisms such as sarcasm, negation and the use of highly domain-specific and contextual 49 cues. This brings a multi-disciplinary flavor to the problem, drawing interest from both Natural Language Processing and Machine Learning communities. Many methodo</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lilian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval: Vol. 2: No 1, pp 1-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="10187" citStr="Pang et al. (2002)" startWordPosition="1644" endWordPosition="1647"> in our experiments, as done in (Melville et al., 2009). 2.2 Learning from example vs. feature labels Dual supervision makes it possible to learn from labeled examples and labeled features simultaneously; and, as in most supervised learning tasks, one would expect more labeled data of either form to lead to more accurate models. In this section we explore the influence of increased number of instance labels and feature labels independently, and also in tandem. For these, and all subsequent experiments, we use 10-fold cross-validation on the publicly available data of movie reviews provided by Pang et al. (2002). This data consists of 1000 positive and 1000 negative reviews from the Internet Movie Database; where positive labels were assigned to reviews that had a rating above 3.5 stars and negative labels were assigned to ratings of 2 stars and below. We use a bag-of-words representation of reviews, where each review is represented by the term frequencies of the 5000 most frequent words across all reviews, excluding stop-words. In order to study the effect of increasing number of labels we need to simulate a human oracle labeling data. In the case of examples this is straightforward, since all examp</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Foster Provost</author>
<author>Prem Melville</author>
<author>Maytal SaarTsechansky</author>
</authors>
<title>Data acquisition and cost-effective predictive modeling: Targeting offers for electronic commerce.</title>
<date>2007</date>
<booktitle>In ICEC ’07: Proceedings of the ninth international conference on Electronic commerce.</booktitle>
<contexts>
<context position="31912" citStr="Provost et al. (2007)" startWordPosition="5186" endWordPosition="5189">ever, a more intelligent active scheme should be able to assess if an instance or feature would be more beneficial at each step. Furthermore, we do not currently consider the cost of acquiring labels. Presumably labeling a feature versus labeling an instance could incur very different costs – which could be monetary costs or time taken for each annotation. Fortunately, the Expected Utility method is very flexible, and allows us to address all these issues within a single framework. We can specifically estimate the expected utility of different forms of annotation, per unit cost. For instance, Provost et al. (2007) use such an approach to estimate the utility of acquiring class labels and feature values (not labels) per unit cost, within one unified framework. A similar method can be applied for a holistic approach to active dual supervision, where the Expected Utility of an instance or feature label query q, can be computed as EU(q) = EKk=1 P(q = ck)U( q=ck); where ωq is ωq cost of the query q, and utility U can be computed as in Eq. 3. By evaluating instances and features on the same scale, and by measuring utility per unit cost of acquisition, such a framework should enable us to handle the trade-off</context>
</contexts>
<marker>Provost, Melville, SaarTsechansky, 2007</marker>
<rawString>Foster Provost, Prem Melville, and Maytal SaarTsechansky. 2007. Data acquisition and cost-effective predictive modeling: Targeting offers for electronic commerce. In ICEC ’07: Proceedings of the ninth international conference on Electronic commerce.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hema Raghavan</author>
<author>Omid Madani</author>
<author>Rosie Jones</author>
</authors>
<title>Active learning with feedback on features and instances.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>7--1655</pages>
<contexts>
<context position="29299" citStr="Raghavan et al. (2006)" startWordPosition="4776" endWordPosition="4779">y 80 75 70 65 60 55 50 Active Interleaving Expected Feature Utility Instance Uncertainty Passive Interleaving 55 work, to the best of our knowledge. Most prior work has focused on pooled-based active learning, where examples from an unlabeled pool are selected for labeling (Cohn et al., 1994; Tong and Koller, 2000). In contrast, active feature-value acquisition (Melville et al., 2005) and budgeted learning (Lizotte et al., 2003) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis.</context>
</contexts>
<marker>Raghavan, Madani, Jones, 2006</marker>
<rawString>Hema Raghavan, Omid Madani, and Rosie Jones. 2006. Active learning with feedback on features and instances. J. Mach. Learn. Res., 7:1655–1686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Raghavan</author>
<author>O Madani</author>
<author>R Jones</author>
</authors>
<title>An interactive algorithm for asking and incorporating feature feedback into support vector machines.</title>
<date>2007</date>
<booktitle>In SIGIR.</booktitle>
<marker>Raghavan, Madani, Jones, 2007</marker>
<rawString>H. Raghavan, O. Madani, and R. Jones. 2007. An interactive algorithm for asking and incorporating feature feedback into support vector machines. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Roy</author>
<author>Andrew McCallum</author>
</authors>
<title>Toward optimal active learning through sampling estimation of error reduction.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="19142" citStr="Roy and McCallum, 2001" startWordPosition="3098" endWordPosition="3102">nderlying the feature certainty heuristic is that it serves to confirm or correct the orientation of model probabilities on different words during the active learning process. One can argue that feature certainty is also suboptimal in that queries may be wasted simply confirming confident predictions, which is of limited utility to the model. An alternative to using a certainty-based heuristic, is to directly estimate the expected value of acquiring each feature label. Such Expected Utility (Estimated Risk Minimization) approaches have been applied successfully to traditional active learning (Roy and McCallum, 2001), and to active feature-value acquisition (Melville et al., 2005). In this section we describe how this Expected Utility framework can be adapted for feature-label acquisition. At every step of active learning for features, the next best feature to label is one that will result in the highest improvement in classifier performance. Since the true label of the unlabeled features are unknown prior to acquisition, it is necessary to estimate the potential impact of every feature query for all possible outcomes.1 Hence, the decisiontheoretic optimal policy is to ask for feature labels which, once i</context>
<context position="30916" citStr="Roy and McCallum, 2001" startWordPosition="5025" endWordPosition="5028"> multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Though Active Interleaving is a very effective approach to active dual supervision, there is still a lot of room for improvement. Firstly, Active Interleaving relies on Uncertainty Sampling for the selection of instances. Though Uncertainty Sampling has the advantage of being fast and effective, there exist approaches that lead to better models with fewer examples – usually at the cost of computation time. One such method, estimating error reduction (Roy and McCallum, 2001), is a direct analog of Expected Feature Utility applied to instance selection. One would expect that an improvement in instance selection, should directly improve any method that combines instance and feature label selection. Secondly, Active Interleaving uses the simple approach of probabilistically choosing to select an instance or feature for each subsequent query. However, a more intelligent active scheme should be able to assess if an instance or feature would be more beneficial at each step. Furthermore, we do not currently consider the cost of acquiring labels. Presumably labeling a fe</context>
</contexts>
<marker>Roy, McCallum, 2001</marker>
<rawString>Nicholas Roy and Andrew McCallum. 2001. Toward optimal active learning through sampling estimation of error reduction. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maytal Saar-Tsechansky</author>
<author>Prem Melville</author>
<author>Foster Provost</author>
</authors>
<title>Active feature-value acquisition.</title>
<date>2008</date>
<booktitle>In Management Science.</booktitle>
<contexts>
<context position="21551" citStr="Saar-Tsechansky et al., 2008" startWordPosition="3530" endWordPosition="3533">a queried feature. 53 computed as follows. For a model induced from a training set T, let Pˆ(ck|xi) be the probability estimated by the model that instance xi belongs to class ck; and ]l is an indicator function such that ]l(ck,xi) = 1 if ck is the correct class for xi and ]l(ck, xi) = 0, otherwise. Log Gain is then defined as: K LG(xi) = − ]l(ck) Pˆ(ck|xi) (3) k=1 Then the utility of a classifier, U, can be measured by summing the Log Gain for all instances in the training set T. A lower value of Log Gain indicates a better classifier performance. For a deeper discussion of this measure see (Saar-Tsechansky et al., 2008). In Eq. 2, apart from the measure of utility, we also do not know the true probability distribution of labels for the feature under consideration. This too can be estimated from the training data, by seeing how frequently the word appears in documents of each class. In a multinomial Naive Bayes model we already collect these statistics in order to determine the conditional probability of a class given a word, i.e. P(fj|ck). We can use these probabilities to get an estimate of the feature label distribution, Pˆ (fj = ck) = P(fj|ck) EKk=1 P(fj|ck). Given the estimated values of the feature-labe</context>
</contexts>
<marker>Saar-Tsechansky, Melville, Provost, 2008</marker>
<rawString>Maytal Saar-Tsechansky, Prem Melville, and Foster Provost. 2008. Active feature-value acquisition. In Management Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Marie Rochery</author>
<author>Mazin G Rahim</author>
<author>Narendra Gupta</author>
</authors>
<title>Incorporating prior knowledge into boosting.</title>
<date>2002</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="30053" citStr="Schapire et al., 2002" startWordPosition="4890" endWordPosition="4893">feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Though Active Interleaving is a very effective approach to active dual supervision, there is still a lot of room for improvement. Firstly, Active Interleaving relies on Uncertainty Sampling for the selection of inst</context>
</contexts>
<marker>Schapire, Rochery, Rahim, Gupta, 2002</marker>
<rawString>Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and Narendra Gupta. 2002. Incorporating prior knowledge into boosting. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Prem Melville</author>
</authors>
<title>Documentword co-regularization for semi-supervised sentiment analysis.</title>
<date>2008</date>
<booktitle>In ICDM.</booktitle>
<contexts>
<context position="3324" citStr="Sindhwani and Melville, 2008" startWordPosition="513" endWordPosition="516"> negative. This kind of annotation requires a human to condense prior linguistic experience with a word into a sentiment label that reflects the net emotion that the word evokes. We refer to the general setting of learning from both labels on examples and features as dual supervision. This setting arises more broadly in tasks where in addition to labeled documents, it is frequently possible to provide domain knowledge in the form of words, or phrases (Zaidan and Eisner, 2008) or even more sophisticated linguistic features, that associate strongly with a class. Recent work (Druck et al., 2008; Sindhwani and Melville, 2008) has demonstrated that the presence of word supervision can greatly reduce the number of labeled documents Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics required to build high quality text classifiers. In general, these two sources of supervision are not mutually redundant, and have different annotation costs, human response quality, and degrees of utility towards learning a dual supervision model. This leads naturally to the problem of active dual supervision</context>
<context position="16532" citStr="Sindhwani and Melville, 2008" startWordPosition="2670" endWordPosition="2673">y. Despite its simplicity, Uncertainty Sampling is usually quite effective in practice; which raises the question of whether one can apply the same principle to feature-label acquisition. In this case, we want to select unlabeled features that the current model is most uncertain about. Much like instance uncertainty, feature uncertainty can be measured in different ways, depending on the underlying method used for dual supervision. For instance, if the learner produces a lin90 85 80 75 70 65 60 55 50 Instances-then-features Features-then-instances Passive Interleaving 52 ear classifier as in (Sindhwani and Melville, 2008), we could use the magnitude of the weights on the features as a measure of uncertainty – where lower weights indicate less certainty. Since Pooling Multinomials builds a multinomial Naive Bayes model, we can directly use the model’s conditional probabilities of each word (feature) given a class. For ease of exposition we refer to the two classes in binary classification as postive (+) and negative (-), without loss of generality. Given the probabilities of word f belonging to the positive and negative class, P(f|+) and P(f|−), we can determine the uncertainty of a feature using the absolute v</context>
<context position="29843" citStr="Sindhwani and Melville (2008)" startWordPosition="4856" endWordPosition="4859">rom feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Thoug</context>
</contexts>
<marker>Sindhwani, Melville, 2008</marker>
<rawString>Vikas Sindhwani and Prem Melville. 2008. Documentword co-regularization for semi-supervised sentiment analysis. In ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Jianying Hu</author>
<author>Alexandra Mojsilovic</author>
</authors>
<title>Regularized co-clustering with dual supervision.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="29731" citStr="Sindhwani et al. (2008)" startWordPosition="4840" endWordPosition="4843">) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model</context>
</contexts>
<marker>Sindhwani, Hu, Mojsilovic, 2008</marker>
<rawString>Vikas Sindhwani, Jianying Hu, and Alexandra Mojsilovic. 2008. Regularized co-clustering with dual supervision. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Prem Melville</author>
<author>Richard Lawrence</author>
</authors>
<title>Uncertainty sampling and transductive experimental design for active dual supervision.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="5350" citStr="Sindhwani et al., 2009" startWordPosition="826" endWordPosition="829">e on associations of features to particular classes. Since we deal only with text classification in this paper, all features represent term-frequencies of words, and as such we use feature and word interchangeably. The active learning schemes we explore in this paper are broadly applicable to any learner that can support dual supervision, but here we focus on active learning for the Pooling Multinomials classifier (Melville et al., 2009) described below. In concurrent related work, we propose active dual supervision schemes for a class of graph-based and kernel-based dual supervision methods (Sindhwani et al., 2009). 2.1 Pooling Multinomials The Pooling Multinomials classifier was introduced by Melville et al. (2009) as an approach to incorporate prior lexical knowledge into supervised learning for better sentiment detection. In the context of sentiment analysis, lexical knowledge is available in terms of the prior sentiment-polarity of words. From a dual supervision point of view, this knowledge can be seen as labeled features, since the lexicon effectively provides associations of a set of words with the positive or negative class. Pooling Multinomials classifies unlabeled examples just as in multinomi</context>
</contexts>
<marker>Sindhwani, Melville, Lawrence, 2009</marker>
<rawString>Vikas Sindhwani, Prem Melville, and Richard Lawrence. 2009. Uncertainty sampling and transductive experimental design for active dual supervision. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2000</date>
<booktitle>In Proc. of 17th Intl. Conf. on Machine Learning (ICML-2000).</booktitle>
<contexts>
<context position="28993" citStr="Tong and Koller, 2000" startWordPosition="4728" endWordPosition="4731">ame performance – as evidenced by Fig. 1 0 50 100 150 200 250 300 350 400 Number of queries Figure 3: Comparing Active Interleaving to alternative label acquisition strategies. 5 Related work Active learning in the context of dual supervision models is a new area of research with very little prior Accuracy 80 75 70 65 60 55 50 Active Interleaving Expected Feature Utility Instance Uncertainty Passive Interleaving 55 work, to the best of our knowledge. Most prior work has focused on pooled-based active learning, where examples from an unlabeled pool are selected for labeling (Cohn et al., 1994; Tong and Koller, 2000). In contrast, active feature-value acquisition (Melville et al., 2005) and budgeted learning (Lizotte et al., 2003) focus on estimating the value of acquiring missing features, but do not deal with the task of learning from feature labels. In contrast, Raghavan and Allan (2007) and Raghavan et al. (2006) study the problem of tandem learning where they combine uncertainty sampling for instances along with cooccurence based interactive feature selection. Godbole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term m</context>
</contexts>
<marker>Tong, Koller, 2000</marker>
<rawString>Simon Tong and Daphne Koller. 2000. Support vector machine active learning with applications to text classification. In Proc. of 17th Intl. Conf. on Machine Learning (ICML-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyun Wu</author>
<author>Rohini Srihari</author>
</authors>
<title>Incorporating prior knowledge with weighted margin support vector machines.</title>
<date>2004</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="30075" citStr="Wu and Srihari, 2004" startWordPosition="4894" endWordPosition="4897">ole et al. (2004) propose notions of feature uncertainty and incorporate the acquired feature labels, into learning by creating one-term mini-documents. Learning from labeled examples and features via dual supervision, is itself a new area of research. Sindhwani et al. (2008) use a kernel-based framework to build dual supervision into co-clustering models. Sindhwani and Melville (2008) apply similar ideas for graph-based sentiment analysis. There have also been previous attempts at using only feature supervision, mostly along with unlabeled documents. Much of this work (Schapire et al., 2002; Wu and Srihari, 2004; Liu et al., 2004; Dayanik et al., 2006) has focused on using labeled features to generate pseudo-labeled examples that are then used with well-known models. In contrast, Druck et al. (2008) constrain the outputs of a multinomial logistic regression model to match certain reference distributions associated with labeled features. 6 Perspectives and future work Though Active Interleaving is a very effective approach to active dual supervision, there is still a lot of room for improvement. Firstly, Active Interleaving relies on Uncertainty Sampling for the selection of instances. Though Uncertai</context>
</contexts>
<marker>Wu, Srihari, 2004</marker>
<rawString>Xiaoyun Wu and Rohini Srihari. 2004. Incorporating prior knowledge with weighted margin support vector machines. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O F Zaidan</author>
<author>J Eisner</author>
</authors>
<title>Modeling annotators: A generative approach to learning from annotator rationales.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3175" citStr="Zaidan and Eisner, 2008" startWordPosition="491" endWordPosition="494">esmerizing”, “thrilling” etc) may be labeled positive, while words that evoke negative sentiment (e.g., “boring”,“disappointing”) may be labeled negative. This kind of annotation requires a human to condense prior linguistic experience with a word into a sentiment label that reflects the net emotion that the word evokes. We refer to the general setting of learning from both labels on examples and features as dual supervision. This setting arises more broadly in tasks where in addition to labeled documents, it is frequently possible to provide domain knowledge in the form of words, or phrases (Zaidan and Eisner, 2008) or even more sophisticated linguistic features, that associate strongly with a class. Recent work (Druck et al., 2008; Sindhwani and Melville, 2008) has demonstrated that the presence of word supervision can greatly reduce the number of labeled documents Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics required to build high quality text classifiers. In general, these two sources of supervision are not mutually redundant, and have different annotation costs, hum</context>
</contexts>
<marker>Zaidan, Eisner, 2008</marker>
<rawString>O. F. Zaidan and J. Eisner. 2008. Modeling annotators: A generative approach to learning from annotator rationales. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>