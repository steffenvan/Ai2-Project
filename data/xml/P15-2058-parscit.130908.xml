<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000450">
<title confidence="0.997048">
Semantic Clustering and Convolutional Neural Network
for Short Text Categorization
</title>
<author confidence="0.814786">
Peng Wang, Jiaming Xu, Bo Xu, Cheng-Lin Liu, Heng Zhang
Fangyuan Wang, Hongwei Hao
</author>
<email confidence="0.931043">
{peng.wang, jiaming.xu, boxu}@ia.ac.cn, liucl@nlpr.ia.ac.cn
{heng.zhang, fangyuan.wang, hongwei.hao}@ia.ac.cn
</email>
<affiliation confidence="0.826884">
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, P.R. China
</affiliation>
<sectionHeader confidence="0.975798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974764705882">
Short texts usually encounter data sparsi-
ty and ambiguity problems in representa-
tions for their lack of context. In this pa-
per, we propose a novel method to mod-
el short texts based on semantic clustering
and convolutional neural network. Partic-
ularly, we first discover semantic cliques
in embedding spaces by a fast clustering
algorithm. Then, multi-scale semantic u-
nits are detected under the supervision of
semantic cliques, which introduce useful
external knowledge for short texts. These
meaningful semantic units are combined
and fed into convolutional layer, followed
by max-pooling operation. Experimental
results on two open benchmarks validate
the effectiveness of the proposed method.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999852886363636">
Conventional texts mining methods based on bag-
of-words (BoW) easily encounter data sparsi-
ty and ambiguity problems in short text model-
ing (Chen et al., 2011), which ignore semantic re-
lations between words (Sriram et al., 2010). How
to acquire effective representation for short tex-
t has been an active research issue (Chen et al.,
2011; Phan et al., 2008).
In order to overcome the weakness of BoW, re-
searchers have proposed to expand the represen-
tation of short text using latent semantics, where
the words are mapped to distributional representa-
tions by Latent Dirichlet Allocation (LDA) (Blei
et al., 2003) and its extensions. Phan et al. (2008)
presented a general framework to expand the short
and sparse text by appending topic names discov-
ered using LDA. Yan et al. (2013) presented a vari-
ant of LDA, dubbed Biterm Topic Model (BTM),
especially for short text modeling to alleviate the
problem of sparsity. However, the methods dis-
cussed above still view a piece of text as BoW.
Therefore, they are not effective in capturing fine-
grained semantic information for short texts mod-
eling.
Recently, neural network related methods have
received much attention, including learning word
embeddings (Bengio et al., 2003; Mikolov et al.,
2013a) and performing semantic composition to
obtain phrase or sentence level representation-
s (Collobert et al., 2011; Le and Mikolov, 2014).
For learning word embedding, the training objec-
tive of continuous Skip-gram model (Mikolov et
al., 2013b) is to predict its context. Thus, the co-
occurrence information can be effectively used to
describe a word, and each component of word em-
bedding might have a semantic or grammatical in-
terpretation.
In embedding spaces, semantically close word-
s are likely to cluster together and form semantic
cliques (or word embedding cliques). Moreover,
the embedding spaces exhibit linear structure that
the word vectors can be meaningfully combined
using simple additive operation (Mikolov et al.,
2013b), for example:
</bodyText>
<equation confidence="0.636550666666667">
vec (Germany) +vec (Capital)Pz�vec (Berlin) (1)
vec(Athlete)+vec (Football)Pz�vec (Football Player)
(2)
</equation>
<bodyText confidence="0.999885533333333">
The above examples indicate that the additive
composition can often produce meaningful result-
s. In Equation (1), the token ′Berlin′ can be viewed
that it has an embedding offset vec (Capital) to the
token ′Germany′ in embedding spaces. Further-
more, the embedding offsets represent the syntac-
tical and semantic relations among words.
In this paper, we propose a method to mod-
el short texts using semantic clustering and con-
volutional neural network (CNN). Firstly, the fast
clustering algorithm (Rodriguez and Laio, 2014),
based on searching density peaks, is utilized to
cluster word embeddings and discover semantic
cliques, as shown in Figure 1. Then semantic com-
position is performed over n-gram embeddings to
</bodyText>
<page confidence="0.961867">
352
</page>
<note confidence="0.988548666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 352–357,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<title confidence="0.404309">
Decision Graph Word Embeddings Clustering
</title>
<figure confidence="0.999168931818182">
100
80
60
0
12
may
150
100
car
food huanar
mdic
test hospital
fires
weapons
attack
v radi
nwork
interncompuer
echoloy
wp fnthing qution
?y td
mat
l v’askd
ca
am ting prson
n’daily system
’−−
d
using
! going ied eath
dea
televison killed
station
itrvie
new prss
media job
’’‘‘iwe
go... your
says gd owsa
’llthught
’rek
kow
ttmnt
yioffcal aidtldforeig sdis
agency minisy
spokesmn
reportrs
pli army fr fore troops
taffmsn
military security
tter
ing
rall thigs smething
.n.un eulyprilsptr
mach augst january october
february
real ng
blieve
stry book
written love
published never
best
done alwys hywanted
look ever dec
november
little
kind
bad song album music
3218379426051/();: &amp;quot; sherodfren
es man
chlds woman
history
wh
age himsef young living
suct diffn
rhtw
fewmso s
m ir
i iiy ws
t,.ofn ’r
t a otpp
hich
its e _thm
fr oy y
bng
til
nw
jut
c antr fulsw shviknow esm d hwver
gvn lessonc aking
evrybigfartody altough
aeicns
much wy
lik
par
betn
undr
wl
bt
tgh ararctly course
narly almst eough
eithr
instead
pres
md
he hard
wiithr
pi clt gai
ff bc dow
upwh
hlf ps
awayvr
t
tims
currently
on rnd rught
gveson
sw
okca
then
after
before
until
home
round
racnn even
ourfivesx wothree
eight
1 5 0 3 0
2012 7 9 4 6 1998 finl
match
games
am
ja
medical
killed member months
systm
huanmexico bnd
million
per german
angeles
iraqi games
low
0 20 ρ 40 60 80
50
senior council
independent
members
nato israeli
iraqiafghanst
palestinian
stfederal
admniration
gverment
cuntrs
nats
society
wlds.s.unsts
iterational
naon
europe
meican
country
in canada
british n
australia
germany
tco
−150
−150 −100 −50 0 50 100 150 200
world
2004
msstocks
capital
mdiadecember
race
troops
saying
old
0
−50
investn
s
bank financil
italy mexi
ste by ting s
city
ne twpil cnral
commuity
sal workers
ares
villlocaed borde investors
market
exhange
indx fllhars
stock prices
t wsuhern
eastern
northen
regio
provinc percent
rose
bilion $milion dollars
dvid michel hngeorge
pul markst.james yorkwashingn
%otlper10540annual
county dstrict
rober
de texas losangeles florida
sancalifornia
X
δ
2
9
d
a
m
Y
40
20
</figure>
<figureCaption confidence="0.999891">
Figure 1: Fast clustering based on density peaks of embeddings
</figureCaption>
<bodyText confidence="0.936230285714286">
Neural networks have been used to model lan-
afixed-size feature representation
for documents.
of our method with experiments. Finall
y, conclud-
ing remarks are offered in Section 6.
353 expand short texts. Chen et al. (2011) proved that
</bodyText>
<sectionHeader confidence="0.999229" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.922337">
d
</bodyText>
<subsectionHeader confidence="0.468844">
information can
</subsectionHeader>
<bodyText confidence="0.9881349125">
be exploited.
leveraging topics at multiple granularity can mod-
el short texts more precisely.
guages, and the word embeddings can be learned
simultaneously (Mnih and Teh, 2012). Mikolov et
al. (2013b) introduced the continuous Skip-gram
model that is an efficient method for learning high
quality word embeddings from large-scale un-
structured text data. Recently, various pre-trained
word embeddings are publicly available, and many
composition-based methods are proposed to in-
duce the semantic representation of texts. Le and
Mikolov (2014) presented the Paragraph Vector al-
gorithm to learn
−100
detect candidate Semantic Unitsl(abbr. to SUs)
appearing in short texts. The part of candidate
SUs meeting the preset threshold are chosen to
constitute semantic matrices, which are used as in-
put for the CNN, otherwise dropout. In this stage,
semantic cliques are used as supervision informa-
tion, which guarantee meaningful SUs can be ex-
tracted.
The motivation of our work is to introduce extra
knowledge by pre-trained word embeddings and
fully exploit the contextual information of short
texts to improve their representations. The main
contributions include: (1) semantic cliques are
discovered using fast clustering method based on
searching density peaks; (2) for fine-tuning multi-
scale SUs, the semantic cliques are used to super-
vise the selection stage.
The remainder of this paper is organized as fol-
lows. The related works are briefly reviewed in
Section 2. Section 3 introduces the semantic clus-
tering based on fast searching density peaks. Sec-
tion 4 describes the architecture of the proposed
method. Section 5 demonstrates the effectiveness
Traditional statistics-based methods usually fail to
achieve satisfactory performance for short texts
classification due to their sparsity of representa-
tions (Sriram et al., 2010). Based on external
Wikipedia corpus, Phan et al. (2008) proposed a
method to discover hidden topics using LDA an
&apos;Semantic units are defined as n-grams which have domi-
nant meaning of text. With n varying, multi-scale contextual
Kalchbrenner et al. (2014) introduced the Dy-
namic Convolutional Neural Network (DCNN) for
modeling sentences. Their work is closely relat-
ed to our study in that k-max pooling is utilized
to capture global feature vector and do not rely
on parse tree. Kim (2014) proposed a simple im-
provement to the convolutional architecture that t-
wo input channels are used to allow the employ-
ment of task-specific and static word embeddings
simultaneously.
Zeng et al. (2014) developed a deep convo-
lutional neural network (DNN) to extract lexical
and sentence level features, which are concate-
nated and fed into the softmax classifier. Socher
et al. (2013) proposed the Recursive Neural Net-
work (RNN) that has been proven to be efficient
in terms of constructing sentences representation-
s. In order to reduce the overfitting of neural net-
work especially trained on small data set, Hin-
ton et al. (2012) used ran
dom dropout to prevent
complex co-adaptations. To exploit more struc-
ture information of text, based on CNN and direc-
t embedding of small text regions, an alternative
mechanism for effective use of word order for text
categorization was proposed (Johnson and Zhang,
2014).
Although the popular methods can capture
high-order information and word relations to pro-
duce complex features, they cannot guarantee the
classification performance for very short texts. In
this paper, we design a method to exploit more
contextual information for short text classification
using semantic clustering and CNN.
</bodyText>
<sectionHeader confidence="0.993699" genericHeader="method">
3 Semantic Clustering
</sectionHeader>
<bodyText confidence="0.999883">
Since the neighbors of each word are semanti-
cally related in embedding space (Mikolov et al.,
2013b), clustering methods (Rodriguez and Laio,
2014) can be used to discover semantic cliques.
For implementation, two quantities of data point i
are computed, include: local density pi, defined as
follows,
</bodyText>
<equation confidence="0.9678495">
Epi = X(dij − dc) (3)
j
</equation>
<bodyText confidence="0.9999858">
where dij is the distance between data points, dc
is a cutoff distance. Furthermore, distance Si from
points of higher density is measured by,
An example of semantic clustering is illustrat-
ed in Figure 1. The decision graph shows the two
quantities p and S of each word embedding. Ac-
cording to the definitions above, these word em-
beddings with large p and S simultaneously are
chosen as cluster centers, which are labeled using
the corresponding words.
</bodyText>
<sectionHeader confidence="0.987054" genericHeader="method">
4 Proposed Architecture
</sectionHeader>
<bodyText confidence="0.999960666666667">
As shown in Figure 2, the proposed architecture
use well pre-trained word embeddings to initialize
the lookup table, and higher levels extract more
complexity features.
For short text S = {w1, w2, · · · , wN}, its project-
ed matrix PM E Rd×N is obtained by table look-
ing up in the first layer, where d is the dimension
of word embedding. The second layer is used to
obtain multi-scale SUs to constitute the semantic
</bodyText>
<figureCaption confidence="0.995239">
Figure 2: Architecture for short text modeling
</figureCaption>
<bodyText confidence="0.993922">
matrices, which are combined and fed into convo-
lutional layer, followed by k-max pooling opera-
tion. Finally, a softmax function is employed as
classifier.
</bodyText>
<subsectionHeader confidence="0.883823">
4.1 Detection for Multi-scale SUs
</subsectionHeader>
<bodyText confidence="0.999957615384615">
Methods for modeling short text S mainly have
problem that its semantic meaning is determined
by a few of key-phrases, however, these meaning-
ful phrases may appear at any position of S. Thus,
simply combining all words of S may introduce
unnecessary divergence and hurt the overall se-
mantic representation. Therefore, the detection for
SUs are useful, which capture salient local infor-
mation, as shown in Figure 2.
In particular, to obtain the representations of
candidate SUs, multiple windows with variable
width over word embeddings are used to perfor-
m element-wise additive composition, as follows:
</bodyText>
<equation confidence="0.626502">
[SU1, SU2, ··· , SUN−m+1] = PM ® Ewin (5)
</equation>
<bodyText confidence="0.9895075">
where, Ewin E Rd×m is a window matrix with all
weights equal to one, and
</bodyText>
<equation confidence="0.987281333333333">
PMwin,i (6)
j
PMwin,i
</equation>
<bodyText confidence="0.995622428571428">
j is the jth column from the sub-matrix
PMwin,i, which is windowed on projected matrix
PM by Ewin with the ith times sliding. m is the
width of the window matrix Ewin. With m vary-
ing, multi-scale contextual information can be ex-
ploited, which is helpful to reduce the impact of
ambiguous words.
</bodyText>
<figure confidence="0.943087222222222">
The cat sat on the red mat
Softmax Decision
K-Max Pooling
Convolution
Multi-scale
Semantic
Units
Projected
Sentence
Matrix
min (dij) , if pi &lt; pm.
j:ρj&gt;ρi(4)
Si = I
max(dij) , otherwise
j
|PMwin,i|
SUi= E
j=1
</figure>
<page confidence="0.997643">
354
</page>
<bodyText confidence="0.999889">
The meaningful SUs are assumed that they have
one close neighbor at least in embedding space.
Thus, we compute Euclidean distance between
candidate SUs and semantic cliques. If the dis-
tance between candidate SUs and nearest word
embeddings are smaller than the preset threshold,
the candidate SUs are selected to constitute the se-
mantic matrices, otherwise dropout.
</bodyText>
<subsectionHeader confidence="0.999093">
4.2 Convolution Layer
</subsectionHeader>
<bodyText confidence="0.9999275">
In our network, the convolutional layer is used to
extract local features. Kernel matrices k with cer-
tain width n are utilized to calculate convolution
with the input matrices M, as Equation (7).
</bodyText>
<equation confidence="0.980961888888889">
C = [c1, c2, ··· ,cd/2]T = KT ⊗ M (7)
where,
K = [k1, k2, ··· , kd/2] (8)
M = [Mwin
1 ,Mwin
2 ,···, Mwin
d/2 ] (9)
cj i = ki · (Mwin,j
i )T (10)
</equation>
<bodyText confidence="0.978748">
The cji is generated from the jth n-gram in M.
Equation (7) produce the feature maps of convolu-
tional layer.
</bodyText>
<subsectionHeader confidence="0.99818">
4.3 K-Max Pooling
</subsectionHeader>
<bodyText confidence="0.99998825">
This operator is a non-linear sub-sampling func-
tion that returns the sub-sequence of K maximum
values (LeCun et al., 1998), which is used to cap-
ture the most relevant global features with fixed-
length. Then, tangent transformation over the re-
sults of K-max pooling is performed, the output
of which is concatenated to used as representation
for the input short texts.
</bodyText>
<subsectionHeader confidence="0.997717">
4.4 Network Training
</subsectionHeader>
<bodyText confidence="0.999981666666667">
The last layer is fully connected, where a soft-
max classifier is applied to predict the proba-
bility distribution over categories. The network
is trained with the objective that minimizes the
cross-entropy of the predicted distributions and the
actual distributions (Turian et al., 2010),
</bodyText>
<equation confidence="0.995452">
1 ∑t
J(0) = −i�� log p(c†|xi, 0) + α110112 (11)
t
</equation>
<bodyText confidence="0.96071975">
where t is number of training examples x, and 0 is
the parameters set which comprises the kernels of
weights used in convolutional layer and the con-
nective weights from the fully connected layer.
</bodyText>
<table confidence="0.9995165">
Embedding Senna2 GloVe3 Word2Vec4
Corpus Wikipedia Wikipedia Google News
Dimension 50 50 300
|V ocab. |130,000 400,000 3,000,000
</table>
<tableCaption confidence="0.987651">
Table 1: Details of word embeddings
</tableCaption>
<table confidence="0.999930666666667">
Methods Google TREC
Snippets
Semantic-CNN Senna 83.6 96.4
GloVe 84.4 97.2
Word2Vec 85.1 95.6
DCNN – 93
(Kalchbrenner et al,2014)
SVMS – 95
(Silva et al., 2011)
CNN-TwoChannel – 93.6
(Kim, 2014)
LDA+MaxEnt 82.7 –
(Phan et al., 2008)
Multi-Topics+MaxEnt 84.17 –
(Chen et al., 2011)
</table>
<tableCaption confidence="0.9876115">
Table 2: The classification accuracy of proposed
method against other models
</tableCaption>
<sectionHeader confidence="0.994954" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.936336">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999747545454546">
Experiments are conducted on two benchmarks:
Google Snippets (Phan et al., 2008) and TREC (Li
and Roth, 2002).
Google Snippets This dataset consists of
10,060 training snippets and 2,280 test snippets
from 8 categories. On average, each snippet has
18.07 words.
TREC The TREC questions dataset contains 6
different question types. The training dataset con-
sists of 5,452 labeled questions whereas the test
dataset consists of 500 questions.
</bodyText>
<subsectionHeader confidence="0.998163">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999585">
Three pre-trained word embeddings for initializ-
ing the lookup table are summarized in Table 1.
To discover semantic cliques, we take pmin = 16
and Smin = 1.54. Through our experiments, 6 k-
ernel matrices in convolutional layer, K = 3 for
max pooling, and mini-batch size of 100 are used.
</bodyText>
<sectionHeader confidence="0.655283" genericHeader="method">
5.3 Results and Discussions
5.3.1 Comparison with state-of-the-art
methods
</sectionHeader>
<bodyText confidence="0.905153">
As shown in Table 2, we introduce 5 popular meth-
ods as baselines, and the details are described:
DCNN Kalchbrenner et al. (2014) proposed D-
CNN for sentence modeling with dynamic k-max
pooling.
</bodyText>
<page confidence="0.993221">
355
</page>
<figure confidence="0.9998381">
0 2 4 6
0 2 4 6
0.8
0.93
0.85
0.97
0.84
Accuracy
0.96
0.83
0.95
0.82
0.94
0.81
Google Snippets
TREC
Senna
GloVe
Word2Vec
Number of window matrices
</figure>
<figureCaption confidence="0.992464">
Figure 3: Number of windows for multi-scale SUs
</figureCaption>
<figure confidence="0.99300205">
Senna GloVe Word2Vec
Accuracy
0.835
0.825
0.84
0.83
0.82
0.85
0.84
0.83
0.82
0.5 1 1.5 2
Euclidean Distance
0.845
0.835
0.85
0.84
0.83
1 2 3 4
3 4 5 6
</figure>
<figureCaption confidence="0.999987">
Figure 4: Influence of threshold in SUs detection
</figureCaption>
<bodyText confidence="0.996722904761905">
SVMs Parser, wh word, head word, POS, hy-
pernyms, and 60 hand-coded rules were used as
features to train SVMs (Silva et al., 2011).
CNN-TwoChannel An improved CNN that al-
lows task-specific and static word embeddings are
used simultaneously (Kim, 2014).
LDA+MaxEnt LDA was used to discover hid-
den topics for expanding short texts (Phan et al.,
2008).
Multi-topics+MaxEnt Multiple granularity
topics from LDA were utilized to model short
texts (Chen et al., 2011).
For valid comparisons, we respectively initial-
ize the lookup table with the word embeddings in
Table 1, and three experiments are conducted for
each benchmark. As a whole, our method achieves
the best performance, especially for TREC with
97.2% when the GloVe word embedding is em-
ployed. For Google snippets, our method achieves
the highest result of 85.1% corresponding to the
word embedding induced by Word2Vec.
</bodyText>
<subsectionHeader confidence="0.975336">
5.3.2 Effect of Hyper-parameters
</subsectionHeader>
<bodyText confidence="0.999921333333333">
In Figure 2, for obtaining SUs with multi-scale,
multiple window matrices with increasing width
m are used. With respect to the variable m, the re-
</bodyText>
<footnote confidence="0.999938333333333">
2http://ml.nec-labs.com/senna/
3http://nlp.stanford.edu/projects/glove/
4https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.99983">
sults are shown in Figure 3. We find small size of
window may result in loss of critical information,
however, the window with large size may intro-
duce noise.
Figure 4 demonstrate how preset threshold d
impact our method over benchmark Goggle snip-
pets. We can draw a conclusion that when d is too
small, only a few of SUs can be detected, where-
as meaningless features are enrolled. The optimal
threshold d can be chosen by cross-validation.
The impacts of other hyper-parameters like the
number and size of the feature detectors in convo-
lutional layer, and the variable k in k-max pooling
layer are beyond the scope of this paper.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998276">
This paper proposes a novel semantic hierarchical
model for short text classification. The model us-
es pre-trained word embeddings to introduce extra
knowledge, and multi-scale SUs in short texts are
detected.
</bodyText>
<sectionHeader confidence="0.974822" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.8172656">
This work is supported by the National Natural
Science Foundation of China (No. 61203281, No.
61303172, No. 61403385) and Hundred Talents
Program of Chinese Academy of Sciences (No.
Y3S4011D31).
</bodyText>
<page confidence="0.998314">
356
</page>
<sectionHeader confidence="0.989586" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998684841121495">
Ainur Yessenalina and Claire Cardie. Composition-
al matrix-space models for sentiment analysis. In
EMNLP, pages 172–182. Association for Computa-
tional Linguistics, 2011.
Alex Rodriguez and Alessandro Laio. Clustering by
fast search and find of density peaks. Science,
344(6191):1492–1496, 2014.
Andriy Mnih and Yee Whye Teh. A fast and simple
algorithm for training neural probabilistic language
models. arXiv preprint arXiv:1206.6426, 2012.
Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Fer-
hatosmanoglu, and Murat Demirbas. Short text clas-
sification in twitter to improve information filtering.
In SIGIR, pages 841–842. ACM, 2010.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. Relation classification via convolu-
tional deep neural network. In Proceedings of COL-
ING, pages 2335–2344, 2014.
David M Blei, Andrew Y Ng, and Michael I Jordan.
Latent dirichlet allocation. the Journal of machine
Learning research, 3:993–1022, 2003.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint arX-
iv:1207.0580, 2012.
Jeff Mitchell and Mirella Lapata. Composition in dis-
tributional models of semantics. Cognitive science,
34(8):1388–1429, 2010.
John Duchi, Elad Hazan, and Yoram Singer. Adap-
tive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159, 2011.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word
representations: a simple and general method for
semi-supervised learning. In ACL, pages 384–394.
Association for Computational Linguistics, 2010.
Mehran Sahami and Timothy D Heilman. A web-based
kernel function for measuring the similarity of short
text snippets. In Proceedings of the 15th interna-
tional conference on World Wide Web, pages 377–
386. AcM, 2006.
Mengen Chen, Xiaoming Jin, and Dou Shen. Short
text classification improved by learning multi-
granularity topics. In IJCAI, pages 1776–1781.
Citeseer, 2011.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. A convolutional neural network for modelling
sentences. arXiv preprint arXiv:1404.2188, 2014.
Quoc V Le and Tomas Mikolov. Distributed represen-
tations of sentences and documents. arXiv preprint
arXiv:1405.4053, 2014.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for
semantic compositionality over a sentiment tree-
bank. In EMNLP, volume 1631, page 1642. Cite-
seer, 2013.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Nat-
ural language processing (almost) from scratch. The
Journal of Machine Learning Research, 12:2493–
2537, 2011.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efficient estimation of word representation-
s in vector space. arXiv preprint arXiv:1301.3781,
2013.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing System-
s, pages 3111–3119, 2013.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
Linguistic regularities in continuous space word rep-
resentations. In HLT-NAACL, pages 746–751, 2013.
Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
Cheng. A biterm topic model for short texts. In
WWW, pages 1445–1456. International World Wide
Web Conferences Steering Committee, 2013.
Xin Li and Dan Roth. Learning question classifiers.
In Proceedings of the 19th international conference
on Computational linguistics-Volume 1, pages 1–7.
Association for Computational Linguistics, 2002.
Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu
Horiguchi. Learning to classify short and sparse
text &amp; web with hidden topics from large-scale data
collections. In Proceedings of the 17th internation-
al conference on World Wide Web, pages 91–100.
ACM, 2008.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to doc-
ument recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998.
Yoon Kim. Convolutional neural networks for sentence
classification. arXiv preprint arXiv:1408.5882,
2014.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. A neural probabilistic language
model. The Journal of Machine Learning Research,
3:1137–1155, 2003.
Joao Silva, Lu´ısa Coheur, Ana Cristina Mendes, and
Andreas Wichert. From symbolic to sub-symbolic
information in question classification. Artificial In-
telligence Review, 35(2):137–154, 2011.
Rie Johnson and Tong Zhang. Effective use of word or-
der for text categorization with convolutional neural
networks. arXiv preprint arXiv:1412.1058, 2014.
</reference>
<page confidence="0.998147">
357
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.340879">
<title confidence="0.9986465">Semantic Clustering and Convolutional Neural for Short Text Categorization</title>
<author confidence="0.861892">Peng Wang</author>
<author confidence="0.861892">Jiaming Xu</author>
<author confidence="0.861892">Bo Xu</author>
<author confidence="0.861892">Cheng-Lin Liu</author>
<author confidence="0.861892">Heng Fangyuan Wang</author>
<author confidence="0.861892">Hongwei</author>
<email confidence="0.9146515">jiaming.xu,fangyuan.wang,</email>
<affiliation confidence="0.555164">Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, P.R. China</affiliation>
<abstract confidence="0.998844722222222">Short texts usually encounter data sparsity and ambiguity problems in representations for their lack of context. In this paper, we propose a novel method to model short texts based on semantic clustering and convolutional neural network. Particularly, we first discover semantic cliques in embedding spaces by a fast clustering algorithm. Then, multi-scale semantic units are detected under the supervision of semantic cliques, which introduce useful external knowledge for short texts. These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation. Experimental results on two open benchmarks validate the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>172--182</pages>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. Compositional matrix-space models for sentiment analysis. In EMNLP, pages 172–182. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Rodriguez</author>
<author>Alessandro Laio</author>
</authors>
<title>Clustering by fast search and find of density peaks.</title>
<date>2014</date>
<journal>Science,</journal>
<volume>344</volume>
<issue>6191</issue>
<contexts>
<context position="3691" citStr="Rodriguez and Laio, 2014" startWordPosition="556" endWordPosition="559"> vec (Germany) +vec (Capital)Pz�vec (Berlin) (1) vec(Athlete)+vec (Football)Pz�vec (Football Player) (2) The above examples indicate that the additive composition can often produce meaningful results. In Equation (1), the token ′Berlin′ can be viewed that it has an embedding offset vec (Capital) to the token ′Germany′ in embedding spaces. Furthermore, the embedding offsets represent the syntactical and semantic relations among words. In this paper, we propose a method to model short texts using semantic clustering and convolutional neural network (CNN). Firstly, the fast clustering algorithm (Rodriguez and Laio, 2014), based on searching density peaks, is utilized to cluster word embeddings and discover semantic cliques, as shown in Figure 1. Then semantic composition is performed over n-gram embeddings to 352 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 352–357, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Decision Graph Word Embeddings Clustering 100 80 60 0 12 may 150 100 car food huanar mdic test hospital fires weapons attack v </context>
<context position="10324" citStr="Rodriguez and Laio, 2014" startWordPosition="1622" endWordPosition="1625"> regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word relations to produce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering and CNN. 3 Semantic Clustering Since the neighbors of each word are semantically related in embedding space (Mikolov et al., 2013b), clustering methods (Rodriguez and Laio, 2014) can be used to discover semantic cliques. For implementation, two quantities of data point i are computed, include: local density pi, defined as follows, Epi = X(dij − dc) (3) j where dij is the distance between data points, dc is a cutoff distance. Furthermore, distance Si from points of higher density is measured by, An example of semantic clustering is illustrated in Figure 1. The decision graph shows the two quantities p and S of each word embedding. According to the definitions above, these word embeddings with large p and S simultaneously are chosen as cluster centers, which are labeled</context>
</contexts>
<marker>Rodriguez, Laio, 2014</marker>
<rawString>Alex Rodriguez and Alessandro Laio. Clustering by fast search and find of density peaks. Science, 344(6191):1492–1496, 2014.</rawString>
</citation>
<citation valid="true">
<title>Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426,</title>
<date>2012</date>
<contexts>
<context position="9548" citStr="(2012)" startWordPosition="1507" endWordPosition="1507">mprovement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data set, Hinton et al. (2012) used ran dom dropout to prevent complex co-adaptations. To exploit more structure information of text, based on CNN and direct embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word relations to produce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering an</context>
</contexts>
<marker>2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bharath Sriram</author>
</authors>
<title>Dave Fuhry, Engin Demir, Hakan Ferhatosmanoglu, and Murat Demirbas. Short text classification in twitter to improve information filtering.</title>
<date>2010</date>
<booktitle>In SIGIR,</booktitle>
<pages>841--842</pages>
<publisher>ACM,</publisher>
<marker>Sriram, 2010</marker>
<rawString>Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Ferhatosmanoglu, and Murat Demirbas. Short text classification in twitter to improve information filtering. In SIGIR, pages 841–842. ACM, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation classification via convolutional deep neural network.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2335--2344</pages>
<contexts>
<context position="9122" citStr="Zeng et al. (2014)" startWordPosition="1433" endWordPosition="1436">od to discover hidden topics using LDA an &apos;Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual Kalchbrenner et al. (2014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data set, Hinton et al. (2012) used ran dom dropout to prevent complex co-adaptations. To exploit more structure information of text, based on CNN and direct embedding of small text regions, an alternativ</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In Proceedings of COLING, pages 2335–2344, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<volume>3</volume>
<contexts>
<context position="1687" citStr="Blei et al., 2003" startWordPosition="246" endWordPosition="249">oduction Conventional texts mining methods based on bagof-words (BoW) easily encounter data sparsity and ambiguity problems in short text modeling (Chen et al., 2011), which ignore semantic relations between words (Sriram et al., 2010). How to acquire effective representation for short text has been an active research issue (Chen et al., 2011; Phan et al., 2008). In order to overcome the weakness of BoW, researchers have proposed to expand the representation of short text using latent semantics, where the words are mapped to distributional representations by Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its extensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580,</title>
<date>2012</date>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive science,</journal>
<volume>34</volume>
<issue>8</issue>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="14409" citStr="Turian et al., 2010" startWordPosition="2324" endWordPosition="2327">at returns the sub-sequence of K maximum values (LeCun et al., 1998), which is used to capture the most relevant global features with fixedlength. Then, tangent transformation over the results of K-max pooling is performed, the output of which is concatenated to used as representation for the input short texts. 4.4 Network Training The last layer is fully connected, where a softmax classifier is applied to predict the probability distribution over categories. The network is trained with the objective that minimizes the cross-entropy of the predicted distributions and the actual distributions (Turian et al., 2010), 1 ∑t J(0) = −i�� log p(c†|xi, 0) + α110112 (11) t where t is number of training examples x, and 0 is the parameters set which comprises the kernels of weights used in convolutional layer and the connective weights from the fully connected layer. Embedding Senna2 GloVe3 Word2Vec4 Corpus Wikipedia Wikipedia Google News Dimension 50 50 300 |V ocab. |130,000 400,000 3,000,000 Table 1: Details of word embeddings Methods Google TREC Snippets Semantic-CNN Senna 83.6 96.4 GloVe 84.4 97.2 Word2Vec 85.1 95.6 DCNN – 93 (Kalchbrenner et al,2014) SVMS – 95 (Silva et al., 2011) CNN-TwoChannel – 93.6 (Kim,</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In ACL, pages 384–394. Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A web-based kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>377--386</pages>
<publisher>AcM,</publisher>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D Heilman. A web-based kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, pages 377– 386. AcM, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengen Chen</author>
<author>Xiaoming Jin</author>
<author>Dou Shen</author>
</authors>
<title>Short text classification improved by learning multigranularity topics.</title>
<date>2011</date>
<booktitle>In IJCAI,</booktitle>
<pages>1776--1781</pages>
<publisher>Citeseer,</publisher>
<contexts>
<context position="1235" citStr="Chen et al., 2011" startWordPosition="171" endWordPosition="174">, we first discover semantic cliques in embedding spaces by a fast clustering algorithm. Then, multi-scale semantic units are detected under the supervision of semantic cliques, which introduce useful external knowledge for short texts. These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation. Experimental results on two open benchmarks validate the effectiveness of the proposed method. 1 Introduction Conventional texts mining methods based on bagof-words (BoW) easily encounter data sparsity and ambiguity problems in short text modeling (Chen et al., 2011), which ignore semantic relations between words (Sriram et al., 2010). How to acquire effective representation for short text has been an active research issue (Chen et al., 2011; Phan et al., 2008). In order to overcome the weakness of BoW, researchers have proposed to expand the representation of short text using latent semantics, where the words are mapped to distributional representations by Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its extensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using </context>
<context position="6556" citStr="Chen et al. (2011)" startWordPosition="1036" endWordPosition="1039">l cnral commuity sal workers ares villlocaed borde investors market exhange indx fllhars stock prices t wsuhern eastern northen regio provinc percent rose bilion $milion dollars dvid michel hngeorge pul markst.james yorkwashingn %otlper10540annual county dstrict rober de texas losangeles florida sancalifornia X δ 2 9 d a m Y 40 20 Figure 1: Fast clustering based on density peaks of embeddings Neural networks have been used to model lanafixed-size feature representation for documents. of our method with experiments. Finall y, concluding remarks are offered in Section 6. 353 expand short texts. Chen et al. (2011) proved that 2 Related Works d information can be exploited. leveraging topics at multiple granularity can model short texts more precisely. guages, and the word embeddings can be learned simultaneously (Mnih and Teh, 2012). Mikolov et al. (2013b) introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale unstructured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to induce the semantic representation of texts. Le and Mikolov (2014) presented t</context>
<context position="15101" citStr="Chen et al., 2011" startWordPosition="2440" endWordPosition="2443">raining examples x, and 0 is the parameters set which comprises the kernels of weights used in convolutional layer and the connective weights from the fully connected layer. Embedding Senna2 GloVe3 Word2Vec4 Corpus Wikipedia Wikipedia Google News Dimension 50 50 300 |V ocab. |130,000 400,000 3,000,000 Table 1: Details of word embeddings Methods Google TREC Snippets Semantic-CNN Senna 83.6 96.4 GloVe 84.4 97.2 Word2Vec 85.1 95.6 DCNN – 93 (Kalchbrenner et al,2014) SVMS – 95 (Silva et al., 2011) CNN-TwoChannel – 93.6 (Kim, 2014) LDA+MaxEnt 82.7 – (Phan et al., 2008) Multi-Topics+MaxEnt 84.17 – (Chen et al., 2011) Table 2: The classification accuracy of proposed method against other models 5 Experiments 5.1 Datasets Experiments are conducted on two benchmarks: Google Snippets (Phan et al., 2008) and TREC (Li and Roth, 2002). Google Snippets This dataset consists of 10,060 training snippets and 2,280 test snippets from 8 categories. On average, each snippet has 18.07 words. TREC The TREC questions dataset contains 6 different question types. The training dataset consists of 5,452 labeled questions whereas the test dataset consists of 500 questions. 5.2 Experimental Setup Three pre-trained word embedding</context>
<context position="17084" citStr="Chen et al., 2011" startWordPosition="2772" endWordPosition="2775">.84 0.83 0.82 0.85 0.84 0.83 0.82 0.5 1 1.5 2 Euclidean Distance 0.845 0.835 0.85 0.84 0.83 1 2 3 4 3 4 5 6 Figure 4: Influence of threshold in SUs detection SVMs Parser, wh word, head word, POS, hypernyms, and 60 hand-coded rules were used as features to train SVMs (Silva et al., 2011). CNN-TwoChannel An improved CNN that allows task-specific and static word embeddings are used simultaneously (Kim, 2014). LDA+MaxEnt LDA was used to discover hidden topics for expanding short texts (Phan et al., 2008). Multi-topics+MaxEnt Multiple granularity topics from LDA were utilized to model short texts (Chen et al., 2011). For valid comparisons, we respectively initialize the lookup table with the word embeddings in Table 1, and three experiments are conducted for each benchmark. As a whole, our method achieves the best performance, especially for TREC with 97.2% when the GloVe word embedding is employed. For Google snippets, our method achieves the highest result of 85.1% corresponding to the word embedding induced by Word2Vec. 5.3.2 Effect of Hyper-parameters In Figure 2, for obtaining SUs with multi-scale, multiple window matrices with increasing width m are used. With respect to the variable m, the re2http</context>
</contexts>
<marker>Chen, Jin, Shen, 2011</marker>
<rawString>Mengen Chen, Xiaoming Jin, and Dou Shen. Short text classification improved by learning multigranularity topics. In IJCAI, pages 1776–1781. Citeseer, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188,</title>
<date>2014</date>
<contexts>
<context position="8687" citStr="Kalchbrenner et al. (2014)" startWordPosition="1361" endWordPosition="1364">on 2. Section 3 introduces the semantic clustering based on fast searching density peaks. Section 4 describes the architecture of the proposed method. Section 5 demonstrates the effectiveness Traditional statistics-based methods usually fail to achieve satisfactory performance for short texts classification due to their sparsity of representations (Sriram et al., 2010). Based on external Wikipedia corpus, Phan et al. (2008) proposed a method to discover hidden topics using LDA an &apos;Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual Kalchbrenner et al. (2014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Soch</context>
<context position="16159" citStr="Kalchbrenner et al. (2014)" startWordPosition="2609" endWordPosition="2612">es. The training dataset consists of 5,452 labeled questions whereas the test dataset consists of 500 questions. 5.2 Experimental Setup Three pre-trained word embeddings for initializing the lookup table are summarized in Table 1. To discover semantic cliques, we take pmin = 16 and Smin = 1.54. Through our experiments, 6 kernel matrices in convolutional layer, K = 3 for max pooling, and mini-batch size of 100 are used. 5.3 Results and Discussions 5.3.1 Comparison with state-of-the-art methods As shown in Table 2, we introduce 5 popular methods as baselines, and the details are described: DCNN Kalchbrenner et al. (2014) proposed DCNN for sentence modeling with dynamic k-max pooling. 355 0 2 4 6 0 2 4 6 0.8 0.93 0.85 0.97 0.84 Accuracy 0.96 0.83 0.95 0.82 0.94 0.81 Google Snippets TREC Senna GloVe Word2Vec Number of window matrices Figure 3: Number of windows for multi-scale SUs Senna GloVe Word2Vec Accuracy 0.835 0.825 0.84 0.83 0.82 0.85 0.84 0.83 0.82 0.5 1 1.5 2 Euclidean Distance 0.845 0.835 0.85 0.84 0.83 1 2 3 4 3 4 5 6 Figure 4: Influence of threshold in SUs detection SVMs Parser, wh word, head word, POS, hypernyms, and 60 hand-coded rules were used as features to train SVMs (Silva et al., 2011). CNN-</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053,</title>
<date>2014</date>
<contexts>
<context position="2454" citStr="Mikolov, 2014" startWordPosition="371" endWordPosition="372">Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). For learning word embedding, the training objective of continuous Skip-gram model (Mikolov et al., 2013b) is to predict its context. Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation. In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques). Moreover, the embedding spaces exhibit linear structure that the word vectors can be meaningfully combined using simple additive operation (Mikolov et al., 2013b), </context>
<context position="7144" citStr="Mikolov (2014)" startWordPosition="1125" endWordPosition="1126"> texts. Chen et al. (2011) proved that 2 Related Works d information can be exploited. leveraging topics at multiple granularity can model short texts more precisely. guages, and the word embeddings can be learned simultaneously (Mnih and Teh, 2012). Mikolov et al. (2013b) introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale unstructured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to induce the semantic representation of texts. Le and Mikolov (2014) presented the Paragraph Vector algorithm to learn −100 detect candidate Semantic Unitsl(abbr. to SUs) appearing in short texts. The part of candidate SUs meeting the preset threshold are chosen to constitute semantic matrices, which are used as input for the CNN, otherwise dropout. In this stage, semantic cliques are used as supervision information, which guarantee meaningful SUs can be extracted. The motivation of our work is to introduce extra knowledge by pre-trained word embeddings and fully exploit the contextual information of short texts to improve their representations. The main contr</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<volume>1631</volume>
<pages>1642</pages>
<location>Citeseer,</location>
<contexts>
<context position="9303" citStr="Socher et al. (2013)" startWordPosition="1462" endWordPosition="1465">014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data set, Hinton et al. (2012) used ran dom dropout to prevent complex co-adaptations. To exploit more structure information of text, based on CNN and direct embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, volume 1631, page 1642. Citeseer, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="2431" citStr="Collobert et al., 2011" startWordPosition="365" endWordPosition="368">ic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). For learning word embedding, the training objective of continuous Skip-gram model (Mikolov et al., 2013b) is to predict its context. Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation. In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques). Moreover, the embedding spaces exhibit linear structure that the word vectors can be meaningfully combined using simple additive operation (M</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493– 2537, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781,</title>
<date>2013</date>
<contexts>
<context position="2318" citStr="Mikolov et al., 2013" startWordPosition="349" endWordPosition="352">tensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). For learning word embedding, the training objective of continuous Skip-gram model (Mikolov et al., 2013b) is to predict its context. Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation. In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques). Moreover, the embedding spac</context>
<context position="6801" citStr="Mikolov et al. (2013" startWordPosition="1075" endWordPosition="1078">nual county dstrict rober de texas losangeles florida sancalifornia X δ 2 9 d a m Y 40 20 Figure 1: Fast clustering based on density peaks of embeddings Neural networks have been used to model lanafixed-size feature representation for documents. of our method with experiments. Finall y, concluding remarks are offered in Section 6. 353 expand short texts. Chen et al. (2011) proved that 2 Related Works d information can be exploited. leveraging topics at multiple granularity can model short texts more precisely. guages, and the word embeddings can be learned simultaneously (Mnih and Teh, 2012). Mikolov et al. (2013b) introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale unstructured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to induce the semantic representation of texts. Le and Mikolov (2014) presented the Paragraph Vector algorithm to learn −100 detect candidate Semantic Unitsl(abbr. to SUs) appearing in short texts. The part of candidate SUs meeting the preset threshold are chosen to constitute semantic matrices, which are used as input for t</context>
<context position="10275" citStr="Mikolov et al., 2013" startWordPosition="1616" endWordPosition="1619">ed on CNN and direct embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word relations to produce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering and CNN. 3 Semantic Clustering Since the neighbors of each word are semantically related in embedding space (Mikolov et al., 2013b), clustering methods (Rodriguez and Laio, 2014) can be used to discover semantic cliques. For implementation, two quantities of data point i are computed, include: local density pi, defined as follows, Epi = X(dij − dc) (3) j where dij is the distance between data points, dc is a cutoff distance. Furthermore, distance Si from points of higher density is measured by, An example of semantic clustering is illustrated in Figure 1. The decision graph shows the two quantities p and S of each word embedding. According to the definitions above, these word embeddings with large p and S simultaneously</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2318" citStr="Mikolov et al., 2013" startWordPosition="349" endWordPosition="352">tensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). For learning word embedding, the training objective of continuous Skip-gram model (Mikolov et al., 2013b) is to predict its context. Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation. In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques). Moreover, the embedding spac</context>
<context position="6801" citStr="Mikolov et al. (2013" startWordPosition="1075" endWordPosition="1078">nual county dstrict rober de texas losangeles florida sancalifornia X δ 2 9 d a m Y 40 20 Figure 1: Fast clustering based on density peaks of embeddings Neural networks have been used to model lanafixed-size feature representation for documents. of our method with experiments. Finall y, concluding remarks are offered in Section 6. 353 expand short texts. Chen et al. (2011) proved that 2 Related Works d information can be exploited. leveraging topics at multiple granularity can model short texts more precisely. guages, and the word embeddings can be learned simultaneously (Mnih and Teh, 2012). Mikolov et al. (2013b) introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale unstructured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to induce the semantic representation of texts. Le and Mikolov (2014) presented the Paragraph Vector algorithm to learn −100 detect candidate Semantic Unitsl(abbr. to SUs) appearing in short texts. The part of candidate SUs meeting the preset threshold are chosen to constitute semantic matrices, which are used as input for t</context>
<context position="10275" citStr="Mikolov et al., 2013" startWordPosition="1616" endWordPosition="1619">ed on CNN and direct embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word relations to produce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering and CNN. 3 Semantic Clustering Since the neighbors of each word are semantically related in embedding space (Mikolov et al., 2013b), clustering methods (Rodriguez and Laio, 2014) can be used to discover semantic cliques. For implementation, two quantities of data point i are computed, include: local density pi, defined as follows, Epi = X(dij − dc) (3) j where dij is the distance between data points, dc is a cutoff distance. Furthermore, distance Si from points of higher density is measured by, An example of semantic clustering is illustrated in Figure 1. The decision graph shows the two quantities p and S of each word embedding. According to the definitions above, these word embeddings with large p and S simultaneously</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="2318" citStr="Mikolov et al., 2013" startWordPosition="349" endWordPosition="352">tensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). For learning word embedding, the training objective of continuous Skip-gram model (Mikolov et al., 2013b) is to predict its context. Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation. In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques). Moreover, the embedding spac</context>
<context position="6801" citStr="Mikolov et al. (2013" startWordPosition="1075" endWordPosition="1078">nual county dstrict rober de texas losangeles florida sancalifornia X δ 2 9 d a m Y 40 20 Figure 1: Fast clustering based on density peaks of embeddings Neural networks have been used to model lanafixed-size feature representation for documents. of our method with experiments. Finall y, concluding remarks are offered in Section 6. 353 expand short texts. Chen et al. (2011) proved that 2 Related Works d information can be exploited. leveraging topics at multiple granularity can model short texts more precisely. guages, and the word embeddings can be learned simultaneously (Mnih and Teh, 2012). Mikolov et al. (2013b) introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale unstructured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to induce the semantic representation of texts. Le and Mikolov (2014) presented the Paragraph Vector algorithm to learn −100 detect candidate Semantic Unitsl(abbr. to SUs) appearing in short texts. The part of candidate SUs meeting the preset threshold are chosen to constitute semantic matrices, which are used as input for t</context>
<context position="10275" citStr="Mikolov et al., 2013" startWordPosition="1616" endWordPosition="1619">ed on CNN and direct embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word relations to produce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering and CNN. 3 Semantic Clustering Since the neighbors of each word are semantically related in embedding space (Mikolov et al., 2013b), clustering methods (Rodriguez and Laio, 2014) can be used to discover semantic cliques. For implementation, two quantities of data point i are computed, include: local density pi, defined as follows, Epi = X(dij − dc) (3) j where dij is the distance between data points, dc is a cutoff distance. Furthermore, distance Si from points of higher density is measured by, An example of semantic clustering is illustrated in Figure 1. The decision graph shows the two quantities p and S of each word embedding. According to the definitions above, these word embeddings with large p and S simultaneously</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746–751, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohui Yan</author>
<author>Jiafeng Guo</author>
<author>Yanyan Lan</author>
<author>Xueqi Cheng</author>
</authors>
<title>A biterm topic model for short texts.</title>
<date>2013</date>
<booktitle>In WWW,</booktitle>
<pages>1445--1456</pages>
<contexts>
<context position="1857" citStr="Yan et al. (2013)" startWordPosition="276" endWordPosition="279">ch ignore semantic relations between words (Sriram et al., 2010). How to acquire effective representation for short text has been an active research issue (Chen et al., 2011; Phan et al., 2008). In order to overcome the weakness of BoW, researchers have proposed to expand the representation of short text using latent semantics, where the words are mapped to distributional representations by Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its extensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). F</context>
</contexts>
<marker>Yan, Guo, Lan, Cheng, 2013</marker>
<rawString>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. A biterm topic model for short texts. In WWW, pages 1445–1456. International World Wide Web Conferences Steering Committee, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="15315" citStr="Li and Roth, 2002" startWordPosition="2473" endWordPosition="2476"> Wikipedia Wikipedia Google News Dimension 50 50 300 |V ocab. |130,000 400,000 3,000,000 Table 1: Details of word embeddings Methods Google TREC Snippets Semantic-CNN Senna 83.6 96.4 GloVe 84.4 97.2 Word2Vec 85.1 95.6 DCNN – 93 (Kalchbrenner et al,2014) SVMS – 95 (Silva et al., 2011) CNN-TwoChannel – 93.6 (Kim, 2014) LDA+MaxEnt 82.7 – (Phan et al., 2008) Multi-Topics+MaxEnt 84.17 – (Chen et al., 2011) Table 2: The classification accuracy of proposed method against other models 5 Experiments 5.1 Datasets Experiments are conducted on two benchmarks: Google Snippets (Phan et al., 2008) and TREC (Li and Roth, 2002). Google Snippets This dataset consists of 10,060 training snippets and 2,280 test snippets from 8 categories. On average, each snippet has 18.07 words. TREC The TREC questions dataset contains 6 different question types. The training dataset consists of 5,452 labeled questions whereas the test dataset consists of 500 questions. 5.2 Experimental Setup Three pre-trained word embeddings for initializing the lookup table are summarized in Table 1. To discover semantic cliques, we take pmin = 16 and Smin = 1.54. Through our experiments, 6 kernel matrices in convolutional layer, K = 3 for max pooli</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
<author>Le-Minh Nguyen</author>
<author>Susumu Horiguchi</author>
</authors>
<title>Learning to classify short and sparse text &amp; web with hidden topics from large-scale data collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>91--100</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1433" citStr="Phan et al., 2008" startWordPosition="205" endWordPosition="208">ul external knowledge for short texts. These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation. Experimental results on two open benchmarks validate the effectiveness of the proposed method. 1 Introduction Conventional texts mining methods based on bagof-words (BoW) easily encounter data sparsity and ambiguity problems in short text modeling (Chen et al., 2011), which ignore semantic relations between words (Sriram et al., 2010). How to acquire effective representation for short text has been an active research issue (Chen et al., 2011; Phan et al., 2008). In order to overcome the weakness of BoW, researchers have proposed to expand the representation of short text using latent semantics, where the words are mapped to distributional representations by Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its extensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above stil</context>
<context position="8488" citStr="Phan et al. (2008)" startWordPosition="1329" endWordPosition="1332">ine-tuning multiscale SUs, the semantic cliques are used to supervise the selection stage. The remainder of this paper is organized as follows. The related works are briefly reviewed in Section 2. Section 3 introduces the semantic clustering based on fast searching density peaks. Section 4 describes the architecture of the proposed method. Section 5 demonstrates the effectiveness Traditional statistics-based methods usually fail to achieve satisfactory performance for short texts classification due to their sparsity of representations (Sriram et al., 2010). Based on external Wikipedia corpus, Phan et al. (2008) proposed a method to discover hidden topics using LDA an &apos;Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual Kalchbrenner et al. (2014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings </context>
<context position="15053" citStr="Phan et al., 2008" startWordPosition="2433" endWordPosition="2436">†|xi, 0) + α110112 (11) t where t is number of training examples x, and 0 is the parameters set which comprises the kernels of weights used in convolutional layer and the connective weights from the fully connected layer. Embedding Senna2 GloVe3 Word2Vec4 Corpus Wikipedia Wikipedia Google News Dimension 50 50 300 |V ocab. |130,000 400,000 3,000,000 Table 1: Details of word embeddings Methods Google TREC Snippets Semantic-CNN Senna 83.6 96.4 GloVe 84.4 97.2 Word2Vec 85.1 95.6 DCNN – 93 (Kalchbrenner et al,2014) SVMS – 95 (Silva et al., 2011) CNN-TwoChannel – 93.6 (Kim, 2014) LDA+MaxEnt 82.7 – (Phan et al., 2008) Multi-Topics+MaxEnt 84.17 – (Chen et al., 2011) Table 2: The classification accuracy of proposed method against other models 5 Experiments 5.1 Datasets Experiments are conducted on two benchmarks: Google Snippets (Phan et al., 2008) and TREC (Li and Roth, 2002). Google Snippets This dataset consists of 10,060 training snippets and 2,280 test snippets from 8 categories. On average, each snippet has 18.07 words. TREC The TREC questions dataset contains 6 different question types. The training dataset consists of 5,452 labeled questions whereas the test dataset consists of 500 questions. 5.2 Exp</context>
<context position="16971" citStr="Phan et al., 2008" startWordPosition="2756" endWordPosition="2759">er of window matrices Figure 3: Number of windows for multi-scale SUs Senna GloVe Word2Vec Accuracy 0.835 0.825 0.84 0.83 0.82 0.85 0.84 0.83 0.82 0.5 1 1.5 2 Euclidean Distance 0.845 0.835 0.85 0.84 0.83 1 2 3 4 3 4 5 6 Figure 4: Influence of threshold in SUs detection SVMs Parser, wh word, head word, POS, hypernyms, and 60 hand-coded rules were used as features to train SVMs (Silva et al., 2011). CNN-TwoChannel An improved CNN that allows task-specific and static word embeddings are used simultaneously (Kim, 2014). LDA+MaxEnt LDA was used to discover hidden topics for expanding short texts (Phan et al., 2008). Multi-topics+MaxEnt Multiple granularity topics from LDA were utilized to model short texts (Chen et al., 2011). For valid comparisons, we respectively initialize the lookup table with the word embeddings in Table 1, and three experiments are conducted for each benchmark. As a whole, our method achieves the best performance, especially for TREC with 97.2% when the GloVe word embedding is employed. For Google snippets, our method achieves the highest result of 85.1% corresponding to the word embedding induced by Word2Vec. 5.3.2 Effect of Hyper-parameters In Figure 2, for obtaining SUs with mu</context>
</contexts>
<marker>Phan, Nguyen, Horiguchi, 2008</marker>
<rawString>Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu Horiguchi. Learning to classify short and sparse text &amp; web with hidden topics from large-scale data collections. In Proceedings of the 17th international conference on World Wide Web, pages 91–100. ACM, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>L´eon Bottou</author>
<author>Yoshua Bengio</author>
<author>Patrick Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>86</volume>
<issue>11</issue>
<contexts>
<context position="13857" citStr="LeCun et al., 1998" startWordPosition="2236" endWordPosition="2239">opout. 4.2 Convolution Layer In our network, the convolutional layer is used to extract local features. Kernel matrices k with certain width n are utilized to calculate convolution with the input matrices M, as Equation (7). C = [c1, c2, ··· ,cd/2]T = KT ⊗ M (7) where, K = [k1, k2, ··· , kd/2] (8) M = [Mwin 1 ,Mwin 2 ,···, Mwin d/2 ] (9) cj i = ki · (Mwin,j i )T (10) The cji is generated from the jth n-gram in M. Equation (7) produce the feature maps of convolutional layer. 4.3 K-Max Pooling This operator is a non-linear sub-sampling function that returns the sub-sequence of K maximum values (LeCun et al., 1998), which is used to capture the most relevant global features with fixedlength. Then, tangent transformation over the results of K-max pooling is performed, the output of which is concatenated to used as representation for the input short texts. 4.4 Network Training The last layer is fully connected, where a softmax classifier is applied to predict the probability distribution over categories. The network is trained with the objective that minimizes the cross-entropy of the predicted distributions and the actual distributions (Turian et al., 2010), 1 ∑t J(0) = −i�� log p(c†|xi, 0) + α110112 (11</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882,</title>
<date>2014</date>
<contexts>
<context position="8922" citStr="Kim (2014)" startWordPosition="1403" endWordPosition="1404">e satisfactory performance for short texts classification due to their sparsity of representations (Sriram et al., 2010). Based on external Wikipedia corpus, Phan et al. (2008) proposed a method to discover hidden topics using LDA an &apos;Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual Kalchbrenner et al. (2014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data</context>
<context position="15015" citStr="Kim, 2014" startWordPosition="2428" endWordPosition="2429">010), 1 ∑t J(0) = −i�� log p(c†|xi, 0) + α110112 (11) t where t is number of training examples x, and 0 is the parameters set which comprises the kernels of weights used in convolutional layer and the connective weights from the fully connected layer. Embedding Senna2 GloVe3 Word2Vec4 Corpus Wikipedia Wikipedia Google News Dimension 50 50 300 |V ocab. |130,000 400,000 3,000,000 Table 1: Details of word embeddings Methods Google TREC Snippets Semantic-CNN Senna 83.6 96.4 GloVe 84.4 97.2 Word2Vec 85.1 95.6 DCNN – 93 (Kalchbrenner et al,2014) SVMS – 95 (Silva et al., 2011) CNN-TwoChannel – 93.6 (Kim, 2014) LDA+MaxEnt 82.7 – (Phan et al., 2008) Multi-Topics+MaxEnt 84.17 – (Chen et al., 2011) Table 2: The classification accuracy of proposed method against other models 5 Experiments 5.1 Datasets Experiments are conducted on two benchmarks: Google Snippets (Phan et al., 2008) and TREC (Li and Roth, 2002). Google Snippets This dataset consists of 10,060 training snippets and 2,280 test snippets from 8 categories. On average, each snippet has 18.07 words. TREC The TREC questions dataset contains 6 different question types. The training dataset consists of 5,452 labeled questions whereas the test data</context>
<context position="16874" citStr="Kim, 2014" startWordPosition="2741" endWordPosition="2742">.84 Accuracy 0.96 0.83 0.95 0.82 0.94 0.81 Google Snippets TREC Senna GloVe Word2Vec Number of window matrices Figure 3: Number of windows for multi-scale SUs Senna GloVe Word2Vec Accuracy 0.835 0.825 0.84 0.83 0.82 0.85 0.84 0.83 0.82 0.5 1 1.5 2 Euclidean Distance 0.845 0.835 0.85 0.84 0.83 1 2 3 4 3 4 5 6 Figure 4: Influence of threshold in SUs detection SVMs Parser, wh word, head word, POS, hypernyms, and 60 hand-coded rules were used as features to train SVMs (Silva et al., 2011). CNN-TwoChannel An improved CNN that allows task-specific and static word embeddings are used simultaneously (Kim, 2014). LDA+MaxEnt LDA was used to discover hidden topics for expanding short texts (Phan et al., 2008). Multi-topics+MaxEnt Multiple granularity topics from LDA were utilized to model short texts (Chen et al., 2011). For valid comparisons, we respectively initialize the lookup table with the word embeddings in Table 1, and three experiments are conducted for each benchmark. As a whole, our method achieves the best performance, especially for TREC with 97.2% when the GloVe word embedding is employed. For Google snippets, our method achieves the highest result of 85.1% corresponding to the word embed</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="2296" citStr="Bengio et al., 2003" startWordPosition="345" endWordPosition="348">al., 2003) and its extensions. Phan et al. (2008) presented a general framework to expand the short and sparse text by appending topic names discovered using LDA. Yan et al. (2013) presented a variant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods discussed above still view a piece of text as BoW. Therefore, they are not effective in capturing finegrained semantic information for short texts modeling. Recently, neural network related methods have received much attention, including learning word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) and performing semantic composition to obtain phrase or sentence level representations (Collobert et al., 2011; Le and Mikolov, 2014). For learning word embedding, the training objective of continuous Skip-gram model (Mikolov et al., 2013b) is to predict its context. Thus, the cooccurrence information can be effectively used to describe a word, and each component of word embedding might have a semantic or grammatical interpretation. In embedding spaces, semantically close words are likely to cluster together and form semantic cliques (or word embedding cliques). Moreov</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Silva</author>
</authors>
<title>Lu´ısa Coheur, Ana Cristina Mendes, and Andreas Wichert. From symbolic to sub-symbolic information in question classification.</title>
<date>2011</date>
<journal>Artificial Intelligence Review,</journal>
<volume>35</volume>
<issue>2</issue>
<marker>Silva, 2011</marker>
<rawString>Joao Silva, Lu´ısa Coheur, Ana Cristina Mendes, and Andreas Wichert. From symbolic to sub-symbolic information in question classification. Artificial Intelligence Review, 35(2):137–154, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Johnson</author>
<author>Tong Zhang</author>
</authors>
<title>Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058,</title>
<date>2014</date>
<contexts>
<context position="9828" citStr="Johnson and Zhang, 2014" startWordPosition="1549" endWordPosition="1552">ce level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data set, Hinton et al. (2012) used ran dom dropout to prevent complex co-adaptations. To exploit more structure information of text, based on CNN and direct embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed (Johnson and Zhang, 2014). Although the popular methods can capture high-order information and word relations to produce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering and CNN. 3 Semantic Clustering Since the neighbors of each word are semantically related in embedding space (Mikolov et al., 2013b), clustering methods (Rodriguez and Laio, 2014) can be used to discover semantic cliques. For implementation, two quantities of data point i are comput</context>
</contexts>
<marker>Johnson, Zhang, 2014</marker>
<rawString>Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058, 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>