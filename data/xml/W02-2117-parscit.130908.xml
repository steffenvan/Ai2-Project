<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000107">
<title confidence="0.953236">
An Evaluation of Procedural Instructional Text
</title>
<note confidence="0.56419425">
Nathalie Colineau, Cécile Paris
CSIRO, Mathematical and Information Sciences,
Locked Bag 17, North Ryde, NSW 1670, AU
{nathalie.colineau,
</note>
<email confidence="0.974041">
cecile.paris}@csiro.au
</email>
<note confidence="0.612466333333333">
Keith Vander Linden
Dept. of Computer Science, Calvin College,
Grand Rapids, MI 49546, USA
</note>
<email confidence="0.91558">
kvlinden@calvin.edu
</email>
<sectionHeader confidence="0.995778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99983175">
This paper presents an evaluation of the
instructional text generated by Isolde, an
authoring tool for technical writers that
automates the production of procedural
on-line help. The evaluation compares the
effectiveness of the instructional text pro-
duced by Isolde with that of profession-
ally authored instructions, such as MS
Word Help. The results suggest that the
documentation produced by Isolde is of
comparable quality to similar texts found
in commercial manuals.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938244897959">
Instructional text is a useful and relatively con-
strained sub-language and has thus been a popular
target for research-oriented natural language gen-
eration (NLG) systems. Much work has been done
in this area, e.g., (Rösner &amp; Stede, 1992; Kosseim
&amp; Lapalme, 1994; Paris &amp; Vander Linden, 1996;
Power et al., 1998), demonstrating that existing
technology is adequate for generating draft instruc-
tions. However, only a few of these projects have
been formally evaluated, e.g., (Hartley et al.,
2000), and the evaluations performed have focus-
sed on the fluency and grammaticality of the out-
put text rather than on its effectiveness. This tends
to be the case, in fact, for evaluations of NLG sys-
tems in general. People are asked to rate the ac-
ceptability of the generated texts or to compare
them to human-authored texts - e.g., (Lester &amp; Por-
ter, 1997; Callaway &amp; Lester, 2001), without
measuring the actual impact of the texts on their
intended users.
The evaluation of the STOP system (Reiter et
al., 2001) is a notable exception to this trend.
STOP produced texts tailored to individual smok-
ers intended to convince them to stop smoking. As
an evaluation, the researchers performed a large-
scale study of how effective the generated texts
were at achieving this goal. Rather than checking
the output text for errors, or comparing its fluency
with that of hand-written text, they compared how
often readers of STOP&apos;s individually tailored texts
actually stopped smoking as compared to readers
of generic, generated texts. Thus, the evaluation
assessed the relative effectiveness of tailored and
generic texts at achieving their intended goals.
In our evaluation, we also sought to perform an
evaluation of the effectiveness of the instructional
texts produced by Isolde. Isolde1 is an authoring
tool for technical writers that automatically gener-
ates parts of a system&apos;s on-line help (Paris et al.,
1998b). In this domain, the writer&apos;s goal is to help
the users achieve their goals. It is thus crucial to
assess the effectiveness of the instructional texts in
a real task. Unlike the STOP evaluation, however,
we compared the effectiveness of our generated
texts with that of human-authored texts. In this pa-
per, we first introduce the type of texts that Isolde
generates and give an overview of Isolde. We then
present the evaluation we conducted and draw
some conclusions.
</bodyText>
<sectionHeader confidence="0.980235" genericHeader="method">
2 Procedural Help
</sectionHeader>
<bodyText confidence="0.99890476744186">
Documentation for interactive devices typically
includes conceptual help, business help, and pro-
cedural help (Paris et al., 1998a). Conceptual help
defines the concepts used in an application, busi-
ness help indicates how the application is embed-
ded in its context of use, and procedural help enu-
merates the series of steps required to perform a
goal. Isolde focuses on procedural help. Procedural
help, which can be seen as answering the question
&amp;quot;How to?&amp;quot;, is an attractive target text for NLG sys-
tems because it is:
Constrained: Procedural help describes a sys-
tem’s functions in terms of users’ actions on the
interface. It is highly structured and heavily
based on the system behavior. Its automatic
production thus seems realistic.
Significant within the help system: Procedural
help is a key element of &amp;quot;minimalist instruc-
tions&amp;quot; (Carroll 1990), whose philosophy is
based on the argument that learning software is
more effective when the software documenta-
tion is short, simple and directed towards real
work activities. This notion of brevity can also
be linked to the Grice’s maxim of quantity in
the discourse theory (Grice, 1979). Young
(1999), in fact, uses this principle to select the
content of plan descriptions and compute con-
cise instructions.
Routine and time consuming to produce: Tech-
nical writers consider procedural help as the
easiest but also the most routine and tedious
part of the documentation process because they
must systematically perform all possible func-
tions, recording their actions step by step. Writ-
ing procedural help can be the most time-
consuming activity of the technical writing
process (Paris et al., 1998a). It is thus desirable
to automate it, or at least support its production
(cf. Power et al., 1994).
Based on these motivations, Isolde aims at auto-
mating the production of procedural on-line help.
We believe this addresses an issue that is both im-
portant and relevant in documentation production.
</bodyText>
<sectionHeader confidence="0.960583" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.999942967741936">
Isolde provides an interactive on-line help drafting
tool aimed at supporting technical writers. From an
analysis of technical writers at work, we noted that
they typically begin by building a representation of
the application’s functionalities by executing tasks
step by step, recording all the steps and the inter-
face feedback (Power et al., 1994; Ozkan et al.,
1998). This representation ranges from textual to
semi-formal, e.g., flowcharts. Then, from this rep-
resentation, they write the text proper.
Isolde offers facilities to support and formalize
the first step, i.e., building a representation of the
application’s functionalities that can be reused later
in the documentation maintenance process. Isolde
then also automates the second step, i.e., the gen-
eration of the text. Isolde aims at being integrated
into both the writers’ environment and the software
design process.
The representation chosen for the application’s
functionalities is a task model, a semi-formal nota-
tion. It is graphically represented in the Diane+
notation (Tarby &amp; Barthet, 1996) and was tested
with technical writers for usability (Ozkan et al.,
1998). Figure 1 shows the Isolde architecture. The
task model can be manually entered using a dedi-
cated graphical editor Tamot. Alternatively, this
model can be acquired automatically or semi-
automatically from available sources of informa-
tion through a variety of tools - e.g., a text-to-task
extraction tool, an interaction recorder or a tool to
construct automatically a draft task model from the
</bodyText>
<figure confidence="0.996682041666667">
Knowledge
Acquisition
Tools
Task
Description
Text to
Task
UML
Model
UML to
Task
System
Prototype
...
TAMOT
Task/Domain Model in XML
Instruction
...
Generator
Analysis or
Application
Tools
Interaction
Recorder
</figure>
<figureCaption confidence="0.999989">
Figure 1: Isolde Architecture
</figureCaption>
<bodyText confidence="0.9917025">
output of a CASE tool2 (Lu et al., 1998; Vander
Linden et al., 2000). Note that the task model for
the application may have already been built by
HCI specialists as part of their involvement in
software team - e.g., (Balbo &amp; Lindley, 1997; Dia-
per &amp; Stanton, in press) and can then be re-used or
augmented by technical writers to ensure their
suitability for on-line help generation.
</bodyText>
<figureCaption confidence="0.999958">
Figure 2: Task model design to create mailing labels with MS Word
Figure 3: Hypertext output displayed in a Netscape browser - the mailing labels task
</figureCaption>
<bodyText confidence="0.999749517241379">
When satisfied with the task model, the techni-
cal writer exports the model to the generator for
the on-line help to be generated. In Isolde, the
technical writer refines the input and controls the
generation of instructions through Tamot. A task
model example, shown in Tamot, is presented in
Figure 2. This window includes a tree-structured
representation of the task hierarchy on the left, and
a graphical representation of the tasks on the right.
The Instruction generator uses: (1) the Moore &amp;
Paris text planner (Moore &amp; Paris, 1993); (2) a
sentence planner implemented as extensions to the
text planner; and (3) the KPML development envi-
ronment and lexico-grammatical resources (Bate-
man, 1997). This system is implemented as a LISP
server. It plans the instructions using discourse
plans that handle any task model configuration,
including sequences, compositions and Boolean
connectors. It plans hypertext links and can inte-
grate canned text with generated text. Style pa-
rameters have also been implemented, giving the
technical writers some control at the discourse or
sentence level. For example, writers can decide to
produce a concise text by aggregating simple tasks
and suppressing levels of decomposition, or they
can choose to produce step-by-step instructions, in
which each task decomposition is presented in a
separate frame. Figure 3 shows the instructions
generated for the task model in Figure 2.
</bodyText>
<sectionHeader confidence="0.544561" genericHeader="method">
4 Evaluation of the Generated Instruc-
tional Text
</sectionHeader>
<bodyText confidence="0.9916075">
Our experiment attempted to assess the effective-
ness of the help generated by Isolde as compared
with manually authored help. For both types of
texts, we:
</bodyText>
<listItem confidence="0.994081">
• Measured the user’s performance in accom-
plishing a specific task (i.e., task achieve-
ment and time needed); and
• Asked the users to rate the usefulness of the
texts, the adequacy of the content and the
coherence of the organization.
</listItem>
<bodyText confidence="0.999840833333333">
We did not evaluate grammatical correctness as
in AGILE (Hartley et al., 2000). Given the aim of
documentation, we need to ensure that the gener-
ated instructions allow the users to achieve or learn
about their task, whatever the quality or the com-
plexity of the text generated.
</bodyText>
<subsectionHeader confidence="0.982048">
4.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999336888888889">
Our methodology involved four steps: (1) choosing
three tasks in Word; (2) designing the three corre-
sponding task models; (3) producing the on-line
help with Isolde for these models, and (4) evaluat-
ing the help on two user groups: one group re-
ceived the Word help, the other the automatically
generated help. Participants to the experiment were
not aware of which help they were using.
Task Selection: To compare the effectiveness
of Isolde help with that of manually authored help,
we asked users to perform a real task, using the on-
line help as a resource. We decided to work with
Word, as it provides both a task environment and
on-line instructions. Thus, our aim was to select 3
tasks, 2 simple and 1 complex3 that would be new
for the subjects, to help prevent introducing a bias
based on prior knowledge and encourage users to
read the help. We also chose tasks such that their
Word help text was “self-contained” (i.e., without
extensive reference to other parts of the help) and
the text generated by Isolde for the task would be
of similar reading complexity. Our final constraint
was that the 2 simple tasks had the same number of
elementary actions. We had no prior assumption as
to what task would be easier to model or document
than any other task. With these constraints, we
chose:
</bodyText>
<listItem confidence="0.959817166666667">
• Task 1: create a document template and save
it in a specific directory;
• Task 2: create index entries;
• Task 3: create mailing labels by merging a
label template with an address list, and save
the label pattern.
</listItem>
<bodyText confidence="0.860658615384615">
Task Design: To generate the on-line help for
to these tasks, we first had to design the task mod-
els. We did so using Tamot. As typically done by
technical writers, we executed the tasks step by
step, recording all the steps. Feedback expressions
(e.g., display of windows, confirmation messages)
were also included, either as system actions, or as
notes or warnings with canned text. The aim was to
be as close as possible to the system behavior.
Hypertext Generation: When the task model
was completed, we generated the corresponding
on-line help. We then used the Flesch (1974)
score4 to
</bodyText>
<table confidence="0.998795666666667">
Task 1 Task 2 Task 3
Readability scores Word Help Isolde Help Word Help Isolde Help Word Help Isolde Help
Average Sentence 16.241 7.821 11.882 9.791 15.548 7.609
Length (ASL)
Flesh Reading Ease 62.821 70.644 68.712 75.671 77.668 71.951
Score
</table>
<tableCaption confidence="0.99997">
Table 1: Comparison of Word and Isolde readability score
</tableCaption>
<bodyText confidence="0.999928714285714">
compare the readability of the generated texts with
the Word help (see Table 1). This was to ensure
both texts would be of similar reading complexity
and would thus involve the same amount of time to
consult. For the experiment, only one of the help
texts was accessible to the user, displayed in a Net-
scape browser to preserve anonymity.
Formation of Subject Groups: A total of 35
subjects did the experiment (3 tasks per subject),
split into 2 groups. Subjects were randomly as-
signed to a group, but we ensured an even number
of men and women in each group. The subjects
were not expert in Word, but they knew how to use
the software.
</bodyText>
<subsectionHeader confidence="0.992958">
4.2 Scenario of Experiment
</subsectionHeader>
<bodyText confidence="0.9999604">
The experiment consisted of asking subjects to per-
form the 3 tasks described above with Word. For
each task, they were given some directions as to
what was expected of them (e.g., create a template
with the CSIRO logo, and save it in a specific di-
rectory). The directions did not include explana-
tions on how to achieve the task. These were to be
found in the on-line help provided. Subjects could
consult the help at any time (i.e., before or while
performing the task). Subjects were told to read the
directions and ask for clarification if required be-
fore starting on the task. After each task, they filled
out a questionnaire asking them to rate the help
they used.
The questionnaire aimed at evaluating the use-
fulness of the help, the quality of its content (i.e.,
its quantity and relevance) and the coherence of its
organization. The questions asked and the factors
of acceptability that they rate are shown in Table 2.
Each question was answered using a six-point
scale, assigning letter grades A (high) through F
(low). These letters were later converted into digits
from 6 to 1 for the statistical analysis. The ques-
tionnaire also included questions that checked the
users’ previous level of familiarity with the task.
During the experiment, we recorded the time to
measure the user’s performance. We limited the
allowable time (10 minutes for the simple tasks
and 15 minutes for the complex task) to encourage
the users to consult the help instead of exploring
the application by themselves5. We observed
whether subjects consulted the help or not, and the
number of times they did so. Finally, to evaluate
the success rate on each task, we recorded the er-
rors made. The marking scale was set as follows:
</bodyText>
<listItem confidence="0.989267">
• For Task 1, the subject lost one point if the
document was saved in the wrong directory
and two points if it was not saved in the
template format.
• For Task 2, the subject lost one point for
each index entry that they marked incor-
rectly.
• For Task 3, the subject lost one point if the
</listItem>
<bodyText confidence="0.812032307692308">
mailing labels were not created, and another
point if the label pattern was not saved.
Group 1 was assigned Word help for Tasks 1
and 3, and Isolde help for Task 2, while Group 2
was assigned Isolde help for Task 1 and 3, and
Word for Task 2.
Factors Questions
Usefulness How would you evaluate the usefulness of the help?
Adequacy of Did the help provide you with enough information to perform the task?
Content
Was the information provided in the help relevant for your task?
Coherence Did the help give you a clear picture of the steps required to accomplish the task?
How well was the help organised?
</bodyText>
<tableCaption confidence="0.985141">
Table 2: Grading factors presented to subjects
</tableCaption>
<subsectionHeader confidence="0.998541">
4.3 The Results
</subsectionHeader>
<bodyText confidence="0.999243588235294">
Because we wanted to assess the effectiveness of
the help in aiding a user to accomplish a task, we
first screened out users who knew the tasks before
hand (based on the questionnaire)6, and those who
did not consult the help at all (based on our obser-
vations). As a result, we were left with 12 subjects
out of 35 for Task 1, 34 for Task 2, and 29 for
Task 3. We analyzed the data for task performance
in terms of the time it took to finish the task and
the number of errors made. In all cases, we ran an
Anova single factor, and, when results are signifi-
cant, we report them for a 0.05 level of confidence.
Results on Task Performance: With respect to
errors, there was no evidence that either help was
more effective than the other. The small differ-
ences observed were not statistically significant.
This is shown in Figure 4.
</bodyText>
<figure confidence="0.777791">
0 1 2 3 4 5 6
</figure>
<figureCaption confidence="0.999617">
Figure 4: Task performance comparison - all tasks
</figureCaption>
<bodyText confidence="0.99459825">
With respect to time, we observed interesting
differences that were contrary to our expectations.
Table 3 presents the results obtained by running an
Anova. The times are reported in seconds.
</bodyText>
<table confidence="0.992857428571429">
Tasks 1 &amp; 2 Task 2 Task 3
Isolde Help 388.86 428.58 398.83
Word Help 278.91 296.70 553.63
Difference 109.94 131.88 154.80
Anova (F-test) 5.77 6.33 7.82
Level of Confi- 0.02 0.01 0.01
dence
</table>
<tableCaption confidence="0.999878">
Table 3: Time Performance (in seconds)
</tableCaption>
<bodyText confidence="0.995645375">
The first column combines Tasks 1 and 2; Task
1 alone did not allow separate computation due to
the small number of subjects. The difference in
time performance was in favor of Word for the
simple tasks (indicated in italics in the table), while
it was in favor of Isolde for the complex task (indi-
cated in bold).
Results on acceptability of the Help: Figures
5, 6 and 7 show the ratings of the different help
texts for the different tasks, based on the responses
on the questionnaire. We computed means for both
Isolde and Word, using the six-point scale, for each
task and for each of the help dimension we would
like to observe: (1) usefulness regarding the task,
(2) quality of content provided, and (3) coherence
and clarity of the help organization.
</bodyText>
<table confidence="0.98776125">
Marks given
Overall Usefulness Content Organisation
Isolde 4.39 4.14 4.68 4.23
Word 4.63 4.43 4.93 4.41
</table>
<figureCaption confidence="0.982244">
Figure 5: Rate for the simple tasks (Task 1 and 2)
</figureCaption>
<figure confidence="0.996944857142857">
6
5
4
3
2
1
0
</figure>
<figureCaption confidence="0.999855">
Figure 6: Rate for the complex task (Task 3)
</figureCaption>
<bodyText confidence="0.999236">
The Overall column summarizes these different
values. As shown in Figures 5, 6 and 7, Isolde
scored closely to Word, within approximately 1/4
of a point for the content and the organization, and
1/3 of a point for the usefulness. In all cases, both
Isolde and Word were positively evaluated on av-
erage.
</bodyText>
<figure confidence="0.99837475">
Number of
subjects
Word
Isolde
25
20
15
10
5
0
0 error
21
19
1 error
8
9
2 errors
10
8
Marks given
Overall Usefulness Content Organisation
4.15
3.87
4.43
3.87
Isolde
Word 4.29 4.36 4.46 4.07
6
5
4
3
2
1
0
Isolde 4.29 4.03 4.58 4.14
Word 4.50 4.41 4.76 4.28
</figure>
<figureCaption confidence="0.999975">
Figure 7: Rate combining all the tasks
</figureCaption>
<bodyText confidence="0.99998775">
We checked whether the differences were sig-
nificant or not by running an Anova on each task
for each dimension. The results did not show any
significant differences between the two help texts.
</bodyText>
<subsectionHeader confidence="0.567944">
Usefulness Content Organisation
</subsectionHeader>
<table confidence="0.7572825">
1.61 0.67 0.00
0.20 0.41 0.94
</table>
<tableCaption confidence="0.99421">
Table 4: Anova result combining all the tasks
</tableCaption>
<bodyText confidence="0.859759666666667">
Table 4 reports the results obtained when the
scores are aggregated over all the tasks, though we
also performed the test for each separate task. Our
results indicate that, in terms of acceptability and
usefulness of the help, Isolde’s performance ap-
proaches that of the manually authored texts.
</bodyText>
<sectionHeader confidence="0.997545" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999484">
People are usually reluctant to consult help and
typically consider on-line help to be unuseful. Our
work aims at providing help texts such that users
can quickly find the information they need. Our
challenge then was to provide enough relevant in-
formation, without extraneous details, within the
constraints imposed by the knowledge available to
generate text automatically. As shown in Table 1,
the help generated by Isolde and manually-
authored texts are similar in terms of their readabil-
ity score. Isolde, however, generates shorter sen-
tences, with strictly the information required to
achieve a task. While our generated texts thus of-
ten contain less information than manually-
authored texts (because of the knowledge available
to produce them), they constitute less text to
browse (i.e., each instruction is shorter) and may
thus make it easier to access important informa-
tion. From the experiment, it seems that providing
this type of text has a significant impact on com-
plex tasks (where the amount of consultation and
the time spent understanding the tasks are greater),
but no impact on simple tasks (where users seem
more comfortable reading the manually-authored
texts).
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999984235294118">
This paper has discussed an evaluation of the pro-
cedural instructions generated by the Isolde author-
ing tool. The evaluation compared the
effectiveness of the instructional texts generated by
Isolde with those written by technical writers in the
context of a real task. The results showed: (1) no
significant differences with respect to the number
of errors made while performing the tasks; (2)
some significant differences in time performance,
in favour of Isolde for complex task and in favor of
the manually-authored texts for simple tasks; and
finally, (3) no significant differences with respect
to the acceptability of the help. These results are
encouraging because they show that the effective-
ness of Isolde’s automatically generated texts is
comparable with that of manually-authored texts,
even though they often contain less information.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999870166666667">
We wish to thank Lu, S., past members of the
Isolde team, the people who participated in the ex-
periment and Bétrancourt, M. for her advice during
the experiment. We acknowledge the support of
ONR (grant N00014-99-0906), CSIRO and Calvin
College.
</bodyText>
<sectionHeader confidence="0.997981" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9236022">
Balbo, S. &amp; Lindley, C. (1997). Adaptation of a task
analysis methodology. In Proc. of Interact’97. Syd-
ney: Australia, Chapman and Hall. 355-361.
Bateman, J. (1997). Enabling technology for multilin-
gual natural language generation: the KPML devel-
opment environment. In Natural Language
Engineering, 3(1):15-55.
Callaway, C. &amp; Lester, J. (2001). Evaluating the effects
of natural language generation techniques on reader
satisfaction. In Proc. of CogSci’01, Edinburgh, UK.
</reference>
<figure confidence="0.995669666666667">
Marks given
Overall Usefulness Content Organisation
Anova
(F-test)
Level of
Confidence
Overall
2.18
0.14
</figure>
<reference confidence="0.992804594936709">
Carroll, J. (1990). The Nurnberg Funnel: Designing
Minimalist Instruction for Practical Computer Skill.
MIT Press, Cambridge, Massachusetts.
Diaper, D. &amp; Stanton, N. (Eds)(in press). The Handbook
of Task Analysis for Human-Computer Interaction.
Lawrence Erlbaum Associates.
Flesch, R. (1974). The art of readable writing. Harper-
collins.
Grice, H.P. (1979). Logique et Conversations. In
Communications nº30, 57-72.
Hartley, T., Scott, D., Kruijff-Korbayová, I., Sharoff, S.,
Sokolova, L., Dochev, D., Staykova, K., Cmejrek,
M., Hana, J. &amp; Teich E. (2000). Evaluation of the fi-
nal prototype, deliverable of the AGILE project.
URL: http://www.itri.brighton.ac.uk/projects/agile
Kosseim L. &amp; Lapalme G. (1994). Content and rhetori-
cal status selection in instructional text. In Proc. of
INLG, Kennebunkport, ME. 53-60.
Lester, J. &amp; Porter, B. (1997). Developing and empiri-
cally evaluating robust explanation generators: The
KNIGHT experiments. In Computational Linguistics,
23(1):65-101.
Lu, S., Paris, C. &amp; Vander Linden, K. (1998). Towards
the Automatic Construction of Task Models from
Object-Oriented Diagrams. In Chatty, S., and Dewan,
P., (Eds.) Engineering for Human-Computer Interac-
tion, Kluwer Academic Publishers, 169-189.
Moore, J. &amp; Paris, C. (1993). Planning text for advisory
dialogues: Capturing intentional and rhetorical in-
formation. Computational Linguistics 19(4):651-694.
Ozkan, N., Paris, C. &amp; Balbo, S. (1998). Understanding
a Task Model: An Experiment. In Proc. of HCI’98,
H. Johnson. K. Nigay and C. Roast (Eds), Springer
123-138.
Paris, C. &amp; Vander Linden, K. (1996). Drafter: An In-
teractive Support Tool for Writing Multilingual
Manuals, IEEE Computer, 29(7):49-56.
Paris, C., Ozkan, N. &amp; Bonifacio, F. (1998a). The De-
sign of New Technology for Writing On-Line Help.
In Proc. HCI’98, H. Johnson. K. Nigay and C. Roast
(Eds), Springer, 189-206.
Paris, C., Ozkan, N. &amp; Bonifacio, F. (1998b). Novel
Help for On-Line Help. In ACM SIGDOC’98, Que-
bec City, Canada, September, 70-79.
Paris, C., Tarby, J. &amp; Vander Linden, K., (2001). A
Flexible Environment for Building Task Models. In
Proc. of the HCI’01, Lille, France.
Power, R., Pemberton, L., Hartley, A. &amp; Gorman, L.
(1994) Drafter: User requirements analysis, WP2 De-
liverable, Drafter Project IED4/1/5827.
Power, R., Scott, D. &amp; Evans, R. (1998). What You See
is What You Meant: direct knowledge editing with
natural language feedback, In Proc. of ECAI’98,
Brighton, UK, August.
Reiter, E., Robertson, R., Lennox A. S. &amp; Osman, L.
(2001). Using a randomised controlled clinical trial
to evaluate an NLG system. In Proc. of ACL’01, Tou-
louse, France, 434-441.
Rösner, D. &amp; Stede, M. (1992). TECHDOC: A system
for the automatic production of multilingual technical
documents. In: G. Görz (Eds): KONVENS 92 - Proc.
of the German conference on natural language proc-
essing. Springer, Berlin/Heidelberg.
Tarby, J.C. &amp; Barthet, M.F. (1996). The DIANE+
Method. In Proc. of CADUI’96. Namur, June 5-7,
J.Vanderdonckt (ed). 95-119.
Vander Linden, K., Paris, C. &amp; Lu, S. (2000). &amp;quot;Where
Do Instructions Come From?&amp;quot; Knowledge Acquisi-
tion and Specification for Instructional Text. In
IMPACTS in Natural Language Generation: NLG
Between Technology and Applications, Schloss
Dagstuhl, Germany. Becker, T. &amp; Busemann, S.
(Eds). DFKI report D-00-01, 1-10.
Young, R.M. (1999). Using Grice&apos;s maxim of quantity
to select the content of plan descriptions. Artificial
Intelligence (115), 215-256.
1 Integrated Software and On-Line Documentation En-
vironment.
2 Computer Aided Software Engineering Tool.
</reference>
<bodyText confidence="0.809124842105263">
3 The difference between a simple task and a complex
one is the number of elementary actions required to per-
form the task (18 vs. 40, in our experiment). It is impor-
tant to note that the higher the number of elementary
steps, the deeper the decomposition will be.
4 A standard document has a Flesh Reading score of
approximately 60 to 70. The higher the score, the easiest
a document is considered to be.
5 This is not meant to go against the minimalist ap-
proach, which intends to encourage an exploration of
the system in a learning environment. This is a means to
control our experimental situation and observe the
effectiveness of the help in a task situation.
6 The subjects who knew the tasks were not filtered
ahead of time to allow us to observe task performance
and help usage differences between “novices” and “ex-
perts”. No major differences were found (due to the lack
of subjects), but it was still interesting to have qualita-
tive input on these issues to inform future experiments.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.398919">
<title confidence="0.994021">An Evaluation of Procedural Instructional Text</title>
<author confidence="0.551857">Nathalie Colineau</author>
<author confidence="0.551857">Cécile</author>
<affiliation confidence="0.598566">CSIRO, Mathematical and Information</affiliation>
<address confidence="0.565838">Locked Bag 17, North Ryde, NSW 1670, AU</address>
<email confidence="0.9891">cecile.paris}@csiro.au</email>
<author confidence="0.959575">Keith Vander Linden</author>
<affiliation confidence="0.999258">Dept. of Computer Science, Calvin</affiliation>
<address confidence="0.961901">Grand Rapids, MI 49546, USA</address>
<email confidence="0.999695">kvlinden@calvin.edu</email>
<abstract confidence="0.997262846153846">This paper presents an evaluation of the instructional text generated by Isolde, an authoring tool for technical writers that automates the production of procedural on-line help. The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help. The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Balbo</author>
<author>C Lindley</author>
</authors>
<title>Adaptation of a task analysis methodology.</title>
<date>1997</date>
<booktitle>In Proc. of Interact’97.</booktitle>
<pages>355--361</pages>
<publisher>Australia, Chapman and Hall.</publisher>
<location>Sydney:</location>
<contexts>
<context position="7177" citStr="Balbo &amp; Lindley, 1997" startWordPosition="1128" endWordPosition="1131">riety of tools - e.g., a text-to-task extraction tool, an interaction recorder or a tool to construct automatically a draft task model from the Knowledge Acquisition Tools Task Description Text to Task UML Model UML to Task System Prototype ... TAMOT Task/Domain Model in XML Instruction ... Generator Analysis or Application Tools Interaction Recorder Figure 1: Isolde Architecture output of a CASE tool2 (Lu et al., 1998; Vander Linden et al., 2000). Note that the task model for the application may have already been built by HCI specialists as part of their involvement in software team - e.g., (Balbo &amp; Lindley, 1997; Diaper &amp; Stanton, in press) and can then be re-used or augmented by technical writers to ensure their suitability for on-line help generation. Figure 2: Task model design to create mailing labels with MS Word Figure 3: Hypertext output displayed in a Netscape browser - the mailing labels task When satisfied with the task model, the technical writer exports the model to the generator for the on-line help to be generated. In Isolde, the technical writer refines the input and controls the generation of instructions through Tamot. A task model example, shown in Tamot, is presented in Figure 2. T</context>
</contexts>
<marker>Balbo, Lindley, 1997</marker>
<rawString>Balbo, S. &amp; Lindley, C. (1997). Adaptation of a task analysis methodology. In Proc. of Interact’97. Sydney: Australia, Chapman and Hall. 355-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bateman</author>
</authors>
<title>Enabling technology for multilingual natural language generation: the KPML development environment.</title>
<date>1997</date>
<booktitle>In Natural Language Engineering,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="8172" citStr="Bateman, 1997" startWordPosition="1292" endWordPosition="1294">erator for the on-line help to be generated. In Isolde, the technical writer refines the input and controls the generation of instructions through Tamot. A task model example, shown in Tamot, is presented in Figure 2. This window includes a tree-structured representation of the task hierarchy on the left, and a graphical representation of the tasks on the right. The Instruction generator uses: (1) the Moore &amp; Paris text planner (Moore &amp; Paris, 1993); (2) a sentence planner implemented as extensions to the text planner; and (3) the KPML development environment and lexico-grammatical resources (Bateman, 1997). This system is implemented as a LISP server. It plans the instructions using discourse plans that handle any task model configuration, including sequences, compositions and Boolean connectors. It plans hypertext links and can integrate canned text with generated text. Style parameters have also been implemented, giving the technical writers some control at the discourse or sentence level. For example, writers can decide to produce a concise text by aggregating simple tasks and suppressing levels of decomposition, or they can choose to produce step-by-step instructions, in which each task dec</context>
</contexts>
<marker>Bateman, 1997</marker>
<rawString>Bateman, J. (1997). Enabling technology for multilingual natural language generation: the KPML development environment. In Natural Language Engineering, 3(1):15-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callaway</author>
<author>J Lester</author>
</authors>
<title>Evaluating the effects of natural language generation techniques on reader satisfaction.</title>
<date>2001</date>
<booktitle>In Proc. of CogSci’01,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="1687" citStr="Callaway &amp; Lester, 2001" startWordPosition="256" endWordPosition="259">Lapalme, 1994; Paris &amp; Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Porter, 1997; Callaway &amp; Lester, 2001), without measuring the actual impact of the texts on their intended users. The evaluation of the STOP system (Reiter et al., 2001) is a notable exception to this trend. STOP produced texts tailored to individual smokers intended to convince them to stop smoking. As an evaluation, the researchers performed a largescale study of how effective the generated texts were at achieving this goal. Rather than checking the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actually stopped smoking as com</context>
</contexts>
<marker>Callaway, Lester, 2001</marker>
<rawString>Callaway, C. &amp; Lester, J. (2001). Evaluating the effects of natural language generation techniques on reader satisfaction. In Proc. of CogSci’01, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>The Nurnberg Funnel: Designing Minimalist Instruction for Practical Computer Skill.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4052" citStr="Carroll 1990" startWordPosition="637" endWordPosition="638">n is embedded in its context of use, and procedural help enumerates the series of steps required to perform a goal. Isolde focuses on procedural help. Procedural help, which can be seen as answering the question &amp;quot;How to?&amp;quot;, is an attractive target text for NLG systems because it is: Constrained: Procedural help describes a system’s functions in terms of users’ actions on the interface. It is highly structured and heavily based on the system behavior. Its automatic production thus seems realistic. Significant within the help system: Procedural help is a key element of &amp;quot;minimalist instructions&amp;quot; (Carroll 1990), whose philosophy is based on the argument that learning software is more effective when the software documentation is short, simple and directed towards real work activities. This notion of brevity can also be linked to the Grice’s maxim of quantity in the discourse theory (Grice, 1979). Young (1999), in fact, uses this principle to select the content of plan descriptions and compute concise instructions. Routine and time consuming to produce: Technical writers consider procedural help as the easiest but also the most routine and tedious part of the documentation process because they must sy</context>
</contexts>
<marker>Carroll, 1990</marker>
<rawString>Carroll, J. (1990). The Nurnberg Funnel: Designing Minimalist Instruction for Practical Computer Skill. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Diaper</author>
<author>N Stanton</author>
</authors>
<title>(Eds)(in press). The Handbook of Task Analysis for Human-Computer Interaction. Lawrence Erlbaum Associates.</title>
<marker>Diaper, Stanton, </marker>
<rawString>Diaper, D. &amp; Stanton, N. (Eds)(in press). The Handbook of Task Analysis for Human-Computer Interaction. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Flesch</author>
</authors>
<title>The art of readable writing.</title>
<date>1974</date>
<publisher>Harpercollins.</publisher>
<contexts>
<context position="11725" citStr="Flesch (1974)" startWordPosition="1894" endWordPosition="1895">ist, and save the label pattern. Task Design: To generate the on-line help for to these tasks, we first had to design the task models. We did so using Tamot. As typically done by technical writers, we executed the tasks step by step, recording all the steps. Feedback expressions (e.g., display of windows, confirmation messages) were also included, either as system actions, or as notes or warnings with canned text. The aim was to be as close as possible to the system behavior. Hypertext Generation: When the task model was completed, we generated the corresponding on-line help. We then used the Flesch (1974) score4 to Task 1 Task 2 Task 3 Readability scores Word Help Isolde Help Word Help Isolde Help Word Help Isolde Help Average Sentence 16.241 7.821 11.882 9.791 15.548 7.609 Length (ASL) Flesh Reading Ease 62.821 70.644 68.712 75.671 77.668 71.951 Score Table 1: Comparison of Word and Isolde readability score compare the readability of the generated texts with the Word help (see Table 1). This was to ensure both texts would be of similar reading complexity and would thus involve the same amount of time to consult. For the experiment, only one of the help texts was accessible to the user, displa</context>
</contexts>
<marker>Flesch, 1974</marker>
<rawString>Flesch, R. (1974). The art of readable writing. Harpercollins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logique et Conversations.</title>
<date>1979</date>
<booktitle>In Communications nº30,</booktitle>
<pages>57--72</pages>
<contexts>
<context position="4341" citStr="Grice, 1979" startWordPosition="684" endWordPosition="685">ined: Procedural help describes a system’s functions in terms of users’ actions on the interface. It is highly structured and heavily based on the system behavior. Its automatic production thus seems realistic. Significant within the help system: Procedural help is a key element of &amp;quot;minimalist instructions&amp;quot; (Carroll 1990), whose philosophy is based on the argument that learning software is more effective when the software documentation is short, simple and directed towards real work activities. This notion of brevity can also be linked to the Grice’s maxim of quantity in the discourse theory (Grice, 1979). Young (1999), in fact, uses this principle to select the content of plan descriptions and compute concise instructions. Routine and time consuming to produce: Technical writers consider procedural help as the easiest but also the most routine and tedious part of the documentation process because they must systematically perform all possible functions, recording their actions step by step. Writing procedural help can be the most timeconsuming activity of the technical writing process (Paris et al., 1998a). It is thus desirable to automate it, or at least support its production (cf. Power et a</context>
</contexts>
<marker>Grice, 1979</marker>
<rawString>Grice, H.P. (1979). Logique et Conversations. In Communications nº30, 57-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hartley</author>
<author>D Scott</author>
<author>I Kruijff-Korbayová</author>
<author>S Sharoff</author>
<author>L Sokolova</author>
<author>D Dochev</author>
<author>K Staykova</author>
<author>M Cmejrek</author>
<author>J Hana</author>
<author>E Teich</author>
</authors>
<title>Evaluation of the final prototype, deliverable of the AGILE project.</title>
<date>2000</date>
<note>URL: http://www.itri.brighton.ac.uk/projects/agile</note>
<contexts>
<context position="1310" citStr="Hartley et al., 2000" startWordPosition="188" endWordPosition="191">at the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals. 1 Introduction Instructional text is a useful and relatively constrained sub-language and has thus been a popular target for research-oriented natural language generation (NLG) systems. Much work has been done in this area, e.g., (Rösner &amp; Stede, 1992; Kosseim &amp; Lapalme, 1994; Paris &amp; Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Porter, 1997; Callaway &amp; Lester, 2001), without measuring the actual impact of the texts on their intended users. The evaluation of the STOP system (Reiter et al., 2001) is a notable exception to this trend. STOP produced texts tailored to individual smokers in</context>
<context position="9408" citStr="Hartley et al., 2000" startWordPosition="1489" endWordPosition="1492">resented in a separate frame. Figure 3 shows the instructions generated for the task model in Figure 2. 4 Evaluation of the Generated Instructional Text Our experiment attempted to assess the effectiveness of the help generated by Isolde as compared with manually authored help. For both types of texts, we: • Measured the user’s performance in accomplishing a specific task (i.e., task achievement and time needed); and • Asked the users to rate the usefulness of the texts, the adequacy of the content and the coherence of the organization. We did not evaluate grammatical correctness as in AGILE (Hartley et al., 2000). Given the aim of documentation, we need to ensure that the generated instructions allow the users to achieve or learn about their task, whatever the quality or the complexity of the text generated. 4.1 Experimental Design Our methodology involved four steps: (1) choosing three tasks in Word; (2) designing the three corresponding task models; (3) producing the on-line help with Isolde for these models, and (4) evaluating the help on two user groups: one group received the Word help, the other the automatically generated help. Participants to the experiment were not aware of which help they we</context>
</contexts>
<marker>Hartley, Scott, Kruijff-Korbayová, Sharoff, Sokolova, Dochev, Staykova, Cmejrek, Hana, Teich, 2000</marker>
<rawString>Hartley, T., Scott, D., Kruijff-Korbayová, I., Sharoff, S., Sokolova, L., Dochev, D., Staykova, K., Cmejrek, M., Hana, J. &amp; Teich E. (2000). Evaluation of the final prototype, deliverable of the AGILE project. URL: http://www.itri.brighton.ac.uk/projects/agile</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kosseim</author>
<author>G Lapalme</author>
</authors>
<title>Content and rhetorical status selection in instructional text.</title>
<date>1994</date>
<booktitle>In Proc. of INLG,</booktitle>
<pages>53--60</pages>
<location>Kennebunkport, ME.</location>
<contexts>
<context position="1076" citStr="Kosseim &amp; Lapalme, 1994" startWordPosition="152" endWordPosition="155"> automates the production of procedural on-line help. The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help. The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals. 1 Introduction Instructional text is a useful and relatively constrained sub-language and has thus been a popular target for research-oriented natural language generation (NLG) systems. Much work has been done in this area, e.g., (Rösner &amp; Stede, 1992; Kosseim &amp; Lapalme, 1994; Paris &amp; Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Porter, 1997; Callaway &amp; Le</context>
</contexts>
<marker>Kosseim, Lapalme, 1994</marker>
<rawString>Kosseim L. &amp; Lapalme G. (1994). Content and rhetorical status selection in instructional text. In Proc. of INLG, Kennebunkport, ME. 53-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lester</author>
<author>B Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The KNIGHT experiments.</title>
<date>1997</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>23--1</pages>
<contexts>
<context position="1661" citStr="Lester &amp; Porter, 1997" startWordPosition="251" endWordPosition="255">Stede, 1992; Kosseim &amp; Lapalme, 1994; Paris &amp; Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Porter, 1997; Callaway &amp; Lester, 2001), without measuring the actual impact of the texts on their intended users. The evaluation of the STOP system (Reiter et al., 2001) is a notable exception to this trend. STOP produced texts tailored to individual smokers intended to convince them to stop smoking. As an evaluation, the researchers performed a largescale study of how effective the generated texts were at achieving this goal. Rather than checking the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actua</context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>Lester, J. &amp; Porter, B. (1997). Developing and empirically evaluating robust explanation generators: The KNIGHT experiments. In Computational Linguistics, 23(1):65-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lu</author>
<author>C Paris</author>
<author>Vander Linden</author>
<author>K</author>
</authors>
<title>Towards the Automatic Construction of Task Models from Object-Oriented Diagrams. In</title>
<date>1998</date>
<pages>169--189</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="6978" citStr="Lu et al., 1998" startWordPosition="1093" endWordPosition="1096">can be manually entered using a dedicated graphical editor Tamot. Alternatively, this model can be acquired automatically or semiautomatically from available sources of information through a variety of tools - e.g., a text-to-task extraction tool, an interaction recorder or a tool to construct automatically a draft task model from the Knowledge Acquisition Tools Task Description Text to Task UML Model UML to Task System Prototype ... TAMOT Task/Domain Model in XML Instruction ... Generator Analysis or Application Tools Interaction Recorder Figure 1: Isolde Architecture output of a CASE tool2 (Lu et al., 1998; Vander Linden et al., 2000). Note that the task model for the application may have already been built by HCI specialists as part of their involvement in software team - e.g., (Balbo &amp; Lindley, 1997; Diaper &amp; Stanton, in press) and can then be re-used or augmented by technical writers to ensure their suitability for on-line help generation. Figure 2: Task model design to create mailing labels with MS Word Figure 3: Hypertext output displayed in a Netscape browser - the mailing labels task When satisfied with the task model, the technical writer exports the model to the generator for the on-li</context>
</contexts>
<marker>Lu, Paris, Linden, K, 1998</marker>
<rawString>Lu, S., Paris, C. &amp; Vander Linden, K. (1998). Towards the Automatic Construction of Task Models from Object-Oriented Diagrams. In Chatty, S., and Dewan, P., (Eds.) Engineering for Human-Computer Interaction, Kluwer Academic Publishers, 169-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Moore</author>
<author>C Paris</author>
</authors>
<title>Planning text for advisory dialogues: Capturing intentional and rhetorical information.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<pages>19--4</pages>
<contexts>
<context position="8011" citStr="Moore &amp; Paris, 1993" startWordPosition="1267" endWordPosition="1270">ure 3: Hypertext output displayed in a Netscape browser - the mailing labels task When satisfied with the task model, the technical writer exports the model to the generator for the on-line help to be generated. In Isolde, the technical writer refines the input and controls the generation of instructions through Tamot. A task model example, shown in Tamot, is presented in Figure 2. This window includes a tree-structured representation of the task hierarchy on the left, and a graphical representation of the tasks on the right. The Instruction generator uses: (1) the Moore &amp; Paris text planner (Moore &amp; Paris, 1993); (2) a sentence planner implemented as extensions to the text planner; and (3) the KPML development environment and lexico-grammatical resources (Bateman, 1997). This system is implemented as a LISP server. It plans the instructions using discourse plans that handle any task model configuration, including sequences, compositions and Boolean connectors. It plans hypertext links and can integrate canned text with generated text. Style parameters have also been implemented, giving the technical writers some control at the discourse or sentence level. For example, writers can decide to produce a </context>
</contexts>
<marker>Moore, Paris, 1993</marker>
<rawString>Moore, J. &amp; Paris, C. (1993). Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics 19(4):651-694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ozkan</author>
<author>C Paris</author>
<author>S Balbo</author>
</authors>
<title>Understanding a Task Model: An Experiment.</title>
<date>1998</date>
<booktitle>In Proc. of HCI’98,</booktitle>
<pages>123--138</pages>
<publisher>Springer</publisher>
<contexts>
<context position="5536" citStr="Ozkan et al., 1998" startWordPosition="873" endWordPosition="876">oduction (cf. Power et al., 1994). Based on these motivations, Isolde aims at automating the production of procedural on-line help. We believe this addresses an issue that is both important and relevant in documentation production. 3 System Overview Isolde provides an interactive on-line help drafting tool aimed at supporting technical writers. From an analysis of technical writers at work, we noted that they typically begin by building a representation of the application’s functionalities by executing tasks step by step, recording all the steps and the interface feedback (Power et al., 1994; Ozkan et al., 1998). This representation ranges from textual to semi-formal, e.g., flowcharts. Then, from this representation, they write the text proper. Isolde offers facilities to support and formalize the first step, i.e., building a representation of the application’s functionalities that can be reused later in the documentation maintenance process. Isolde then also automates the second step, i.e., the generation of the text. Isolde aims at being integrated into both the writers’ environment and the software design process. The representation chosen for the application’s functionalities is a task model, a s</context>
</contexts>
<marker>Ozkan, Paris, Balbo, 1998</marker>
<rawString>Ozkan, N., Paris, C. &amp; Balbo, S. (1998). Understanding a Task Model: An Experiment. In Proc. of HCI’98, H. Johnson. K. Nigay and C. Roast (Eds), Springer 123-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>Vander Linden</author>
<author>K</author>
</authors>
<title>Drafter: An Interactive Support Tool for Writing Multilingual Manuals,</title>
<date>1996</date>
<journal>IEEE Computer,</journal>
<pages>29--7</pages>
<marker>Paris, Linden, K, 1996</marker>
<rawString>Paris, C. &amp; Vander Linden, K. (1996). Drafter: An Interactive Support Tool for Writing Multilingual Manuals, IEEE Computer, 29(7):49-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>N Ozkan</author>
<author>F Bonifacio</author>
</authors>
<title>The Design of New Technology for Writing On-Line Help.</title>
<date>1998</date>
<booktitle>In Proc. HCI’98,</booktitle>
<pages>189--206</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="2714" citStr="Paris et al., 1998" startWordPosition="418" endWordPosition="421">king the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actually stopped smoking as compared to readers of generic, generated texts. Thus, the evaluation assessed the relative effectiveness of tailored and generic texts at achieving their intended goals. In our evaluation, we also sought to perform an evaluation of the effectiveness of the instructional texts produced by Isolde. Isolde1 is an authoring tool for technical writers that automatically generates parts of a system&apos;s on-line help (Paris et al., 1998b). In this domain, the writer&apos;s goal is to help the users achieve their goals. It is thus crucial to assess the effectiveness of the instructional texts in a real task. Unlike the STOP evaluation, however, we compared the effectiveness of our generated texts with that of human-authored texts. In this paper, we first introduce the type of texts that Isolde generates and give an overview of Isolde. We then present the evaluation we conducted and draw some conclusions. 2 Procedural Help Documentation for interactive devices typically includes conceptual help, business help, and procedural help (</context>
<context position="4850" citStr="Paris et al., 1998" startWordPosition="763" endWordPosition="766">s notion of brevity can also be linked to the Grice’s maxim of quantity in the discourse theory (Grice, 1979). Young (1999), in fact, uses this principle to select the content of plan descriptions and compute concise instructions. Routine and time consuming to produce: Technical writers consider procedural help as the easiest but also the most routine and tedious part of the documentation process because they must systematically perform all possible functions, recording their actions step by step. Writing procedural help can be the most timeconsuming activity of the technical writing process (Paris et al., 1998a). It is thus desirable to automate it, or at least support its production (cf. Power et al., 1994). Based on these motivations, Isolde aims at automating the production of procedural on-line help. We believe this addresses an issue that is both important and relevant in documentation production. 3 System Overview Isolde provides an interactive on-line help drafting tool aimed at supporting technical writers. From an analysis of technical writers at work, we noted that they typically begin by building a representation of the application’s functionalities by executing tasks step by step, recor</context>
</contexts>
<marker>Paris, Ozkan, Bonifacio, 1998</marker>
<rawString>Paris, C., Ozkan, N. &amp; Bonifacio, F. (1998a). The Design of New Technology for Writing On-Line Help. In Proc. HCI’98, H. Johnson. K. Nigay and C. Roast (Eds), Springer, 189-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>N Ozkan</author>
<author>F Bonifacio</author>
</authors>
<title>Novel Help for On-Line Help.</title>
<date>1998</date>
<booktitle>In ACM SIGDOC’98,</booktitle>
<pages>70--79</pages>
<location>Quebec City, Canada,</location>
<contexts>
<context position="2714" citStr="Paris et al., 1998" startWordPosition="418" endWordPosition="421">king the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actually stopped smoking as compared to readers of generic, generated texts. Thus, the evaluation assessed the relative effectiveness of tailored and generic texts at achieving their intended goals. In our evaluation, we also sought to perform an evaluation of the effectiveness of the instructional texts produced by Isolde. Isolde1 is an authoring tool for technical writers that automatically generates parts of a system&apos;s on-line help (Paris et al., 1998b). In this domain, the writer&apos;s goal is to help the users achieve their goals. It is thus crucial to assess the effectiveness of the instructional texts in a real task. Unlike the STOP evaluation, however, we compared the effectiveness of our generated texts with that of human-authored texts. In this paper, we first introduce the type of texts that Isolde generates and give an overview of Isolde. We then present the evaluation we conducted and draw some conclusions. 2 Procedural Help Documentation for interactive devices typically includes conceptual help, business help, and procedural help (</context>
<context position="4850" citStr="Paris et al., 1998" startWordPosition="763" endWordPosition="766">s notion of brevity can also be linked to the Grice’s maxim of quantity in the discourse theory (Grice, 1979). Young (1999), in fact, uses this principle to select the content of plan descriptions and compute concise instructions. Routine and time consuming to produce: Technical writers consider procedural help as the easiest but also the most routine and tedious part of the documentation process because they must systematically perform all possible functions, recording their actions step by step. Writing procedural help can be the most timeconsuming activity of the technical writing process (Paris et al., 1998a). It is thus desirable to automate it, or at least support its production (cf. Power et al., 1994). Based on these motivations, Isolde aims at automating the production of procedural on-line help. We believe this addresses an issue that is both important and relevant in documentation production. 3 System Overview Isolde provides an interactive on-line help drafting tool aimed at supporting technical writers. From an analysis of technical writers at work, we noted that they typically begin by building a representation of the application’s functionalities by executing tasks step by step, recor</context>
</contexts>
<marker>Paris, Ozkan, Bonifacio, 1998</marker>
<rawString>Paris, C., Ozkan, N. &amp; Bonifacio, F. (1998b). Novel Help for On-Line Help. In ACM SIGDOC’98, Quebec City, Canada, September, 70-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>J Tarby</author>
<author>Vander Linden</author>
<author>K</author>
</authors>
<title>A Flexible Environment for Building Task Models.</title>
<date>2001</date>
<booktitle>In Proc. of the HCI’01,</booktitle>
<location>Lille, France.</location>
<marker>Paris, Tarby, Linden, K, 2001</marker>
<rawString>Paris, C., Tarby, J. &amp; Vander Linden, K., (2001). A Flexible Environment for Building Task Models. In Proc. of the HCI’01, Lille, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
<author>L Pemberton</author>
<author>A Hartley</author>
<author>L Gorman</author>
</authors>
<title>Drafter: User requirements analysis,</title>
<date>1994</date>
<booktitle>WP2 Deliverable, Drafter Project IED4/1/5827.</booktitle>
<contexts>
<context position="4950" citStr="Power et al., 1994" startWordPosition="781" endWordPosition="784">ice, 1979). Young (1999), in fact, uses this principle to select the content of plan descriptions and compute concise instructions. Routine and time consuming to produce: Technical writers consider procedural help as the easiest but also the most routine and tedious part of the documentation process because they must systematically perform all possible functions, recording their actions step by step. Writing procedural help can be the most timeconsuming activity of the technical writing process (Paris et al., 1998a). It is thus desirable to automate it, or at least support its production (cf. Power et al., 1994). Based on these motivations, Isolde aims at automating the production of procedural on-line help. We believe this addresses an issue that is both important and relevant in documentation production. 3 System Overview Isolde provides an interactive on-line help drafting tool aimed at supporting technical writers. From an analysis of technical writers at work, we noted that they typically begin by building a representation of the application’s functionalities by executing tasks step by step, recording all the steps and the interface feedback (Power et al., 1994; Ozkan et al., 1998). This represe</context>
</contexts>
<marker>Power, Pemberton, Hartley, Gorman, 1994</marker>
<rawString>Power, R., Pemberton, L., Hartley, A. &amp; Gorman, L. (1994) Drafter: User requirements analysis, WP2 Deliverable, Drafter Project IED4/1/5827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
<author>D Scott</author>
<author>R Evans</author>
</authors>
<title>What You See is What You Meant: direct knowledge editing with natural language feedback,</title>
<date>1998</date>
<booktitle>In Proc. of ECAI’98,</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="1126" citStr="Power et al., 1998" startWordPosition="161" endWordPosition="164">The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help. The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals. 1 Introduction Instructional text is a useful and relatively constrained sub-language and has thus been a popular target for research-oriented natural language generation (NLG) systems. Much work has been done in this area, e.g., (Rösner &amp; Stede, 1992; Kosseim &amp; Lapalme, 1994; Paris &amp; Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Porter, 1997; Callaway &amp; Lester, 2001), without measuring the actual impact o</context>
</contexts>
<marker>Power, Scott, Evans, 1998</marker>
<rawString>Power, R., Scott, D. &amp; Evans, R. (1998). What You See is What You Meant: direct knowledge editing with natural language feedback, In Proc. of ECAI’98, Brighton, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Robertson</author>
<author>A S Lennox</author>
<author>L Osman</author>
</authors>
<title>Using a randomised controlled clinical trial to evaluate an NLG system.</title>
<date>2001</date>
<booktitle>In Proc. of ACL’01,</booktitle>
<pages>434--441</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="1818" citStr="Reiter et al., 2001" startWordPosition="278" endWordPosition="281">t instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Porter, 1997; Callaway &amp; Lester, 2001), without measuring the actual impact of the texts on their intended users. The evaluation of the STOP system (Reiter et al., 2001) is a notable exception to this trend. STOP produced texts tailored to individual smokers intended to convince them to stop smoking. As an evaluation, the researchers performed a largescale study of how effective the generated texts were at achieving this goal. Rather than checking the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actually stopped smoking as compared to readers of generic, generated texts. Thus, the evaluation assessed the relative effectiveness of tailored and generic text</context>
</contexts>
<marker>Reiter, Robertson, Lennox, Osman, 2001</marker>
<rawString>Reiter, E., Robertson, R., Lennox A. S. &amp; Osman, L. (2001). Using a randomised controlled clinical trial to evaluate an NLG system. In Proc. of ACL’01, Toulouse, France, 434-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rösner</author>
<author>M Stede</author>
</authors>
<title>TECHDOC: A system for the automatic production of multilingual technical documents. In:</title>
<date>1992</date>
<booktitle>G. Görz (Eds): KONVENS 92 - Proc. of the German conference on natural language processing.</booktitle>
<publisher>Springer, Berlin/Heidelberg.</publisher>
<contexts>
<context position="1051" citStr="Rösner &amp; Stede, 1992" startWordPosition="148" endWordPosition="151">technical writers that automates the production of procedural on-line help. The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help. The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals. 1 Introduction Instructional text is a useful and relatively constrained sub-language and has thus been a popular target for research-oriented natural language generation (NLG) systems. Much work has been done in this area, e.g., (Rösner &amp; Stede, 1992; Kosseim &amp; Lapalme, 1994; Paris &amp; Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester &amp; Po</context>
</contexts>
<marker>Rösner, Stede, 1992</marker>
<rawString>Rösner, D. &amp; Stede, M. (1992). TECHDOC: A system for the automatic production of multilingual technical documents. In: G. Görz (Eds): KONVENS 92 - Proc. of the German conference on natural language processing. Springer, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Tarby</author>
<author>M F Barthet</author>
</authors>
<date>1996</date>
<booktitle>The DIANE+ Method. In Proc. of CADUI’96.</booktitle>
<pages>95--119</pages>
<location>Namur,</location>
<contexts>
<context position="6233" citStr="Tarby &amp; Barthet, 1996" startWordPosition="976" endWordPosition="979"> Then, from this representation, they write the text proper. Isolde offers facilities to support and formalize the first step, i.e., building a representation of the application’s functionalities that can be reused later in the documentation maintenance process. Isolde then also automates the second step, i.e., the generation of the text. Isolde aims at being integrated into both the writers’ environment and the software design process. The representation chosen for the application’s functionalities is a task model, a semi-formal notation. It is graphically represented in the Diane+ notation (Tarby &amp; Barthet, 1996) and was tested with technical writers for usability (Ozkan et al., 1998). Figure 1 shows the Isolde architecture. The task model can be manually entered using a dedicated graphical editor Tamot. Alternatively, this model can be acquired automatically or semiautomatically from available sources of information through a variety of tools - e.g., a text-to-task extraction tool, an interaction recorder or a tool to construct automatically a draft task model from the Knowledge Acquisition Tools Task Description Text to Task UML Model UML to Task System Prototype ... TAMOT Task/Domain Model in XML I</context>
</contexts>
<marker>Tarby, Barthet, 1996</marker>
<rawString>Tarby, J.C. &amp; Barthet, M.F. (1996). The DIANE+ Method. In Proc. of CADUI’96. Namur, June 5-7, J.Vanderdonckt (ed). 95-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vander Linden</author>
<author>K Paris</author>
<author>C</author>
<author>S Lu</author>
</authors>
<title>Where Do Instructions Come From?&amp;quot; Knowledge Acquisition and Specification for Instructional Text.</title>
<date>2000</date>
<booktitle>In IMPACTS in Natural Language Generation: NLG Between Technology</booktitle>
<tech>DFKI report</tech>
<pages>00--01</pages>
<contexts>
<context position="7007" citStr="Linden et al., 2000" startWordPosition="1098" endWordPosition="1101">using a dedicated graphical editor Tamot. Alternatively, this model can be acquired automatically or semiautomatically from available sources of information through a variety of tools - e.g., a text-to-task extraction tool, an interaction recorder or a tool to construct automatically a draft task model from the Knowledge Acquisition Tools Task Description Text to Task UML Model UML to Task System Prototype ... TAMOT Task/Domain Model in XML Instruction ... Generator Analysis or Application Tools Interaction Recorder Figure 1: Isolde Architecture output of a CASE tool2 (Lu et al., 1998; Vander Linden et al., 2000). Note that the task model for the application may have already been built by HCI specialists as part of their involvement in software team - e.g., (Balbo &amp; Lindley, 1997; Diaper &amp; Stanton, in press) and can then be re-used or augmented by technical writers to ensure their suitability for on-line help generation. Figure 2: Task model design to create mailing labels with MS Word Figure 3: Hypertext output displayed in a Netscape browser - the mailing labels task When satisfied with the task model, the technical writer exports the model to the generator for the on-line help to be generated. In I</context>
</contexts>
<marker>Linden, Paris, C, Lu, 2000</marker>
<rawString>Vander Linden, K., Paris, C. &amp; Lu, S. (2000). &amp;quot;Where Do Instructions Come From?&amp;quot; Knowledge Acquisition and Specification for Instructional Text. In IMPACTS in Natural Language Generation: NLG Between Technology and Applications, Schloss Dagstuhl, Germany. Becker, T. &amp; Busemann, S. (Eds). DFKI report D-00-01, 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Young</author>
</authors>
<title>Using Grice&apos;s maxim of quantity to select the content of plan descriptions.</title>
<date>1999</date>
<journal>Artificial Intelligence</journal>
<volume>115</volume>
<pages>215--256</pages>
<contexts>
<context position="4355" citStr="Young (1999)" startWordPosition="686" endWordPosition="687">al help describes a system’s functions in terms of users’ actions on the interface. It is highly structured and heavily based on the system behavior. Its automatic production thus seems realistic. Significant within the help system: Procedural help is a key element of &amp;quot;minimalist instructions&amp;quot; (Carroll 1990), whose philosophy is based on the argument that learning software is more effective when the software documentation is short, simple and directed towards real work activities. This notion of brevity can also be linked to the Grice’s maxim of quantity in the discourse theory (Grice, 1979). Young (1999), in fact, uses this principle to select the content of plan descriptions and compute concise instructions. Routine and time consuming to produce: Technical writers consider procedural help as the easiest but also the most routine and tedious part of the documentation process because they must systematically perform all possible functions, recording their actions step by step. Writing procedural help can be the most timeconsuming activity of the technical writing process (Paris et al., 1998a). It is thus desirable to automate it, or at least support its production (cf. Power et al., 1994). Bas</context>
</contexts>
<marker>Young, 1999</marker>
<rawString>Young, R.M. (1999). Using Grice&apos;s maxim of quantity to select the content of plan descriptions. Artificial Intelligence (115), 215-256.</rawString>
</citation>
<citation valid="false">
<booktitle>1 Integrated Software and On-Line Documentation Environment.</booktitle>
<marker></marker>
<rawString>1 Integrated Software and On-Line Documentation Environment.</rawString>
</citation>
<citation valid="false">
<institution>2 Computer Aided Software Engineering Tool.</institution>
<marker></marker>
<rawString>2 Computer Aided Software Engineering Tool.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>