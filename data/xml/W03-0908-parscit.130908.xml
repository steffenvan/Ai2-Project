<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.990411">
Towards Light Semantic Processing for Question Answering
</title>
<author confidence="0.998231">
Benjamin Van Durme*, Yifen Huang*, Anna Kup´s´c*+, Eric Nyberg*
</author>
<affiliation confidence="0.929959">
*Language Technologies Institute, Carnegie Mellon University
+Polish Academy of Sciences
</affiliation>
<email confidence="0.998534">
{vandurme,hyifen,aniak,ehn}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967375">
The paper&apos; presents a lightweight knowledge-
based reasoning framework for the JAVELIN
open-domain Question Answering (QA) sys-
tem. We propose a constrained representation
of text meaning, along with a flexible unifica-
tion strategy that matches questions with re-
trieved passages based on semantic similarities
and weighted relations between words.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990682771428571">
Modern Question Answering (QA) systems aim at pro-
viding answers to natural language questions in an open-
domain context. This task is usually achieved by com-
bining information retrieval (IR) with information extrac-
tion (IE) techniques, modified to be applicable to unre-
stricted texts. Although semantics-poor techniques, such
as surface pattern matching (Soubbotin, 2002; Ravichan-
dran and Hovy, 2002) or statistical methods (Ittycheriah
et al., 2002), have been successful in answering fac-
toid questions, more complex tasks require a consider-
ation of text meaning. This requirement has motivated
work on QA systems to incorporate knowledge process-
ing components such as semantic representation, ontolo-
gies, reasoning and inference engines, e.g., (Moldovan et
al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003).
Since world knowledge databases for open-domain tasks
are unavailable, alternative approaches for meaning rep-
resentation must be adopted. In this paper, we present
our preliminary approach to semantics-based answer de-
tection in the JAVELIN QA system (Nyberg et al., 2003).
In contrast to other QA systems, we are trying to realize
a formal model for a lightweight semantics-based open-
domain question answering. We propose a constrained
semantic representation as well as an explicit unification
&apos;The authors appear in alphabetical order.
framework based on semantic similarities and weighted
relations between words. We obtain a lightweight roboust
mechanism to match questions with answer candidates.
The organization of the paper is as follows: Section 2
briefly presents system components; Section 3 discusses
syntactic processing strategies; Sections 4 and 5 describe
our preliminary semantic representation and the unifica-
tion framework which assigns confidence values to an-
swer candidates. The final section contains a summary
and future plans.
</bodyText>
<sectionHeader confidence="0.988778" genericHeader="method">
2 System Components
</sectionHeader>
<bodyText confidence="0.999887307692308">
The JAVELIN system consists of four basic components:
a question analysis module, a retrieval engine, a passage
analysis module (supporting both statistical and NLP
techniques), and an answer selection module. JAVELIN
also includes a planner module, which supports feedback
loops and finer control over specific components (Nyberg
et al., 2003). In this paper we are concerned with the two
components which support linguistic analysis: the ques-
tion analysis and passage understanding modules (Ques-
tion Analyzer and Information Extractor, respectively).
The relevant aspects of syntactic processing in both mod-
ules are presented in Section 3, whereas the semantic rep-
resentation is introduced in Section 4.
</bodyText>
<sectionHeader confidence="0.997871" genericHeader="method">
3 Parsing
</sectionHeader>
<bodyText confidence="0.998142529411765">
The system employs two different parsing techniques:
a chart parser with hand-written grammars for ques-
tion analysis, and a lexicalized, broad coverage skipping
parser for passage analysis. For question analysis, pars-
ing serves two goals: to identify the finest answer focus
(Moldovan et al., 2000; Hermjakob, 2001), and to pro-
duce a grammatical analysis (f-structure) for questions.
Due to the lack of publicly available parsers which have
suitable coverage of question forms, we have manually
developed a set of grammars to achieve these goals. On
the other hand, the limited coverage and ambiguity in
these grammars made adopting the same approach for
passage analysis inefficient. In effect, we use two dis-
tinct parsers which provide two syntactic representations,
including grammatical functions. These syntactic struc-
tures are then transformed into a common semantic rep-
resentation discussed in Section 4.
</bodyText>
<equation confidence="0.992998717391304">
( (Brill-pos VBN)
(adjunct (
(object (
(Brill-pos WRB)
(atype temporal)
(cat n)
(ortho When)
(q-focus +)
(q-token +)
(root when)
(tokens 1)))
(time +)))
(cat v)
(finite +)
(form finite)
(modified +)
(ortho founded)
(passive +)
(punctuation (
(Brill-pos &amp;quot;.&amp;quot;)
(cat punct)
(ortho ?)
(root ?)
(tokens 6)))
(qa (
(gap (
(atype temporal)
(path (*MULT* adjunct
object))))
(qtype entity)))
(root found)
(subject (
(BBN-name person)
(Brill-pos NNP)
(cat n)
(definite +)
(gen-pn +)
(human +)
(number sg)
(ortho &amp;quot;Wendy’s&amp;quot;)
(person third)
(proper-noun +)
(root wendy)
(tokens 3)))
(tense past)
(tokens 5))
</equation>
<figureCaption confidence="0.920496">
Figure 1: When was Wendy’s founded: KANTOO f-
structure
</figureCaption>
<subsectionHeader confidence="0.985133">
3.1 Questions
</subsectionHeader>
<bodyText confidence="0.977909555555555">
The question analysis consists of two steps: lexical pro-
cessing and syntactic parsing. For the lexical process-
ing step, we have integrated several external resources:
the Brill part-of-speech tagger (Brill, 1995), BBN Identi-
Finder (BBN, 2000) (to tag named entities such as proper
names, time expressions, numbers, etc.), WordNet (Fell-
baum, 1998) (for semantic categorization), and the KAN-
TOO Lexifier (Nyberg and Mitamura, 2000) (to access a
syntactic lexicon for verb valence information).
The hand-written grammars employed in the project
are based on the Lexical Functional Grammar (LFG) for-
malism (Bresnan, 1982), and are used with the KANTOO
parser (Nyberg and Mitamura, 2000). The parser out-
puts a functional structure (f-structure) which specifies
the grammatical functions of question components, e.g.,
subject, object, adjunct, etc. As illustrated in Fig. 1, the
resulting f-structure provides a deep, detailed syntactic
analysis of the question.
</bodyText>
<subsectionHeader confidence="0.994581">
3.2 Passages
</subsectionHeader>
<bodyText confidence="0.9803116875">
Passages selected by the retrieval engine are processed
by the Link Grammar parser (Grinberg et al., 1995). The
parser uses a lexicalized grammar which specifies links,
i.e., grammatical functions, and provides a constituent
structure as output. The parser covers a wide range of
syntactic constructions and is robust: it can skip over un-
recognized fragments of text, and is able to handle un-
known words.
An example of the passage analysis produced by the
Link Parser is presented in Fig. 2. Links are treated as
predicates which relate various arguments. For exam-
ple, 0 in Fig. 2 indicates that Wendy’s is an object of the
verb founded. In parallel to the Link parser, passages are
tagged with the BBN IdentiFinder (BBN, 2000), in or-
der to group together multi-word proper names such as
R. David Thomas.
</bodyText>
<sectionHeader confidence="0.966758" genericHeader="method">
4 Semantic Representation
</sectionHeader>
<bodyText confidence="0.999958714285714">
At the core of our linguistic analysis is the semantic rep-
resentation, which bridges the distinct representations of
the functional structure obtained for questions and pas-
sages. Although our semantic representation is quite sim-
ple, it aims at providing the means of understanding and
processing broad-coverage linguistic data. The represen-
tation uses the following main constructs:2
</bodyText>
<listItem confidence="0.814209857142857">
• formula is a conjunction of literals and represents
the meaning of the entire sentence (or question);
• literal is a predicate relation over two terms; in par-
ticular, we distinguish two types of literals: extrin-
sic literal, a literal which relates a label to a label,
and intrinsic literal, a literal which relates a label to
a word;
</listItem>
<footnote confidence="0.55747175">
2The use of terminology common in the field of formal logic
is aimed at providing an intuitive understanding to the reader,
but is not meant to give the impression that our work is built on
a firm logic-theoretic framework.
</footnote>
<table confidence="0.8942115">
+ Xp +
 |+ MVp + |
+ Wd + + O +  ||
 |+-G-+---G--+---S ---+ +--YS-+ +-IN+ |
            |
LEFT-WALL R. David Thomas founded.v Wendy ’s.p in 1969 .
</table>
<figure confidence="0.594876857142857">
Constituent tree:
(S (NP R. David Thomas)
(VP founded
(NP (NP Wendy ’s))
(PP in
(NP 1969)))
.)
</figure>
<figureCaption confidence="0.989849">
Figure 2: R. David Thomas founded Wendy’s in 1969.: Link Grammar parser output
</figureCaption>
<listItem confidence="0.983143142857143">
• predicate is used to capture relations between
terms;
• term is either a label, a variable which refers to a
specific entity or an event, or a word, which is either
a single word (e.g., John) or a sequence of words
separated by whitespace (e.g., for proper names such
as John Smith).
</listItem>
<bodyText confidence="0.9318265">
The BNF syntax corresponding to this representation
is given in (1).
</bodyText>
<equation confidence="0.999570166666667">
(1) &lt;formula&gt; := &lt;literal&gt;+
&lt;literal&gt; := &lt;pred&gt;(&lt;term&gt;,&lt;term&gt;)
&lt;term&gt; := &lt;label&gt;|&lt;word&gt;
&lt;word&gt; := |[a-nA-Z0-9\s]+|
&lt;label&gt; := [a-z]+[0-9]+
&lt;pred&gt; := [A-Z_-]+
</equation>
<bodyText confidence="0.999220095238095">
With the exception of the unary ANS predicate which
indicates the sought answer, all predicates are binary re-
lations (see examples in Fig. 3). Currently, most pred-
icate names are based on grammatical functions (e.g.,
SUBJECT, OBJECT, DET) which link events and entities
with their arguments. Unlike in (Moldovan et al., 2003),
names of predicates belong to a fixed vocabulary, which
provides a more sound basis for a formal interpretation.
Names of labels and terms are restricted only by the syn-
tax in (1). Examples of semantic representations for the
question When was Wendy’s founded? and the passage R.
David Thomas founded Wendy’s in 1969. are shown in
Fig. 4.
Note that our semantic representation reflects the
‘canonical’ structure of an active sentence. This design
decision was made in order to eliminate structural differ-
ences between semantically equivalent structures. Hence,
at the semantic level, all passive sentences correspond to
their equivalents in the active form. Semantic representa-
tion of questions is not always derived directly from the
f-structure. For some types of questions, e.g., definition
</bodyText>
<table confidence="0.571551923076923">
When was Wendy’s R. David Thomas founded
founded? Wendy’s in 1969.
ROOT(x6,|Wendy’s|) ROOT(x6,|Wendy’s|)
ROOT(x2,|found|) ROOT(x2,|ound|)
ADJUNCT(x2,x1) ADJUNCT(x2,x1)
OBJECT(x2,x6) OBJECT(x2,x6)
SUBJECT(x2,x7) SUBJECT(x2,x7)
ROOT(x7,|R David Thomas|)
TYPE(x2,|event|) TYPE(x2,|event|)
TENSE(x2,|past|)
ROOT(x1,x9) ROOT(x1,|1969|)
TYPE(x1,|time|) TYPE(x1,|time|)
ANS(x9)
</table>
<figureCaption confidence="0.934821">
Figure 4: An example of question and passage semantic
representation
</figureCaption>
<bodyText confidence="0.9902470625">
questions such as What is the definition of hazmat?, spe-
cialized (dedicated) grammars are used, which allows us
to more easily arrive at an appropriate representation of
meaning. Also, in the preliminary implementation of the
unification algorithm (see Section 5), we have adopted
some simplifying assumptions, and we do not incorpo-
rate sets in the current representation.
The present formalism can quite successfully handle
questions (or sentences) which refer to specific events or
relations. However, it is more difficult to represent ques-
tions like What is the relationship between Jesse Ventura
and Target Stores?, which seek a relation between enti-
ties or a common event they participated in. In the next
section, we discuss the unification scheme which allows
us to select answer candidates based on the proposed rep-
resentation.
</bodyText>
<sectionHeader confidence="0.996395" genericHeader="method">
5 Fuzzy Unification
</sectionHeader>
<bodyText confidence="0.999104333333333">
A unification algorithm is required to match question rep-
resentations with the representations of extracted pas-
sages which might contain answers. Using a precursor
</bodyText>
<table confidence="0.999057481481481">
predicate example comments
ROOT ROOT(x13,|John|) the root form of entity/event x13
OBJECT OBJECT(x2,x3) x3 is the object of verb
or preposition x2
SUBJECT SUBJECT(x2,x3) x3 is the subject of verb x2
DET DET(x2,x1) x1 is a determiner/quantifier of x2
TYPE TYPE(x3,|event|) x3 is of the type event
TENSE TENSE(x1,|present|) x1 is a verb in present tense
EQUIV EQUIV(x1,x3) semantic equivalence:
apposition: ”John, a student of CMU”
equality operator in copular sentences:
”John is a student of CMU”
ATTRIBUTE ATTRIBUTE(x1,x3) x3 is an adjective modifier of x1:
adjective-noun: ”stupid John”
copular constructions: ”John is stupid”
PREDICATE PREDICATE(x2,x3) copular constructions: ”Y is x3”
ROOT(x2,|be|) SUBJECT(x2,Y)
PREDICATE(x2,x3)
POSSESSOR POSSESSOR(x2,x4) x4 is the possessor of x2
”x4’s x2” or ”x2 of x4”
AND AND(x3,x1) ”John and Mary laughed.”
AND(x3,x2) ROOT(x1,|John|) ROOT(x2,|Mary|)
ROOT(x4,|laugh|) TYPE(x4,|event|)
AND(x3,x1)
AND(x3,x2)
SUBJECT(x4,x3)
ANS ANS(x1) only for questions: x1 indicates the answer
</table>
<figureCaption confidence="0.998036">
Figure 3: Examples of predicates
</figureCaption>
<bodyText confidence="0.999964086956522">
to the representation presented above, we constructed
an initial prototype using a traditional theorem prover
(Kalman, 2001). Answer extraction was performed by at-
tempting a unification between logical forms of the ques-
tion and retrieved passages. Early tests showed that a uni-
fication strategy based on a strict boolean logic was not
as flexible as we desired, given the lack of traditional do-
main constraints that one normally possesses when con-
sidering this type of approach. Unless a retrieved pas-
sage exactly matched the question, as in Fig. 4, the sys-
tem would fail due to lack of information. For instance,
knowing that Benjamin killed Jefferson. would not an-
swer the question Who murdered Jefferson?, using a strict
unification strategy.
This has led to more recent experimentation with prob-
abilistic models that perform what we informally refer to
as fuzzy unification.3 The basic idea of our unification
strategy is to treat relationships between question terms
as a set of weighted constraints. The confidence score
assigned to each extracted answer candidate is related to
the number of constraints the retrieved passage satisfies,
along with a measure of similarity between the relevant
terms.
</bodyText>
<subsectionHeader confidence="0.951848">
5.1 Definitions
</subsectionHeader>
<bodyText confidence="0.9995013">
In this section, we present definitions which are necessary
for discussion of the similarity measure employed by our
fuzzy unification framework.
Given a user query Q, where Q is a formula, we re-
trieve a set of passages P. Our task to is find the best
passage Pbest E P from which an answer candidate can
be extracted. An answer candidate exists within a pas-
sage P if the result of a fuzzy unification between Q and
P results in the single term of ANS(x0) being ground in a
term from P.
</bodyText>
<listItem confidence="0.797065">
(2) Pbest = argmaxP∈ Psim(Q, P)
</listItem>
<bodyText confidence="0.999401916666667">
The restriction that an answer candidate must be found
within a passage P must be made explicit, as our no-
tion of fuzzy unification is such that a passage can unify
against a query with a non-zero level of confidence even
if one or more constraints from the query are left unsat-
isfied. Since the final goal is to find and return the best
possible answer, we are not concerned with those pas-
sages which seem highly related yet do not offer answer
candidates.
In Section 4, we introduced extrinsic literals where
predicates serve as relations over two labels. Extrinsic lit-
erals can be thought of as relations defined over distinct
</bodyText>
<footnote confidence="0.953305833333333">
3Fuzzy unification in a formal setting generally refers to a
unification framework that is employed in the realm of fuzzy
logics. Our current representation is of an ad-hoc nature, but
our usage of this term does foreshadow future progression to-
wards a representation scheme dependent on such a formal,
non-boolean model.
</footnote>
<bodyText confidence="0.999967222222222">
entities in our formula. For example, SUBJECT(x1, x2)
is an extrinsic literal, while ROOT(x1, |Benjamin|) is not.
The latter has been defined as an intrinsic literal in Sec-
tion 4 and it relates a label and a word.
This terminology is motivated by the intuitive distinc-
tion between intrinsic and extrinsic properties of an entity
in the world. We use this distinction as a simplifying as-
sumption in our measurements of similarity, which we
will now explain in more detail.
</bodyText>
<subsectionHeader confidence="0.998808">
5.2 Similarity Measure
</subsectionHeader>
<bodyText confidence="0.998630375">
Given a set of extrinsic literals PE and QE from a pas-
sage and the question, respectively, we measure the sim-
ilarity between QE and a given ordering of PE as the
geometric mean of the similarity between each pair of
extrinsic literals from the sets QE and PE.
Let O be the set of all possible orderings of PE, O
an element of O, QEj literal j of QE, and Oj literal j of
ordering O. Then:
</bodyText>
<equation confidence="0.890324666666667">
(3) sim(Q, P)= sim(QE, PE)
&amp;quot;
= maxO∈ O(�n j=0 sim(QE j , Oj)) 1
</equation>
<bodyText confidence="0.996498782608696">
The similarity of two extrinsic literals, lE and lE0, is
computed by the square root of the similarity scores of
each pair of labels, multiplied by the weight of the given
literal, dependent on the equivilance of the predicates
p,p/ of the respective literals lE, lE0. If the predicates are
not equivilant, we rely on the engineers tactic of assign-
ing an epsilon value of similarity, where a is lower than
any possible similarity score4. Note that the similarity
score is still dependent on the weight of the literal, mean-
ing that failing to satisfy a heavier constraint imposes a
greater penalty than if we fail to satisfy a constraint of
lesser importance.
Let tj and t�j be the respective j-th term of lE, lE0.
Then:
E,otherwise
The weight of a literal is meant to capture the relative
importance of a particular constraint in a query. In stan-
dard boolean unification the importance of a literal is uni-
form, as any local failure dooms the entire attempt.5 In a
non-boolean framework the importance of one literal vs.
another becomes an issue. As an example, given a ques-
tion concerning a murder we might be more interested in
the suspect’s name than in the fact that he was tall. This
</bodyText>
<footnote confidence="0.986364">
4The use of a constant value of E is ad hoc, and we are in-
vestigating more principled methods for assigning this penalty.
5That is to say, classic unification is usually an all or nothing
affair.
</footnote>
<equation confidence="0.998010333333333">
(4) sim(lE,lE0) = weight(lE)∗
(sim(t0,t00)∗sim(t1,t01)) 2 ,p=p0
t
</equation>
<bodyText confidence="0.999946944444445">
idea is similar to that commonly seen in information re-
trieval systems which place higher relative importance on
terms in a query that are judged a priori to posses higher
information value. While our prototype currently sets all
literals with a weight of 1.0, we are investigating methods
to train these weights to be specific to question type.
Per our definition, all terms within an extrinsic literal
will be labels. Thus, in equation (10), t0 is a label, as is
t1, and so on. Given a pair of labels, b and b0, we let I, I0
be the respective sets of intrinsic literals from the formula
containing b, b0 such that for all intrinsic literals lI E I,
the first term of lI is b, and likewise for b0, I0.
Much like similarity between two formulae, the sim-
ilarity between two labels relies on finding the maximal
score over all possible orderings of a set of literals.
Now let O be the set of all possible orderings of I0, O
an element of O, Ij the j-th literal of I, and Oj the j-th
literal of O. Then:
</bodyText>
<equation confidence="0.741248">
n
(5) sim(b,b0) = maxO∈ O(�n j=0 sim(Ij,Oj)) 1
</equation>
<bodyText confidence="0.9285586">
We measure the similarity between a pair of intrinsic
literals as the similarity between the two words multi-
plied by the weight of the first literal, dependent on the
predicates p, p0 of the respective literals being equivilant.
sim(lI, lI&apos;) = weight(lI) * {Sim(t1,t&apos;1),p=p&apos;
</bodyText>
<sectionHeader confidence="0.434589" genericHeader="method">
l E,otherwise
</sectionHeader>
<bodyText confidence="0.9999024">
The similarity between two words is currently measured
using a WordNet distance metric, applying weights intro-
duced in (Moldovan et al., 2003). We will soon be inte-
grating metrics which rely on other dimensions of simi-
larity.
</bodyText>
<subsectionHeader confidence="0.993166">
5.3 Example
</subsectionHeader>
<bodyText confidence="0.9997871">
We now walk through a simple example in order to
present the current framework used to measure the level
of constraint satisfaction (confidence score) achieved by
a given passage. While a complete traversal of even a
small passage would exceed the space available here, we
will present a single instance of each type of usage of the
sim() function.
If we limit our focus to only a few key relationships,
we get the following analysis of a given question and pas-
sage.
</bodyText>
<equation confidence="0.796617">
(7) Who killed Jefferson?
ANS(x0), ROOT(x1,x0), ROOT(x2,|kill|),
ROOT(x3,|Jefferson|), TYPE(x2,|event|),
TYPE(x1,|person|), TYPE(x3,|person|), SUB-
JECT(x2,x1), OBJECT(x2,x3)
(8) Benjamin murdered Jefferson.
ROOT(y1,|Benjamin|), ROOT(y2,|murder|),
ROOT(y3,|Jefferson|), TYPE(y2,|event|),
TYPE(y1,|person|), TYPE(y3,|person|), SUB-
JECT(y2,y1), OBJECT(y2,y3)
</equation>
<bodyText confidence="0.996497333333333">
Computing the similarity between two formulae,
(loosely referred to here by their original text), gives the
following:
</bodyText>
<equation confidence="0.8014305">
(9) sim[|Who killed Jefferson?|,
|Benjamin murdered Jefferson.|] =
(sim[ SUBJECT(x2,x1), SUBJECT(y2,y1)]*
sim[ OBJECT(x2,x3), OBJECT(y2,y3)])12
</equation>
<bodyText confidence="0.995388">
The similarity between the given extrinsic literals shar-
ing the predicate SUBJECT:
</bodyText>
<equation confidence="0.988046">
(10) sim[SUBJECT(x2,x1), SUBJECT(y2,y1)] =
(sim[x2, y2] * sim[x1, y1])12 *
weight[SUBJECT(x2,x1)]
</equation>
<bodyText confidence="0.99979375">
In order to find the result of this extrinsic similarity
evaluation, we need to determine the similarity between
the paired terms, (x1,y1) and (x2,y2). The similarity be-
tween x1 and y1 is measured as:
</bodyText>
<equation confidence="0.535610666666667">
(11) sim[x2, y2] =
(sim[ROOT(x2,|kill|), ROOT(y2,|murder|)]*
sim[TYPE(x2,|event|), TYPE(y2,|event|)])12
</equation>
<bodyText confidence="0.96462925">
The result of this function depends on the combined
similarity of the intrinsic literals that relate the given
terms to values. The similarity between one of these in-
trinsic literal pairs is measured by:
(12) sim[ROOT(x2,|kill|), ROOT(y2,|murder|)] =
sim[|kill|, |murder|]*weight[ROOT(x2,|kill|)]
Finally, the similarity between a pair of words is com-
puted as:
</bodyText>
<equation confidence="0.668775">
(13) sim[|kill|, |murder|] = 0.8
</equation>
<bodyText confidence="0.999922">
As stated earlier, our similarity metrics at the word
level are currently based on recent work on WordNet dis-
tance functions. We are actively developing methods to
complement this approach.
</bodyText>
<sectionHeader confidence="0.985727" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999981473684211">
The paper presents a lightweight semantic processing
technique for open-domain question answering. We pro-
pose a uniform semantic representation for questions and
passages, derived from their functional structure. We also
describe the unification framework which allows for flex-
ible matching of query terms with retrieved passages.
One characteristics of the current representation is that
it is built from grammatical functions and does not uti-
lize a canonical set of semantic roles and concepts. Our
overall approach in JAVELIN was to start with the sim-
plest form of meaning-based matching that could extend
simple keyword-based approaches. Since it was possi-
ble to extract grammatical functions from unrestricted
text fairly quickly (using KANTOO for questions and the
Link Grammar parser for answer passages), this frame-
work provides a reasonable first step. We intend to extend
our representation and unification algorithm by incorpo-
rating the Lexical Conceptual Structure Database (Dorr,
2001), which will allow us to use semantic roles instead
of grammatical relations as predicates in the represen-
tation. We also plan to enrich the representation with
temporal expressions, incorporating the ideas presented
in (Han, 2003).
Another limitation of the current implementation is
the limited scope of the similarity function. At present,
the similarity function is based on relationships found
in WordNet, and only relates words which belong to the
same syntactic category. We plan to extend our similar-
ity measure by using name lists, gazetteers and statistical
cooccurrence in text corpora. A complete approach to
word similarity will also require a suitable algorithm for
reference resolution. Unrestricted text makes heavy use
of various forms of co-reference, such as anaphora, def-
inite description, etc. We intend to adapt the anaphora
resolution algorithms used in KANTOO for this purpose,
but a general solution to resolving definite reference (e.g.,
the use of “the organization” to refer to “Microsoft”) is a
topic for ongoing research.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999839777777778">
Research presented in this paper has been supported in
part by an ARDA grant under Phase I of the AQUAINT
program. The authors wish to thank all members of the
JAVELIN project for their support in preparing the work
presented in this paper. We are also grateful to two anony-
mous reviewers, Laurie Hiyakumoto and Kathryn Baker
for their comments and suggestions for improving this
paper. Needless to say, all remaining errors and omis-
sions are entirely our responsibility.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994686348484848">
BBN Technologies, 2000. IdentiFinder User Manual.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press Series on Cog-
nitive Theory and Mental Representation. The MIT
Press, Cambridge, MA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543–565.
Jenifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In TREC 2002 Proceedings.
Bonnie J. Dorr. 2001. LCS Database Docu-
mentation. HTML Manual. available from
http://www.umiacs.umd.edu/˜bonnie/LCS Data-
base Documentation.html.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
In Proceedings of the Fourth International Workshop
on Parsing Technologies, Prague, September.
Benjamin Han. 2003. Text temporal analysis. Research
status summary. Draft of January 2003.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
Workshop on Open-Domain Question Answering at
ACL-2001.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2002.
The use of external knowledge in factoid qa. In Pro-
ceedings of the TREC-10 Conference.
Abraham Ittycheriah and Salim Roukos. 2003. IBM’s
statistical question answering system – TREC-11. In
TREC 2002 Proceedings.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2002. IBM’s statistical question answering system –
TREC-10. In TREC 2001 Proceedings.
John A. Kalman. 2001. Automated Reasoning with OT-
TER. Rinton Press.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL-2000).
Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul
Morarescu, Finley Lacatusu, Adrian Novischi, Adriana
Badulescu, and Orest Bolohan. 2003. LCC tools for
question answering. In TREC 2002 Proceedings.
Eric Nyberg and Teruko Mitamura. 2000. The KAN-
TOO machine translation environment. In Proceed-
ings ofAMTA 2000.
Eric Nyberg, Teruko Mitamura, Jaime Carbonell, Jaime
Callan, Kevyn Collins-Thompson, Krzysztof Czuba,
Michael Duggan, Laurie Hiyakumoto, Ng Hu, Yifen
Huang, Jeongwoo Ko, Lucian V. Lita, Stephen
Murtagh, Vasco Pedro, and David Svoboda. 2003. The
JAVELIN question answering system at TREC 2002.
In TREC 2002 Proceedings.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering system.
In Proceedings of the ACL Conference.
Martin M. Soubbotin. 2002. Patterns of potential answer
expressions as clues to the right answer. In TREC 2001
Proceedings.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657074">
<title confidence="0.999653">Towards Light Semantic Processing for Question Answering</title>
<author confidence="0.999575">Van_Yifen Anna Eric</author>
<affiliation confidence="0.777404">Technologies Institute, Carnegie Mellon Academy of</affiliation>
<abstract confidence="0.999275222222222">a lightweight knowledgebased reasoning framework for the JAVELIN open-domain Question Answering (QA) system. We propose a constrained representation of text meaning, along with a flexible unification strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BBN Technologies</author>
</authors>
<date>2000</date>
<note>IdentiFinder User Manual.</note>
<marker>Technologies, 2000</marker>
<rawString>BBN Technologies, 2000. IdentiFinder User Manual.</rawString>
</citation>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<booktitle>Series on Cognitive Theory and Mental Representation. The</booktitle>
<editor>Joan Bresnan, editor.</editor>
<publisher>MIT Press</publisher>
<location>Cambridge, MA.</location>
<marker>1982</marker>
<rawString>Joan Bresnan, editor. 1982. The Mental Representation of Grammatical Relations. MIT Press Series on Cognitive Theory and Mental Representation. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--543</pages>
<contexts>
<context position="5010" citStr="Brill, 1995" startWordPosition="733" endWordPosition="734"> punct) (ortho ?) (root ?) (tokens 6))) (qa ( (gap ( (atype temporal) (path (*MULT* adjunct object)))) (qtype entity))) (root found) (subject ( (BBN-name person) (Brill-pos NNP) (cat n) (definite +) (gen-pn +) (human +) (number sg) (ortho &amp;quot;Wendy’s&amp;quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21:543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenifer Chu-Carroll</author>
<author>John Prager</author>
<author>Christopher Welty</author>
<author>Krzysztof Czuba</author>
<author>David Ferrucci</author>
</authors>
<title>A multistrategy and multi-source approach to question answering.</title>
<date>2003</date>
<booktitle>In TREC 2002 Proceedings.</booktitle>
<contexts>
<context position="1435" citStr="Chu-Carroll et al., 2003" startWordPosition="196" endWordPosition="199">th information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in the JAVELIN QA system (Nyberg et al., 2003). In contrast to other QA systems, we are trying to realize a formal model for a lightweight semantics-based opendomain question answering. We propose a constrained semantic representation as well as an explicit unification &apos;The authors appear in alphabetical order. framework based on semantic similarities and weighted relati</context>
</contexts>
<marker>Chu-Carroll, Prager, Welty, Czuba, Ferrucci, 2003</marker>
<rawString>Jenifer Chu-Carroll, John Prager, Christopher Welty, Krzysztof Czuba, and David Ferrucci. 2003. A multistrategy and multi-source approach to question answering. In TREC 2002 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>LCS Database Documentation.</title>
<date>2001</date>
<note>HTML Manual. available from http://www.umiacs.umd.edu/˜bonnie/LCS Database Documentation.html.</note>
<contexts>
<context position="21866" citStr="Dorr, 2001" startWordPosition="3411" endWordPosition="3412">t it is built from grammatical functions and does not utilize a canonical set of semantic roles and concepts. Our overall approach in JAVELIN was to start with the simplest form of meaning-based matching that could extend simple keyword-based approaches. Since it was possible to extract grammatical functions from unrestricted text fairly quickly (using KANTOO for questions and the Link Grammar parser for answer passages), this framework provides a reasonable first step. We intend to extend our representation and unification algorithm by incorporating the Lexical Conceptual Structure Database (Dorr, 2001), which will allow us to use semantic roles instead of grammatical relations as predicates in the representation. We also plan to enrich the representation with temporal expressions, incorporating the ideas presented in (Han, 2003). Another limitation of the current implementation is the limited scope of the similarity function. At present, the similarity function is based on relationships found in WordNet, and only relates words which belong to the same syntactic category. We plan to extend our similarity measure by using name lists, gazetteers and statistical cooccurrence in text corpora. A </context>
</contexts>
<marker>Dorr, 2001</marker>
<rawString>Bonnie J. Dorr. 2001. LCS Database Documentation. HTML Manual. available from http://www.umiacs.umd.edu/˜bonnie/LCS Database Documentation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5144" citStr="Fellbaum, 1998" startWordPosition="753" endWordPosition="755">) (subject ( (BBN-name person) (Brill-pos NNP) (cat n) (definite +) (gen-pn +) (human +) (number sg) (ortho &amp;quot;Wendy’s&amp;quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Grinberg</author>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
</authors>
<title>A robust parsing algorithm for link grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourth International Workshop on Parsing Technologies,</booktitle>
<location>Prague,</location>
<contexts>
<context position="5874" citStr="Grinberg et al., 1995" startWordPosition="860" endWordPosition="863">exicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the question. 3.2 Passages Passages selected by the retrieval engine are processed by the Link Grammar parser (Grinberg et al., 1995). The parser uses a lexicalized grammar which specifies links, i.e., grammatical functions, and provides a constituent structure as output. The parser covers a wide range of syntactic constructions and is robust: it can skip over unrecognized fragments of text, and is able to handle unknown words. An example of the passage analysis produced by the Link Parser is presented in Fig. 2. Links are treated as predicates which relate various arguments. For example, 0 in Fig. 2 indicates that Wendy’s is an object of the verb founded. In parallel to the Link parser, passages are tagged with the BBN Ide</context>
</contexts>
<marker>Grinberg, Lafferty, Sleator, 1995</marker>
<rawString>Dennis Grinberg, John Lafferty, and Daniel Sleator. 1995. A robust parsing algorithm for link grammars. In Proceedings of the Fourth International Workshop on Parsing Technologies, Prague, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Han</author>
</authors>
<title>Text temporal analysis. Research status summary.</title>
<date>2003</date>
<journal>Draft of</journal>
<contexts>
<context position="22097" citStr="Han, 2003" startWordPosition="3446" endWordPosition="3447">word-based approaches. Since it was possible to extract grammatical functions from unrestricted text fairly quickly (using KANTOO for questions and the Link Grammar parser for answer passages), this framework provides a reasonable first step. We intend to extend our representation and unification algorithm by incorporating the Lexical Conceptual Structure Database (Dorr, 2001), which will allow us to use semantic roles instead of grammatical relations as predicates in the representation. We also plan to enrich the representation with temporal expressions, incorporating the ideas presented in (Han, 2003). Another limitation of the current implementation is the limited scope of the similarity function. At present, the similarity function is based on relationships found in WordNet, and only relates words which belong to the same syntactic category. We plan to extend our similarity measure by using name lists, gazetteers and statistical cooccurrence in text corpora. A complete approach to word similarity will also require a suitable algorithm for reference resolution. Unrestricted text makes heavy use of various forms of co-reference, such as anaphora, definite description, etc. We intend to ada</context>
</contexts>
<marker>Han, 2003</marker>
<rawString>Benjamin Han. 2003. Text temporal analysis. Research status summary. Draft of January 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
</authors>
<title>Parsing and question classification for question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Open-Domain Question Answering at ACL-2001.</booktitle>
<contexts>
<context position="3539" citStr="Hermjakob, 2001" startWordPosition="507" endWordPosition="508">linguistic analysis: the question analysis and passage understanding modules (Question Analyzer and Information Extractor, respectively). The relevant aspects of syntactic processing in both modules are presented in Section 3, whereas the semantic representation is introduced in Section 4. 3 Parsing The system employs two different parsing techniques: a chart parser with hand-written grammars for question analysis, and a lexicalized, broad coverage skipping parser for passage analysis. For question analysis, parsing serves two goals: to identify the finest answer focus (Moldovan et al., 2000; Hermjakob, 2001), and to produce a grammatical analysis (f-structure) for questions. Due to the lack of publicly available parsers which have suitable coverage of question forms, we have manually developed a set of grammars to achieve these goals. On the other hand, the limited coverage and ambiguity in these grammars made adopting the same approach for passage analysis inefficient. In effect, we use two distinct parsers which provide two syntactic representations, including grammatical functions. These syntactic structures are then transformed into a common semantic representation discussed in Section 4. ( (</context>
</contexts>
<marker>Hermjakob, 2001</marker>
<rawString>Ulf Hermjakob. 2001. Parsing and question classification for question answering. In Proceedings of the Workshop on Open-Domain Question Answering at ACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Ulf Hermjakob</author>
<author>Chin-Yew Lin</author>
</authors>
<title>The use of external knowledge in factoid qa.</title>
<date>2002</date>
<booktitle>In Proceedings of the TREC-10 Conference.</booktitle>
<contexts>
<context position="1407" citStr="Hovy et al., 2002" startWordPosition="192" endWordPosition="195">ion retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in the JAVELIN QA system (Nyberg et al., 2003). In contrast to other QA systems, we are trying to realize a formal model for a lightweight semantics-based opendomain question answering. We propose a constrained semantic representation as well as an explicit unification &apos;The authors appear in alphabetical order. framework based on semantic simi</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2002</marker>
<rawString>Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2002. The use of external knowledge in factoid qa. In Proceedings of the TREC-10 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>IBM’s statistical question answering system – TREC-11.</title>
<date>2003</date>
<booktitle>In TREC 2002 Proceedings.</booktitle>
<marker>Ittycheriah, Roukos, 2003</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2003. IBM’s statistical question answering system – TREC-11. In TREC 2002 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Martin Franz</author>
<author>Salim Roukos</author>
</authors>
<title>IBM’s statistical question answering system – TREC-10. In TREC</title>
<date>2002</date>
<note>Proceedings.</note>
<contexts>
<context position="1067" citStr="Ittycheriah et al., 2002" startWordPosition="141" endWordPosition="144">, along with a flexible unification strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words. 1 Introduction Modern Question Answering (QA) systems aim at providing answers to natural language questions in an opendomain context. This task is usually achieved by combining information retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in th</context>
</contexts>
<marker>Ittycheriah, Franz, Roukos, 2002</marker>
<rawString>Abraham Ittycheriah, Martin Franz, and Salim Roukos. 2002. IBM’s statistical question answering system – TREC-10. In TREC 2001 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Kalman</author>
</authors>
<title>Automated Reasoning with OTTER.</title>
<date>2001</date>
<publisher>Rinton Press.</publisher>
<contexts>
<context position="12146" citStr="Kalman, 2001" startWordPosition="1819" endWordPosition="1820">ive-noun: ”stupid John” copular constructions: ”John is stupid” PREDICATE PREDICATE(x2,x3) copular constructions: ”Y is x3” ROOT(x2,|be|) SUBJECT(x2,Y) PREDICATE(x2,x3) POSSESSOR POSSESSOR(x2,x4) x4 is the possessor of x2 ”x4’s x2” or ”x2 of x4” AND AND(x3,x1) ”John and Mary laughed.” AND(x3,x2) ROOT(x1,|John|) ROOT(x2,|Mary|) ROOT(x4,|laugh|) TYPE(x4,|event|) AND(x3,x1) AND(x3,x2) SUBJECT(x4,x3) ANS ANS(x1) only for questions: x1 indicates the answer Figure 3: Examples of predicates to the representation presented above, we constructed an initial prototype using a traditional theorem prover (Kalman, 2001). Answer extraction was performed by attempting a unification between logical forms of the question and retrieved passages. Early tests showed that a unification strategy based on a strict boolean logic was not as flexible as we desired, given the lack of traditional domain constraints that one normally possesses when considering this type of approach. Unless a retrieved passage exactly matched the question, as in Fig. 4, the system would fail due to lack of information. For instance, knowing that Benjamin killed Jefferson. would not answer the question Who murdered Jefferson?, using a strict </context>
</contexts>
<marker>Kalman, 2001</marker>
<rawString>John A. Kalman. 2001. Automated Reasoning with OTTER. Rinton Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Roxana Girju</author>
<author>Richard Goodrum</author>
<author>Vasile Rus</author>
</authors>
<title>The structure and performance of an opendomain question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL-2000).</booktitle>
<contexts>
<context position="3521" citStr="Moldovan et al., 2000" startWordPosition="503" endWordPosition="506">mponents which support linguistic analysis: the question analysis and passage understanding modules (Question Analyzer and Information Extractor, respectively). The relevant aspects of syntactic processing in both modules are presented in Section 3, whereas the semantic representation is introduced in Section 4. 3 Parsing The system employs two different parsing techniques: a chart parser with hand-written grammars for question analysis, and a lexicalized, broad coverage skipping parser for passage analysis. For question analysis, parsing serves two goals: to identify the finest answer focus (Moldovan et al., 2000; Hermjakob, 2001), and to produce a grammatical analysis (f-structure) for questions. Due to the lack of publicly available parsers which have suitable coverage of question forms, we have manually developed a set of grammars to achieve these goals. On the other hand, the limited coverage and ambiguity in these grammars made adopting the same approach for passage analysis inefficient. In effect, we use two distinct parsers which provide two syntactic representations, including grammatical functions. These syntactic structures are then transformed into a common semantic representation discussed</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum, and Vasile Rus. 2000. The structure and performance of an opendomain question answering system. In Proceedings of the Conference of the Association for Computational Linguistics (ACL-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Roxana Girju</author>
<author>Paul Morarescu</author>
<author>Finley Lacatusu</author>
<author>Adrian Novischi</author>
<author>Adriana Badulescu</author>
<author>Orest Bolohan</author>
</authors>
<title>LCC tools for question answering.</title>
<date>2003</date>
<booktitle>In TREC 2002 Proceedings.</booktitle>
<contexts>
<context position="1386" citStr="Moldovan et al., 2003" startWordPosition="188" endWordPosition="191">ved by combining information retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in the JAVELIN QA system (Nyberg et al., 2003). In contrast to other QA systems, we are trying to realize a formal model for a lightweight semantics-based opendomain question answering. We propose a constrained semantic representation as well as an explicit unification &apos;The authors appear in alphabetical order. framework b</context>
<context position="8712" citStr="Moldovan et al., 2003" startWordPosition="1332" endWordPosition="1335">(e.g., for proper names such as John Smith). The BNF syntax corresponding to this representation is given in (1). (1) &lt;formula&gt; := &lt;literal&gt;+ &lt;literal&gt; := &lt;pred&gt;(&lt;term&gt;,&lt;term&gt;) &lt;term&gt; := &lt;label&gt;|&lt;word&gt; &lt;word&gt; := |[a-nA-Z0-9\s]+| &lt;label&gt; := [a-z]+[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]+ &lt;pred&gt; := [A-Z_-]+ With the exception of the unary ANS predicate which indicates the sought answer, all predicates are binary relations (see examples in Fig. 3). Currently, most predicate names are based on grammatical functions (e.g., SUBJECT, OBJECT, DET) which link events and entities with their arguments. Unlike in (Moldovan et al., 2003), names of predicates belong to a fixed vocabulary, which provides a more sound basis for a formal interpretation. Names of labels and terms are restricted only by the syntax in (1). Examples of semantic representations for the question When was Wendy’s founded? and the passage R. David Thomas founded Wendy’s in 1969. are shown in Fig. 4. Note that our semantic representation reflects the ‘canonical’ structure of an active sentence. This design decision was made in order to eliminate structural differences between semantically equivalent structures. Hence, at the semantic level, all passive se</context>
<context position="18592" citStr="Moldovan et al., 2003" startWordPosition="2947" endWordPosition="2950">iterals. Now let O be the set of all possible orderings of I0, O an element of O, Ij the j-th literal of I, and Oj the j-th literal of O. Then: n (5) sim(b,b0) = maxO∈ O(�n j=0 sim(Ij,Oj)) 1 We measure the similarity between a pair of intrinsic literals as the similarity between the two words multiplied by the weight of the first literal, dependent on the predicates p, p0 of the respective literals being equivilant. sim(lI, lI&apos;) = weight(lI) * {Sim(t1,t&apos;1),p=p&apos; l E,otherwise The similarity between two words is currently measured using a WordNet distance metric, applying weights introduced in (Moldovan et al., 2003). We will soon be integrating metrics which rely on other dimensions of similarity. 5.3 Example We now walk through a simple example in order to present the current framework used to measure the level of constraint satisfaction (confidence score) achieved by a given passage. While a complete traversal of even a small passage would exceed the space available here, we will present a single instance of each type of usage of the sim() function. If we limit our focus to only a few key relationships, we get the following analysis of a given question and passage. (7) Who killed Jefferson? ANS(x0), RO</context>
</contexts>
<marker>Moldovan, Harabagiu, Girju, Morarescu, Lacatusu, Novischi, Badulescu, Bolohan, 2003</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul Morarescu, Finley Lacatusu, Adrian Novischi, Adriana Badulescu, and Orest Bolohan. 2003. LCC tools for question answering. In TREC 2002 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nyberg</author>
<author>Teruko Mitamura</author>
</authors>
<title>The KANTOO machine translation environment.</title>
<date>2000</date>
<booktitle>In Proceedings ofAMTA</booktitle>
<contexts>
<context position="5227" citStr="Nyberg and Mitamura, 2000" startWordPosition="764" endWordPosition="767">-pn +) (human +) (number sg) (ortho &amp;quot;Wendy’s&amp;quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the question. 3.2 Passages Passages selected by the retrieval engine are processed by </context>
</contexts>
<marker>Nyberg, Mitamura, 2000</marker>
<rawString>Eric Nyberg and Teruko Mitamura. 2000. The KANTOO machine translation environment. In Proceedings ofAMTA 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nyberg</author>
<author>Teruko Mitamura</author>
<author>Jaime Carbonell</author>
<author>Jaime Callan</author>
<author>Kevyn Collins-Thompson</author>
<author>Krzysztof Czuba</author>
<author>Michael Duggan</author>
<author>Laurie Hiyakumoto</author>
<author>Ng Hu</author>
<author>Yifen Huang</author>
<author>Jeongwoo Ko</author>
<author>Lucian V Lita</author>
<author>Stephen Murtagh</author>
<author>Vasco Pedro</author>
<author>David Svoboda</author>
</authors>
<title>The JAVELIN question answering system at TREC</title>
<date>2003</date>
<booktitle>In TREC 2002 Proceedings.</booktitle>
<contexts>
<context position="1708" citStr="Nyberg et al., 2003" startWordPosition="236" endWordPosition="239">l in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in the JAVELIN QA system (Nyberg et al., 2003). In contrast to other QA systems, we are trying to realize a formal model for a lightweight semantics-based opendomain question answering. We propose a constrained semantic representation as well as an explicit unification &apos;The authors appear in alphabetical order. framework based on semantic similarities and weighted relations between words. We obtain a lightweight roboust mechanism to match questions with answer candidates. The organization of the paper is as follows: Section 2 briefly presents system components; Section 3 discusses syntactic processing strategies; Sections 4 and 5 describe</context>
</contexts>
<marker>Nyberg, Mitamura, Carbonell, Callan, Collins-Thompson, Czuba, Duggan, Hiyakumoto, Hu, Huang, Ko, Lita, Murtagh, Pedro, Svoboda, 2003</marker>
<rawString>Eric Nyberg, Teruko Mitamura, Jaime Carbonell, Jaime Callan, Kevyn Collins-Thompson, Krzysztof Czuba, Michael Duggan, Laurie Hiyakumoto, Ng Hu, Yifen Huang, Jeongwoo Ko, Lucian V. Lita, Stephen Murtagh, Vasco Pedro, and David Svoboda. 2003. The JAVELIN question answering system at TREC 2002. In TREC 2002 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a Question Answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Conference.</booktitle>
<contexts>
<context position="1017" citStr="Ravichandran and Hovy, 2002" startWordPosition="133" endWordPosition="137"> propose a constrained representation of text meaning, along with a flexible unification strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words. 1 Introduction Modern Question Answering (QA) systems aim at providing answers to natural language questions in an opendomain context. This task is usually achieved by combining information retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a Question Answering system. In Proceedings of the ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin M Soubbotin</author>
</authors>
<title>Patterns of potential answer expressions as clues to the right answer.</title>
<date>2002</date>
<booktitle>In TREC 2001 Proceedings.</booktitle>
<contexts>
<context position="987" citStr="Soubbotin, 2002" startWordPosition="131" endWordPosition="132">g (QA) system. We propose a constrained representation of text meaning, along with a flexible unification strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words. 1 Introduction Modern Question Answering (QA) systems aim at providing answers to natural language questions in an opendomain context. This task is usually achieved by combining information retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this pape</context>
</contexts>
<marker>Soubbotin, 2002</marker>
<rawString>Martin M. Soubbotin. 2002. Patterns of potential answer expressions as clues to the right answer. In TREC 2001 Proceedings.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>