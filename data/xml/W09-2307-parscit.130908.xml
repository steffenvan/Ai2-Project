<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.992077">
Discriminative Reordering with Chinese Grammatical Relations Features
</title>
<author confidence="0.951607">
Pi-Chuan Changa, Huihsin Tsengb, Dan Jurafskya, and Christopher D. Manninga
</author>
<affiliation confidence="0.979512">
aComputer Science Department, Stanford University, Stanford, CA 94305
bYahoo! Inc., Santa Clara, CA 95054
</affiliation>
<email confidence="0.993466">
{pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.com
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999231181818182">
The prevalence in Chinese of grammatical
structures that translate into English in dif-
ferent word orders is an important cause of
translation difficulty. While previous work has
used phrase-structure parses to deal with such
ordering problems, we introduce a richer set of
Chinese grammatical relations that describes
more semantically abstract relations between
words. Using these Chinese grammatical re-
lations, we improve a phrase orientation clas-
sifier (introduced by Zens and Ney (2006))
that decides the ordering of two phrases when
translated into English by adding path fea-
tures designed over the Chinese typed depen-
dencies. We then apply the log probabil-
ity of the phrase orientation classifier as an
extra feature in a phrase-based MT system,
and get significant BLEU point gains on three
test sets: MT02 (+0.59), MT03 (+1.00) and
MT05 (+0.77). Our Chinese grammatical re-
lations are also likely to be useful for other
NLP tasks.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999366823529412">
Structural differences between Chinese and English
are a major factor in the difficulty of machine trans-
lation from Chinese to English. The wide variety
of such Chinese-English differences include the or-
dering of head nouns and relative clauses, and the
ordering of prepositional phrases and the heads they
modify. Previous studies have shown that using syn-
tactic structures from the source side can help MT
performance on these constructions. Most of the
previous syntactic MT work has used phrase struc-
ture parses in various ways, either by doing syntax-
directed translation to directly translate parse trees
into strings in the target language (Huang et al.,
2006), or by using source-side parses to preprocess
the source sentences (Wang et al., 2007).
One intuition for using syntax is to capture dif-
ferent Chinese structures that might have the same
</bodyText>
<page confidence="0.98833">
51
</page>
<figure confidence="0.999925933333333">
(a)
(ROOT
(IP
(LCP
(QP (CD Կ)
(CLP (M ڣ)))
(LC 䝢))
(PU �)
(NP
(DP (DT ��))
(NP (NN ��)))
(VP
(ADVP (AD .}}))
(VP (VV 5M)
(NP
(NP
(ADJP (JJ hMP))
(NP (NN ��)))
(NP (NN ��)))
(QP (CD —_ff=+fZ)
(CLP (M t)))))
(PU I)))
(b)
(ROOT
(IP
(NP
(DP (DT 㪤ࠄ))
(NP (NN ৄؑ)))
(VP
(LCP
(QP (CD Կ)
(CLP (M ڣ)))
(LC 䝢))
(ADVP (AD ี儳))
(VP (VV ݙګ)
(NP
(NP
(ADJP (JJ ࡐࡳ))
(NP (NN 凹䣈)))
(NP (NN ދ凹)))
(QP (CD ԫۍԲԼ䣐)
(CLP (M ց)))))
(PU Ζ)))
Կ (three) ࡳࡐࡳ
Կ (fixed)
</figure>
<figureCaption confidence="0.999924">
Figure 1: Sentences (a) and (b) have the same mean-
</figureCaption>
<bodyText confidence="0.7158025">
ing, but different phrase structure parses. Both sentences,
however, have the same typed dependencies shown at the
bottom of the figure.
loc nsubi advmod dobi range
</bodyText>
<equation confidence="0.984360952380952">
ݙګ
ݙګ
(complete)
䝢 (over; in) (city)
䝢 ৄؑ
lobi det nn
ี儳ี儳ދ凹
ދ凹
(collectively) (invest)
ց
(yuan)
nummod
㪤ࠄ (these) 凹䣈
(asset)
amod
ۍԲԼ䣐
ԫۍԲԼ䣐
(12 billion)
ڣ (year)
ڣ
nummod
</equation>
<bodyText confidence="0.998962083333333">
meaning and hence the same translation in English.
But it turns out that phrase structure (and linear or-
der) are not sufficient to capture this meaning rela-
tion. Two sentences with the same meaning can have
different phrase structures and linear orders. In the
example in Figure 1, sentences (a) and (b) have the
same meaning, but different linear orders and dif-
ferent phrase structure parses. The translation of
sentence (a) is: “In the past three years these mu-
nicipalities have collectively put together investment
in fixed assets in the amount of 12 billion yuan.” In
sentence (b), “in the past three years” has moved its
</bodyText>
<note confidence="0.430922">
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 51–59,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999377">
position. The temporal adverbial “� 4-l&apos; *” (in the
past three years) has different linear positions in the
sentences. The phrase structures are different too: in
(a) the LCP is immediately under TP while in (b) it
is under VP.
We propose to use typed dependency parses in-
stead of phrase structure parses. Typed dependency
parses give information about grammatical relations
between words, instead of constituency informa-
tion. They capture syntactic relations, such as nsubj
(nominal subject) and dobj (direct object) , but also
encode semantic information such as in the loc (lo-
calizer) relation. For the example in Figure 1, if we
look at the sentence structure from the typed depen-
dency parse (bottom of Figure 1), “� 4-l&apos; *” is con-
nected to the main verb 51Dh (finish) by a loc (lo-
calizer) relation, and the structure is the same for
sentences (a) and (b). This suggests that this kind
of semantic and syntactic representation could have
more benefit than phrase structure parses.
Our Chinese typed dependencies are automati-
cally extracted from phrase structure parses. In En-
glish, this kind of typed dependencies has been in-
troduced by de Marneffe and Manning (2008) and
de Marneffe et al. (2006). Using typed dependen-
cies, it is easier to read out relations between words,
and thus the typed dependencies have been used in
meaning extraction tasks.
We design features over the Chinese typed depen-
dencies and use them in a phrase-based MT sys-
tem when deciding whether one chunk of Chinese
words (MT system statistical phrase) should appear
before or after another. To achieve this, we train a
discriminative phrase orientation classifier follow-
ing the work by Zens and Ney (2006), and we use
the grammatical relations between words as extra
features to build the classifier. We then apply the
phrase orientation classifier as a feature in a phrase-
based MT system to help reordering.
</bodyText>
<sectionHeader confidence="0.997283" genericHeader="method">
2 Discriminative Reordering Model
</sectionHeader>
<bodyText confidence="0.999607736842105">
Basic reordering models in phrase-based systems
use linear distance as the cost for phrase move-
ments (Koehn et al., 2003). The disadvantage of
these models is their insensitivity to the content of
the words or phrases. More recent work (Tillman,
2004; Och et al., 2004; Koehn et al., 2007) has in-
troduced lexicalized reordering models which esti-
mate reordering probabilities conditioned on the ac-
tual phrases. Lexicalized reordering models have
brought significant gains over the baseline reorder-
ing models, but one concern is that data sparseness
can make estimation less reliable. Zens and Ney
(2006) proposed a discriminatively trained phrase
orientation model and evaluated its performance as a
classifier and when plugged into a phrase-based MT
system. Their framework allows us to easily add in
extra features. Therefore we use it as a testbed to see
if we can effectively use features from Chinese typed
dependency structures to help reordering in MT.
</bodyText>
<subsectionHeader confidence="0.981368">
2.1 Phrase Orientation Classifier
</subsectionHeader>
<bodyText confidence="0.999955727272727">
We build up the target language (English) translation
from left to right. The phrase orientation classifier
predicts the start position of the next phrase in the
source sentence. In our work, we use the simplest
class definition where we group the start positions
into two classes: one class for a position to the left of
the previous phrase (reversed) and one for a position
to the right (ordered).
Let cj,j0 be the class denoting the movement from
source position j to source position j0 of the next
phrase. The definition is:
</bodyText>
<equation confidence="0.6560665">
= r reversed if j0 &lt; j
cj,j0 Sl ordered if j0 &gt; j
</equation>
<bodyText confidence="0.9119385">
The phrase orientation classifier model is in the log-
linear form:
</bodyText>
<equation confidence="0.99921925">
pλN1 (cj,j0 |f1J,eI1,i, j)
� l
exp (∑Nn=1 λn hn (f1J , e771, i, j, cj,j0) )
∑c0 exp(∑Nn=1λnhn(f1J,eI1,i,j,c0))
</equation>
<bodyText confidence="0.999477166666667">
i is the target position of the current phrase, and f1J
and eI1 denote the source and target sentences respec-
tively. c0 represents possible categories of cj,j0.
We can train this log-linear model on lots of la-
beled examples extracted from all of the aligned MT
training data. Figure 2 is an example of an aligned
sentence pair and the labeled examples that can be
extracted from it. Also, different from conventional
MERT training, we can have a large number of bi-
nary features for the discriminative phrase orienta-
tion classifier. The experimental setting will be de-
scribed in Section 4.1.
</bodyText>
<page confidence="0.889379">
52
</page>
<equation confidence="0.6731755">
j (0) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15)
i &lt;s&gt; ק௧ բ ګ䢠 խ㧺 㢑 ؆ 䬞࣋ խ ֒ದ ऱ ԫ 咨 ࣔਣ Ζ &lt;/s&gt;
</equation>
<listItem confidence="0.97597735">
(0) &lt;s&gt;
(1) Beihai
(2) has
(3) already
(4) become
(5) a
(6) bright
(7) star
(8) arising
(9) from
(10) China
(11) &apos;s
(12) policy
(13) of
(14) opening
(15) up
(16) to
(17) the
(18) outside
(19) world
</listItem>
<figure confidence="0.984957823529412">
(20) .
(21) &lt;/s&gt;
i j j&apos; class
0 0 1 ordered
1 1 2 ordered
3 2 3 ordered
4 3 11 ordered
5 11 12 ordered
6 12 13 ordered
7 13 9 reversed
8 9 10 ordered
9 10 8 reversed
10 8 7 reversed
15 7 5 reversed
16 5 6 ordered
18 6 14 ordered
20 14 15 ordered
</figure>
<figureCaption confidence="0.987525333333333">
Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the
labeled examples for the phrase orientation classifier. Note that the alignment grid in this example is automatically
generated.
</figureCaption>
<bodyText confidence="0.99826455">
The basic feature functions are similar to what
Zens and Ney (2006) used in their MT experiments.
The basic binary features are source words within a
window of size 3 (d ∈ −1,0,1) around the current
source position j, and target words within a window
of size 3 around the current target position i. In the
classifier experiments in Zens and Ney (2006) they
also use word classes to introduce generalization ca-
pabilities. In the MT setting it’s harder to incorpo-
rate the part-of-speech information on the target lan-
guage. Zens and Ney (2006) also exclude word class
information in the MT experiments. In our work
we will simply use the word features as basic fea-
tures for the classification experiments as well. As
a concrete example, we look at the labeled example
(i = 4, j = 3, j0 = 11) in Figure 2. We include the
word features in a window of size 3 around j and i
as in Zens and Ney (2006), we also include words
around j0 as features. So we will have nine word
features for (i = 4, j = 3, j0 = 11):
</bodyText>
<table confidence="0.631246333333333">
Src−1: Src0:)At, Src1:rPQ
Src2−1:In Src20: Src21:111
Tgt−1:already Tgt0:become Tgt1:a
</table>
<subsectionHeader confidence="0.994112">
2.2 Path Features Using Typed Dependencies
</subsectionHeader>
<bodyText confidence="0.999997111111111">
Assuming we have parsed the Chinese sentence that
we want to translate and have extracted the gram-
matical relations in the sentence, we design features
using the grammatical relations. We use the path be-
tween the two words annotated by the grammatical
relations. Using this feature helps the model learn
about what the relation is between the two chunks
of Chinese words. The feature is defined as follows:
for two words at positions p and q in the Chinese
</bodyText>
<page confidence="0.988597">
53
</page>
<table confidence="0.999952315789474">
Shared relations Chinese English
nn 15.48% 6.81%
punct 12.71% 9.64%
nsubj 6.87% 4.46%
rcmod 2.74% 0.44%
dobj 6.09% 3.89%
advmod 4.93% 2.73%
conj 6.34% 4.50%
num/nummod 3.36% 1.65%
attr 0.62% 0.01%
tmod 0.79% 0.25%
ccomp 1.30% 0.84%
xsubj 0.22% 0.34%
cop 0.07% 0.85%
cc 2.06% 3.73%
amod 3.14% 7.83%
prep 3.66% 10.73%
det 1.30% 8.57%
pobj 2.82% 10.49%
</table>
<tableCaption confidence="0.951748333333333">
Table 1: The percentage of typed dependencies in files
1–325 in Chinese (CTB6) and English (English-Chinese
Translation Treebank)
</tableCaption>
<bodyText confidence="0.999196277777778">
sentence (p &lt; q), we find the shortest path in the
typed dependency parse from p to q, concatenate all
the relations on the path and use that as a feature.
A concrete example is the sentences in Figure 3,
where the alignment grid and labeled examples are
shown in Figure 2. The glosses of the Chinese words
in the sentence are in Figure 3, and the English trans-
lation is “Beihai has already become a bright star
arising from China’s policy of opening up to the out-
side world.” which is also listed in Figure 2.
For the labeled example (i = 4, j = 3, j� = 11),
we look at the typed dependency parse to find the
path feature between )At, and ---. The relevant
dependencies are: dobj(At 1, HJRMI), clf (AMI, 01)
and nummod(Vii, ---). Therefore the path feature is
PATH:dobjR-clfR-nummodR. We also use the direc-
tionality: we add an R to the dependency name if it’s
going against the direction of the arrow.
</bodyText>
<sectionHeader confidence="0.995233" genericHeader="method">
3 Chinese Grammatical Relations
</sectionHeader>
<bodyText confidence="0.99900325">
Our Chinese grammatical relations are designed to
be very similar to the Stanford English typed depen-
dencies (de Marneffe and Manning, 2008; de Marn-
effe et al., 2006).
</bodyText>
<subsectionHeader confidence="0.966207">
3.1 Description
</subsectionHeader>
<bodyText confidence="0.999980727272727">
There are 45 named grammatical relations, and a de-
fault 46th relation dep (dependent). If a dependency
matches no patterns, it will have the most generic
relation dep. The descriptions of the 45 grammat-
ical relations are listed in Table 2 ordered by their
frequencies in files 1–325 of CTB6 (LDC2007T36).
The total number of dependencies is 85748, and
other than the ones that fall into the 45 grammatical
relations, there are also 7470 dependencies (8.71%
of all dependencies) that do not match any patterns,
and therefore keep the generic name dep.
</bodyText>
<subsectionHeader confidence="0.999727">
3.2 Chinese Specific Structures
</subsectionHeader>
<bodyText confidence="0.999991761904762">
Although we designed the typed dependencies to
show structures that exist both in Chinese and En-
glish, there are many other syntactic structures that
only exist in Chinese. The typed dependencies we
designed also cover those Chinese specific struc-
tures. For example, the usage of “n, ” (DE) is one
thing that could lead to different English transla-
tions. In the Chinese typed dependencies, there
are relations such as cpm (DE as complementizer)
or assm (DE as associative marker) that are used
to mark these different structures. The Chinese-
specific “7E” (BA) construction also has a relation
ba dedicated to it.
The typed dependencies annotate these Chinese
specific relations, but do not directly provide a map-
ping onto how they are translated into English. It
becomes more obvious how those structures affect
the ordering when Chinese sentences are translated
into English when we apply the typed dependencies
as features in the phrase orientation classifier. This
will be further discussed in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.999743">
3.3 Comparison with English
</subsectionHeader>
<bodyText confidence="0.999911714285714">
To compare the distribution of Chinese typed de-
pendencies with English, we extracted the English
typed dependencies from the translation of files 1–
325 in the English Chinese Translation Treebank
1.0 (LDC2007T02), which correspond to files 1–325
in CTB6. The English typed dependencies are ex-
tracted using the Stanford Parser.
There are 116,799 total English dependencies,
and 85,748 Chinese ones. On the corpus we use,
there are 45 distinct dependency types (not includ-
ing dep) in Chinese, and 50 in English. The cov-
erage of named relations is 91.29% in Chinese and
90.48% in English; the remainder are the unnamed
relation dep. We looked at the 18 shared relations
</bodyText>
<page confidence="0.977074">
54
</page>
<figure confidence="0.996092285714286">
dobj
nsubj nsubjlccomp loc rcmod
IM z MM, +Q Af A ffa + �fk_E n — Mf HAR °
Beihai already become China to outside open during rising (DE) one measure bright
word star
prep cpm
punct
</figure>
<figureCaption confidence="0.745953">
Figure 3: A Chinese example sentence labeled with typed dependencies
</figureCaption>
<figure confidence="0.8274185">
advmod
nummod
clf
.
</figure>
<bodyText confidence="0.97997375">
between Chinese and English in Table 1. Chinese
has more nn, punct, nsubj, rcmod, dobj, advmod,
conj, nummod, attr, tmod, and ccomp while English
uses more pobj, det, prep, amod, cc, cop, and xsubj,
due mainly to grammatical differences between Chi-
nese and English. For example, some determiners
in English (e.g., “the” in (1b)) are not mandatory in
Chinese:
</bodyText>
<listItem confidence="0.897404">
(1a) Ÿñ=/import and export Af/total value
(1b) The total value of imports and exports
</listItem>
<bodyText confidence="0.911428153846154">
In another difference, English uses adjectives
(amod) to modify a noun (“financial” in (2b)) where
Chinese can use noun compounds (“— /finance”
in (2a)).
(2a) NO/Tibet —�/finance 4›/system iMV/reform
(2b) the reform in Tibet ’s financial system
We also noticed some larger differences between
the English and Chinese typed dependency distribu-
tions. We looked at specific examples and provide
the following explanations.
prep and pobj English has much more uses of prep
and pobj. We examined the data and found three
major reasons:
</bodyText>
<listItem confidence="0.743719136363636">
1. Chinese uses both prepositions and postposi-
tions while English only has prepositions. “Af-
ter” is used as a postposition in Chinese exam-
ple (3a), but a preposition in English (3b):
(3a) Ê E/1997 ƒÀ/after
(3b) after 1997
2. Chinese uses noun phrases in some cases where
English uses prepositions. For example, “ƒ
-” (period, or during) is used as a noun phrase
in (4a), but it’s a preposition in English.
(4a) Ê E/1997 91/to ÊA/1998 ƒ- /period
(4b) during 1997-1998
3. Chinese can use noun phrase modification in
situations where English uses prepositions. In
example (5a), Chinese does not use any prepo-
sitions between “apple company” and “new
product”, but English requires use of either
“of” or “from”.
(5a) * ��P/apple company c—¬/new product
(5b) the new product of (or from) Apple
The Chinese DE constructions are also often
translated into prepositions in English.
</listItem>
<bodyText confidence="0.9991168">
cc and punct The Chinese sentences contain more
punctuation (punct) while the English translation
has more conjunctions (cc), because English uses
conjunctions to link clauses (“and” in (6b)) while
Chinese tends to use only punctuation (“,” in (6a)).
</bodyText>
<listItem confidence="0.94924125">
(6a) YJ/these Â=/city öÌ/social 29F/economic
&amp;0/development ·¤/rapid , •) /local
29F/economic &amp;quot;Å/strength Ò-HI/clearly
*51/enhance
</listItem>
<bodyText confidence="0.796671">
(6b) In these municipalities the social and economic de-
velopment has been rapid, and the local economic
strength has clearly been enhanced
rcmod and ccomp There are more rcmod and
ccomp in the Chinese sentences and less in the En-
glish translation, because of the following reasons:
</bodyText>
<listItem confidence="0.993123666666667">
1. Some English adjectives act as verbs in Chi-
nese. For example, JW (new) is an adjectival
predicate in Chinese and the relation between
� (new) and › 1 (system) is rcmod. But
“new” is an adjective in English and the En-
glish relation between “new” and “system” is
amod. This difference contributes to more rc-
mod in Chinese.
(7a)Ni/new n/(DE) X=/verify and write off
(7b) a new sales verification system
2. Chinese has two special verbs (VC): 4 (SHI)
and t, (WEI) which English doesn’t use. For
</listItem>
<page confidence="0.993306">
55
</page>
<table confidence="0.968061155555556">
abbreviation short description Chinese example typed dependency counts percentage
nn noun compound modifier q* ¥e nn(¥e, q*) 13278 15.48%
punct punctuation 0)c cif- ,Ò , punct(,Ò, ,) 10896 12.71%
nsubj nominal subject ’“ Off nsubj(4ff, ’“) 5893 6.87%
conj conjunct (links two conjuncts) ÷÷ Z Æa&apos;f4 conj(Æa&apos;f4, ÷÷) 5438 6.34%
dobj direct object ËÀ ÅY T Ôt _ G )ZG dobj(ÅY, )ZG) 5221 6.09%
advmod adverbial modifier \ �c ˜Þ )ZG advmod(˜Þ, `lc) 4231 4.93%
prep prepositional modifier ó &amp;quot;B ¥ ÅZ Õ prep( Õ, ó) 3138 3.66%
nummod number modifier Ôt _ G )ZG nummod(G, Ôt&apos;-) 2885 3.36%
amod adjectival modifier *-E ÓÇ amod(ÓÇ, *-Q 2691 3.14%
pobj prepositional object Êâ ‹O� N1/2 pobj(Êâ, R1/2) 2417 2.82%
rcmod relative clause modifier X 0 ±t , { &lt;&amp; rcmod(&lt;&amp;, ±t) 2348 2.74%
cpm complementizer ff*- ËÀ { 299 ÙÄ cpm(ff*-, {) 2013 2.35%
assm associative marker èA { Û¬ assm(èA, {) 1969 2.30%
assmod associative modifier èA { Û¬ assmod(Û¬, èA) 1941 2.26%
cc coordinating conjunction ÷÷ Z Æa&apos;f4 cc(Æa&apos;f4, Z) 1763 2.06%
clf classifier modifier Ô�� G )ZG clf()ZG, G) 1558 1.82%
ccomp clausal complement Uq û1/2 `lc Rz f~ µÿ ccomp(û1/2, Rz) 1113 1.30%
det determiner YJ 299 ÙÄ det(ÙÄ, YJ) 1113 1.30%
lobj localizer object £# * lobj(*, £#) 1010 1.18%
range dative object that is a quantifier phrase Äb M¬ _7õ Ã range(Äb, Ã) 891 1.04%
asp aspect marker &amp;3/4 T *~ asp(* -I$, T) 857 1.00%
tmod temporal modifier 1H�1 X 0 ±t , tmod(±t, 1H�1) 679 0.79%
plmod localizer modifier of a preposition ó Y ¡ y::E Þ plmod(ó, Þ) 630 0.73%
attr attributive Ž4R Y9 -º7 ›Ã attr(Y9, ›Ã) 534 0.62%
mmod modal verb modifier *C 91&amp;quot; zt âF mmod(zt, R) 497 0.58%
loc localizer 3 ÊÄ 1Þ loc(3, 1Þ) 428 0.50%
top topic OÓ 4 Ì_E ÙÄ top(4, OÓ) 380 0.44%
pccomp clausal complement of a preposition â ‹)� \ •ë pccomp(â, •ë) 374 0.44%
etc etc modifier �4R - )Zs • IM etc()Zs, •) 295 0.34%
lccomp clausal complement of a localizer ¥) é �h ff8 ¥ f&apos;å { Òh lccomp(¥, ff8) 207 0.24%
ordmod ordinal number modifier ‘Ô Ç åè ordmod(Ç, ‘Ô) 199 0.23%
xsubj controlling subject Uq û1/2 `lc Rz f~ µÿ xsubj(Rz, Uq) 192 0.22%
neg negative modifier 1H�1 X 0 ±t , neg(±t, X) 186 0.22%
rcomp resultative complement ÏÄ ÄÕ rcomp(ÏÄ, ÄÕ) 176 0.21%
comod coordinated verb compound modifier ÅY &amp;quot;q comod(ÅY, &amp;quot;q) 150 0.17%
vmod verb modifier Ù ó |Ñ � Û è &apos; 0Á { *~ vmod(0Á, |Ñ) 133 0.16%
prtmod particles such as Ä,1,�, fTA ó —Aš Ä Rz { ÄÒ prtmod(Rz, Ä) 124 0.14%
ba “ba” construction 2 Õ?y�,j7 Ý 5 =œ ba(Ý 5, 2) 95 0.11%
dvpm manner DE(•) modifier ‹H Ay�1, �� Ay�µ- dvpm(‹H, •) 73 0.09%
dvpmod a “XP+DEV(•)” phrase that modifies VP ‹H A PIŽ 11, dvpmod(3Ž, ‹H) 69 0.08%
prnmod parenthetical modifier ¬« Ï- ( 1990 – 1995 ) prnmod(Ï-, 1995) 67 0.08%
cop copular Æ 4 NÉ T { 299 cop(NÉ /T--, 4) 59 0.07%
pass passive marker ú €1/2 Y9 ° Rb —A pass(€1/2, ú) 53 0.06%
nsubjpass nominal passive subject 1 ú Á* AM Ó1L { Mt£ nsubjpass(Á*, 1) 14 0.02%
</table>
<tableCaption confidence="0.99952">
Table 2: Chinese grammatical relations and examples. The counts are from files 1–325 in CTB6.
</tableCaption>
<bodyText confidence="0.9991245">
example, there is an additional relation, ccomp,
between the verb 4/(SHI) and \„/reduce in
(8a). The relation is not necessary in English,
since 4/SHI is not translated.
</bodyText>
<listItem confidence="0.86997">
(8a) ./second 4/(SHI) -ÊÊ7&apos;I-%#/1996
¥)/China LlÝ/substantially
\„/reduce )c{/tariff
(8b) Second, China reduced tax substantially in
1996.
</listItem>
<bodyText confidence="0.999366117647059">
conj There are more conj in Chinese than in En-
glish for three major reasons. First, sometimes one
complete Chinese sentence is translated into sev-
eral English sentences. Our conj is defined for two
grammatical roles occurring in the same sentence,
and therefore, when a sentence breaks into multiple
ones, the original relation does not apply. Second,
we define the two grammatical roles linked by the
conj relation to be in the same word class. However,
words which are in the same word class in Chinese
may not be in the same word class in English. For
example, adjective predicates act as verbs in Chi-
nese, but as adjectives in English. Third, certain con-
structions with two verbs are described differently
between the two languages: verb pairs are described
as coordinations in a serial verb construction in Chi-
nese, but as the second verb being the complement
</bodyText>
<page confidence="0.992467">
56
</page>
<bodyText confidence="0.993737">
of the first verb in English.
</bodyText>
<sectionHeader confidence="0.983795" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.993219">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.99995985">
We use various Chinese-English parallel corpora1
for both training the phrase orientation classifier, and
for extracting statistical phrases for the phrase-based
MT system. The parallel data contains 1,560,071
sentence pairs from various parallel corpora. There
are 12,259,997 words on the English side. Chi-
nese word segmentation is done by the Stanford Chi-
nese segmenter (Chang et al., 2008). After segmen-
tation, there are 11,061,792 words on the Chinese
side. The alignment is done by the Berkeley word
aligner (Liang et al., 2006) and then we symmetrized
the word alignment using the grow-diag heuristic.
For the phrase orientation classifier experiments,
we extracted labeled examples using the parallel
data and the alignment as in Figure 2. We extracted
9,194,193 total valid examples: 86.09% of them are
ordered and the other 13.91% are reversed. To eval-
uate the classifier performance, we split these exam-
ples into training, dev and test set (8 : 1 : 1). The
phrase orientation classifier used in MT experiments
is trained with all of the available labeled examples.
Our MT experiments use a re-implementation of
Moses (Koehn et al., 2003) called Phrasal, which
provides an easier API for adding features. We
use a 5-gram language model trained on the Xin-
hua and AFP sections of the Gigaword corpus
(LDC2007T40) and also the English side of all the
LDC parallel data permissible under the NIST08
rules. Documents of Gigaword released during the
epochs of MT02, MT03, MT05, and MT06 were
removed. For features in MT experiments, we in-
corporate Moses’ standard eight features as well as
the lexicalized reordering features. To have a more
comparable setting with (Zens and Ney, 2006), we
also have a baseline experiment with only the stan-
dard eight features. Parameter tuning is done with
Minimum Error Rate Training (MERT) (Och, 2003).
The tuning set for MERT is the NIST MT06 data
set, which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919 sen-
</bodyText>
<footnote confidence="0.29537">
1LDC2002E18, LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,
LDC2002L27 and LDC2005T34.
</footnote>
<table confidence="0.928689666666667">
tences), and MT05 (1082 sentences).
4.2 Phrase Orientation Classifier
Feature Sets #features Train. Acc. Train. Dev Dev
Acc. (%) Macro-F Acc. (%) Macro-F
Majority class - 86.09 - 86.09 -
Src 1483696 89.02 71.33 88.14 69.03
Src+Tgt 2976108 92.47 82.52 91.29 79.80
Src+Src2+Tgt 4440492 95.03 88.76 93.64 85.58
Src+Src2+Tgt+PATH 4691887 96.01 91.15 94.27 87.22
</table>
<tableCaption confidence="0.904176">
Table 3: Feature engineering of the phrase orientation
classifier. Accuracy is defined as (#correctly labeled ex-
amples) divided by (#all examples). The macro-F is an
average of the accuracies of the two classes.
</tableCaption>
<bodyText confidence="0.999953176470588">
The basic source word features described in Sec-
tion 2 are referred to as Src, and the target word
features as Tgt. The feature set that Zens and Ney
(2006) used in their MT experiments is Src+Tgt. In
addition to that, we also experimented with source
word features Src2 which are similar to Src, but take
a window of 3 around j� instead of j. In Table 3
we can see that adding the Src2 features increased
the total number of features by almost 50%, but also
improved the performance. The PATH features add
fewer total number of features than the lexical fea-
tures, but still provide a 10% error reduction and
1.63 on the macro-F1 on the dev set. We use the best
feature sets from the feature engineering in Table 3
and test it on the test set. We get 94.28% accuracy
and 87.17 macro-F1. The overall improvement of
accuracy over the baseline is 8.19 absolute points.
</bodyText>
<subsectionHeader confidence="0.992918">
4.3 MT Experiments
</subsectionHeader>
<bodyText confidence="0.9999745">
In the MT setting, we use the log probability from
the phrase orientation classifier as an extra feature.
The weight of this discriminative reordering feature
is also tuned by MERT, along with other Moses
features. In order to understand how much the
PATH features add value to the MT experiments, we
trained two phrase orientation classifiers with differ-
ent features: one with the Src+Src2+Tgt feature set,
and the other one with Src+Src2+Tgt+PATH. The re-
sults are listed in Table 4. We compared to two
different baselines: one is Moses8Features which
has a distance-based reordering model, the other is
Baseline which also includes lexicalized reorder-
ing features. From the table we can see that using
the discriminative reordering model with PATH fea-
tures gives significant improvement over both base-
</bodyText>
<page confidence="0.994472">
57
</page>
<table confidence="0.999934285714286">
Setting #MERT features MT06(tune) MT02 MT03 MT05
Moses8Features 8 31.49 31.63 31.26 30.26
Moses8Features+DiscrimRereorderNoPATH 9 31.76( +0.27) 31.86( +0.23) 32.09( +0.83) 31.14( +0.88)
Moses8Features+DiscrimRereorderWithPATH 9 32.34( +0.85) 32.59( +0.96) 32.70( +1.44) 31.84( +1.58)
Baseline (Moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89
Baseline+DiscrimRereorderNoPATH 17 32.73( +0.18) 32.58( +0.02) 32.99( +0.34) 31.80( −0.09)
Baseline+DiscrimRereorderWithPATH 17 32.97( +0.42) 33.15( +0.59) 33.65( +1.00) 32.66( +0.77)
</table>
<tableCaption confidence="0.9868">
Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold
are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).
</tableCaption>
<figure confidence="0.917885">
det nn
ٺ 呂੄ 䣈঴
every level product
products of all level
det nn
</figure>
<figureCaption confidence="0.9990905">
Figure 4: Two examples for the feature PATH:det-nn and
how the reordering occurs.
</figureCaption>
<bodyText confidence="0.997333727272727">
lines. If we use the discriminative reordering model
without PATH features and only with word features,
we still get improvement over the Moses8Features
baseline, but the MT performance is not signifi-
cantly different from Baseline which uses lexical-
ized reordering features. From Table 4 we see that
using the Src+Src2+Tgt+PATH features significantly
outperforms both baselines. Also, if we compare be-
tween Src+Src2+Tgt and Src+Src2+Tgt+PATH, the
differences are also statistically significant, which
shows the effectiveness of the path features.
</bodyText>
<subsectionHeader confidence="0.9089635">
4.4 Analysis: Highly-weighted Features in the
Phrase Orientation Model
</subsectionHeader>
<bodyText confidence="0.993175">
There are a lot of features in the log-linear phrase
orientation model. We looked at some highly-
weighted PATH features to understand what kind
of grammatical constructions were informative for
phrase orientation. We found that many path fea-
tures corresponded to our intuitions. For example,
the feature PATH:prep-dobjR has a high weight for
being reversed. This feature informs the model that
in Chinese a PP usually appears before VP, but in
English they should be reversed. Other features
with high weights include features related to the
DE construction that is more likely to translate to
a relative clause, such as PATH:advmod-rcmod and
PATH:rcmod. They also indicate the phrases are
ՠ䢓 䭇 䣈ଖ
more likely to be chosen in reversed order. Another
frequent pattern that has not been emphasized in the
previous literature is PATH:det-nn, meaning that a
[DT NP1NP2] in Chinese is translated into English
as [NP2 DT NP1]. Examples with this feature are
in Figure 4. We can see that the important features
decided by the phrase orientation model are also im-
portant from a linguistic perspective.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992294117647">
We introduced a set of Chinese typed dependencies
that gives information about grammatical relations
between words, and which may be useful in other
NLP applications as well as MT. We used the typed
dependencies to build path features and used them to
improve a phrase orientation classifier. The path fea-
tures gave a 10% error reduction on the accuracy of
the classifier and 1.63 points on the macro-F1 score.
We applied the log probability as an additional fea-
ture in a phrase-based MT system, which improved
the BLEU score of the three test sets significantly
(0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).
This shows that typed dependencies on the source
side are informative for the reordering component in
a phrase-based system. Whether typed dependen-
cies can lead to improvements in other syntax-based
MT systems remains a question for future research.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.886193777777778">
The authors would like to thank Marie-Catherine de
Marneffe for her help on the typed dependencies,
and Daniel Cer for building the decoder. This work
is funded by a Stanford Graduate Fellowship to the
first author and gift funding from Google for the
project “Translating Chinese Correctly”.
i_t- fl վڣ ZAV ,0, NO
whole city this year industry total output value
gross industrial output value of the whole city this year
</bodyText>
<page confidence="0.982774">
58
</page>
<sectionHeader confidence="0.998238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873044776119">
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 224–232, Columbus, Ohio, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1–8, Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449–454.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Demonstration Session.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104–111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of HLT-NAACL.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57–
64, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101–104.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 737–745, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55–63, New York City, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.999261">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.383731">
<title confidence="0.996593">Discriminative Reordering with Chinese Grammatical Relations Features</title>
<author confidence="0.868843">D</author>
<affiliation confidence="0.6613845">Science Department, Stanford University, Stanford, CA Inc., Santa Clara, CA</affiliation>
<email confidence="0.996091">huihui@yahoo-inc.com</email>
<abstract confidence="0.997322608695652">The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three sets: MT02 MT03 and Our Chinese grammatical relations are also likely to be useful for other NLP tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="22218" citStr="Chang et al., 2008" startWordPosition="3788" endWordPosition="3791">he two languages: verb pairs are described as coordinations in a serial verb construction in Chinese, but as the second verb being the complement 56 of the first verb in English. 4 Experimental Results 4.1 Experimental Setting We use various Chinese-English parallel corpora1 for both training the phrase orientation classifier, and for extracting statistical phrases for the phrase-based MT system. The parallel data contains 1,560,071 sentence pairs from various parallel corpora. There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For the phrase orientation classifier experiments, we extracted labeled examples using the parallel data and the alignment as in Figure 2. We extracted 9,194,193 total valid examples: 86.09% of them are ordered and the other 13.91% are reversed. To evaluate the classifier performance, we split these examples into training, dev and test set (8 : 1 : 1). The phrase orientation classi</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<location>Manchester, UK,</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC-06, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1942" citStr="Huang et al., 2006" startWordPosition="286" endWordPosition="289">ish are a major factor in the difficulty of machine translation from Chinese to English. The wide variety of such Chinese-English differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads they modify. Previous studies have shown that using syntactic structures from the source side can help MT performance on these constructions. Most of the previous syntactic MT work has used phrase structure parses in various ways, either by doing syntaxdirected translation to directly translate parse trees into strings in the target language (Huang et al., 2006), or by using source-side parses to preprocess the source sentences (Wang et al., 2007). One intuition for using syntax is to capture different Chinese structures that might have the same 51 (a) (ROOT (IP (LCP (QP (CD Կ) (CLP (M ڣ))) (LC 䝢)) (PU �) (NP (DP (DT ��)) (NP (NN ��))) (VP (ADVP (AD .}})) (VP (VV 5M) (NP (NP (ADJP (JJ hMP)) (NP (NN ��))) (NP (NN ��))) (QP (CD —_ff=+fZ) (CLP (M t))))) (PU I))) (b) (ROOT (IP (NP (DP (DT 㪤)) (NP (NN ৄؑ))) (VP (LCP (QP (CD Կ) (CLP (M ڣ))) (LC 䝢)) (ADVP (AD ี儳)) (VP (VV ݙګ) (NP (NP (ADJP (JJ )) (NP (NN 凹䣈))) (NP (NN ދ凹))) (QP (CD ۍԲԼ䣐) (CLP (M ց))))) </context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="5814" citStr="Koehn et al., 2003" startWordPosition="951" endWordPosition="954">n a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when pl</context>
<context position="22973" citStr="Koehn et al., 2003" startWordPosition="3912" endWordPosition="3915">2006) and then we symmetrized the word alignment using the grow-diag heuristic. For the phrase orientation classifier experiments, we extracted labeled examples using the parallel data and the alignment as in Figure 2. We extracted 9,194,193 total valid examples: 86.09% of them are ordered and the other 13.91% are reversed. To evaluate the classifier performance, we split these examples into training, dev and test set (8 : 1 : 1). The phrase orientation classifier used in MT experiments is trained with all of the available labeled examples. Our MT experiments use a re-implementation of Moses (Koehn et al., 2003) called Phrasal, which provides an easier API for adding features. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. For features in MT experiments, we incorporate Moses’ standard eight features as well as the lexicalized reordering features. To have a more comparable setting with (Zens and Ney, 2006), we also have a baseline experiment with only the standar</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,</location>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), Demonstration Session.</booktitle>
<marker>Constantin, Herbst, 2007</marker>
<rawString>Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="22359" citStr="Liang et al., 2006" startWordPosition="3812" endWordPosition="3815">ment 56 of the first verb in English. 4 Experimental Results 4.1 Experimental Setting We use various Chinese-English parallel corpora1 for both training the phrase orientation classifier, and for extracting statistical phrases for the phrase-based MT system. The parallel data contains 1,560,071 sentence pairs from various parallel corpora. There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For the phrase orientation classifier experiments, we extracted labeled examples using the parallel data and the alignment as in Figure 2. We extracted 9,194,193 total valid examples: 86.09% of them are ordered and the other 13.91% are reversed. To evaluate the classifier performance, we split these examples into training, dev and test set (8 : 1 : 1). The phrase orientation classifier used in MT experiments is trained with all of the available labeled examples. Our MT experiments use a re-implementation of Moses (Koehn</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23667" citStr="Och, 2003" startWordPosition="4029" endWordPosition="4030">anguage model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. For features in MT experiments, we incorporate Moses’ standard eight features as well as the lexicalized reordering features. To have a more comparable setting with (Zens and Ney, 2006), we also have a baseline experiment with only the standard eight features. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sen1LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85, LDC2002L27 and LDC2005T34. tences), and MT05 (1082 sentences). 4.2 Phrase Orientation Classifier Feature Sets #features Train. Acc. Train. Dev Dev Acc. (%) Macro-F Acc. (%) Macro-F Majority class - 86.09 - 86.09 - Src 1483696 89.02 71.33 88.14 69.03 Src+Tgt 2976108 92.47 82.52 91.29 79.80 Src+Src2+Tgt 4440492 95.03 88.76 93.64 85.58 Src+Src2+Tgt+PATH 4691887 96.01 9</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="26950" citStr="Riezler and Maxwell, 2005" startWordPosition="4539" endWordPosition="4542">0.27) 31.86( +0.23) 32.09( +0.83) 31.14( +0.88) Moses8Features+DiscrimRereorderWithPATH 9 32.34( +0.85) 32.59( +0.96) 32.70( +1.44) 31.84( +1.58) Baseline (Moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89 Baseline+DiscrimRereorderNoPATH 17 32.73( +0.18) 32.58( +0.02) 32.99( +0.34) 31.80( −0.09) Baseline+DiscrimRereorderWithPATH 17 32.97( +0.42) 33.15( +0.59) 33.65( +1.00) 32.66( +0.77) Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005). det nn ٺ 呂 䣈 every level product products of all level det nn Figure 4: Two examples for the feature PATH:det-nn and how the reordering occurs. lines. If we use the discriminative reordering model without PATH features and only with word features, we still get improvement over the Moses8Features baseline, but the MT performance is not significantly different from Baseline which uses lexicalized reordering features. From Table 4 we see that using the Src+Src2+Tgt+PATH features significantly outperforms both baselines. Also, if we compare between Src+Src2+Tgt and Src+Src2+Tgt+PATH, the diffe</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57– 64, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004: Short Papers,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="5943" citStr="Tillman, 2004" startWordPosition="974" endWordPosition="975">r another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system. Their framework allows us to easily add in extra features. Therefore we use it as a testbed </context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL 2004: Short Papers, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>737--745</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2029" citStr="Wang et al., 2007" startWordPosition="300" endWordPosition="303"> The wide variety of such Chinese-English differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads they modify. Previous studies have shown that using syntactic structures from the source side can help MT performance on these constructions. Most of the previous syntactic MT work has used phrase structure parses in various ways, either by doing syntaxdirected translation to directly translate parse trees into strings in the target language (Huang et al., 2006), or by using source-side parses to preprocess the source sentences (Wang et al., 2007). One intuition for using syntax is to capture different Chinese structures that might have the same 51 (a) (ROOT (IP (LCP (QP (CD Կ) (CLP (M ڣ))) (LC 䝢)) (PU �) (NP (DP (DT ��)) (NP (NN ��))) (VP (ADVP (AD .}})) (VP (VV 5M) (NP (NP (ADJP (JJ hMP)) (NP (NN ��))) (NP (NN ��))) (QP (CD —_ff=+fZ) (CLP (M t))))) (PU I))) (b) (ROOT (IP (NP (DP (DT 㪤)) (NP (NN ৄؑ))) (VP (LCP (QP (CD Կ) (CLP (M ڣ))) (LC 䝢)) (ADVP (AD ี儳)) (VP (VV ݙګ) (NP (NP (ADJP (JJ )) (NP (NN 凹䣈))) (NP (NN ދ凹))) (QP (CD ۍԲԼ䣐) (CLP (M ց))))) (PU Ζ))) Կ (three)  Կ (fixed) Figure 1: Sentences (a) and (b) have the same meaning,</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 737–745, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="813" citStr="Zens and Ney (2006)" startWordPosition="102" endWordPosition="105">y, Stanford, CA 94305 bYahoo! Inc., Santa Clara, CA 95054 {pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.com Abstract The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other NLP tasks. 1 Introduction Structural differences between Chinese and English are a major factor in the difficulty of machine translation from Chinese to English. T</context>
<context position="5455" citStr="Zens and Ney (2006)" startWordPosition="893" endWordPosition="896"> parses. In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and de Marneffe et al. (2006). Using typed dependencies, it is easier to read out relations between words, and thus the typed dependencies have been used in meaning extraction tasks. We design features over the Chinese typed dependencies and use them in a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has introduced lexicalized reordering models which estimate reordering p</context>
<context position="8910" citStr="Zens and Ney (2006)" startWordPosition="1511" endWordPosition="1514">4) opening (15) up (16) to (17) the (18) outside (19) world (20) . (21) &lt;/s&gt; i j j&apos; class 0 0 1 ordered 1 1 2 ordered 3 2 3 ordered 4 3 11 ordered 5 11 12 ordered 6 12 13 ordered 7 13 9 reversed 8 9 10 ordered 9 10 8 reversed 10 8 7 reversed 15 7 5 reversed 16 5 6 ordered 18 6 14 ordered 20 14 15 ordered Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the labeled examples for the phrase orientation classifier. Note that the alignment grid in this example is automatically generated. The basic feature functions are similar to what Zens and Ney (2006) used in their MT experiments. The basic binary features are source words within a window of size 3 (d ∈ −1,0,1) around the current source position j, and target words within a window of size 3 around the current target position i. In the classifier experiments in Zens and Ney (2006) they also use word classes to introduce generalization capabilities. In the MT setting it’s harder to incorporate the part-of-speech information on the target language. Zens and Ney (2006) also exclude word class information in the MT experiments. In our work we will simply use the word features as basic features </context>
<context position="23515" citStr="Zens and Ney, 2006" startWordPosition="4002" endWordPosition="4005">les. Our MT experiments use a re-implementation of Moses (Koehn et al., 2003) called Phrasal, which provides an easier API for adding features. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. For features in MT experiments, we incorporate Moses’ standard eight features as well as the lexicalized reordering features. To have a more comparable setting with (Zens and Ney, 2006), we also have a baseline experiment with only the standard eight features. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sen1LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85, LDC2002L27 and LDC2005T34. tences), and MT05 (1082 sentences). 4.2 Phrase Orientation Classifier Feature Sets #features Train. Acc. Train. Dev Dev Acc. (%) Macro-F Acc. (%) Macro-F Majority class - 86.09 - 86.09 - Sr</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine translation. In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>