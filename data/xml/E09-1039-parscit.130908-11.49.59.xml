<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002371">
<title confidence="0.977234">
Person Identification from Text and Speech Genre Samples
</title>
<author confidence="0.96061">
Jade Goldstein-Stewart Ransom Winder Roberta Evans Sabin
</author>
<affiliation confidence="0.92458">
U.S. Department of Defense The MITRE Corporation Loyola University
</affiliation>
<address confidence="0.812017">
Hanover, MD, USA Baltimore, MD, USA
</address>
<email confidence="0.99645">
jadeg@acm.org rwinder@mitre.org res@loyola.edu
</email>
<sectionHeader confidence="0.993797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895190476191">
In this paper, we describe experiments con-
ducted on identifying a person using a novel
unique correlated corpus of text and audio
samples of the person’s communication in six
genres. The text samples include essays,
emails, blogs, and chat. Audio samples were
collected from individual interviews and group
discussions and then transcribed to text. For
each genre, samples were collected for six top-
ics. We show that we can identify the com-
municant with an accuracy of 71% for six fold
cross validation using an average of 22,000
words per individual across the six genres.
For person identification in a particular genre
(train on five genres, test on one), an average
accuracy of 82% is achieved. For identifica-
tion from topics (train on five topics, test on
one), an average accuracy of 94% is achieved.
We also report results on identifying a per-
son’s communication in a genre using text ge-
nres only as well as audio genres only.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999641315789474">
Can one identify a person from samples of
his/her communication? What common patterns
of communication can be used to identify
people? Are such patterns consistent across va-
rying genres?
People tend to be interested in subjects and
topics that they discuss with friends, family, col-
leagues and acquaintances. They can communi-
cate with these people textually via email, text
messages and chat rooms. They can also com-
municate via verbal conversations. Other forms
of communication could include blogs or even
formal writings such as essays or scientific ar-
ticles. People communicating in these different
“genres” may have different stylistic patterns and
we are interested in whether or not we could
identify people from their communications in
different genres.
The attempt to identify authorship of written
text has a long history that predates electronic
computing. The idea that features such as aver-
age word length and average sentence length
could allow an author to be identified dates to
Mendenhall (1887). Mosteller and Wallace
(1964) used function words in a groundbreaking
study that identified authors of The Federalist
Papers. Since then many attempts at authorship
attribution have used function words and other
features, such as word class frequencies and
measures derived from syntactic analysis, often
combined using multivariable statistical tech-
niques.
Recently, McCarthy (2006) was able to diffe-
rentiate three authors’ works, and Hill and Prov-
ost (2003), using a feature of co-citations,
showed that they could successfully identify
scientific articles by the same person, achieving
85% accuracy when the person has authored over
100 papers. Levitan and Argamon (2006) and
McCombe (2002) further investigated authorship
identification of The Federalist Papers (three
authors).
The genre of the text may affect the authorship
identification task. The attempt to characterize
genres dates to Biber (1988) who selected 67
linguistic features and analyzed samples of 23
spoken and written genres. He determined six
factors that could be used to identify written text.
Since his study, new “cybergenres” have
evolved, including email, blogs, chat, and text
messaging. Efforts have been made to character-
ize the linguistic features of these genres (Baron,
2003; Crystal, 2001; Herring, 2001; Shepherd
and Watters, 1999; Yates, 1996). The task is
complicated by the great diversity that can be
exhibited within even a single genre. Email can
be business-related, personal, or spam; the style
</bodyText>
<note confidence="0.923079">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997994">
336
</page>
<bodyText confidence="0.998731791044776">
can be tremendously affected by demographic
factors, including gender and age of the sender.
The context of communication influences lan-
guage style (Thomson and Murachver, 2001;
Coupland, et al., 1988). Some people use ab-
breviations to ease the efficiency of communica-
tion in informal genres – items that one would
not find in a formal essay. Informal writing may
also contain emoticons (e.g., “:-)” or “”) to
convey mood.
Successes have been achieved in categorizing
web page decriptions (Calvo, et al., 2004) and
genre determination (Goldstein-Stewart, et al.,
2007; Santini 2007). Genders of authors have
been successfully identified within the British
National Corpus (Koppel, et al., 2002). In
authorship identification, recent research has fo-
cused on identifying authors within a particular
genre: email collections, news stories, scientific
papers, listserv forums, and computer programs
(de Vel, et al., 2001; Krsul and Spafford, 1997;
Madigan, et al., 2005; McCombe, 2002). In the
KDD Cup 2003 Competitive Task, systems at-
tempted to identify successfully scientific articles
authored by the same person. The best system
(Hill and Provost, 2003) was able to identify
successfully scientific articles by the same per-
son 45% of the time; for authors with over 100
papers, 85% accuracy was achieved.
Are there common features of communication
of an individual across and within genres? Un-
doubtedly, the lack of corpora has been an impe-
diment to answering this question, as gathering
personal communication samples faces consider-
able privacy and accessibility hurdles. To our
knowledge, all previous studies have focused on
individual communications in one or possibly
two genres.
To analyze, compare, and contrast the com-
munication of individuals across and within dif-
ferent modalities, we collected a corpus consist-
ing of communication samples of 21 people in
six genres on six topics. We believe this corpus
is the first attempt to create such a correlated
corpus.
From this corpus, we are able to perform expe-
riments on person identification. Specifically,
this means recognizing which individual of a set
of people composed a document or spoke an ut-
terance which was transcribed. We believe using
text and transcribed speech in this manner is a
novel research area. In particular, the following
types of experiments can be performed:
- Identification of person in a novel genre
(using five genres as training)
- Identification of person in a novel topic
(using five topics as training)
- Identification of person in written genres,
after training on the two spoken genres
- Identification of person in spoken genres,
after training on the written genres
- Identification of person in written genres,
after training on the other written genres
In this paper, we discuss the formation and
statistics of this corpus and report results for
identifying individual people using techniques
that utilize several different feature sets.
</bodyText>
<sectionHeader confidence="0.952787" genericHeader="method">
2 Corpus Collection
</sectionHeader>
<bodyText confidence="0.999946">
Our interest was in the research question: can a
person be identified from their writing and audio
samples? Since we hypothesize that people
communicate about items of interest to them
across various genres, we decided to test this
theory. Email and chat were chosen as textual
genres (Table 1), since text messages, although
very common, were not easy to collect. We also
collected blogs and essays as samples of textual
genres. For audio genres, to simulate
conversational speech as much as possible, we
collected data from interviews and discussion
groups that consisted of sets of subjects
participating in the study. Genres labeled “peer
give and take” allowed subjects to interact.
Such a collection of genres allows us to
examine both conversational and non-
conversational genres, both written and spoken
modalities, and both formal and informal writing
with the aim of contrasting and comparing
computer-mediated and non-computer-mediated
genres as well as informal and formal genres.
</bodyText>
<table confidence="0.995499923076923">
Genre Com- Peer Mode Con Au-
puter- Give versa- dience
me- and tional
diated Take
Email yes no text yes ad-
dressee
Essay No no text no unspec
Inter- No no speech yes inter-
view viewer
Blog yes yes text no world
Chat yes yes text yes group
Dis- No yes speech yes group
cussion
</table>
<tableCaption confidence="0.999675">
Table 1. Genres
</tableCaption>
<bodyText confidence="0.985121666666667">
In order to ensure that the students could pro-
duce enough data, we chose six topics that were
controversial and politically and/or socially rele-
</bodyText>
<page confidence="0.997284">
337
</page>
<bodyText confidence="0.9655116875">
vant for college students from among whom the
subjects would be drawn. These six topics were
chosen from a pilot study consisting of twelve
topics, in which we analyzed the amount of in-
formation that people tended to “volunteer” on
the topics as well as their thoughts about being
able to write/speak on such a topic. The six top-
ics are listed in Table 2.
Topic Question
Church Do you feel the Catholic Church
needs to change its ways to adapt to
life in the 21st Century?
Gay Marriage While some states have legalized gay
marriage, others are still opposed to
it. Do you think either side is right or
wrong?
Privacy Rights Recently, school officials prevented a
school shooting because one of the
shooters posted a myspace bulletin.
Do you think this was an invasion of
privacy?
Legalization of The city of Denver has decided to
Marijuana legalize small amounts of marijuana
for persons over 21. How do you feel
about this?
War in Iraq The controversial war in Iraq has
made news headlines almost every
day since it began. How do you feel
about the war?
Gender Do you feel that gender discrimina-
Discrimination tion is still an issue in the present-day
United States?
</bodyText>
<tableCaption confidence="0.989659">
Table 2. Topics
</tableCaption>
<bodyText confidence="0.999990733333333">
The corpus was created in three phases
(Goldstein-Stewart, 2008). In Phase I, emails,
essays and interviews were collected. In Phase
II, blogs and chat and discussion groups were
created and samples collected. For blogs, sub-
jects blogged over a period of time and could
read and/or comment on other subjects’ blogs in
their own blog. A graduate research assistant
acted as interviewer and discussion and chat
group moderator.
Of the 24 subjects who completed Phase I, 7
decided not to continue into Phase II. Seven
additional students were recruited for Phase II.
In Phase III, these replacement students were
then asked to provide samples for the Phase I
genres. Four students fully complied, resulting
in a corpus with a full set of samples for 21
subjects, 11 women and 10 men.
All audio recordings, interviews and discus-
sions, were transcribed. Interviewer/moderator
comments were removed and, for each discus-
sion, four individual files, one for each partici-
pant’s contribution, were produced.
Our data is somewhat homogeneous: it sam-
ples only undergraduate university students and
was collected in controlled settings. But we be-
lieve that controlling the topics, genres, and de-
mographics of subjects allows the elimination of
many variables that effect communicative style
and aids the identification of common features.
</bodyText>
<sectionHeader confidence="0.890758" genericHeader="method">
3 Corpus Statistics
</sectionHeader>
<subsectionHeader confidence="0.998543">
3.1 Word Count
</subsectionHeader>
<bodyText confidence="0.999920833333333">
The mean word counts for the 21 students per
genre and per topic are shown in Figures 1 and 2,
respectively. Figure 1 shows that the students
produced more content in the directly interactive
genres – interview and discussion (the spoken
genres) as well as chat (a written genre).
</bodyText>
<figureCaption confidence="0.99997">
Figure 1. Mean word counts for gender and genre
Figure 2. Mean word counts for gender and topic
</figureCaption>
<page confidence="0.991962">
338
</page>
<bodyText confidence="0.984142333333333">
The email genre had the lowest mean word
count, perhaps indicating that it is a genre in-
tended for succinct messaging.
</bodyText>
<subsectionHeader confidence="0.999457">
3.2 Word Usage By Individuals
</subsectionHeader>
<bodyText confidence="0.9999385">
We performed an analysis of the word usage of
individuals. Among the top 20 most frequently
occurring words, the most frequent word used by
all males was “the”. For the 11 females, six most
frequently used “the”, four used “I”, and one
used “like”. Among abbreviations, 13 individu-
als used “lol”. Abbreviations were mainly used
in chat. Other abbreviations were used to vary-
ing degrees such as the abbreviation “u”. Emoti-
cons were used by five participants.
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="method">
4 Classification
</sectionHeader>
<subsectionHeader confidence="0.691229">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.97996447761194">
Frequencies of words in word categories were
determined using Linguistic Inquiry and Word
Count (LIWC). LIWC2001 analyzes text and
produces 88 output variables, among them word
count and average words per sentence. All oth-
ers are percentages, including percentage of
words that are parts of speech or belong to given
dictionaries (Pennebaker, et al., 2001). Default
dictionaries contain categories of words that in-
dicate basic emotional and cognitive dimensions
and were used here. LIWC was designed for
both text and speech and has categories, such
negations, numbers, social words, and emotion.
Refer to LIWC (www.liwc.net) for a full descrip-
tion of categories. Here the 88 LIWC features
are denoted feature set L.
From the original 24 participants’ documents
and the new 7 participants’ documents from
Phase II, we aggregated all samples from all ge-
nres and computed the top 100 words for males
and for females, including stop words. Six
words differed between males and females. Of
these top words, the 64 words with counts that
varied by 10% or more between male and female
usage were selected. Excluded from this list
were 5 words that appeared frequently but were
highly topic-specific: “catholic”, “church”, “ma-
rijuana”, “marriage”, and “school.”
Most of these words appeared on a large stop
word list (www.webconfs.com/stop-words.php).
Non-stop word terms included the word “feel”,
which was used more frequently by females than
males, as well as the terms “yea” and “lot” (used
more commonly by women) and “uh” (used
more commonly by men). Some stop words
were used more by males (“some”, “any”), oth-
ers by females (“I”, “and”). Since this set mainly
consists of stop words, we refer to it as the func-
tional word features or set F.
The third feature set (T) consisted of the five
topic specific words excluded from F.
The fourth feature set (S) consisted of the stop
word list of 659 words mentioned above.
The fifth feature set (I) we consider informal
features. It contains nine common words not in
set S: “feel”, “lot”, “uh”, “women”, “people”,
“men”, “gonna”, “yea” and “yeah”. This set also
contains the abbreviations and emotional expres-
sions “lol”, “ur”, “tru”, “wat”, and “haha”. Some
of the expressions could be characteristic of par-
ticular individuals. For example the term “wat”
was consistently used by one individual in the
informal chat genre.
Another feature set (E) was built around the
emoticons that appeared in the corpus. These
included “:)”, “:(”, “:-(”, “;)”, “:-/”, and “&gt;:o)”.
For our results, we use eight feature set com-
binations: 1. All 88 LIWC features (denoted L);
2. LIWC and functional word features, (L+F); 3.
LIWC plus all functional word features and the
topic words (L+F+T); 4. LIWC plus all function-
al word features and emoticons (L+F+E); 5.
LIWC plus all stop word features (L+S); 6.
LIWC plus all stop word and informal features
(L+S+I); 7. LIWC supplemented by informal,
topic, and stop word features, (L+S+I+T). Note
that, when combined, sets S and I cover set F.
</bodyText>
<subsectionHeader confidence="0.940095">
4.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.995955222222222">
Classification of all samples was performed us-
ing four classifiers of the Weka workbench, ver-
sion 3.5 (Witten and Frank, 2005). All were
used with default settings except the Random
Forest classifier (Breiman, 2001), which used
100 trees. We collected classification results for
Naïve-Bayes, J48 (decision tree), SMO (support
vector machine) (Cortes and Vapnik, 1995; Platt,
1998) and RF (Random Forests) methods.
</bodyText>
<sectionHeader confidence="0.994278" genericHeader="method">
5 Person Identification Results
</sectionHeader>
<subsectionHeader confidence="0.985485">
5.1 Cross Validation Across Genres
</subsectionHeader>
<bodyText confidence="0.999970857142857">
To identify a person as the author of a text, six
fold cross validation was used. All 756 samples
were divided into 126 “documents,” each con-
sisting of all six samples of a person’s expression
in a single genre, regardless of topic. There is a
baseline of approximately 5% accuracy if ran-
domly guessing the person. Table 3 shows the
</bodyText>
<page confidence="0.996232">
339
</page>
<bodyText confidence="0.999799588235294">
accuracy results of classification using combina-
tions of the feature sets and classifiers.
The results show that SMO is by far the best
classifier of the four and, thus, we used only this
classifier on subsequent experiments. L+S per-
formed better alone than when adding the infor-
mal features – a surprising result.
Table 4 shows a comparison of results using
feature sets L+F and L+F+T. The five topic
words appear to grant a benefit in the best trained
case (SMO).
Table 5 shows a comparison of results using
feature sets L+F and L+F+E, and this shows that
the inclusion of the individual emoticon features
does provide a benefit, which is interesting con-
sidering that these are relatively few and are typ-
ically concentrated in the chat documents.
</bodyText>
<table confidence="0.999850333333333">
Feature SMO RF100 J48 NB
L 52 30 15 17
L+F 60 44 21 25
L+S 71 42 19 33
L+S+I 71 39 17 33
L+S+I+T 71 40 17 33
</table>
<tableCaption confidence="0.98367">
Table 3. Person identification accuracy (%) using six
fold cross validation
</tableCaption>
<table confidence="0.999942333333333">
Feature SMO RF100 J48 NB
L+F 60 44 21 25
L+F+T 67 40 21 25
</table>
<tableCaption confidence="0.991871">
Table 4. Accuracy (%) using six fold cross validation
with and without topic word features (T)
</tableCaption>
<table confidence="0.999931333333333">
Feature SMO RF100 J48 NB
L+F 60 44 21 25
L+F+E 65 41 21 25
</table>
<tableCaption confidence="0.996367">
Table 5. Accuracy (%) using six fold cross validation
with and without emoticon features (E)
</tableCaption>
<subsectionHeader confidence="0.9317525">
5.2 Predict Communicant in One Genre
Given Information on Other Genres
</subsectionHeader>
<bodyText confidence="0.9998259">
The next set of experiments we performed was to
identify a person based on knowledge of the per-
son’s communication in other genres. We first
train on five genres, and we then test on one – a
“hold out” or test genre.
Again, as in six fold cross validation, a total of
126 “documents” were used: for each genre, 21
samples were constructed, each the concatena-
tion of all text produced by an individual in that
genre, across all topics. Table 6 shows the re-
sults of this experiment. The result of 100% for
L+F, L+F+T, and L+F+E in email was surpris-
ing, especially since the word counts for email
were the lowest. The lack of difference in L+F
and L+F+E results is not surprising since the
emoticon features appear only in chat docu-
ments, with one exception of a single emoticon
in a blog document (“:-/”), which did not appear
in any chat documents. So there was no emoti-
con feature that appeared across different genres.
</bodyText>
<table confidence="0.997815">
SMO HOLD OUT (TEST GENRE)
Features A B C D E S I
L 60 76 52 43 76 81 29
L+F 75 81 57 48 100 90 71
L+F+T 76 86 62 52 100 86 71
L+F+E 75 81 57 48 100 90 71
L+S 82 81 67 67 86 90 100
L+S+I 79 86 52 57 86 90 100
L+S+I+T 81 86 52 67 90 90 100
</table>
<tableCaption confidence="0.9772605">
Table 6. Person identification accuracy (%) training
with SMO on 5 genres and testing on 1. A=Average
over all genres, B=Blog, C=Chat, D=Discussion,
E=Email, S=Essay, I=Interview
</tableCaption>
<table confidence="0.999880166666667">
Train Test L+F L+F+T
CDSI Email 67 95
BDSI Email 71 52
BCSI Email 76 100
BCDI Email 57 90
BCDS Email 57 81
</table>
<tableCaption confidence="0.987814">
Table 7. Accuracy (%) using SMO for predicting
email author after training on 4 other genres. B=Blog,
C=Chat, D=Discussion, S=Essay, I=Interview
</tableCaption>
<bodyText confidence="0.999981260869565">
We attempted to determine which genres were
most influential in identifying email authorship,
by reducing the number of genres in its training
set. Results are reported in Table 7. The differ-
ence between the two sets, which differ only in
five topic specific word features, is more marked
here. The lack of these features causes accuracy
to drop far more rapidly as the training set is re-
duced. It also appears that the chat genre is im-
portant when identifying the email genre when
topical features are included. This is probably
not just due to the volume of data since discus-
sion groups also have a great deal of data. We
need to investigate further the reason for such a
high performance on the email genre.
The results in Table 6 are also interesting for
the case of L+S (which has more stop words than
L+F). With this feature set, classification for the
interview genre improved significantly, while
that of email decreased. This may indicate that
the set of stop words may be very genre specific
– a hypothesis we will test in future work. If this
in indeed the case, perhaps certain different sets
</bodyText>
<page confidence="0.99398">
340
</page>
<bodyText confidence="0.9995925">
of stop words may be important for identifying
certain genres, genders and individual author-
ship. Previous results indicate that the usage of
certain stop words as features assists with identi-
fying gender (Sabin, et al., 2008).
Table 6 also shows that, using the informal
words (feature set I) decreased performance in
two genres: chat (the genre in which the abbrevi-
ations are mostly used) and discussion. We plan
to run further experiments to investigate this.
The sections that follow will typically show the
results achieved with L+F and L+S features.
</bodyText>
<table confidence="0.998957857142857">
Train\Test B C D E S I
Blog 100 14 14 76 57 5
Chat 24 100 29 38 19 10
Discussion 10 5 100 5 10 29
Email 43 10 5 100 48 0
Essay 67 5 5 33 100 5
Interview 5 5 5 5 5 100
</table>
<tableCaption confidence="0.990237">
Table 8. Accuracy (%) using SMO for predicting per-
son between genres after training on one genre using
L+F features
</tableCaption>
<bodyText confidence="0.998530387096774">
Table 8 displays the accuracies when the L+F
feature set of single genre is used for training a
model tested on one genre. This generally sug-
gests the contribution of each genre when all are
used in training. When the training and testing
sets are the same, 100% accuracy is achieved.
Examining this chart, the highest accuracies are
achieved when training and test sets are textual.
Excluding models trained and tested on the same
genre, the average accuracy for training and test-
ing within written genres is 36% while the aver-
age accuracy for training and testing within spo-
ken genres is 17%. Even lower are average ac-
curacies of the models trained on spoken and
tested on textual genres (9%) and the models
trained on textual and tested on spoken genres
(6%). This indicates that the accuracies that fea-
ture the same mode (textual or spoken) in train-
ing and testing tend to be higher.
Of particular interest here is further examina-
tion of the surprising results of testing on email
with the L+F feature set. Of these tests, a model
trained on blogs achieved the highest score, per-
haps due to a greater stylistic similarity to email
than the other genres. This is also the highest
score in the chart apart from cases where train
and test genres were the same. Training on chat
and essay genres shows some improvement over
the baseline, but models trained with the two
spoken genres do not rise above baseline accura-
cy when tested on the textual email genre.
</bodyText>
<subsectionHeader confidence="0.7179835">
5.3 Predict Communicant in One Topic
Given Information on Five Topics
</subsectionHeader>
<bodyText confidence="0.9996586">
This set of experiments was designed to deter-
mine if there was no training data provided for a
certain topic, yet there were samples of commu-
nication for an individual across genres for other
topics, could an author be determined?
</bodyText>
<table confidence="0.997888166666667">
SMO HOLD OUT (TEST TOPIC)
Features Avg Ch Gay Iraq Mar Pri Sex
L+F 87 81 95 86 95 100 67
L+F+T 65 76 71 86 29 62 67
L+F+E 87 81 95 86 95 95 67
L+S 94 95 95 81 100 100 95
</table>
<tableCaption confidence="0.9718614">
Table 9. Person identification accuracy (%) training
with SMO on 5 topics and testing on 1. Avg = Aver-
age over all topics: Ch=Catholic Church, Gay=Gay
Marriage, Iraq=Iraq War, Mar=Marijuana Legaliza-
tion, Pri=Privacy Rights, Sex=Sex Discrimination
</tableCaption>
<bodyText confidence="0.999938363636364">
Again a total of 126 “documents” were used:
for each topic, 21 samples were constructed,
each the concatenation of all text produced by an
individual on that topic, across all genres. One
topic was withheld and 105 documents (on the
other 5 topics) were used for training. Table 9
shows that overall the L+S feature set performed
better than either the L+F or L+F+T sets. The
most noticeable differences are the drops in the
accuracy when the five topic words are added,
particularly on the topics of marijuana and priva-
cy rights. For L+F+T, if “marijuana” is withheld
from the topic word features when the marijuana
topic is the test set, the accuracy rises to 90%.
Similarly, if “school” is withheld from the topic
word features when the privacy rights topic is the
test set, the accuracy rises to 100%. This indi-
cates the topic words are detrimental to deter-
mining the communicant, and this appears to be
supported by the lack of an accuracy drop in the
testing on the Iraq and sexual discrimination top-
ics, both of which featured the fewest uses of the
five topic words. That the results rise when us-
ing the L+S features shows that more features
that are independent of the topic tend to help dis-
tinguish the person (as only the Iraq set expe-
rienced a small drop using these features in train-
ing and testing, while the others either increased
or remained the same). The similarity here of the
results using L+F features when compared to
L+F+E is likely due to the small number of emo-
ticons observed in the corpus (16 total exam-
ples).
</bodyText>
<page confidence="0.995565">
341
</page>
<note confidence="0.4142885">
5.4 Predict Communicant in a Speech Ge-
nre Given Information on the Other
</note>
<bodyText confidence="0.9986765">
One interesting experiment used one speech ge-
nre for training, and the other speech genre for
testing. The results (Table 10) show that the ad-
ditional stop words (S compared to F) make a
positive difference in both sets. We hypothesize
that the increased performance of training with
discussion data and testing on interview data is
due to the larger amount of training data availa-
ble in discussions. We will test this in future
work.
</bodyText>
<table confidence="0.999125333333333">
Train Test L+F L+S
Inter Disc 5 19
Disc Inter 29 48
</table>
<tableCaption confidence="0.98211">
Table 10. Person identification accuracy (%) training
and testing SMO on spoken genres
</tableCaption>
<table confidence="0.932835571428572">
5.5 Predict Authorship in a Textual Genre
Given Information on Speech Genres
Train Test L+F L+S
Disc+Inter Blog 19 24
Disc+Inter Chat 5 14
Disc+Inter Email 5 10
Disc+Inter Essay 10 29
</table>
<tableCaption confidence="0.9945115">
Table 11. Person identification accuracy (%) training
SMO on spoken genres and testing on textual genres
</tableCaption>
<bodyText confidence="0.984937">
Table 11 shows the results of training on speech
data only and predicting the author of the text
genre. Again, the speech genres alone do not do
well at determining the individual author of the
text genre. The best score was 29% for essays.
</bodyText>
<subsectionHeader confidence="0.989101666666667">
5.6 Predict Authorship in a Textual Genre
Given Information on Other Textual
Genres
</subsectionHeader>
<bodyText confidence="0.99990675">
Table 12 shows the results of training on text
data only and predicting authorship for one of the
four text genres. Recognizing the authors in chat
is the most difficult, which is not surprising since
the blogs, essays and emails are more similar to
each other than the chat genre, which uses ab-
breviations and more informal language as well
as being immediately interactive.
</bodyText>
<table confidence="0.9993852">
Train Test L+F L+S
C+E+S Blog 76 86
B+E+S Chat 10 19
B+C+S Email 90 81
B+C+E Essay 90 86
</table>
<tableCaption confidence="0.9982645">
Table 12. Person identification accuracy (%) train-
ing and testing SMO on textual genres
</tableCaption>
<note confidence="0.412729">
5.7 Predict Communicant in a Speech Ge-
nre Given Information on Textual Ge-
nres
</note>
<bodyText confidence="0.999203833333333">
Training on text and classifying speech-based
samples by author showed poor results. Similar
to the results for speech genres, using the text
genres alone to determine the individual in the
speech genre results in a maximum score of 29%
for the interview genre (Table 13).
</bodyText>
<table confidence="0.999049">
Train Test L+F L+S
B+C+E+S Discussion 14 23
B+C+E+S Interview 14 29
</table>
<tableCaption confidence="0.991151">
Table 13. Person identification accuracy (%) training
SMO on textual genres and testing on speech genres
</tableCaption>
<subsectionHeader confidence="0.595345">
5.8 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999927515151515">
Results for different training and test sets vary
considerably. A key factor in determining which
sets can successfully be used to train other sets
seems to be the mode, that is, whether or not a
set is textual or spoken, as the lowest accuracies
tend to be found between genres of different
modes. This suggests that how people write and
how they speak may be somewhat distinct.
Typically, more data samples in the training
tends to increase the accuracy of the tests, but
more features does not guarantee the same result.
An examination of the feature sets revealed fur-
ther explanations for this apart from any inherent
difficulties in recognizing authors between sets.
For many tests, there is a tendency for the same
person to be chosen for classification, indicating
a bias to that person in the training data. This is
typically caused by features that have mostly, but
not all, zero values in training samples, but have
many non-zero values in testing. The most
striking examples of this are described in 5.3,
where the removal of certain topic-related
features was found to dramatically increase the
accruacy. Targetted removal of other features
that have the same biasing effect could increase
accuracy.
While Weka normalizes the incoming features
for SMO, it was also discovered that a simple
initial normalization of the feature sets by
dividing by the maximum or standardization by
subtracting the mean and dividing by the
standard deviation of the feature sets could
increase the accuracy across the different tests.
</bodyText>
<sectionHeader confidence="0.999156" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.967325">
In this paper, we have described a novel unique
corpus consisting of samples of communication
</bodyText>
<page confidence="0.99388">
342
</page>
<bodyText confidence="0.999955578947369">
of 21 individuals in six genres across six topics
as well as experiments conducted to identify a
person’s samples within the corpus. We have
shown that we can identify individuals with rea-
sonably high accuracy for several cases: (1)
when we have samples of their communication
across genres (71%), (2) when we have samples
of their communication in specific genres other
than the one being tested (81%), and (3) when
they are communicating on a new topic (94%).
For predicting a person’s communication in
one text genre using other text genres only, we
were able to achieve a good accuracy for all
genres (above 76%) except chat. We believe this
is because chat, due to its “real-time
communication” nature is quite different from
the other text genres of emails, essays and blogs.
Identifying a person in one speech genre after
training with the other speech genre had lower
accuracies (less than 48%). Since these results
differed significantly, we hypothesize this is due
to the amount of data available for training – a
hypothesis we plan to test in the future.
Future plans also include further investigation
of some of the suprising results mentioned in this
paper as well investigation of stop word lists
particular to communicative genres. We also
plan to investigate if it is easier to identify those
participants who have produced more data
(higher total word count) as well as perform a
systematic study the effects of the number of
words gathered on person identificaton.
İn addition, we plan to investigate the efficacy
of using other features besides those available in
LIWC, stopwords and emoticons in person
identification. These include spelling errors,
readability measures, complexity measures,
suffixes, and content analysis measures.
</bodyText>
<sectionHeader confidence="0.998612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992796969697">
Naomi S. Baron. 2003. Why email looks like speech.
In J. Aitchison and D. M. Lewis, editors, New Me-
dia Language. Routledge, London, UK.
Douglas Biber. 1988. Variation across speech and
writing. Cambridge University Press, Cambridge,
UK.
Leo Breiman. 2001. Random forests. Technical Re-
port for Version 3, University of California, Berke-
ley, CA.
Rafael A. Calvo, Jae-Moon Lee, and Xiaobo Li. 2004.
Managing content with automatic document classi-
fication. Journal of Digital Information, 5(2).
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20(3):273-
297.
Nikolas Coupland, Justine Coupland, Howard Giles,
and Karen L. Henwood. 1988. Accommodating the
elderly: Invoking and extending a theory, Lan-
guage in Society, 17(1):1-41.
David Crystal. 2001. Language and the Internet.
Cambridge University Press, Cambridge, UK.
Olivier de Vel, Alison Anderson, Malcolm Corney,
George Mohay. 2001. Mining e-mail content for
author identification forensics, In SIGMOD: Spe-
cial Section on Data Mining for Intrusion Detec-
tion and Threat Analysis.
Jade Goldstein-Stewart, Gary Ciany, and Jaime Car-
bonell. 2007. Genre identification and goal-focused
summarization, In Proceedings of the ACM 16th
Conference on Information and Knowledge Man-
agement (CIKM) 2007, pages 889-892.
Jade Goldstein-Stewart, Kerri A. Goodwin, Roberta
E. Sabin, and Ransom K. Winder. 2008. Creating
and using a correlated corpora to glean communic-
ative commonalities. In LREC2008 Proceedings,
Marrakech, Morocco.
Susan Herring. 2001. Gender and power in online
communication. Center for Social Informatics,
Working Paper, WP-01-05.
Susan Herring. 1996. Two variants of an electronic
message schema. In Susan Herring, editor, Com-
puter-Mediated Communication: Linguistic, Social
and Cross-Cultural Perspectives. John Benjamins,
Amsterdam, pages 81-106.
Shawndra Hill and Foster Provost. 2003. The myth of
the double-blind review? Author identification us-
ing only citations. SIGKDD Explorations.
5(2):179-184.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing written
texts by author gender. Literary and Linguistic
Computation. 17(4):401-412.
Ivan Krsul and Eugene H. Spafford. 1997. Author-
ship analysis: Identifying the author of a program.
Computers and Security 16(3):233-257.
Shlomo Levitan and Shlomo Argamon. 2006. Fixing
the federalist: correcting results and evaluating edi-
tions for automated attribution. In Digital Humani-
ties, pages 323-328, Paris.
LIWC, Linguistic Inquiry and Word Count.
http://www.liwc.net/
David Madigan, Alexander Genkin, David Lewis,
Shlomo Argamon, Dmitriy Fradkin, and Li Ye.
2005. Author identification on the large scale.
Proc. of the Meeting of the Classification Society
of North America.
</reference>
<page confidence="0.990935">
343
</page>
<reference confidence="0.999671347826087">
Philip M. McCarthy, Gwyneth A. Lewis, David F.
Dufty, and Danielle S. McNamara. 2006. Analyz-
ing writing styles with Coh-Metrix, In Proceedings
of AI Research Society International Conference
(FLAIRS), pages 764-769.
Niamh McCombe. 2002. Methods of author identifi-
cation, Final Year Project, Trinity College, Ireland.
Thomas C. Mendenhall. 1887. The characteristic
curves of composition. Science, 9(214):237-249.
Frederick Mosteller and David L. Wallace. 1964.
Inference and Disputed Authorship: The Federal-
ist. Addison-Wesley, Boston.
James W. Pennebaker, Martha E. Francis, and Roger
J. Booth. 2001. Linguistic Inquiry and Word Count
(LIWC): LIWC2001. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ.
John C. Platt. 1998. Using sparseness and analytic QP
to speed training of support vector machines. In M.
S. Kearns, S. A. Solla, and D. A. Cohn, editors,
Advances in Neural Information Processing Sys-
tems 11. MIT Press, Cambridge, Mass.
Roberta E. Sabin, Kerri A. Goodwin, Jade Goldstein-
Stewart, and Joseph A. Pereira. 2008. Gender dif-
ferences across correlated corpora: preliminary re-
sults. FLAIRS Conference 2008, Florida, pages
207-212.
Marina Santini. 2007. Automatic Identification of
Genre in Web Pages. Ph.D., thesis, University of
Brighton, Brighton, UK.
Michael Shepherd and Carolyn Watters. 1999. The
functionality attribute of cybergenres. In Proceed-
ings of the 32nd Hawaii International Conf. on
System Sciences (HICSS1999), Maui, HI.
Rob Thomson and Tamar Murachver. 2001. Predict-
ing gender from electronic discourse. British Jour-
nal of Social Psychology. 40(2):193-208.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques
(Second Edition). Morgan Kaufmann, San Francis-
co, CA.
Simeon J. Yates. 1996. Oral and written linguistic
aspects of computer conferencing: a corpus based
study. In Susan Herring, editor, Computer-
mediated Communication: Linguistic, Social, and
Cross-Cultural Perspectives. John Benjamins,
Amsterdam, pages 29-46.
</reference>
<page confidence="0.99899">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959073">
<title confidence="0.996526">Person Identification from Text and Speech Genre Samples</title>
<author confidence="0.988949">Jade Goldstein-Stewart Ransom Winder Roberta Evans Sabin</author>
<affiliation confidence="0.999878">U.S. Department of Defense The MITRE Corporation Loyola University</affiliation>
<address confidence="0.999933">Hanover, MD, USA Baltimore, MD, USA</address>
<email confidence="0.995857">jadeg@acm.orgrwinder@mitre.orgres@loyola.edu</email>
<abstract confidence="0.998914909090909">In this paper, we describe experiments conducted on identifying a person using a novel unique correlated corpus of text and audio samples of the person’s communication in six genres. The text samples include essays, emails, blogs, and chat. Audio samples were collected from individual interviews and group discussions and then transcribed to text. For each genre, samples were collected for six topics. We show that we can identify the communicant with an accuracy of 71% for six fold cross validation using an average of 22,000 words per individual across the six genres. For person identification in a particular genre (train on five genres, test on one), an average accuracy of 82% is achieved. For identification from topics (train on five topics, test on one), an average accuracy of 94% is achieved. We also report results on identifying a percommunication in a genre using text nres only as well as audio genres only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naomi S Baron</author>
</authors>
<title>Why email looks like speech.</title>
<date>2003</date>
<editor>In J. Aitchison and D. M. Lewis, editors,</editor>
<location>New Media Language. Routledge, London, UK.</location>
<contexts>
<context position="3495" citStr="Baron, 2003" startWordPosition="539" endWordPosition="540">evitan and Argamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, e</context>
</contexts>
<marker>Baron, 2003</marker>
<rawString>Naomi S. Baron. 2003. Why email looks like speech. In J. Aitchison and D. M. Lewis, editors, New Media Language. Routledge, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Variation across speech and writing.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3143" citStr="Biber (1988)" startWordPosition="484" endWordPosition="485">s, often combined using multivariable statistical techniques. Recently, McCarthy (2006) was able to differentiate three authors’ works, and Hill and Provost (2003), using a feature of co-citations, showed that they could successfully identify scientific articles by the same person, achieving 85% accuracy when the person has authored over 100 papers. Levitan and Argamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of th</context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Douglas Biber. 1988. Variation across speech and writing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<tech>Technical Report for Version 3,</tech>
<institution>University of California,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="15100" citStr="Breiman, 2001" startWordPosition="2434" endWordPosition="2435"> features, (L+F); 3. LIWC plus all functional word features and the topic words (L+F+T); 4. LIWC plus all functional word features and emoticons (L+F+E); 5. LIWC plus all stop word features (L+S); 6. LIWC plus all stop word and informal features (L+S+I); 7. LIWC supplemented by informal, topic, and stop word features, (L+S+I+T). Note that, when combined, sets S and I cover set F. 4.2 Classifiers Classification of all samples was performed using four classifiers of the Weka workbench, version 3.5 (Witten and Frank, 2005). All were used with default settings except the Random Forest classifier (Breiman, 2001), which used 100 trees. We collected classification results for Naïve-Bayes, J48 (decision tree), SMO (support vector machine) (Cortes and Vapnik, 1995; Platt, 1998) and RF (Random Forests) methods. 5 Person Identification Results 5.1 Cross Validation Across Genres To identify a person as the author of a text, six fold cross validation was used. All 756 samples were divided into 126 “documents,” each consisting of all six samples of a person’s expression in a single genre, regardless of topic. There is a baseline of approximately 5% accuracy if randomly guessing the person. Table 3 shows the 3</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Technical Report for Version 3, University of California, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael A Calvo</author>
<author>Jae-Moon Lee</author>
<author>Xiaobo Li</author>
</authors>
<title>Managing content with automatic document classification.</title>
<date>2004</date>
<journal>Journal of Digital Information,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="4417" citStr="Calvo, et al., 2004" startWordPosition="683" endWordPosition="686">pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the efficiency of communication in informal genres – items that one would not find in a formal essay. Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same perso</context>
</contexts>
<marker>Calvo, Lee, Li, 2004</marker>
<rawString>Rafael A. Calvo, Jae-Moon Lee, and Xiaobo Li. 2004. Managing content with automatic document classification. Journal of Digital Information, 5(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="15251" citStr="Cortes and Vapnik, 1995" startWordPosition="2453" endWordPosition="2456">ns (L+F+E); 5. LIWC plus all stop word features (L+S); 6. LIWC plus all stop word and informal features (L+S+I); 7. LIWC supplemented by informal, topic, and stop word features, (L+S+I+T). Note that, when combined, sets S and I cover set F. 4.2 Classifiers Classification of all samples was performed using four classifiers of the Weka workbench, version 3.5 (Witten and Frank, 2005). All were used with default settings except the Random Forest classifier (Breiman, 2001), which used 100 trees. We collected classification results for Naïve-Bayes, J48 (decision tree), SMO (support vector machine) (Cortes and Vapnik, 1995; Platt, 1998) and RF (Random Forests) methods. 5 Person Identification Results 5.1 Cross Validation Across Genres To identify a person as the author of a text, six fold cross validation was used. All 756 samples were divided into 126 “documents,” each consisting of all six samples of a person’s expression in a single genre, regardless of topic. There is a baseline of approximately 5% accuracy if randomly guessing the person. Table 3 shows the 339 accuracy results of classification using combinations of the feature sets and classifiers. The results show that SMO is by far the best classifier o</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Support vector networks. Machine Learning, 20(3):273-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolas Coupland</author>
<author>Justine Coupland</author>
<author>Howard Giles</author>
<author>Karen L Henwood</author>
</authors>
<title>Accommodating the elderly: Invoking and extending a theory, Language in Society,</title>
<date>1988</date>
<pages>17--1</pages>
<contexts>
<context position="4107" citStr="Coupland, et al., 1988" startWordPosition="631" endWordPosition="634">Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the efficiency of communication in informal genres – items that one would not find in a formal essay. Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular gen</context>
</contexts>
<marker>Coupland, Coupland, Giles, Henwood, 1988</marker>
<rawString>Nikolas Coupland, Justine Coupland, Howard Giles, and Karen L. Henwood. 1988. Accommodating the elderly: Invoking and extending a theory, Language in Society, 17(1):1-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>Language and the Internet.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3510" citStr="Crystal, 2001" startWordPosition="541" endWordPosition="542">gamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). S</context>
</contexts>
<marker>Crystal, 2001</marker>
<rawString>David Crystal. 2001. Language and the Internet. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier de Vel</author>
<author>Alison Anderson</author>
<author>Malcolm Corney</author>
<author>George Mohay</author>
</authors>
<title>Mining e-mail content for author identification forensics, In SIGMOD: Special Section on Data Mining for Intrusion Detection and Threat Analysis.</title>
<date>2001</date>
<marker>de Vel, Anderson, Corney, Mohay, 2001</marker>
<rawString>Olivier de Vel, Alison Anderson, Malcolm Corney, George Mohay. 2001. Mining e-mail content for author identification forensics, In SIGMOD: Special Section on Data Mining for Intrusion Detection and Threat Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein-Stewart</author>
<author>Gary Ciany</author>
<author>Jaime Carbonell</author>
</authors>
<title>Genre identification and goal-focused summarization,</title>
<date>2007</date>
<booktitle>In Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM)</booktitle>
<pages>889--892</pages>
<contexts>
<context position="4474" citStr="Goldstein-Stewart, et al., 2007" startWordPosition="690" endWordPosition="693">ril 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the efficiency of communication in informal genres – items that one would not find in a formal essay. Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person. The best system (Hill and Provost, 2003) was able to i</context>
</contexts>
<marker>Goldstein-Stewart, Ciany, Carbonell, 2007</marker>
<rawString>Jade Goldstein-Stewart, Gary Ciany, and Jaime Carbonell. 2007. Genre identification and goal-focused summarization, In Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM) 2007, pages 889-892.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein-Stewart</author>
<author>Kerri A Goodwin</author>
<author>Roberta E Sabin</author>
<author>Ransom K Winder</author>
</authors>
<title>Creating and using a correlated corpora to glean communicative commonalities.</title>
<date>2008</date>
<booktitle>In LREC2008 Proceedings,</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Goldstein-Stewart, Goodwin, Sabin, Winder, 2008</marker>
<rawString>Jade Goldstein-Stewart, Kerri A. Goodwin, Roberta E. Sabin, and Ransom K. Winder. 2008. Creating and using a correlated corpora to glean communicative commonalities. In LREC2008 Proceedings, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Herring</author>
</authors>
<title>Gender and power in online communication. Center for Social Informatics, Working Paper,</title>
<date>2001</date>
<pages>01--05</pages>
<contexts>
<context position="3525" citStr="Herring, 2001" startWordPosition="543" endWordPosition="544">d McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use </context>
</contexts>
<marker>Herring, 2001</marker>
<rawString>Susan Herring. 2001. Gender and power in online communication. Center for Social Informatics, Working Paper, WP-01-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Herring</author>
</authors>
<title>Two variants of an electronic message schema.</title>
<date>1996</date>
<pages>81--106</pages>
<editor>In Susan Herring, editor, Computer-Mediated</editor>
<location>Amsterdam,</location>
<marker>Herring, 1996</marker>
<rawString>Susan Herring. 1996. Two variants of an electronic message schema. In Susan Herring, editor, Computer-Mediated Communication: Linguistic, Social and Cross-Cultural Perspectives. John Benjamins, Amsterdam, pages 81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shawndra Hill</author>
<author>Foster Provost</author>
</authors>
<title>The myth of the double-blind review? Author identification using only citations.</title>
<date>2003</date>
<journal>SIGKDD Explorations.</journal>
<pages>5--2</pages>
<contexts>
<context position="2694" citStr="Hill and Provost (2003)" startWordPosition="416" endWordPosition="420">ctronic computing. The idea that features such as average word length and average sentence length could allow an author to be identified dates to Mendenhall (1887). Mosteller and Wallace (1964) used function words in a groundbreaking study that identified authors of The Federalist Papers. Since then many attempts at authorship attribution have used function words and other features, such as word class frequencies and measures derived from syntactic analysis, often combined using multivariable statistical techniques. Recently, McCarthy (2006) was able to differentiate three authors’ works, and Hill and Provost (2003), using a feature of co-citations, showed that they could successfully identify scientific articles by the same person, achieving 85% accuracy when the person has authored over 100 papers. Levitan and Argamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify wri</context>
<context position="5060" citStr="Hill and Provost, 2003" startWordPosition="776" endWordPosition="779">tion (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person. The best system (Hill and Provost, 2003) was able to identify successfully scientific articles by the same person 45% of the time; for authors with over 100 papers, 85% accuracy was achieved. Are there common features of communication of an individual across and within genres? Undoubtedly, the lack of corpora has been an impediment to answering this question, as gathering personal communication samples faces considerable privacy and accessibility hurdles. To our knowledge, all previous studies have focused on individual communications in one or possibly two genres. To analyze, compare, and contrast the communication of individuals a</context>
</contexts>
<marker>Hill, Provost, 2003</marker>
<rawString>Shawndra Hill and Foster Provost. 2003. The myth of the double-blind review? Author identification using only citations. SIGKDD Explorations. 5(2):179-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
</authors>
<title>Shlomo Argamon, and Anat Rachel Shimoni.</title>
<date>2002</date>
<pages>17--4</pages>
<marker>Koppel, 2002</marker>
<rawString>Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2002. Automatically categorizing written texts by author gender. Literary and Linguistic Computation. 17(4):401-412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Krsul</author>
<author>Eugene H Spafford</author>
</authors>
<title>Authorship analysis: Identifying the author of a program.</title>
<date>1997</date>
<journal>Computers and Security</journal>
<pages>16--3</pages>
<contexts>
<context position="4849" citStr="Krsul and Spafford, 1997" startWordPosition="743" endWordPosition="746">find in a formal essay. Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person. The best system (Hill and Provost, 2003) was able to identify successfully scientific articles by the same person 45% of the time; for authors with over 100 papers, 85% accuracy was achieved. Are there common features of communication of an individual across and within genres? Undoubtedly, the lack of corpora has been an impediment to answering this question, as gathering personal communication samples faces considerable priv</context>
</contexts>
<marker>Krsul, Spafford, 1997</marker>
<rawString>Ivan Krsul and Eugene H. Spafford. 1997. Authorship analysis: Identifying the author of a program. Computers and Security 16(3):233-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Levitan</author>
<author>Shlomo Argamon</author>
</authors>
<title>Fixing the federalist: correcting results and evaluating editions for automated attribution.</title>
<date>2006</date>
<booktitle>In Digital Humanities,</booktitle>
<pages>323--328</pages>
<location>Paris.</location>
<contexts>
<context position="2909" citStr="Levitan and Argamon (2006)" startWordPosition="449" endWordPosition="452">s in a groundbreaking study that identified authors of The Federalist Papers. Since then many attempts at authorship attribution have used function words and other features, such as word class frequencies and measures derived from syntactic analysis, often combined using multivariable statistical techniques. Recently, McCarthy (2006) was able to differentiate three authors’ works, and Hill and Provost (2003), using a feature of co-citations, showed that they could successfully identify scientific articles by the same person, achieving 85% accuracy when the person has authored over 100 papers. Levitan and Argamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 200</context>
</contexts>
<marker>Levitan, Argamon, 2006</marker>
<rawString>Shlomo Levitan and Shlomo Argamon. 2006. Fixing the federalist: correcting results and evaluating editions for automated attribution. In Digital Humanities, pages 323-328, Paris.</rawString>
</citation>
<citation valid="false">
<authors>
<author>LIWC</author>
</authors>
<title>Linguistic Inquiry and Word Count.</title>
<note>http://www.liwc.net/</note>
<marker>LIWC, </marker>
<rawString>LIWC, Linguistic Inquiry and Word Count. http://www.liwc.net/</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Madigan</author>
<author>Alexander Genkin</author>
<author>David Lewis</author>
<author>Shlomo Argamon</author>
<author>Dmitriy Fradkin</author>
<author>Li Ye</author>
</authors>
<title>Author identification on the large scale.</title>
<date>2005</date>
<booktitle>Proc. of the Meeting of the Classification Society of</booktitle>
<publisher>North America.</publisher>
<contexts>
<context position="4872" citStr="Madigan, et al., 2005" startWordPosition="747" endWordPosition="750">formal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person. The best system (Hill and Provost, 2003) was able to identify successfully scientific articles by the same person 45% of the time; for authors with over 100 papers, 85% accuracy was achieved. Are there common features of communication of an individual across and within genres? Undoubtedly, the lack of corpora has been an impediment to answering this question, as gathering personal communication samples faces considerable privacy and accessibility h</context>
</contexts>
<marker>Madigan, Genkin, Lewis, Argamon, Fradkin, Ye, 2005</marker>
<rawString>David Madigan, Alexander Genkin, David Lewis, Shlomo Argamon, Dmitriy Fradkin, and Li Ye. 2005. Author identification on the large scale. Proc. of the Meeting of the Classification Society of North America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M McCarthy</author>
<author>Gwyneth A Lewis</author>
<author>David F Dufty</author>
<author>Danielle S McNamara</author>
</authors>
<title>Analyzing writing styles with Coh-Metrix,</title>
<date>2006</date>
<booktitle>In Proceedings of AI Research Society International Conference (FLAIRS),</booktitle>
<pages>764--769</pages>
<marker>McCarthy, Lewis, Dufty, McNamara, 2006</marker>
<rawString>Philip M. McCarthy, Gwyneth A. Lewis, David F. Dufty, and Danielle S. McNamara. 2006. Analyzing writing styles with Coh-Metrix, In Proceedings of AI Research Society International Conference (FLAIRS), pages 764-769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niamh McCombe</author>
</authors>
<title>Methods of author identification, Final Year Project,</title>
<date>2002</date>
<location>Trinity College, Ireland.</location>
<contexts>
<context position="2928" citStr="McCombe (2002)" startWordPosition="454" endWordPosition="455">t identified authors of The Federalist Papers. Since then many attempts at authorship attribution have used function words and other features, such as word class frequencies and measures derived from syntactic analysis, often combined using multivariable statistical techniques. Recently, McCarthy (2006) was able to differentiate three authors’ works, and Hill and Provost (2003), using a feature of co-citations, showed that they could successfully identify scientific articles by the same person, achieving 85% accuracy when the person has authored over 100 papers. Levitan and Argamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; S</context>
<context position="4888" citStr="McCombe, 2002" startWordPosition="751" endWordPosition="752"> contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person. The best system (Hill and Provost, 2003) was able to identify successfully scientific articles by the same person 45% of the time; for authors with over 100 papers, 85% accuracy was achieved. Are there common features of communication of an individual across and within genres? Undoubtedly, the lack of corpora has been an impediment to answering this question, as gathering personal communication samples faces considerable privacy and accessibility hurdles. To our k</context>
</contexts>
<marker>McCombe, 2002</marker>
<rawString>Niamh McCombe. 2002. Methods of author identification, Final Year Project, Trinity College, Ireland.</rawString>
</citation>
<citation valid="false">
<title>The characteristic curves of composition.</title>
<journal>Science,</journal>
<pages>9--214</pages>
<marker></marker>
<rawString>Thomas C. Mendenhall. 1887. The characteristic curves of composition. Science, 9(214):237-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley,</publisher>
<location>Boston.</location>
<contexts>
<context position="2264" citStr="Mosteller and Wallace (1964)" startWordPosition="354" endWordPosition="357">ommunicate via verbal conversations. Other forms of communication could include blogs or even formal writings such as essays or scientific articles. People communicating in these different “genres” may have different stylistic patterns and we are interested in whether or not we could identify people from their communications in different genres. The attempt to identify authorship of written text has a long history that predates electronic computing. The idea that features such as average word length and average sentence length could allow an author to be identified dates to Mendenhall (1887). Mosteller and Wallace (1964) used function words in a groundbreaking study that identified authors of The Federalist Papers. Since then many attempts at authorship attribution have used function words and other features, such as word class frequencies and measures derived from syntactic analysis, often combined using multivariable statistical techniques. Recently, McCarthy (2006) was able to differentiate three authors’ works, and Hill and Provost (2003), using a feature of co-citations, showed that they could successfully identify scientific articles by the same person, achieving 85% accuracy when the person has authore</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1964. Inference and Disputed Authorship: The Federalist. Addison-Wesley, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
<author>Roger J Booth</author>
</authors>
<date>2001</date>
<booktitle>Linguistic Inquiry and Word Count (LIWC): LIWC2001. Lawrence Erlbaum Associates,</booktitle>
<location>Mahwah, NJ.</location>
<contexts>
<context position="12211" citStr="Pennebaker, et al., 2001" startWordPosition="1955" endWordPosition="1958">sed “I”, and one used “like”. Among abbreviations, 13 individuals used “lol”. Abbreviations were mainly used in chat. Other abbreviations were used to varying degrees such as the abbreviation “u”. Emoticons were used by five participants. 4 Classification 4.1 Features Frequencies of words in word categories were determined using Linguistic Inquiry and Word Count (LIWC). LIWC2001 analyzes text and produces 88 output variables, among them word count and average words per sentence. All others are percentages, including percentage of words that are parts of speech or belong to given dictionaries (Pennebaker, et al., 2001). Default dictionaries contain categories of words that indicate basic emotional and cognitive dimensions and were used here. LIWC was designed for both text and speech and has categories, such negations, numbers, social words, and emotion. Refer to LIWC (www.liwc.net) for a full description of categories. Here the 88 LIWC features are denoted feature set L. From the original 24 participants’ documents and the new 7 participants’ documents from Phase II, we aggregated all samples from all genres and computed the top 100 words for males and for females, including stop words. Six words differed </context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>James W. Pennebaker, Martha E. Francis, and Roger J. Booth. 2001. Linguistic Inquiry and Word Count (LIWC): LIWC2001. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Using sparseness and analytic QP to speed training of support vector machines.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processing Systems 11.</booktitle>
<editor>In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="15265" citStr="Platt, 1998" startWordPosition="2457" endWordPosition="2458">all stop word features (L+S); 6. LIWC plus all stop word and informal features (L+S+I); 7. LIWC supplemented by informal, topic, and stop word features, (L+S+I+T). Note that, when combined, sets S and I cover set F. 4.2 Classifiers Classification of all samples was performed using four classifiers of the Weka workbench, version 3.5 (Witten and Frank, 2005). All were used with default settings except the Random Forest classifier (Breiman, 2001), which used 100 trees. We collected classification results for Naïve-Bayes, J48 (decision tree), SMO (support vector machine) (Cortes and Vapnik, 1995; Platt, 1998) and RF (Random Forests) methods. 5 Person Identification Results 5.1 Cross Validation Across Genres To identify a person as the author of a text, six fold cross validation was used. All 756 samples were divided into 126 “documents,” each consisting of all six samples of a person’s expression in a single genre, regardless of topic. There is a baseline of approximately 5% accuracy if randomly guessing the person. Table 3 shows the 339 accuracy results of classification using combinations of the feature sets and classifiers. The results show that SMO is by far the best classifier of the four and</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John C. Platt. 1998. Using sparseness and analytic QP to speed training of support vector machines. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberta E Sabin</author>
<author>Kerri A Goodwin</author>
<author>Jade GoldsteinStewart</author>
<author>Joseph A Pereira</author>
</authors>
<title>Gender differences across correlated corpora: preliminary results.</title>
<date>2008</date>
<booktitle>FLAIRS Conference</booktitle>
<pages>207--212</pages>
<location>Florida,</location>
<contexts>
<context position="19938" citStr="Sabin, et al., 2008" startWordPosition="3310" endWordPosition="3313">results in Table 6 are also interesting for the case of L+S (which has more stop words than L+F). With this feature set, classification for the interview genre improved significantly, while that of email decreased. This may indicate that the set of stop words may be very genre specific – a hypothesis we will test in future work. If this in indeed the case, perhaps certain different sets 340 of stop words may be important for identifying certain genres, genders and individual authorship. Previous results indicate that the usage of certain stop words as features assists with identifying gender (Sabin, et al., 2008). Table 6 also shows that, using the informal words (feature set I) decreased performance in two genres: chat (the genre in which the abbreviations are mostly used) and discussion. We plan to run further experiments to investigate this. The sections that follow will typically show the results achieved with L+F and L+S features. Train\Test B C D E S I Blog 100 14 14 76 57 5 Chat 24 100 29 38 19 10 Discussion 10 5 100 5 10 29 Email 43 10 5 100 48 0 Essay 67 5 5 33 100 5 Interview 5 5 5 5 5 100 Table 8. Accuracy (%) using SMO for predicting person between genres after training on one genre using </context>
</contexts>
<marker>Sabin, Goodwin, GoldsteinStewart, Pereira, 2008</marker>
<rawString>Roberta E. Sabin, Kerri A. Goodwin, Jade GoldsteinStewart, and Joseph A. Pereira. 2008. Gender differences across correlated corpora: preliminary results. FLAIRS Conference 2008, Florida, pages 207-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Santini</author>
</authors>
<title>Automatic Identification of Genre in Web Pages.</title>
<date>2007</date>
<tech>Ph.D., thesis,</tech>
<institution>University of Brighton,</institution>
<location>Brighton, UK.</location>
<contexts>
<context position="4489" citStr="Santini 2007" startWordPosition="694" endWordPosition="695">Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the efficiency of communication in informal genres – items that one would not find in a formal essay. Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002). In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person. The best system (Hill and Provost, 2003) was able to identify success</context>
</contexts>
<marker>Santini, 2007</marker>
<rawString>Marina Santini. 2007. Automatic Identification of Genre in Web Pages. Ph.D., thesis, University of Brighton, Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Shepherd</author>
<author>Carolyn Watters</author>
</authors>
<title>The functionality attribute of cybergenres.</title>
<date>1999</date>
<booktitle>In Proceedings of the 32nd Hawaii International Conf. on System Sciences (HICSS1999),</booktitle>
<location>Maui, HI.</location>
<contexts>
<context position="3553" citStr="Shepherd and Watters, 1999" startWordPosition="545" endWordPosition="548">) further investigated authorship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the ef</context>
</contexts>
<marker>Shepherd, Watters, 1999</marker>
<rawString>Michael Shepherd and Carolyn Watters. 1999. The functionality attribute of cybergenres. In Proceedings of the 32nd Hawaii International Conf. on System Sciences (HICSS1999), Maui, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Thomson</author>
<author>Tamar Murachver</author>
</authors>
<title>Predicting gender from electronic discourse.</title>
<date>2001</date>
<journal>British Journal of Social Psychology.</journal>
<pages>40--2</pages>
<contexts>
<context position="4082" citStr="Thomson and Murachver, 2001" startWordPosition="627" endWordPosition="630">ic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the efficiency of communication in informal genres – items that one would not find in a formal essay. Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood. Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007). Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002). In authorship identification, recent research has focused on identifying author</context>
</contexts>
<marker>Thomson, Murachver, 2001</marker>
<rawString>Rob Thomson and Tamar Murachver. 2001. Predicting gender from electronic discourse. British Journal of Social Psychology. 40(2):193-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques (Second Edition).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="15011" citStr="Witten and Frank, 2005" startWordPosition="2419" endWordPosition="2422">e eight feature set combinations: 1. All 88 LIWC features (denoted L); 2. LIWC and functional word features, (L+F); 3. LIWC plus all functional word features and the topic words (L+F+T); 4. LIWC plus all functional word features and emoticons (L+F+E); 5. LIWC plus all stop word features (L+S); 6. LIWC plus all stop word and informal features (L+S+I); 7. LIWC supplemented by informal, topic, and stop word features, (L+S+I+T). Note that, when combined, sets S and I cover set F. 4.2 Classifiers Classification of all samples was performed using four classifiers of the Weka workbench, version 3.5 (Witten and Frank, 2005). All were used with default settings except the Random Forest classifier (Breiman, 2001), which used 100 trees. We collected classification results for Naïve-Bayes, J48 (decision tree), SMO (support vector machine) (Cortes and Vapnik, 1995; Platt, 1998) and RF (Random Forests) methods. 5 Person Identification Results 5.1 Cross Validation Across Genres To identify a person as the author of a text, six fold cross validation was used. All 756 samples were divided into 126 “documents,” each consisting of all six samples of a person’s expression in a single genre, regardless of topic. There is a b</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques (Second Edition). Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simeon J Yates</author>
</authors>
<title>Oral and written linguistic aspects of computer conferencing: a corpus based study.</title>
<date>1996</date>
<pages>29--46</pages>
<editor>In Susan Herring, editor, Computermediated Communication: Linguistic, Social, and</editor>
<location>Amsterdam,</location>
<contexts>
<context position="3567" citStr="Yates, 1996" startWordPosition="549" endWordPosition="550">rship identification of The Federalist Papers (three authors). The genre of the text may affect the authorship identification task. The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres. He determined six factors that could be used to identify written text. Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging. Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996). The task is complicated by the great diversity that can be exhibited within even a single genre. Email can be business-related, personal, or spam; the style Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336–344, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 336 can be tremendously affected by demographic factors, including gender and age of the sender. The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988). Some people use abbreviations to ease the efficiency of co</context>
</contexts>
<marker>Yates, 1996</marker>
<rawString>Simeon J. Yates. 1996. Oral and written linguistic aspects of computer conferencing: a corpus based study. In Susan Herring, editor, Computermediated Communication: Linguistic, Social, and Cross-Cultural Perspectives. John Benjamins, Amsterdam, pages 29-46.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>