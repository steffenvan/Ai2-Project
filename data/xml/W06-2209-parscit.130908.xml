<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004264">
<title confidence="0.951586">
Active Annotation
</title>
<author confidence="0.635924">
Andreas Vlachos
</author>
<affiliation confidence="0.723513333333333">
William Gates Building
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.997621">
av308@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999686111111111">
This paper introduces a semi-supervised
learning framework for creating training
material, namely active annotation. The
main intuition is that an unsupervised
method is used to initially annotate imper-
fectly the data and then the errors made
are detected automatically and corrected
by a human annotator. We applied ac-
tive annotation to named entity recogni-
tion in the biomedical domain and encour-
aging results were obtained. The main
advantages over the popular active learn-
ing framework are that no seed annotated
data is needed and that the reusability of
the data is maintained. In addition to the
framework, an efficient uncertainty esti-
mation for Hidden Markov Models is pre-
sented.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920577777778">
Training material is always an issue when applying
machine learning to deal with information extrac-
tion tasks. It is generally accepted that increasing
the amount of training data used improves perfor-
mance. However, training material comes at a cost,
since it requires annotation.
As a consequence, when adapting existing meth-
ods and techniques to a new domain, researchers and
users are faced with the problem of absence of an-
notated material that could be used for training. A
good example is the biomedical domain, which has
attracted the attention of the NLP community rel-
atively recently (Kim et al., 2004). Even though
there are plenty of biomedical texts, very little of it
is annotated, such as the GENIA corpus (Kim et al.,
2003).
A very popular and well investigated framework
in order to cope with the lack of training mate-
rial is the active learning framework (Cohn et al.,
1995; Seung et al., 1992). It has been applied
to various NLP/IE tasks, including named entity
recognition (Shen et al., 2004) and parse selec-
tion (Baldridge and Osborne, 2004) with rather im-
pressive results in reducing the amount of anno-
tated training data. However, some criticism of ac-
tive learning has been expressed recently, concern-
ing the reusability of the data (Baldridge and Os-
borne, 2004).
This paper presents a framework in order to deal
with the lack of training data for NLP tasks. The
intuition behind it is that annotated training data is
produced by applying an (imperfect) unsupervised
method, and then the errors inserted in the annota-
tion are detected automatically and reannotated by
a human annotator. The main difference compared
to active learning is that instead of selecting unla-
beled instances for annotation, possible erroneous
instances are selected for checking and correction
if they are indeed erroneous. We will refer to this
framework as “active annotation” in the rest of the
paper.
The structure of this paper is as follows. In Sec-
tion 2 we describe the software and the dataset used.
Section 3 explores the effect of errors in the training
data and motivates the active annotation framework.
</bodyText>
<page confidence="0.998822">
64
</page>
<bodyText confidence="0.999648428571429">
In Section 4 we describe the framework in detail,
while Section 5 presents a method for estimating un-
certainty for HMMs. Section 6 presents results from
applying the active annotation. Section 7 compares
the proposed framework to active learning and Sec-
tion 8 attempts an analysis of its performance. Fi-
nally, Section 9 suggests some future work.
</bodyText>
<sectionHeader confidence="0.996269" genericHeader="introduction">
2 Experimental setup
</sectionHeader>
<bodyText confidence="0.97732876">
The data used in the experiments that follow are
taken from the BioNLP 2004 named entity recog-
nition shared task (Kim et al., 2004). The text pas-
sages have been annotated with five classes of en-
tities, “DNA”, “RNA”, “protein”, “cell type” and
“cell line”. In our experiments, following the ex-
ample of Dingare et al. (2004), we simplified the an-
notation to one entity class, namely “gene”, which
includes the DNA, RNA and protein classes. In or-
der to evaluate the performance on the task, we used
the evaluation script supplied with the data, which
computes the F-score (F1 = 2∗Precision∗Recall for
Precision+Recall )
each entity class. It must be noted that all tokens
of an entity must be recognized correctly in order to
count as a correct prediction. A partially recognized
entity counts both as a precision and recall error. In
all the experiments that follow, the official split of
the data in training and testing was maintained.
The named entity recognition system used in our
experiments is the open source NLP toolkit Ling-
pipe&apos;. The named entity recognition module is
an HMM model using Witten-Bell smoothing. In
our experiments, using the data mentioned earlier it
achieved 70.06% F-score.
</bodyText>
<sectionHeader confidence="0.872519" genericHeader="method">
3 Effect of errors
</sectionHeader>
<bodyText confidence="0.999786363636364">
Noise in the training data is a common issue in train-
ing machine learning for NLP tasks. It can have sig-
nificant effect on the performance, as it was pointed
out by Dingare et al. (2004), where the performance
of the same system on the same task (named entity
recognition in the biomedical domain) was lower
when using noisier material. The effect of noise in
the data used to train machine learning algorithms
for NLP tasks has been explored by Osborne (2002),
using the task of shallow parsing as the case study
and a variety of learners. The impact of different
</bodyText>
<footnote confidence="0.941158">
1http://www.alias-i.com/lingpipe/
</footnote>
<bodyText confidence="0.9980696">
types of noise was explored and learner specific ex-
tensions were proposed in order to deal with it.
In our experiments we explored the effect of noise
in training the selected named entity recognition sys-
tem, keeping in mind that we are going to use an
unsupervised method to create the training material.
The kind of noise we expect is mislabelled instances.
In order to simulate the behavior of a hypothetical
unsupervised method, we corrupted the training data
artificially using the following models:
</bodyText>
<listItem confidence="0.972547071428571">
• LowRecall: Change tokens labelled as entities
to non-entities. It must be noted that in this
model, due to the presence of multi-token en-
tities precision is reduced too, albeit less than
recall.
• LowRecall WholeEntities: Change the label-
ing of whole entities to non-entities. In this
model, precision is kept intact.
• LowPrecision: Change tokens labelled as non-
entities to entities.
• Random: Entities and non-entities are changed
randomly. It can be viewed alternatively as a
random tagger which labels the data with some
accuracy.
</listItem>
<bodyText confidence="0.999806">
The level of noise inserted is adjusted by specify-
ing the probability with which a candidate label is
changed. In all the experiments in this paper, for a
particular model and level of noise, the corruption
of the dataset was repeated five times in order to
produce more reliable results. In practice, the be-
havior of an unsupervised method is likely to be a
mixture of the above models. However, given that
the method would tag the data with a certain per-
formance, we attempted through our experiments to
identify which of these (extreme) behaviors would
be less harmful. In Figure 1, we present graphs
showing the effect of noise inserted with the above
models. The experimental procedure was to add
noise to the training data according to a model, eval-
uate the performance of the hypothetical tagger that
produced it, train Lingpipe on the noisy training data
and evaluate the performance of the latter on the test
data. The process was repeated for various levels
of noise. In the top graph, the F-score achieved by
Lingpipe (F-ling) is plotted against the F-score of
</bodyText>
<page confidence="0.999273">
65
</page>
<bodyText confidence="0.9998575">
the hypothetical tagger (F-tag), while in the bottom
graph the F-score achieved by Lingpipe (F-ling) is
plotted against the number of erroneous classifica-
tions made by the hypothetical tagger.
</bodyText>
<figure confidence="0.9972545">
0 0.2 0.4 0.6 0.8 1
F-tag
0 50 100 150 200 250 300 350 400 450 500
raw errors (in thousands)
</figure>
<figureCaption confidence="0.929844">
Figure 1: F-score achieved by Lingpipe is plotted
against (a) the F-score of the hypothetical tagger in
the top graph and (b) the number of errors made by
the hypothetical tagger in the bottom graph.
</figureCaption>
<bodyText confidence="0.999880555555556">
A first observation is that limited noise does not
affect the performance significantly, a phenomenon
that can be attributed to the capacity of the machine
learning method to deal with noise. From the point
of view of correcting mistakes in the training data
this suggests that not all mistakes need to be cor-
rected. Another observation is that while the perfor-
mance for all the models follow similar curves when
plotted against the F-score of the hypothetical tag-
ger, the same does not hold when plotted against the
number of errors. While this can be attributed to the
unbalanced nature of the task (very few entity to-
kens compared to non-entities), it also suggests that
the raw number of errors in the training data is not
a good indicator for the performance obtained by
training on it. However, it represents the effort re-
quired to obtain the maximum performance from the
data by correcting it.
</bodyText>
<sectionHeader confidence="0.990819" genericHeader="method">
4 Active Annotation
</sectionHeader>
<bodyText confidence="0.96411625">
In this section we present a detailed description of
the active annotation framework. Initially, we have
a pool of unlabeled data, D, whose instances are an-
notated using an unsupervised method u, which does
not need training data. As expected, a significant
amount of errors is inserted during this process. A
list L is created containing the tokens that have not
been checked by a human annotator. Then, a super-
vised learner s is used to train a model M on this
noisy training material. A query module q, which
uses the model created by s decides which instances
of D will be selected to be checked for errors by
a human annotator. The selected instances are re-
moved from L so that q does not select them again
in future. The learner s is then trained on this par-
tially corrected training data and the sequence is re-
peated from the point of applying the querying mod-
ule q. The algorithm written in pseudocode appears
in Figure 2.
Data D, unsupervised tagger u,
supervised learner s, query module q.
Initialization:
Apply u to D.
Create list of instances L.
Loop:
Using s train a model M on D.
Using q and M select a batch of instances B
to be checked.
Correct the instances of B in D.
Remove the instances of B from L.
Repeat until:
L is empty or annotator stops.
</bodyText>
<figureCaption confidence="0.995006">
Figure 2: Active annotation algorithm
</figureCaption>
<bodyText confidence="0.999897727272727">
Comparing it with active learning, the similarities
are apparent. Both frameworks have a loop in which
a query module q, using a model produced by the
learner, selects instances to be presented to a human
annotator. The efficiency of active annotation can be
measured in two ways, both of them used in evalu-
ating active learning. The first is to measure the re-
duction in the checked instances needed in order to
achieve a certain level of performance. The second
is the increase in performance for a fixed number
of checked instances. Following the active learning
</bodyText>
<figure confidence="0.998589107142857">
F-ling
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
random
lowrecall
wholeentities
lowprecision
F-ling
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
random
lowrecall
wholeentities
lowprecision
</figure>
<page confidence="0.67458">
66
</page>
<bodyText confidence="0.999968743243244">
paradigm, a baseline for active annotation is random
selection of instances to be checked.
There are though some notable differences. Dur-
ing initialization, an unsupervised method u is re-
quired to provide an initial tagging on the data D.
This is an important restriction which is imposed
by the lack of any annotated data. Even under this
restriction, there are some options available, espe-
cially for tasks which have compiled resources. One
option is to use an unsupervised learning algorithm,
such the one presented by Collins &amp; Singer (1999),
where a seed set of rules is used to bootstrap a rule-
based named entity recognizer. A different approach
could be the use of a dictionary-based tagger, as in
Morgan et al. (2003). It must be noted that the unsu-
pervised method used to provide the initial tagging
does not need to generalize to any data (a common
problem for such methods), it only needs to perform
well on the data used during active annotation. Gen-
eralization on unseen data is an attribute we hope
that the supervised learning method s will have af-
ter training on the annotated material created with
active annotation.
The query module q is also different from the cor-
responding module in active learning. Instead of se-
lecting unlabeled informative instances to be anno-
tated and added to the training data, its purpose is
to identify likely errors in the imperfectly labelled
training data, so that they are checked and corrected
by the human annotator.
In order to perform error-detection, we chose
to adapt the approach of Nakagawa and Mat-
sumoto (2002) which resembles uncertainty based
sampling for active learning. According to their
paradigm, likely errors in the training data are in-
stances that are “hard” for the classifier and incon-
sistent with the rest of the data. In our case, we used
the uncertainty of the classifier as the measure of the
“hardness” of an instance. As an indication of in-
consistency, we used the disagreement of the label
assigned by the classifier with the current label of the
instance. Intuitively, if the classifier disagrees with
the label of an instance used in its training, it indi-
cates that there have been other similar instances in
the training data that were labelled differently. Re-
turning to the description of active annotation, the
query module q ranks the instances in L first by their
inconsistency and then by decreasing uncertainty of
the classifier. As a result, instances that are inconsis-
tent with the rest of the data and hard for the classi-
fier are selected first, then those that are inconsistent
but easy for the classifier, then the consistent ones
but hard for the classifier and finally the consistent
and easy ones.
While this method of detecting errors resembles
uncertainty sampling, there are other approaches
that could have been used instead and they can be
very different. Sj¨obergh and Knutsson (2005) in-
serted artificial errors and trained a classifier to rec-
ognize them. Dickinson and Meuers (2003) pro-
posed methods based on n-grams occurring with dif-
ferent labellings in the corpus. Therefore, while it is
reasonable to expect some correlation between the
selections of active annotation and active learning
(hard instances are likely to be erroneously anno-
tated by the unsupervised tagger), the task of select-
ing hard instances is quite different from detecting
errors. The use of the disagreement between tag-
gers for selecting candidates for manual correction
is reminiscent of corrected co-training (Pierce and
Cardie, 2001). However, the main difference is cor-
rected co-training results in a manually annotated
corpus, while active annotation allows automatically
annotated instances to be kept.
</bodyText>
<sectionHeader confidence="0.976791" genericHeader="method">
5 HMM uncertainty estimation
</sectionHeader>
<bodyText confidence="0.999754833333333">
In order to perform error detection according to the
previous section we need to obtain uncertainty es-
timations over each token from the named entity
recognition module of Lingpipe. For each token t
and possible label l, Lingpipe estimates the follow-
ing Hidden Markov Model from the training data:
</bodyText>
<equation confidence="0.897408">
P(t[n],l[n]|l[n − 1], t[n − 1], t[n − 2]) (1)
</equation>
<bodyText confidence="0.99975">
When annotating a certain text passage, the tokens
are fixed and the joint probability of Equation 1 is
computed for each possible combination of labels.
From Bayes’ rule, we obtain:
</bodyText>
<equation confidence="0.995282333333333">
P(l[n]|t[n],l[n − 1],t[n − 1],t[n − 2]) =
P(t[n],l[n]|l[n − 1], t[n − 1], t[n − 2]) (2)
P(t[n]|l[n − 1], t[n − 1], t[n − 2])
</equation>
<bodyText confidence="0.9688085">
For fixed token sequence t[n], t[n − 1], t[n − 2]
and previous label (l[n − 1]) the second term of the
</bodyText>
<page confidence="0.997543">
67
</page>
<bodyText confidence="0.994984">
left part of Equation 2 is a fixed value. Therefore,
under these conditions, we can write:
</bodyText>
<equation confidence="0.996364">
P(l[n]|t[n],l[n − 1],t[n − 1],t[n − 2]) a
P(t[n], l[n]|l[n − 1],t[n − 1],t[n − 2]) (3)
</equation>
<bodyText confidence="0.999720461538462">
From Equation 3 we obtain an approximation for
the conditional distribution of the current label (l[n])
conditioned on the previous label (l[n − 1]) for a
fixed sequence of tokens. It must be stressed that
the later restriction is very important. The result-
ing distribution from Equation 3 cannot be com-
pared across different token sequences. However,
for the purpose of computing the uncertainty over
a fixed token sequence it is a reasonable approxi-
mation. One way to estimate the uncertainty of the
classifier is to calculate the conditional entropy of
this distribution. The conditional entropy for a dis-
tribution P(X|Y ) can be computed as:
</bodyText>
<equation confidence="0.931055666666667">
H[X|Y ] = � P (Y = y) � logP(X = x|Y = y)
y x
(4)
</equation>
<bodyText confidence="0.991883774193548">
In our case, X is l[n] and Y is l[n − 1]. Func-
tion 4 can be interpreted as the weighted sum of
the entropies of P(l[n]|l[n − 1]) for each value
of l[n − 1], in our case the weighted sum of en-
tropies of the distribution of the current label for
each possible previous label. The probabilities for
each tag (needed for P(l[n − 1])) are not calcu-
lated directly from the model. P(l[n]) corresponds
to P(l[n]|t[n],t[n − 1],t[n − 2]), but since we are
considering a fixed token sequence, we approxi-
mate its distribution using the conditional proba-
bility P(l[n]|t[n], l[n − 1],t[n − 1],t[n − 2]), by
marginalizing over l[n − 1].
Again, it must be noted that the above calculations
are to be used in estimating uncertainty over a single
word. One property of the conditional entropy is that
it estimates the uncertainty of the predictions for the
current label given knowledge of the previous tag,
which is important in our application because we
need the uncertainty over each label independently
from the rest of the sequence. This is confirmed by
the theory, from which we know that for a condi-
tional distribution of X given Y the following equa-
tion holds, H[X|Y ] = H[X, Y ] − H[Y ], where H
denotes the entropy.
A different way of obtaining uncertainty estima-
tions from HMMs in the framework of active learn-
ing has been presented in (Scheffer et al., 2001).
There, the uncertainty is estimated by the margin
between the two most likely predictions that would
result in a different current label, explicitly:
</bodyText>
<equation confidence="0.994583">
M = maxi,j {P(t[n] = i|t[n − 1] = j)} −
maxk,l,k�i {P(t[n] = k|t[n − 1] = l)} (5)
</equation>
<bodyText confidence="0.9999705">
Intuitively, the margin M is the difference be-
tween the two highest scored predictions that dis-
agree. The lower the margin, the higher the uncer-
tainty of the HMM on the token at question. A draw-
back of this method is that it doesn’t take into ac-
count the distribution of the previous label. It is pos-
sible that the two highest scored predictions are ob-
tained for two different previous labels. It may also
be the case that a highly scored label can be obtained
given a very improbable previous label. Finally, an
alternative that we did not explore in this work is
the Field Confidence Estimation (Culotta and Mc-
Callum, 2004), which allows the estimation of con-
fidence over sequences of tokens, instead of single-
ton tokens only. However, in this work confidence
estimation over singleton tokens is sufficient.
</bodyText>
<sectionHeader confidence="0.997631" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999972842105263">
In this section we present results from applying ac-
tive annotation to biomedical named entity recogni-
tion. Using the noise models described in Section 3,
we corrupted the training data and then using Ling-
pipe as the supervised learner we applied the algo-
rithm of Figure 2. The batch of tokens selected to be
checked in each round was 2000 tokens. As a base-
line for comparison we used random selection of to-
kens to be checked. The results for various noise
models and levels are presented in the graphs of Fig-
ure 3. In each of these graphs, the performance of
Lingpipe trained on the partially corrected material
(F-ling) is plotted against the number of checked in-
stances, under the label “entropy”.
In all the experiments, active annotation signifi-
cantly outperformed random selection, with the ex-
ception of 50% Random, where the high level of
noise (the F-score of the hypothetical tagger that
provided the initial data was 0.1) affected the initial
</bodyText>
<page confidence="0.995349">
68
</page>
<figure confidence="0.999855691489362">
0.72
0.7
0.68
F-ling
0.66
0.64
0.62
0.6
0.58
0.56
0.75
0.7
0.65
F-ling
0.6
0.55
0.5
0.45
0.4
0.35
0.75
0.7
0.65
F-ling
0.6
0.55
0.5
0.45
0.8
0.7
0.6
F-ling
0.5
0.4
0.3
0.2
0.1
0.705
0.7
0.695
F-ling
0.69
0.685
0.68
0.675
Random 10%
random
margin
entropy
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
LowRecall 50%
random
margin
entropy
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
LowPrecision 20%
random
uncertainty
entropy
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
LowRecall_WholeEntities 20%
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
LowRecall 20%
random
uncertainty
entropy
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
Random 50%
random
uncertainty
entropy
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
random
margin
entropy
0.71
0.7
0.69
0.68
F-ling
0.67
0.66
0.65
0.64
0.63
0.62
0.61
0.6
</figure>
<figureCaption confidence="0.987691333333333">
Figure 3: F-score achieved by Lingpipe is plotted
against the number of checked instances for various
models and levels of noise.
</figureCaption>
<bodyText confidence="0.99958308">
judgements of the query module on which instances
should be checked. After having checked some por-
tion of the dataset though, active annotation started
outperforming random selection. In the graphs for
the 10% Random, 20% LowRecall WholeEntities
and 50% LowRecall noise models, under the la-
bel “margin”, appear the performance curves ob-
tained using the uncertainty estimation of Scheffer
et al. (2001). Even though active annotation us-
ing this method performs better than random se-
lection, active annotation using conditional entropy
performs significantly better. These results provide
evidence of the theoretical advantages of conditional
entropy described earlier. We also ran experiments
using pure uncertainty based sampling (i.e. with-
out checking the consistency of the labels) on se-
lecting instances to be checked. The performance
curves appear under the label “uncertainty” for the
20% LowRecall, 50% Random and 20% LowPreci-
sion noise models. The uncertainty was estimated
using the method described in Section 5. As ex-
pected, uncertainty based sampling performed rea-
sonably well, better than random selection but worse
than using labelling consistency, except for the ini-
tial stage of 20% LowPrecision.
</bodyText>
<note confidence="0.557193">
7 Active Annotation versus Active
Learning
</note>
<bodyText confidence="0.99805835">
In order to compare active annotation to active learn-
ing, we run active learning experiments using the
same dataset and software. The paradigm employed
was uncertainty based sampling, using the uncer-
tainty estimation presented in Sec. 5. HMMs require
annotated sequences of tokens, therefore annotating
whole sentences seemed as the natural choice, as
in (Becker et al., 2005). While token-level selec-
tions could be used in combination with EM, (as in
(Scheffer et al., 2001)), constructing a corpus of in-
dividual tokens would result in a corpus that would
be very difficult to be reused, since it would be par-
tially labelled. We employed the two standard op-
tions of selecting sentences, selecting the sentences
with the highest average uncertainty over the tokens
or selecting the sentence containing the most uncer-
tain token. As cost metric we used the number of
tokens, which allows more straightforward compar-
ison with active annotation.
In Figure 4 (left graph), each active learning ex-
periment is started by selecting a random sentence as
seed data, repeating the seed selection 5 times. The
random selection is repeated 5 times for each seed
selection. As in (Becker et al., 2005), selecting the
sentences with the highest average uncertainty (ave)
performs better than selecting those with the most
uncertain token (max).
In the right graph, we compare the best active
learning method with active annotation. Apparently,
the performance of active annotation is highly de-
pendent on the performance of the unsupervised tag-
ger used to provide us with the initial annotation of
the data. In the graph, we include curves for two
of the noise models reported in the previous sec-
tion, LowRecall20% and LowRecall50% which cor-
respond to tagging performance of 0.66 / 0.69 / 0.67
and 0.33 / 0.43 / 0.37 respectively, in terms of Re-
call / Precision / F. We consider such tagging per-
formances feasible with a dictionary-based tagger,
since Morgan et al. (2003) report performance of
</bodyText>
<figure confidence="0.962171333333333">
Active learning
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
AA vs AL
0 75 150 225 300 375 450 525
tokens_checked (in thousands)
</figure>
<figureCaption confidence="0.939712333333333">
Figure 4: Left, comparison among various active
learning methods. Right, comparison of active
learning and active annotation.
</figureCaption>
<bodyText confidence="0.999103111111111">
0.88 / 0/78 / 83 with such a method.
These results demonstrate that active annotation,
given a reasonable starting point, can achieve reduc-
tions in the annotation cost comparable to those of
active learning. Furthermore, active annotation pro-
duces an actual corpus, albeit noisy. Active learn-
ing, as pointed out by Baldridge &amp; Osborne (2004),
while it reduces the amount of training material
needed, it selects data that might not be useful to
train a different learner. In the active annotation
framework, it is likely to preserve correct instances
that might not be useful to the machine learning
method used to create it, but maybe beneficial to a
different method. Furthermore, producing an actual
corpus can be very important when adding new fea-
tures to the model. In the case of biomedical NER,
one could consider adding document-level features,
such as whether a token has been seen as part of a
gene name earlier in the document. With the cor-
pus constructed using active learning this is not fea-
sible, since it is unlikely that all the sentences of a
document are selected for annotation. Also, if one
intended to use the same corpus for a different task,
such as anaphora resolution, again the imperfectly
annotated corpus constructed using active annota-
tion can be used more efficiently than the partially
annotated one produced by active learning.
</bodyText>
<sectionHeader confidence="0.893642" genericHeader="method">
8 Selecting errors
</sectionHeader>
<bodyText confidence="0.97883294117647">
In order to investigate further the behavior of ac-
tive annotation, we evaluated the performance of the
trained supervised method against the number of er-
rors corrected by the human annotator. The aim of
this experiment was to verify whether the improve-
ment in performance compared to random selection
is due to selecting “informative” errors to correct, or
due to the efficiency of the error detection technique.
Figure 5: Left: F-score achieved by Lingpipe
is plotted against the number of corrected errors.
Right: Errors corrected plotted against the number
of checked tokens.
In Figure 5, we present such graphs for the 10%
Random noise model. Similar results were obtained
with different noise models. As can be observed on
the left graph, the errors corrected initially during
random selection are far more informative compared
to those corrected at the early stages of active anno-
tation (labelled “entropy”). The explanation for this
is that using the error detection method described in
Section 4, the errors that are detected are those on
which the supervised method s disagrees with the
training material, which implies that even if such an
instance is indeed an error then it didn’t affect s.
Therefore, correcting such errors will not improve
the performance significantly. Informative errors are
those that s has learnt to reproduce with high cer-
tainty. However, such errors are hard to detect be-
cause similar attributes are exhibited usually by cor-
rectly labelled instances. This can be verified by the
curves labelled “reverse” in the graphs of Figure 5,
in which the ranking of the instances to be selected
was reversed, so that instances where the supervised
method agrees confidently with the training material
</bodyText>
<figure confidence="0.998858303571428">
F-ling
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
ave
max
random
F-ling
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
AA-20%
AA-50%
AL-ave
0 5 10 15 20 25 30 35 40 45 50
errors corrected (in thousands)
random
entropy
reverse
0 75 150 225 300 375 450 525
tokens checked (in thousands)
0.72
0.7
0.68
0.66
F-ling
0.64
0.62
0.6
random
entropy
reverse
0.58
0.56
50
45
40
35
30
25
20
15
10
5
0
errors corrected (in thousands)
</figure>
<page confidence="0.985635">
70
</page>
<bodyText confidence="0.9999843">
are selected first. The fact that errors with high un-
certainty are less informative than those with low un-
certainty suggests that active annotation, while be-
ing related to active learning, it is sufficiently differ-
ent. The right graph suggests that the error-detection
performance during active annotation is much better
than that of random selection. Therefore, the per-
formance of active annotation could be improved by
preserving the high error-detection performance and
selecting more informative errors.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="discussions">
9 Future work
</sectionHeader>
<bodyText confidence="0.999985545454545">
This paper described active annotation, a semi-
supervised learning framework that reduces the ef-
fort needed to create training material, which is
very important in adapting existing trainable meth-
ods to new domains. Future work should investi-
gate the applicability of the framework in a variety
of NLP/IE tasks and settings. We intend to apply this
framework to NER for biomedical literature from
the FlyBase project for which no annotated datasets
exist.
While we have used the number of instances
checked by a human annotator to measure the cost
of annotation, this might not be representative of the
actual cost. The task of checking and possibly cor-
recting instances differs from annotating them from
scratch. In this direction, experiments in realistic
conditions with human annotators should be carried
out. We also intend to explore the possibility of
grouping similar mistakes detected in a round of ac-
tive annotation, so that the human annotator can cor-
rect them with less effort. Finally, alternative error-
detection methods should be investigated.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999774333333333">
The author was funded by BBSRC, grant number
38688. I would like to thank Ted Briscoe and Bob
Carpenter for their feedback and comments.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916075471698">
J. Baldridge and M. Osborne. 2004. Active learning and
the total cost of annotation. In Proceedings of EMNLP
2004, Barcelona, Spain.
M. Becker, B. Hachey, B. Alex, and C. Grover.
2005. Optimising selective sampling for bootstrap-
ping named entity recognition. In Proceedings of the
Workshop on Learning with Multiple Views, ICML.
D. A. Cohn, Z. Ghahramani, and M. I. Jordan. 1995.
Active learning with statistical models. In Advances
in Neural Information Processing Systems, volume 7.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC.
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In Proceedings
of HLT 2004, Boston, MA.
M. Dickinson and W. D. Meurers. 2003. Detecting errors
in part-of-speech annotation. In Proceedings of EACL
2003, pages 107–114, Budapest, Hungary.
S. Dingare, J. Finkel, M. Nissim, C. Manning, and
C. Grover. 2004. A system for identifying named en-
tities in biomedical text: How results from two evalua-
tions reflect on both the system and the evaluations. In
The 2004 BioLink meeting at ISMB.
J. D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. In ISMB (Supplement of Bioinformatics).
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,
editors. 2004. Proceedings of JNLPBA, Geneva.
A. Morgan, L. Hirschman, A. Yeh, and M. Colosimo.
2003. Gene name extraction using FlyBase resources.
In Proceedings of the ACL 2003 Workshop on NLP in
Biomedicine, pages 1–8.
T. Nakagawa and Y. Matsumoto. 2002. Detecting errors
in corpora using support vector machines. In Proceed-
ings of COLING 2002.
M. Osborne. 2002. Shallow parsing using noisy and
non-stationary training material. J. Mach. Learn. Res.,
2:695–719.
D. Pierce and C. Cardie. 2001. Limitations of co-training
for natural language learning from large datasets. In
Proceedings of EMNLP 2001, pages 1–9.
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extraction.
Lecture Notes in Computer Science, 2189:309+.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of COLT 1992.
D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 2004.
Multi-criteria-based active learning for named entity
recongition. In Proceedings ofACL 2004, Barcelona.
J. Sj¨obergh and O. Knutsson. 2005. Faking errors to
avoid making errors: Machine learning for error de-
tection in writing. In Proceedings of RANLP 2005.
</reference>
<page confidence="0.999145">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.437101">
<title confidence="0.999894">Active Annotation</title>
<author confidence="0.778607">Andreas William Gates</author>
<affiliation confidence="0.9888425">Computer University of</affiliation>
<email confidence="0.954724">av308@cl.cam.ac.uk</email>
<abstract confidence="0.991309894736842">This paper introduces a semi-supervised learning framework for creating training material, namely active annotation. The main intuition is that an unsupervised method is used to initially annotate imperfectly the data and then the errors made are detected automatically and corrected by a human annotator. We applied active annotation to named entity recognition in the biomedical domain and encouraging results were obtained. The main advantages over the popular active learning framework are that no seed annotated data is needed and that the reusability of the data is maintained. In addition to the framework, an efficient uncertainty estimation for Hidden Markov Models is presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>M Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1895" citStr="Baldridge and Osborne, 2004" startWordPosition="299" endWordPosition="302"> material that could be used for training. A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed recently, concerning the reusability of the data (Baldridge and Osborne, 2004). This paper presents a framework in order to deal with the lack of training data for NLP tasks. The intuition behind it is that annotated training data is produced by applying an (imperfect) unsupervised method, and then the errors inserted in the annotation are detected automatically and reannotated by a human annotator. The main difference compared to active learning is t</context>
<context position="24281" citStr="Baldridge &amp; Osborne (2004)" startWordPosition="4091" endWordPosition="4094"> performance of Active learning 0 75 150 225 300 375 450 525 tokens_checked (in thousands) AA vs AL 0 75 150 225 300 375 450 525 tokens_checked (in thousands) Figure 4: Left, comparison among various active learning methods. Right, comparison of active learning and active annotation. 0.88 / 0/78 / 83 with such a method. These results demonstrate that active annotation, given a reasonable starting point, can achieve reductions in the annotation cost comparable to those of active learning. Furthermore, active annotation produces an actual corpus, albeit noisy. Active learning, as pointed out by Baldridge &amp; Osborne (2004), while it reduces the amount of training material needed, it selects data that might not be useful to train a different learner. In the active annotation framework, it is likely to preserve correct instances that might not be useful to the machine learning method used to create it, but maybe beneficial to a different method. Furthermore, producing an actual corpus can be very important when adding new features to the model. In the case of biomedical NER, one could consider adding document-level features, such as whether a token has been seen as part of a gene name earlier in the document. Wit</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>J. Baldridge and M. Osborne. 2004. Active learning and the total cost of annotation. In Proceedings of EMNLP 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Becker</author>
<author>B Hachey</author>
<author>B Alex</author>
<author>C Grover</author>
</authors>
<title>Optimising selective sampling for bootstrapping named entity recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Learning with Multiple Views, ICML.</booktitle>
<contexts>
<context position="22068" citStr="Becker et al., 2005" startWordPosition="3725" endWordPosition="3728">Section 5. As expected, uncertainty based sampling performed reasonably well, better than random selection but worse than using labelling consistency, except for the initial stage of 20% LowPrecision. 7 Active Annotation versus Active Learning In order to compare active annotation to active learning, we run active learning experiments using the same dataset and software. The paradigm employed was uncertainty based sampling, using the uncertainty estimation presented in Sec. 5. HMMs require annotated sequences of tokens, therefore annotating whole sentences seemed as the natural choice, as in (Becker et al., 2005). While token-level selections could be used in combination with EM, (as in (Scheffer et al., 2001)), constructing a corpus of individual tokens would result in a corpus that would be very difficult to be reused, since it would be partially labelled. We employed the two standard options of selecting sentences, selecting the sentences with the highest average uncertainty over the tokens or selecting the sentence containing the most uncertain token. As cost metric we used the number of tokens, which allows more straightforward comparison with active annotation. In Figure 4 (left graph), each act</context>
</contexts>
<marker>Becker, Hachey, Alex, Grover, 2005</marker>
<rawString>M. Becker, B. Hachey, B. Alex, and C. Grover. 2005. Optimising selective sampling for bootstrapping named entity recognition. In Proceedings of the Workshop on Learning with Multiple Views, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cohn</author>
<author>Z Ghahramani</author>
<author>M I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1995</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>7</volume>
<contexts>
<context position="1723" citStr="Cohn et al., 1995" startWordPosition="271" endWordPosition="274">tation. As a consequence, when adapting existing methods and techniques to a new domain, researchers and users are faced with the problem of absence of annotated material that could be used for training. A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed recently, concerning the reusability of the data (Baldridge and Osborne, 2004). This paper presents a framework in order to deal with the lack of training data for NLP tasks. The intuition behind it is that annotated training data is produced by applying an (imperfect) unsupervised </context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1995</marker>
<rawString>D. A. Cohn, Z. Ghahramani, and M. I. Jordan. 1995. Active learning with statistical models. In Advances in Neural Information Processing Systems, volume 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on EMNLP and VLC.</booktitle>
<contexts>
<context position="11247" citStr="Collins &amp; Singer (1999)" startWordPosition="1891" endWordPosition="1894"> 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 random lowrecall wholeentities lowprecision 66 paradigm, a baseline for active annotation is random selection of instances to be checked. There are though some notable differences. During initialization, an unsupervised method u is required to provide an initial tagging on the data D. This is an important restriction which is imposed by the lack of any annotated data. Even under this restriction, there are some options available, especially for tasks which have compiled resources. One option is to use an unsupervised learning algorithm, such the one presented by Collins &amp; Singer (1999), where a seed set of rules is used to bootstrap a rulebased named entity recognizer. A different approach could be the use of a dictionary-based tagger, as in Morgan et al. (2003). It must be noted that the unsupervised method used to provide the initial tagging does not need to generalize to any data (a common problem for such methods), it only needs to perform well on the data used during active annotation. Generalization on unseen data is an attribute we hope that the supervised learning method s will have after training on the annotated material created with active annotation. The query m</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on EMNLP and VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Confidence estimation for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT 2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="18262" citStr="Culotta and McCallum, 2004" startWordPosition="3108" endWordPosition="3112">) Intuitively, the margin M is the difference between the two highest scored predictions that disagree. The lower the margin, the higher the uncertainty of the HMM on the token at question. A drawback of this method is that it doesn’t take into account the distribution of the previous label. It is possible that the two highest scored predictions are obtained for two different previous labels. It may also be the case that a highly scored label can be obtained given a very improbable previous label. Finally, an alternative that we did not explore in this work is the Field Confidence Estimation (Culotta and McCallum, 2004), which allows the estimation of confidence over sequences of tokens, instead of singleton tokens only. However, in this work confidence estimation over singleton tokens is sufficient. 6 Experimental Results In this section we present results from applying active annotation to biomedical named entity recognition. Using the noise models described in Section 3, we corrupted the training data and then using Lingpipe as the supervised learner we applied the algorithm of Figure 2. The batch of tokens selected to be checked in each round was 2000 tokens. As a baseline for comparison we used random s</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Aron Culotta and Andrew McCallum. 2004. Confidence estimation for information extraction. In Proceedings of HLT 2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dickinson</author>
<author>W D Meurers</author>
</authors>
<title>Detecting errors in part-of-speech annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 2003,</booktitle>
<pages>107--114</pages>
<location>Budapest, Hungary.</location>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>M. Dickinson and W. D. Meurers. 2003. Detecting errors in part-of-speech annotation. In Proceedings of EACL 2003, pages 107–114, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dingare</author>
<author>J Finkel</author>
<author>M Nissim</author>
<author>C Manning</author>
<author>C Grover</author>
</authors>
<title>A system for identifying named entities in biomedical text: How results from two evaluations reflect on both the system and the evaluations.</title>
<date>2004</date>
<booktitle>In The</booktitle>
<contexts>
<context position="3641" citStr="Dingare et al. (2004)" startWordPosition="590" endWordPosition="593">sents a method for estimating uncertainty for HMMs. Section 6 presents results from applying the active annotation. Section 7 compares the proposed framework to active learning and Section 8 attempts an analysis of its performance. Finally, Section 9 suggests some future work. 2 Experimental setup The data used in the experiments that follow are taken from the BioNLP 2004 named entity recognition shared task (Kim et al., 2004). The text passages have been annotated with five classes of entities, “DNA”, “RNA”, “protein”, “cell type” and “cell line”. In our experiments, following the example of Dingare et al. (2004), we simplified the annotation to one entity class, namely “gene”, which includes the DNA, RNA and protein classes. In order to evaluate the performance on the task, we used the evaluation script supplied with the data, which computes the F-score (F1 = 2∗Precision∗Recall for Precision+Recall ) each entity class. It must be noted that all tokens of an entity must be recognized correctly in order to count as a correct prediction. A partially recognized entity counts both as a precision and recall error. In all the experiments that follow, the official split of the data in training and testing wa</context>
</contexts>
<marker>Dingare, Finkel, Nissim, Manning, Grover, 2004</marker>
<rawString>S. Dingare, J. Finkel, M. Nissim, C. Manning, and C. Grover. 2004. A system for identifying named entities in biomedical text: How results from two evaluations reflect on both the system and the evaluations. In The 2004 BioLink meeting at ISMB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>Genia corpus - a semantically annotated corpus for biotextmining. In</title>
<date>2003</date>
<journal>ISMB (Supplement of Bioinformatics).</journal>
<contexts>
<context position="1571" citStr="Kim et al., 2003" startWordPosition="244" endWordPosition="247">rally accepted that increasing the amount of training data used improves performance. However, training material comes at a cost, since it requires annotation. As a consequence, when adapting existing methods and techniques to a new domain, researchers and users are faced with the problem of absence of annotated material that could be used for training. A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed recently, concerning the reusability of the data (Baldridge and Osborne, 2004). This paper presents a framework in order to deal wi</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J. D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Genia corpus - a semantically annotated corpus for biotextmining. In ISMB (Supplement of Bioinformatics).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>T Ohta</author>
<author>Y Tsuruoka</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of JNLPBA,</booktitle>
<editor>Y. Tateisi, and N. Collier, editors.</editor>
<location>Geneva.</location>
<contexts>
<context position="1444" citStr="Kim et al., 2004" startWordPosition="221" endWordPosition="224">ction Training material is always an issue when applying machine learning to deal with information extraction tasks. It is generally accepted that increasing the amount of training data used improves performance. However, training material comes at a cost, since it requires annotation. As a consequence, when adapting existing methods and techniques to a new domain, researchers and users are faced with the problem of absence of annotated material that could be used for training. A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed rece</context>
<context position="3450" citStr="Kim et al., 2004" startWordPosition="557" endWordPosition="560"> used. Section 3 explores the effect of errors in the training data and motivates the active annotation framework. 64 In Section 4 we describe the framework in detail, while Section 5 presents a method for estimating uncertainty for HMMs. Section 6 presents results from applying the active annotation. Section 7 compares the proposed framework to active learning and Section 8 attempts an analysis of its performance. Finally, Section 9 suggests some future work. 2 Experimental setup The data used in the experiments that follow are taken from the BioNLP 2004 named entity recognition shared task (Kim et al., 2004). The text passages have been annotated with five classes of entities, “DNA”, “RNA”, “protein”, “cell type” and “cell line”. In our experiments, following the example of Dingare et al. (2004), we simplified the annotation to one entity class, namely “gene”, which includes the DNA, RNA and protein classes. In order to evaluate the performance on the task, we used the evaluation script supplied with the data, which computes the F-score (F1 = 2∗Precision∗Recall for Precision+Recall ) each entity class. It must be noted that all tokens of an entity must be recognized correctly in order to count as</context>
</contexts>
<marker>Kim, Ohta, Tsuruoka, 2004</marker>
<rawString>J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier, editors. 2004. Proceedings of JNLPBA, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Morgan</author>
<author>L Hirschman</author>
<author>A Yeh</author>
<author>M Colosimo</author>
</authors>
<title>Gene name extraction using FlyBase resources.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on NLP in Biomedicine,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="11427" citStr="Morgan et al. (2003)" startWordPosition="1924" endWordPosition="1927">gh some notable differences. During initialization, an unsupervised method u is required to provide an initial tagging on the data D. This is an important restriction which is imposed by the lack of any annotated data. Even under this restriction, there are some options available, especially for tasks which have compiled resources. One option is to use an unsupervised learning algorithm, such the one presented by Collins &amp; Singer (1999), where a seed set of rules is used to bootstrap a rulebased named entity recognizer. A different approach could be the use of a dictionary-based tagger, as in Morgan et al. (2003). It must be noted that the unsupervised method used to provide the initial tagging does not need to generalize to any data (a common problem for such methods), it only needs to perform well on the data used during active annotation. Generalization on unseen data is an attribute we hope that the supervised learning method s will have after training on the annotated material created with active annotation. The query module q is also different from the corresponding module in active learning. Instead of selecting unlabeled informative instances to be annotated and added to the training data, its</context>
<context position="23648" citStr="Morgan et al. (2003)" startWordPosition="3989" endWordPosition="3992"> right graph, we compare the best active learning method with active annotation. Apparently, the performance of active annotation is highly dependent on the performance of the unsupervised tagger used to provide us with the initial annotation of the data. In the graph, we include curves for two of the noise models reported in the previous section, LowRecall20% and LowRecall50% which correspond to tagging performance of 0.66 / 0.69 / 0.67 and 0.33 / 0.43 / 0.37 respectively, in terms of Recall / Precision / F. We consider such tagging performances feasible with a dictionary-based tagger, since Morgan et al. (2003) report performance of Active learning 0 75 150 225 300 375 450 525 tokens_checked (in thousands) AA vs AL 0 75 150 225 300 375 450 525 tokens_checked (in thousands) Figure 4: Left, comparison among various active learning methods. Right, comparison of active learning and active annotation. 0.88 / 0/78 / 83 with such a method. These results demonstrate that active annotation, given a reasonable starting point, can achieve reductions in the annotation cost comparable to those of active learning. Furthermore, active annotation produces an actual corpus, albeit noisy. Active learning, as pointed </context>
</contexts>
<marker>Morgan, Hirschman, Yeh, Colosimo, 2003</marker>
<rawString>A. Morgan, L. Hirschman, A. Yeh, and M. Colosimo. 2003. Gene name extraction using FlyBase resources. In Proceedings of the ACL 2003 Workshop on NLP in Biomedicine, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>Y Matsumoto</author>
</authors>
<title>Detecting errors in corpora using support vector machines.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="12271" citStr="Nakagawa and Matsumoto (2002)" startWordPosition="2067" endWordPosition="2071">active annotation. Generalization on unseen data is an attribute we hope that the supervised learning method s will have after training on the annotated material created with active annotation. The query module q is also different from the corresponding module in active learning. Instead of selecting unlabeled informative instances to be annotated and added to the training data, its purpose is to identify likely errors in the imperfectly labelled training data, so that they are checked and corrected by the human annotator. In order to perform error-detection, we chose to adapt the approach of Nakagawa and Matsumoto (2002) which resembles uncertainty based sampling for active learning. According to their paradigm, likely errors in the training data are instances that are “hard” for the classifier and inconsistent with the rest of the data. In our case, we used the uncertainty of the classifier as the measure of the “hardness” of an instance. As an indication of inconsistency, we used the disagreement of the label assigned by the classifier with the current label of the instance. Intuitively, if the classifier disagrees with the label of an instance used in its training, it indicates that there have been other s</context>
</contexts>
<marker>Nakagawa, Matsumoto, 2002</marker>
<rawString>T. Nakagawa and Y. Matsumoto. 2002. Detecting errors in corpora using support vector machines. In Proceedings of COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
</authors>
<title>Shallow parsing using noisy and non-stationary training material.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--695</pages>
<contexts>
<context position="4998" citStr="Osborne (2002)" startWordPosition="822" endWordPosition="823">n module is an HMM model using Witten-Bell smoothing. In our experiments, using the data mentioned earlier it achieved 70.06% F-score. 3 Effect of errors Noise in the training data is a common issue in training machine learning for NLP tasks. It can have significant effect on the performance, as it was pointed out by Dingare et al. (2004), where the performance of the same system on the same task (named entity recognition in the biomedical domain) was lower when using noisier material. The effect of noise in the data used to train machine learning algorithms for NLP tasks has been explored by Osborne (2002), using the task of shallow parsing as the case study and a variety of learners. The impact of different 1http://www.alias-i.com/lingpipe/ types of noise was explored and learner specific extensions were proposed in order to deal with it. In our experiments we explored the effect of noise in training the selected named entity recognition system, keeping in mind that we are going to use an unsupervised method to create the training material. The kind of noise we expect is mislabelled instances. In order to simulate the behavior of a hypothetical unsupervised method, we corrupted the training da</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>M. Osborne. 2002. Shallow parsing using noisy and non-stationary training material. J. Mach. Learn. Res., 2:695–719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pierce</author>
<author>C Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1--9</pages>
<contexts>
<context position="14216" citStr="Pierce and Cardie, 2001" startWordPosition="2384" endWordPosition="2387">rted artificial errors and trained a classifier to recognize them. Dickinson and Meuers (2003) proposed methods based on n-grams occurring with different labellings in the corpus. Therefore, while it is reasonable to expect some correlation between the selections of active annotation and active learning (hard instances are likely to be erroneously annotated by the unsupervised tagger), the task of selecting hard instances is quite different from detecting errors. The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (Pierce and Cardie, 2001). However, the main difference is corrected co-training results in a manually annotated corpus, while active annotation allows automatically annotated instances to be kept. 5 HMM uncertainty estimation In order to perform error detection according to the previous section we need to obtain uncertainty estimations over each token from the named entity recognition module of Lingpipe. For each token t and possible label l, Lingpipe estimates the following Hidden Markov Model from the training data: P(t[n],l[n]|l[n − 1], t[n − 1], t[n − 2]) (1) When annotating a certain text passage, the tokens are</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>D. Pierce and C. Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of EMNLP 2001, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Scheffer</author>
<author>C Decomain</author>
<author>S Wrobel</author>
</authors>
<title>Active hidden Markov models for information extraction.</title>
<date>2001</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>2189--309</pages>
<contexts>
<context position="17403" citStr="Scheffer et al., 2001" startWordPosition="2953" endWordPosition="2956">rd. One property of the conditional entropy is that it estimates the uncertainty of the predictions for the current label given knowledge of the previous tag, which is important in our application because we need the uncertainty over each label independently from the rest of the sequence. This is confirmed by the theory, from which we know that for a conditional distribution of X given Y the following equation holds, H[X|Y ] = H[X, Y ] − H[Y ], where H denotes the entropy. A different way of obtaining uncertainty estimations from HMMs in the framework of active learning has been presented in (Scheffer et al., 2001). There, the uncertainty is estimated by the margin between the two most likely predictions that would result in a different current label, explicitly: M = maxi,j {P(t[n] = i|t[n − 1] = j)} − maxk,l,k�i {P(t[n] = k|t[n − 1] = l)} (5) Intuitively, the margin M is the difference between the two highest scored predictions that disagree. The lower the margin, the higher the uncertainty of the HMM on the token at question. A drawback of this method is that it doesn’t take into account the distribution of the previous label. It is possible that the two highest scored predictions are obtained for two</context>
<context position="20838" citStr="Scheffer et al. (2001)" startWordPosition="3541" endWordPosition="3544">ds) random margin entropy 0.71 0.7 0.69 0.68 F-ling 0.67 0.66 0.65 0.64 0.63 0.62 0.61 0.6 Figure 3: F-score achieved by Lingpipe is plotted against the number of checked instances for various models and levels of noise. judgements of the query module on which instances should be checked. After having checked some portion of the dataset though, active annotation started outperforming random selection. In the graphs for the 10% Random, 20% LowRecall WholeEntities and 50% LowRecall noise models, under the label “margin”, appear the performance curves obtained using the uncertainty estimation of Scheffer et al. (2001). Even though active annotation using this method performs better than random selection, active annotation using conditional entropy performs significantly better. These results provide evidence of the theoretical advantages of conditional entropy described earlier. We also ran experiments using pure uncertainty based sampling (i.e. without checking the consistency of the labels) on selecting instances to be checked. The performance curves appear under the label “uncertainty” for the 20% LowRecall, 50% Random and 20% LowPrecision noise models. The uncertainty was estimated using the method des</context>
<context position="22167" citStr="Scheffer et al., 2001" startWordPosition="3742" endWordPosition="3745">selection but worse than using labelling consistency, except for the initial stage of 20% LowPrecision. 7 Active Annotation versus Active Learning In order to compare active annotation to active learning, we run active learning experiments using the same dataset and software. The paradigm employed was uncertainty based sampling, using the uncertainty estimation presented in Sec. 5. HMMs require annotated sequences of tokens, therefore annotating whole sentences seemed as the natural choice, as in (Becker et al., 2005). While token-level selections could be used in combination with EM, (as in (Scheffer et al., 2001)), constructing a corpus of individual tokens would result in a corpus that would be very difficult to be reused, since it would be partially labelled. We employed the two standard options of selecting sentences, selecting the sentences with the highest average uncertainty over the tokens or selecting the sentence containing the most uncertain token. As cost metric we used the number of tokens, which allows more straightforward comparison with active annotation. In Figure 4 (left graph), each active learning experiment is started by selecting a random sentence as seed data, repeating the seed </context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>T. Scheffer, C. Decomain, and S. Wrobel. 2001. Active hidden Markov models for information extraction. Lecture Notes in Computer Science, 2189:309+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of COLT</booktitle>
<contexts>
<context position="1744" citStr="Seung et al., 1992" startWordPosition="275" endWordPosition="278">uence, when adapting existing methods and techniques to a new domain, researchers and users are faced with the problem of absence of annotated material that could be used for training. A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed recently, concerning the reusability of the data (Baldridge and Osborne, 2004). This paper presents a framework in order to deal with the lack of training data for NLP tasks. The intuition behind it is that annotated training data is produced by applying an (imperfect) unsupervised method, and then the </context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proceedings of COLT 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C L Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recongition.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL 2004,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="1845" citStr="Shen et al., 2004" startWordPosition="291" endWordPosition="294">with the problem of absence of annotated material that could be used for training. A good example is the biomedical domain, which has attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed recently, concerning the reusability of the data (Baldridge and Osborne, 2004). This paper presents a framework in order to deal with the lack of training data for NLP tasks. The intuition behind it is that annotated training data is produced by applying an (imperfect) unsupervised method, and then the errors inserted in the annotation are detected automatically and reannotated by a human annotator. Th</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 2004. Multi-criteria-based active learning for named entity recongition. In Proceedings ofACL 2004, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sj¨obergh</author>
<author>O Knutsson</author>
</authors>
<title>Faking errors to avoid making errors: Machine learning for error detection in writing.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP</booktitle>
<marker>Sj¨obergh, Knutsson, 2005</marker>
<rawString>J. Sj¨obergh and O. Knutsson. 2005. Faking errors to avoid making errors: Machine learning for error detection in writing. In Proceedings of RANLP 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>