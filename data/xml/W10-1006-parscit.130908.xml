<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003271">
<title confidence="0.99616">
Rethinking Grammatical Error Annotation and Evaluation with the
Amazon Mechanical Turk
</title>
<author confidence="0.7564185">
Joel R. Tetreault Elena Filatova Martin Chodorow
Educational Testing Service Fordham University Hunter College of CUNY
</author>
<affiliation confidence="0.43175">
Princeton, NJ, 08540, USA Bronx, NY, 10458, USA New York, NY, USA
</affiliation>
<email confidence="0.746561">
JTetreault@ets.org filatova@fordham.edu martin.chodorow@
hunter.cuny.edu
</email>
<sectionHeader confidence="0.990377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.989682666666667">
In this paper we present results from two pi-
lot studies which show that using the Amazon
Mechanical Turk for preposition error anno-
tation is as effective as using trained raters,
but at a fraction of the time and cost. Based
on these results, we propose a new evaluation
method which makes it feasible to compare
two error detection systems tested on different
learner data sets.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971428571429">
The last few years have seen an explosion in the de-
velopment of NLP tools to detect and correct errors
made by learners of English as a Second Language
(ESL). While there has been considerable empha-
sis placed on the system development aspect of the
field, with researchers tackling some of the tough-
est ESL errors such as those involving articles (Han
et al., 2006) and prepositions (Gamon et al., 2008),
(Felice and Pullman, 2009), there has been a woeful
lack of attention paid to developing best practices for
annotation and evaluation.
Annotation in the field of ESL error detection has
typically relied on just one trained rater, and that
rater’s judgments then become the gold standard for
evaluating a system. So it is very rare that inter-rater
reliability is reported, although, in other NLP sub-
fields, reporting reliability is the norm. Time and
cost are probably the two most important reasons
why past work has relied on only one rater because
using multiple annotators on the same ESL texts
would obviously increase both considerably. This is
</bodyText>
<page confidence="0.500328">
45
</page>
<bodyText confidence="0.999890838709677">
especially problematic for this field of research since
some ESL errors, such as preposition usage, occur at
error rates as low as 10%. This means that to collect
a corpus of 1,000 preposition errors, an annotator
would have to check over 10,000 prepositions.1
(Tetreault and Chodorow, 2008b) challenged the
view that using one rater is adequate by showing
that preposition usage errors actually do not have
high inter-annotator reliability. For example, trained
raters typically annotate preposition errors with a
kappa around 0.60. This low rater reliability has
repercussions for system evaluation: Their experi-
ments showed that system precision could vary as
much as 10% depending on which rater’s judgments
they used as the gold standard. For some grammat-
ical errors such as subject-verb agreement, where
rules are clearly defined, it may be acceptable to
use just one rater. But for usage errors, the rules
are less clearly defined and two native speakers can
have very different judgments of what is acceptable.
One way to address this is by aggregating a multi-
tude of judgments for each preposition and treating
this as the gold standard, however such a tactic has
been impractical due to time and cost limitations.
While annotation is a problem in this field, com-
paring one system to another has also been a major
issue. To date, none of the preposition and article
error detection systems in the literature have been
evaluated on the same corpus. This is mostly due to
the fact that learner corpora are difficult to acquire
(and then annotate), but also to the fact that they are
</bodyText>
<footnote confidence="0.996110333333333">
1(Tetreault and Chodorow, 2008b) report that it would take
80hrs for one of their trained raters to find and mark 1,000
preposition errors.
</footnote>
<note confidence="0.729796">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999828095238095">
usually proprietary and cannot be shared. Examples
include the Cambridge Learners Corpus2 used in
(Felice and Pullman, 2009), and TOEFL data, used
in (Tetreault and Chodorow, 2008a). This makes it
difficult to compare systems since learner corpora
can be quite different. For example, the “difficulty”
of a corpus can be affected by the L1 of the writ-
ers, the number of years they have been learning En-
glish, their age, and also where they learn English (in
a native-speaking country or a non-native speaking
country). In essence, learner corpora are not equal,
so a system that performs at 50% precision in one
corpus may actually perform at 80% precision on
a different one. Such an inability to compare sys-
tems makes it difficult for this NLP research area to
progress as quickly as it otherwise might.
In this paper we show that the Amazon Mechani-
cal Turk (AMT), a fast and cheap source of untrained
raters, can be used to alleviate several of the evalua-
tion and annotation issues described above. Specifi-
cally we show:
</bodyText>
<listItem confidence="0.9968762">
• In terms of cost and time, AMT is an effec-
tive alternative to trained raters on the tasks of
preposition selection in well-formed text and
preposition error annotation in ESL text.
• With AMT, it is possible to efficiently collect
</listItem>
<bodyText confidence="0.9182376">
multiple judgments for a target construction.
Given this, we propose a new method for evalu-
ation that finally allows two systems to be com-
pared to one another even if they are tested on
different corpora.
</bodyText>
<sectionHeader confidence="0.895599" genericHeader="method">
2 Amazon Mechnical Turk
</sectionHeader>
<bodyText confidence="0.999512916666667">
Amazon provides a service called the Mechani-
cal Turk which allows requesters (companies, re-
searchers, etc.) to post simple tasks (known as Hu-
man Intelligence Tasks, or HITs) to the AMT web-
site for untrained raters to perform for payments as
low as $0.01 in many cases (Sheng et al., 2008).
Recently, AMT has been shown to be an effective
tool for annotation and evalatuation in NLP tasks
ranging from word similarity detection and emotion
detection (Snow et al., 2008) to Machine Transla-
tion quality evaluation (Callison-Burch, 2009). In
these cases, a handful of untrained AMT workers
</bodyText>
<footnote confidence="0.571178">
2http://www.cambridge.org/elt
</footnote>
<bodyText confidence="0.999973294117647">
(or Turkers) were found to be as effective as trained
raters, but with the advantage of being considerably
faster and less expensive. Given the success of us-
ing AMT in other areas of NLP, we test whether we
can leverage it for our work in grammatical error de-
tection, which is the focus of the pilot studies in the
next two sections.
The presence of a gold standard in the above pa-
pers is crucial. In fact, the usability of AMT for text
annotation has been demostrated in those studies by
showing that non-experts’ annotation converges to
the gold standard developed by expert annotators.
However, in our work we concentrate on tasks where
there is no single gold standard, either because there
are multiple prepositions that are acceptable in a
given context or because the conventions of preposi-
tion usage simply do not conform to strict rules.
</bodyText>
<sectionHeader confidence="0.934118" genericHeader="method">
3 Selection Task
</sectionHeader>
<figureCaption confidence="0.988364">
Figure 1: Error Detection Task: Reliability of AMT as a
function of number of judgments
</figureCaption>
<bodyText confidence="0.994577076923077">
Typically, an early step in developing a preposi-
tion or article error detection system is to test the
system on well-formed text written by native speak-
ers to see how well the system can predict, or select,
the writer’s preposition given the context around
the preposition. (Tetreault and Chodorow, 2008b)
showed that trained human raters can achieve very
high agreement (78%) on this task. In their work, a
rater was shown a sentence with a target preposition
replaced with a blank, and the rater was asked to se-
lect the preposition that the writer may have used.
We replicate this experiment not with trained raters
but with the AMT to answer two research questions:
</bodyText>
<figure confidence="0.895783142857143">
1. Can untrained raters be as effective as trained
Kappa
0.90
0.85
0.80
0.75
0.70
0.65
0.60
1 2 3 4 5 6 7 8 9 10
Number of Turkers
Writer vs. AMT
Rater 1 vs. AMT
Rater 2 vs. AMT
</figure>
<page confidence="0.989037">
46
</page>
<bodyText confidence="0.983108823529412">
raters? 2. If so, how many raters does it take to
match trained raters?
In the experiment, a Turker was presented with
a sentence from Microsoft’s Encarta encyclopedia,
with one preposition in that sentence replaced with
a blank. There were 194 HITs (sentences) in all, and
we requested 10 Turker judgments per HIT. Some
Turkers did only one HIT, while others completed
more than 100, though none did all 194. The Turk-
ers’ performance was analyzed by comparing their
responses to those of two trained annotators and to
the Encarta writer’s preposition, which was consid-
ered the gold standard in this task. Comparing each
trained annotator to the writer yielded a kappa of
0.822 and 0.778, and the two raters had a kappa of
0.742. To determine how many Turker responses
would be required to match or exceed these levels of
reliability, we randomly selected samples of various
sizes from the sets of Turker responses for each sen-
tence. For example, when samples were of size N =
4, four responses were randomly drawn from the set
of ten responses that had been collected. The prepo-
sition that occurred most frequently in the sample
was used as the Turker response for that sentence. In
the case of a tie, a preposition was randomly drawn
from those tied for most frequent. For each sample
size, 100 samples were drawn and the mean values
of agreement and kappa were calculated. The reli-
ability results presented in Table 1 show that, with
just three Turker responses, kappa with the writer
(top line) is comparable to the values obtained from
the trained annotators (around 0.8). Most notable is
that with ten judgments, the reliability measures are
much higher than those of the trained annotators. 3
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="method">
4 Error Detection Task
</sectionHeader>
<bodyText confidence="0.995261666666666">
While the previous results look quite encouraging,
the task they are based on, preposition selection in
well-formed text, is quite different from, and less
challenging than, the task that a system must per-
form in detecting errors in learner writing. To exam-
ine the reliability of Turker preposition error judg-
ments, we ran another experiment in which Turkers
were presented with a preposition highlighted in a
sentence taken from an ESL corpus, and were in-
3We also experimented with 50 judgments per sentence, but
agreement and kappa improved only negligibly.
structed to judge its usage as either correct, incor-
rect, or the context is too ungrammatical to make
a judgment. The set consisted of 152 prepositions
in total, and we requested 20 judgments per prepo-
sition. Previous work has shown this task to be a
difficult one for trainer raters to attain high reliabil-
ity. For example, (Tetreault and Chodorow, 2008b)
found kappa between two raters averaged 0.630.
Because there is no gold standard for the er-
ror detection task, kappa was used to compare
Turker responses to those of three trained anno-
tators. Among the trained annotators, inter-kappa
agreement ranged from 0.574 to 0.650, for a mean
kappa of 0.606. In Figure 2, kappa is shown for the
comparisons of Turker responses to each annotator
for samples of various sizes ranging from N = 1 to
N = 18. At sample size N = 13, the average kappa is
0.608, virtually identical to the mean found among
the trained annotators.
</bodyText>
<figureCaption confidence="0.969896">
Figure 2: Error Detection Task: Reliability of AMT as a
function of number of judgments
</figureCaption>
<sectionHeader confidence="0.924634" genericHeader="method">
5 Rethinking Evaluation
</sectionHeader>
<bodyText confidence="0.9999304">
We contend that the Amazon Mechanical Turk can
not only be used as an effective alternative annota-
tion source, but can also be used to revamp evalu-
ation since multiple judgments are now easily ac-
quired. Instead of treating the task of error detection
as a “black or white” distinction, where a preposi-
tion is either correct or incorrect, cases of prepo-
sition use can now be grouped into bins based on
the level of agreement of the Turkers. For example,
if 90% or more judge a preposition to be an error,
</bodyText>
<figure confidence="0.998629153846154">
0.65
0.60
0.55
Rater 1 vs. AMT
Rater 2 vs. AMT
Rater 3 vs. AMT
Mean
0.45
0.40
1 2 3 4 5 6 78 9 10 11 12 13 14 15 16 17 18
Number of Turkers
Kappa
0.50
</figure>
<page confidence="0.994747">
47
</page>
<table confidence="0.979757333333333">
Task # of HITs Judgments/HIT Total Judgments Cost Total Cost # of Turkers Total Time
Selection 194 10 1,940 $0.02 $48.50 49 0.5 hours
Error Detection 152 20 3,040 $0.02 $76.00 74 6 hours
</table>
<tableCaption confidence="0.9997">
Table 1: AMT Experiment Statistics
</tableCaption>
<bodyText confidence="0.99997272">
the high agreement is strong evidence that this is a
clear case of an error. Conversely, agreement lev-
els around 50% would indicate that the use of a par-
ticular preposition is highly contentious, and, most
likely, it should not be flagged by an automated er-
ror detection system.
The current standard method treats all cases of
preposition usage equally, however, some are clearly
harder to annotate than others. By breaking an eval-
uation set into agreement bins, it should be possible
to separate the “easy” cases from the “hard” cases
and report precision and recall results for the differ-
ent levels of human agreement represented by differ-
ent bins. This method not only gives a clearer pic-
ture of how a system is faring, but it also ameliorates
the problem of cross-system evaluation when two
systems are evaluated on different corpora. If each
evaluation corpus is annotated by the same number
of Turkers and with the same annotation scheme, it
will now be possible to compare systems by sim-
ply comparing their performance on each respective
bin. The assumption here is that prepositions which
show X% agreement in corpus A are of equivalent
difficulty to those that show X% agreement in cor-
pus B.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999603">
In this paper, we showed that the AMT is an ef-
fective tool for annotating grammatical errors. At
a fraction of the time and cost, it is possible to
acquire high quality judgments from multiple un-
trained raters without sacrificing reliability. A sum-
mary of the cost and time of the two experiments
described here can be seen in Table 1. In the task of
preposition selection, only three Turkers are needed
to match the reliability of two trained raters; in the
more complicated task of error detection, up to 13
Turkers are needed. However, it should be noted
that these numbers can be viewed as upper bounds.
The error annotation scheme that was used is a very
simple one. We intend to experiment with different
guidelines and instructions, and to screen (Callison-
Burch, 2009) and weight Turkers’ responses (Snow
et al., 2008), in order to lower the number of Turk-
ers required for this task. Finally, we will look at
other errors, such as articles, to determine how many
Turkers are necessary for optimal annotation.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999888333333333">
We thank Sarah Ohls and Waverely VanWinkle for
their annotation work, and Jennifer Foster and the
two reviewers for their comments and feedback.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886870967742">
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In EMNLP.
Rachele De Felice and Stephen G. Pullman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for esl error cor-
rection. In Proceedings of IJCNLP, Hyderabad, India,
January.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115–129.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get another label? Improving data quality and
data mining using multiple, noisy labelers. In Pro-
ceeding of ACM SIGKDD, Las Vegas, Nevada, USA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast – but is it good?
evaluating non-expert annotations for natural language
tasks. In EMNLP.
Joel R. Tetreault and Martin Chodorow. 2008a. The ups
and downs of preposition error detection in ESL writ-
ing. In COLING.
Joel Tetreault and Martin Chodorow. 2008b. Native
Judgments of non-native usage: Experiments in prepo-
sition error detection. In COLING Workshop on Hu-
man Judgments in Computational Linguistics.
</reference>
<page confidence="0.999352">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.627743">
<title confidence="0.91806">Rethinking Grammatical Error Annotation and Evaluation with Amazon Mechanical Turk</title>
<author confidence="0.998546">Joel R Elena Martin</author>
<affiliation confidence="0.764451">Educational Testing Fordham Hunter College of</affiliation>
<address confidence="0.874623">Princeton, NJ, 08540, Bronx, NY, 10458, New York, NY,</address>
<email confidence="0.997257">JTetreault@ets.orgfilatova@fordham.eduhunter.cuny.edu</email>
<abstract confidence="0.9993559">In this paper we present results from two pilot studies which show that using the Amazon Mechanical Turk for preposition error annotation is as effective as using trained raters, but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5768" citStr="Callison-Burch, 2009" startWordPosition="947" endWordPosition="948">ed to one another even if they are tested on different corpora. 2 Amazon Mechnical Turk Amazon provides a service called the Mechanical Turk which allows requesters (companies, researchers, etc.) to post simple tasks (known as Human Intelligence Tasks, or HITs) to the AMT website for untrained raters to perform for payments as low as $0.01 in many cases (Sheng et al., 2008). Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al., 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). In these cases, a handful of untrained AMT workers 2http://www.cambridge.org/elt (or Turkers) were found to be as effective as trained raters, but with the advantage of being considerably faster and less expensive. Given the success of using AMT in other areas of NLP, we test whether we can leverage it for our work in grammatical error detection, which is the focus of the pilot studies in the next two sections. The presence of a gold standard in the above papers is crucial. In fact, the usability of AMT for text annotation has been demostrated in those studies by showing that non-experts’ an</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen G Pullman</author>
</authors>
<title>Automatic detection of preposition errors in learner writing.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>De Felice, Pullman, 2009</marker>
<rawString>Rachele De Felice and Stephen G. Pullman. 2009. Automatic detection of preposition errors in learner writing. CALICO Journal, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alex Klementiev</author>
<author>William B Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for esl error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="1152" citStr="Gamon et al., 2008" startWordPosition="179" endWordPosition="182">but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets. 1 Introduction The last few years have seen an explosion in the development of NLP tools to detect and correct errors made by learners of English as a Second Language (ESL). While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al., 2006) and prepositions (Gamon et al., 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation in the field of ESL error detection has typically relied on just one trained rater, and that rater’s judgments then become the gold standard for evaluating a system. So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alex Klementiev, William B. Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using contextual speller techniques and language modeling for esl error correction. In Proceedings of IJCNLP, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<pages>12--115</pages>
<contexts>
<context position="1114" citStr="Han et al., 2006" startWordPosition="173" endWordPosition="176"> effective as using trained raters, but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets. 1 Introduction The last few years have seen an explosion in the development of NLP tools to detect and correct errors made by learners of English as a Second Language (ESL). While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al., 2006) and prepositions (Gamon et al., 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation in the field of ESL error detection has typically relied on just one trained rater, and that rater’s judgments then become the gold standard for evaluating a system. So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using mul</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12:115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis Ipeirotis</author>
</authors>
<title>Get another label? Improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceeding of ACM SIGKDD,</booktitle>
<location>Las Vegas, Nevada, USA.</location>
<contexts>
<context position="5523" citStr="Sheng et al., 2008" startWordPosition="908" endWordPosition="911"> text and preposition error annotation in ESL text. • With AMT, it is possible to efficiently collect multiple judgments for a target construction. Given this, we propose a new method for evaluation that finally allows two systems to be compared to one another even if they are tested on different corpora. 2 Amazon Mechnical Turk Amazon provides a service called the Mechanical Turk which allows requesters (companies, researchers, etc.) to post simple tasks (known as Human Intelligence Tasks, or HITs) to the AMT website for untrained raters to perform for payments as low as $0.01 in many cases (Sheng et al., 2008). Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al., 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). In these cases, a handful of untrained AMT workers 2http://www.cambridge.org/elt (or Turkers) were found to be as effective as trained raters, but with the advantage of being considerably faster and less expensive. Given the success of using AMT in other areas of NLP, we test whether we can leverage it for our work in grammatical error detection, whic</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor Sheng, Foster Provost, and Panagiotis Ipeirotis. 2008. Get another label? Improving data quality and data mining using multiple, noisy labelers. In Proceeding of ACM SIGKDD, Las Vegas, Nevada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2098" citStr="Tetreault and Chodorow, 2008" startWordPosition="333" endWordPosition="336">t is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts would obviously increase both considerably. This is 45 especially problematic for this field of research since some ESL errors, such as preposition usage, occur at error rates as low as 10%. This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions.1 (Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. For example, trained raters typically annotate preposition errors with a kappa around 0.60. This low rater reliability has repercussions for system evaluation: Their experiments showed that system precision could vary as much as 10% depending on which rater’s judgments they used as the gold standard. For some grammatical errors such as subject-verb agreement, where rules are clearly defined, it may be acceptable to use just one rater. But for u</context>
<context position="3428" citStr="Tetreault and Chodorow, 2008" startWordPosition="553" endWordPosition="556">nts of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue. To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1(Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics usually proprietary and cannot be shared. Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). This makes it difficult to compare systems since learner corpora can be quite different. For exa</context>
<context position="7106" citStr="Tetreault and Chodorow, 2008" startWordPosition="1170" endWordPosition="1173"> tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. 3 Selection Task Figure 1: Error Detection Task: Reliability of AMT as a function of number of judgments Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer’s preposition given the context around the preposition. (Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained Kappa 0.90 0.85 0.80 0.75 0.70 0.65 0.60 1 2 3 4 5 6 7 8 9 10 Number of Turkers Writer vs. AMT Rater 1 vs. AMT Rater 2 vs. AMT 46 raters? 2. If so, how many raters does it take to mat</context>
<context position="10290" citStr="Tetreault and Chodorow, 2008" startWordPosition="1722" endWordPosition="1725"> preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were in3We also experimented with 50 judgments per sentence, but agreement and kappa improved only negligibly. structed to judge its usage as either correct, incorrect, or the context is too ungrammatical to make a judgment. The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. Previous work has shown this task to be a difficult one for trainer raters to attain high reliability. For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. Among the trained annotators, inter-kappa agreement ranged from 0.574 to 0.650, for a mean kappa of 0.606. In Figure 2, kappa is shown for the comparisons of Turker responses to each annotator for samples of various sizes ranging from N = 1 to N = 18. At sample size N = 13, the average kappa is 0.608, virtually identical to the mean found among the trained annotators. Figure 2: Error Detection Task: Re</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008a. The ups and downs of preposition error detection in ESL writing. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Native Judgments of non-native usage: Experiments in preposition error detection.</title>
<date>2008</date>
<booktitle>In COLING Workshop on Human Judgments in Computational Linguistics.</booktitle>
<contexts>
<context position="2098" citStr="Tetreault and Chodorow, 2008" startWordPosition="333" endWordPosition="336">t is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts would obviously increase both considerably. This is 45 especially problematic for this field of research since some ESL errors, such as preposition usage, occur at error rates as low as 10%. This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions.1 (Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. For example, trained raters typically annotate preposition errors with a kappa around 0.60. This low rater reliability has repercussions for system evaluation: Their experiments showed that system precision could vary as much as 10% depending on which rater’s judgments they used as the gold standard. For some grammatical errors such as subject-verb agreement, where rules are clearly defined, it may be acceptable to use just one rater. But for u</context>
<context position="3428" citStr="Tetreault and Chodorow, 2008" startWordPosition="553" endWordPosition="556">nts of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue. To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1(Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics usually proprietary and cannot be shared. Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). This makes it difficult to compare systems since learner corpora can be quite different. For exa</context>
<context position="7106" citStr="Tetreault and Chodorow, 2008" startWordPosition="1170" endWordPosition="1173"> tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. 3 Selection Task Figure 1: Error Detection Task: Reliability of AMT as a function of number of judgments Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer’s preposition given the context around the preposition. (Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained Kappa 0.90 0.85 0.80 0.75 0.70 0.65 0.60 1 2 3 4 5 6 7 8 9 10 Number of Turkers Writer vs. AMT Rater 1 vs. AMT Rater 2 vs. AMT 46 raters? 2. If so, how many raters does it take to mat</context>
<context position="10290" citStr="Tetreault and Chodorow, 2008" startWordPosition="1722" endWordPosition="1725"> preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were in3We also experimented with 50 judgments per sentence, but agreement and kappa improved only negligibly. structed to judge its usage as either correct, incorrect, or the context is too ungrammatical to make a judgment. The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. Previous work has shown this task to be a difficult one for trainer raters to attain high reliability. For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. Among the trained annotators, inter-kappa agreement ranged from 0.574 to 0.650, for a mean kappa of 0.606. In Figure 2, kappa is shown for the comparisons of Turker responses to each annotator for samples of various sizes ranging from N = 1 to N = 18. At sample size N = 13, the average kappa is 0.608, virtually identical to the mean found among the trained annotators. Figure 2: Error Detection Task: Re</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel Tetreault and Martin Chodorow. 2008b. Native Judgments of non-native usage: Experiments in preposition error detection. In COLING Workshop on Human Judgments in Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>