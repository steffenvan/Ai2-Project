<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9975665">
Stochastically Evaluating the Validity of Partial Parse Trees in
Incremental Parsing
</title>
<author confidence="0.996852">
Yoshihide Kato1, Shigeki Matsubara2 and Yasuyoshi Inagaki3
</author>
<affiliation confidence="0.98154925">
Graduate School of International Development, Nagoya University 1
Information Technology Center, Nagoya University 2
Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan
Faculty of Information Science and Technology, Aichi Prefectural University 3
</affiliation>
<address confidence="0.886217">
1522-3 Ibaragabasama, Kumabari, Nagakute-cho, Aichi-gun, 480-1198 Japan
</address>
<email confidence="0.999272">
yosihide@gsid.nagoya-u.ac.jp
</email>
<sectionHeader confidence="0.993906" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964">
This paper proposes a method for evaluating the
validity of partial parse trees constructed in incre-
mental parsing. Our method is based on stochastic
incremental parsing, and it incrementally evaluates
the validity for each partial parse tree on a word-
by-word basis. In our method, incremental parser
returns partial parse trees at the point where the va-
lidity for the partial parse tree becomes greater than
a threshold. Our technique is effective for improv-
ing the accuracy of incremental parsing.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979333333333">
Real-time spoken language processing systems,
such as simultaneous machine interpretation sys-
tems, are required to quickly respond to users’ utter-
ances. To fulfill the requirement, the system needs
to understand spoken language at least incremen-
tally (Allen et al., 2001; Inagaki and Matsubara,
1995; Milward and Cooper, 1994), that is, to ana-
lyze each input sentence from left to right and ac-
quire the content.
Several incremental parsing methods have been
proposed to date (Costa et al., 2001; Haddock,
1987; Matsubara et al., 1997; Milward, 1995;
Roark, 2001). These methods construct candidate
partial parse trees for initial fragments of the input
sentence on a word-by-word basis. However, these
methods contain local ambiguity problems that par-
tial parse trees representing valid syntactic relations
can not be determined without using information
from the rest of the input sentence.
On the other hand, Marcus proposed a method
of deterministically constructing valid partial parse
trees by looking ahead several words (Marcus,
1980), while Kato et al. proposed an incremental
parsing which delays the decision of valid partial
parse trees (Kato et al., 2000). However, it is hard to
say that these methods realize broad-coverage incre-
mental parsing. The method in the literature (Mar-
cus, 1980) uses lookahead rules, which are con-
structed by hand, but it is not clear whether broad
coverage lookahead rules can be obtained. The
incremental parsing in the literature (Kato et al.,
2000), which is based on context free grammar, is
infeasible to deal with large scale grammar, because
the parser exhaustively searches all candidate partial
parse trees in top-down fashion.
This paper proposes a probabilistic incremental
parser which evaluates the validity of partial parse
trees. Our method extracts a grammar from a tree-
bank, and the incremental parsing uses a beam-
search strategy so that it realizes broad-coverage
parsing. To resolve local ambiguity, the parser in-
crementally evaluates the validity of partial parse
trees on a word-by-word basis, and delays the deci-
sion of which partial parse trees should be returned,
until the validity for the partial parse tree becomes
greater than a threshold. Our technique is effective
for improving the accuracy of incremental parsing.
This paper is organized as follows: The next
section proposes a probabilistic incremental parser.
Section 3 discusses the validity of partial parse tree
constructed in incremental parsing. Section 4 pro-
poses a method of incrementally evaluating the va-
lidity of partial parse tree. In section 5, we report an
experimental evaluation of our method.
</bodyText>
<sectionHeader confidence="0.787617" genericHeader="method">
2 TAG-based Incremental Parsing
</sectionHeader>
<bodyText confidence="0.998465666666667">
Our incremental parsing is based on tree adjoining
grammar (TAG) (Joshi, 1985). This section pro-
poses a TAG-based incremental parsing method.
</bodyText>
<sectionHeader confidence="0.759304" genericHeader="method">
2.1 TAG for Incremental Parsing
</sectionHeader>
<bodyText confidence="0.975105428571429">
Firstly, we propose incremental-parsing-oriented
TAG (ITAG). An ITAG comprises two sets of ele-
mentary trees just like TAG: initial trees and auxil-
iary trees. The difference between ITAG and TAG
is the form of elementary trees. Every ITAG ini-
tial tree is leftmost-expanded. A tree is leftmost-
expanded if it is of the following forms:
</bodyText>
<listItem confidence="0.7582015">
1. [t]X, where t is a terminal symbol and X is a
nonterminal symbol.
</listItem>
<figureCaption confidence="0.929142">
Figure 1: Examples of ITAG elementary trees
</figureCaption>
<listItem confidence="0.6001705">
2. [σX1 · · · Xk]X, where σ is a leftmost expanded
tree, X1, ... , Xk, X are nonterminal symbols.
</listItem>
<bodyText confidence="0.9111715">
On the other hand, every ITAG auxiliary tree is of
the following form:
</bodyText>
<equation confidence="0.82557">
[X*σX1 ··· Xk]X
</equation>
<bodyText confidence="0.998299787234043">
where σ is a leftmost expanded tree and X,
X1, ... , Xk are nonterminal symbols. X* is called
a foot node. Figure 1 shows examples of ITAG ele-
mentary trees.
These elemental trees can be combined by using
two operations: substitution and adjunction.
substitution The substitution operation replaces a
leftmost nonterminal leaf of a partial parse tree
σ with an initial tree α having the same nonter-
minal symbol at its root. We write sα for the
operation of substituting α and sα(σ) for the
result of applying sα to σ.
adjunction The adjunction operation splits a par-
tial parse tree σ at a nonterminal node having
no nonterminal leaf, and inserts an auxiliary
tree β having the same nonterminal symbol at
its root. We write ao for the operation of ad-
joining β and ao(σ) for the result of applying
ao to σ.
The substitution operation is similar to rule expan-
sion of top-down incremental parsing such as (Mat-
subara et al., 1997; Roark, 2001). Furthermore,
by introducing the adjunction operation to incre-
mental parsing, we can expect that local ambiguity
of left-recursive structures is decreased (Lombardo
and Sturt, 1997).
Our proposed incremental parsing is based on
ITAG. When i-th word wi is scanned, the parser
combines elementary trees for wi with partial parse
trees for w1 · · · wi_1 to construct the partial parse
trees for w1 · · · wi_1wi.
As an example, let us consider incremental pars-
ing of the following sentence by using ITAG shown
in Figure 1:
I found a dime in the wood. (1)
Table 1 shows the process of tree construction
for the sentence (1). When the word “found” is
scanned, partial parse trees #3, #4 and #5 are con-
structed by applying substitution operations to par-
tial parse tree #2 for the initial fragment “I”. When
the word “in” is scanned, partial parse trees #12 and
#13 are constructed by applying adjunction opera-
tions to partial parse tree #10 for the initial frag-
ment “I found a dime”. This example shows that
the ITAG based incremental parsing is capable of
constructing partial parse trees of initial fragments
for every word input.
</bodyText>
<subsectionHeader confidence="0.995723">
2.2 ITAG Extraction from Treebank
</subsectionHeader>
<bodyText confidence="0.9999426">
Here, we propose a method for extracting an ITAG
from a treebank to realize broad-coverage incre-
mental parsing. Our method decomposes parse trees
in treebank to obtain ITAG elementary trees. The
decomposition is as follows:
</bodyText>
<listItem confidence="0.997674454545454">
• for each node η1 having no left-sibling, if the
parent ηp has the same nonterminal symbol as
η1, split the parse tree at η1 and ηp, and com-
bine the upper tree and the lower tree. η1 of
intermediate tree is a foot node.
• for each node η2 having only one left-sibling,
if the parent ηp does not have the same nonter-
minal symbol as the left-sibling η1 of η2, split
the parse tree at η2.
• for the other node η in the parse tree, split the
parse tree at η.
</listItem>
<bodyText confidence="0.993753916666667">
For example, The initial trees α1, α2, α5, α7 αs and
α10 and the auxiliary tree β2 are extracted from the
parse tree #18 in Table 1.
Our proposed tree extraction is similar to the TAG
extractions proposed in the literatures (Chen and
Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999).
The main difference between these methods is the
position of nodes at which parse trees are split.
While the methods in the literatures (Chen and
Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) uti-
lize a head percolation rule to split the parse trees at
complement nodes, our method splits the parse trees
</bodyText>
<figure confidence="0.999640948717949">
a
a
10
NN
wood
5 NP b NP 7 NN 8
DT NN DT JJ NN dime
NP
NP
9
DT JJ NN
the
DT NN
the
ADJP
VB NP
found
Initial trees:
1 S 2
NP VP
PRP
I
VB NP
found
VP
3
VP
4
VP
VB
found
Auxiliary trees:
1 NP 2
NP* PP VP* PP
IN NP
in
IN NP
in
VP
</figure>
<tableCaption confidence="0.92821">
Table 1: Incremental parsing process of “I found a dime in the wood.”
</tableCaption>
<figure confidence="0.9950706">
word # partial parse tree
1 s
I 2 [[[I]p*p]npvpF]s
found 3 [[[I]p*p]np[[JF ound]vbnp]vp]s
4 [[[I]p*p]np[[JF ound]vbnp adjp]vp]s
5 [[[I]p*p]np[[J ound]vb]vp] s
a 6 [[[I]p*p]np[[found]vb[[a]dtnn]np]vp]s
7 [[[I]p*p]np[[found]vb[[a]dtjj nn]np]vp]s
8 [[[I]p*p]np[[found]vb[[a]dtnn]npadjp]vp]s
9 [[[I]p*p]np[[found]vb[[a]dtjj nn]npadjp]vp]s
dime 10 [[[I]p*p]np[[found]vb[[a]dt[dime]nn]np]vp]s
11 [[[I]p*p]np[[found]vb[[a]dt[dime]nn]npadjp]vp]s
in 12 [[[I]p*p]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]innp]pp]vp]s
13 [[[I]p*p]np[[found]vb[[[a]dt[dime]nn]np[[in]innp]pp]np]vp]s
the 14 [[[I]p*p]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtnn]np]pp]vp]s
15 [[[I]p*p]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtjj nn]np]pp]vp]s
16 [[[I]p*p]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtnn]np]pp]np]vp]s
17 [[[I]p*p]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtjj nn]np]pp]np]vp]s
wood 18 [[[I]p*p]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dt[wood]nn]np]pp]vp]s
19 [[[I]p*p]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dt[wood]nn]np]pp]np]vp]s
</figure>
<bodyText confidence="0.998044625">
at left recursive nodes and nodes having left-sibling.
The elementary trees extracted by our method are of
the forms described in section 2.1, and can be com-
bined from left to right on a word-by-word basis.
The property is suitable for incremental parsing. On
the other hand, the elementary trees obtained by the
method based on head information does not neces-
sarily have this property 1.
</bodyText>
<subsectionHeader confidence="0.996355">
2.3 Probabilistic ITAG
</subsectionHeader>
<bodyText confidence="0.999966222222222">
This section describes probabilistic ITAG (PITAG)
which is utilized by evaluating partial parse trees in
incremental parsing. PITAG assigns a probability
to the event that an elementary tree is combined by
substitution or adjunction with another tree.
We induce the probability by maximum likeli-
hood estimation. Let α be an initial tree and X be
the root symbol of α. The probability that α is sub-
stituted is calculated as follows:
</bodyText>
<equation confidence="0.996583">
C(sα)
P (sα) = � (2)
α�∈I(X) C(sα�)
</equation>
<bodyText confidence="0.944979714285714">
where C(sα) is the count of the number of times of
applying substitution sα in the treebank, and I(X)
is the set of initial trees whose root is labeled with
X.
1For example, the tree extraction based on head informa-
tion splits the parse tree #18 at the node labeled with dt to ob-
tain the elementary tree [a]dt for “a”. However, the tree [a]dt
cannot be combined with the partial parse tree for “I found”,
since substitution node labeled with dt exists in the initial tree
[dt[dime]nn]np for “dime” and not the partial parse trees for “I
found”.
Let β be a auxiliary tree and X be the root symbol
of β. The probability that β is adjoined is calculated
as follows:
</bodyText>
<equation confidence="0.999899">
P(a0) = C(X) (3)
</equation>
<bodyText confidence="0.999385">
where C(X) is the count of the number of occur-
rences of symbol X. The probability that adjunction
is not applied is calculated as follows:
</bodyText>
<equation confidence="0.9981415">
P(nilX) = 1 − � P(a0) (4)
0∈A(X)
</equation>
<bodyText confidence="0.9945964375">
where nilX means that the adjunction is not applied
to a node labeled with X, and A(X) is the set of all
auxiliary trees whose root is labeled X.
In this PITAG formalism, the probability that el-
ementary trees are combined at each node depends
only on the nonterminal symbol of that node 2.
The probability of a parse tree is calculated by the
product of the probability of the operations which
are used in construction of the parse tree. For ex-
ample, the probability of each operation is given as
shown in Table 2. The probability of the partial
parse tree #12, which is constructed by using sα1,
sα2, sαy, sα7, nilNP and a02, is 1 x 0.7 x 0.3 x
0.5 x 0.7 x 0.7 = 0.05145.
We write P(σ) for the probability of a partial
parse tree σ.
</bodyText>
<footnote confidence="0.978018">
2The PITAG formalism corresponds to SLG(1) in the liter-
ature (Carroll and Weir, 2003).
</footnote>
<tableCaption confidence="0.979174">
Table 2: Probability of operations
</tableCaption>
<subsectionHeader confidence="0.997871">
2.4 Parsing Strategies
</subsectionHeader>
<bodyText confidence="0.9995985">
In order to improve the efficiency of the parsing, we
adapt two parsing strategies as follows:
</bodyText>
<listItem confidence="0.9708135">
• If two partial parse trees have the same se-
quence of nodes to which ITAG operations are
applicable, then the lower probability tree can
be safely discarded.
• The parser only keeps n-best partial parse trees.
3 Validity of Partial Parse Trees
</listItem>
<bodyText confidence="0.92707225">
This section gives some definitions about the valid-
ity of a partial parse tree. Before describing the va-
lidity of a partial parse tree, we define the subsump-
tion relation between partial parse trees.
Definition 1 (subsumption relation) Let U and τ
be partial parse trees. Then we write U D τ, if
sα(U) = τ, for some initial tree α or a,a(U) = τ,
for some auxiliary tree β. Let D* be the reflexive
transitive closure of D. We say that U subsumes τ,
if U D* τ. ❑
That U subsumes τ means that τ is the result of ap-
plying a substitution or an adjunction to U. Figure 2
shows the subsumption relation between the partial
parse trees constructed for the sentence (1).
If a partial parse tree for an initial fragment repre-
sents a syntactic relation correctly, the partial parse
tree subsumes the correct parse tree for the input
sentence. We say that such a partial parse tree is
valid. The validity of a partial parse tree is defined
as follows:
Definition 2 (valid partial parse tree) Let U be a
partial parse tree and w1 · · · w,,, be an input sen-
tence. We say that U is valid for w1 · · · w,,, if U sub-
sumes the correct parse tree for w1 · · · w,,,. ❑
</bodyText>
<figureCaption confidence="0.999256333333333">
Figure 2: Subsumption relation between partial
parse trees
Figure 3: Valid partial parse trees
</figureCaption>
<bodyText confidence="0.999613833333333">
For example, assume that the #18 is correct parse
tree for the sentence (1). Then partial parse tree #3
is valid for the sentence (1), because #3 D* #18. On
the other hand, partial parse tree #4 and #5 are not
valid for (1). Figure 3 shows the valid partial parse
trees for the sentence (1).
</bodyText>
<sectionHeader confidence="0.954866" genericHeader="method">
4 Evaluating the Validity of Partial Parse
Tree
</sectionHeader>
<bodyText confidence="0.999861571428571">
The validity of a partial parse tree for an initial frag-
ment depends on the rest of the sentence. For ex-
ample, the validity of the partial parse trees #3, #4
and #5 depends on the remaining input that follows
the word “found.” This means that the validity dy-
namically varies for every word input. We define a
conditional validity of partial parse tree:
</bodyText>
<equation confidence="0.999148">
ErESub(v,w1···wj) P(τ)
V (U  |w1 ··· wj) = ErET(w1···wj) P(τ) (5)
</equation>
<bodyText confidence="0.862735">
where U is a partial parse tree for an initial frag-
</bodyText>
<construct confidence="0.565553857142857">
ment w1 · · · wz(i &lt; j), T(w1 · · · wj) is the set of
constructed partial parse trees for the initial frag-
ment w1 · · · wj and Sub(U, w1 · · · wj) is the subset
of T (w1 · · · wj) whose elements are subsumed by U.
The equation (5) represents the validity of U on the
condition w1 · · · wj. U is valid for input sentence
if and only if some partial parse tree for w1 · · · wj
</construct>
<figure confidence="0.998638086206896">
1.0
0.7
sα1
sα2
0.5
0.3
0.2
0.1
0.3
0.7
0.7
0.3
operation
probability
sαl, sα10
sα5, sα8
sαa, sα6, sαs
sα3
a,a1
a,a2
nilNP
nilvP
found a dime
#2 #3 #6
in the wood
#10 #12 #14 #18
I
#1
#7
#15
#4 #8
#11
#13
#16
#19
#17
#9
#5
subsumption relation
found a dime
#2 #3 #6
in the wood
#10 #12 #14
I
#1
#18
#7
#15
#4 #8
#11
#13
#16
#19
#17
#9
valid partial parse tree
#5
subsumption relation
</figure>
<bodyText confidence="0.816001333333333">
subsumed by Q is valid. The equation 5 is the ratio
of such partial parse trees to the constructed partial
parse trees.
</bodyText>
<subsectionHeader confidence="0.98418">
4.1 Output Partial Parse Trees
</subsectionHeader>
<bodyText confidence="0.999419571428572">
Kato et al. proposed a method of delaying the deci-
sion of which partial parse trees should be returned
as the output, until the validity of partial parse trees
are guaranteed (Kato et al., 2000). The idea of
delaying the decision of the output is interesting.
However, delaying the decision until the validity are
guaranteed may cause the loss of incrementality of
the parsing.
To solve the problem, in our method, the in-
cremental parser returns high validity partial parse
trees rather than validity guaranteed partial parse
trees.
When the j-th word wj is scanned, our incremen-
tal parser returns the following partial parse:
</bodyText>
<equation confidence="0.994504">
argmax{σ:V (σ,w1···wj)&gt;θ1l(Q) (6)
</equation>
<bodyText confidence="0.999924">
where 0 is a threshold between [0, 1] and l(Q) is
the length of the initial fragment which is yielded
by Q. The output partial parse tree is the one for
the longest initial fragment in the partial parse trees
whose validity are greater than a threshold 0.
</bodyText>
<subsectionHeader confidence="0.99938">
4.2 An Example
</subsectionHeader>
<bodyText confidence="0.97642225">
Let us consider a parsing example for the sentence
(1). We assume that the threshold 0 = 0.8.
Let us consider when the partial parse tree
#3, which is valid for (1), is returned as output.
When the word “found” is scanned, partial parse
trees #3, #4 and #5 are constructed. That is,
T(I found) = 1#3, #4, #51. As shown in Figure
2, Sub(#3, I found) = 1#31. Furthermore,
</bodyText>
<equation confidence="0.718512333333333">
P(#3) = 0.7, P(#4) = 0.1 and P(#5) = 0.2.
Therefore, Validity(#3, I found)
0.7/(0.7 + 0.1 + 0.2) = 0.7. Because
</equation>
<bodyText confidence="0.9697154375">
Validity(#3, I found) &lt; 0, partial parse tree
#3 is not returned as the output at this point. The
parser only keeps #3 as a candidate partial parse
tree.
When the next word “a” is scanned, partial parse
trees #6, #7, #8 and #9 are constructed, where
P(#6) = 0.21, P(#7) = 0.14, P(#8) = 0.03 and
P(#9) = 0.02. Sub(#3, I found a) = 1#6, #71.
Therefore, V alidity(#3, I found a) = (0.21 +
0.14)/(0.21+0.14+0.03+0.02) = 0.875. Because
Validity(#3, I found a) &gt; 0, partial parse tree #3
is returned as the output.
Table 3 shows the output partial parse tree for ev-
ery word input.
Our incremental parser delays the decision of the
output as shown in this example.
</bodyText>
<tableCaption confidence="0.995078">
Table 3: Output partial parse trees
</tableCaption>
<sectionHeader confidence="0.984819" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.998167703703704">
To evaluate the performance of our proposed
method, we performed a parsing experiment. The
parser was implemented in GNU Common Lisp on a
Linux PC. In the experiment, the inputs of the incre-
mental parser are POS sequences rather than word
sequences. We used 47247 initial trees and 2931
auxiliary trees for the experiment. The elementary
trees were extracted from the parse trees in sec-
tions 02-21 of the Wall Street Journal in Penn Tree-
bank (Marcus et al., 1993), which is transformed
by using parent-child annotation and left factoring
(Roark and Johnson, 1999). We set the beam-width
at 500.
The labeled precision and recall of the parsing
are 80.8% and 78.5%, respectively for the section
23 in Penn Treebank. We used the set of sentences
for which the outputs of the incremental parser are
identical to the correct parse trees in the Penn Tree-
bank. The number of these sentences is 451. The
average length of these sentences is 13.5 words.
We measured the delays and the precisions for va-
lidity thresholds 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0.
We define the degree of delay as follows: Let
s = w1 · · · wn be an input sentence and oj(s) be
the partial parse tree that is the output when the j-th
word wj is scanned. We define the degree of delay
when j-th word is scanned as follows:
</bodyText>
<equation confidence="0.675339">
D(j, s) = j − l(oj(s)) (7)
</equation>
<bodyText confidence="0.9992755">
We define maximum delay Dmax(s) and average
delay Dave(s) as follows:
</bodyText>
<equation confidence="0.997914">
Dmax(s) = max D(j, s) (8)
1�j�n
D(j, s) (9)
</equation>
<bodyText confidence="0.999761">
The precision is defined as the percentage of valid
partial parse trees in the output.
Moreover, we measured the precision of the pars-
ing whose delay is always 0 and which returns the
</bodyText>
<figure confidence="0.8892579">
#2
#3
#10
#12
#18
I
found
a
dime
in
the
wood
input word
output partial parse tree
=
1
Dave(s) =
n
n
E
</figure>
<page confidence="0.905046">
j=1
</page>
<tableCaption confidence="0.973698">
Table 4: Precisions and delays
</tableCaption>
<figure confidence="0.997766090909091">
precision(%) Dmax Dave
0 = 1.0 100.0 11.9 6.4
0 = 0.9 97.3 7.5 2.9
0 = 0.8 95.4 6.4 2.2
0 = 0.7 92.5 5.5 1.8
0 = 0.6 88.4 4.5 1.3
0 = 0.5 83.0 3.4 0.9
baseline 73.6 0.0 0.0
delay(number of words)
70 75 80 85 90 95 100
precision(%)
</figure>
<figureCaption confidence="0.999723">
Figure 4: Relation between precision and delay
</figureCaption>
<bodyText confidence="0.999511111111111">
partial parse tree having highest probability. We call
it the parsing baseline.
Table 4 shows the precisions and delays. Figure
4 illustrates the relation between the precisions and
delays.
The experimental result demonstrates that there
is a precision/delay trade-off. Our proposed method
increases the precision in comparison with the base-
line, while returning the output is delayed. When
0 = 1, it is guaranteed that the output partial parse
trees are valid, that is, our method is similar to the
method in the literature (Kato et al., 2000). In com-
parison with this case, our method when 0 &lt; 1 dra-
matically decreases the delay.
Although the result does not necessarily demon-
strates that our method is the best one, it achieves
both high-accuracy and short-delay to a certain ex-
tent.
</bodyText>
<sectionHeader confidence="0.993478" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999911052631579">
In this paper, we have proposed a method of evalu-
ating the validity that a partial parse tree constructed
in incremental parsing becomes valid. The method
is based on probabilistic incremental parsing. When
a word is scanned, the method incrementally calcu-
lates the validity for each partial parse tree and re-
turns the partial parse tree whose validity is greater
than a threshold. Our method delays the decision of
which partial parse tree should be returned.
To evaluate the performance of our method, we
conducted a parsing experiment using the Penn
Treebank. The experimental result shows that our
method improves the accuracy of incremental pars-
ing.
The experiment demonstrated a precision/delay
trade-off. To evaluate overall performance of in-
cremental parsing, we would like to investigate a
single measure into which delay and precision are
combined.
</bodyText>
<sectionHeader confidence="0.978685" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99937975">
This work is partially supported by the Grant-in-Aid
for Scientific Research of the Ministry of Education,
Science, Sports and Culture, Japan (No. 15300044),
and The Tatematsu Foundation.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997719484848485">
J. Allen, G. Ferguson, and A. Stent. 2001. An Ar-
chitecture for More Realistic Conversational Sys-
tems. In Proceedings of International Confer-
ence ofIntelligent User Interfaces, pages 1–8.
J. Carroll and D. Weir. 2003. Encoding Frequency
Information in Stochastic Parsing Models. In
R. Bod, R. Scha, and K. Sima’an, editors, Data-
Oriented Parsing, pages 43–60. CSLI Publica-
tions, Stanford.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings ofthe 6th International Workshop on
Parsing Technologies, pages 65–76.
D. Chiang. 2003. Statistical Parsing with an Auto-
matically Extracted Tree Adjoining Grammar. In
R. Bod, R. Scha, and K. Sima’an, editors, Data-
Oriented Parsing, pages 299–316. CSLI Publica-
tions, Stanford.
F. Costa, V. Lombardo, P. Frasconi, and Soda G.
2001. Wide Coverage Incremental Parsing by
Learning Attachment Preferences. In Proceed-
ings ofthe 7th Congress of the Italian Association
for Artificial Intelligence, pages 297–307.
N. J. Haddock. 1987. Incremental Interpretation
and Combinatory Categorial Grammar. In Pro-
ceedings of the 10th International Joint Confer-
ence on Artificial Intelligence, pages 661–663.
Y. Inagaki and S. Matsubara. 1995. Models for In-
cremental Interpretation of Natural Language. In
Proceedings of the 2nd Symposium on Natural
Language Processing, pages 51–60.
A. K. Joshi. 1985. Tree Adjoining Grammar: How
Much Context-Sensitivity is required to provide
</reference>
<figure confidence="0.987951">
14
12
10
✸
Dmax
Dave
×
×
×
✸
×
✸
×
✸
×
✸
2
4
6
8
baseline
0
✷
✸
×
✸
✷
</figure>
<reference confidence="0.994792255319149">
reasonable structural descriptions? In D. R.
Dowty, L. Karttunen, and A. Zwicky, editors,
Natural Language Parsing, pages 206–250. Cam-
bridge University Press, Cambridge.
Y. Kato, S. Matsubara, K. Toyama, and Y. Ina-
gaki. 2000. Spoken Language Parsing based on
Incremental Disambiguation. In Proceedings of
the 6th International Conference on Spoken Lan-
guage Processing, volume 2, pages 999–1002.
V. Lombardo and P. Sturt. 1997. Incremental Pro-
cessing and Infinite Local Ambiguity. In Pro-
ceedings of the 19th Annual Conference of the
Cognitive Science Siciety, pages 448–453.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a Large Anno-
tated Corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):310–330.
M Marcus. 1980. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press, Cam-
brige, MA.
S. Matsubara, S. Asai, K. Toyama, and Y. Inagaki.
1997. Chart-based Parsing and Transfer in In-
cremental Spoken Language Translation. In Pro-
ceedings of the 4th Natural Language Processing
Pacific Rim Symposium, pages 521–524.
D. Milward and R. Cooper. 1994. Incremental In-
terpretation: Applications, Theory, and Relation-
ship to Dynamic Semantics. In Proceedings of
the 15th International Conference on Computa-
tional Linguistics, pages 748–754.
D. Milward. 1995. Incremental Interpretation of
Categorial Grammar. In Proceedings of the 7th
Conference of European Chapter of the Associ-
ation for Computational Linguistics, pages 119–
126.
B. Roark and M. Johnson. 1999. Efficient Prob-
abilistic Top-down and Left-corner Parsing. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages
421–428.
B. Roark. 2001. Probabilistic Top-Down Parsing
and Language Modeling. Computational Lin-
guistics, 27(2):249–276.
F. Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed corpora. In Proceedings of
the 5th Natural Language Processing Pacific Rim
Symposium, pages 398–403.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892197">
<title confidence="0.9939415">Stochastically Evaluating the Validity of Partial Parse Trees Incremental Parsing</title>
<author confidence="0.98476">Shigeki</author>
<affiliation confidence="0.9787805">School of International Development, Nagoya University Technology Center, Nagoya University</affiliation>
<address confidence="0.999268">Furo-cho, Chikusa-ku, Nagoya, 464-8601</address>
<affiliation confidence="0.998733">of Information Science and Technology, Aichi Prefectural University</affiliation>
<address confidence="0.992431">1522-3 Ibaragabasama, Kumabari, Nagakute-cho, Aichi-gun, 480-1198</address>
<email confidence="0.982974">yosihide@gsid.nagoya-u.ac.jp</email>
<abstract confidence="0.998317363636364">This paper proposes a method for evaluating the validity of partial parse trees constructed in incremental parsing. Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>G Ferguson</author>
<author>A Stent</author>
</authors>
<title>An Architecture for More Realistic Conversational Systems.</title>
<date>2001</date>
<booktitle>In Proceedings of International Conference ofIntelligent User Interfaces,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1279" citStr="Allen et al., 2001" startWordPosition="174" endWordPosition="177">ng, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest o</context>
</contexts>
<marker>Allen, Ferguson, Stent, 2001</marker>
<rawString>J. Allen, G. Ferguson, and A. Stent. 2001. An Architecture for More Realistic Conversational Systems. In Proceedings of International Conference ofIntelligent User Interfaces, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>D Weir</author>
</authors>
<title>Encoding Frequency Information in Stochastic Parsing Models. In</title>
<date>2003</date>
<booktitle>DataOriented Parsing,</booktitle>
<pages>43--60</pages>
<editor>R. Bod, R. Scha, and K. Sima’an, editors,</editor>
<publisher>CSLI Publications, Stanford.</publisher>
<contexts>
<context position="11741" citStr="Carroll and Weir, 2003" startWordPosition="1885" endWordPosition="1888">entary trees are combined at each node depends only on the nonterminal symbol of that node 2. The probability of a parse tree is calculated by the product of the probability of the operations which are used in construction of the parse tree. For example, the probability of each operation is given as shown in Table 2. The probability of the partial parse tree #12, which is constructed by using sα1, sα2, sαy, sα7, nilNP and a02, is 1 x 0.7 x 0.3 x 0.5 x 0.7 x 0.7 = 0.05145. We write P(σ) for the probability of a partial parse tree σ. 2The PITAG formalism corresponds to SLG(1) in the literature (Carroll and Weir, 2003). Table 2: Probability of operations 2.4 Parsing Strategies In order to improve the efficiency of the parsing, we adapt two parsing strategies as follows: • If two partial parse trees have the same sequence of nodes to which ITAG operations are applicable, then the lower probability tree can be safely discarded. • The parser only keeps n-best partial parse trees. 3 Validity of Partial Parse Trees This section gives some definitions about the validity of a partial parse tree. Before describing the validity of a partial parse tree, we define the subsumption relation between partial parse trees. </context>
</contexts>
<marker>Carroll, Weir, 2003</marker>
<rawString>J. Carroll and D. Weir. 2003. Encoding Frequency Information in Stochastic Parsing Models. In R. Bod, R. Scha, and K. Sima’an, editors, DataOriented Parsing, pages 43–60. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated Extraction of TAGs from the Penn Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings ofthe 6th International Workshop on Parsing Technologies,</booktitle>
<pages>65--76</pages>
<contexts>
<context position="7512" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="1228" endWordPosition="1231">nal symbol as η1, split the parse tree at η1 and ηp, and combine the upper tree and the lower tree. η1 of intermediate tree is a foot node. • for each node η2 having only one left-sibling, if the parent ηp does not have the same nonterminal symbol as the left-sibling η1 of η2, split the parse tree at η2. • for the other node η in the parse tree, split the parse tree at η. For example, The initial trees α1, α2, α5, α7 αs and α10 and the auxiliary tree β2 are extracted from the parse tree #18 in Table 1. Our proposed tree extraction is similar to the TAG extractions proposed in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999). The main difference between these methods is the position of nodes at which parse trees are split. While the methods in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) utilize a head percolation rule to split the parse trees at complement nodes, our method splits the parse trees a a 10 NN wood 5 NP b NP 7 NN 8 DT NN DT JJ NN dime NP NP 9 DT JJ NN the DT NN the ADJP VB NP found Initial trees: 1 S 2 NP VP PRP I VB NP found VP 3 VP 4 VP VB found Auxiliary trees: 1 NP 2 NP* PP VP* PP IN NP in IN NP in VP Table 1: Incremental parsing p</context>
</contexts>
<marker>Chen, Vijay-Shanker, 2000</marker>
<rawString>J. Chen and K. Vijay-Shanker. 2000. Automated Extraction of TAGs from the Penn Treebank. In Proceedings ofthe 6th International Workshop on Parsing Technologies, pages 65–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Statistical Parsing with an Automatically Extracted Tree Adjoining Grammar. In</title>
<date>2003</date>
<booktitle>DataOriented Parsing,</booktitle>
<pages>299--316</pages>
<editor>R. Bod, R. Scha, and K. Sima’an, editors,</editor>
<publisher>CSLI Publications, Stanford.</publisher>
<contexts>
<context position="7526" citStr="Chiang, 2003" startWordPosition="1232" endWordPosition="1233">rse tree at η1 and ηp, and combine the upper tree and the lower tree. η1 of intermediate tree is a foot node. • for each node η2 having only one left-sibling, if the parent ηp does not have the same nonterminal symbol as the left-sibling η1 of η2, split the parse tree at η2. • for the other node η in the parse tree, split the parse tree at η. For example, The initial trees α1, α2, α5, α7 αs and α10 and the auxiliary tree β2 are extracted from the parse tree #18 in Table 1. Our proposed tree extraction is similar to the TAG extractions proposed in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999). The main difference between these methods is the position of nodes at which parse trees are split. While the methods in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) utilize a head percolation rule to split the parse trees at complement nodes, our method splits the parse trees a a 10 NN wood 5 NP b NP 7 NN 8 DT NN DT JJ NN dime NP NP 9 DT JJ NN the DT NN the ADJP VB NP found Initial trees: 1 S 2 NP VP PRP I VB NP found VP 3 VP 4 VP VB found Auxiliary trees: 1 NP 2 NP* PP VP* PP IN NP in IN NP in VP Table 1: Incremental parsing process of “I f</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>D. Chiang. 2003. Statistical Parsing with an Automatically Extracted Tree Adjoining Grammar. In R. Bod, R. Scha, and K. Sima’an, editors, DataOriented Parsing, pages 299–316. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Costa</author>
<author>V Lombardo</author>
<author>P Frasconi</author>
<author>G Soda</author>
</authors>
<title>Wide Coverage Incremental Parsing by Learning Attachment Preferences.</title>
<date>2001</date>
<booktitle>In Proceedings ofthe 7th Congress of the Italian Association for Artificial Intelligence,</booktitle>
<pages>297--307</pages>
<contexts>
<context position="1503" citStr="Costa et al., 2001" startWordPosition="212" endWordPosition="215">omes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing w</context>
</contexts>
<marker>Costa, Lombardo, Frasconi, Soda, 2001</marker>
<rawString>F. Costa, V. Lombardo, P. Frasconi, and Soda G. 2001. Wide Coverage Incremental Parsing by Learning Attachment Preferences. In Proceedings ofthe 7th Congress of the Italian Association for Artificial Intelligence, pages 297–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Haddock</author>
</authors>
<title>Incremental Interpretation and Combinatory Categorial Grammar.</title>
<date>1987</date>
<booktitle>In Proceedings of the 10th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>661--663</pages>
<contexts>
<context position="1518" citStr="Haddock, 1987" startWordPosition="216" endWordPosition="217">threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the</context>
</contexts>
<marker>Haddock, 1987</marker>
<rawString>N. J. Haddock. 1987. Incremental Interpretation and Combinatory Categorial Grammar. In Proceedings of the 10th International Joint Conference on Artificial Intelligence, pages 661–663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Inagaki</author>
<author>S Matsubara</author>
</authors>
<title>Models for Incremental Interpretation of Natural Language.</title>
<date>1995</date>
<booktitle>In Proceedings of the 2nd Symposium on Natural Language Processing,</booktitle>
<pages>51--60</pages>
<contexts>
<context position="1308" citStr="Inagaki and Matsubara, 1995" startWordPosition="178" endWordPosition="181">ally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the </context>
</contexts>
<marker>Inagaki, Matsubara, 1995</marker>
<rawString>Y. Inagaki and S. Matsubara. 1995. Models for Incremental Interpretation of Natural Language. In Proceedings of the 2nd Symposium on Natural Language Processing, pages 51–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>Tree Adjoining Grammar: How Much Context-Sensitivity is required to provide reasonable structural descriptions? In</title>
<date>1985</date>
<booktitle>Natural Language Parsing,</booktitle>
<pages>206--250</pages>
<editor>D. R. Dowty, L. Karttunen, and A. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3754" citStr="Joshi, 1985" startWordPosition="562" endWordPosition="563">il the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. This paper is organized as follows: The next section proposes a probabilistic incremental parser. Section 3 discusses the validity of partial parse tree constructed in incremental parsing. Section 4 proposes a method of incrementally evaluating the validity of partial parse tree. In section 5, we report an experimental evaluation of our method. 2 TAG-based Incremental Parsing Our incremental parsing is based on tree adjoining grammar (TAG) (Joshi, 1985). This section proposes a TAG-based incremental parsing method. 2.1 TAG for Incremental Parsing Firstly, we propose incremental-parsing-oriented TAG (ITAG). An ITAG comprises two sets of elementary trees just like TAG: initial trees and auxiliary trees. The difference between ITAG and TAG is the form of elementary trees. Every ITAG initial tree is leftmost-expanded. A tree is leftmostexpanded if it is of the following forms: 1. [t]X, where t is a terminal symbol and X is a nonterminal symbol. Figure 1: Examples of ITAG elementary trees 2. [σX1 · · · Xk]X, where σ is a leftmost expanded tree, X</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>A. K. Joshi. 1985. Tree Adjoining Grammar: How Much Context-Sensitivity is required to provide reasonable structural descriptions? In D. R. Dowty, L. Karttunen, and A. Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kato</author>
<author>S Matsubara</author>
<author>K Toyama</author>
<author>Y Inagaki</author>
</authors>
<title>Spoken Language Parsing based on Incremental Disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>999--1002</pages>
<contexts>
<context position="2176" citStr="Kato et al., 2000" startWordPosition="313" endWordPosition="316"> Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid partial parse trees (Kato et al., 2000). However, it is hard to say that these methods realize broad-coverage incremental parsing. The method in the literature (Marcus, 1980) uses lookahead rules, which are constructed by hand, but it is not clear whether broad coverage lookahead rules can be obtained. The incremental parsing in the literature (Kato et al., 2000), which is based on context free grammar, is infeasible to deal with large scale grammar, because the parser exhaustively searches all candidate partial parse trees in top-down fashion. This paper proposes a probabilistic incremental parser which evaluates the validity of p</context>
<context position="15298" citStr="Kato et al., 2000" startWordPosition="2580" endWordPosition="2583"> a,a2 nilNP nilvP found a dime #2 #3 #6 in the wood #10 #12 #14 #18 I #1 #7 #15 #4 #8 #11 #13 #16 #19 #17 #9 #5 subsumption relation found a dime #2 #3 #6 in the wood #10 #12 #14 I #1 #18 #7 #15 #4 #8 #11 #13 #16 #19 #17 #9 valid partial parse tree #5 subsumption relation subsumed by Q is valid. The equation 5 is the ratio of such partial parse trees to the constructed partial parse trees. 4.1 Output Partial Parse Trees Kato et al. proposed a method of delaying the decision of which partial parse trees should be returned as the output, until the validity of partial parse trees are guaranteed (Kato et al., 2000). The idea of delaying the decision of the output is interesting. However, delaying the decision until the validity are guaranteed may cause the loss of incrementality of the parsing. To solve the problem, in our method, the incremental parser returns high validity partial parse trees rather than validity guaranteed partial parse trees. When the j-th word wj is scanned, our incremental parser returns the following partial parse: argmax{σ:V (σ,w1···wj)&gt;θ1l(Q) (6) where 0 is a threshold between [0, 1] and l(Q) is the length of the initial fragment which is yielded by Q. The output partial parse </context>
<context position="19803" citStr="Kato et al., 2000" startWordPosition="3398" endWordPosition="3401"> 85 90 95 100 precision(%) Figure 4: Relation between precision and delay partial parse tree having highest probability. We call it the parsing baseline. Table 4 shows the precisions and delays. Figure 4 illustrates the relation between the precisions and delays. The experimental result demonstrates that there is a precision/delay trade-off. Our proposed method increases the precision in comparison with the baseline, while returning the output is delayed. When 0 = 1, it is guaranteed that the output partial parse trees are valid, that is, our method is similar to the method in the literature (Kato et al., 2000). In comparison with this case, our method when 0 &lt; 1 dramatically decreases the delay. Although the result does not necessarily demonstrates that our method is the best one, it achieves both high-accuracy and short-delay to a certain extent. 6 Concluding Remarks In this paper, we have proposed a method of evaluating the validity that a partial parse tree constructed in incremental parsing becomes valid. The method is based on probabilistic incremental parsing. When a word is scanned, the method incrementally calculates the validity for each partial parse tree and returns the partial parse tre</context>
</contexts>
<marker>Kato, Matsubara, Toyama, Inagaki, 2000</marker>
<rawString>Y. Kato, S. Matsubara, K. Toyama, and Y. Inagaki. 2000. Spoken Language Parsing based on Incremental Disambiguation. In Proceedings of the 6th International Conference on Spoken Language Processing, volume 2, pages 999–1002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lombardo</author>
<author>P Sturt</author>
</authors>
<title>Incremental Processing and Infinite Local Ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 19th Annual Conference of the Cognitive Science Siciety,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="5600" citStr="Lombardo and Sturt, 1997" startWordPosition="879" endWordPosition="882">t of applying sα to σ. adjunction The adjunction operation splits a partial parse tree σ at a nonterminal node having no nonterminal leaf, and inserts an auxiliary tree β having the same nonterminal symbol at its root. We write ao for the operation of adjoining β and ao(σ) for the result of applying ao to σ. The substitution operation is similar to rule expansion of top-down incremental parsing such as (Matsubara et al., 1997; Roark, 2001). Furthermore, by introducing the adjunction operation to incremental parsing, we can expect that local ambiguity of left-recursive structures is decreased (Lombardo and Sturt, 1997). Our proposed incremental parsing is based on ITAG. When i-th word wi is scanned, the parser combines elementary trees for wi with partial parse trees for w1 · · · wi_1 to construct the partial parse trees for w1 · · · wi_1wi. As an example, let us consider incremental parsing of the following sentence by using ITAG shown in Figure 1: I found a dime in the wood. (1) Table 1 shows the process of tree construction for the sentence (1). When the word “found” is scanned, partial parse trees #3, #4 and #5 are constructed by applying substitution operations to partial parse tree #2 for the initial </context>
</contexts>
<marker>Lombardo, Sturt, 1997</marker>
<rawString>V. Lombardo and P. Sturt. 1997. Incremental Processing and Infinite Local Ambiguity. In Proceedings of the 19th Annual Conference of the Cognitive Science Siciety, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="17698" citStr="Marcus et al., 1993" startWordPosition="3006" endWordPosition="3009">ry word input. Our incremental parser delays the decision of the output as shown in this example. Table 3: Output partial parse trees 5 Experimental Results To evaluate the performance of our proposed method, we performed a parsing experiment. The parser was implemented in GNU Common Lisp on a Linux PC. In the experiment, the inputs of the incremental parser are POS sequences rather than word sequences. We used 47247 initial trees and 2931 auxiliary trees for the experiment. The elementary trees were extracted from the parse trees in sections 02-21 of the Wall Street Journal in Penn Treebank (Marcus et al., 1993), which is transformed by using parent-child annotation and left factoring (Roark and Johnson, 1999). We set the beam-width at 500. The labeled precision and recall of the parsing are 80.8% and 78.5%, respectively for the section 23 in Penn Treebank. We used the set of sentences for which the outputs of the incremental parser are identical to the correct parse trees in the Penn Treebank. The number of these sentences is 451. The average length of these sentences is 13.5 words. We measured the delays and the precisions for validity thresholds 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0. We define the degre</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2):310–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambrige, MA.</location>
<contexts>
<context position="2050" citStr="Marcus, 1980" startWordPosition="294" endWordPosition="295">tal parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid partial parse trees (Kato et al., 2000). However, it is hard to say that these methods realize broad-coverage incremental parsing. The method in the literature (Marcus, 1980) uses lookahead rules, which are constructed by hand, but it is not clear whether broad coverage lookahead rules can be obtained. The incremental parsing in the literature (Kato et al., 2000), which is based on context free grammar, is infeasible to deal with large scale grammar, because the parser exhaustively searches all candidate par</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>M Marcus. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambrige, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Matsubara</author>
<author>S Asai</author>
<author>K Toyama</author>
<author>Y Inagaki</author>
</authors>
<title>Chart-based Parsing and Transfer in Incremental Spoken Language Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 4th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>521--524</pages>
<contexts>
<context position="1542" citStr="Matsubara et al., 1997" startWordPosition="218" endWordPosition="221">technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid parti</context>
<context position="5404" citStr="Matsubara et al., 1997" startWordPosition="851" endWordPosition="855">eftmost nonterminal leaf of a partial parse tree σ with an initial tree α having the same nonterminal symbol at its root. We write sα for the operation of substituting α and sα(σ) for the result of applying sα to σ. adjunction The adjunction operation splits a partial parse tree σ at a nonterminal node having no nonterminal leaf, and inserts an auxiliary tree β having the same nonterminal symbol at its root. We write ao for the operation of adjoining β and ao(σ) for the result of applying ao to σ. The substitution operation is similar to rule expansion of top-down incremental parsing such as (Matsubara et al., 1997; Roark, 2001). Furthermore, by introducing the adjunction operation to incremental parsing, we can expect that local ambiguity of left-recursive structures is decreased (Lombardo and Sturt, 1997). Our proposed incremental parsing is based on ITAG. When i-th word wi is scanned, the parser combines elementary trees for wi with partial parse trees for w1 · · · wi_1 to construct the partial parse trees for w1 · · · wi_1wi. As an example, let us consider incremental parsing of the following sentence by using ITAG shown in Figure 1: I found a dime in the wood. (1) Table 1 shows the process of tree </context>
</contexts>
<marker>Matsubara, Asai, Toyama, Inagaki, 1997</marker>
<rawString>S. Matsubara, S. Asai, K. Toyama, and Y. Inagaki. 1997. Chart-based Parsing and Transfer in Incremental Spoken Language Translation. In Proceedings of the 4th Natural Language Processing Pacific Rim Symposium, pages 521–524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
<author>R Cooper</author>
</authors>
<title>Incremental Interpretation: Applications, Theory, and Relationship to Dynamic Semantics.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>748--754</pages>
<contexts>
<context position="1335" citStr="Milward and Cooper, 1994" startWordPosition="182" endWordPosition="185">or each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed</context>
</contexts>
<marker>Milward, Cooper, 1994</marker>
<rawString>D. Milward and R. Cooper. 1994. Incremental Interpretation: Applications, Theory, and Relationship to Dynamic Semantics. In Proceedings of the 15th International Conference on Computational Linguistics, pages 748–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Incremental Interpretation of Categorial Grammar.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>119--126</pages>
<contexts>
<context position="1557" citStr="Milward, 1995" startWordPosition="222" endWordPosition="223">or improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid partial parse trees </context>
</contexts>
<marker>Milward, 1995</marker>
<rawString>D. Milward. 1995. Incremental Interpretation of Categorial Grammar. In Proceedings of the 7th Conference of European Chapter of the Association for Computational Linguistics, pages 119– 126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Johnson</author>
</authors>
<title>Efficient Probabilistic Top-down and Left-corner Parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>421--428</pages>
<contexts>
<context position="17798" citStr="Roark and Johnson, 1999" startWordPosition="3020" endWordPosition="3023">. Table 3: Output partial parse trees 5 Experimental Results To evaluate the performance of our proposed method, we performed a parsing experiment. The parser was implemented in GNU Common Lisp on a Linux PC. In the experiment, the inputs of the incremental parser are POS sequences rather than word sequences. We used 47247 initial trees and 2931 auxiliary trees for the experiment. The elementary trees were extracted from the parse trees in sections 02-21 of the Wall Street Journal in Penn Treebank (Marcus et al., 1993), which is transformed by using parent-child annotation and left factoring (Roark and Johnson, 1999). We set the beam-width at 500. The labeled precision and recall of the parsing are 80.8% and 78.5%, respectively for the section 23 in Penn Treebank. We used the set of sentences for which the outputs of the incremental parser are identical to the correct parse trees in the Penn Treebank. The number of these sentences is 451. The average length of these sentences is 13.5 words. We measured the delays and the precisions for validity thresholds 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0. We define the degree of delay as follows: Let s = w1 · · · wn be an input sentence and oj(s) be the partial parse tree </context>
</contexts>
<marker>Roark, Johnson, 1999</marker>
<rawString>B. Roark and M. Johnson. 1999. Efficient Probabilistic Top-down and Left-corner Parsing. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 421–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic Top-Down Parsing and Language Modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1571" citStr="Roark, 2001" startWordPosition="224" endWordPosition="225">e accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid partial parse trees (Kato et al., </context>
<context position="5418" citStr="Roark, 2001" startWordPosition="856" endWordPosition="857"> of a partial parse tree σ with an initial tree α having the same nonterminal symbol at its root. We write sα for the operation of substituting α and sα(σ) for the result of applying sα to σ. adjunction The adjunction operation splits a partial parse tree σ at a nonterminal node having no nonterminal leaf, and inserts an auxiliary tree β having the same nonterminal symbol at its root. We write ao for the operation of adjoining β and ao(σ) for the result of applying ao to σ. The substitution operation is similar to rule expansion of top-down incremental parsing such as (Matsubara et al., 1997; Roark, 2001). Furthermore, by introducing the adjunction operation to incremental parsing, we can expect that local ambiguity of left-recursive structures is decreased (Lombardo and Sturt, 1997). Our proposed incremental parsing is based on ITAG. When i-th word wi is scanned, the parser combines elementary trees for wi with partial parse trees for w1 · · · wi_1 to construct the partial parse trees for w1 · · · wi_1wi. As an example, let us consider incremental parsing of the following sentence by using ITAG shown in Figure 1: I found a dime in the wood. (1) Table 1 shows the process of tree construction f</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic Top-Down Parsing and Language Modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
</authors>
<title>Extracting Tree Adjoining Grammars from Bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>398--403</pages>
<contexts>
<context position="7538" citStr="Xia, 1999" startWordPosition="1234" endWordPosition="1235"> and ηp, and combine the upper tree and the lower tree. η1 of intermediate tree is a foot node. • for each node η2 having only one left-sibling, if the parent ηp does not have the same nonterminal symbol as the left-sibling η1 of η2, split the parse tree at η2. • for the other node η in the parse tree, split the parse tree at η. For example, The initial trees α1, α2, α5, α7 αs and α10 and the auxiliary tree β2 are extracted from the parse tree #18 in Table 1. Our proposed tree extraction is similar to the TAG extractions proposed in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999). The main difference between these methods is the position of nodes at which parse trees are split. While the methods in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) utilize a head percolation rule to split the parse trees at complement nodes, our method splits the parse trees a a 10 NN wood 5 NP b NP 7 NN 8 DT NN DT JJ NN dime NP NP 9 DT JJ NN the DT NN the ADJP VB NP found Initial trees: 1 S 2 NP VP PRP I VB NP found VP 3 VP 4 VP VB found Auxiliary trees: 1 NP 2 NP* PP VP* PP IN NP in IN NP in VP Table 1: Incremental parsing process of “I found a dime </context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>F. Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed corpora. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium, pages 398–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>