<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978427">
Some Tests of an Unsupervised Model of Language Acquisition
</title>
<author confidence="0.998309">
Bo Pedersen and Shimon Edelman
</author>
<affiliation confidence="0.877162">
Department of Psychology
Cornell University
Ithaca, NY 14853, USA
</affiliation>
<email confidence="0.991103">
{bp64,se37}@cornell.edu
</email>
<author confidence="0.973908">
Zach Solan, David Horn, Eytan Ruppin
</author>
<affiliation confidence="0.943421">
Faculty of Exact Sciences
Tel Aviv University
</affiliation>
<address confidence="0.792784">
Tel Aviv, Israel 69978
</address>
<email confidence="0.994234">
{zsolan,horn,ruppin}@post.tau.ac.il
</email>
<sectionHeader confidence="0.982801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.986107580246914">
We outline an unsupervised language acquisition
algorithm and offer some psycholinguistic support
for a model based on it. Our approach resem-
bles the Construction Grammar in its general phi-
losophy, and the Tree Adjoining Grammar in its
computational characteristics. The model is trained
on a corpus of transcribed child-directed speech
(CHILDES). The model’s ability to process novel
inputs makes it capable of taking various standard
tests of English that rely on forced-choice judgment
and on magnitude estimation of linguistic accept-
ability. We report encouraging results from several
such tests, and discuss the limitations revealed by
other tests in our present method of dealing with
novel stimuli.
1 The empirical problem of language
acquisition
The largely unsupervised, amazingly fast and al-
most invariably successful learning stint that is lan-
guage acquisition by children has long been the
envy of computer scientists (Bod, 1998; Clark,
2001; Roberts and Atwell, 2002) and a daunting
enigma for linguists (Chomsky, 1986; Elman et al.,
1996). Computational models of language acqui-
sition or “grammar induction” are usually divided
into two categories, depending on whether they sub-
scribe to the classical generative theory of syn-
tax, or invoke “general-purpose” statistical learning
mechanisms. We believe that polarization between
classical and statistical approaches to syntax ham-
pers the integration of the stronger aspects of each
method into a common powerful framework. On
the one hand, the statistical approach is geared to
take advantage of the considerable progress made
to date in the areas of distributed representation
and probabilistic learning, yet generic “connection-
ist” architectures are ill-suited to the abstraction
and processing of symbolic information. On the
other hand, classical rule-based systems excel in
just those tasks, yet are brittle and difficult to train.
We are developing an approach to the acquisi-
tion of distributional information from raw input
(e.g., transcribed speech corpora) that also supports
the distillation of structural regularities comparable
to those captured by Context Sensitive Grammars
out of the accrued statistical knowledge. In think-
ing about such regularities, we adopt Langacker’s
notion of grammar as “simply an inventory of lin-
guistic units” ((Langacker, 1987), p.63). To de-
tect potentially useful units, we identify and pro-
cess partially redundant sentences that share the
same word sequences. We note that the detection
of paradigmatic variation within a slot in a set of
otherwise identical aligned sequences (syntagms) is
the basis for the classical distributional theory of
language (Harris, 1954), as well as for some mod-
ern work (van Zaanen, 2000). Likewise, the pat-
tern — the syntagm and the equivalence class of
complementary-distribution symbols that may ap-
pear in its open slot — is the main representational
building block of our system, ADIOS (for Automatic
DIstillation Of Structure).
Our goal in the present short paper is to illus-
trate some of the capabilities of the representa-
tions learned by our method vis a vis standard tests
used by developmental psychologists, by second-
language instructors, and by linguists. Thus, the
main computational principles behind the ADIOS
model are outlined here only briefly. The algo-
rithmic details of our approach and accounts of its
learning from CHILDES corpora appear elsewhere
(Solan et al., 2003a; Solan et al., 2003b; Solan et al.,
2004; Edelman et al., 2004).
2 The principles behind the ADIOS
algorithm
The representational power of ADIOS and its capac-
ity for unsupervised learning rest on three princi-
ples: (1) probabilistic inference of pattern signifi-
cance, (2) context-sensitive generalization, and (3)
recursive construction of complex patterns. Each of
these is described briefly below.
</bodyText>
<figure confidence="0.989231893939394">
77
84 0.0001
58
63
51
50
48
that
Beth
Cindy
George
Jim
Joe
Pam
a
bird
cat
cow
dog
horse
rabbit
r the
ird
L bcat
cow
dog
I horse
I rabbit
L flies
jumps
• laughs
annoyes
bothers
I disturbs
&apos; I worries
Beth
Cindy
George
Jim
Joe Pam
who
adores
I loves
i scolds
I worships
p Beth
Cindy
George
&apos; Jim
Joe
L Pam
1 a
bird
cat
I cow
dog
horse
rabbit
the
bird
cat
cow
dog
horse
rabbit,
doesn&apos;t
</figure>
<equation confidence="0.862184545454545">
it
64
49 51
50 50 60 85 53 62
that the bird jumps disturbes Jim who adores the cat, doesn&apos;t it?
P84 &amp;quot;that&amp;quot; P58 P63
E63 E64 P48
E64 &amp;quot;Beth&amp;quot;  |&amp;quot;Cindy&amp;quot;  |&amp;quot;George&amp;quot;  |&amp;quot;Jim&amp;quot;  |&amp;quot;Joe&amp;quot;  |&amp;quot;Pam&amp;quot;  |P49  |P51
P48 &amp;quot; &amp;quot; &amp;quot;doesn&apos;t&amp;quot; &amp;quot;it&amp;quot;
P51 &amp;quot;the&amp;quot; E50
P49 &amp;quot;a&amp;quot; E50
E50 &amp;quot;bird&amp;quot;  |&amp;quot;cat&amp;quot;  |&amp;quot;cow&amp;quot;  |&amp;quot;dog&amp;quot;  |&amp;quot;horse&amp;quot;  |&amp;quot;rabbit&amp;quot;
P61 &amp;quot;who&amp;quot; E62
E62 &amp;quot;adores&amp;quot;  |&amp;quot;loves&amp;quot;  |&amp;quot;scolds&amp;quot;  |&amp;quot;worships&amp;quot;
E53 &amp;quot;Beth&amp;quot;  |&amp;quot;Cindy&amp;quot;  |&amp;quot;George&amp;quot;  |&amp;quot;Jim&amp;quot;  |&amp;quot;Joe&amp;quot;  |&amp;quot;Pam&amp;quot;
E85 &amp;quot;annoyes&amp;quot;  |&amp;quot;bothers&amp;quot;  |&amp;quot;disturbes&amp;quot;  |&amp;quot;worries&amp;quot;
P58 E60 E64
2
E60 &amp;quot;flies&amp;quot;  |&amp;quot;Jumps&amp;quot;  |&amp;quot;laughs&amp;quot;
64
61
49
50
Figure 1: Left: a pattern (presented in a tree form), capturing a long range dependency (equivalence class
55 84
labels are underscored). This and other examples here were distilled from a 400-sentence corpus generated
by a 40-rule Context Free Grammar. Right: the same pattern recast as a set of rewriting rules that can be
that P58 P63
E64 48
seen as a Context Free Grammar fragment.
P210 P55 P84
BEGIN P55 P84 BEGIN E56 &amp;quot;thinks&amp;quot; &amp;quot;that&amp;quot; P84
P55 P84 P178 P55 E75 &amp;quot;thinks&amp;quot; &amp;quot;that&amp;quot; P178
</equation>
<figure confidence="0.987982145833333">
BEGIN
Beth
Cindy
Pam
56 75 75
Geo
55 84
thinks
that
Beth
Cindy
George
Jim
Joe
Pam
210
bbit
thinks
that
Agreement
Beth
Cindy
George
Jim
Joe
Pam
barks
meows
and
a
the
bird
cat
cow
dog
horse
rabbit
flies
jumps
laughs
,
doesn&apos;t
she
?
73 89 109 65
92
178
114
</figure>
<figureCaption confidence="0.818728">
Figure 2: Left: because ADIOS does not rewire all the occurrences of a specific pattern, but only those that
</figureCaption>
<figure confidence="0.92001725">
P Bh d J k tht J n th Grg
share the same context, its power is comparable to that of Context Sensitive Grammars. In this example,
Pm th and Jm think that Jo thinks that George think that Cindy beleves that Jim who dors a
equivalence class #75 is not extended to subsume the subject position, because that position appears in
eo d , d y
cat meows and the bir lies , dont ty?
0027
ht Pa
a different context (e.g., immediately to the right of the symbol BEGIN). Thus, long-range agreement is
4 tat Pam laughs worries a og , doesnt it?
that a cow jumps diurbs Jim who loves horse ,
d
enforced and over-generalization prevented. Right: the context-sensitive “ rules” corresponding to pattern
70 that a cow jum disturbs Jm who loves a horse ,
the
#210.
meo
they?
o
that
Beth pese
ndy
da Jim
m s is
asy hto
to o
ple yes
d rie
Probabilistic inference of pattern significance.
Beth thinks that Jim believe hat Beth who loves a 55 71 53 67
ADIOS represents a corpus of sentences as an ini-
that Pam tough to please worries th cat
</figure>
<bodyText confidence="0.9998422">
tially highly redundant directed graph, which can be
informally visualized as a tangle of strands that are
partially segregated into bundles. Each of these con-
sists of some strands clumped together; a bundle is
formed when two or more strands join together and
</bodyText>
<equation confidence="0.451353">
53 53
</equation>
<bodyText confidence="0.999978">
run in parallel and is dissolved when more strands
leave the bundle than stay in. In a given corpus,
there will be many bundles, with each strand (sen-
tence) possibly participating in several. Our algo-
rithm, described in detail in (Solan et al., 2004),
identifies significant bundles that balance high com-
pression (small size of the bundle “lexicon”) against
good generalization (the ability to generate new
grammatical sentences by splicing together various
strand fragments each of which belongs to a differ-
ent bundle).
Context sensitivity of patterns. A pattern is an
abstraction of a bundle of sentences that are identi-
cal up to variation in one place, where one of several
symbols — the members of the equivalence class
associated with the pattern — may appear (Fig-
</bodyText>
<equation confidence="0.84906875">
eorg
C
Jim
eay
</equation>
<bodyText confidence="0.935436275862069">
ure 1). Because this variation is only allowed in
they?
the context specified by the pattern, the generaliza-
that Joe is eager to please disurbs the bird
tion afforded by a set of patterns is inherently safer
Cindy hink that Jm beeves that to ead is tough
than in approaches that posit globally valid cate-
Beth thik tht J blive tht eth h lv
gories (“parts of speech”) and rules (“grammar”).
horse meows and the horse umps doesn&apos;t sh e?
The reliance of ADIOS on many context-sensitive o
that Pm is ough to plese worries the at
patterns rather than on traditional rules can be com-
pared both to the Construction Grammar (discussed
later) and to Langacker’s concept of the grammar as
a collection of “patterns of all intermediate degrees
of generality” ((Langacker, 1987), p.46).
Hierarchical structure of patterns. The ADIOS
graph is rewired every time a new pattern is de-
tected, so that a bundle of strings subsumed by it
is represented by a single new edge. Following the
rewiring, which is context-specific, potentially far-
apart symbols that used to straddle the newly ab-
stracted pattern become close neighbors. Patterns
thus become hierarchically structured in that their
elements may be either terminals (i.e., fully speci-
fied strings) or other patterns. Moreover, patterns
may refer to themselves, which opens the door for
recursion.
</bodyText>
<figure confidence="0.9813104375">
�
eorge
Joe
ager
hgu
ease
dear
hers
durbs
Ji
C�
H
c
c
a
78
</figure>
<sectionHeader confidence="0.6318165" genericHeader="keywords">
3 Related computational and linguistic
formalisms and psycholinguistic findings
</sectionHeader>
<bodyText confidence="0.999712396226415">
Unlike ADIOS, very few existing algorithms for un-
supervised language acquisition use raw, unanno-
tated corpus data (as opposed, say, to sentences con-
verted into sequences of POS tags). The only work
described in a recent review (Roberts and Atwell,
2002) as completely unsupervised — the GraSp
model (Henrichsen, 2002) — does attempt to in-
duce syntax from raw transcribed speech, yet it is
not completely data-driven in that it makes a prior
commitment to a particular theory of syntax (Cate-
gorial Grammar, complete with a pre-specified set
of allowed categories). Because of the unique na-
ture of our chosen challenge — finding structure
in language rather than imposing it — the follow-
ing brief survey of grammar induction focuses on
contrasts and comparisons to approaches that gen-
erally stop short of attempting to do what our al-
gorithm does. We distinguish between approaches
that are motivated computationally (Local Grammar
and Variable Order Markov models, and Tree Ad-
joining Grammar, discussed elsewhere (Edelman et
al., 2004), and those whose main motivation is lin-
guistic and cognitive psychological (Cognitive and
Construction grammars, discussed below).
Local Grammar and Markov models. In cap-
turing the regularities inherent in multiple criss-
crossing paths through a corpus, ADIOS su-
perficially resembles finite-state Local Grammars
(Gross, 1997) and Variable Order Markov (VOM)
models (Guyon and Pereira, 1995). The VOM ap-
proach starts by postulating a maximum-n struc-
ture, which is then fitted to the data by maximizing
the likelihood of the training corpus. The ADIOS
philosophy differs from the VOM approach in sev-
eral key respects. First, rather than fitting a model
to the data, we use the data to construct a (recur-
sively structured) graph. Thus, our algorithm nat-
urally addresses the inference of the graph’s struc-
ture, a task that is more difficult than the estima-
tion of parameters for a given configuration. Sec-
ond, because ADIOS works from the bottom up in a
recursive, data-driven fashion, it is less susceptible
to complexity issues. It can be used on huge graphs,
and may yield very large patterns, which in a VOM
model would correspond to an unmanageably high
order n. Third, ADIOS transcends the idea of VOM
structure, in the following sense. Consider a set of
patterns of the form b1[c1]b2[c2]b3, etc. The equiv-
alence classes [] may include vertices of the graph
(both words and word patterns turned into nodes),
wild cards (i.e., any node), as well as ambivalent
cards (any node or no node). This means that the
terminal-level length of the string represented by
a pattern does not have to be of a fixed length.
This goes conceptually beyond the variable order
Markov structure: b2[c2]b3 do not have to appear in
a Markov chain of a finite order ||b2||+||c2||+||b3||
because the size of [c2] is ill-defined, as explained
above. Fourth, as we showed earlier (Figure 2),
ADIOS incorporates both context-sensitive substitu-
tion and recursion.
Tree Adjoining Grammar. The proper place in
the Chomsky hierarchy for the class of strings ac-
cepted by our model is between Context Free and
Context Sensitive Languages. The pattern-based
representations employed by ADIOS have counter-
parts for each of the two composition operations,
substitution and adjoining, that characterize a Tree
Adjoining Grammar, or TAG, developed by Joshi
and others (Joshi and Schabes, 1997). Specifically,
both substitution and adjoining are subsumed in the
relationships that hold among ADIOS patterns, such
as the membership of one pattern in another. Con-
sider a pattern PZ and its equivalence class £(PZ);
any other pattern P; E £(PZ) can be seen as substi-
tutable in PZ. Likewise, if P; E £(PZ), Pk E £(PZ)
and Pk E £(P;), then the pattern P; can be seen
as adjoinable to PZ. Because of this correspon-
dence between the TAG operations and the ADIOS
patterns, we believe that the latter represent regu-
larities that are best described by Mildly Context-
Sensitive Language formalism (Joshi and Schabes,
1997). Importantly, because the ADIOS patterns
are learned from data, they already incorporate the
constraints on substitution and adjoining that in the
original TAG framework must be specified manu-
ally.
Psychological and linguistic evidence for pattern-
based representations. Recent advances in un-
derstanding the psychological role of representa-
tions based on what we call patterns, or construc-
tions (Goldberg, 2003), focus on the use of statis-
tical cues such as conditional probabilities in pat-
tern learning (Saffran et al., 1996; G´omez, 2002),
and on the importance of exemplars and construc-
tions in children’s language acquisition (Cameron-
Faulkner et al., 2003). Converging evidence for the
centrality of pattern-like structures is provided by
corpus-based studies of prefabs — sequences, con-
tinuous or discontinuous, of words that appear to
be prefabricated, that is, stored and retrieved as a
whole, rather than being subject to syntactic pro-
cessing (Wray, 2002). Similar ideas concerning the
ubiquity in syntax of structural peculiarities hitherto
marginalized as “exceptions” are now being voiced
by linguists (Culicover, 1999; Croft, 2001).
</bodyText>
<page confidence="0.681562">
79
</page>
<bodyText confidence="0.97680588">
Cognitive Grammar; Construction Grammar.
The main methodological tenets of ADIOS — pop-
ulating the lexicon with “units” of varying com-
plexity and degree of entrenchment, and using
cognition-general mechanisms for learning and rep-
resentation — fit the spirit of the foundations of
Cognitive Grammar (Langacker, 1987). At the
same time, whereas the cognitive grammarians typ-
ically face the chore of hand-crafting structures that
would reflect the logic of language as they per-
ceive it, ADIOS discovers the primitives of gram-
mar empirically and autonomously. The same is
true also for the comparison between ADIOS and the
various Construction Grammars (Goldberg, 2003;
Croft, 2001), which are all hand-crafted. A con-
struction grammar consists of elements that differ
in their complexity and in the degree to which they
are specified: an idiom such as “big deal” is a fully
specified, immutable construction, whereas the ex-
pression “the X, the Y” – as in “the more, the bet-
ter” (Kay and Fillmore, 1999) – is a partially spec-
ified template. The patterns learned by ADIOS like-
wise vary along the dimensions of complexity and
specificity (e.g., not every pattern has an equiva-
lence class).
</bodyText>
<sectionHeader confidence="0.904717" genericHeader="method">
4 ADIOS: a psycholinguistic evaluation
</sectionHeader>
<bodyText confidence="0.982040080645161">
To illustrate the applicability of our method to real
data, we first describe briefly the outcome of run-
ning it on a subset of the CHILDES collection
(MacWhinney and Snow, 1985), consisting of tran-
scribed speech directed at children. The corpus we
selected contained 300, 000 sentences (1.3 million
tokens) produced by parents. After 14 real-time
days, the algorithm (version 7.3) identified 3400
patterns and 3200 equivalence classes. The outcome
was encouraging: the algorithm found intuitively
significant patterns and produced semantically ad-
equate corresponding equivalence sets. The algo-
rithm’s ability to recombine and reuse the acquired
patterns is exemplified in the legend of Figure 3,
which lists some of the novel sentences it generated.
The input module. The ADIOS system’s input
module allows it to process a novel sentence by
forming its distributed representation in terms of ac-
tivities of existing patterns. We stress that this mod-
ule plays a crucial role in the tests described below,
all of which require dealing with novel inputs. Fig-
ure 4 shows the activation of two patterns (#141 and
#120) by a phrase that contains a word in a novel
context (stay), as well as another word never before
encountered in any context (5pm).
Acceptability of correct and perturbed novel sen-
tences. To test the quality of the representations
Figure 3: a typical pattern extracted from the
CHILDES collection (MacWhinney and Snow,
1985). Hundreds of such patterns and equivalence
classes (underscored) together constitute a concise
representation of the raw data. Some of the phrases
that can be described/generated by these patterns
are: let’s change her...; I thought you were
gonna change her... ; I was going to change
your... ; none of these appear in the training data,
illustrating the ability of ADIOS to generalize. The
generation process operates as a depth-first search
of the tree corresponding to a pattern. For details
see (Solan et al., 2003a; Solan et al., 2004).
(patterns and their associated equivalence classes)
acquired by ADIOS, we have examined their abil-
ity to support various kinds of grammaticality judg-
ments. The first experiment we report sought to
make a distinction between a set of (presumably
grammatical) CHILDES sentences not seen by the
algorithm during training, and the same sentences
in which the word order has been perturbed. We
first trained the model on 10, 000 sentences from
CHILDES, then compared its performance on (1)
1000 previously unseen sentences and (2) the same
sentences in each of which a single random word
order switch has been carried out. The results,
shown in Figure 5, indicate a substantial sensitiv-
ity of the ADIOS input module to simple deviations
from grammaticality in novel data, even after a very
brief training.
Learnability of nonadjacent dependencies
Within the ADIOS framework, the “nonadjacent
dependencies” that characterize the artificial lan-
guages used by (G´omez, 2002) translate, simply,
into patterns with embedded equivalence classes.
</bodyText>
<figure confidence="0.998367375">
16555 (0.14)
1
16556
2
15543 (0.33)
3
18
15539 (1)
11
5
15540
15544
6
12
14374 (1)
4
14819
(1) 14384 (1)
7
13
17
14335
14378 (1) 14818(1)
14383
(1)
16557
8 9
10
14 1516
19
let
&apos;
s
I
&apos;m
wa
s
thought
you
were
gonna
go
ing
to
change
her
your
80
</figure>
<figureCaption confidence="0.99592275">
Figure 4: The two most active patterns responding to the partially novel input Joe and Beth are staying
until 5pm. Leaf activation, which is proportional to the mutual information between input words and various
members of the equivalence classes, is propagated upward by taking the average at each junction (Solan et
al., 2003a).
</figureCaption>
<figure confidence="0.99846825">
120... activation level: 0.667
C14 C15 C16 C17 C18
C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13
C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13
are
liv
Joe
Joe
Jim
Jim
BEGIN
work
ing
play
Pam
and
Pam
Beth
Cindy
George
sday
Beth
Cindy
George
until
tomorrow
Friday
Monday
Saturday
Sunday
Thursday
Tuesday
Wednesday
next
month
week
winter
END
141... activation level: 0.972
119
W0=1.0 W13=1.0
86
113
W0=1.0
W1=E W2..8=F
100
74
112
93
W8=1.0
W15=0.8
89
</figure>
<figureCaption confidence="0.840087">
Figure 5: Grammaticality of perturbed sentences
(CHILDES data). The figure shows a histogram
</figureCaption>
<bodyText confidence="0.99223998">
of the input module output values for two kinds of
stimuli: novel grammatical sentences (dark/blue),
and sentences obtained from these by a single word-
order permutation (light/red).
G´omez showed that the ability of subjects to learn
a language L1 of the form {aXd, bXe, cXf}1,
as measured by their ability to distinguish it
implicitly from L2={aXe, bXf, cXd}, depends
on the amount of variation introduced at X. We
replicated this experiment by training ADIOS on
432 strings from L1, with |X |= 2, 6,12, 24. The
stimuli were the same strings as in the original
experiment, with the individual letters serving as
the basic symbols. A subsequent test resulted in
1Symbols a−f here stand for nonce words such as pel, vot,
or dak, whereas X denotes a slot in which a subset of 24 other
nonce words may appear.
a perfect acceptance of L1 and a perfect rejection
of L2. Training with the original words (rather
than letters) as the basic symbols resulted in L2
rejection rates of 0%, 55%,100%, and 100%,
respectively, for |X |= 2, 6, 12, 24. Thus, the
ADIOS performance both mirrors that of the human
subjects and suggests a potentially interesting new
effect (of the granularity of the input stimuli) that
may be explored in further psycholinguistic studies.
A developmental test. The CASL test (Compre-
hensive Assessment of Spoken Language) is widely
used in the USA to assess language comprehen-
sion in children (Carrow-Woolfolk, 1999). One of
its many components is a grammaticality judgment
test, which consists of 57 sentences and is admin-
istered as follows: a sentence is read to the child,
who then has to decide whether or not it is correct.
If not, the child has to suggest a correct version of
the sentence. For every incorrect sentence, the test
lists 2-3 acceptable correct ones. The present ver-
sion of the ADIOS algorithm can compare sentences
but cannot score single sentences. We therefore ig-
nored 11 out of the 57 sentences, which were correct
to begin with. The remaining 46 incorrect sentences
and their corrected versions were scored by ADIOS
(which for this test had been trained on a 300,000-
sentence corpus from the CHILDES database); the
highest scoring sentence in each trial was inter-
preted as the model’s choice. The model labeled
17 of the test sentences correctly, yielding a score
of 108 (100 = norm) for the age interval 7-0 through
7-2. This score is the norm for the age interval 8-3
through 8-5.2
</bodyText>
<figure confidence="0.882525153846154">
2ADIOS was undecided about the majority of the other sen-
tences on which it did not score correctly.
80
60
50
40
30
20
70
10
0
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
81
</figure>
<table confidence="0.946134090909091">
ADIOS are equal, which usually indicates that there
are too many unfamiliar words. Omitting these sen-
tences yields a significant R2 = 9.7%, p &lt; 0.001;
removing sentences for which the choices score al-
most equally (within 10%) results in R2 = 12.7%,
p &lt; 0.001.4
Figure 6: The results of several grammaticality tests
(the G¨oteborg ESL test is described in the text).
ESL test (forced choice). We next used a stan-
dard test developed for English as Second Lan-
guage (ESL) classes, which has been administered
</table>
<bodyText confidence="0.975675242424242">
in G¨oteborg (Sweden) to more than 10, 000 upper
secondary levels students (that is, children who typ-
ically had 9 years of school, but only 6-7 years of
English). The test consists of 100 three-choice ques-
tions, such as She asked me at once (choices:
come, to come, coming) and The tickets have
been paid for, so you not worry (choices: may,
dare, need); the average score for the population
mentioned is 65%. As before, the choice given the
highest score by the algorithm won; if two choices
received the same top score, the answer was “don’t
know”. The algorithm’s performance in this and
several other tests is summarized in Figure 6 (these
tests have been conducted with an earlier version of
the algorithm (Solan et al., 2003a)). In the ESL test,
ADIOS scored at just under 60%; compare this to
the 45% precision (with 20% recall) achieved by a
straightforward bi-gram benchmark.3
ESL test (magnitude estimation). In this exper-
iment, six subjects were asked to provide magni-
tude estimates of linguistic acceptability (Gurman-
Bard et al., 1996) for all the 3 × 100 sentences in
the G¨oteborg ESL test. The test was paper based
and included the instructions from (Keller, 2000).
No measures were taken to randomize the order of
the sentences or otherwise control the experiment.
The same 300 sentences were processed by ADIOS,
whose responses were normalized by dividing the
output by the sum of each triplet’s score. The re-
sults indicate a significant correlation (R2 = 6.3%,
p &lt; 0.001) between the scores produced by the sub-
jects and by ADIOS. In some cases the scores of
Figure 7: Magnitude estimation study from Keller,
plotted against the ADIOS score on the same sen-
tences (R2 = 0.53,p &lt; 0.05). The sentences
(ranked by increasing score) are:
How many men did you destroy the picture of?
How many men did you destroy a picture of?
How many men did you take the picture of?
How many men did you take a picture of?
Which man did you destroy the picture of?
Which man did you destroy a picture of?
Which man did you take the picture of?
Which man did you take a picture of?
Modeling Keller’s data. A manuscript by Frank
Keller lists magnitude estimation data for eight sen-
tences.5 We compared these to the scores pro-
duced by ADIOS, and obtained a significant corre-
lation (Figure 7). The input module seems capa-
ble of dealing with the substitution of a with the
or of take with destroy, and it does reasonably
well on the substitution of How many men with
Which man. We conjecture that this performance
can be improved by a more sophisticated normal-
ization of the score produced by the input module,
which should do a better job quantifying the cover
(Edelman, 2004) of a novel sentence by the stored
patterns. The limitations of the present version of
the model became apparent when we tested it on the
3Chance performance in this test is 33%. We note that the 82 4Four of the subjects only filled out the test partially (the
corpus used here was too small to train an n-gram model for numbers of responses were 300, 300, 186, 159, 96, 60), but the
n &gt; 2; thus, our algorithm effectively overcomes the problem correlation was highly significant despite the missing data.
of sparse data by putting the available data to a better use. 5http://elib.uni-stuttgart.de/opus/volltexte/1999/81/pdf/81.pdf
52 sentences from Keller’s dissertation, using his
magnitude estimation method (Keller, 2000).6 For
these sentences, no correlation was found between
the human and the model scores. One of the more
challenging aspects of this set is the central role of
pronoun binding in many of the sentences, e.g., The
woman/Each woman saw Peter’s photograph
of her/herself/him/himself. Moreover, this test set
contains examples of context effects, where infor-
mation in an earlier sentence can help resolve a later
ambiguity. Thus, many of the grammatical contrasts
that appear in Keller’s test sentences are too subtle
for the present version of the ADIOS input module
to handle.
Acceptability of correct and perturbed artifi-
cial sentences. In this experiment 64 random sen-
tences was produced with a CFG. For uniformity the
sentence length was kept within 15-20 words. 16 of
the sentences had two adjacent words switched and
another 16 had two random words switched. The 64
sentences were presented to 17 subjects, who placed
each on a computer screen at a lateral position re-
flecting the perceived acceptability. As expected,
the perturbed sentences were rated as less accept-
able than the non-perturbed ones (R2 = 50.3% with
p &lt; 0.01). We controlled for sentence number, for
how high on the screen the sentence was placed, for
the reaction time and for sentence length; only the
latter had a significant contribution to the correla-
tion. The random permutations scored significantly
(p &lt; 0.01) lower than the adjacent permutations.
Furthermore, the variance in the scores of the ran-
domly permuted sentences was significantly larger
(p &lt; 0.005), suggesting that this kind of permu-
tation violates the sentence structure more severely,
but may also sometimes create acceptable sentences
by chance. Previous tests showed that ADIOS is very
good at recognizing perturbed CFG-generated sen-
tences as such, but it remains to be seen whether or
not ADIOS also exhibits differential behavior on the
adjacent and non-adjacent permutations.
Acceptability of ADIOS-generated sentences.
ADIOS was trained on 12,700 sentences (out of a
total of 12,966 sentences) in the ATIS (Air Travel
Information System) corpus; the remaining 226 sen-
tences were used for precision/recall tests. Because
6We remark that this methodology is not without its prob-
lems. As one of our linguistically naive subjects remarked,
“The instructions were (purposefully?) vague about what I
was supposed to judge — understandability, grammar, correct
use of language, or getting the point through... ”. Indeed, the
scores in a magnitude experiment must be composites of sev-
eral factors — at the very least, well-formedness and meaning-
fulness. We are presently exploring various means of acquiring
and dealing with such multidimensional “acceptability” data.
ADIOS is sensitive to the presentation order of the
training sentences, 30 instances were trained on ran-
domized versions of the training set. Eight hu-
man subjects were then asked to estimate accept-
ability of 20 sentences from the original corpus, in-
termixed randomly with 20 sentences generated by
the trained versions of ADIOS. The precision, calcu-
lated as the average number of sentences accepted
by the subjects divided by the total number of sen-
tences in the set (20), was 0.73 ± 0.2 for sentences
from the original corpus and 0.67 ± 0.07 for the
sentences generated by ADIOS. Thus, the ADIOS-
generated sentences are, on the average, as accept-
able to human subjects as the original ones.
</bodyText>
<sectionHeader confidence="0.975285" genericHeader="method">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999594972222222">
The ADIOS approach to the representation of
linguistic knowledge resembles the Construction
Grammar in its general philosophy (e.g., in its re-
liance on structural generalizations rather than on
syntax projected by the lexicon), and the Tree Ad-
joining Grammar in its computational capacity (e.g.,
in its apparent ability to accept Mildly Context Sen-
sitive Languages). The representations learned by
the ADIOS algorithm are truly emergent from the
(unannotated) corpus data. Previous studies focused
on the algorithm that makes such learning possible
(Solan et al., 2004; Edelman et al., 2004). In the
present paper, we concentrated on testing the input
module that allows the acquired patterns to be used
in processing novel stimuli.
The results of the tests we described here are en-
couraging, but there is clearly room for improve-
ment. We believe that the most pressing issue in
this regard is developing a conceptually and com-
putationally well-founded approach to the notion of
cover (that is, a distributed representation of a novel
sentence in terms of the existing patterns). Intu-
itively, the best case, which should receive the top
score, is when there is a single pattern that precisely
covers the entire input, possibly in addition to other
evoked patterns that are only partially active. We are
currently investigating various approaches to scor-
ing distributed representations in which several pat-
terns are highly active. A crucial constraint that ap-
plies to such cases is that a good cover should give a
proper expression to the subtleties of long-range de-
pendencies and binding, many of which are already
captured by the ADIOS learning algorithm.
Acknowledgments. Supported by the US-Israel Bi-
national Science Foundation and by the Thanks to
Scandinavia Graduate Scholarship at Cornell.
</bodyText>
<page confidence="0.908965">
83
</page>
<sectionHeader confidence="0.995325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999733754716981">
R. Bod. 1998. Beyond grammar: an experience-
based theory of language. CSLI Publications,
Stanford, US.
T. Cameron-Faulkner, E. Lieven, and M. Tomasello.
2003. A construction-based analysis of child di-
rected speech. Cognitive Science, 27:843–874.
E. Carrow-Woolfolk. 1999. Comprehensive As-
sessment ofSpoken Language (CASL). AGS Pub-
lishing, Circle Pines, MN.
N. Chomsky. 1986. Knowledge of language: its na-
ture, origin, and use. Praeger, New York.
A. Clark. 2001. Unsupervised Language Acquisi-
tion: Theory and Practice. Ph.D. thesis, COGS,
U. of Sussex.
W. Croft. 2001. Radical Construction Grammar:
syntactic theory in typological perspective. Ox-
ford U. Press, Oxford.
P. W. Culicover. 1999. Syntactic nuts: hard cases,
syntactic theory, and language acquisition. Ox-
ford U. Press, Oxford.
S. Edelman, Z. Solan, D. Horn, and E. Ruppin.
2004. Bridging computational, formal and psy-
cholinguistic approaches to language. In Proc. of
the 26th Conference of the Cognitive Science So-
ciety, Chicago, IL.
S. Edelman. 2004. Bridging language with the
rest of cognition: computational, algorithmic
and neurobiological issues and methods. In
M. Gonzalez-Marquez, M. J. Spivey, S. Coulson,
and I. Mittelberg, eds., Proc. of the Ithaca work-
shop on Empirical Methods in Cognitive Linguis-
tics. John Benjamins.
J. L. Elman, E. A. Bates, M. H. Johnson,
A. Karmiloff-Smith, D. Parisi, and K. Plunkett.
1996. Rethinking innateness: A connectionist
perspective on development. MIT Press, Cam-
bridge, MA.
A. E. Goldberg. 2003. Constructions: a new theo-
retical approach to language. Trends in Cognitive
Sciences, 7:219–224.
R. L. G´omez. 2002. Variability and detection
of invariant structure. Psychological Science,
13:431–436.
M. Gross. 1997. The construction of local gram-
mars. In E. Roche and Y. Schab`es, eds., Finite-
State Language Processing, pages 329–354. MIT
Press, Cambridge, MA.
E. Gurman-Bard, D. Robertson, and A. Sorace.
1996. Magnitude estimation of linguistic accept-
ability. Language, 72:32–68.
I. Guyon and F. Pereira. 1995. Design of a linguis-
tic postprocessor using Variable Memory Length
Markov Models. In Proc. 3rd Int’l Conf. Doc-
ument Analysis and Recogition, pages 454–457,
Montreal, Canada.
Z. S. Harris. 1954. Distributional structure. Word,
10:140–162.
P. J. Henrichsen. 2002. GraSp: Grammar learning
form unlabeled speech corpora. In Proceedings
of CoNLL-2002, pages 22–28. Taipei, Taiwan.
A. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. In G. Rozenberg and A. Salomaa,
eds., Handbook of Formal Languages, volume 3,
pages 69 – 124. Springer, Berlin.
P. Kay and C. J. Fillmore. 1999. Grammatical
constructions and linguistic generalizations: the
What’s X Doing Y? construction. Language,
75:1–33.
F. Keller. 2000. Gradience in Grammar: Experi-
mental and Computational Aspects ofDegrees of
Grammaticality. Ph.D. thesis, U. of Edinburgh.
R. W. Langacker. 1987. Foundations of cogni-
tive grammar, volume I: theoretical prerequisites.
Stanford U. Press, Stanford, CA.
B. MacWhinney and C. Snow. 1985. The Child
Language Exchange System. Journal of Compu-
tational Lingustics, 12:271–296.
A. Roberts and E. Atwell. 2002. Unsupervised
grammar inference systems for natural language.
Technical Report 2002.20, School of Computing,
U. of Leeds.
J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996.
Statistical learning by 8-month-old infants. Sci-
ence, 274:1926–1928.
Z. Solan, E. Ruppin, D. Horn, and S. Edelman.
2003a. Automatic acquisition and efficient rep-
resentation of syntactic structures. In S. Thrun,
editor, Advances in Neural Information Process-
ing, volume 15, Cambridge, MA. MIT Press.
Z. Solan, E. Ruppin, D. Horn, and S. Edelman.
2003b. Unsupervised efficient learning and rep-
resentation of language structure. In R. Alter-
man and D. Kirsh, eds., Proc. 25th Conference
of the Cognitive Science Society, Hillsdale, NJ.
Erlbaum.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman.
2004. Unsupervised context sensitive language
acquisition from a large corpus. In L. Saul, ed-
itor, Advances in Neural Information Processing,
volume 16, Cambridge, MA. MIT Press.
M. van Zaanen. 2000. ABL: Alignment-Based
Learning. In COLING 2000 - Proceedings of the
18th International Conference on Computational
Linguistics, pages 961–967.
A. Wray. 2002. Formulaic language and the lexi-
con. Cambridge U. Press, Cambridge, UK.
</reference>
<page confidence="0.946439">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999963">Some Tests of an Unsupervised Model of Language Acquisition</title>
<author confidence="0.999797">Bo Pedersen</author>
<author confidence="0.999797">Shimon</author>
<affiliation confidence="0.9797795">Department of Cornell</affiliation>
<address confidence="0.934672">Ithaca, NY 14853,</address>
<author confidence="0.929726">Zach Solan</author>
<author confidence="0.929726">David Horn</author>
<author confidence="0.929726">Eytan</author>
<affiliation confidence="0.774635">Faculty of Exact Tel Aviv</affiliation>
<address confidence="0.928929">Tel Aviv, Israel</address>
<abstract confidence="0.999360048780488">We outline an unsupervised language acquisition algorithm and offer some psycholinguistic support for a model based on it. Our approach resembles the Construction Grammar in its general philosophy, and the Tree Adjoining Grammar in its computational characteristics. The model is trained on a corpus of transcribed child-directed speech (CHILDES). The model’s ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al., 1996). Computational models of language acquisition or “grammar induction” are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke “general-purpose” statistical learning mechanisms. We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework. On the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation and probabilistic learning, yet generic “connectionist” architectures are ill-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difficult to train. We are developing an approach to the acquisition of distributional information from raw input (e.g., transcribed speech corpora) that also supports the distillation of structural regularities comparable to those captured by Context Sensitive Grammars out of the accrued statistical knowledge. In thinking about such regularities, we adopt Langacker’s notion of grammar as “simply an inventory of linguistic units” ((Langacker, 1987), p.63). To detect potentially useful units, we identify and process partially redundant sentences that share the same word sequences. We note that the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language (Harris, 1954), as well as for some modwork (van Zaanen, 2000). Likewise, the patthe syntagm and the class complementary-distribution symbols that may appear in its open slot — is the main representational block of our system, Automatic DIstillation Of Structure). Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists. Thus, the computational principles behind the model are outlined here only briefly. The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al., 2003a; Solan et al., 2003b; Solan et al., 2004; Edelman et al., 2004). The principles behind the algorithm representational power of its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns. Each of these is described briefly below.</abstract>
<note confidence="0.687201555555556">77 58 63 51 50 48 that Beth Cindy</note>
<author confidence="0.827006">George Jim Joe Pam</author>
<abstract confidence="0.990525111111111">a bird cat cow dog horse rabbit rthe ird Lbcat dog Ihorse Irabbit Lflies jumps • laughs annoyes bothers</abstract>
<title confidence="0.6384035">Idisturbs Iworries</title>
<author confidence="0.8005068">Beth Cindy George Jim Joe Pam</author>
<abstract confidence="0.93075692">who adores Iloves iscolds Iworships pBeth George &apos;Jim Joe LPam 1a cat I dog horse rabbit the bird cat cow dog horse doesn&apos;t it 64</abstract>
<phone confidence="0.6820605">49 51 50 50 60 85 53 62</phone>
<note confidence="0.8170785">that the bird jumps disturbes Jim who adores the cat, doesn&apos;t it? P84 &amp;quot;that&amp;quot; P58 P63 E63 E64 P48 E64 &amp;quot;Beth&amp;quot;  |&amp;quot;Cindy&amp;quot;  |&amp;quot;George&amp;quot;  |&amp;quot;Jim&amp;quot;  |&amp;quot;Joe&amp;quot;  |&amp;quot;Pam&amp;quot;  |P49  |P51 P48 &amp;quot; &amp;quot; &amp;quot;doesn&apos;t&amp;quot; &amp;quot;it&amp;quot; P51 &amp;quot;the&amp;quot; E50 P49 &amp;quot;a&amp;quot; E50 E50 &amp;quot;bird&amp;quot;  |&amp;quot;cat&amp;quot;  |&amp;quot;cow&amp;quot;  |&amp;quot;dog&amp;quot;  |&amp;quot;horse&amp;quot;  |&amp;quot;rabbit&amp;quot; P61 &amp;quot;who&amp;quot; E62 E62 &amp;quot;adores&amp;quot;  |&amp;quot;loves&amp;quot;  |&amp;quot;scolds&amp;quot;  |&amp;quot;worships&amp;quot; E53 &amp;quot;Beth&amp;quot;  |&amp;quot;Cindy&amp;quot;  |&amp;quot;George&amp;quot;  |&amp;quot;Jim&amp;quot;  |&amp;quot;Joe&amp;quot;  |&amp;quot;Pam&amp;quot; E85 &amp;quot;annoyes&amp;quot;  |&amp;quot;bothers&amp;quot;  |&amp;quot;disturbes&amp;quot;  |&amp;quot;worries&amp;quot; P58 E60 E64 2 E60 &amp;quot;flies&amp;quot;  |&amp;quot;Jumps&amp;quot;  |&amp;quot;laughs&amp;quot; 64 61 49</note>
<date confidence="0.288254">50</date>
<abstract confidence="0.4616745">1: pattern (presented in a tree form), capturing a long range dependency (equivalence class 55 84 labels are underscored). This and other examples here were distilled from a 400-sentence corpus generated a 40-rule Context Free Grammar. same pattern recast as a set of rewriting rules that can be P63 E64 48 seen as a Context Free Grammar fragment. P210 P55 P84 BEGINP55P84 BEGINE56 &amp;quot;thinks&amp;quot; &amp;quot;that&amp;quot;P84 P55P84P178 P55E75 &amp;quot;thinks&amp;quot; &amp;quot;that&amp;quot;P178 BEGIN Pam 56 75 75 Geo 55 84 thinks that Beth</abstract>
<author confidence="0.7014015">Cindy George Jim Joe</author>
<affiliation confidence="0.499166">Pam</affiliation>
<address confidence="0.695774">210</address>
<abstract confidence="0.781029">bbit thinks that Agreement</abstract>
<author confidence="0.602787333333333">Beth Cindy George Jim Joe Pam</author>
<abstract confidence="0.99089993670886">barks meows and a the bird cat cow dog horse rabbit flies jumps laughs , doesn&apos;t she ? 73 89 109 65 92 178 114 2: not rewire all the occurrences of a specific pattern, but only those that P Bh d J k tht J n th Grg share the same context, its power is comparable to that of Context Sensitive Grammars. In this example, th and Jm think that Jo thinks that George that Cindy beleves that Jim who dors a equivalence class #75 is not extended to subsume the subject position, because that position appears in eo d , d y cat meows and the bir lies , dont ty? 0027 ht Pa a different context (e.g., immediately to the right of the symbol BEGIN). Thus, long-range agreement is Pam laughs worries a og , doesnt it? that a cow jumps diurbs Jim who loves horse , d and over-generalization prevented. context-sensitive “ rules” corresponding to pattern a cow jum disturbs Jm who loves a horse , the meo they? o that Bethpese ndy daJim yes d rie Probabilistic inference of pattern significance. thinks that Jim believe hat Beth who loves a 71 53 67 a corpus of sentences as an inithat Pam tough to please worries th cat tially highly redundant directed graph, which can be informally visualized as a tangle of strands that are segregated into Each of these consists of some strands clumped together; a bundle is formed when two or more strands join together and run in parallel and is dissolved when more strands leave the bundle than stay in. In a given corpus, there will be many bundles, with each strand (sentence) possibly participating in several. Our algorithm, described in detail in (Solan et al., 2004), identifies significant bundles that balance high compression (small size of the bundle “lexicon”) against good generalization (the ability to generate new grammatical sentences by splicing together various strand fragments each of which belongs to a different bundle). sensitivity of patterns. pattern is an abstraction of a bundle of sentences that are identical up to variation in one place, where one of several symbols — the members of the equivalence class with the pattern — may appear (Figeorg C Jim eay ure 1). Because this variation is only allowed in they? the context specified by the pattern, the generalizathat Joe is eager to please disurbs the bird tion afforded by a set of patterns is inherently safer Cindy hink that Jm beeves that to ead is tough than in approaches that posit globally valid cate- Beth thik tht J blive tht eth h lv gories (“parts of speech”) and rules (“grammar”). horse meows and the horse umps doesn&apos;t sh e? reliance of many context-sensitive that Pm is ough to plese worries the at patterns rather than on traditional rules can be compared both to the Construction Grammar (discussed later) and to Langacker’s concept of the grammar as a collection of “patterns of all intermediate degrees of generality” ((Langacker, 1987), p.46). structure of patterns. graph is rewired every time a new pattern is detected, so that a bundle of strings subsumed by it is represented by a single new edge. Following the rewiring, which is context-specific, potentially farapart symbols that used to straddle the newly abstracted pattern become close neighbors. Patterns thus become hierarchically structured in that their elements may be either terminals (i.e., fully specified strings) or other patterns. Moreover, patterns may refer to themselves, which opens the door for recursion. � eorge Joe ager hgu ease dear hers durbs Ji H c c a 78 3 Related computational and linguistic formalisms and psycholinguistic findings very few existing algorithms for unsupervised language acquisition use raw, unannotated corpus data (as opposed, say, to sentences converted into sequences of POS tags). The only work described in a recent review (Roberts and Atwell, 2002) as completely unsupervised — the GraSp model (Henrichsen, 2002) — does attempt to induce syntax from raw transcribed speech, yet it is not completely data-driven in that it makes a prior commitment to a particular theory of syntax (Categorial Grammar, complete with a pre-specified set of allowed categories). Because of the unique nature of our chosen challenge — finding structure in language rather than imposing it — the following brief survey of grammar induction focuses on contrasts and comparisons to approaches that generally stop short of attempting to do what our algorithm does. We distinguish between approaches that are motivated computationally (Local Grammar and Variable Order Markov models, and Tree Adjoining Grammar, discussed elsewhere (Edelman et al., 2004), and those whose main motivation is linguistic and cognitive psychological (Cognitive and Construction grammars, discussed below). Grammar and Markov models. capturing the regularities inherent in multiple crisspaths through a corpus, superficially resembles finite-state Local Grammars (Gross, 1997) and Variable Order Markov (VOM) models (Guyon and Pereira, 1995). The VOM apstarts by postulating a structure, which is then fitted to the data by maximizing likelihood of the training corpus. The philosophy differs from the VOM approach in sevkey respects. rather than fitting a model to the data, we use the data to construct a (recursively structured) graph. Thus, our algorithm naturally addresses the inference of the graph’s structure, a task that is more difficult than the estimaof parameters for a given configuration. Secbecause from the bottom up in a recursive, data-driven fashion, it is less susceptible to complexity issues. It can be used on huge graphs, and may yield very large patterns, which in a VOM model would correspond to an unmanageably high the idea of VOM structure, in the following sense. Consider a set of of the form etc. The equivclasses include vertices of the graph (both words and word patterns turned into nodes), wild cards (i.e., any node), as well as ambivalent cards (any node or no node). This means that the terminal-level length of the string represented by a pattern does not have to be of a fixed length. This goes conceptually beyond the variable order structure: do not have to appear in Markov chain of a finite order the size of ill-defined, as explained as we showed earlier (Figure 2), both context-sensitive substitution and recursion. Adjoining Grammar. proper place in the Chomsky hierarchy for the class of strings accepted by our model is between Context Free and Context Sensitive Languages. The pattern-based employed by counterparts for each of the two composition operations, substitution and adjoining, that characterize a Tree Adjoining Grammar, or TAG, developed by Joshi and others (Joshi and Schabes, 1997). Specifically, both substitution and adjoining are subsumed in the that hold among such as the membership of one pattern in another. Cona pattern its equivalence class other pattern be seen as substiin Likewise, if then the pattern be seen adjoinable to Because of this corresponbetween the TAG operations and the patterns, we believe that the latter represent regularities that are best described by Mildly Context- Sensitive Language formalism (Joshi and Schabes, Importantly, because the are learned from data, they already incorporate the constraints on substitution and adjoining that in the original TAG framework must be specified manually. Psychological and linguistic evidence for patternrepresentations. advances in understanding the psychological role of representabased on what we call patterns, or construc- 2003), focus on the use of statistical cues such as conditional probabilities in pattern learning (Saffran et al., 1996; G´omez, 2002), and on the importance of exemplars and constructions in children’s language acquisition (Cameron- Faulkner et al., 2003). Converging evidence for the centrality of pattern-like structures is provided by studies of sequences, continuous or discontinuous, of words that appear to be prefabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in syntax of structural peculiarities hitherto marginalized as “exceptions” are now being voiced by linguists (Culicover, 1999; Croft, 2001). 79 Cognitive Grammar; Construction Grammar. main methodological tenets of populating the lexicon with “units” of varying complexity and degree of entrenchment, and using cognition-general mechanisms for learning and representation — fit the spirit of the foundations of Cognitive Grammar (Langacker, 1987). At the same time, whereas the cognitive grammarians typically face the chore of hand-crafting structures that would reflect the logic of language as they perit, the primitives of grammar empirically and autonomously. The same is also for the comparison between the various Construction Grammars (Goldberg, 2003; Croft, 2001), which are all hand-crafted. A construction grammar consists of elements that differ in their complexity and in the degree to which they are specified: an idiom such as “big deal” is a fully specified, immutable construction, whereas the expression “the X, the Y” – as in “the more, the better” (Kay and Fillmore, 1999) – is a partially spectemplate. The patterns learned by likewise vary along the dimensions of complexity and specificity (e.g., not every pattern has an equivalence class). a psycholinguistic evaluation To illustrate the applicability of our method to real data, we first describe briefly the outcome of running it on a subset of the CHILDES collection (MacWhinney and Snow, 1985), consisting of transcribed speech directed at children. The corpus we contained tokens) produced by parents. After 14 real-time days, the algorithm (version 7.3) identified 3400 patterns and 3200 equivalence classes. The outcome was encouraging: the algorithm found intuitively significant patterns and produced semantically adequate corresponding equivalence sets. The algorithm’s ability to recombine and reuse the acquired patterns is exemplified in the legend of Figure 3, which lists some of the novel sentences it generated. input module. it to process a novel sentence by forming its distributed representation in terms of acof existing patterns. stress that this module plays a crucial role in the tests described below, of which require dealing with novel inputs. Figure 4 shows the activation of two patterns (#141 and as well as another word never before in any context Acceptability of correct and perturbed novel sentest the quality of the representations Figure 3: a typical pattern extracted from the CHILDES collection (MacWhinney and Snow, 1985). Hundreds of such patterns and equivalence classes (underscored) together constitute a concise representation of the raw data. Some of the phrases that can be described/generated by these patterns change thought you were change ; was going to change ; none of these appear in the training data, the ability of generalize. The generation process operates as a depth-first search of the tree corresponding to a pattern. For details see (Solan et al., 2003a; Solan et al., 2004). (patterns and their associated equivalence classes) by we have examined their ability to support various kinds of grammaticality judgments. The first experiment we report sought to make a distinction between a set of (presumably grammatical) CHILDES sentences not seen by the algorithm during training, and the same sentences in which the word order has been perturbed. We trained the model on from CHILDES, then compared its performance on (1) unseen sentences and (2) the same sentences in each of which a single random word order switch has been carried out. The results, shown in Figure 5, indicate a substantial sensitivof the module to simple deviations from grammaticality in novel data, even after a very brief training. Learnability of nonadjacent dependencies the the “nonadjacent dependencies” that characterize the artificial languages used by (G´omez, 2002) translate, simply, into patterns with embedded equivalence classes.</abstract>
<note confidence="0.6126876">1 16556 2 3 18 11 5 15540 15544 6 12 4 14819 7 13</note>
<date confidence="0.579003">17 14335 14383</date>
<note confidence="0.410669">(1) 16557 8 9 10</note>
<date confidence="0.418832">19</date>
<abstract confidence="0.97825248">let &apos; s I &apos;m wa s thought you were gonna go ing to change her your 80 4: The two most active patterns responding to the partially novel input and Beth are staying Leaf activation, which is proportional to the mutual information between input words and various members of the equivalence classes, is propagated upward by taking the average at each junction (Solan et al., 2003a). level: 0.667 are liv</abstract>
<author confidence="0.644679">Joe Joe Jim Jim</author>
<abstract confidence="0.791513666666667">BEGIN work ing play Pam and</abstract>
<author confidence="0.616126">Pam Beth Cindy George</author>
<email confidence="0.63055">sday</email>
<author confidence="0.820062333333333">Beth Cindy George</author>
<email confidence="0.939687">until</email>
<abstract confidence="0.839942973333333">tomorrow Friday Monday Saturday Sunday Thursday Tuesday Wednesday next month week winter END level: 0.972 119 86 113 100 74 112 93 89 Figure 5: Grammaticality of perturbed sentences (CHILDES data). The figure shows a histogram of the input module output values for two kinds of stimuli: novel grammatical sentences (dark/blue), and sentences obtained from these by a single wordorder permutation (light/red). G´omez showed that the ability of subjects to learn language L1 of the form bXe, as measured by their ability to distinguish it from bXf, depends the amount of variation introduced at We this experiment by training strings from L1, with The stimuli were the same strings as in the original experiment, with the individual letters serving as the basic symbols. A subsequent test resulted in stand for nonce words such as pel, vot, dak, whereas a slot in which a subset of 24 other nonce words may appear. a perfect acceptance of L1 and a perfect rejection of L2. Training with the original words (rather than letters) as the basic symbols resulted in L2 rates of and for Thus, the both mirrors that of the human subjects and suggests a potentially interesting new effect (of the granularity of the input stimuli) that may be explored in further psycholinguistic studies. developmental test. CASL test (Comprehensive Assessment of Spoken Language) is widely used in the USA to assess language comprehension in children (Carrow-Woolfolk, 1999). One of its many components is a grammaticality judgment test, which consists of 57 sentences and is administered as follows: a sentence is read to the child, who then has to decide whether or not it is correct. If not, the child has to suggest a correct version of the sentence. For every incorrect sentence, the test lists 2-3 acceptable correct ones. The present verof the can compare sentences but cannot score single sentences. We therefore ignored 11 out of the 57 sentences, which were correct to begin with. The remaining 46 incorrect sentences their corrected versions were scored by (which for this test had been trained on a 300,000sentence corpus from the CHILDES database); the highest scoring sentence in each trial was interpreted as the model’s choice. The model labeled 17 of the test sentences correctly, yielding a score of 108 (100 = norm) for the age interval 7-0 through 7-2. This score is the norm for the age interval 8-3 undecided about the majority of the other sentences on which it did not score correctly.</abstract>
<note confidence="0.9215306">80 60 50 40 30 20 70 10 0 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5</note>
<date confidence="0.258793">81</date>
<abstract confidence="0.995157557522124">equal, which usually indicates that there are too many unfamiliar words. Omitting these senyields a significant = &lt; removing sentences for which the choices score alequally (within results in = &lt; Figure 6: The results of several grammaticality tests (the G¨oteborg ESL test is described in the text). test (forced choice). next used a stan-dard test developed for English as Second Lan-guage (ESL) classes, which has been administered G¨oteborg (Sweden) to more than 000 secondary levels students (that is, children who typ-ically had 9 years of school, but only 6-7 years of The test consists of quessuch as asked me at once and tickets have paid for, so you not worry the average score for the population is As before, the choice given the highest score by the algorithm won; if two choices received the same top score, the answer was “don’t know”. The algorithm’s performance in this and several other tests is summarized in Figure 6 (these tests have been conducted with an earlier version of the algorithm (Solan et al., 2003a)). In the ESL test, at just under compare this to (with achieved by a bi-gram test (magnitude estimation). this exper-iment, six subjects were asked to provide magni-tude estimates of linguistic acceptability (Gurmanet al., 1996) for all the in the G¨oteborg ESL test. The test was paper based and included the instructions from (Keller, 2000). No measures were taken to randomize the order of the sentences or otherwise control the experiment. same 300 sentences were processed by whose responses were normalized by dividing the output by the sum of each triplet’s score. The reindicate a significant correlation = &lt; between the scores produced by the suband by In some cases the scores of Figure 7: Magnitude estimation study from Keller, against the on the same sen- = 0.53,p &lt; The sentences (ranked by increasing score) are: How many men did you destroy the picture of? How many men did you destroy a picture of? How many men did you take the picture of? How many men did you take a picture of? Which man did you destroy the picture of? Which man did you destroy a picture of? Which man did you take the picture of? Which man did you take a picture of? Keller’s data. manuscript by Frank Keller lists magnitude estimation data for eight sen-We compared these to the scores proby and obtained a significant corre-lation (Figure 7). The input module seems capaof dealing with the substitution of of and it does reasonably on the substitution of many men We conjecture that this performance can be improved by a more sophisticated normal-ization of the score produced by the input module, should do a better job quantifying the (Edelman, 2004) of a novel sentence by the stored patterns. The limitations of the present version of the model became apparent when we tested it on the performance in this test is We note that the used here was too small to train an model for &gt; thus, our algorithm effectively overcomes the problem of sparse data by putting the available data to a better use. 82 of the subjects only filled out the test partially (the numbers of responses were 300, 300, 186, 159, 96, 60), but the correlation was highly significant despite the missing data. 52 sentences from Keller’s dissertation, using his estimation method (Keller, For these sentences, no correlation was found between the human and the model scores. One of the more challenging aspects of this set is the central role of binding in many of the sentences, e.g., woman/Each woman saw Peter’s photograph her/herself/him/himself. this test set contains examples of context effects, where information in an earlier sentence can help resolve a later ambiguity. Thus, many of the grammatical contrasts that appear in Keller’s test sentences are too subtle the present version of the module to handle. Acceptability of correct and perturbed artifisentences. this experiment 64 random sentences was produced with a CFG. For uniformity the sentence length was kept within 15-20 words. 16 of the sentences had two adjacent words switched and another 16 had two random words switched. The 64 sentences were presented to 17 subjects, who placed each on a computer screen at a lateral position reflecting the perceived acceptability. As expected, the perturbed sentences were rated as less acceptthan the non-perturbed ones = 50.3% &lt; We controlled for sentence number, for how high on the screen the sentence was placed, for the reaction time and for sentence length; only the latter had a significant contribution to the correlation. The random permutations scored significantly &lt; lower than the adjacent permutations. Furthermore, the variance in the scores of the randomly permuted sentences was significantly larger &lt; suggesting that this kind of permutation violates the sentence structure more severely, but may also sometimes create acceptable sentences chance. Previous tests showed that very good at recognizing perturbed CFG-generated sentences as such, but it remains to be seen whether or exhibits differential behavior on the adjacent and non-adjacent permutations. of sentences. trained on 12,700 sentences (out of a total of 12,966 sentences) in the ATIS (Air Travel Information System) corpus; the remaining 226 sentences were used for precision/recall tests. Because remark that this methodology is not without its problems. As one of our linguistically naive subjects remarked, “The instructions were (purposefully?) vague about what I was supposed to judge — understandability, grammar, correct use of language, or getting the point through... ”. Indeed, the scores in a magnitude experiment must be composites of several factors — at the very least, well-formedness and meaningfulness. We are presently exploring various means of acquiring and dealing with such multidimensional “acceptability” data. sensitive to the presentation order of the training sentences, 30 instances were trained on randomized versions of the training set. Eight human subjects were then asked to estimate acceptability of 20 sentences from the original corpus, intermixed randomly with 20 sentences generated by trained versions of The precision, calculated as the average number of sentences accepted by the subjects divided by the total number of senin the set (20), was sentences the original corpus and the generated by Thus, the generated sentences are, on the average, as acceptable to human subjects as the original ones. 5 Concluding remarks to the representation of linguistic knowledge resembles the Construction Grammar in its general philosophy (e.g., in its reliance on structural generalizations rather than on syntax projected by the lexicon), and the Tree Adjoining Grammar in its computational capacity (e.g., in its apparent ability to accept Mildly Context Sensitive Languages). The representations learned by are truly emergent from the (unannotated) corpus data. Previous studies focused on the algorithm that makes such learning possible (Solan et al., 2004; Edelman et al., 2004). In the present paper, we concentrated on testing the input module that allows the acquired patterns to be used in processing novel stimuli. The results of the tests we described here are encouraging, but there is clearly room for improvement. We believe that the most pressing issue in this regard is developing a conceptually and computationally well-founded approach to the notion of cover (that is, a distributed representation of a novel sentence in terms of the existing patterns). Intuitively, the best case, which should receive the top score, is when there is a single pattern that precisely covers the entire input, possibly in addition to other evoked patterns that are only partially active. We are currently investigating various approaches to scoring distributed representations in which several patterns are highly active. A crucial constraint that applies to such cases is that a good cover should give a proper expression to the subtleties of long-range dependencies and binding, many of which are already by the algorithm. by the US-Israel Binational Science Foundation and by the Thanks to</abstract>
<note confidence="0.4729835">Scandinavia Graduate Scholarship at Cornell. 83</note>
<title confidence="0.724112">References</title>
<author confidence="0.755658">grammar an experience-</author>
<affiliation confidence="0.989376">theory of CSLI Publications,</affiliation>
<address confidence="0.989931">Stanford, US.</address>
<note confidence="0.720173185185185">T. Cameron-Faulkner, E. Lieven, and M. Tomasello. 2003. A construction-based analysis of child dispeech. 27:843–874. Carrow-Woolfolk. 1999. AsofSpoken Language AGS Publishing, Circle Pines, MN. Chomsky. 1986. of language: its naorigin, and Praeger, New York. Clark. 2001. Language Acquisi- Theory and Ph.D. thesis, COGS, U. of Sussex. Croft. 2001. Construction Grammar: theory in typological Oxford U. Press, Oxford. W. Culicover. 1999. nuts: hard cases, theory, and language Oxford U. Press, Oxford. S. Edelman, Z. Solan, D. Horn, and E. Ruppin. Bridging computational, formal and psyapproaches to language. In of the 26th Conference of the Cognitive Science So- Chicago, IL. S. Edelman. 2004. Bridging language with the rest of cognition: computational, algorithmic and neurobiological issues and methods. In M. Gonzalez-Marquez, M. J. Spivey, S. Coulson, I. Mittelberg, eds., of the Ithaca work-</note>
<title confidence="0.901878">shop on Empirical Methods in Cognitive Linguis-</title>
<author confidence="0.9755625">J L Elman</author>
<author confidence="0.9755625">E A Bates</author>
<author confidence="0.9755625">M H Johnson</author>
<abstract confidence="0.537041555555556">A. Karmiloff-Smith, D. Parisi, and K. Plunkett. innateness: A connectionist on MIT Press, Cambridge, MA. A. E. Goldberg. 2003. Constructions: a new theoapproach to language. in Cognitive 7:219–224. R. L. G´omez. 2002. Variability and detection invariant structure.</abstract>
<note confidence="0.918000166666667">13:431–436. M. Gross. 1997. The construction of local gram- In E. Roche and Y. Schab`es, eds., Finite- Language pages 329–354. MIT Press, Cambridge, MA. E. Gurman-Bard, D. Robertson, and A. Sorace. 1996. Magnitude estimation of linguistic accept- 72:32–68. I. Guyon and F. Pereira. 1995. Design of a linguistic postprocessor using Variable Memory Length Models. In 3rd Int’l Conf. Doc- Analysis and pages 454–457,</note>
<address confidence="0.73492">Montreal, Canada.</address>
<note confidence="0.837790288461538">S. Harris. 1954. Distributional structure. 10:140–162. P. J. Henrichsen. 2002. GraSp: Grammar learning unlabeled speech corpora. In pages 22–28. Taipei, Taiwan. A. Joshi and Y. Schabes. 1997. Tree-Adjoining Grammars. In G. Rozenberg and A. Salomaa, of Formal volume 3, pages 69 – 124. Springer, Berlin. P. Kay and C. J. Fillmore. 1999. Grammatical constructions and linguistic generalizations: the X Doing Y? construction. 75:1–33. Keller. 2000. in Grammar: Experimental and Computational Aspects ofDegrees of Ph.D. thesis, U. of Edinburgh. W. Langacker. 1987. of cognivolume I: theoretical prerequisites. Stanford U. Press, Stanford, CA. B. MacWhinney and C. Snow. 1985. The Child Exchange System. of Compu- 12:271–296. A. Roberts and E. Atwell. 2002. Unsupervised grammar inference systems for natural language. Technical Report 2002.20, School of Computing, U. of Leeds. J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996. learning by 8-month-old infants. Sci- 274:1926–1928. Z. Solan, E. Ruppin, D. Horn, and S. Edelman. 2003a. Automatic acquisition and efficient representation of syntactic structures. In S. Thrun, in Neural Information Processvolume 15, Cambridge, MA. MIT Press. Z. Solan, E. Ruppin, D. Horn, and S. Edelman. 2003b. Unsupervised efficient learning and representation of language structure. In R. Alterand D. Kirsh, eds., 25th Conference the Cognitive Science Hillsdale, NJ. Erlbaum. Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2004. Unsupervised context sensitive language acquisition from a large corpus. In L. Saul, edin Neural Information volume 16, Cambridge, MA. MIT Press. M. van Zaanen. 2000. ABL: Alignment-Based In 2000 - Proceedings of the 18th International Conference on Computational pages 961–967. Wray. 2002. language and the lexi- Cambridge U. Press, Cambridge, UK. 84</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Beyond grammar: an experiencebased theory of language.</title>
<date>1998</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, US.</location>
<contexts>
<context position="1272" citStr="Bod, 1998" startWordPosition="183" endWordPosition="184">peech (CHILDES). The model’s ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al., 1996). Computational models of language acquisition or “grammar induction” are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke “general-purpose” statistical learning mechanisms. We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework. On the one hand, the statistical approach is geared t</context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>R. Bod. 1998. Beyond grammar: an experiencebased theory of language. CSLI Publications, Stanford, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cameron-Faulkner</author>
<author>E Lieven</author>
<author>M Tomasello</author>
</authors>
<title>A construction-based analysis of child directed speech.</title>
<date>2003</date>
<journal>Cognitive Science,</journal>
<pages>27--843</pages>
<marker>Cameron-Faulkner, Lieven, Tomasello, 2003</marker>
<rawString>T. Cameron-Faulkner, E. Lieven, and M. Tomasello. 2003. A construction-based analysis of child directed speech. Cognitive Science, 27:843–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Carrow-Woolfolk</author>
</authors>
<title>Comprehensive Assessment ofSpoken Language (CASL).</title>
<date>1999</date>
<publisher>AGS Publishing,</publisher>
<location>Circle Pines, MN.</location>
<contexts>
<context position="21371" citStr="Carrow-Woolfolk, 1999" startWordPosition="3527" endWordPosition="3528">perfect acceptance of L1 and a perfect rejection of L2. Training with the original words (rather than letters) as the basic symbols resulted in L2 rejection rates of 0%, 55%,100%, and 100%, respectively, for |X |= 2, 6, 12, 24. Thus, the ADIOS performance both mirrors that of the human subjects and suggests a potentially interesting new effect (of the granularity of the input stimuli) that may be explored in further psycholinguistic studies. A developmental test. The CASL test (Comprehensive Assessment of Spoken Language) is widely used in the USA to assess language comprehension in children (Carrow-Woolfolk, 1999). One of its many components is a grammaticality judgment test, which consists of 57 sentences and is administered as follows: a sentence is read to the child, who then has to decide whether or not it is correct. If not, the child has to suggest a correct version of the sentence. For every incorrect sentence, the test lists 2-3 acceptable correct ones. The present version of the ADIOS algorithm can compare sentences but cannot score single sentences. We therefore ignored 11 out of the 57 sentences, which were correct to begin with. The remaining 46 incorrect sentences and their corrected versi</context>
</contexts>
<marker>Carrow-Woolfolk, 1999</marker>
<rawString>E. Carrow-Woolfolk. 1999. Comprehensive Assessment ofSpoken Language (CASL). AGS Publishing, Circle Pines, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Knowledge of language: its nature, origin, and use.</title>
<date>1986</date>
<publisher>Praeger,</publisher>
<location>New York.</location>
<contexts>
<context position="1363" citStr="Chomsky, 1986" startWordPosition="197" endWordPosition="198"> various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al., 1996). Computational models of language acquisition or “grammar induction” are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke “general-purpose” statistical learning mechanisms. We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework. On the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed repr</context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>N. Chomsky. 1986. Knowledge of language: its nature, origin, and use. Praeger, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Unsupervised Language Acquisition: Theory and Practice.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>COGS, U. of Sussex.</institution>
<contexts>
<context position="1285" citStr="Clark, 2001" startWordPosition="185" endWordPosition="186">DES). The model’s ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al., 1996). Computational models of language acquisition or “grammar induction” are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke “general-purpose” statistical learning mechanisms. We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework. On the one hand, the statistical approach is geared to take advant</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>A. Clark. 2001. Unsupervised Language Acquisition: Theory and Practice. Ph.D. thesis, COGS, U. of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
</authors>
<title>Radical Construction Grammar: syntactic theory in typological perspective.</title>
<date>2001</date>
<publisher>U. Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="14585" citStr="Croft, 2001" startWordPosition="2399" endWordPosition="2400">ez, 2002), and on the importance of exemplars and constructions in children’s language acquisition (CameronFaulkner et al., 2003). Converging evidence for the centrality of pattern-like structures is provided by corpus-based studies of prefabs — sequences, continuous or discontinuous, of words that appear to be prefabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in syntax of structural peculiarities hitherto marginalized as “exceptions” are now being voiced by linguists (Culicover, 1999; Croft, 2001). 79 Cognitive Grammar; Construction Grammar. The main methodological tenets of ADIOS — populating the lexicon with “units” of varying complexity and degree of entrenchment, and using cognition-general mechanisms for learning and representation — fit the spirit of the foundations of Cognitive Grammar (Langacker, 1987). At the same time, whereas the cognitive grammarians typically face the chore of hand-crafting structures that would reflect the logic of language as they perceive it, ADIOS discovers the primitives of grammar empirically and autonomously. The same is true also for the comparison</context>
</contexts>
<marker>Croft, 2001</marker>
<rawString>W. Croft. 2001. Radical Construction Grammar: syntactic theory in typological perspective. Oxford U. Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Culicover</author>
</authors>
<title>Syntactic nuts: hard cases, syntactic theory, and language acquisition.</title>
<date>1999</date>
<publisher>U. Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="14571" citStr="Culicover, 1999" startWordPosition="2397" endWordPosition="2398">t al., 1996; G´omez, 2002), and on the importance of exemplars and constructions in children’s language acquisition (CameronFaulkner et al., 2003). Converging evidence for the centrality of pattern-like structures is provided by corpus-based studies of prefabs — sequences, continuous or discontinuous, of words that appear to be prefabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in syntax of structural peculiarities hitherto marginalized as “exceptions” are now being voiced by linguists (Culicover, 1999; Croft, 2001). 79 Cognitive Grammar; Construction Grammar. The main methodological tenets of ADIOS — populating the lexicon with “units” of varying complexity and degree of entrenchment, and using cognition-general mechanisms for learning and representation — fit the spirit of the foundations of Cognitive Grammar (Langacker, 1987). At the same time, whereas the cognitive grammarians typically face the chore of hand-crafting structures that would reflect the logic of language as they perceive it, ADIOS discovers the primitives of grammar empirically and autonomously. The same is true also for </context>
</contexts>
<marker>Culicover, 1999</marker>
<rawString>P. W. Culicover. 1999. Syntactic nuts: hard cases, syntactic theory, and language acquisition. Oxford U. Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Edelman</author>
<author>Z Solan</author>
<author>D Horn</author>
<author>E Ruppin</author>
</authors>
<title>Bridging computational, formal and psycholinguistic approaches to language.</title>
<date>2004</date>
<booktitle>In Proc. of the 26th Conference of the Cognitive Science Society,</booktitle>
<location>Chicago, IL.</location>
<contexts>
<context position="3824" citStr="Edelman et al., 2004" startWordPosition="574" endWordPosition="577">main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure). Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists. Thus, the main computational principles behind the ADIOS model are outlined here only briefly. The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al., 2003a; Solan et al., 2003b; Solan et al., 2004; Edelman et al., 2004). 2 The principles behind the ADIOS algorithm The representational power of ADIOS and its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns. Each of these is described briefly below. 77 84 0.0001 58 63 51 50 48 that Beth Cindy George Jim Joe Pam a bird cat cow dog horse rabbit r the ird L bcat cow dog I horse I rabbit L flies jumps • laughs annoyes bothers I disturbs &apos; I worries Beth Cindy George Jim Joe Pam who adores I loves i scolds I worsh</context>
<context position="10470" citStr="Edelman et al., 2004" startWordPosition="1741" endWordPosition="1744">ven in that it makes a prior commitment to a particular theory of syntax (Categorial Grammar, complete with a pre-specified set of allowed categories). Because of the unique nature of our chosen challenge — finding structure in language rather than imposing it — the following brief survey of grammar induction focuses on contrasts and comparisons to approaches that generally stop short of attempting to do what our algorithm does. We distinguish between approaches that are motivated computationally (Local Grammar and Variable Order Markov models, and Tree Adjoining Grammar, discussed elsewhere (Edelman et al., 2004), and those whose main motivation is linguistic and cognitive psychological (Cognitive and Construction grammars, discussed below). Local Grammar and Markov models. In capturing the regularities inherent in multiple crisscrossing paths through a corpus, ADIOS superficially resembles finite-state Local Grammars (Gross, 1997) and Variable Order Markov (VOM) models (Guyon and Pereira, 1995). The VOM approach starts by postulating a maximum-n structure, which is then fitted to the data by maximizing the likelihood of the training corpus. The ADIOS philosophy differs from the VOM approach in severa</context>
<context position="30437" citStr="Edelman et al., 2004" startWordPosition="5047" endWordPosition="5050">al ones. 5 Concluding remarks The ADIOS approach to the representation of linguistic knowledge resembles the Construction Grammar in its general philosophy (e.g., in its reliance on structural generalizations rather than on syntax projected by the lexicon), and the Tree Adjoining Grammar in its computational capacity (e.g., in its apparent ability to accept Mildly Context Sensitive Languages). The representations learned by the ADIOS algorithm are truly emergent from the (unannotated) corpus data. Previous studies focused on the algorithm that makes such learning possible (Solan et al., 2004; Edelman et al., 2004). In the present paper, we concentrated on testing the input module that allows the acquired patterns to be used in processing novel stimuli. The results of the tests we described here are encouraging, but there is clearly room for improvement. We believe that the most pressing issue in this regard is developing a conceptually and computationally well-founded approach to the notion of cover (that is, a distributed representation of a novel sentence in terms of the existing patterns). Intuitively, the best case, which should receive the top score, is when there is a single pattern that precisel</context>
</contexts>
<marker>Edelman, Solan, Horn, Ruppin, 2004</marker>
<rawString>S. Edelman, Z. Solan, D. Horn, and E. Ruppin. 2004. Bridging computational, formal and psycholinguistic approaches to language. In Proc. of the 26th Conference of the Cognitive Science Society, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Edelman</author>
</authors>
<title>Bridging language with the rest of cognition: computational, algorithmic and neurobiological issues and methods.</title>
<date>2004</date>
<booktitle>Proc. of the Ithaca workshop on Empirical Methods in Cognitive Linguistics.</booktitle>
<editor>In M. Gonzalez-Marquez, M. J. Spivey, S. Coulson, and I. Mittelberg, eds.,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="25700" citStr="Edelman, 2004" startWordPosition="4297" endWordPosition="4298">you take a picture of? Modeling Keller’s data. A manuscript by Frank Keller lists magnitude estimation data for eight sentences.5 We compared these to the scores produced by ADIOS, and obtained a significant correlation (Figure 7). The input module seems capable of dealing with the substitution of a with the or of take with destroy, and it does reasonably well on the substitution of How many men with Which man. We conjecture that this performance can be improved by a more sophisticated normalization of the score produced by the input module, which should do a better job quantifying the cover (Edelman, 2004) of a novel sentence by the stored patterns. The limitations of the present version of the model became apparent when we tested it on the 3Chance performance in this test is 33%. We note that the 82 4Four of the subjects only filled out the test partially (the corpus used here was too small to train an n-gram model for numbers of responses were 300, 300, 186, 159, 96, 60), but the n &gt; 2; thus, our algorithm effectively overcomes the problem correlation was highly significant despite the missing data. of sparse data by putting the available data to a better use. 5http://elib.uni-stuttgart.de/op</context>
</contexts>
<marker>Edelman, 2004</marker>
<rawString>S. Edelman. 2004. Bridging language with the rest of cognition: computational, algorithmic and neurobiological issues and methods. In M. Gonzalez-Marquez, M. J. Spivey, S. Coulson, and I. Mittelberg, eds., Proc. of the Ithaca workshop on Empirical Methods in Cognitive Linguistics. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
<author>E A Bates</author>
<author>M H Johnson</author>
<author>A Karmiloff-Smith</author>
<author>D Parisi</author>
<author>K Plunkett</author>
</authors>
<title>Rethinking innateness: A connectionist perspective on development.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1384" citStr="Elman et al., 1996" startWordPosition="199" endWordPosition="202">rd tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al., 1996). Computational models of language acquisition or “grammar induction” are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke “general-purpose” statistical learning mechanisms. We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework. On the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation and probab</context>
</contexts>
<marker>Elman, Bates, Johnson, Karmiloff-Smith, Parisi, Plunkett, 1996</marker>
<rawString>J. L. Elman, E. A. Bates, M. H. Johnson, A. Karmiloff-Smith, D. Parisi, and K. Plunkett. 1996. Rethinking innateness: A connectionist perspective on development. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E Goldberg</author>
</authors>
<title>Constructions: a new theoretical approach to language. Trends in Cognitive Sciences,</title>
<date>2003</date>
<pages>7--219</pages>
<contexts>
<context position="13853" citStr="Goldberg, 2003" startWordPosition="2289" endWordPosition="2290">ondence between the TAG operations and the ADIOS patterns, we believe that the latter represent regularities that are best described by Mildly ContextSensitive Language formalism (Joshi and Schabes, 1997). Importantly, because the ADIOS patterns are learned from data, they already incorporate the constraints on substitution and adjoining that in the original TAG framework must be specified manually. Psychological and linguistic evidence for patternbased representations. Recent advances in understanding the psychological role of representations based on what we call patterns, or constructions (Goldberg, 2003), focus on the use of statistical cues such as conditional probabilities in pattern learning (Saffran et al., 1996; G´omez, 2002), and on the importance of exemplars and constructions in children’s language acquisition (CameronFaulkner et al., 2003). Converging evidence for the centrality of pattern-like structures is provided by corpus-based studies of prefabs — sequences, continuous or discontinuous, of words that appear to be prefabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in synta</context>
<context position="15253" citStr="Goldberg, 2003" startWordPosition="2501" endWordPosition="2502"> methodological tenets of ADIOS — populating the lexicon with “units” of varying complexity and degree of entrenchment, and using cognition-general mechanisms for learning and representation — fit the spirit of the foundations of Cognitive Grammar (Langacker, 1987). At the same time, whereas the cognitive grammarians typically face the chore of hand-crafting structures that would reflect the logic of language as they perceive it, ADIOS discovers the primitives of grammar empirically and autonomously. The same is true also for the comparison between ADIOS and the various Construction Grammars (Goldberg, 2003; Croft, 2001), which are all hand-crafted. A construction grammar consists of elements that differ in their complexity and in the degree to which they are specified: an idiom such as “big deal” is a fully specified, immutable construction, whereas the expression “the X, the Y” – as in “the more, the better” (Kay and Fillmore, 1999) – is a partially specified template. The patterns learned by ADIOS likewise vary along the dimensions of complexity and specificity (e.g., not every pattern has an equivalence class). 4 ADIOS: a psycholinguistic evaluation To illustrate the applicability of our met</context>
</contexts>
<marker>Goldberg, 2003</marker>
<rawString>A. E. Goldberg. 2003. Constructions: a new theoretical approach to language. Trends in Cognitive Sciences, 7:219–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L G´omez</author>
</authors>
<title>Variability and detection of invariant structure.</title>
<date>2002</date>
<journal>Psychological Science,</journal>
<pages>13--431</pages>
<marker>G´omez, 2002</marker>
<rawString>R. L. G´omez. 2002. Variability and detection of invariant structure. Psychological Science, 13:431–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gross</author>
</authors>
<title>The construction of local grammars.</title>
<date>1997</date>
<booktitle>FiniteState Language Processing,</booktitle>
<pages>329--354</pages>
<editor>In E. Roche and Y. Schab`es, eds.,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10795" citStr="Gross, 1997" startWordPosition="1788" endWordPosition="1789">d comparisons to approaches that generally stop short of attempting to do what our algorithm does. We distinguish between approaches that are motivated computationally (Local Grammar and Variable Order Markov models, and Tree Adjoining Grammar, discussed elsewhere (Edelman et al., 2004), and those whose main motivation is linguistic and cognitive psychological (Cognitive and Construction grammars, discussed below). Local Grammar and Markov models. In capturing the regularities inherent in multiple crisscrossing paths through a corpus, ADIOS superficially resembles finite-state Local Grammars (Gross, 1997) and Variable Order Markov (VOM) models (Guyon and Pereira, 1995). The VOM approach starts by postulating a maximum-n structure, which is then fitted to the data by maximizing the likelihood of the training corpus. The ADIOS philosophy differs from the VOM approach in several key respects. First, rather than fitting a model to the data, we use the data to construct a (recursively structured) graph. Thus, our algorithm naturally addresses the inference of the graph’s structure, a task that is more difficult than the estimation of parameters for a given configuration. Second, because ADIOS works</context>
</contexts>
<marker>Gross, 1997</marker>
<rawString>M. Gross. 1997. The construction of local grammars. In E. Roche and Y. Schab`es, eds., FiniteState Language Processing, pages 329–354. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gurman-Bard</author>
<author>D Robertson</author>
<author>A Sorace</author>
</authors>
<title>Magnitude estimation of linguistic acceptability.</title>
<date>1996</date>
<journal>Language,</journal>
<pages>72--32</pages>
<marker>Gurman-Bard, Robertson, Sorace, 1996</marker>
<rawString>E. Gurman-Bard, D. Robertson, and A. Sorace. 1996. Magnitude estimation of linguistic acceptability. Language, 72:32–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Guyon</author>
<author>F Pereira</author>
</authors>
<title>Design of a linguistic postprocessor using Variable Memory Length Markov Models.</title>
<date>1995</date>
<booktitle>In Proc. 3rd Int’l Conf. Document Analysis and Recogition,</booktitle>
<pages>454--457</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="10860" citStr="Guyon and Pereira, 1995" startWordPosition="1796" endWordPosition="1799"> of attempting to do what our algorithm does. We distinguish between approaches that are motivated computationally (Local Grammar and Variable Order Markov models, and Tree Adjoining Grammar, discussed elsewhere (Edelman et al., 2004), and those whose main motivation is linguistic and cognitive psychological (Cognitive and Construction grammars, discussed below). Local Grammar and Markov models. In capturing the regularities inherent in multiple crisscrossing paths through a corpus, ADIOS superficially resembles finite-state Local Grammars (Gross, 1997) and Variable Order Markov (VOM) models (Guyon and Pereira, 1995). The VOM approach starts by postulating a maximum-n structure, which is then fitted to the data by maximizing the likelihood of the training corpus. The ADIOS philosophy differs from the VOM approach in several key respects. First, rather than fitting a model to the data, we use the data to construct a (recursively structured) graph. Thus, our algorithm naturally addresses the inference of the graph’s structure, a task that is more difficult than the estimation of parameters for a given configuration. Second, because ADIOS works from the bottom up in a recursive, data-driven fashion, it is le</context>
</contexts>
<marker>Guyon, Pereira, 1995</marker>
<rawString>I. Guyon and F. Pereira. 1995. Design of a linguistic postprocessor using Variable Memory Length Markov Models. In Proc. 3rd Int’l Conf. Document Analysis and Recogition, pages 454–457, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--140</pages>
<contexts>
<context position="3007" citStr="Harris, 1954" startWordPosition="441" endWordPosition="442">ion of structural regularities comparable to those captured by Context Sensitive Grammars out of the accrued statistical knowledge. In thinking about such regularities, we adopt Langacker’s notion of grammar as “simply an inventory of linguistic units” ((Langacker, 1987), p.63). To detect potentially useful units, we identify and process partially redundant sentences that share the same word sequences. We note that the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language (Harris, 1954), as well as for some modern work (van Zaanen, 2000). Likewise, the pattern — the syntagm and the equivalence class of complementary-distribution symbols that may appear in its open slot — is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure). Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists. Thus, the main computational principles behind the ADIOS model are ou</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. S. Harris. 1954. Distributional structure. Word, 10:140–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Henrichsen</author>
</authors>
<title>GraSp: Grammar learning form unlabeled speech corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>22--28</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="9754" citStr="Henrichsen, 2002" startWordPosition="1627" endWordPosition="1628">ments may be either terminals (i.e., fully specified strings) or other patterns. Moreover, patterns may refer to themselves, which opens the door for recursion. � eorge Joe ager hgu ease dear hers durbs Ji C� H c c a 78 3 Related computational and linguistic formalisms and psycholinguistic findings Unlike ADIOS, very few existing algorithms for unsupervised language acquisition use raw, unannotated corpus data (as opposed, say, to sentences converted into sequences of POS tags). The only work described in a recent review (Roberts and Atwell, 2002) as completely unsupervised — the GraSp model (Henrichsen, 2002) — does attempt to induce syntax from raw transcribed speech, yet it is not completely data-driven in that it makes a prior commitment to a particular theory of syntax (Categorial Grammar, complete with a pre-specified set of allowed categories). Because of the unique nature of our chosen challenge — finding structure in language rather than imposing it — the following brief survey of grammar induction focuses on contrasts and comparisons to approaches that generally stop short of attempting to do what our algorithm does. We distinguish between approaches that are motivated computationally (Lo</context>
</contexts>
<marker>Henrichsen, 2002</marker>
<rawString>P. J. Henrichsen. 2002. GraSp: Grammar learning form unlabeled speech corpora. In Proceedings of CoNLL-2002, pages 22–28. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-Adjoining Grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, eds.,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="12826" citStr="Joshi and Schabes, 1997" startWordPosition="2122" endWordPosition="2125">inite order ||b2||+||c2||+||b3|| because the size of [c2] is ill-defined, as explained above. Fourth, as we showed earlier (Figure 2), ADIOS incorporates both context-sensitive substitution and recursion. Tree Adjoining Grammar. The proper place in the Chomsky hierarchy for the class of strings accepted by our model is between Context Free and Context Sensitive Languages. The pattern-based representations employed by ADIOS have counterparts for each of the two composition operations, substitution and adjoining, that characterize a Tree Adjoining Grammar, or TAG, developed by Joshi and others (Joshi and Schabes, 1997). Specifically, both substitution and adjoining are subsumed in the relationships that hold among ADIOS patterns, such as the membership of one pattern in another. Consider a pattern PZ and its equivalence class £(PZ); any other pattern P; E £(PZ) can be seen as substitutable in PZ. Likewise, if P; E £(PZ), Pk E £(PZ) and Pk E £(P;), then the pattern P; can be seen as adjoinable to PZ. Because of this correspondence between the TAG operations and the ADIOS patterns, we believe that the latter represent regularities that are best described by Mildly ContextSensitive Language formalism (Joshi an</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A. Joshi and Y. Schabes. 1997. Tree-Adjoining Grammars. In G. Rozenberg and A. Salomaa, eds., Handbook of Formal Languages, volume 3, pages 69 – 124. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kay</author>
<author>C J Fillmore</author>
</authors>
<title>Grammatical constructions and linguistic generalizations: the What’s X Doing Y? construction.</title>
<date>1999</date>
<journal>Language,</journal>
<pages>75--1</pages>
<contexts>
<context position="15587" citStr="Kay and Fillmore, 1999" startWordPosition="2558" endWordPosition="2561">lly face the chore of hand-crafting structures that would reflect the logic of language as they perceive it, ADIOS discovers the primitives of grammar empirically and autonomously. The same is true also for the comparison between ADIOS and the various Construction Grammars (Goldberg, 2003; Croft, 2001), which are all hand-crafted. A construction grammar consists of elements that differ in their complexity and in the degree to which they are specified: an idiom such as “big deal” is a fully specified, immutable construction, whereas the expression “the X, the Y” – as in “the more, the better” (Kay and Fillmore, 1999) – is a partially specified template. The patterns learned by ADIOS likewise vary along the dimensions of complexity and specificity (e.g., not every pattern has an equivalence class). 4 ADIOS: a psycholinguistic evaluation To illustrate the applicability of our method to real data, we first describe briefly the outcome of running it on a subset of the CHILDES collection (MacWhinney and Snow, 1985), consisting of transcribed speech directed at children. The corpus we selected contained 300, 000 sentences (1.3 million tokens) produced by parents. After 14 real-time days, the algorithm (version </context>
</contexts>
<marker>Kay, Fillmore, 1999</marker>
<rawString>P. Kay and C. J. Fillmore. 1999. Grammatical constructions and linguistic generalizations: the What’s X Doing Y? construction. Language, 75:1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
</authors>
<date>2000</date>
<booktitle>Gradience in Grammar: Experimental and Computational Aspects ofDegrees of Grammaticality. Ph.D. thesis,</booktitle>
<institution>U. of Edinburgh.</institution>
<contexts>
<context position="24210" citStr="Keller, 2000" startWordPosition="4029" endWordPosition="4030"> performance in this and several other tests is summarized in Figure 6 (these tests have been conducted with an earlier version of the algorithm (Solan et al., 2003a)). In the ESL test, ADIOS scored at just under 60%; compare this to the 45% precision (with 20% recall) achieved by a straightforward bi-gram benchmark.3 ESL test (magnitude estimation). In this experiment, six subjects were asked to provide magnitude estimates of linguistic acceptability (GurmanBard et al., 1996) for all the 3 × 100 sentences in the G¨oteborg ESL test. The test was paper based and included the instructions from (Keller, 2000). No measures were taken to randomize the order of the sentences or otherwise control the experiment. The same 300 sentences were processed by ADIOS, whose responses were normalized by dividing the output by the sum of each triplet’s score. The results indicate a significant correlation (R2 = 6.3%, p &lt; 0.001) between the scores produced by the subjects and by ADIOS. In some cases the scores of Figure 7: Magnitude estimation study from Keller, plotted against the ADIOS score on the same sentences (R2 = 0.53,p &lt; 0.05). The sentences (ranked by increasing score) are: How many men did you destroy </context>
<context position="26425" citStr="Keller, 2000" startWordPosition="4412" endWordPosition="4413"> when we tested it on the 3Chance performance in this test is 33%. We note that the 82 4Four of the subjects only filled out the test partially (the corpus used here was too small to train an n-gram model for numbers of responses were 300, 300, 186, 159, 96, 60), but the n &gt; 2; thus, our algorithm effectively overcomes the problem correlation was highly significant despite the missing data. of sparse data by putting the available data to a better use. 5http://elib.uni-stuttgart.de/opus/volltexte/1999/81/pdf/81.pdf 52 sentences from Keller’s dissertation, using his magnitude estimation method (Keller, 2000).6 For these sentences, no correlation was found between the human and the model scores. One of the more challenging aspects of this set is the central role of pronoun binding in many of the sentences, e.g., The woman/Each woman saw Peter’s photograph of her/herself/him/himself. Moreover, this test set contains examples of context effects, where information in an earlier sentence can help resolve a later ambiguity. Thus, many of the grammatical contrasts that appear in Keller’s test sentences are too subtle for the present version of the ADIOS input module to handle. Acceptability of correct a</context>
</contexts>
<marker>Keller, 2000</marker>
<rawString>F. Keller. 2000. Gradience in Grammar: Experimental and Computational Aspects ofDegrees of Grammaticality. Ph.D. thesis, U. of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Langacker</author>
</authors>
<title>Foundations of cognitive grammar, volume I: theoretical prerequisites. Stanford U.</title>
<date>1987</date>
<publisher>Press,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="2665" citStr="Langacker, 1987" startWordPosition="388" endWordPosition="389">ll-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difficult to train. We are developing an approach to the acquisition of distributional information from raw input (e.g., transcribed speech corpora) that also supports the distillation of structural regularities comparable to those captured by Context Sensitive Grammars out of the accrued statistical knowledge. In thinking about such regularities, we adopt Langacker’s notion of grammar as “simply an inventory of linguistic units” ((Langacker, 1987), p.63). To detect potentially useful units, we identify and process partially redundant sentences that share the same word sequences. We note that the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language (Harris, 1954), as well as for some modern work (van Zaanen, 2000). Likewise, the pattern — the syntagm and the equivalence class of complementary-distribution symbols that may appear in its open slot — is the main representational building block of our system, ADIOS (for</context>
<context position="8728" citStr="Langacker, 1987" startWordPosition="1463" endWordPosition="1464">ird tion afforded by a set of patterns is inherently safer Cindy hink that Jm beeves that to ead is tough than in approaches that posit globally valid cateBeth thik tht J blive tht eth h lv gories (“parts of speech”) and rules (“grammar”). horse meows and the horse umps doesn&apos;t sh e? The reliance of ADIOS on many context-sensitive o that Pm is ough to plese worries the at patterns rather than on traditional rules can be compared both to the Construction Grammar (discussed later) and to Langacker’s concept of the grammar as a collection of “patterns of all intermediate degrees of generality” ((Langacker, 1987), p.46). Hierarchical structure of patterns. The ADIOS graph is rewired every time a new pattern is detected, so that a bundle of strings subsumed by it is represented by a single new edge. Following the rewiring, which is context-specific, potentially farapart symbols that used to straddle the newly abstracted pattern become close neighbors. Patterns thus become hierarchically structured in that their elements may be either terminals (i.e., fully specified strings) or other patterns. Moreover, patterns may refer to themselves, which opens the door for recursion. � eorge Joe ager hgu ease dear</context>
<context position="14904" citStr="Langacker, 1987" startWordPosition="2446" endWordPosition="2447">efabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in syntax of structural peculiarities hitherto marginalized as “exceptions” are now being voiced by linguists (Culicover, 1999; Croft, 2001). 79 Cognitive Grammar; Construction Grammar. The main methodological tenets of ADIOS — populating the lexicon with “units” of varying complexity and degree of entrenchment, and using cognition-general mechanisms for learning and representation — fit the spirit of the foundations of Cognitive Grammar (Langacker, 1987). At the same time, whereas the cognitive grammarians typically face the chore of hand-crafting structures that would reflect the logic of language as they perceive it, ADIOS discovers the primitives of grammar empirically and autonomously. The same is true also for the comparison between ADIOS and the various Construction Grammars (Goldberg, 2003; Croft, 2001), which are all hand-crafted. A construction grammar consists of elements that differ in their complexity and in the degree to which they are specified: an idiom such as “big deal” is a fully specified, immutable construction, whereas th</context>
</contexts>
<marker>Langacker, 1987</marker>
<rawString>R. W. Langacker. 1987. Foundations of cognitive grammar, volume I: theoretical prerequisites. Stanford U. Press, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
<author>C Snow</author>
</authors>
<title>The Child Language Exchange System.</title>
<date>1985</date>
<journal>Journal of Computational Lingustics,</journal>
<pages>12--271</pages>
<contexts>
<context position="15988" citStr="MacWhinney and Snow, 1985" startWordPosition="2625" endWordPosition="2628">complexity and in the degree to which they are specified: an idiom such as “big deal” is a fully specified, immutable construction, whereas the expression “the X, the Y” – as in “the more, the better” (Kay and Fillmore, 1999) – is a partially specified template. The patterns learned by ADIOS likewise vary along the dimensions of complexity and specificity (e.g., not every pattern has an equivalence class). 4 ADIOS: a psycholinguistic evaluation To illustrate the applicability of our method to real data, we first describe briefly the outcome of running it on a subset of the CHILDES collection (MacWhinney and Snow, 1985), consisting of transcribed speech directed at children. The corpus we selected contained 300, 000 sentences (1.3 million tokens) produced by parents. After 14 real-time days, the algorithm (version 7.3) identified 3400 patterns and 3200 equivalence classes. The outcome was encouraging: the algorithm found intuitively significant patterns and produced semantically adequate corresponding equivalence sets. The algorithm’s ability to recombine and reuse the acquired patterns is exemplified in the legend of Figure 3, which lists some of the novel sentences it generated. The input module. The ADIOS</context>
<context position="17247" citStr="MacWhinney and Snow, 1985" startWordPosition="2824" endWordPosition="2827">to process a novel sentence by forming its distributed representation in terms of activities of existing patterns. We stress that this module plays a crucial role in the tests described below, all of which require dealing with novel inputs. Figure 4 shows the activation of two patterns (#141 and #120) by a phrase that contains a word in a novel context (stay), as well as another word never before encountered in any context (5pm). Acceptability of correct and perturbed novel sentences. To test the quality of the representations Figure 3: a typical pattern extracted from the CHILDES collection (MacWhinney and Snow, 1985). Hundreds of such patterns and equivalence classes (underscored) together constitute a concise representation of the raw data. Some of the phrases that can be described/generated by these patterns are: let’s change her...; I thought you were gonna change her... ; I was going to change your... ; none of these appear in the training data, illustrating the ability of ADIOS to generalize. The generation process operates as a depth-first search of the tree corresponding to a pattern. For details see (Solan et al., 2003a; Solan et al., 2004). (patterns and their associated equivalence classes) acqu</context>
</contexts>
<marker>MacWhinney, Snow, 1985</marker>
<rawString>B. MacWhinney and C. Snow. 1985. The Child Language Exchange System. Journal of Computational Lingustics, 12:271–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roberts</author>
<author>E Atwell</author>
</authors>
<title>Unsupervised grammar inference systems for natural language.</title>
<date>2002</date>
<tech>Technical Report 2002.20,</tech>
<institution>School of Computing, U. of Leeds.</institution>
<contexts>
<context position="1312" citStr="Roberts and Atwell, 2002" startWordPosition="187" endWordPosition="190">el’s ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al., 1996). Computational models of language acquisition or “grammar induction” are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke “general-purpose” statistical learning mechanisms. We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework. On the one hand, the statistical approach is geared to take advantage of the considerable pro</context>
<context position="9690" citStr="Roberts and Atwell, 2002" startWordPosition="1616" endWordPosition="1619">ghbors. Patterns thus become hierarchically structured in that their elements may be either terminals (i.e., fully specified strings) or other patterns. Moreover, patterns may refer to themselves, which opens the door for recursion. � eorge Joe ager hgu ease dear hers durbs Ji C� H c c a 78 3 Related computational and linguistic formalisms and psycholinguistic findings Unlike ADIOS, very few existing algorithms for unsupervised language acquisition use raw, unannotated corpus data (as opposed, say, to sentences converted into sequences of POS tags). The only work described in a recent review (Roberts and Atwell, 2002) as completely unsupervised — the GraSp model (Henrichsen, 2002) — does attempt to induce syntax from raw transcribed speech, yet it is not completely data-driven in that it makes a prior commitment to a particular theory of syntax (Categorial Grammar, complete with a pre-specified set of allowed categories). Because of the unique nature of our chosen challenge — finding structure in language rather than imposing it — the following brief survey of grammar induction focuses on contrasts and comparisons to approaches that generally stop short of attempting to do what our algorithm does. We disti</context>
</contexts>
<marker>Roberts, Atwell, 2002</marker>
<rawString>A. Roberts and E. Atwell. 2002. Unsupervised grammar inference systems for natural language. Technical Report 2002.20, School of Computing, U. of Leeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical learning by 8-month-old infants.</title>
<date>1996</date>
<journal>Science,</journal>
<pages>274--1926</pages>
<contexts>
<context position="13967" citStr="Saffran et al., 1996" startWordPosition="2307" endWordPosition="2310">that are best described by Mildly ContextSensitive Language formalism (Joshi and Schabes, 1997). Importantly, because the ADIOS patterns are learned from data, they already incorporate the constraints on substitution and adjoining that in the original TAG framework must be specified manually. Psychological and linguistic evidence for patternbased representations. Recent advances in understanding the psychological role of representations based on what we call patterns, or constructions (Goldberg, 2003), focus on the use of statistical cues such as conditional probabilities in pattern learning (Saffran et al., 1996; G´omez, 2002), and on the importance of exemplars and constructions in children’s language acquisition (CameronFaulkner et al., 2003). Converging evidence for the centrality of pattern-like structures is provided by corpus-based studies of prefabs — sequences, continuous or discontinuous, of words that appear to be prefabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in syntax of structural peculiarities hitherto marginalized as “exceptions” are now being voiced by linguists (Culicover, </context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996. Statistical learning by 8-month-old infants. Science, 274:1926–1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Solan</author>
<author>E Ruppin</author>
<author>D Horn</author>
<author>S Edelman</author>
</authors>
<title>Automatic acquisition and efficient representation of syntactic structures.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing,</booktitle>
<volume>15</volume>
<editor>In S. Thrun, editor,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3759" citStr="Solan et al., 2003" startWordPosition="562" endWordPosition="565">istribution symbols that may appear in its open slot — is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure). Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists. Thus, the main computational principles behind the ADIOS model are outlined here only briefly. The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al., 2003a; Solan et al., 2003b; Solan et al., 2004; Edelman et al., 2004). 2 The principles behind the ADIOS algorithm The representational power of ADIOS and its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns. Each of these is described briefly below. 77 84 0.0001 58 63 51 50 48 that Beth Cindy George Jim Joe Pam a bird cat cow dog horse rabbit r the ird L bcat cow dog I horse I rabbit L flies jumps • laughs annoyes bothers I disturbs &apos; I worries </context>
<context position="17767" citStr="Solan et al., 2003" startWordPosition="2908" endWordPosition="2911">ions Figure 3: a typical pattern extracted from the CHILDES collection (MacWhinney and Snow, 1985). Hundreds of such patterns and equivalence classes (underscored) together constitute a concise representation of the raw data. Some of the phrases that can be described/generated by these patterns are: let’s change her...; I thought you were gonna change her... ; I was going to change your... ; none of these appear in the training data, illustrating the ability of ADIOS to generalize. The generation process operates as a depth-first search of the tree corresponding to a pattern. For details see (Solan et al., 2003a; Solan et al., 2004). (patterns and their associated equivalence classes) acquired by ADIOS, we have examined their ability to support various kinds of grammaticality judgments. The first experiment we report sought to make a distinction between a set of (presumably grammatical) CHILDES sentences not seen by the algorithm during training, and the same sentences in which the word order has been perturbed. We first trained the model on 10, 000 sentences from CHILDES, then compared its performance on (1) 1000 previously unseen sentences and (2) the same sentences in each of which a single rando</context>
<context position="19395" citStr="Solan et al., 2003" startWordPosition="3182" endWordPosition="3185">into patterns with embedded equivalence classes. 16555 (0.14) 1 16556 2 15543 (0.33) 3 18 15539 (1) 11 5 15540 15544 6 12 14374 (1) 4 14819 (1) 14384 (1) 7 13 17 14335 14378 (1) 14818(1) 14383 (1) 16557 8 9 10 14 1516 19 let &apos; s I &apos;m wa s thought you were gonna go ing to change her your 80 Figure 4: The two most active patterns responding to the partially novel input Joe and Beth are staying until 5pm. Leaf activation, which is proportional to the mutual information between input words and various members of the equivalence classes, is propagated upward by taking the average at each junction (Solan et al., 2003a). 120... activation level: 0.667 C14 C15 C16 C17 C18 C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 are liv Joe Joe Jim Jim BEGIN work ing play Pam and Pam Beth Cindy George sday Beth Cindy George until tomorrow Friday Monday Saturday Sunday Thursday Tuesday Wednesday next month week winter END 141... activation level: 0.972 119 W0=1.0 W13=1.0 86 113 W0=1.0 W1=E W2..8=F 100 74 112 93 W8=1.0 W15=0.8 89 Figure 5: Grammaticality of perturbed sentences (CHILDES data). The figure shows a histogram of the input module output values for two kinds of stim</context>
<context position="23761" citStr="Solan et al., 2003" startWordPosition="3953" endWordPosition="3956">9 years of school, but only 6-7 years of English). The test consists of 100 three-choice questions, such as She asked me at once (choices: come, to come, coming) and The tickets have been paid for, so you not worry (choices: may, dare, need); the average score for the population mentioned is 65%. As before, the choice given the highest score by the algorithm won; if two choices received the same top score, the answer was “don’t know”. The algorithm’s performance in this and several other tests is summarized in Figure 6 (these tests have been conducted with an earlier version of the algorithm (Solan et al., 2003a)). In the ESL test, ADIOS scored at just under 60%; compare this to the 45% precision (with 20% recall) achieved by a straightforward bi-gram benchmark.3 ESL test (magnitude estimation). In this experiment, six subjects were asked to provide magnitude estimates of linguistic acceptability (GurmanBard et al., 1996) for all the 3 × 100 sentences in the G¨oteborg ESL test. The test was paper based and included the instructions from (Keller, 2000). No measures were taken to randomize the order of the sentences or otherwise control the experiment. The same 300 sentences were processed by ADIOS, w</context>
</contexts>
<marker>Solan, Ruppin, Horn, Edelman, 2003</marker>
<rawString>Z. Solan, E. Ruppin, D. Horn, and S. Edelman. 2003a. Automatic acquisition and efficient representation of syntactic structures. In S. Thrun, editor, Advances in Neural Information Processing, volume 15, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Solan</author>
<author>E Ruppin</author>
<author>D Horn</author>
<author>S Edelman</author>
</authors>
<title>Unsupervised efficient learning and representation of language structure.</title>
<date>2003</date>
<booktitle>Proc. 25th Conference of the Cognitive Science Society,</booktitle>
<editor>In R. Alterman and D. Kirsh, eds.,</editor>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="3759" citStr="Solan et al., 2003" startWordPosition="562" endWordPosition="565">istribution symbols that may appear in its open slot — is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure). Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists. Thus, the main computational principles behind the ADIOS model are outlined here only briefly. The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al., 2003a; Solan et al., 2003b; Solan et al., 2004; Edelman et al., 2004). 2 The principles behind the ADIOS algorithm The representational power of ADIOS and its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns. Each of these is described briefly below. 77 84 0.0001 58 63 51 50 48 that Beth Cindy George Jim Joe Pam a bird cat cow dog horse rabbit r the ird L bcat cow dog I horse I rabbit L flies jumps • laughs annoyes bothers I disturbs &apos; I worries </context>
<context position="17767" citStr="Solan et al., 2003" startWordPosition="2908" endWordPosition="2911">ions Figure 3: a typical pattern extracted from the CHILDES collection (MacWhinney and Snow, 1985). Hundreds of such patterns and equivalence classes (underscored) together constitute a concise representation of the raw data. Some of the phrases that can be described/generated by these patterns are: let’s change her...; I thought you were gonna change her... ; I was going to change your... ; none of these appear in the training data, illustrating the ability of ADIOS to generalize. The generation process operates as a depth-first search of the tree corresponding to a pattern. For details see (Solan et al., 2003a; Solan et al., 2004). (patterns and their associated equivalence classes) acquired by ADIOS, we have examined their ability to support various kinds of grammaticality judgments. The first experiment we report sought to make a distinction between a set of (presumably grammatical) CHILDES sentences not seen by the algorithm during training, and the same sentences in which the word order has been perturbed. We first trained the model on 10, 000 sentences from CHILDES, then compared its performance on (1) 1000 previously unseen sentences and (2) the same sentences in each of which a single rando</context>
<context position="19395" citStr="Solan et al., 2003" startWordPosition="3182" endWordPosition="3185">into patterns with embedded equivalence classes. 16555 (0.14) 1 16556 2 15543 (0.33) 3 18 15539 (1) 11 5 15540 15544 6 12 14374 (1) 4 14819 (1) 14384 (1) 7 13 17 14335 14378 (1) 14818(1) 14383 (1) 16557 8 9 10 14 1516 19 let &apos; s I &apos;m wa s thought you were gonna go ing to change her your 80 Figure 4: The two most active patterns responding to the partially novel input Joe and Beth are staying until 5pm. Leaf activation, which is proportional to the mutual information between input words and various members of the equivalence classes, is propagated upward by taking the average at each junction (Solan et al., 2003a). 120... activation level: 0.667 C14 C15 C16 C17 C18 C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 are liv Joe Joe Jim Jim BEGIN work ing play Pam and Pam Beth Cindy George sday Beth Cindy George until tomorrow Friday Monday Saturday Sunday Thursday Tuesday Wednesday next month week winter END 141... activation level: 0.972 119 W0=1.0 W13=1.0 86 113 W0=1.0 W1=E W2..8=F 100 74 112 93 W8=1.0 W15=0.8 89 Figure 5: Grammaticality of perturbed sentences (CHILDES data). The figure shows a histogram of the input module output values for two kinds of stim</context>
<context position="23761" citStr="Solan et al., 2003" startWordPosition="3953" endWordPosition="3956">9 years of school, but only 6-7 years of English). The test consists of 100 three-choice questions, such as She asked me at once (choices: come, to come, coming) and The tickets have been paid for, so you not worry (choices: may, dare, need); the average score for the population mentioned is 65%. As before, the choice given the highest score by the algorithm won; if two choices received the same top score, the answer was “don’t know”. The algorithm’s performance in this and several other tests is summarized in Figure 6 (these tests have been conducted with an earlier version of the algorithm (Solan et al., 2003a)). In the ESL test, ADIOS scored at just under 60%; compare this to the 45% precision (with 20% recall) achieved by a straightforward bi-gram benchmark.3 ESL test (magnitude estimation). In this experiment, six subjects were asked to provide magnitude estimates of linguistic acceptability (GurmanBard et al., 1996) for all the 3 × 100 sentences in the G¨oteborg ESL test. The test was paper based and included the instructions from (Keller, 2000). No measures were taken to randomize the order of the sentences or otherwise control the experiment. The same 300 sentences were processed by ADIOS, w</context>
</contexts>
<marker>Solan, Ruppin, Horn, Edelman, 2003</marker>
<rawString>Z. Solan, E. Ruppin, D. Horn, and S. Edelman. 2003b. Unsupervised efficient learning and representation of language structure. In R. Alterman and D. Kirsh, eds., Proc. 25th Conference of the Cognitive Science Society, Hillsdale, NJ. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Solan</author>
<author>D Horn</author>
<author>E Ruppin</author>
<author>S Edelman</author>
</authors>
<title>Unsupervised context sensitive language acquisition from a large corpus.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing,</booktitle>
<volume>16</volume>
<editor>In L. Saul, editor,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3801" citStr="Solan et al., 2004" startWordPosition="570" endWordPosition="573"> open slot — is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure). Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists. Thus, the main computational principles behind the ADIOS model are outlined here only briefly. The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al., 2003a; Solan et al., 2003b; Solan et al., 2004; Edelman et al., 2004). 2 The principles behind the ADIOS algorithm The representational power of ADIOS and its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns. Each of these is described briefly below. 77 84 0.0001 58 63 51 50 48 that Beth Cindy George Jim Joe Pam a bird cat cow dog horse rabbit r the ird L bcat cow dog I horse I rabbit L flies jumps • laughs annoyes bothers I disturbs &apos; I worries Beth Cindy George Jim Joe Pam who adores I</context>
<context position="7429" citStr="Solan et al., 2004" startWordPosition="1240" endWordPosition="1243">a 55 71 53 67 ADIOS represents a corpus of sentences as an inithat Pam tough to please worries th cat tially highly redundant directed graph, which can be informally visualized as a tangle of strands that are partially segregated into bundles. Each of these consists of some strands clumped together; a bundle is formed when two or more strands join together and 53 53 run in parallel and is dissolved when more strands leave the bundle than stay in. In a given corpus, there will be many bundles, with each strand (sentence) possibly participating in several. Our algorithm, described in detail in (Solan et al., 2004), identifies significant bundles that balance high compression (small size of the bundle “lexicon”) against good generalization (the ability to generate new grammatical sentences by splicing together various strand fragments each of which belongs to a different bundle). Context sensitivity of patterns. A pattern is an abstraction of a bundle of sentences that are identical up to variation in one place, where one of several symbols — the members of the equivalence class associated with the pattern — may appear (Figeorg C Jim eay ure 1). Because this variation is only allowed in they? the contex</context>
<context position="17789" citStr="Solan et al., 2004" startWordPosition="2912" endWordPosition="2915">cal pattern extracted from the CHILDES collection (MacWhinney and Snow, 1985). Hundreds of such patterns and equivalence classes (underscored) together constitute a concise representation of the raw data. Some of the phrases that can be described/generated by these patterns are: let’s change her...; I thought you were gonna change her... ; I was going to change your... ; none of these appear in the training data, illustrating the ability of ADIOS to generalize. The generation process operates as a depth-first search of the tree corresponding to a pattern. For details see (Solan et al., 2003a; Solan et al., 2004). (patterns and their associated equivalence classes) acquired by ADIOS, we have examined their ability to support various kinds of grammaticality judgments. The first experiment we report sought to make a distinction between a set of (presumably grammatical) CHILDES sentences not seen by the algorithm during training, and the same sentences in which the word order has been perturbed. We first trained the model on 10, 000 sentences from CHILDES, then compared its performance on (1) 1000 previously unseen sentences and (2) the same sentences in each of which a single random word order switch ha</context>
<context position="30414" citStr="Solan et al., 2004" startWordPosition="5043" endWordPosition="5046">bjects as the original ones. 5 Concluding remarks The ADIOS approach to the representation of linguistic knowledge resembles the Construction Grammar in its general philosophy (e.g., in its reliance on structural generalizations rather than on syntax projected by the lexicon), and the Tree Adjoining Grammar in its computational capacity (e.g., in its apparent ability to accept Mildly Context Sensitive Languages). The representations learned by the ADIOS algorithm are truly emergent from the (unannotated) corpus data. Previous studies focused on the algorithm that makes such learning possible (Solan et al., 2004; Edelman et al., 2004). In the present paper, we concentrated on testing the input module that allows the acquired patterns to be used in processing novel stimuli. The results of the tests we described here are encouraging, but there is clearly room for improvement. We believe that the most pressing issue in this regard is developing a conceptually and computationally well-founded approach to the notion of cover (that is, a distributed representation of a novel sentence in terms of the existing patterns). Intuitively, the best case, which should receive the top score, is when there is a singl</context>
</contexts>
<marker>Solan, Horn, Ruppin, Edelman, 2004</marker>
<rawString>Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2004. Unsupervised context sensitive language acquisition from a large corpus. In L. Saul, editor, Advances in Neural Information Processing, volume 16, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M van Zaanen</author>
</authors>
<title>ABL: Alignment-Based Learning.</title>
<date>2000</date>
<booktitle>In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>961--967</pages>
<marker>van Zaanen, 2000</marker>
<rawString>M. van Zaanen. 2000. ABL: Alignment-Based Learning. In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics, pages 961–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wray</author>
</authors>
<title>Formulaic language and the lexicon. Cambridge U.</title>
<date>2002</date>
<publisher>Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="14405" citStr="Wray, 2002" startWordPosition="2375" endWordPosition="2376">on what we call patterns, or constructions (Goldberg, 2003), focus on the use of statistical cues such as conditional probabilities in pattern learning (Saffran et al., 1996; G´omez, 2002), and on the importance of exemplars and constructions in children’s language acquisition (CameronFaulkner et al., 2003). Converging evidence for the centrality of pattern-like structures is provided by corpus-based studies of prefabs — sequences, continuous or discontinuous, of words that appear to be prefabricated, that is, stored and retrieved as a whole, rather than being subject to syntactic processing (Wray, 2002). Similar ideas concerning the ubiquity in syntax of structural peculiarities hitherto marginalized as “exceptions” are now being voiced by linguists (Culicover, 1999; Croft, 2001). 79 Cognitive Grammar; Construction Grammar. The main methodological tenets of ADIOS — populating the lexicon with “units” of varying complexity and degree of entrenchment, and using cognition-general mechanisms for learning and representation — fit the spirit of the foundations of Cognitive Grammar (Langacker, 1987). At the same time, whereas the cognitive grammarians typically face the chore of hand-crafting struc</context>
</contexts>
<marker>Wray, 2002</marker>
<rawString>A. Wray. 2002. Formulaic language and the lexicon. Cambridge U. Press, Cambridge, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>