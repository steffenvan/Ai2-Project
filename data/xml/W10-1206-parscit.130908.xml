<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011431">
<title confidence="0.931966">
Object Search: Supporting Structured Queries in Web Search Engines
</title>
<author confidence="0.970416">
Kim Cuong Pham†, Nicholas Rizzolo†, Kevin Small$, Kevin Chen-Chuan Chang†, Dan Roth†
</author>
<affiliation confidence="0.9825555">
University of Illinois at Urbana-Champaign† Tufts University$
Department of Computer Science Department of Computer Science
</affiliation>
<email confidence="0.987698">
{kimpham2, rizzolo, kcchang, danr}@illinois.edu kevin.small@tufts.edu
</email>
<sectionHeader confidence="0.996487" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999158458333333">
As the web evolves, increasing quantities of
structured information is embedded in web
pages in disparate formats. For example, a
digital camera’s description may include its
price and megapixels whereas a professor’s
description may include her name, univer-
sity, and research interests. Both types of
pages may include additional ambiguous in-
formation. General search engines (GSEs)
do not support queries over these types of
data because they ignore the web document
semantics. Conversely, describing requi-
site semantics through structured queries into
databases populated by information extraction
(IE) techniques are expensive and not easily
adaptable to new domains. This paper de-
scribes a methodology for rapidly develop-
ing search engines capable of answering struc-
tured queries over unstructured corpora by uti-
lizing machine learning to avoid explicit IE.
We empirically show that with minimum ad-
ditional human effort, our system outperforms
a GSE with respect to structured queries with
clear object semantics.
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999703">
General search engines (GSEs) are sufficient for
fulfilling the information needs of most queries.
However, they are often inadequate for retrieving
web pages that concisely describe real world ob-
jects as these queries require analysis of both un-
structured text and structured data contained in web
pages. For example, digital cameras with specific
brand, megapixel, zoom, and price attributes might
</bodyText>
<page confidence="0.991982">
44
</page>
<bodyText confidence="0.997652441176471">
be found on an online shopping website, or a pro-
fessor with her name, university, department, and
research interest attributes might be found on her
homepage. Correspondingly, as the web continues
to evolve from a general text corpus into a hetero-
geneous collection of documents, targeted retrieval
strategies must be developed for satisfying these
more precise information needs. We accomplish this
by using structured queries to capture the intended
semantics of a user query and learning domain spe-
cific ranking functions to represent the hidden se-
mantics of object classes contained in web pages.
It is not uncommon for a user to want to pose an
object query on the web. For example, an online
shopper might be looking for shopping pages that
sell canon digital cameras with 5 megapixels cost-
ing no more than $300. A graduate student might
be looking for homepages of computer science pro-
fessors who work in the information retrieval area.
Such users expect to get a list web pages containing
objects they are looking for, or object pages, which
we will define more precisely in later sections.
GSEs rarely return satisfactory results when the
user has a structured query in mind for two primary
reasons. Firstly, GSEs only handle keyword queries
whereas structured queries frequently involve data
field semantics (e.g. numerical constraints) and ex-
hibit field interdependencies. Secondly, since GSEs
are domain-agnostic, they will generally rank cam-
era pages utilizing the same functions as a profes-
sor’s homepage, ignoring much of the structured in-
formation specific to particular domains.
Conversely, vertical search engines (e.g. DBLife,
cazoodle.com, Rexa.info, etc.) approach this prob-
</bodyText>
<note confidence="0.9587835">
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44–52,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999947451612903">
lem from the information extraction (IE) perspec-
tive. Instead of searching an inverted index directly,
they first extract data records from text (Kushmer-
ick et al., 1997; McCallum et al., 2000). IE solu-
tions, even with large scale techniques (Agichtein,
2005), do not scale to the entire web and cost signif-
icantly more than GSEs. Secondly, creating domain-
specific models or wrappers require labeling training
examples and human expertise for each individual
site. Thirdly, pre-extracting information lacks flexi-
bility; decisions made during IE are irrevocable, and
at query time, users may find additional value in par-
tial or noisy records that were discarded by the IE
system.
These issues motivate our novel approach for de-
signing a GSE capable of answering complex struc-
tured queries, which we refer to as Object Search.
At a high level, we search web pages containing
structured information directly over their feature in-
dex, similarly to GSEs, adding expressivity by re-
formulating the structured query such that it can be
executed on a traditional inverted index. Thus, we
avoid the expense incurred by IE approaches when
supporting new object domains. From a techni-
cal perspective, this work describes a principled ap-
proach to customizing GSEs to answer structured
queries from any domain by proposing a composi-
tional ranking model for ranking web pages with
regards to structured queries and presenting an in-
teractive learning approach that eases the process of
training for a new domain.
</bodyText>
<sectionHeader confidence="0.976199" genericHeader="introduction">
2 The Object Search Problem
</sectionHeader>
<bodyText confidence="0.99072875">
The Object Search problem is to find the object
pages that answer a user’s object query. An object
query belongs to an object domain. An object do-
main defines a set of object attributes. An object
query is simply a set of constraints over these at-
tributes. Thus we define an object query as a tuple
of n constraints q - c1 n c2 n .. n cn, where ci is a
constraint on attribute ai. More specifically, a con-
straint ci is defined as a set of acceptable values Oi
for attribute ai; i.e. ci = (ai E Oi). For example, an
equality constraint such as “the brand is Canon” can
be specified as (abrand E {Canon}) and a numeric
range constraint such as “the price is at most $200”
can be specified as (apri,, E [0, 200]). When the
user does not care about an attribute, the constraint
is the constant true.
Given an object query, we want a set of satis-
fying object pages. Specifically, object pages are
pages that represent exactly one inherent object on
the web. Pages that list several objects such as a
department directory page or camera listing pages
are not considered object pages because even though
they mentioned the object, they do not represent any
particular object. There is often a single object page
but there are many web pages that mention the ob-
ject.
The goal of Object Search is similar to learning to
rank problems (Liu, 2009), in that its goal is to learn
a ranking function p : D x 2 —* R that ranks any
(document, query) pairs. This is accomplished by
learning an function over a set of relevant features.
Each feature can be modeled as a function that takes
the pair and outputs a real value 0 : D x 2 —* R.
For example, a term frequency feature outputs the
number of times the query appears in the document.
We define a function 4b = (01, 02, ...0n) that takes a
(document, query) pair and outputs a vector of fea-
tures. The original ranking function can be written
as p(d, q) = p&apos;(4b(d, q)) where p&apos; : Rn —* R is the
function; i.e.:
</bodyText>
<equation confidence="0.989839">
p = p&apos; ° `b (1)
</equation>
<bodyText confidence="0.984102315789474">
Despite the similarities, Object Search differs
from traditional information retrieval (IR) problems
in many respects. First, IR can answer only keyword
queries whereas an object query is structured by
keyword constraints as well as numeric constraints.
Second, Object Search results are “focused”, in the
sense that they must contain an object, as opposed
to the broad notion of relevance in IR. Finally, since
object pages of different domains might have little
in common, we cannot apply the same ranking func-
tion for different object domains.
As a consequence, in a learning to rank problem,
the set of features 4b are fixed for all query. The
major concern is learning the function p&apos;. In Object
Search settings, we expect different 4b for each ob-
ject domain. Thus, we have to derive both 4b and
p&apos;.
There are a number of challenges in solving these
problems. First, we need a deeper understanding of
</bodyText>
<page confidence="0.998787">
45
</page>
<bodyText confidence="0.999943142857143">
structured information embedded in web pages. In
many cases, an object attribute such as professor’s
university might appear only once in his homepage.
Thus, using a traditional bag-of-words model is of-
ten insufficient, because one cannot distinguish the
professor own university from other university men-
tioned in his homepage. Second, we will need train-
ing data to train a new ranking function for each
new object domain. Thus, we require an efficient
bootstrapping method to tackle this problem. Fi-
nally, any acceptable solution must scale to the size
of the web. This requirement poses challenges for
efficient query processing and efficient ranking via
the learned ranking function.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="method">
3 Object Search Framework
</sectionHeader>
<bodyText confidence="0.999718166666667">
In this section, we illustrate the primary intuitions
behind our aproach for an Object Search solu-
tion. We describe its architecture, which serves
as a search engine framework to support structured
queries of any domain. The technical details of ma-
jor components are left for subsequent sections.
</bodyText>
<subsectionHeader confidence="0.986937">
3.1 Intuition
</subsectionHeader>
<bodyText confidence="0.999904957746479">
The main idea behind our proposed approach is that
we develop different vertical search engines to sup-
port object queries in different domains. However,
we want to keep the cost of supporting each new
domain as small as possible. The key principles to
keep the cost small are to 1) share as much as pos-
sible between search engines of different domains
and 2) automate the process as much as possible
using machine learning techniques. To illustrate
our proposed approach, we suppose that an user is
searching the web for cameras. Her object query is
q = abrand E {canon} n apriee E [0, 200].
First, we have to automatically learn a function p
that ranks web pages given an object query as de-
scribed in Section 2. We observe web pages rele-
vant to the query and notice several salient features
such as “the word canon appears in the title”, “the
word canon appears near manufacturer”, “interest-
ing words that appear include powershot, eos, ixus”,
and “a price value appears after ’$’ near the word
price or sale”. Intuitively, pages containing these
features have a much higher chance of containing
the Canon camera being searched. Given labeled
training data, we can learn a ranking function that
combines these features to produce the probability
of a page containing the desired camera object.
Furthermore, we need to answer user query at
query time. We need to be able to look up these
features efficiently from our index of the web. A
naive method to index the web is to store a list of
web pages that have the above features, and at query
time, union all pages that have one or more features,
aggregate the score for each web page, and return
the ranked result. There are three problems with this
method. First, these features are dependent on each
object domain; thus, the size of the index will in-
crease as the number of domains grows. Second,
each time a new domain is added, a new set of fea-
tures needs to be indexed, and we have to extract
features for every single web page again. Third, we
have to know beforehand the list of camera brands,
megapixel ranges, price ranges, etc, which is infea-
sible for most object domain.
However, we observe that the above query de-
pendent features can be computed efficiently from
a query independent index. For example, whether
“the word canon appears near manufacturer” can be
computed if we index all occurrences of the words
canon and manufacturer. Similarly, the feature “the
word canon appears in the title” can be computed if
we index all the words from web pages’ title, which
only depends on the web pages themselves. Since
the words and numbers from different parts of a web
page can be indexed independently of the object do-
main, we can share them across different domains.
Thus, we follow the first principle mentioned above.
Of course, computing query dependent features
from the domain independent index is more expen-
sive than computing it from the naive index above.
However, this cost is scalable to the web. As a mat-
ter of fact, these features are equivalent to “phrase
search” features in modern search engines.
Thus, at a high level, we solve the Object Search
problem by learning a domain dependent ranking
function for each object domain. We store basic do-
main independent features of the web in our index.
At query time, we compute domain dependent fea-
tures from this index and apply the ranking function
to return a ranked list of web pages. In this paper, we
focus on the learning problems, leaving the problem
of efficient query processing for future work.
</bodyText>
<page confidence="0.999237">
46
</page>
<figureCaption confidence="0.997185">
Figure 1: Object Search Architecture
</figureCaption>
<subsectionHeader confidence="0.998297">
3.2 System Architecture
</subsectionHeader>
<bodyText confidence="0.999788860465116">
The main goal of our Object Search system is to en-
able searching the web with object queries. In order
to do this, the system must address the challenges
described in Section 2. From the end-user’s point
of view, the system must promptly and accurately
return web pages for their object query. From the
developer’s point of view, the system must facilitate
building a new search engine to support his object
domain of interest. The goal of the architecture is to
orchestrate all of these requirements.
Figure 1 depicts Object Search architecture. It
shows how different components of Object Search
interact with an end-user and a developer. The end-
user can issue any object query of known domains.
Each time the system receives an object query from
the end-user, it translates the query into a domain in-
dependent feature query. Then the Query Processor
executes the feature query on the inverted index, ag-
gregates the features using learned function ρ&apos;, and
returns a ranked list of web pages to the user.
The developer’s job is to define his object domain
and train a ranking function for it. He does it by
incrementally training the function. He starts by an-
notating a few web pages and running a learning al-
gorithm to produce a ranking function, which is then
used to retrieve more data for the developer to anno-
tate. The process iterates until the developer is satis-
fied with his trained ranking function for the object
domain.
More specifically, the Ranking Function Learner
module learns the function ρ&apos; and 4b as mentioned in
Section 2. The Query Translator instantiates 4b with
user object query q, resulting in 4b(q). Recall that 4b
is a set of feature functions φZ. Each φZ is a function
of a (d, q) pair such as “term frequency of ak in title”
(ak is an attribute of the object). Thus we can instan-
tiate φ(q) by replacing ak with θk, which is part of
the query q. For example, if θk = {canon} in the
previous example, then φ(q) is “term frequency of
canon in title”. Thus φ(q) becomes a query indepen-
dent feature and 4b(q) becomes a feature query that
can be executed in our inverted index by the Query
Processor.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="method">
4 Learning for Structured Ranking
</sectionHeader>
<bodyText confidence="0.999997083333333">
We now describe how we learn the domain depen-
dent ranking function ρ, which is the core learn-
ing aspect of Object Search. As mentioned in the
previous section, ρ differs from existing learning
to rank work due to the structure in object queries.
We exploit this structure to decompose the ranking
function into several components (Section 4.1) and
combine them using a probabilistic model. Exist-
ing learning to rank methods can then be leveraged
to rank the individual components. Section 4.2 de-
scribes how we fit individual ranking scores into our
probabilistic model by calibrating their probability.
</bodyText>
<subsectionHeader confidence="0.98347">
4.1 Ranking model
</subsectionHeader>
<bodyText confidence="0.9999725">
As stated, ρ models the joint probability distribu-
tion over the space of documents and queries ρ =
P(d, q). Once estimated, this distribution can rank
documents in D according to their probability of sat-
isfying q. Since we are only interested in finding
satisfying object pages, we introduce a variable ω
which indicates if the document d is an object page.
Furthermore, we introduce n variables ζZ which in-
dicate whether constraint cZ in the query q is satis-
fied. The probability computed by ρ is then:
</bodyText>
<equation confidence="0.8876473">
P(d, q) = P(ζ1, ... ,ζn, d)
= P(ζ1, ... ,ζn, d, ω)
+P(ζ1, ... , ζn, d, ω)
= P(d)P(ω|d)P(ζ1, ... , ζn|d, ω)
+P(d)P(ω|d)P(ζ1, ... , ζn|d, ω)
= P(d)P(ω|d)P(ζ1, ... , ζn|d, ω) (2)
47
n
&apos; P(w|d) P((i|d,w) (3)
i=1
</equation>
<bodyText confidence="0.999912363636363">
Equation 2 holds because non-object pages do
not satisfy the query, thus, P((1, ... , (n|d, w) = 0.
Equation 3 holds because we assume a uniform dis-
tribution over d and conditional independence over
(i given d and w.
Thus, the rest of the problem is estimating P(w|d)
and P((i|d, w). The difference between these prob-
ability estimates lies in the features we use. Since w
depends only in d but not q, we use query indepen-
dent features. Similarly, (i only depends on d and
ci, thus we use features depending on ci and d.
</bodyText>
<subsectionHeader confidence="0.989472">
4.2 Calibrating ranking probability
</subsectionHeader>
<bodyText confidence="0.999931578947368">
In theory, we can use any learning algorithm men-
tioned in (Liu, 2009)’s survey to obtain the terms in
Equation 3. In practice, however, such learning al-
gorithms often output a ranking score that does not
estimate the probability. Thus, in order to use them
in our ranking model, we must transform that rank-
ing score into a probability.
For empirical purposes, we use the averaged Per-
ceptron (Freund and Schapire, 1999) to discrimina-
tively train each component of the factored distri-
bution independently. This algorithm requires a set
of input vectors, which we obtain by applying the
relational feature functions to the paired documents
and queries. For each constraint ci, we have a fea-
ture vector xi = 4bi(d, q). The algorithm produces a
weight vector of parameters wi as output. The prob-
ability of ci being satisfied by d given that d contains
an object can then be estimated with a sigmoid func-
tion as:
</bodyText>
<equation confidence="0.98647825">
1
P(ci|d, w) ≡ P(true|�i(d, q)) ≡
1 + exp(−wz xi)
(4)
</equation>
<bodyText confidence="0.993644">
Similarly, to estimate P(w|d), we use a fea-
ture vector that is dependent only on d. De-
noting the function as 4b0, we have P(w|d) =
P (true|4b0(d, q)), which can be obtained from (4).
While the sigmoid function has performed well
empirically, probabilities it produces are not cali-
brated. For better calibrated probabilities, one can
apply Platt scaling (Platt, 1999). This method intro-
duces two parameters A and B, which can be com-
puted using maximum likelihood estimation:
</bodyText>
<equation confidence="0.997478">
1
P (true|�i(d, q)) ≡
1 + exp(Awz `Fi(d, q) + B)
</equation>
<bodyText confidence="0.9953275">
(5)
In contrast to the sigmoid function, Platt scaling can
also be applied to methods that give un-normalized
scores such as RankSVM (Cao et al., 2006).
Substituting (4) and (5) into (3), we see that our
final learned ranking function has the form
</bodyText>
<equation confidence="0.85881">
(1 + exp(Aiwz 4&apos;i(d, q) + Bi))
</equation>
<sectionHeader confidence="0.979943" genericHeader="method">
5 Learning Based Programming
</sectionHeader>
<bodyText confidence="0.999277466666667">
Learning plays a crucial role in developing a new ob-
ject domain. In addition to using supervised meth-
ods to learn p, we also exploit active learning to ac-
quire training data from unlabeled web pages. The
combination of these efforts would benefit from a
unified framework and interface to machine learn-
ing. Learning Based Programming (LBP) (Roth,
2005) is such a principled framework. In this sec-
tion, we describe how we applied and extended LBP
to provide a user friendly interface for the developer
to specify features and guide the learning process.
Section 5.1 describes how we structured our frame-
work around Learning Based Java (LBJ), an instance
of LBP. Section 5.2 extends the framework to sup-
port interactive learning.
</bodyText>
<subsectionHeader confidence="0.998651">
5.1 Learning Based Java
</subsectionHeader>
<bodyText confidence="0.999948125">
LBP is a programming paradigm for systems whose
behaviors depend on naturally occurring data and
that require reasoning about data and concepts in
ways that are hard, if not impossible, to write explic-
itly. This is exactly our situation. Not only do we
not know how to specify a ranking function for an
object query, we might not even know exactly what
features to use. Using LBP, we can specify abstract
information sources that might contribute to deci-
sions and apply a learning operator to them, thereby
letting a learning algorithm figure out their impor-
tances in a data-driven way.
Learning Based Java (LBJ) (Rizzolo and Roth,
2007) is an implementation of LBP which we used
and extended for our purposes. The most useful
abstraction in LBJ is that of the feature generation
</bodyText>
<equation confidence="0.992021">
n
p(d, q) = ri
i=0
1 (6)
</equation>
<page confidence="0.991928">
48
</page>
<bodyText confidence="0.9999712">
function (FGF). This allows the programmer to rea-
son in terms of feature types, rather than specifying
individual features separately, and to treat them as
native building blocks in a language for constructing
learned functions. For example, instead of specify-
ing individual features such as the phrases “profes-
sor of”,“product description”, etc., we can specify a
higher level feature type called “bigram”, and let an
algorithm select individual features for ranking pur-
poses.
From the programming point of view, LBJ pro-
vides a clean interface and abstracts away the te-
dium of feature extraction and learning implemen-
tations. This enabled us to build our system quickly
and shorten our development cycle.
</bodyText>
<subsectionHeader confidence="0.976823">
5.2 Interactive Machine Learning
</subsectionHeader>
<bodyText confidence="0.960719063492063">
We advocate an interactive training process (Fails
and Olsen, 2003), in which the developer iteratively
improves the learner via two types of interaction
(Algorithm 1).
The first type of interaction is similar to active
learning where the learner presents unlabeled in-
stances to the developer for annotation which it be-
lieves will most positively impact learning. In rank-
ing problems, top ranked documents are presented
as they strongly influence the loss function. The
small difference from traditional active learning in
our setting is that the developer assists this process
by also providing more queries other than those en-
countered in the current training set.
The second type of interaction is feature selec-
tion. We observed that feature selection contributed
significantly in the performance of the learner espe-
cially when training data is scarce. This is because
with little training data and a huge feature space, the
learner tends to over-fit. Fortunately in web search,
the features used in ranking are in natural language
and thereby intuitive to the developer. For example,
one type of feature used in ranking the university
constraint of a professor object query is the words
surrounding the query field as in “university of ...”
or “... university”. If the learner only sees examples
from the University of Anystate at Anytown, then
it’s likely that Anytown will have a high weight in
addition to University and of. However, the Any-
town feature will not generalize for documents from
other universities. Having background knowledge
like this, the developer can unselect such features.
Furthermore, the fact that Anytown has a high weight
is also an indication that the developer needs to pro-
vide more examples of other universities so that the
learner can generalize (the first type of interaction).
Algorithm 1 Interactive Learning Algorithm
1: The developer uses keyword search to find and
annotate an initial training set.
2: The system presents a ranked list of features
computed from labeled data.
3: The developer adds/removes features.
4: The system learns the ranking function using se-
lected features.
5: The developer issues queries and annotates top
ranked unlabeled documents returned by the
system.
6: If performance is not satisfactory, go to step 2.
The iterative algorithm starts with zero training
data and continues until the learner’s performance
reaches a satisfactory point. At step 2, the developer
is presented with a ranked list of features. To deter-
mine which features played the biggest role in the
classifier’s decision making, we use a simple rank-
ing metric called expected entropy loss (Glover et
al., 2001). Let f represent the event that a given
feature is active. Let C be the event that the given
example is classified as true. The conditional en-
tropy of the classification distribution given that
f occurs is H(C|f) ≡ −P(C|f) log(P(C|f)) −
P(C|f) log(P(C|f) and similarly, when f does not
occur, we replace f by f. The expected entropy loss
is
</bodyText>
<equation confidence="0.999748">
L(C|f) ≡ H(C) − E[H(C|f)]
� H(C) − (P(f)H(C|f) +
P(f)H(C|f) (7)
</equation>
<bodyText confidence="0.999974222222222">
The intuition here is that if the classification loses
a lot of entropy when conditioned on a particular
feature, that feature must be very discriminative and
correlated with the classification itself.
It is noted that feature selection plays two impor-
tant roles in our framework. First, it avoids over-
fitting when training data is scarce, thus increas-
ing the effectiveness of our active learning protocol.
Second, since search time depends on how many
</bodyText>
<page confidence="0.998135">
49
</page>
<table confidence="0.999747166666667">
domain # pages train test
homepage 22.1 11.1 11
laptop 21 10.6 10.4
camera 18 9 9
random 97.8 48.9 48.8
total 158.9 79.6 79.2
</table>
<tableCaption confidence="0.8117745">
Table 1: Number of web pages (in thousands) collected
for experiment
</tableCaption>
<bodyText confidence="0.999826">
features we use to query the web pages, keeping the
number of features small will ensure that searching
is fast enough to be useful.
</bodyText>
<sectionHeader confidence="0.995515" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9861065">
In this section we present an experiment that com-
pares Object Search with keyword search engines.
</bodyText>
<subsectionHeader confidence="0.996009">
6.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999617285714286">
Since we are the first to tackle this problem of an-
swering structured query on the web, there is no
known dataset available for our experiment. We col-
lected the data ourselves using various sources from
the web. Then we labeled search results from differ-
ent object queries using the same annotation proce-
dure described in Section 5.
We collected URLs from two main sources: the
open directory (DMOZ) and existing search en-
gines (SE). For DMOZ, we included URLs from
relevant categories. For SE, we manually entered
queries with keywords related to professors’ home-
pages, laptops, and digital cameras, and included
all returned URLs. Having collected the URLs, we
crawled their content and indexed them. Table 1
summarizes web page data we have collected.
We split the data randomly into two parts, one for
training and one for testing, and created a single in-
verted index for both of them. The developer can
only see the training documents to select features
and train ranking functions. At testing time, we ran-
domly generate object queries, and evaluate on the
testing set. Since Google’s results come not from
our corpus but the whole web, it might not be fair to
compare against our small corpus. To accommodate
this, we also added Google’s results into our testing
corpus. We believe that most ‘difficult’ web pages
that hurt Google’s performance would have been in-
</bodyText>
<table confidence="0.999165181818182">
Field Keywords Example
Laptop domain
brand laptop,notebook lenovo laptop
processor ghz, processor 2.2 ghz
price $, price $1000..1100
Professor domain
name professor, re- research profes-
search professor, sor scott
faculty
university university, uni- stanford
versity of university
</table>
<tableCaption confidence="0.999517">
Table 2: Sample keyword reformulation for Google
</tableCaption>
<bodyText confidence="0.99959896">
cluded in the top Google result. Thus, they are also
available to test ours. In the future, we plan to im-
plement a local IR engine to compare against ours
and conduct a larger scale experiment to compare to
Google.
We evaluated the experiment with two different
domains: professor and laptop. We consider home-
pages and online shopping pages as object pages for
the professor and laptop domains respectively.
For each domain, we generated 5 random object
queries with different field configurations. Since
Google does not understand structured queries, we
reformulated each structured query into a simple
keyword query. We do so by pairing the query field
with several keywords. For example, a query field
abrand E {lenovo} can be reformulated as “lenovo
laptop”. We tried different combinations of key-
words as shown in table 2. To deal with numbers,
we use Google’s advanced search feature that sup-
ports numeric range queries1. For example, a price
constraint aprZce E [100, 200] might be reformulated
as “price $100..200”. Since it is too expensive to
find the best keyword formulations for every query,
we picked the combination that gives the best result
for the first Google result page (Top 10 URLs).
</bodyText>
<subsectionHeader confidence="0.948978">
6.2 Result
</subsectionHeader>
<bodyText confidence="0.998613">
We measure the ranking performance with average
precision. Table 3 shows the results for our search
engine (OSE) and Google. Our ranking function
outperforms Google for most queries, especially in
</bodyText>
<footnote confidence="0.7697475">
1A numeric range written as “100..200” is treated as a key-
word that appears everywhere a number in the range appears
</footnote>
<page confidence="0.877792">
50
</page>
<table confidence="0.999772857142857">
Qry Professor Laptop
OSE Google OSE Google
1 0.92 (71) 0.90(65) 0.7 (15) 0.44 (12)
2 0.83(88) 0.91(73) 0.62 (12) 0.26 (11)
3 0.51(73) 0.66(48) 0.44 (40) 0.31 (24)
4 0.42(49) 0.3(30) 0.36 (3) 0.09 (1)
5 0.91(18) 0.2(16) 0.77 (17) 0.42 (3)
</table>
<tableCaption confidence="0.944942">
Table 3: Average precision for 5 random queries. The
number of positive documents are in brackets
</tableCaption>
<bodyText confidence="0.999857777777778">
the laptop domain. In the professor domain, Google
wins in two queries (“UC Berkeley professor” and
“economics professors”). This suggests that in cer-
tain cases, reformulating to keyword query is a sen-
sible approach, especially if all the fields in the ob-
ject query are keywords. Even though Google can
be used to reformulate some queries, it is not clear
how and when this will succeed. Therefore, we need
a principled solution as proposed in this paper.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999927531914894">
Many recent works propose methods for supporting
structured queries on unstructured text (Jain et al.,
2007), (Cafarella et al., 2007), (Gruhl et al., 2004).
These works follow a typical extract-then-query ap-
proach, which has several problems as we discussed
in section 1. (Agichtein, 2005) proposed using sev-
eral large scale techniques. Their idea of using spe-
cialized index and search engine is similar to our
work. However those methods assumes that struc-
tured data follows some textual patterns whereas our
system can flexibly handle structured object using
textual patterns as well as web page features.
Interestingly, the approach of translating struc-
tured queries to unstructured queries has been stud-
ied in (Liu et al., 2006). The main difference is
that SEMEX relies on carefully hand-tuned heuris-
tics on open-domain SQL queries while we use ma-
chine learning to do the translation on domain spe-
cific queries.
Machine Learning approaches to rank documents
have been studied extensively in IR (Liu, 2009).
Even though much of existing works can be used to
rank individual constraints in the structured query.
We proposed an effective way to aggregate these
ranking scores. Further more, existing learning to
rank works assumed a fixed set of features, whereas,
the feature set in object search depends on object
domain. As we have shown, the effectiveness of
the ranking function depends much on the set of
features. Thus, an semi-automatic method to learn
these was proposed in section 5.
Our interactive learning protocol inherits features
from existing works in Active Learning (see (Set-
tles, 2009) for a survey). (Fails and Olsen, 2003)
coined the term “interactive machine learning” and
showed that a learner can take advantage of user in-
teraction to quickly acquire necessary training data.
(Roth and Small, 2009) proposed another interactive
learning protocol that improves upon a relation ex-
traction task by incremetally modifying the feature
representation.
Finally, this work is related to document re-
trieval mechanisms used for question answering
tasks (Voorhees, 2001) where precise retrieval meth-
ods are necessary to find documents which con-
tain specific information for answering factoids
(Agichtein et al., 2001).
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999525">
We introduces the Object Search framework that
searches the web for documents containing real-
world objects. We formalized the problem as a
learning to rank for IR problem and showed an ef-
fective method to solve it. Our approach goes be-
yond the traditional bag-of-words representation and
views each web page as a set of domain independent
features. This representation enabled us to rank web
pages with respect to object query. Our experiments
showed that, with small human effort, it is possi-
ble to create specialized search engines that out-
performs GSEs on domain specific queries. More-
over, it is possible to search the web for documents
with deeper meaning, such as those found in object
pages. Our work is a small step toward semantic
search engines by handling deeper semantic queries.
</bodyText>
<sectionHeader confidence="0.991884" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.5223818">
This work is supported by DARPA funding under
the Bootstrap Learning Program, MIAS, a DHS-
IDS Center for Multimodal Information Access and
Synthesis at UIUC, NSF grant NSF SoD-HCER-
0613885 and a grant from Yahoo! Inc.
</bodyText>
<page confidence="0.998728">
51
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999956633802817">
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2001. Learning search engine specific query trans-
formations for question answering. In WWW ’01:
Proceedings of the 10th international conference on
World Wide Web, pages 169–178, New York, NY,
USA. ACM.
Eugene Agichtein. 2005. Scaling Information Extraction
to Large Document Collections. IEEE Data Eng. Bull,
28:3.
Michael Cafarella, Christopher Re, Dan Suciu, and Oren
Etzioni. 2007. Structured Querying of Web Text Data:
A Technical Challenge. In CIDR.
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang,
and Hsiao-Wuen Hon. 2006. Adapting Ranking SVM
to Document Retrieval. In SIGIR ’06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 186–193, New York, NY, USA. ACM.
Jerry Alan Fails and Dan R. Olsen, Jr. 2003. Interactive
machine learning. In IUI ’03: Proceedings of the 8th
international conference on Intelligent user interfaces,
pages 39–45, New York, NY, USA. ACM.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277–296.
Eric J. Glover, Gary W. Flake, Steve Lawrence, Andries
Kruger, David M. Pennock, William P. Birmingham,
and C. Lee Giles. 2001. Improving Category Specific
Web Search by Learning Query Modifications. Ap-
plications and the Internet, IEEE/IPSJ International
Symposium on, 0:23.
D. Gruhl, L. Chavet, D. Gibson, J. Meyer, P. Pattanayak,
A. Tomkins, and J. Zien. 2004. How to Build a Web-
Fountain: An Architecture for Very Large Scale Text
Analytics. IBM Systems Journal.
A. Jain, A. Doan, and L. Gravano. 2007. SQL Queries
Over Unstructured Text Databases. In Data Engineer-
ing, 2007. ICDE 2007. IEEE 23rd International Con-
ference on, pages 1255–1257.
N. Kushmerick, D. Weld, and R. Doorenbos. 1997.
Wrapper Induction for Information Extraction. In IJ-
CAI, pages 729–737.
Jing Liu, Xin Dong, and Alon Halevy. 2006. Answering
Structured Queries on Unstructured Data. In WebDB.
Tie-Yan Liu. 2009. Learning to Rank for Information
Retrieval. Found. Trends Inf. Retr., 3(3):225–331.
Andrew Kachites McCallum, Kamal Nigam, Jason Ren-
nie, and Kristie Seymore. 2000. Automating the Con-
struction of Internet Portals with Machine Learning.
Information Retrieval, 3(2):127–163.
J. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. In In Advances in Large Margin Classifiers.
MIT Press.
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597–604, Irvine, California, September. IEEE.
Dan Roth and Kevin Small. 2009. Interactive feature
space construction using semantic information. In
CoNLL ’09: Proceedings of the Thirteenth Conference
on Computational Natural Language Learning, pages
66–74, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Dan Roth. 2005. Learning Based Programming. Innova-
tions in Machine Learning: Theory and Applications.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, University
of Wisconsin-Madison.
Ellen M. Voorhees. 2001. The trec question answering
track. Nat. Lang. Eng., 7(4):361–378.
</reference>
<page confidence="0.998859">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703944">
<title confidence="0.998595">Object Search: Supporting Structured Queries in Web Search Engines</title>
<author confidence="0.994086">Cuong Nicholas Kevin Kevin Chen-Chuan Dan</author>
<affiliation confidence="0.8552675">of Illinois at Department of Computer Science Department of Computer Science</affiliation>
<email confidence="0.997421">rizzolo,kcchang,kevin.small@tufts.edu</email>
<abstract confidence="0.99972188">As the web evolves, increasing quantities of structured information is embedded in web pages in disparate formats. For example, a digital camera’s description may include its a professor’s may include her univerand Both types of pages may include additional ambiguous information. General search engines (GSEs) do not support queries over these types of data because they ignore the web document Conversely, describing site semantics through structured queries into databases populated by information extraction (IE) techniques are expensive and not easily adaptable to new domains. This paper describes a methodology for rapidly developing search engines capable of answering structured queries over unstructured corpora by utilizing machine learning to avoid explicit IE. We empirically show that with minimum additional human effort, our system outperforms a GSE with respect to structured queries with clear object semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Steve Lawrence</author>
<author>Luis Gravano</author>
</authors>
<title>Learning search engine specific query transformations for question answering.</title>
<date>2001</date>
<booktitle>In WWW ’01: Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>169--178</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31061" citStr="Agichtein et al., 2001" startWordPosition="5177" endWordPosition="5180">r a survey). (Fails and Olsen, 2003) coined the term “interactive machine learning” and showed that a learner can take advantage of user interaction to quickly acquire necessary training data. (Roth and Small, 2009) proposed another interactive learning protocol that improves upon a relation extraction task by incremetally modifying the feature representation. Finally, this work is related to document retrieval mechanisms used for question answering tasks (Voorhees, 2001) where precise retrieval methods are necessary to find documents which contain specific information for answering factoids (Agichtein et al., 2001). 8 Conclusion We introduces the Object Search framework that searches the web for documents containing realworld objects. We formalized the problem as a learning to rank for IR problem and showed an effective method to solve it. Our approach goes beyond the traditional bag-of-words representation and views each web page as a set of domain independent features. This representation enabled us to rank web pages with respect to object query. Our experiments showed that, with small human effort, it is possible to create specialized search engines that outperforms GSEs on domain specific queries. M</context>
</contexts>
<marker>Agichtein, Lawrence, Gravano, 2001</marker>
<rawString>Eugene Agichtein, Steve Lawrence, and Luis Gravano. 2001. Learning search engine specific query transformations for question answering. In WWW ’01: Proceedings of the 10th international conference on World Wide Web, pages 169–178, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
</authors>
<title>Scaling Information Extraction to Large Document Collections.</title>
<date>2005</date>
<journal>IEEE Data Eng. Bull,</journal>
<pages>28--3</pages>
<contexts>
<context position="3907" citStr="Agichtein, 2005" startWordPosition="584" endWordPosition="585">rofessor’s homepage, ignoring much of the structured information specific to particular domains. Conversely, vertical search engines (e.g. DBLife, cazoodle.com, Rexa.info, etc.) approach this probProceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44–52, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics lem from the information extraction (IE) perspective. Instead of searching an inverted index directly, they first extract data records from text (Kushmerick et al., 1997; McCallum et al., 2000). IE solutions, even with large scale techniques (Agichtein, 2005), do not scale to the entire web and cost significantly more than GSEs. Secondly, creating domainspecific models or wrappers require labeling training examples and human expertise for each individual site. Thirdly, pre-extracting information lacks flexibility; decisions made during IE are irrevocable, and at query time, users may find additional value in partial or noisy records that were discarded by the IE system. These issues motivate our novel approach for designing a GSE capable of answering complex structured queries, which we refer to as Object Search. At a high level, we search web pag</context>
<context position="29117" citStr="Agichtein, 2005" startWordPosition="4874" endWordPosition="4875">n certain cases, reformulating to keyword query is a sensible approach, especially if all the fields in the object query are keywords. Even though Google can be used to reformulate some queries, it is not clear how and when this will succeed. Therefore, we need a principled solution as proposed in this paper. 7 Related Work Many recent works propose methods for supporting structured queries on unstructured text (Jain et al., 2007), (Cafarella et al., 2007), (Gruhl et al., 2004). These works follow a typical extract-then-query approach, which has several problems as we discussed in section 1. (Agichtein, 2005) proposed using several large scale techniques. Their idea of using specialized index and search engine is similar to our work. However those methods assumes that structured data follows some textual patterns whereas our system can flexibly handle structured object using textual patterns as well as web page features. Interestingly, the approach of translating structured queries to unstructured queries has been studied in (Liu et al., 2006). The main difference is that SEMEX relies on carefully hand-tuned heuristics on open-domain SQL queries while we use machine learning to do the translation </context>
</contexts>
<marker>Agichtein, 2005</marker>
<rawString>Eugene Agichtein. 2005. Scaling Information Extraction to Large Document Collections. IEEE Data Eng. Bull, 28:3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Cafarella</author>
<author>Christopher Re</author>
<author>Dan Suciu</author>
<author>Oren Etzioni</author>
</authors>
<title>Structured Querying of Web Text Data: A Technical Challenge.</title>
<date>2007</date>
<booktitle>In CIDR.</booktitle>
<contexts>
<context position="28961" citStr="Cafarella et al., 2007" startWordPosition="4848" endWordPosition="4851">s are in brackets the laptop domain. In the professor domain, Google wins in two queries (“UC Berkeley professor” and “economics professors”). This suggests that in certain cases, reformulating to keyword query is a sensible approach, especially if all the fields in the object query are keywords. Even though Google can be used to reformulate some queries, it is not clear how and when this will succeed. Therefore, we need a principled solution as proposed in this paper. 7 Related Work Many recent works propose methods for supporting structured queries on unstructured text (Jain et al., 2007), (Cafarella et al., 2007), (Gruhl et al., 2004). These works follow a typical extract-then-query approach, which has several problems as we discussed in section 1. (Agichtein, 2005) proposed using several large scale techniques. Their idea of using specialized index and search engine is similar to our work. However those methods assumes that structured data follows some textual patterns whereas our system can flexibly handle structured object using textual patterns as well as web page features. Interestingly, the approach of translating structured queries to unstructured queries has been studied in (Liu et al., 2006).</context>
</contexts>
<marker>Cafarella, Re, Suciu, Etzioni, 2007</marker>
<rawString>Michael Cafarella, Christopher Re, Dan Suciu, and Oren Etzioni. 2007. Structured Querying of Web Text Data: A Technical Challenge. In CIDR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunbo Cao</author>
<author>Jun Xu</author>
<author>Tie-Yan Liu</author>
<author>Hang Li</author>
<author>Yalou Huang</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Adapting Ranking SVM to Document Retrieval.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>186--193</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18345" citStr="Cao et al., 2006" startWordPosition="3099" endWordPosition="3102">at is dependent only on d. Denoting the function as 4b0, we have P(w|d) = P (true|4b0(d, q)), which can be obtained from (4). While the sigmoid function has performed well empirically, probabilities it produces are not calibrated. For better calibrated probabilities, one can apply Platt scaling (Platt, 1999). This method introduces two parameters A and B, which can be computed using maximum likelihood estimation: 1 P (true|�i(d, q)) ≡ 1 + exp(Awz `Fi(d, q) + B) (5) In contrast to the sigmoid function, Platt scaling can also be applied to methods that give un-normalized scores such as RankSVM (Cao et al., 2006). Substituting (4) and (5) into (3), we see that our final learned ranking function has the form (1 + exp(Aiwz 4&apos;i(d, q) + Bi)) 5 Learning Based Programming Learning plays a crucial role in developing a new object domain. In addition to using supervised methods to learn p, we also exploit active learning to acquire training data from unlabeled web pages. The combination of these efforts would benefit from a unified framework and interface to machine learning. Learning Based Programming (LBP) (Roth, 2005) is such a principled framework. In this section, we describe how we applied and extended L</context>
</contexts>
<marker>Cao, Xu, Liu, Li, Huang, Hon, 2006</marker>
<rawString>Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsiao-Wuen Hon. 2006. Adapting Ranking SVM to Document Retrieval. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186–193, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Alan Fails</author>
<author>Dan R Olsen</author>
</authors>
<title>Interactive machine learning.</title>
<date>2003</date>
<booktitle>In IUI ’03: Proceedings of the 8th international conference on Intelligent user interfaces,</booktitle>
<pages>39--45</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20868" citStr="Fails and Olsen, 2003" startWordPosition="3519" endWordPosition="3522">language for constructing learned functions. For example, instead of specifying individual features such as the phrases “professor of”,“product description”, etc., we can specify a higher level feature type called “bigram”, and let an algorithm select individual features for ranking purposes. From the programming point of view, LBJ provides a clean interface and abstracts away the tedium of feature extraction and learning implementations. This enabled us to build our system quickly and shorten our development cycle. 5.2 Interactive Machine Learning We advocate an interactive training process (Fails and Olsen, 2003), in which the developer iteratively improves the learner via two types of interaction (Algorithm 1). The first type of interaction is similar to active learning where the learner presents unlabeled instances to the developer for annotation which it believes will most positively impact learning. In ranking problems, top ranked documents are presented as they strongly influence the loss function. The small difference from traditional active learning in our setting is that the developer assists this process by also providing more queries other than those encountered in the current training set. </context>
<context position="30474" citStr="Fails and Olsen, 2003" startWordPosition="5091" endWordPosition="5094">much of existing works can be used to rank individual constraints in the structured query. We proposed an effective way to aggregate these ranking scores. Further more, existing learning to rank works assumed a fixed set of features, whereas, the feature set in object search depends on object domain. As we have shown, the effectiveness of the ranking function depends much on the set of features. Thus, an semi-automatic method to learn these was proposed in section 5. Our interactive learning protocol inherits features from existing works in Active Learning (see (Settles, 2009) for a survey). (Fails and Olsen, 2003) coined the term “interactive machine learning” and showed that a learner can take advantage of user interaction to quickly acquire necessary training data. (Roth and Small, 2009) proposed another interactive learning protocol that improves upon a relation extraction task by incremetally modifying the feature representation. Finally, this work is related to document retrieval mechanisms used for question answering tasks (Voorhees, 2001) where precise retrieval methods are necessary to find documents which contain specific information for answering factoids (Agichtein et al., 2001). 8 Conclusio</context>
</contexts>
<marker>Fails, Olsen, 2003</marker>
<rawString>Jerry Alan Fails and Dan R. Olsen, Jr. 2003. Interactive machine learning. In IUI ’03: Proceedings of the 8th international conference on Intelligent user interfaces, pages 39–45, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="17128" citStr="Freund and Schapire, 1999" startWordPosition="2885" endWordPosition="2888">eatures we use. Since w depends only in d but not q, we use query independent features. Similarly, (i only depends on d and ci, thus we use features depending on ci and d. 4.2 Calibrating ranking probability In theory, we can use any learning algorithm mentioned in (Liu, 2009)’s survey to obtain the terms in Equation 3. In practice, however, such learning algorithms often output a ranking score that does not estimate the probability. Thus, in order to use them in our ranking model, we must transform that ranking score into a probability. For empirical purposes, we use the averaged Perceptron (Freund and Schapire, 1999) to discriminatively train each component of the factored distribution independently. This algorithm requires a set of input vectors, which we obtain by applying the relational feature functions to the paired documents and queries. For each constraint ci, we have a feature vector xi = 4bi(d, q). The algorithm produces a weight vector of parameters wi as output. The probability of ci being satisfied by d given that d contains an object can then be estimated with a sigmoid function as: 1 P(ci|d, w) ≡ P(true|�i(d, q)) ≡ 1 + exp(−wz xi) (4) Similarly, to estimate P(w|d), we use a feature vector th</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric J Glover</author>
<author>Gary W Flake</author>
<author>Steve Lawrence</author>
<author>Andries Kruger</author>
<author>David M Pennock</author>
<author>William P Birmingham</author>
<author>C Lee Giles</author>
</authors>
<title>Improving Category Specific Web Search by Learning Query Modifications. Applications and the Internet,</title>
<date>2001</date>
<booktitle>IEEE/IPSJ International Symposium on,</booktitle>
<pages>0--23</pages>
<contexts>
<context position="23452" citStr="Glover et al., 2001" startWordPosition="3932" endWordPosition="3935">oper adds/removes features. 4: The system learns the ranking function using selected features. 5: The developer issues queries and annotates top ranked unlabeled documents returned by the system. 6: If performance is not satisfactory, go to step 2. The iterative algorithm starts with zero training data and continues until the learner’s performance reaches a satisfactory point. At step 2, the developer is presented with a ranked list of features. To determine which features played the biggest role in the classifier’s decision making, we use a simple ranking metric called expected entropy loss (Glover et al., 2001). Let f represent the event that a given feature is active. Let C be the event that the given example is classified as true. The conditional entropy of the classification distribution given that f occurs is H(C|f) ≡ −P(C|f) log(P(C|f)) − P(C|f) log(P(C|f) and similarly, when f does not occur, we replace f by f. The expected entropy loss is L(C|f) ≡ H(C) − E[H(C|f)] � H(C) − (P(f)H(C|f) + P(f)H(C|f) (7) The intuition here is that if the classification loses a lot of entropy when conditioned on a particular feature, that feature must be very discriminative and correlated with the classification </context>
</contexts>
<marker>Glover, Flake, Lawrence, Kruger, Pennock, Birmingham, Giles, 2001</marker>
<rawString>Eric J. Glover, Gary W. Flake, Steve Lawrence, Andries Kruger, David M. Pennock, William P. Birmingham, and C. Lee Giles. 2001. Improving Category Specific Web Search by Learning Query Modifications. Applications and the Internet, IEEE/IPSJ International Symposium on, 0:23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gruhl</author>
<author>L Chavet</author>
<author>D Gibson</author>
<author>J Meyer</author>
<author>P Pattanayak</author>
<author>A Tomkins</author>
<author>J Zien</author>
</authors>
<title>How to Build a WebFountain: An Architecture for Very Large Scale Text Analytics.</title>
<date>2004</date>
<journal>IBM Systems Journal.</journal>
<contexts>
<context position="28983" citStr="Gruhl et al., 2004" startWordPosition="4852" endWordPosition="4855">op domain. In the professor domain, Google wins in two queries (“UC Berkeley professor” and “economics professors”). This suggests that in certain cases, reformulating to keyword query is a sensible approach, especially if all the fields in the object query are keywords. Even though Google can be used to reformulate some queries, it is not clear how and when this will succeed. Therefore, we need a principled solution as proposed in this paper. 7 Related Work Many recent works propose methods for supporting structured queries on unstructured text (Jain et al., 2007), (Cafarella et al., 2007), (Gruhl et al., 2004). These works follow a typical extract-then-query approach, which has several problems as we discussed in section 1. (Agichtein, 2005) proposed using several large scale techniques. Their idea of using specialized index and search engine is similar to our work. However those methods assumes that structured data follows some textual patterns whereas our system can flexibly handle structured object using textual patterns as well as web page features. Interestingly, the approach of translating structured queries to unstructured queries has been studied in (Liu et al., 2006). The main difference i</context>
</contexts>
<marker>Gruhl, Chavet, Gibson, Meyer, Pattanayak, Tomkins, Zien, 2004</marker>
<rawString>D. Gruhl, L. Chavet, D. Gibson, J. Meyer, P. Pattanayak, A. Tomkins, and J. Zien. 2004. How to Build a WebFountain: An Architecture for Very Large Scale Text Analytics. IBM Systems Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>A Doan</author>
<author>L Gravano</author>
</authors>
<title>SQL Queries Over Unstructured Text Databases.</title>
<date>2007</date>
<booktitle>In Data Engineering,</booktitle>
<pages>1255--1257</pages>
<contexts>
<context position="28935" citStr="Jain et al., 2007" startWordPosition="4844" endWordPosition="4847"> of positive documents are in brackets the laptop domain. In the professor domain, Google wins in two queries (“UC Berkeley professor” and “economics professors”). This suggests that in certain cases, reformulating to keyword query is a sensible approach, especially if all the fields in the object query are keywords. Even though Google can be used to reformulate some queries, it is not clear how and when this will succeed. Therefore, we need a principled solution as proposed in this paper. 7 Related Work Many recent works propose methods for supporting structured queries on unstructured text (Jain et al., 2007), (Cafarella et al., 2007), (Gruhl et al., 2004). These works follow a typical extract-then-query approach, which has several problems as we discussed in section 1. (Agichtein, 2005) proposed using several large scale techniques. Their idea of using specialized index and search engine is similar to our work. However those methods assumes that structured data follows some textual patterns whereas our system can flexibly handle structured object using textual patterns as well as web page features. Interestingly, the approach of translating structured queries to unstructured queries has been stud</context>
</contexts>
<marker>Jain, Doan, Gravano, 2007</marker>
<rawString>A. Jain, A. Doan, and L. Gravano. 2007. SQL Queries Over Unstructured Text Databases. In Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, pages 1255–1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushmerick</author>
<author>D Weld</author>
<author>R Doorenbos</author>
</authors>
<title>Wrapper Induction for Information Extraction. In</title>
<date>1997</date>
<booktitle>IJCAI,</booktitle>
<pages>729--737</pages>
<contexts>
<context position="3817" citStr="Kushmerick et al., 1997" startWordPosition="567" endWordPosition="571">Es are domain-agnostic, they will generally rank camera pages utilizing the same functions as a professor’s homepage, ignoring much of the structured information specific to particular domains. Conversely, vertical search engines (e.g. DBLife, cazoodle.com, Rexa.info, etc.) approach this probProceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44–52, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics lem from the information extraction (IE) perspective. Instead of searching an inverted index directly, they first extract data records from text (Kushmerick et al., 1997; McCallum et al., 2000). IE solutions, even with large scale techniques (Agichtein, 2005), do not scale to the entire web and cost significantly more than GSEs. Secondly, creating domainspecific models or wrappers require labeling training examples and human expertise for each individual site. Thirdly, pre-extracting information lacks flexibility; decisions made during IE are irrevocable, and at query time, users may find additional value in partial or noisy records that were discarded by the IE system. These issues motivate our novel approach for designing a GSE capable of answering complex </context>
</contexts>
<marker>Kushmerick, Weld, Doorenbos, 1997</marker>
<rawString>N. Kushmerick, D. Weld, and R. Doorenbos. 1997. Wrapper Induction for Information Extraction. In IJCAI, pages 729–737.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Liu</author>
<author>Xin Dong</author>
<author>Alon Halevy</author>
</authors>
<title>Answering Structured Queries on Unstructured Data. In WebDB.</title>
<date>2006</date>
<contexts>
<context position="29560" citStr="Liu et al., 2006" startWordPosition="4943" endWordPosition="4946">ella et al., 2007), (Gruhl et al., 2004). These works follow a typical extract-then-query approach, which has several problems as we discussed in section 1. (Agichtein, 2005) proposed using several large scale techniques. Their idea of using specialized index and search engine is similar to our work. However those methods assumes that structured data follows some textual patterns whereas our system can flexibly handle structured object using textual patterns as well as web page features. Interestingly, the approach of translating structured queries to unstructured queries has been studied in (Liu et al., 2006). The main difference is that SEMEX relies on carefully hand-tuned heuristics on open-domain SQL queries while we use machine learning to do the translation on domain specific queries. Machine Learning approaches to rank documents have been studied extensively in IR (Liu, 2009). Even though much of existing works can be used to rank individual constraints in the structured query. We proposed an effective way to aggregate these ranking scores. Further more, existing learning to rank works assumed a fixed set of features, whereas, the feature set in object search depends on object domain. As we </context>
</contexts>
<marker>Liu, Dong, Halevy, 2006</marker>
<rawString>Jing Liu, Xin Dong, and Alon Halevy. 2006. Answering Structured Queries on Unstructured Data. In WebDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
</authors>
<title>Learning to Rank for Information Retrieval.</title>
<date>2009</date>
<journal>Found. Trends Inf. Retr.,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="6509" citStr="Liu, 2009" startWordPosition="1035" endWordPosition="1036">oes not care about an attribute, the constraint is the constant true. Given an object query, we want a set of satisfying object pages. Specifically, object pages are pages that represent exactly one inherent object on the web. Pages that list several objects such as a department directory page or camera listing pages are not considered object pages because even though they mentioned the object, they do not represent any particular object. There is often a single object page but there are many web pages that mention the object. The goal of Object Search is similar to learning to rank problems (Liu, 2009), in that its goal is to learn a ranking function p : D x 2 —* R that ranks any (document, query) pairs. This is accomplished by learning an function over a set of relevant features. Each feature can be modeled as a function that takes the pair and outputs a real value 0 : D x 2 —* R. For example, a term frequency feature outputs the number of times the query appears in the document. We define a function 4b = (01, 02, ...0n) that takes a (document, query) pair and outputs a vector of features. The original ranking function can be written as p(d, q) = p&apos;(4b(d, q)) where p&apos; : Rn —* R is the func</context>
<context position="16779" citStr="Liu, 2009" startWordPosition="2828" endWordPosition="2829">s because non-object pages do not satisfy the query, thus, P((1, ... , (n|d, w) = 0. Equation 3 holds because we assume a uniform distribution over d and conditional independence over (i given d and w. Thus, the rest of the problem is estimating P(w|d) and P((i|d, w). The difference between these probability estimates lies in the features we use. Since w depends only in d but not q, we use query independent features. Similarly, (i only depends on d and ci, thus we use features depending on ci and d. 4.2 Calibrating ranking probability In theory, we can use any learning algorithm mentioned in (Liu, 2009)’s survey to obtain the terms in Equation 3. In practice, however, such learning algorithms often output a ranking score that does not estimate the probability. Thus, in order to use them in our ranking model, we must transform that ranking score into a probability. For empirical purposes, we use the averaged Perceptron (Freund and Schapire, 1999) to discriminatively train each component of the factored distribution independently. This algorithm requires a set of input vectors, which we obtain by applying the relational feature functions to the paired documents and queries. For each constraint</context>
<context position="29838" citStr="Liu, 2009" startWordPosition="4990" endWordPosition="4991">lar to our work. However those methods assumes that structured data follows some textual patterns whereas our system can flexibly handle structured object using textual patterns as well as web page features. Interestingly, the approach of translating structured queries to unstructured queries has been studied in (Liu et al., 2006). The main difference is that SEMEX relies on carefully hand-tuned heuristics on open-domain SQL queries while we use machine learning to do the translation on domain specific queries. Machine Learning approaches to rank documents have been studied extensively in IR (Liu, 2009). Even though much of existing works can be used to rank individual constraints in the structured query. We proposed an effective way to aggregate these ranking scores. Further more, existing learning to rank works assumed a fixed set of features, whereas, the feature set in object search depends on object domain. As we have shown, the effectiveness of the ranking function depends much on the set of features. Thus, an semi-automatic method to learn these was proposed in section 5. Our interactive learning protocol inherits features from existing works in Active Learning (see (Settles, 2009) fo</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Found. Trends Inf. Retr., 3(3):225–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
<author>Kamal Nigam</author>
<author>Jason Rennie</author>
<author>Kristie Seymore</author>
</authors>
<date>2000</date>
<booktitle>Automating the Construction of Internet Portals with Machine Learning. Information Retrieval,</booktitle>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="3841" citStr="McCallum et al., 2000" startWordPosition="572" endWordPosition="575">hey will generally rank camera pages utilizing the same functions as a professor’s homepage, ignoring much of the structured information specific to particular domains. Conversely, vertical search engines (e.g. DBLife, cazoodle.com, Rexa.info, etc.) approach this probProceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44–52, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics lem from the information extraction (IE) perspective. Instead of searching an inverted index directly, they first extract data records from text (Kushmerick et al., 1997; McCallum et al., 2000). IE solutions, even with large scale techniques (Agichtein, 2005), do not scale to the entire web and cost significantly more than GSEs. Secondly, creating domainspecific models or wrappers require labeling training examples and human expertise for each individual site. Thirdly, pre-extracting information lacks flexibility; decisions made during IE are irrevocable, and at query time, users may find additional value in partial or noisy records that were discarded by the IE system. These issues motivate our novel approach for designing a GSE capable of answering complex structured queries, whic</context>
</contexts>
<marker>McCallum, Nigam, Rennie, Seymore, 2000</marker>
<rawString>Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. 2000. Automating the Construction of Internet Portals with Machine Learning. Information Retrieval, 3(2):127–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparison to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>In In Advances in Large Margin Classifiers.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18037" citStr="Platt, 1999" startWordPosition="3045" endWordPosition="3046">The algorithm produces a weight vector of parameters wi as output. The probability of ci being satisfied by d given that d contains an object can then be estimated with a sigmoid function as: 1 P(ci|d, w) ≡ P(true|�i(d, q)) ≡ 1 + exp(−wz xi) (4) Similarly, to estimate P(w|d), we use a feature vector that is dependent only on d. Denoting the function as 4b0, we have P(w|d) = P (true|4b0(d, q)), which can be obtained from (4). While the sigmoid function has performed well empirically, probabilities it produces are not calibrated. For better calibrated probabilities, one can apply Platt scaling (Platt, 1999). This method introduces two parameters A and B, which can be computed using maximum likelihood estimation: 1 P (true|�i(d, q)) ≡ 1 + exp(Awz `Fi(d, q) + B) (5) In contrast to the sigmoid function, Platt scaling can also be applied to methods that give un-normalized scores such as RankSVM (Cao et al., 2006). Substituting (4) and (5) into (3), we see that our final learned ranking function has the form (1 + exp(Aiwz 4&apos;i(d, q) + Bi)) 5 Learning Based Programming Learning plays a crucial role in developing a new object domain. In addition to using supervised methods to learn p, we also exploit ac</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>J. Platt. 1999. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In In Advances in Large Margin Classifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Modeling Discriminative Global Inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the First International Conference on Semantic Computing (ICSC),</booktitle>
<pages>597--604</pages>
<publisher>IEEE.</publisher>
<location>Irvine, California,</location>
<contexts>
<context position="19891" citStr="Rizzolo and Roth, 2007" startWordPosition="3360" endWordPosition="3363">amming paradigm for systems whose behaviors depend on naturally occurring data and that require reasoning about data and concepts in ways that are hard, if not impossible, to write explicitly. This is exactly our situation. Not only do we not know how to specify a ranking function for an object query, we might not even know exactly what features to use. Using LBP, we can specify abstract information sources that might contribute to decisions and apply a learning operator to them, thereby letting a learning algorithm figure out their importances in a data-driven way. Learning Based Java (LBJ) (Rizzolo and Roth, 2007) is an implementation of LBP which we used and extended for our purposes. The most useful abstraction in LBJ is that of the feature generation n p(d, q) = ri i=0 1 (6) 48 function (FGF). This allows the programmer to reason in terms of feature types, rather than specifying individual features separately, and to treat them as native building blocks in a language for constructing learned functions. For example, instead of specifying individual features such as the phrases “professor of”,“product description”, etc., we can specify a higher level feature type called “bigram”, and let an algorithm </context>
</contexts>
<marker>Rizzolo, Roth, 2007</marker>
<rawString>N. Rizzolo and D. Roth. 2007. Modeling Discriminative Global Inference. In Proceedings of the First International Conference on Semantic Computing (ICSC), pages 597–604, Irvine, California, September. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Kevin Small</author>
</authors>
<title>Interactive feature space construction using semantic information.</title>
<date>2009</date>
<booktitle>In CoNLL ’09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>66--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="30653" citStr="Roth and Small, 2009" startWordPosition="5119" endWordPosition="5122">arning to rank works assumed a fixed set of features, whereas, the feature set in object search depends on object domain. As we have shown, the effectiveness of the ranking function depends much on the set of features. Thus, an semi-automatic method to learn these was proposed in section 5. Our interactive learning protocol inherits features from existing works in Active Learning (see (Settles, 2009) for a survey). (Fails and Olsen, 2003) coined the term “interactive machine learning” and showed that a learner can take advantage of user interaction to quickly acquire necessary training data. (Roth and Small, 2009) proposed another interactive learning protocol that improves upon a relation extraction task by incremetally modifying the feature representation. Finally, this work is related to document retrieval mechanisms used for question answering tasks (Voorhees, 2001) where precise retrieval methods are necessary to find documents which contain specific information for answering factoids (Agichtein et al., 2001). 8 Conclusion We introduces the Object Search framework that searches the web for documents containing realworld objects. We formalized the problem as a learning to rank for IR problem and sh</context>
</contexts>
<marker>Roth, Small, 2009</marker>
<rawString>Dan Roth and Kevin Small. 2009. Interactive feature space construction using semantic information. In CoNLL ’09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 66–74, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
</authors>
<title>Learning Based Programming. Innovations</title>
<date>2005</date>
<booktitle>in Machine Learning: Theory and Applications.</booktitle>
<contexts>
<context position="18854" citStr="Roth, 2005" startWordPosition="3188" endWordPosition="3189">aling can also be applied to methods that give un-normalized scores such as RankSVM (Cao et al., 2006). Substituting (4) and (5) into (3), we see that our final learned ranking function has the form (1 + exp(Aiwz 4&apos;i(d, q) + Bi)) 5 Learning Based Programming Learning plays a crucial role in developing a new object domain. In addition to using supervised methods to learn p, we also exploit active learning to acquire training data from unlabeled web pages. The combination of these efforts would benefit from a unified framework and interface to machine learning. Learning Based Programming (LBP) (Roth, 2005) is such a principled framework. In this section, we describe how we applied and extended LBP to provide a user friendly interface for the developer to specify features and guide the learning process. Section 5.1 describes how we structured our framework around Learning Based Java (LBJ), an instance of LBP. Section 5.2 extends the framework to support interactive learning. 5.1 Learning Based Java LBP is a programming paradigm for systems whose behaviors depend on naturally occurring data and that require reasoning about data and concepts in ways that are hard, if not impossible, to write expli</context>
</contexts>
<marker>Roth, 2005</marker>
<rawString>Dan Roth. 2005. Learning Based Programming. Innovations in Machine Learning: Theory and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey. Computer Sciences</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin-Madison.</institution>
<contexts>
<context position="30435" citStr="Settles, 2009" startWordPosition="5085" endWordPosition="5087">in IR (Liu, 2009). Even though much of existing works can be used to rank individual constraints in the structured query. We proposed an effective way to aggregate these ranking scores. Further more, existing learning to rank works assumed a fixed set of features, whereas, the feature set in object search depends on object domain. As we have shown, the effectiveness of the ranking function depends much on the set of features. Thus, an semi-automatic method to learn these was proposed in section 5. Our interactive learning protocol inherits features from existing works in Active Learning (see (Settles, 2009) for a survey). (Fails and Olsen, 2003) coined the term “interactive machine learning” and showed that a learner can take advantage of user interaction to quickly acquire necessary training data. (Roth and Small, 2009) proposed another interactive learning protocol that improves upon a relation extraction task by incremetally modifying the feature representation. Finally, this work is related to document retrieval mechanisms used for question answering tasks (Voorhees, 2001) where precise retrieval methods are necessary to find documents which contain specific information for answering factoid</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Burr Settles. 2009. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin-Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The trec question answering track.</title>
<date>2001</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="30914" citStr="Voorhees, 2001" startWordPosition="5157" endWordPosition="5158">s proposed in section 5. Our interactive learning protocol inherits features from existing works in Active Learning (see (Settles, 2009) for a survey). (Fails and Olsen, 2003) coined the term “interactive machine learning” and showed that a learner can take advantage of user interaction to quickly acquire necessary training data. (Roth and Small, 2009) proposed another interactive learning protocol that improves upon a relation extraction task by incremetally modifying the feature representation. Finally, this work is related to document retrieval mechanisms used for question answering tasks (Voorhees, 2001) where precise retrieval methods are necessary to find documents which contain specific information for answering factoids (Agichtein et al., 2001). 8 Conclusion We introduces the Object Search framework that searches the web for documents containing realworld objects. We formalized the problem as a learning to rank for IR problem and showed an effective method to solve it. Our approach goes beyond the traditional bag-of-words representation and views each web page as a set of domain independent features. This representation enabled us to rank web pages with respect to object query. Our experi</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001. The trec question answering track. Nat. Lang. Eng., 7(4):361–378.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>