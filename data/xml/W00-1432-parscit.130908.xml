<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.911282">
Sentence generation and neural networks
</title>
<author confidence="0.747883">
Kathrine Hammervold
</author>
<affiliation confidence="0.783466">
University of Bergen
</affiliation>
<address confidence="0.6825645">
Sydnesplass 7
N-5007 Bergen, Norway
</address>
<email confidence="0.990847">
kathrine.hammervold@lhs.be
</email>
<sectionHeader confidence="0.984779" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.548496875">
In this paper we describe a neural networks
approach to generation. The task is to generate
sentences with hotel-information from a
structured database. The system is inspired by
Karen Kukich&apos;s ANA, but expands on it by
adding generality in the form of language
independence in representations and lexical
look-up.
</bodyText>
<sectionHeader confidence="0.972953" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999214759259259">
In the growing field of intelligent
communication (web-browsers, dialogue
systems, etc.) the need for a flexible generator
has become more important (e.g. Hovy &amp; Lin,
1999). NLG is usually seen as a two-stage
process where the planning component takes
care of the inter-sentential content planning,
while the surface realisation component
transforms the content representation into a
string of words. Interactions between the two
components have called for the micro-planning
stage to be postulated in the middle, but still the
rule-based pipeline architecture has problems
with sequential rules and their two-way
relations. Statistical approaches have been
developed, and seem to provide flexibility to
generation tasks.
The approach taken in this thesis, however,
explores generation as a .classification task
whereby the representation that describes the
intended meaning of the utterance is ultimately
to be classified into an appropriate surface form.
Although the task as such is a complex one, the
approach allows its decomposition into a series
of smaller classification tasks formulated as
input-output mappings rather than step-wise
rules. One of the goals of the thesis is to study
the ways generation could be broken down into
suitable sub-classification tasks so as to enhance
flexibility in the generation process in general.
Artificial neural networks are a classification
technique that is robust and resistant to noisy
input, and learns to classify inputs on the basis
of training examples, without specific rules that
describe how the classification is to be done.
There is not much research into using ANN&apos;s for
generation, the main reason being long training
times. Two notable exceptions are Kukich
(1987) and Ward (1997), both argue in favour of
NN&apos;s robustness, but at the same time point out
problems with scalability. We believe that with
improved computer facilities that shorten the
training time, this new way of looking at
generation as a classification task constitutes an
interesting approach to generation. We have
chosen Kukich&apos;s approach, as our application
domain is to generate utterances from structured
databases.
This paper is structured as follows; we first
discuss the general model. The second part
briefly describes neural networks. We continue
with describing a possible implementation of the
model, and finally we draw some conclusions
and point to future challenges.
</bodyText>
<sectionHeader confidence="0.802971" genericHeader="method">
1 The model
</sectionHeader>
<bodyText confidence="0.393172">
The task chosen for the system is to generate
sentences with information about hotels. The
information is presented in a structured way in a
database. It is assumed that certain features and
values are given as input to the system.. The
system&apos;s task is then to generate a syntactically
(and semantically) well-formed sentence as a
response to some user request, based on the
</bodyText>
<page confidence="0.997802">
239
</page>
<bodyText confidence="0.972822727272727">
information it gets from the database. These are
some example sentences the model will be able
to generate:
The hotel Regina has twenty rooms.
The hotel Regina is a small hotel.
The hotel Regina has thirty single rooms
The hotel Regina is an expensive hotel.
The. hotel Regina isan expensive:hotetin.the,city
center and the single room price is 4000 BEF.
A single room costs 2000 BEF.
In Karen Kukich&apos;s stock-reporter system ANA
the task is divided into two parts, represented by
two neural networks, one sememe-to-morpheme
network and one morpheme-to-phrase network.
Facts about the status of the stock market were
given to the network as semantic attributes. The
network was trainde to map eight possible
semantic attributes onto morphemes marked for
semantic attributes. These morphemes were then
linearized in the second network.
The output of Kukich&apos;s first net is a set of
English morphemes. The actual morphemes are
present on the output nodes of the net (72
possible morphemes). This makes the whole
system both language dependent and difficult to
mod(. In order to add a morpheme to the list of
possible morphemes the whole network must be
modified.
In order to introduce more flexibility, the task
could be broken into a language independent
and a language dependent task. The database
facts are not dependent on which language they
are represented in. Instead of letting the output
</bodyText>
<construct confidence="0.681756692307692">
nodes stand jr actual tnorphemes they could
represent the different roles the elements can
play in a sentence. Part of the output will stand
for the subject or theme of the final sen tece. The
mental space theory of Fauconnier (1985)
provides an attractive way of presenting the
relations between the facts without commiting
oneself to a particular language. The output of
Neural Network I (NN I) will therefore be
interpreted as an event space, or template, that
can be used for language dependent generation.
The actual values on the output nodes refer to o
concept stored in a lexicon.
</construct>
<bodyText confidence="0.981054204545454">
There is also a discourse model in the system,
which receives information about which features
in the database are relevant for the next
sentence. It also determines the level of
generalisation required for the output. Consider
the two following sentences:
A double room costs 3000 BEF and a single
roam costs 5000 BEE in hotel Regina.
Hotel Regina is expensive.
Saying exactly how much something costs arid
saying how expensive or cheap it is, is just two
ways of communicating the same thing. Both
sentences are based on the same facts. Which
sentence is chosen may depend on the level of
specificity required. This problem of synonymy
is present in various ways in all language
generation systems. Kukich has an example of
two phrases in her system corresponding to the
exact same semantic values (or &amp;quot;sememes&amp;quot;). To
get around the problem she added two extra
sememes to the input and assigned them random
values. In this model we have also introduced an
extra input category, but it serves as a feature
telling the network whether to output the general
or the specific sentence.
In Kukich&apos;s network the task of the second
network is the ordering of the morphemes into a
syntactic string The second network in this
model will also have to order the concepts and
map them onto forms that can represent real
words in a language. We now also have the
advantage of having the event space to help the
network generate the real sentence.
in English sentences there are phenomena such
as agreement that may span over several words
in a sentence. The target sentences above show
agreement between subject and verb, in other
languages e.g. Norwegian there is agreement
between adjectives and the nouns they modify
There have been experiments using neural nets
to recognise relationships based on constituency
and syntactic structure in sentences. Elm an
(1989 and 1990) has shown that neural nets can
learn to represent the concept of word,
</bodyText>
<page confidence="0.964515">
240
</page>
<bodyText confidence="0.999211634146341">
constituency and structural relations by
introducing the concept of time into the
processing of the representations. Elman takes
as a starting point a neural networks
architechture first described by Jordan (1986).
The novel approach is to represent time by the
effect it has on processing and not as an
additional dimension on the input. The solution
consists in giving, the,system .memory. This is
done by introducing a fourth type of units, called
context units, into the network apart from the
typical input, hidden and output units. At each
cycle the hidden unit activations are copied onto
the context units. In the following cycle the
context nodes combine with the new input to
activate the hidden units.
Elman poses the problem of whether a network
can learn underlying aspects of sentence
structure from word order. For the simulation he
used 15 different sentence templates capable of
being filled with nouns and verbs. He used six
different classes of nouns (human, animate, etc.)
and six different classes of verbs (transitive,
intransitive etc.). The network was trained on
random two or three word sentences. The
network was supposed to learn to predict the
order of successive words. For any given
sequence of words there are a limited number of
possible successors. It is impossible to know
with absolute certainty which word follows the
previous, but generalisations can be made based
on type of verb (e.g. a noun should not be
expected to follow an intransitive verb). The
performance of the network was therefore
measured according to the expected frequencies
of occurrence of possible successors, not which
word in reality occurred.
On the basis of the training the network
developed internal representations which
reflected the facts about the possible sequential
ordering of the inputs.
</bodyText>
<construct confidence="0.877328571428571">
&amp;quot;The network is notable to predict the
precise order of words, but recognizes
that (in this corpus) there is a class of
inputs (namely, verbs) that tYpicallv
follow other inputs (namely. nouns)
This knowledge of class behavior is
quite detailed, form the _fact that there is
</construct>
<bodyText confidence="0.868562333333333">
a class of items which always precedes
&apos;&apos;chase&amp;quot;, &amp;quot;bread&amp;quot;, &amp;quot;smash&amp;quot;, it infers
that the large animals form a class.&amp;quot;
(Elman 1990, p. 199)
He also succeeds in representing agreement
between subject and verb, even in sentences
like:
Dog [who chases cat] sees girl
This method of teaching a network the relation
between different words in a sentence could also
be exploited for language generation. The
network can be trained on possible sentence
structures and agreement between the elements
in the sentence. As a starting point the sentence
types of the example sentences above could be
used. In a symbolic system they could be
represented by the following phrase structure
rules, depending on the language in question:
</bodyText>
<equation confidence="0.554225">
S 9 NP VP
NP 9 DET (MOD) N
VP 9 V NP
PP 9PNP
</equation>
<construct confidence="0.972322555555556">
Each of the categories (N. P etc.) will he output
nodes of NN Il according to the linear order
they may occur in the language in question, and
in addition there will be placeholders for
number on nouns, verbs and modifiers. The
output of NN II is now a linear structure where
we know the phrase types. This information
. could e.g. be used by a text-to-speech system to
assign stress to certain word etc.
</construct>
<bodyText confidence="0.409492">
Our model can now be represented like this:
</bodyText>
<page confidence="0.957975">
241
</page>
<table confidence="0.760910307692308">
Discourse Model
(Level of generalisation,
sentence aggregation etc.)
Database (013)
Facts about hotels
Feature selector
Neural Network I
Features to concepts in event_space
Neural Network II
Concepts to tagged sentence
Post Processor
Look-up in lexicon
Output to user, text-to-speech system etc.
</table>
<figure confidence="0.897344777777778">
e.g.
Hotel Regina nes twenty rooms
Language
Dependent
Lexicon
Concepts and
gramm.
features to
words.
</figure>
<footnote confidence="0.892067">
2 Brief introduction to neural networks
</footnote>
<page confidence="0.99339">
242
</page>
<bodyText confidence="0.999977475">
An artificial neural network is a metaphor of the
way human and other mammalian brains work.
The biological brain consists of a great number
of interacting elements, neuron. A neuron
collects electrical impulses inside the cell
membrane. If the level of impulses reaches a
certain threshold, the neuron will generate an
action potential, a pulse that travels along a thin
fibre to other neuronsimausing them to Aare- the
electrical impulse. Each neuron may have
synaptic connections to thousands of other
neurons.
An artificial neural network consists of nodes or
units, modelled on neurons. These nodes can
receive and transfer activation. Each node is
connected to many other nodes, and the strength
or efficiency of each connection is determined
by a weight. Together with the nodes the
connections are the most salient features of a
neural network. The weights on the connections
can be modified by learning rules, enabling the
network to learn new tasks. There are several
types of learning algorithms that may be used to
train neural networks, but back propagation is
probably the most common one. During training,
an input/output pair is presented to the network.
After a whole set of input/output pairs have been
run through the network the back propagation
algorithm calculates how far the actual output of
the net is from the desired output, and the
weights on the connections adjusted in the right
direction. If there is any overall pattern to the
data, or some consistent relationship between the
inputs and results of each record, the network
should be able to eventually create an internal
mapping of weights that can accurately
reproduce the expected output. Since the
knowledge the network acquires is a result of the
mappings. how the input and output is
represented is of great importance.
</bodyText>
<sectionHeader confidence="0.989783" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.8503695">
The following features are used to describe the
information in the database:
</bodyText>
<table confidence="0.994211863636364">
Feature 1 Possible Value type # units
V alues in
input
_ vector
Service_d Hotel Binary 2
omainl
Name Ariadne, Binary 2
Rabbit,
Regina
#_single_ 5-50 Numerical I
MOMS value
#_double 5-50 Numerical 1
_rooms value
Single ro 1000- Numerical 1
om price 4000 value
Double_r 2000- Numerical 1
oom_pric 6000 value
e
Location City Binary 2
center /
Business
Park
</table>
<bodyText confidence="0.978237904761905">
The feature selector fetches the necessary values
(determined by the discourse model) and inputs
them to NN I. The input vector is eleven units
long. Ten units are the local representations of
the features in the database and the last unit
represents the generalizer feature from the
discourse model. The Stuttgart Neural Networks
Simulator (SNNS2) which will be used for the
implementation only allows values between —1
and I, so the numerical values will be
normalized to fit into the vector. This is also
necessary so the relative importance of the
different features are not out of proportion.
The event space in the output will consist of the
following elements:
(see table 1 at the end)
The vocabulary needed for the generation task is
represented by binary codes, e.g. based on the
alphabetical order of the forms. If we let the
subject/theme part of the vector be 7 units long
At the moment we deal only with hotel info.
</bodyText>
<page confidence="0.868099">
2
</page>
<bodyText confidence="0.4526245">
http://www.informatik.unistuttgart.de/ipyr/bv/projekt
eisnnsisnns.htinl
</bodyText>
<page confidence="0.996643">
243
</page>
<table confidence="0.926967782608696">
we can represent 27 (128) different word with
numerical values. 0000001 is concept number 1,
0000010 is concept number 2 and so on.
Binary code List of concepts
0000001 ADRIANE
0000010 BEF
0000011 BIG
0000100 • CHEAP
0000101 CITY CENTER
0000110 COST
0000111 DOUBLE ROOM
0001000 EXPENSIVE
,
0001001 BUSINESS PARK
0001010 HAVE
0001011 HOTEL
0001100 PRICE
0001101 RABBIT
0001110 REASONABLE
0001111 REGINA
0010000 ROOM
0010001 SINGLE ROOM
0010010 SMALL
</table>
<bodyText confidence="0.870889133333333">
In table 2 are some example inputs and outputs,
a 1 represents activation on an input or output
node.
Now that we have a language independent
representation of the sentence we would like to
generate, it needs to be cast into a sentence in a
natural language. The languages of&apos; this system
will be English and Norwegian, but the intention
is that other languages may also be represented.
These input-output combinations shown above
should ultimately correspond to the following
target sentences (after NN 11 and post
processing):
I) The hotel Regina has twenty single rooms.
Hotel Regina liar tfue enkeltrom.
</bodyText>
<listItem confidence="0.745820461538461">
2) The hotel Regina is a small hotel.
Hotel Regina er et lite hotel
3) A single room costs four thousand 13elgicm
francs.
Et enkeltrom koster.fire trisen helgiskefranc.
4) A double room costs seven thousand Belgian
francs and a single room costs four thousand
Belgian francs.
Et dobbeltrom koster syv tusen belgiske franc
og et enkeltrom lcoster fire tusen belgiske
franc
5) The hotel Regina is expensive.
•1,Hotell.Regincrer dyrt.
</listItem>
<bodyText confidence="0.878885352941177">
6) The hotel Ariadne is a cheap hotel in the city
centre.
Hotel&apos; Ariadne er et hillig hotell i sentrum.
The whole output vector is shown in table 3.
NN II must be trained on agreement. This is
done by teaching it to discover relationships,
such as the fact that the feature SINGULAR on
the subject noun, is associated with the feature
SINGULAR on the main verb. The input nodes
on Network II will be similar to the output of the
first net, but the input will be fed sequentially to
the network (theme, number, main_event,
complement etc.
If we assume that the output of NN I now serves
as the input for NN II, this will be our desired
output (only the activated nodes are shown
here):
</bodyText>
<figure confidence="0.86878625">
II: REG1NA&apos;SG^HAVEA20ASINGLE_ROOM
01:
REGINAADEPSGAHAVE^SGA20^PLURASIN
GLE_ROOM^INDEPPLUR.
After post-processing: The hotel Regina has
twenty single rooms
12: RF,GINAASINGASMALLAF-10TEL^SING
01.
REGINAADEFASINGABE^S1NG^SMALL^SIN
G^INDEFAHOTELAINDEFASING
After post-processing: The Hotel Regina is a
small hotel
</figure>
<footnote confidence="0.642157">
and so on...
</footnote>
<page confidence="0.996585">
244
</page>
<bodyText confidence="0.999920111111111">
After a look-up in an English dictionary we find
that the singular form of BE is is, and the plural
form of SINGLE ROOM is single rooms. The
reason we do not output this directly is that we
would then require different output nodes for all
the different forms of a word. Instead we
combine the word with the feature to find the
correct morphological form. Numbers could in
fact be processed hy.a.special.nurnber grammar
to avoid having to list all numbers in a lexicon.
These tasks could of course also be solved using
other neural networks.
The nodes in the output vector represents
different syntactic categories, so we also get a
surface syntactic structure directly output from
the net, which could be used for stress
information etc. to be input to a speech
generator.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999980333333333">
The two networks were trained using
backpropagation with momentum ( learning rate
0.5), each training set consisting of 57 sentences
was run for 600 cycles. For the first network the
mean square error (MSE) was 0.08, for the
second network 0.175. The number of hidden
nodes in each network was 20, a higher or lower
number of hidden nodes resulted in a higher
MSE.
Using a threshold value of 0.8, the network
could be said to have correctly learned the
mapping from output to input activations.
</bodyText>
<sectionHeader confidence="0.747797" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.9999592">
Extensive experimentation is needed to define
the proper typology and parameters for the
neural networks. We especially need to
experiment more with different learning
methods. A future research topic will be to see
what kinds of further subdivision of the tasks are
needed. Elrnan suggests that simple recurrent
networks could be capable of learning
pronominal reference, and this would be an
interesting extension of the system.
</bodyText>
<sectionHeader confidence="0.995912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999084625">
Fauconnier, G. 1985. Mental Spaces: Aspects of
Meaning construction in Natural Language.
Cambridge, Mass.: MIT Press.
&amp;Imam .,-Ftructure -in-. time.-
Cognitive Science 14, 172-211.
Elman, J. E. 1991. Distributed Representations,
Simple Recurrent Networks, and Grammatical
Structure. Machine Learning 7, 195-225. Kluwer
Academic Publishers, Boston.
Jordan, M 1. 1986. Serial Order: A parallel
distributed processing approach (Tech. Rep. NO.
8604). San Diego: University of California, Institute
for Cognitive Science).
Rovy &amp; Lin 1999. Automated Text Summarization in
SUMMARIST. In Mani &amp; Maybury (eds.) Advances
in Automated Text Summarization. MIT Press.
Kukich, K. 1987. Where do phrases come from some
preliminary experiments in connectionist phrase
generation. In Natural Language Generation: New
results in Artificial Intelligence, Psychology and
Linguistics. Gerard Kempen (editor). Martinus
Nijhoff Publishers, Dordrecht.
Ward, N. 1994. A connectionist language generator.
Norwood, N.J. Ablex Pub. Corp.
</reference>
<page confidence="0.998642">
245
</page>
<table confidence="0.992606551020408">
TABLE 1
Event elements Possible values Number of units in output vector
Theme RABBIT, AR1ADNE, REGINA, SINGLE ROOM, 7 + 2 units representing the feature
DOUBLE ROOM number (possible values sing/plur)
SINGLE PRICE, DOUBLE PRICE
Main event HAVE, COST 7
Complement SINGLE_ROOM , DOUBLE ROOM, ROOM, 7 + 2 units representing the feature
BEF number (possible values sing/plur)
Subject_predicate EXPENSIVE, REASONABLE, CHEAP, SMALL BIG 7+ 2 units representing &apos; the , feature
, number (possible values sing/plur)
Modifier EXPENSIVE, REASONABLE, CHEAP, SMALL, BIG 7 units
Numerical values (e.g. 20, 4000)
TABLE 2
INPUT TO NETWORK I
Name Service_do tisingle_ro itdouble_r Price_single_ro Price_double_ro Location Categorizer
main &apos;cons ooms OM 0171
11 1 1 1 20 0 0 0 0 0 0
12 I 1 1 I 1 0 0 0 0 1
13 0 0 1 0 0 4000 0 0 0 0
14 0 0 l 0 0 0 7000 0 0 0
14&apos; 0- 0 I 0 0 4000 0 0 0 0
15 I I I 0 0 4000 7000 0 0 1
16 0 I 1 0 0 0 0 0 1 I
TABLE)
OUTPUT OF NETWORK I
Theme Main_event Modifier Complement Subject_predicat Location
C
01 000111101 0001010 (HAVE) 20 001000110(SING
(REGINA. LE ROOM. plur.)
sine.)
02 000111101 0010010 000101101
(REGINA, clef, (SMALL) (HOTEL. sing.)
sing.)
03 0010001 0000110 (COST) 4000 000001010
(SINGLE_ROO (BEF. plur)
M. sinE)
04 000011001 0000110 (COSI-) 7000 000001010
(DOUBLE_RO (BEE. plur.)
OM, sine.)
04&apos; 001000101 0000110 4000 000001010
(SINGLE R00 (COST.) (BEE, plur)
M. sing.)
05 000111101 000101101 000100000 0000101
(REGINA. (HOTEL. sing.) (EXPENSIVE, (CITY
sing.) sing.) CENTER.
sing)
06 000000101 0000100
(ARIADNE, (CHEAP)
sing.)
</table>
<page confidence="0.994248">
246
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528148">
<title confidence="0.998145">Sentence generation and neural networks</title>
<author confidence="0.986424">Kathrine</author>
<affiliation confidence="0.89315">University of Sydnesplass</affiliation>
<address confidence="0.706593">N-5007 Bergen,</address>
<email confidence="0.991289">kathrine.hammervold@lhs.be</email>
<abstract confidence="0.986407888888889">In this paper we describe a neural networks approach to generation. The task is to generate sentences with hotel-information from a structured database. The system is inspired by Karen Kukich&apos;s ANA, but expands on it by adding generality in the form of language independence in representations and lexical look-up.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Fauconnier</author>
</authors>
<title>Mental Spaces: Aspects of Meaning construction in Natural Language.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="4912" citStr="Fauconnier (1985)" startWordPosition="764" endWordPosition="765">ystem both language dependent and difficult to mod(. In order to add a morpheme to the list of possible morphemes the whole network must be modified. In order to introduce more flexibility, the task could be broken into a language independent and a language dependent task. The database facts are not dependent on which language they are represented in. Instead of letting the output nodes stand jr actual tnorphemes they could represent the different roles the elements can play in a sentence. Part of the output will stand for the subject or theme of the final sen tece. The mental space theory of Fauconnier (1985) provides an attractive way of presenting the relations between the facts without commiting oneself to a particular language. The output of Neural Network I (NN I) will therefore be interpreted as an event space, or template, that can be used for language dependent generation. The actual values on the output nodes refer to o concept stored in a lexicon. There is also a discourse model in the system, which receives information about which features in the database are relevant for the next sentence. It also determines the level of generalisation required for the output. Consider the two followin</context>
</contexts>
<marker>Fauconnier, 1985</marker>
<rawString>Fauconnier, G. 1985. Mental Spaces: Aspects of Meaning construction in Natural Language. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="false">
<title>Imam .,-Ftructure -in-.</title>
<journal>time.-Cognitive Science</journal>
<volume>14</volume>
<pages>172--211</pages>
<marker></marker>
<rawString>&amp;Imam .,-Ftructure -in-. time.-Cognitive Science 14, 172-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Elman</author>
</authors>
<title>Distributed Representations, Simple Recurrent Networks, and Grammatical Structure.</title>
<date>1991</date>
<journal>Machine Learning</journal>
<volume>7</volume>
<pages>195--225</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<marker>Elman, 1991</marker>
<rawString>Elman, J. E. 1991. Distributed Representations, Simple Recurrent Networks, and Grammatical Structure. Machine Learning 7, 195-225. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jordan</author>
</authors>
<title>Serial Order: A parallel distributed processing approach (Tech.</title>
<date>1986</date>
<tech>Rep. NO. 8604).</tech>
<institution>San Diego: University of California, Institute for Cognitive Science).</institution>
<contexts>
<context position="7419" citStr="Jordan (1986)" startWordPosition="1181" endWordPosition="1182">e. The target sentences above show agreement between subject and verb, in other languages e.g. Norwegian there is agreement between adjectives and the nouns they modify There have been experiments using neural nets to recognise relationships based on constituency and syntactic structure in sentences. Elm an (1989 and 1990) has shown that neural nets can learn to represent the concept of word, 240 constituency and structural relations by introducing the concept of time into the processing of the representations. Elman takes as a starting point a neural networks architechture first described by Jordan (1986). The novel approach is to represent time by the effect it has on processing and not as an additional dimension on the input. The solution consists in giving, the,system .memory. This is done by introducing a fourth type of units, called context units, into the network apart from the typical input, hidden and output units. At each cycle the hidden unit activations are copied onto the context units. In the following cycle the context nodes combine with the new input to activate the hidden units. Elman poses the problem of whether a network can learn underlying aspects of sentence structure from</context>
</contexts>
<marker>Jordan, 1986</marker>
<rawString>Jordan, M 1. 1986. Serial Order: A parallel distributed processing approach (Tech. Rep. NO. 8604). San Diego: University of California, Institute for Cognitive Science).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rovy</author>
<author>Lin</author>
</authors>
<title>Automated Text Summarization</title>
<date>1999</date>
<booktitle>Advances in Automated Text Summarization.</booktitle>
<editor>in SUMMARIST. In Mani &amp; Maybury (eds.)</editor>
<publisher>MIT Press.</publisher>
<marker>Rovy, Lin, 1999</marker>
<rawString>Rovy &amp; Lin 1999. Automated Text Summarization in SUMMARIST. In Mani &amp; Maybury (eds.) Advances in Automated Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Where do phrases come from some preliminary experiments in connectionist phrase generation.</title>
<date>1987</date>
<booktitle>In Natural Language Generation: New results in Artificial Intelligence, Psychology and Linguistics. Gerard Kempen (editor). Martinus Nijhoff Publishers,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="2207" citStr="Kukich (1987)" startWordPosition="325" endWordPosition="326">appings rather than step-wise rules. One of the goals of the thesis is to study the ways generation could be broken down into suitable sub-classification tasks so as to enhance flexibility in the generation process in general. Artificial neural networks are a classification technique that is robust and resistant to noisy input, and learns to classify inputs on the basis of training examples, without specific rules that describe how the classification is to be done. There is not much research into using ANN&apos;s for generation, the main reason being long training times. Two notable exceptions are Kukich (1987) and Ward (1997), both argue in favour of NN&apos;s robustness, but at the same time point out problems with scalability. We believe that with improved computer facilities that shorten the training time, this new way of looking at generation as a classification task constitutes an interesting approach to generation. We have chosen Kukich&apos;s approach, as our application domain is to generate utterances from structured databases. This paper is structured as follows; we first discuss the general model. The second part briefly describes neural networks. We continue with describing a possible implementat</context>
</contexts>
<marker>Kukich, 1987</marker>
<rawString>Kukich, K. 1987. Where do phrases come from some preliminary experiments in connectionist phrase generation. In Natural Language Generation: New results in Artificial Intelligence, Psychology and Linguistics. Gerard Kempen (editor). Martinus Nijhoff Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
</authors>
<title>A connectionist language generator.</title>
<date>1994</date>
<publisher>Ablex Pub. Corp.</publisher>
<location>Norwood, N.J.</location>
<marker>Ward, 1994</marker>
<rawString>Ward, N. 1994. A connectionist language generator. Norwood, N.J. Ablex Pub. Corp.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>