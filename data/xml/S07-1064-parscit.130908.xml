<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.619739">
SICS: Valence annotation based on seeds in word space
</title>
<author confidence="0.409503">
Magnus Sahlgren
</author>
<affiliation confidence="0.224262">
SICS
</affiliation>
<address confidence="0.609723333333333">
Box 1263
SE-164 29 Kista
Sweden
</address>
<email confidence="0.989223">
mange@sics.se
</email>
<author confidence="0.458155">
Jussi Karlgren
</author>
<affiliation confidence="0.2494">
SICS
</affiliation>
<address confidence="0.611097666666667">
Box 1263
SE-164 29 Kista
Sweden
</address>
<email confidence="0.984361">
jussi@sics.se
</email>
<author confidence="0.322994">
Gunnar Eriksson
</author>
<affiliation confidence="0.176368">
SICS
</affiliation>
<address confidence="0.570218333333333">
Box 1263
SE-164 29 Kista
Sweden
</address>
<email confidence="0.984722">
guer@sics.se
</email>
<sectionHeader confidence="0.995269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973473684211">
This paper reports on a experiment to iden-
tify the emotional loading (the “valence”)
of news headlines. The experiment re-
ported is based on a resource-thrifty ap-
proach for valence annotation based on a
word-space model and a set of seed words.
The model was trained on newsprint, and va-
lence was computed using proximity to one
of two manually defined points in a high-
dimensional word space — one represent-
ing positive valence, the other representing
negative valence. By projecting each head-
line into this space, choosing as valence the
similarity score to the point that was closer
to the headline, the experiment provided re-
sults with high recall of negative or positive
headlines. These results show that working
without a high-coverage lexicon is a viable
approach to content analysis of textual data.
</bodyText>
<sectionHeader confidence="0.952858" genericHeader="keywords">
1 The Semeval task
</sectionHeader>
<bodyText confidence="0.9996685">
This a report of an experiment proposed as the “Af-
fective Text” task of the 4th international Work-
shop on Semantic Evaluation (SemEval) to deter-
mine whether news headlines are loaded with pre-
eminently positive or negative emotion or valence.
An example of a test headline can be:
</bodyText>
<sectionHeader confidence="0.866885" genericHeader="introduction">
DISCOVERED BOYS BRING SHOCK, JOY
</sectionHeader>
<subsectionHeader confidence="0.446803">
2 Working without a lexicon
</subsectionHeader>
<bodyText confidence="0.9999034">
Our approach takes as its starting point the obser-
vation that lexical resources always are noisy, out
of date, and most often suffer simultaneously from
being both too specific and too general. For our ex-
periments, our only lexical resource consists of a list
of eight positive words and eight negative words, as
shown below in Table 1. We use a medium-sized
corpus of general newsprint to build a general word
space, and use our minimal lexical resource to orient
ourselves in it.
</bodyText>
<sectionHeader confidence="0.959883" genericHeader="method">
3 Word space
</sectionHeader>
<bodyText confidence="0.996182869565217">
A word space is a high-dimensional vector space
built from distributional statistics (Sch¨utze, 1993;
Sahlgren, 2006), in which each word in the vocab-
ulary is represented as a context vector of occur-
rence frequencies: where is the
frequency of word in some context .
The point of this representation is that seman-
tic similarity between words can be computed us-
ing vector similarity measures. Thus, the similar-
ity in meaning between the words and can
be quantified by computing the similarity between
their respective context vectors: sim
sim .
The semantics of such a word space are deter-
mined by the data from which the occurrence in-
formation has been collected. Since the data set in
the SemEval Affective Text task consists of news
headlines, a relevant word space should be pro-
duced from topically and stylistically similar texts,
such as newswire documents. For this reason, we
trained our model on a corpus of English-language
newsprint which is available for experimentation for
participants in the Cross Language Evaluation Fo-
</bodyText>
<page confidence="0.971053">
296
</page>
<bodyText confidence="0.983280555555556">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 296–299,
Prague, June 2007. c�2007 Association for Computational Linguistics
rum (CLEF).1 The corpus consists of some 100 000
newswire documents from Los Angeles Times for
the year 1994. We presume any similarly sized
collection of newsprint would produce similar re-
sults. We lemmatized the data using tools from
Connexor,2 and removed stop words, leaving some
28 million words with a vocabulary of approxi-
mately 300 000 words. Since the data for the af-
fective task only consisted of news headlines, we
treated each headline in the LA times corpus as
a separate document, thus doubling the number of
documents in the data.
For harvesting occurrence information, we used
documents as contexts and standard tfidf-weighting
of frequencies, resulting in a 220 220-dimensional
word space. No dimensionality reduction was used.
</bodyText>
<sectionHeader confidence="0.995926" genericHeader="method">
4 Seeds
</sectionHeader>
<bodyText confidence="0.999976333333333">
In order to construct valence vectors, we used a set
of manually selected seed words (8 positive and 8
negative words), shown in Table 1. These words
were chosen (subjectively) to represent typical ex-
pression of positive or negative attitude in news
texts. The size of the seed set was determined by
a number of initial experiments on the development
data, where we varied the size of the seed sets from
these 8 words to some 700 words in each set (us-
ing the WordNet Affect hierarchy (Strapparava and
Valitutti, 2004)).
As comparison, Turney and Littman (2003) used
seed sets consisting of 7 words in their word valence
annotation experiments, while Turney (2002) used
minimal seed sets consisting of only one positive
and one negative word (“excellent” and “poor”) in
his experiments on review classification. Such min-
imal seed sets of antonym pairs are not possible to
use in the present experiment because they are often
nearest neighbors to each other in the word space.
Also, it is difficult to find such clear paradigm words
for the newswire domain.
The seed words were used to postulate one posi-
tive and one negative point (i.e. vector) in the word
space by simply taking the centroid of the seed word
points: where is one of the seed
sets, and is a word in this set.
</bodyText>
<footnote confidence="0.9999105">
1http://www.clef-campaign.org/
2http://www.conexor.fi/
</footnote>
<table confidence="0.998896111111111">
Positive Negative
positive negative
good bad
win defeat
success disaster
peace war
happy sad
healthy sick
safe dangerous
</table>
<tableCaption confidence="0.9801325">
Table 1: The seed words used to create valence vec-
tors.
</tableCaption>
<sectionHeader confidence="0.911143" genericHeader="method">
5 Syntagmatic vs paradigmatic relations
</sectionHeader>
<bodyText confidence="0.977457148148148">
Our hypothesis is that words carrying most of the
valence in news headlines in the experimental test
set are syntagmatically rather than paradigmatically
related to the kind of very general words used in
our seed set.3 As an example, consider test headline
501:
TWO HUSSEIN ALLIES ARE HANGED, IRAQI OFFICIAL SAYS.
It seems reasonable to believe that this headline
should be annotated with a negative valence, and
that the desicive word in this case is “hanged.” Ob-
viously, “hanged” has no paradigmatic neighbors
(e.g. synonyms, antonyms or other ’nyms) among
the seed words. However, it is likely that “hanged”
will co-occur with (and therefore have a syntagmatic
relation to) general negative terms such as “danger-
ous” and maybe “war.” In fact, in this example head-
line, the most negatively associated words are prob-
ably “Hussein” and “Iraqi,” which often co-occur
with general negative terms such as “war” and “dan-
gerous” in newswire text.
To produce a word space that contains predomi-
nantely syntagmatic relations, we built the distribu-
tional relations using entire documents as contexts
(i.e. each dimension in the word space corresponds
to a document in the data). If we would have used
words as contexts instead, we would have ended up
with a paradigmatic word space.4
</bodyText>
<footnote confidence="0.977755">
3Syntagmatic relations hold between co-occurring words,
while paradigmatic relations hold between words that do not
co-occur, but that occur with the same other words.
4See Sahlgren (2006) for an explanation of how the choice
of contexts determines the semantic content of the word space.
</footnote>
<page confidence="0.992634">
297
</page>
<sectionHeader confidence="0.973591" genericHeader="method">
6 Compositionality and semantic relations
</sectionHeader>
<bodyText confidence="0.999977">
The relations between words in headlines were mod-
eled using the most simple operation conceivable:
we simply add all words’ context vectors to a com-
pound headline vector and use that as the represen-
tation of the headline: where is a
test headline, and is a word in this headline.
This is obviously a daring, if not foolhardy, ap-
proach to modelling syntactic structure, composi-
tional semantics, and all types of intra-sentential se-
mantic dependencies. It can fairly be expected to be
improved upon through an appropriate finer-grained
analysis of word presence, adjacency and syntactic
relationships. However, this approach is similar to
that taken by most search engines in use today, is
a useful first baseline, and as can be seen from our
results below, does deliver acceptable results.
</bodyText>
<sectionHeader confidence="0.970912" genericHeader="method">
7 Valence annotation
</sectionHeader>
<bodyText confidence="0.999982128205128">
To perform the valence annotation, we first lem-
matized the headlines and removed stop words and
words with frequency above 10 000 in the LA
times corpus. For each headline, we then summed
— as discussed above — the context vectors of
the remaining words, thus producing a 220 220-
dimensional vector for each headline. This vector
was then compared to each of the postulated valence
vectors by computing the cosine of the angles be-
tween the vectors.
We thus have for each headline two cosines, one
between the headline and the positive vector and one
between the headline and the negative vector. The
valence vector with highest cosine score (and thus
the smallest spatial angle) was chosen to annotate
the headline. For the negative valence vector we as-
signed a negative valence value, and for the positive
vector a positive value. In 11 cases, a value of
was ascribed, either because all headline words were
removed by frequency and stop word filtering, or be-
cause none of the remaining words occurred in our
newsprint corpus.
Our method thus only delivers a binary valence
decision — either positive or negative valence.
Granted, we could have assigned a neutral valence
to very low cosine scores, but as any threshold for
deciding on a neutral score would be completely ar-
bitrary, we decided to only give fully positive or neg-
ative scores to the test headlines. Also, since our
aim was to provide a high-recall result, we did not
wish to leave any headline with an equivocal score.
We scaled the scores to fit the requirements of the
coarse-grained evaluation: for each headline with
a non-zero score, we multiplied the value with
and boosted each value with .5 By this scaling op-
eration we guaranteed a positive or a negative score
for each headline (apart from the 11 exceptions, in
effect unanalyzed by our algorithm, as mentioned
above).
</bodyText>
<sectionHeader confidence="0.999582" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.99988175">
The results from the fine-grained and coarse-grained
evaluations are shown in Table 2. They show, much
as we anticipated, that the coarse-grained evaluation
was appropriate for our purposes.
</bodyText>
<table confidence="0.976876">
Fine-grained Coarse-grained Recall
Accuracy Precision
20.68 29.00 28.41 60.17
</table>
<tableCaption confidence="0.998765">
Table 2: The results of the valence annotation.
</tableCaption>
<subsectionHeader confidence="0.54409">
8.1 Correlation coefficients, normality
</subsectionHeader>
<bodyText confidence="0.971860176470588">
assumptions, and validity of results
The fine-grained evaluation as given by the organ-
isers and as shown in Table 2 was computed using
Pearson’s product-moment coefficient. Pearson’s
correlation coefficient is a parametric statistic and
assumes normal distribution of the data it is testing
for correlation. While we have no idea of neither
the other contributions’ score distribution, nor that
of the given test set, we certainly do know that our
data are not normally distributed. We would much
prefer to evaluate our results using a non-parametric
correlation test, such as Spearman’s , and suggest
that the all results would be rescored using some
non-parametric method instead — this would reduce
the risk of inadvertent false positives stemming from
divergence from the normal distribution rather than
divergence from the test set.
</bodyText>
<footnote confidence="0.865622">
5The coarse-grained evaluation collapsed values in the
ranges as negative, as neutral, and
as positive.
</footnote>
<page confidence="0.988557">
298
</page>
<subsectionHeader confidence="0.988253">
8.2 Use cases
</subsectionHeader>
<bodyText confidence="0.999683571428571">
Evaluation of abstract features such as emotional va-
lence can be done within a system oriented frame-
work such as the one used in this experiment. Al-
ternatively, one could evaluate the results using
a parametrized use case scenario. A simple ex-
ample might be to aim for either high recall or
high precision, rather than using an average which
folds in both scenarios into one numeric score
— easy to compare between systems but dubious
in its relevance to any imaginable real life task.
There are metrics, as formal as the simple recall-
precision-framework in traditional adhoc retrieval,
that could be adapted for this purpose (J¨arvelin and
Kek¨al¨ainen, 2002, e.g.).
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="related work">
9 Related research
</sectionHeader>
<bodyText confidence="0.999995625">
Our approach to valence annotation is similar
to the second method described by Turney and
Littman (2003). In short, their method uses sin-
gular value decomposition to produce a reduced-
dimensional word space, in which word valence
is computed by subtracting the cosine between the
word and a set of negative seed words from the co-
sine between the word and a set of positive seed
words.
The difference between our approach and theirs is
that our approach does not require any computation-
ally expensive matrix decomposition, as we do not
see any reason to restructure our word space. Turney
and Littman (2003) hypothesize that singular value
decomposition is beneficial for the results in valency
annotation because it infers paradigmatic relations
between words in the reduced space. However, as
we argued in Section 5, we believe that the headline
valency annotation task calls for syntagmatic rather
than paradigmatic relations. Furthermore, we fail to
see the motivation for using singular value decom-
position, since if paradigmatic relations are what is
needed, then why not simply use words as dimen-
sions of the word space?
</bodyText>
<sectionHeader confidence="0.897892" genericHeader="conclusions">
10 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999992833333333">
Our results show that a resource-poor but data-rich
method can deliver sensible results. This is in keep-
ing with our overall approach, which aims for as lit-
tle pre-computed resources as possible.
At almost every juncture in our processing we
made risky and simplistic assumptions — using sim-
ple frequencies of word occurrence as a semantic
model; using a small seed set of positive and nega-
tive terms as a target; postulating one semantic locus
each for positive and negative emotion; modelling
syntactic and semantic relations between terms by
vector addition — and yet we find that the seman-
tic structure of distributional statistics yields a signal
good enough for distinguishing positive from nega-
tive headlines with a non-random accuracy. Despite
its simplicity, out method produces very good recall
(60.17) in the coarse-grained evaluation (the median
recall for all systems is 29.59). This speaks to the
power of distributional semantics and gives promise
of improvement if some of the choice points during
the process are returned to: some decisions can well
benefit from being made on principled and informed
grounds rather than searching under the street lamp,
as it were.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998268074074074">
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422–446.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. PhD Dissertation, Depart-
ment of Linguistics, Stockholm University.
Hinrich Sch¨utze. 1993. Word space. In Proceedings
of the 1993 Conference on Advances in Neural Infor-
mation Processing Systems, NIPS’93, pages 895–902,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: an affective extension of wordnet. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation, LREC’04, pages
1083–1086.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315–346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Confer-
ence of the Association for Computational Linguistics,
ACL’02, pages 417–424.
</reference>
<page confidence="0.998653">
299
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000434">
<title confidence="0.993754">SICS: Valence annotation based on seeds in word space</title>
<author confidence="0.989173">Magnus Sahlgren</author>
<affiliation confidence="0.953548">SICS</affiliation>
<address confidence="0.709657333333333">Box 1263 SE-164 29 Kista Sweden</address>
<email confidence="0.930751">mange@sics.se</email>
<author confidence="0.995922">Jussi Karlgren</author>
<affiliation confidence="0.966118">SICS</affiliation>
<address confidence="0.848188">Box 1263 SE-164 29 Kista Sweden</address>
<email confidence="0.957333">jussi@sics.se</email>
<author confidence="0.98562">Gunnar Eriksson</author>
<affiliation confidence="0.974671">SICS</affiliation>
<address confidence="0.824386333333333">Box 1263 SE-164 29 Kista Sweden</address>
<email confidence="0.979784">guer@sics.se</email>
<abstract confidence="0.991566996835444">This paper reports on a experiment to identify the emotional loading (the “valence”) of news headlines. The experiment reported is based on a resource-thrifty approach for valence annotation based on a word-space model and a set of seed words. The model was trained on newsprint, and valence was computed using proximity to one of two manually defined points in a highdimensional word space — one representing positive valence, the other representing negative valence. By projecting each headline into this space, choosing as valence the similarity score to the point that was closer to the headline, the experiment provided results with high recall of negative or positive headlines. These results show that working without a high-coverage lexicon is a viable approach to content analysis of textual data. 1 The Semeval task This a report of an experiment proposed as the “Affective Text” task of the 4th international Workshop on Semantic Evaluation (SemEval) to determine whether news headlines are loaded with prepositive or negative emotion or An example of a test headline can be: BOYS BRING 2 Working without a lexicon Our approach takes as its starting point the observation that lexical resources always are noisy, out of date, and most often suffer simultaneously from being both too specific and too general. For our experiments, our only lexical resource consists of a list of eight positive words and eight negative words, as shown below in Table 1. We use a medium-sized of general newsprint to build a general and use our minimal lexical resource to orient ourselves in it. 3 Word space A word space is a high-dimensional vector space built from distributional statistics (Sch¨utze, 1993; Sahlgren, 2006), in which each word in the vocabis represented as a vector occurrence frequencies: where is the frequency of word in some context . The point of this representation is that semantic similarity between words can be computed usvector similarity measures. Thus, the similarity in meaning between the words and can be quantified by computing the similarity between their respective context vectors: sim sim . The semantics of such a word space are determined by the data from which the occurrence information has been collected. Since the data set in the SemEval Affective Text task consists of news headlines, a relevant word space should be produced from topically and stylistically similar texts, such as newswire documents. For this reason, we trained our model on a corpus of English-language newsprint which is available for experimentation for in the Cross Language Evaluation Fo- 296 of the 4th International Workshop on Semantic Evaluations pages 296–299, June 2007. Association for Computational Linguistics The corpus consists of some 100 000 newswire documents from Los Angeles Times for the year 1994. We presume any similarly sized collection of newsprint would produce similar results. We lemmatized the data using tools from and removed stop words, leaving some 28 million words with a vocabulary of approximately 300 000 words. Since the data for the affective task only consisted of news headlines, we treated each headline in the LA times corpus as a separate document, thus doubling the number of documents in the data. For harvesting occurrence information, we used documents as contexts and standard tfidf-weighting of frequencies, resulting in a 220 220-dimensional word space. No dimensionality reduction was used. 4 Seeds In order to construct valence vectors, we used a set of manually selected seed words (8 positive and 8 negative words), shown in Table 1. These words were chosen (subjectively) to represent typical expression of positive or negative attitude in news texts. The size of the seed set was determined by a number of initial experiments on the development data, where we varied the size of the seed sets from these 8 words to some 700 words in each set (using the WordNet Affect hierarchy (Strapparava and Valitutti, 2004)). As comparison, Turney and Littman (2003) used seed sets consisting of 7 words in their word valence annotation experiments, while Turney (2002) used minimal seed sets consisting of only one positive and one negative word (“excellent” and “poor”) in his experiments on review classification. Such minimal seed sets of antonym pairs are not possible to use in the present experiment because they are often nearest neighbors to each other in the word space. Also, it is difficult to find such clear paradigm words for the newswire domain. The seed words were used to postulate one positive and one negative point (i.e. vector) in the word space by simply taking the centroid of the seed word points: where is one of the seed sets, and is a word in this set. Positive Negative positive negative good bad win defeat success disaster peace war happy sad healthy sick safe dangerous Table 1: The seed words used to create valence vectors. 5 Syntagmatic vs paradigmatic relations Our hypothesis is that words carrying most of the valence in news headlines in the experimental test are than paradigmatically related to the kind of very general words used in seed As an example, consider test headline 501: ALLIES ARE OFFICIAL It seems reasonable to believe that this headline should be annotated with a negative valence, and that the desicive word in this case is “hanged.” Obviously, “hanged” has no paradigmatic neighbors (e.g. synonyms, antonyms or other ’nyms) among the seed words. However, it is likely that “hanged” will co-occur with (and therefore have a syntagmatic relation to) general negative terms such as “dangerous” and maybe “war.” In fact, in this example headline, the most negatively associated words are probably “Hussein” and “Iraqi,” which often co-occur with general negative terms such as “war” and “dangerous” in newswire text. To produce a word space that contains predominantely syntagmatic relations, we built the distributional relations using entire documents as contexts (i.e. each dimension in the word space corresponds to a document in the data). If we would have used words as contexts instead, we would have ended up a paradigmatic word relations hold between co-occurring words, while paradigmatic relations hold between words that do not but that occur with the same Sahlgren (2006) for an explanation of how the choice of contexts determines the semantic content of the word space. 297 6 Compositionality and semantic relations The relations between words in headlines were modeled using the most simple operation conceivable: we simply add all words’ context vectors to a comheadline vector and use that as the representation of the headline: where is a test headline, and is a word in this headline. This is obviously a daring, if not foolhardy, approach to modelling syntactic structure, compositional semantics, and all types of intra-sentential semantic dependencies. It can fairly be expected to be improved upon through an appropriate finer-grained analysis of word presence, adjacency and syntactic relationships. However, this approach is similar to that taken by most search engines in use today, is a useful first baseline, and as can be seen from our results below, does deliver acceptable results. 7 Valence annotation To perform the valence annotation, we first lemmatized the headlines and removed stop words and words with frequency above 10 000 in the LA times corpus. For each headline, we then summed — as discussed above — the context vectors of the remaining words, thus producing a 220 220dimensional vector for each headline. This vector was then compared to each of the postulated valence vectors by computing the cosine of the angles between the vectors. We thus have for each headline two cosines, one between the headline and the positive vector and one between the headline and the negative vector. The valence vector with highest cosine score (and thus the smallest spatial angle) was chosen to annotate the headline. For the negative valence vector we assigned a negative valence value, and for the positive vector a positive value. In 11 cases, a value of was ascribed, either because all headline words were removed by frequency and stop word filtering, or because none of the remaining words occurred in our newsprint corpus. Our method thus only delivers a binary valence decision — either positive or negative valence. Granted, we could have assigned a neutral valence to very low cosine scores, but as any threshold for deciding on a neutral score would be completely arbitrary, we decided to only give fully positive or negative scores to the test headlines. Also, since our aim was to provide a high-recall result, we did not wish to leave any headline with an equivocal score. We scaled the scores to fit the requirements of the coarse-grained evaluation: for each headline with a non-zero score, we multiplied the value with boosted each value with By this scaling operation we guaranteed a positive or a negative score for each headline (apart from the 11 exceptions, in effect unanalyzed by our algorithm, as mentioned above). 8 Results The results from the fine-grained and coarse-grained evaluations are shown in Table 2. They show, much as we anticipated, that the coarse-grained evaluation was appropriate for our purposes. Fine-grained Coarse-grained Recall Accuracy Precision 20.68 29.00 28.41 60.17 Table 2: The results of the valence annotation. 8.1 Correlation coefficients, normality assumptions, and validity of results The fine-grained evaluation as given by the organisers and as shown in Table 2 was computed using Pearson’s product-moment coefficient. Pearson’s correlation coefficient is a parametric statistic and assumes normal distribution of the data it is testing for correlation. While we have no idea of neither the other contributions’ score distribution, nor that of the given test set, we certainly do know that our data are not normally distributed. We would much prefer to evaluate our results using a non-parametric correlation test, such as Spearman’s , and suggest that the all results would be rescored using some non-parametric method instead — this would reduce the risk of inadvertent false positives stemming from divergence from the normal distribution rather than divergence from the test set. coarse-grained evaluation collapsed values in the ranges as negative, as neutral, and as positive. 298 8.2 Use cases Evaluation of abstract features such as emotional valence can be done within a system oriented framework such as the one used in this experiment. Alternatively, one could evaluate the results using a parametrized use case scenario. A simple example might be to aim for either high recall or high precision, rather than using an average which folds in both scenarios into one numeric score — easy to compare between systems but dubious in its relevance to any imaginable real life task. There are metrics, as formal as the simple recallprecision-framework in traditional adhoc retrieval, that could be adapted for this purpose (J¨arvelin and Kek¨al¨ainen, 2002, e.g.). 9 Related research Our approach to valence annotation is similar to the second method described by Turney and Littman (2003). In short, their method uses singular value decomposition to produce a reduceddimensional word space, in which word valence is computed by subtracting the cosine between the word and a set of negative seed words from the cosine between the word and a set of positive seed words. The difference between our approach and theirs is that our approach does not require any computationally expensive matrix decomposition, as we do not see any reason to restructure our word space. Turney and Littman (2003) hypothesize that singular value decomposition is beneficial for the results in valency annotation because it infers paradigmatic relations between words in the reduced space. However, as we argued in Section 5, we believe that the headline valency annotation task calls for syntagmatic rather than paradigmatic relations. Furthermore, we fail to see the motivation for using singular value decomposition, since if paradigmatic relations are what is needed, then why not simply use words as dimensions of the word space? 10 Concluding remarks Our results show that a resource-poor but data-rich method can deliver sensible results. This is in keeping with our overall approach, which aims for as little pre-computed resources as possible. At almost every juncture in our processing we made risky and simplistic assumptions — using simple frequencies of word occurrence as a semantic model; using a small seed set of positive and negative terms as a target; postulating one semantic locus each for positive and negative emotion; modelling syntactic and semantic relations between terms by vector addition — and yet we find that the semantic structure of distributional statistics yields a signal good enough for distinguishing positive from negative headlines with a non-random accuracy. Despite its simplicity, out method produces very good recall (60.17) in the coarse-grained evaluation (the median recall for all systems is 29.59). This speaks to the power of distributional semantics and gives promise of improvement if some of the choice points during the process are returned to: some decisions can well benefit from being made on principled and informed grounds rather than searching under the street lamp, as it were. References Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumugain-based evaluation of IR techniques. on Information 20(4):422–446. Sahlgren. 2006. Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highvector PhD Dissertation, Depart-</abstract>
<affiliation confidence="0.867757">ment of Linguistics, Stockholm University.</affiliation>
<address confidence="0.377774">Sch¨utze. 1993. Word space. In</address>
<note confidence="0.825403125">of the 1993 Conference on Advances in Neural Infor- Processing Systems, pages 895–902, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Carlo Strapparava and Alessandro Valitutti. 2004. Wordnet-affect: an affective extension of wordnet. In Proceedings of the 4th International Conference on Resources and Evaluation, pages</note>
<abstract confidence="0.759979625">1083–1086. Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orienfrom association. Transactions on Infor- 21(4):315–346. Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classifiof reviews. In of the 40th Confer-</abstract>
<affiliation confidence="0.615638">ence of the Association for Computational Linguistics,</affiliation>
<address confidence="0.461012">pages 417–424. 299</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>PhD Dissertation,</tech>
<institution>Department of Linguistics, Stockholm University.</institution>
<contexts>
<context position="2041" citStr="Sahlgren, 2006" startWordPosition="336" endWordPosition="337">on Our approach takes as its starting point the observation that lexical resources always are noisy, out of date, and most often suffer simultaneously from being both too specific and too general. For our experiments, our only lexical resource consists of a list of eight positive words and eight negative words, as shown below in Table 1. We use a medium-sized corpus of general newsprint to build a general word space, and use our minimal lexical resource to orient ourselves in it. 3 Word space A word space is a high-dimensional vector space built from distributional statistics (Sch¨utze, 1993; Sahlgren, 2006), in which each word in the vocabulary is represented as a context vector of occurrence frequencies: where is the frequency of word in some context . The point of this representation is that semantic similarity between words can be computed using vector similarity measures. Thus, the similarity in meaning between the words and can be quantified by computing the similarity between their respective context vectors: sim sim . The semantics of such a word space are determined by the data from which the occurrence information has been collected. Since the data set in the SemEval Affective Text task</context>
<context position="6894" citStr="Sahlgren (2006)" startWordPosition="1125" endWordPosition="1126">ch often co-occur with general negative terms such as “war” and “dangerous” in newswire text. To produce a word space that contains predominantely syntagmatic relations, we built the distributional relations using entire documents as contexts (i.e. each dimension in the word space corresponds to a document in the data). If we would have used words as contexts instead, we would have ended up with a paradigmatic word space.4 3Syntagmatic relations hold between co-occurring words, while paradigmatic relations hold between words that do not co-occur, but that occur with the same other words. 4See Sahlgren (2006) for an explanation of how the choice of contexts determines the semantic content of the word space. 297 6 Compositionality and semantic relations The relations between words in headlines were modeled using the most simple operation conceivable: we simply add all words’ context vectors to a compound headline vector and use that as the representation of the headline: where is a test headline, and is a word in this headline. This is obviously a daring, if not foolhardy, approach to modelling syntactic structure, compositional semantics, and all types of intra-sentential semantic dependencies. It</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. PhD Dissertation, Department of Linguistics, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Word space.</title>
<date>1993</date>
<booktitle>In Proceedings of the 1993 Conference on Advances in Neural Information Processing Systems, NIPS’93,</booktitle>
<pages>895--902</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Sch¨utze, 1993</marker>
<rawString>Hinrich Sch¨utze. 1993. Word space. In Proceedings of the 1993 Conference on Advances in Neural Information Processing Systems, NIPS’93, pages 895–902, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alessandro Valitutti</author>
</authors>
<title>Wordnet-affect: an affective extension of wordnet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC’04,</booktitle>
<pages>1083--1086</pages>
<contexts>
<context position="4401" citStr="Strapparava and Valitutti, 2004" startWordPosition="720" endWordPosition="723">hting of frequencies, resulting in a 220 220-dimensional word space. No dimensionality reduction was used. 4 Seeds In order to construct valence vectors, we used a set of manually selected seed words (8 positive and 8 negative words), shown in Table 1. These words were chosen (subjectively) to represent typical expression of positive or negative attitude in news texts. The size of the seed set was determined by a number of initial experiments on the development data, where we varied the size of the seed sets from these 8 words to some 700 words in each set (using the WordNet Affect hierarchy (Strapparava and Valitutti, 2004)). As comparison, Turney and Littman (2003) used seed sets consisting of 7 words in their word valence annotation experiments, while Turney (2002) used minimal seed sets consisting of only one positive and one negative word (“excellent” and “poor”) in his experiments on review classification. Such minimal seed sets of antonym pairs are not possible to use in the present experiment because they are often nearest neighbors to each other in the word space. Also, it is difficult to find such clear paradigm words for the newswire domain. The seed words were used to postulate one positive and one ne</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>Carlo Strapparava and Alessandro Valitutti. 2004. Wordnet-affect: an affective extension of wordnet. In Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC’04, pages 1083–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4444" citStr="Turney and Littman (2003)" startWordPosition="726" endWordPosition="729">sional word space. No dimensionality reduction was used. 4 Seeds In order to construct valence vectors, we used a set of manually selected seed words (8 positive and 8 negative words), shown in Table 1. These words were chosen (subjectively) to represent typical expression of positive or negative attitude in news texts. The size of the seed set was determined by a number of initial experiments on the development data, where we varied the size of the seed sets from these 8 words to some 700 words in each set (using the WordNet Affect hierarchy (Strapparava and Valitutti, 2004)). As comparison, Turney and Littman (2003) used seed sets consisting of 7 words in their word valence annotation experiments, while Turney (2002) used minimal seed sets consisting of only one positive and one negative word (“excellent” and “poor”) in his experiments on review classification. Such minimal seed sets of antonym pairs are not possible to use in the present experiment because they are often nearest neighbors to each other in the word space. Also, it is difficult to find such clear paradigm words for the newswire domain. The seed words were used to postulate one positive and one negative point (i.e. vector) in the word spac</context>
<context position="11816" citStr="Turney and Littman (2003)" startWordPosition="1925" endWordPosition="1928">uld evaluate the results using a parametrized use case scenario. A simple example might be to aim for either high recall or high precision, rather than using an average which folds in both scenarios into one numeric score — easy to compare between systems but dubious in its relevance to any imaginable real life task. There are metrics, as formal as the simple recallprecision-framework in traditional adhoc retrieval, that could be adapted for this purpose (J¨arvelin and Kek¨al¨ainen, 2002, e.g.). 9 Related research Our approach to valence annotation is similar to the second method described by Turney and Littman (2003). In short, their method uses singular value decomposition to produce a reduceddimensional word space, in which word valence is computed by subtracting the cosine between the word and a set of negative seed words from the cosine between the word and a set of positive seed words. The difference between our approach and theirs is that our approach does not require any computationally expensive matrix decomposition, as we do not see any reason to restructure our word space. Turney and Littman (2003) hypothesize that singular value decomposition is beneficial for the results in valency annotation </context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Conference of the Association for Computational Linguistics, ACL’02,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="4547" citStr="Turney (2002)" startWordPosition="744" endWordPosition="745">et of manually selected seed words (8 positive and 8 negative words), shown in Table 1. These words were chosen (subjectively) to represent typical expression of positive or negative attitude in news texts. The size of the seed set was determined by a number of initial experiments on the development data, where we varied the size of the seed sets from these 8 words to some 700 words in each set (using the WordNet Affect hierarchy (Strapparava and Valitutti, 2004)). As comparison, Turney and Littman (2003) used seed sets consisting of 7 words in their word valence annotation experiments, while Turney (2002) used minimal seed sets consisting of only one positive and one negative word (“excellent” and “poor”) in his experiments on review classification. Such minimal seed sets of antonym pairs are not possible to use in the present experiment because they are often nearest neighbors to each other in the word space. Also, it is difficult to find such clear paradigm words for the newswire domain. The seed words were used to postulate one positive and one negative point (i.e. vector) in the word space by simply taking the centroid of the seed word points: where is one of the seed sets, and is a word i</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Conference of the Association for Computational Linguistics, ACL’02, pages 417–424.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>