<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012249">
<title confidence="0.982123">
Speaking without knowing what to say... or when to end
</title>
<author confidence="0.953304">
Anna Hjalmarsson
</author>
<affiliation confidence="0.924442">
Centre for Speech Technology
</affiliation>
<note confidence="0.928373">
KTH
SE-10044, Stockholm, Sweden
</note>
<email confidence="0.753695">
annah@speech.kth.se
</email>
<sectionHeader confidence="0.997009" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999428466666667">
Humans produce speech incrementally and
on-line as the dialogue progresses using in-
formation from several different sources in
parallel. A dialogue system that generates
output in a stepwise manner and not in pre-
planned syntactically correct sentences needs
to signal how new dialogue contributions re-
late to previous discourse. This paper de-
scribes a data collection which is the
foundation for an effort towards more human-
like language generation in DEAL, a spoken
dialogue system developed at KTH. Two an-
notators labelled cue phrases in the corpus
with high inter-annotator agreement (kappa
coefficient 0.82).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954">
This paper describes a data collection with the goal
of modelling more human-like language generation
in DEAL, a spoken dialogue system developed at
KTH. The DEAL objectives are to build a system
which is fun, human-like, and engaging to talk to,
and which gives second language learners of
Swedish conversation training (as described in
Hjalmarsson et al., 2007). The scene of DEAL is
set at a flea market where a talking animated agent
is the owner of a shop selling used objects. The
student is given a mission: to buy items from the
shop-keeper at the best possible price by bargain-
ing. From a language learning perspective and to
keep the students motivated, the agent’s language
is crucial. The agent needs to behave human-like in
a way which allows the users to suspend some of
their disbeliefs and talk to DEAL as if talking to
</bodyText>
<page confidence="0.974395">
72
</page>
<bodyText confidence="0.9998625">
another human being. In an experimental study
(Hjalmarsson &amp; Edlund, in press), where a spoken
dialogue system with human behaviour was simu-
lated, two different systems were compared: a rep-
lica of human behaviour and a constrained version
with less variability. The version based on human
behaviour was rated as more human-like, polite
and intelligent.
</bodyText>
<subsectionHeader confidence="0.999681">
1.1 Human language production
</subsectionHeader>
<bodyText confidence="0.980206481481482">
Humans produce speech incrementally and on-line
as the dialogue progresses using information from
several different sources in parallel (Brennan,
2000; Aist et al., 2006). We anticipate what the
other person is about to say in advance and start
planning our next move while this person is still
speaking. When starting to speak, we typically do
not have a complete plan of how to say something
or even what to say. Yet, we manage to rapidly
integrate information from different sources in par-
allel and simultaneously plan and realize new dia-
logue contributions. Pauses, corrections and
repetitions are used to stepwise refine, alter and
revise our plans as we speak (Clark &amp; Wasow,
1998). These human behaviours bring valuable
information that contains more than the literal
meanings of the words (Arnold et al., 2003).
In order to generate output incrementally in
DEAL we need extended knowledge on how to
signal relations between different segments of
speech. In this paper we report on a data collection
of human-human dialogue aiming at extending the
knowledge of human interaction and in particular
to distinguish different types of cue phrases used in
the DEAL domain.
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 72–75,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.986703" genericHeader="introduction">
2 The DEAL corpus collection
</sectionHeader>
<bodyText confidence="0.9987855">
The dialogue data recorded was informal, human-
human, face-to-face conversation. The task and the
recording environment were set up to mimic the
DEAL domain and role play.
</bodyText>
<subsectionHeader confidence="0.997536">
2.1 Data collection
</subsectionHeader>
<bodyText confidence="0.999990678571429">
The data collection was made with 6 subjects (4
male and 2 female), 2 posing as shop keepers and 4
as potential buyers. Each customer interacted with
the same shop-keeper twice, in two different sce-
narios. The shop-keepers and customers were in-
structed separately. The customers were given a
mission: to buy items at a flea market at the best
possible price from the shop-keeper. The task was
to buy 3 objects for a specific purpose (e.g. to buy
tools to repair a house). The customers were given
a certain amount of toy money, however not
enough to buy what they were instructed to buy
without bargaining. The shop-keeper sat behind a
desk with images of different objects pinned to the
wall behind him. Some of the object had obvious
flaws, for example a puzzle with a missing piece,
to open up for interesting negotiation. None of the
shop-keepers had any professional experience of
bargaining, which was appropriate since we were
more interested in capturing naïve conceptual
metaphors of bargaining rather than real life price
negotiation. Each dialogue was about 15 minutes
long, so about 2 hours of speech were collected
altogether. The shop-keepers used an average of
13.4 words per speaker turn while the buyers’ turns
were generally shorter, 8.5 words per turn (in this
paper turn always refers to speaker turns). In total
16357 words were collected.
</bodyText>
<sectionHeader confidence="0.992167" genericHeader="method">
3 Annotation
</sectionHeader>
<bodyText confidence="0.99998">
All dialogues were first transcribed orthographi-
cally including non-lexical entities such as laughter
and hawks. Filled pauses, repetitions, corrections
and restarts were also labelled manually.
</bodyText>
<subsectionHeader confidence="0.999831">
3.1 Cue phrases
</subsectionHeader>
<bodyText confidence="0.999513694444444">
Linguistic devices used to signal relations between
different segments of speech are often referred to
as cue phrases. Other frequently used terms are
discourse markers, pragmatic markers or discourse
particles. Typical cue phrases in English are: oh,
well, now, then, however, you know, I mean, be-
cause, and, but and or. Much research within dis-
course analysis, communicative analysis and
psycholinguistics has been concerned with these
connectives and what kind of relations they hold
(for an overview see Schourup, 1999). Our defini-
tion of cue phrases is broad and all types of lin-
guistic entities that the speakers use to hold the
dialogue together at different communicative lev-
els are included. A rule of thumb is that cue
phrases are words or chunks of words that have
little lexical impact at the local speech segment
level but serve significant pragmatic function. To
give an exact definition of what cue phrases are is
difficult, as these entities often are ambiguous. Ac-
cording to the definition used here, cue phrases can
be a single word or larger units, occupy various
positions, belong to different syntactic classes, and
be realized with different prosodic contours.
The first dialogue was analyzed and used
to decide which classes to use in the annotation
scheme. Nine of the classes were a subset of the
functional classification scheme of discourse
markers presented in Lindström (2008). A tenth
class, referring, was added. There were 3 different
classes for connectives, 3 classes for responsives
and 4 remaining classes. The classes are presented
in Table 1; the first row contains an example in its
context from data, the word(s) in bold are the la-
belled cue phrase, and the second row presents fre-
quently used instances of that class.
</bodyText>
<table confidence="0.992632288888889">
Additive Connectives (CAD)
och grönt är ju fint
[and green is nice]
och, allts$, s$
[and, therefore, so]
Contrastive Connectives (CC)
men den är ganska antik
[but it is pretty antique]
men, fast, allts$
[but, although, thus]
Alternative Connectives (CAL)
som jag kan titta p$ istiillet
[which I can look at instead]
eller, istället [or, instead]
Responsive (R)
ja jag tycker ju det
[yeah I actually think so]
ja, mm, jaha, ok
[yes, mm, yeah, ok]
Responsive New Information (RNI)
jaha har du n$gra s$dana
[right do you have any of those]
jaha, ok, ja, mm
[right, ok, yes, mm]
73
Responsive Disprefrence (RD)
ja men det är klart dom funkar
[yeah but of course they work]
ja, mm, jo [yes, mm, sure]
Response Eliciting (RE)
vad ska du ha för den då
[how much do you want for that one then]
då, eller hur [then, right]
Repair Correction (RC)
nej nu sa jag fel
[no now I said wrong]
nej, jag menade [no, I meant]
Modifying (MOD)
ja jag tycker ju det
[yeah I actually think so]
ju, liksom, jag tycker ju det [of course, so to speak, I like]
Referring (REF)
fyra hundra kronor sa vi
[four hundred crowns we said]
sa vi, sa vi inte det [we said, wasn’t that what we said]
</table>
<tableCaption confidence="0.999881">
Table 1: The DEAL annotation scheme
</tableCaption>
<bodyText confidence="0.9996384">
The labelling of cue phrases included a two-fold
task, both to decide if a word was a cue phrase or
not – a binary task – but also to classify which
functional class it belongs to according to the an-
notation scheme. The annotators could both see the
transcriptions and listen to the recordings while
labelling. 81% of the speaker turns contained at
least one cue phrase and 21% of all words were
labelled as cue phrases. Table 2 presents the distri-
bution of cue phrases over the different classes.
</bodyText>
<figure confidence="0.93044">
30%
15%
0%
MOD R CAD CC RD RNI RE REF RC CAL
</figure>
<tableCaption confidence="0.987213">
Table 2: Cue phrase distribution over the different classes
</tableCaption>
<bodyText confidence="0.9997876">
Two of the eight dialogues were annotated by two
different annotators. A kappa coefficient was cal-
culated on word level. The kappa coefficient for
the binary task, to classify if a word was a cue
phrase or not, was 0.87 (p=0.05). The kappa coef-
ficient for the classification task was 0.82 (p=0.05).
Three of the classes, referring, connective alterna-
tive and repair correction, had very few instances.
The agreement in percentage distributed over the
different classes is presented in Table 3.
</bodyText>
<figure confidence="0.9971265">
100%
80%
60%
40%
20%
0%
</figure>
<tableCaption confidence="0.997453">
Table 3: % agreement for the different classes
</tableCaption>
<sectionHeader confidence="0.902759" genericHeader="method">
4 Data analysis
</sectionHeader>
<bodyText confidence="0.999763611111111">
To separate cue phrases from other lexical entities
and to determine what they signal is a complex
task. The DEAL corpus is rich in disfluencies and
cue phrases; 86% of the speaker turns contained at
least one cue phrase or disfluency. The annotators
had access to the context and were allowed to lis-
ten to the recordings while labelling. The respon-
sives were generally single words or non lexical
units (e.g. “mm”) and appeared in similar dialogue
contexts (i.e. as responses to assertions). The clas-
sification is likely based on their prosodic realiza-
tion. Acoustic analysis is needed in order to see if
and how they differ in prosodic contour. In
Hirschberg &amp; Litman (1993) prosodic analysis is
used to distinguish between discourse and senten-
tial use of cue phrases. Table 4 presents how the
different cue phrases were distributed over speaker
turns, at initial, middle or end position.
</bodyText>
<table confidence="0.6098497">
100%
80%
60%
40%
20%
0%
end
middle
initial
MOD
</table>
<sectionHeader confidence="0.9543606" genericHeader="method">
R
CAD
CC
RD
RNI
RE
REF
RC
CAL
All
</sectionHeader>
<tableCaption confidence="0.990712">
Table 4: Turn position distribution
</tableCaption>
<sectionHeader confidence="0.988934" genericHeader="method">
5 Generation in DEAL
</sectionHeader>
<bodyText confidence="0.99962675">
The collected and labelled data is a valuable re-
source of information for what cue phrases signal
in the DEAL domain as well as how they are lexi-
cally and prosodically realized. To keep the re-
</bodyText>
<figure confidence="0.9969263">
MOD
R
CAD
CC
RD
RNI
RE
REF
RC
CAL
</figure>
<page confidence="0.996093">
74
</page>
<bodyText confidence="0.999987282051282">
sponse times constant and without unnaturally long
delays, DEAL needs to be capable of grabbing the
turn, hold it while the system is producing the rest
of the message, and release it after completion.
DEAL is implemented using components from the
Higgins project (Skantze et al., 2006) an off-the-
shelf ASR system and a GUI with an embodied
conversational agent (ECA) (Beskow, 2003). A
current research challenge is to redesign the mod-
ules and architecture for incremental processing, to
allow generation of conversational speech. Deep
generation in DEAL – the decision of what to say
on an abstract semantic level – is distributed over
three different modules; (1) the action manger, (2)
the agent manager and the (3) communicative
manager. The action manger is responsible for ac-
tions related to user input and previous discourse1.
The agent manager represents the agents’ personal
motivations and personality. DEAL uses mixed
initiative and the agent manager takes initiatives. It
may for example try to promote certain objects or
suggest prices of objects in focus. It also generates
emotional facial gestures related to events in the
dialogue. The communicative manager generates
responses on a communicative level based on shal-
low analysis of input. For example, it initiates re-
quests for confirmations if speech recognition
confidence scores are low. This module initiates
utterances when the user yields the floor, regard-
less of whether the system has a complete plan of
what to say or not. Using similar strategies as the
subjects recorded here, the dialogue system can
grab the turn and start to say something before
having completed processing input. Many cue
phrases were used in combination, signalling func-
tion on different discourse levels; first a simple
responsive, saying that the previous message was
perceived, and then some type of connective to
signal how the new contribution relates.
</bodyText>
<sectionHeader confidence="0.997645" genericHeader="conclusions">
6 Final remarks
</sectionHeader>
<bodyText confidence="0.999614714285714">
Since DEAL focuses on generation in role play, we
are less interested in the ambiguous cue phrases
and more concerned with the instances where the
annotators agreed. The DEAL users are second
language learners with poor knowledge in Swed-
ish, and it may even be advisable that the agent’s
behaviour is exaggerated.
</bodyText>
<note confidence="0.2403675">
1 For more details on the discourse modeller see Skantze et al,
2006.
</note>
<sectionHeader confidence="0.996277" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99915725">
This research was carried out at Centre for Speech
Technology, KTH. The research is also supported
by the Swedish research council project #2007-
6431, GENDIAL and the Graduate School for Lan-
guage Technology (GSLT). Many thanks to Jenny
Klarenfjord for help on data collection and annota-
tion and thanks to Rolf Carlson, Preben Wik and
Jens Edlund for valuable comments.
</bodyText>
<sectionHeader confidence="0.999155" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862054054054">
G. Aist, J. Allen, E. Campana, L. Galescu, C. A. Gómez
Gallo, S. Stoness, M. Swift, and M. Tanenhaus.
2006. Software Architectures for Incremental Under-
standing of Human Speech. In Proc. of Interspeech.
J. Arnold, M. Fagano, and M. Tanenhaus. 2003. Disflu-
encies signal theee, um, new information. Journal of
Psycholinguistic Research, 32, 25-36.
J. Beskow. 2003. Talking heads - Models and applica-
tions for multimodal speech synthesis. Doctoral dis-
sertation, KTH.
S. Brennan. 2000. Processes that shape conversation and
their implications for computational. In Proc. of the
38th Annual Meeting of the ACL.
H. Clark, and T. Wasow. 1998. Repeating words in
spontaneous speech. Cognitive Psychology, 37(3),
201-242.
J. Hirschberg, and D. Litman. 1993. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3), 501-530.
A. Hjalmarsson, and J. Edlund. In press. Human-
likeness in utterance generation: effects of variability.
To be published in Proc. of the 4th IEEE Workshop
on Perception and Interactive Technologies for
Speech-Based Systems. Kloster Irsee, Germany.
A. Hjalmarsson, P. Wik, and J. Brusk. 2007. Dealing
with DEAL: a dialogue system for conversation
training. In Proc. of SigDial. Antwerp, Belgium.
J. Lindström. 2008. Diskursmarkörer. In Tur och
ordning; introduktion till svensk samtalsgrammatik
(pp. 56-104). Norstedts Akademiska Förlag. Stock-
holm, Sweden.
L. Schourup. 1999. Discourse markers. Lingua, 107(3-
4), 227-265.
G. Skantze, J. Edlund, and R. Carlson. 2006. Talking
with Higgins: Research challenges in a spoken dia-
logue system. In Perception and Interactive Tech-
nologies (pp. 193-196). Berlin/Heidelberg: Springer.
</reference>
<page confidence="0.999133">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903389">
<title confidence="0.988682">Speaking without knowing what to say... or when to end</title>
<author confidence="0.999153">Anna Hjalmarsson</author>
<affiliation confidence="0.999095">Centre for Speech</affiliation>
<address confidence="0.989445">SE-10044, Stockholm,</address>
<email confidence="0.994412">annah@speech.kth.se</email>
<abstract confidence="0.995512">Humans produce speech incrementally and on-line as the dialogue progresses using information from several different sources in parallel. A dialogue system that generates output in a stepwise manner and not in preplanned syntactically correct sentences needs to signal how new dialogue contributions relate to previous discourse. This paper describes a data collection which is the foundation for an effort towards more humanlike language generation in DEAL, a spoken dialogue system developed at KTH. Two annotators labelled cue phrases in the corpus with high inter-annotator agreement (kappa coefficient 0.82).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Allen</author>
<author>E Campana</author>
<author>L Galescu</author>
<author>C A Gómez Gallo</author>
<author>S Stoness</author>
<author>M Swift</author>
<author>M Tanenhaus</author>
</authors>
<title>Software Architectures for Incremental Understanding of Human Speech.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="2182" citStr="Aist et al., 2006" startWordPosition="343" endWordPosition="346">nd some of their disbeliefs and talk to DEAL as if talking to 72 another human being. In an experimental study (Hjalmarsson &amp; Edlund, in press), where a spoken dialogue system with human behaviour was simulated, two different systems were compared: a replica of human behaviour and a constrained version with less variability. The version based on human behaviour was rated as more human-like, polite and intelligent. 1.1 Human language production Humans produce speech incrementally and on-line as the dialogue progresses using information from several different sources in parallel (Brennan, 2000; Aist et al., 2006). We anticipate what the other person is about to say in advance and start planning our next move while this person is still speaking. When starting to speak, we typically do not have a complete plan of how to say something or even what to say. Yet, we manage to rapidly integrate information from different sources in parallel and simultaneously plan and realize new dialogue contributions. Pauses, corrections and repetitions are used to stepwise refine, alter and revise our plans as we speak (Clark &amp; Wasow, 1998). These human behaviours bring valuable information that contains more than the lit</context>
</contexts>
<marker>Aist, Allen, Campana, Galescu, Gallo, Stoness, Swift, Tanenhaus, 2006</marker>
<rawString>G. Aist, J. Allen, E. Campana, L. Galescu, C. A. Gómez Gallo, S. Stoness, M. Swift, and M. Tanenhaus. 2006. Software Architectures for Incremental Understanding of Human Speech. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Arnold</author>
<author>M Fagano</author>
<author>M Tanenhaus</author>
</authors>
<title>Disfluencies signal theee, um, new information.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>32</volume>
<pages>25--36</pages>
<contexts>
<context position="2830" citStr="Arnold et al., 2003" startWordPosition="452" endWordPosition="455">er person is about to say in advance and start planning our next move while this person is still speaking. When starting to speak, we typically do not have a complete plan of how to say something or even what to say. Yet, we manage to rapidly integrate information from different sources in parallel and simultaneously plan and realize new dialogue contributions. Pauses, corrections and repetitions are used to stepwise refine, alter and revise our plans as we speak (Clark &amp; Wasow, 1998). These human behaviours bring valuable information that contains more than the literal meanings of the words (Arnold et al., 2003). In order to generate output incrementally in DEAL we need extended knowledge on how to signal relations between different segments of speech. In this paper we report on a data collection of human-human dialogue aiming at extending the knowledge of human interaction and in particular to distinguish different types of cue phrases used in the DEAL domain. Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 72–75, Columbus, June 2008. c�2008 Association for Computational Linguistics 2 The DEAL corpus collection The dialogue data recorded was informal, humanhuman, face-to-fac</context>
</contexts>
<marker>Arnold, Fagano, Tanenhaus, 2003</marker>
<rawString>J. Arnold, M. Fagano, and M. Tanenhaus. 2003. Disfluencies signal theee, um, new information. Journal of Psycholinguistic Research, 32, 25-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Beskow</author>
</authors>
<title>Talking heads - Models and applications for multimodal speech synthesis.</title>
<date>2003</date>
<note>Doctoral dissertation, KTH.</note>
<contexts>
<context position="10924" citStr="Beskow, 2003" startWordPosition="1833" endWordPosition="1834">L The collected and labelled data is a valuable resource of information for what cue phrases signal in the DEAL domain as well as how they are lexically and prosodically realized. To keep the reMOD R CAD CC RD RNI RE REF RC CAL 74 sponse times constant and without unnaturally long delays, DEAL needs to be capable of grabbing the turn, hold it while the system is producing the rest of the message, and release it after completion. DEAL is implemented using components from the Higgins project (Skantze et al., 2006) an off-theshelf ASR system and a GUI with an embodied conversational agent (ECA) (Beskow, 2003). A current research challenge is to redesign the modules and architecture for incremental processing, to allow generation of conversational speech. Deep generation in DEAL – the decision of what to say on an abstract semantic level – is distributed over three different modules; (1) the action manger, (2) the agent manager and the (3) communicative manager. The action manger is responsible for actions related to user input and previous discourse1. The agent manager represents the agents’ personal motivations and personality. DEAL uses mixed initiative and the agent manager takes initiatives. I</context>
</contexts>
<marker>Beskow, 2003</marker>
<rawString>J. Beskow. 2003. Talking heads - Models and applications for multimodal speech synthesis. Doctoral dissertation, KTH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
</authors>
<title>Processes that shape conversation and their implications for computational.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2162" citStr="Brennan, 2000" startWordPosition="341" endWordPosition="342"> users to suspend some of their disbeliefs and talk to DEAL as if talking to 72 another human being. In an experimental study (Hjalmarsson &amp; Edlund, in press), where a spoken dialogue system with human behaviour was simulated, two different systems were compared: a replica of human behaviour and a constrained version with less variability. The version based on human behaviour was rated as more human-like, polite and intelligent. 1.1 Human language production Humans produce speech incrementally and on-line as the dialogue progresses using information from several different sources in parallel (Brennan, 2000; Aist et al., 2006). We anticipate what the other person is about to say in advance and start planning our next move while this person is still speaking. When starting to speak, we typically do not have a complete plan of how to say something or even what to say. Yet, we manage to rapidly integrate information from different sources in parallel and simultaneously plan and realize new dialogue contributions. Pauses, corrections and repetitions are used to stepwise refine, alter and revise our plans as we speak (Clark &amp; Wasow, 1998). These human behaviours bring valuable information that contai</context>
</contexts>
<marker>Brennan, 2000</marker>
<rawString>S. Brennan. 2000. Processes that shape conversation and their implications for computational. In Proc. of the 38th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Clark</author>
<author>T Wasow</author>
</authors>
<title>Repeating words in spontaneous speech.</title>
<date>1998</date>
<journal>Cognitive Psychology,</journal>
<volume>37</volume>
<issue>3</issue>
<pages>201--242</pages>
<contexts>
<context position="2699" citStr="Clark &amp; Wasow, 1998" startWordPosition="432" endWordPosition="435">gresses using information from several different sources in parallel (Brennan, 2000; Aist et al., 2006). We anticipate what the other person is about to say in advance and start planning our next move while this person is still speaking. When starting to speak, we typically do not have a complete plan of how to say something or even what to say. Yet, we manage to rapidly integrate information from different sources in parallel and simultaneously plan and realize new dialogue contributions. Pauses, corrections and repetitions are used to stepwise refine, alter and revise our plans as we speak (Clark &amp; Wasow, 1998). These human behaviours bring valuable information that contains more than the literal meanings of the words (Arnold et al., 2003). In order to generate output incrementally in DEAL we need extended knowledge on how to signal relations between different segments of speech. In this paper we report on a data collection of human-human dialogue aiming at extending the knowledge of human interaction and in particular to distinguish different types of cue phrases used in the DEAL domain. Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 72–75, Columbus, June 2008. c�2008 Asso</context>
</contexts>
<marker>Clark, Wasow, 1998</marker>
<rawString>H. Clark, and T. Wasow. 1998. Repeating words in spontaneous speech. Cognitive Psychology, 37(3), 201-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>501--530</pages>
<contexts>
<context position="9960" citStr="Hirschberg &amp; Litman (1993)" startWordPosition="1658" endWordPosition="1661">s and to determine what they signal is a complex task. The DEAL corpus is rich in disfluencies and cue phrases; 86% of the speaker turns contained at least one cue phrase or disfluency. The annotators had access to the context and were allowed to listen to the recordings while labelling. The responsives were generally single words or non lexical units (e.g. “mm”) and appeared in similar dialogue contexts (i.e. as responses to assertions). The classification is likely based on their prosodic realization. Acoustic analysis is needed in order to see if and how they differ in prosodic contour. In Hirschberg &amp; Litman (1993) prosodic analysis is used to distinguish between discourse and sentential use of cue phrases. Table 4 presents how the different cue phrases were distributed over speaker turns, at initial, middle or end position. 100% 80% 60% 40% 20% 0% end middle initial MOD R CAD CC RD RNI RE REF RC CAL All Table 4: Turn position distribution 5 Generation in DEAL The collected and labelled data is a valuable resource of information for what cue phrases signal in the DEAL domain as well as how they are lexically and prosodically realized. To keep the reMOD R CAD CC RD RNI RE REF RC CAL 74 sponse times const</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>J. Hirschberg, and D. Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3), 501-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hjalmarsson</author>
<author>J Edlund</author>
</authors>
<title>In press. Humanlikeness in utterance generation: effects of variability. To be published in</title>
<date></date>
<booktitle>Proc. of the 4th IEEE Workshop on Perception and Interactive Technologies</booktitle>
<marker>Hjalmarsson, Edlund, </marker>
<rawString>A. Hjalmarsson, and J. Edlund. In press. Humanlikeness in utterance generation: effects of variability. To be published in Proc. of the 4th IEEE Workshop on Perception and Interactive Technologies for Speech-Based Systems. Kloster Irsee, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hjalmarsson</author>
<author>P Wik</author>
<author>J Brusk</author>
</authors>
<title>Dealing with DEAL: a dialogue system for conversation training.</title>
<date>2007</date>
<booktitle>In Proc. of SigDial.</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="1153" citStr="Hjalmarsson et al., 2007" startWordPosition="172" endWordPosition="175">n which is the foundation for an effort towards more humanlike language generation in DEAL, a spoken dialogue system developed at KTH. Two annotators labelled cue phrases in the corpus with high inter-annotator agreement (kappa coefficient 0.82). 1 Introduction This paper describes a data collection with the goal of modelling more human-like language generation in DEAL, a spoken dialogue system developed at KTH. The DEAL objectives are to build a system which is fun, human-like, and engaging to talk to, and which gives second language learners of Swedish conversation training (as described in Hjalmarsson et al., 2007). The scene of DEAL is set at a flea market where a talking animated agent is the owner of a shop selling used objects. The student is given a mission: to buy items from the shop-keeper at the best possible price by bargaining. From a language learning perspective and to keep the students motivated, the agent’s language is crucial. The agent needs to behave human-like in a way which allows the users to suspend some of their disbeliefs and talk to DEAL as if talking to 72 another human being. In an experimental study (Hjalmarsson &amp; Edlund, in press), where a spoken dialogue system with human be</context>
</contexts>
<marker>Hjalmarsson, Wik, Brusk, 2007</marker>
<rawString>A. Hjalmarsson, P. Wik, and J. Brusk. 2007. Dealing with DEAL: a dialogue system for conversation training. In Proc. of SigDial. Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lindström</author>
</authors>
<title>Diskursmarkörer. In Tur och ordning; introduktion till svensk samtalsgrammatik (pp. 56-104). Norstedts Akademiska Förlag.</title>
<date>2008</date>
<location>Stockholm,</location>
<contexts>
<context position="6547" citStr="Lindström (2008)" startWordPosition="1052" endWordPosition="1053">l impact at the local speech segment level but serve significant pragmatic function. To give an exact definition of what cue phrases are is difficult, as these entities often are ambiguous. According to the definition used here, cue phrases can be a single word or larger units, occupy various positions, belong to different syntactic classes, and be realized with different prosodic contours. The first dialogue was analyzed and used to decide which classes to use in the annotation scheme. Nine of the classes were a subset of the functional classification scheme of discourse markers presented in Lindström (2008). A tenth class, referring, was added. There were 3 different classes for connectives, 3 classes for responsives and 4 remaining classes. The classes are presented in Table 1; the first row contains an example in its context from data, the word(s) in bold are the labelled cue phrase, and the second row presents frequently used instances of that class. Additive Connectives (CAD) och grönt är ju fint [and green is nice] och, allts$, s$ [and, therefore, so] Contrastive Connectives (CC) men den är ganska antik [but it is pretty antique] men, fast, allts$ [but, although, thus] Alternative Connectiv</context>
</contexts>
<marker>Lindström, 2008</marker>
<rawString>J. Lindström. 2008. Diskursmarkörer. In Tur och ordning; introduktion till svensk samtalsgrammatik (pp. 56-104). Norstedts Akademiska Förlag. Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schourup</author>
</authors>
<title>Discourse markers.</title>
<date>1999</date>
<journal>Lingua,</journal>
<volume>107</volume>
<issue>3</issue>
<pages>227--265</pages>
<contexts>
<context position="5665" citStr="Schourup, 1999" startWordPosition="905" endWordPosition="906"> pauses, repetitions, corrections and restarts were also labelled manually. 3.1 Cue phrases Linguistic devices used to signal relations between different segments of speech are often referred to as cue phrases. Other frequently used terms are discourse markers, pragmatic markers or discourse particles. Typical cue phrases in English are: oh, well, now, then, however, you know, I mean, because, and, but and or. Much research within discourse analysis, communicative analysis and psycholinguistics has been concerned with these connectives and what kind of relations they hold (for an overview see Schourup, 1999). Our definition of cue phrases is broad and all types of linguistic entities that the speakers use to hold the dialogue together at different communicative levels are included. A rule of thumb is that cue phrases are words or chunks of words that have little lexical impact at the local speech segment level but serve significant pragmatic function. To give an exact definition of what cue phrases are is difficult, as these entities often are ambiguous. According to the definition used here, cue phrases can be a single word or larger units, occupy various positions, belong to different syntactic</context>
</contexts>
<marker>Schourup, 1999</marker>
<rawString>L. Schourup. 1999. Discourse markers. Lingua, 107(3-4), 227-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>J Edlund</author>
<author>R Carlson</author>
</authors>
<title>Talking with Higgins: Research challenges in a spoken dialogue system.</title>
<date>2006</date>
<booktitle>In Perception and Interactive Technologies</booktitle>
<pages>193--196</pages>
<publisher>Berlin/Heidelberg: Springer.</publisher>
<contexts>
<context position="10828" citStr="Skantze et al., 2006" startWordPosition="1815" endWordPosition="1818">le initial MOD R CAD CC RD RNI RE REF RC CAL All Table 4: Turn position distribution 5 Generation in DEAL The collected and labelled data is a valuable resource of information for what cue phrases signal in the DEAL domain as well as how they are lexically and prosodically realized. To keep the reMOD R CAD CC RD RNI RE REF RC CAL 74 sponse times constant and without unnaturally long delays, DEAL needs to be capable of grabbing the turn, hold it while the system is producing the rest of the message, and release it after completion. DEAL is implemented using components from the Higgins project (Skantze et al., 2006) an off-theshelf ASR system and a GUI with an embodied conversational agent (ECA) (Beskow, 2003). A current research challenge is to redesign the modules and architecture for incremental processing, to allow generation of conversational speech. Deep generation in DEAL – the decision of what to say on an abstract semantic level – is distributed over three different modules; (1) the action manger, (2) the agent manager and the (3) communicative manager. The action manger is responsible for actions related to user input and previous discourse1. The agent manager represents the agents’ personal mo</context>
</contexts>
<marker>Skantze, Edlund, Carlson, 2006</marker>
<rawString>G. Skantze, J. Edlund, and R. Carlson. 2006. Talking with Higgins: Research challenges in a spoken dialogue system. In Perception and Interactive Technologies (pp. 193-196). Berlin/Heidelberg: Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>