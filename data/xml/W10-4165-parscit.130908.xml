<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.109412">
<title confidence="0.941197">
Triplet-Based Chinese Word Sense Induction
</title>
<author confidence="0.999502">
Zhao Liu
</author>
<affiliation confidence="0.901701333333333">
School of Computer Science
Fudan University
Shanghai, China
</affiliation>
<email confidence="0.996239">
ZLiu.fd@gmail.com
</email>
<author confidence="0.997218">
Xipeng Qiu
</author>
<affiliation confidence="0.901706">
School of Computer Science
Fudan University
Shanghai, China
</affiliation>
<email confidence="0.995905">
xpqiu@fudan.edu.cn
</email>
<author confidence="0.998414">
Xuanjing Huang
</author>
<affiliation confidence="0.901647666666667">
School of Computer Science
Fudan University
Shanghai, China
</affiliation>
<email confidence="0.99682">
xjhuang@fudan.edu.cn
</email>
<sectionHeader confidence="0.979972" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999798769230769">
This paper describes the implementa-
tion of our system at CLP 2010 bake-
off of Chinese word sense induction.
We first extract the triplets for the tar-
get word in each sentence, then use
the intersection of all related words of
these triplets from the Internet. We
use the related word to construct fea-
ture vectors for the sentence. At last
we discriminate the word senses by
clustering the sentences. Our system
achieved 77.88% F-score under the of-
ficial evaluation.
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955387096774">
The goal of the CLP 2010 bake-off of Chi-
nese word sense induction is to automati-
cally discriminate the senses of Chinese target
words by the use of only un-annotated data.
The use of word senses instead of word
forms has been shown to improve perfor-
mance in information retrieval, information
extraction and machine translation. Word
Sense Disambiguation generally requires the
use of large-scale manually annotated lexical
resources. Word Sense Induction can over-
come this limitation, and it has become one
of the most important topics in current com-
putational linguistics research.
In this paper we introduce a method to
solve the problem of Chinese word sense in-
duction.For this task, Firstly we constructed
triplets containing the target word in every
instance, then searched the intersection of all
the three words from the Internet with web
searching engine and constructed feature vec-
tors.Then we clustered the vectors with the
sIB clustering algorithm and at last discrimi-
nated the word senses.
This paper is organized as following: firstly
we introduce the related works. Then we talk
about the methods in features selection and
clustering. The method of evaluation and the
result of our system is following. At last we
discuss the improvement and the weakness of
our system.
</bodyText>
<sectionHeader confidence="0.99644" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.998927617647059">
Sense induction is typically treated as a
clustering problem, by considering their co-
occurring contexts, the instances of a target
word are partitioned into classes. Previous
methods have used the first or second or-
der co-occurrence (Pedersen and Bruce, 1997;
Schütze, 1998), parts of speech, and local col-
locations (Niu et al., 2007). The size of con-
text window is also various, it can be as small
as only two words before and after the target
words. It may be the sentence where the tar-
get word is in. Or it will be 20 surrounding
words on either side of the target words and
even more words.
After every instance of the target word is
represented as a feature vector, it will be the
input of the clustering methods. Many clus-
tering methods have been used in the task
of word sense induction. For example, k-
means and agglomerative clustering (Schütze,
1998). sIB (Sequential Information Bottle-
neck) a variation of Information Bottleneck is
applied in (Niu et al., 2007). In (Dorow and
Widdows, 2003) Graph-based clustering algo-
rithm is employed that in a graph a node rep-
resents a noun and two nodes have an edge be-
tween them if they co-occur in list more than
a given number of times. A generative model
based on LDA is proposed in (Brody and La-
pata, 2009).
In our method, we use the triplets (Bordag,
2006) and their intersections from the Internet
to construct the feature vectors then sIB is
used as the clustering method.
</bodyText>
<sectionHeader confidence="0.991617" genericHeader="method">
3 Feature Selection
</sectionHeader>
<bodyText confidence="0.999910030303031">
Our method select the features of the words
similar to (Bordag, 2006) is also using the
triplets. In Chinese there are no natural sep-
arators between the words as English, so the
first step in Chinese language processing is of-
ten the Chinese word segmentation. In our
system we use the FudanNLP toolkit1 to split
the words.
At the first stage, we split the instance of
the target word and filter out the numbers,
English words and stop words from it. So we
get a sequence of the words. Then we select
two words before the target and another two
words after it. If there are no words before or
after then leave it empty. After that we enu-
merate two words from the selected four words
to construct a triplets together with the target
words. So we can get several triplets for every
instance of the target. Because the faulty of
Chinese word segmentation and some special
target word for example a single Chinese char-
acter as a word, there are some errors finding
the position of the target words. If the word
is a single Chinese character and the toolkit
combine it with other Chinese characters to
be a word, we will use that word as the tar-
get instead of the character to construct the
triplets.
The second stage is obtaining corpus from
the Internet. For every triplet we search the
three words sequence in it with a pair of dou-
ble quotation marks in Baidu web searching
engine2. It gives the snippets of the webs
</bodyText>
<footnote confidence="0.999916">
1http://code.google.com/p/fudannlp/
2http://www.baidu.com
</footnote>
<bodyText confidence="0.999966275">
which have all the three words in it. We se-
lect the first 50 snippets of each triplets. If
the number of the snippets is less than 50,
we will ignore that triplet. For some rare
words the snippets searched from the Internet
for all the triplets of the instance is less than
50. In that situation we will search the target
word and another one co-occurring word in
the searching engine to achieve enough snip-
pets as features. After searching the triplets
we select the first three triplets (or doublets)
with largest amount of the webs searched by
the searching engine. For every instance there
are three or less triplets (or doublets) and we
have obtained many snippets for them. After
segmenting and filtering these snippets we use
the bag of words from them as the feature for
this instance.
The last stage of feature selection is to con-
struct the feature vector for every instances
containing the target word. In the previous
stage we get a bag of words for each instance.
For all the instances of one target word we
make a statistic of the frequence of each word
in the bags. In our system we select the words
whose frequence is more than 50 as the dimen-
sions for the feature vectors. From the tests
we find that when this thread varies from 50 to
120 the result of our system is nearly the same,
but outside that bound the result will become
rather bad. So we use 50 as the thread. Af-
ter constructing the dimension of that target
word, we can get a feature vector for each in-
stance that at each dimension the number is
the frequence of that word occurs in that po-
sition.
We obtain the feature vectors for the target
words by employing these three stage. The
following work is clustering these vector to get
the classes of the word senses.
</bodyText>
<sectionHeader confidence="0.986841" genericHeader="method">
4 The Clustering Algorithm
</sectionHeader>
<bodyText confidence="0.999974375">
There are many classical clustering methods
such as k-means, EM and so on. In (Niu et
al., 2007) they applied the sIB (Slonim et al.,
2002) clustering algorithm at SemEval-2007
for task 2 and it achieved a quite good result.
And at first this algorithm is also introduced
for the unsupervised document classification
problem. So we use the sIB algorithm for clus-
tering the feature vectors in our system.
Unlike the situation in (Niu et al., 2007),
the number of the sense classes is provided in
CLP2010 task 4. So we can apply the sIB
algorithm directly without the sense number
estimation procedure in that paper. sIB algo-
rithm is a variant of the information bottle-
neck method.
Let d represent a document, and w repre-
sent a feature word, d  D,w  F. Given
the joint distribution p(d, w), the document
clustering problem is formulated as looking for
a compact representation T for D, which re-
serves as much information as possible about
F. T is the document clustering solution. For
solving this optimization problem, sIB algo-
rithm was proposed in (Slonim et al., 2002),
which found a local maximum of I(T, F) by:
given a initial partition T, iteratively draw-
ing a d  D out of its cluster t(d), t  T,
and merging it into tnew such that tnew =
argmaxtETd(d, t). d(d, t) is the change of
I(T,F) due to merging d into cluster tnew,
which is given by
</bodyText>
<equation confidence="0.999314363636364">
d(d,t) = (p(d)+p(t))JS(p(w|d),p(w|t)). (1)
JS(p, q) is the Jensen-Shannon divergence,
which is defined as
JS(p,q) = pDKL(p||p) + qDKL(q||p), (2)
∑
DKL(p||p) =
y
∑
DKL(q||p) =
y
{p,q}  {p(w|d),p(w|t)}, (5)
</equation>
<bodyText confidence="0.9999702">
In our system we use the sIB algorithm in
the Weka 3.5.8 cluster package to cluster the
feature vectors obtained in the previous sec-
tion. The detailed description of the sIB algo-
rithm in weka can refer to the website 3. And
the parameters for this Weka class is that: the
number of clusters is the number of senses pro-
vided by the task, the random number seed is
zero and the other parameters like maximum
number of iteration and so on is set as default.
</bodyText>
<sectionHeader confidence="0.9597915" genericHeader="method">
5 CLP 2010 Bake-Off of Chinese
Word Sense Induction
</sectionHeader>
<subsectionHeader confidence="0.973416">
5.1 Evaluation Measure
</subsectionHeader>
<bodyText confidence="0.998403">
The evaluation measure is described as fol-
lowing:
We consider the gold standard as a solu-
tion to the clustering problem. All examples
tagged with a given sense in the gold standard
form a class. For the system output, the clus-
ters are formed by instances assigned to the
same sense tag (the sense tag with the highest
weight for that instance). We will compare
clusters output by the system with the classes
in the gold standard and compute F-score as
usual. F-score is computed with the formula
below.
Suppose Cr is a class of the gold standard,
and Si is a cluster of the system generated,
then
</bodyText>
<listItem confidence="0.995788833333333">
1. F − score(Cr, Si) = 2*P*R
P+R
2. P =the number of correctly labeled ex-
amples for a cluster/total cluster size
3. R =the number of correctly labeled ex-
amples for a cluster/total class size
</listItem>
<bodyText confidence="0.739566">
Then for a given class Cr,
</bodyText>
<equation confidence="0.991143846153846">
FScore(Cr) = max(F − score(Cr, Si)) (8)
Si
Then
p log p , (3)
p
q log q , (4)
p
{p,q}  { p(d) , p(t) FScore = c nr FScore(Cr) (9)
p(d) + p(t) p(d) + p(t)}, (6) ∑ n
r=1
3http://pacific.mpi-cbg.de/javadoc/Weka/
clusterers/sIB.html
p = pp(w|d) + qp(w|t). (7)
</equation>
<bodyText confidence="0.9950655">
Where c is total number of classes, nr is the
size of class Cr , and n is the total size.
</bodyText>
<subsectionHeader confidence="0.948581">
5.2 DataSet
</subsectionHeader>
<bodyText confidence="0.9998114375">
The data set includes 100 ambiguous Chi-
nese words and for every word it provided 50
instances. Besides that they also provided a
sample test set of 2500 examples of 50 target
words with the answers to illustrate the data
format.
Besides the sIB algorithm we also apply the
k-means and EM algorithm to cluster the fea-
ture vectors. These algorithms are separately
using the simpleKMeans class and the EM
class in the Weka 3.5.8 cluster package. Ex-
cept the number of clusters set as the given
number of senses and number of seeds set as
zero, all other parameters are set as default.
For the given sample test set with answers the
result is illustrated in the Table 1 below.
</bodyText>
<table confidence="0.92427725">
algorithm F-score
k-means 0.7025
EM 0.7286
sIB 0.8132
</table>
<tableCaption confidence="0.998671">
Table 1: Results of three clustering algorithms
</tableCaption>
<bodyText confidence="0.999572666666667">
From Table 1 we can see the sIB clustering
algorithm improves the result of the Chinese
word sense induction evidently.
In the real test data test containing 100 am-
biguous Chinese words, our system achieves a
F-score 0.7788 ranking 6th among the 18 sys-
tems submitted. The best F-score of these 18
systems is 0.7933 and the average of them is
0.7128.
</bodyText>
<subsectionHeader confidence="0.942139">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999871827160494">
In our system we only use the local collo-
cations and the co-occurrences of the target
words. But the words distance for the target
word in the same sentence and the parts of
speech of the neighboring word together with
the target word is also important in this task.
In our experiment we used the parts of
speech for the target word and each word be-
fore and after it achieved by the Chinese word
segmentation system as part of the features
vectors for clustering. With a proper weight
on each POS dimension in the feature vectors,
the F score for some word in the given sample
test set with answers improved evidently. For
example the Chinese word “便宜”, the F score
of it was developed from 0.5983 to 0.7573. But
because of the fault of the segmentation sys-
tem and other reasons F score of other words
fell and the speed of the system was rather
slower than before, we gave up this improve-
ment finally.
Without the words distance for the target
word in the same sentence the feature vectors
maybe lack some information useful. So if we
can calculate the correlation between the tar-
get word and other words, we will use these
word sufficiently. However because of quan-
tity of the Internet corpus is unknown, we
didn’t find the proper method to weigh the
correlation.
From the previous section we find that the
F score for the real test data test is lower than
that for the sample test set. It is mainly be-
cause there are more single Chinese charac-
ters (as words) in the real test data set. Our
system does not process these characters spe-
cially. For most of the Chinese characters we
can’t judge their correct senses only from the
context where they appear. Their meaning
always depends on the collocations with the
other Chinese characters with which they be-
come a Chinese word. However our system
discriminates the senses of them only refer-
ring to the context of them, it can’t judge the
meaning of these Chinese characters properly.
Maybe the best way is to search them in the
dictionary.
However our system does not always have a
very poor performance for any single Chinese
character (as a word). The result is quite good
for some Chinese characters. For example the
Chinese character “谷” which has three mean-
ing: valley, millet and a family name, the pre-
cision (P) of our system is 0.760. But for most
of single Chinese characters such as “服” and
“公”, it is so bad that the result in the sample
test worked rather better than the real test.
In Chinese the former character “谷” tends
to express a complete meaning and the other
characters in the word which they combine of-
ten modify it such as the characters “山” and
“稻” in the word “山谷” and “稻谷”. So this
character can have a relatively high correla-
tion with the words around and our system
can deal with such characters like it. Unfor-
tunately most characters need other charac-
ters to represent a complete meaning as the
the latter “服” and “公” so they almost have
no correlation with the words around but with
those characters in the word in which they oc-
cur. But our system only uses the context fea-
tures and even doesn’t do any special process
about these single Chinese characters. There-
fore our system can not address those char-
acters appropriately and we need to find a
proper method to solve it, using a dictionary
may be a choice.
This method works better for nouns and ad-
jectives (in the sample test data set there are
only 4 adjectives), but for verbs F score falls
a little, illustrated in the Table 2 below.
</bodyText>
<table confidence="0.9240275">
POS F-score
nouns 0.8473
adjectives 0.8543
verbs 0.7921
</table>
<tableCaption confidence="0.9637205">
Table 2: Results of each POS in the sample
test data set
</tableCaption>
<bodyText confidence="0.9992772">
Only using the local collocations in our sys-
tem the F score is achieve above 80% (in the
sample test), it demonstrates to some extent
the information of collocations is so important
that we should pay more attention to it.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999165">
The triplet-based Chinese word sense induc-
tion method is fitted to the task of Chinese
word sense induction and obtain rather good
result. But for some single characters word
and some verbs, this method is not appropri-
ate enough. In the future work, we will im-
prove the method with more reasonable triplet
selection strategies.
</bodyText>
<sectionHeader confidence="0.997708" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9925078">
This work was (partially) funded by 863
Program (No. 2009AA01A346), 973 Pro-
gram (No. 2010CB327906), and Shanghai Sci-
ence and Technology Development Funds (No.
08511500302).
</bodyText>
<sectionHeader confidence="0.998248" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991715257142857">
Bordag, S. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. Pro-
ceedings of EACL-06. Trento.
Brody, S. and M. Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 103–
111. Association for Computational Linguistics.
Dorow, B. and D. Widdows. 2003. Discovering
corpus-specific word senses. In Proceedings of
the tenth conference on European chapter of
the Association for Computational Linguistics-
Volume 2, pages 79–82. Association for Compu-
tational Linguistics.
Niu, Z.Y., D.H. Ji, and C.L. Tan. 2007. I2r: Three
systems for word sense discrimination, chinese
word sense disambiguation, and english word
sense disambiguation. In Proceedings of the 4th
International Workshop on Semantic Evalua-
tions, pages 177–182. Association for Compu-
tational Linguistics.
Pedersen, T. and R. Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of
the Second Conference on Empirical Methods in
Natural Language Processing, volume 2, pages
197–207.
Schütze, H. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97–
123.
Slonim, N., N. Friedman, and N. Tishby. 2002.
Unsupervised document classification using se-
quential information maximization. In Proceed-
ings of the 25th annual international ACM SI-
GIR conference on Research and development
in information retrieval, pages 129–136. ACM.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.097144">
<title confidence="0.999291">Triplet-Based Chinese Word Sense Induction</title>
<author confidence="0.997031">Zhao</author>
<affiliation confidence="0.924063">School of Computer Fudan</affiliation>
<address confidence="0.774629">Shanghai,</address>
<email confidence="0.999842">ZLiu.fd@gmail.com</email>
<author confidence="0.730401">Xipeng</author>
<affiliation confidence="0.9155105">School of Computer Fudan</affiliation>
<address confidence="0.927468">Shanghai,</address>
<email confidence="0.982626">xpqiu@fudan.edu.cn</email>
<author confidence="0.400725">Xuanjing</author>
<affiliation confidence="0.9048435">School of Computer Fudan</affiliation>
<address confidence="0.925639">Shanghai,</address>
<email confidence="0.993044">xjhuang@fudan.edu.cn</email>
<abstract confidence="0.987699142857143">This paper describes the implementation of our system at CLP 2010 bakeoff of Chinese word sense induction. We first extract the triplets for the target word in each sentence, then use the intersection of all related words of these triplets from the Internet. We use the related word to construct feature vectors for the sentence. At last we discriminate the word senses by clustering the sentences. Our system F-score under the official evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bordag</author>
</authors>
<title>Word sense induction: Tripletbased clustering and automatic evaluation.</title>
<date>2006</date>
<booktitle>Proceedings of EACL-06.</booktitle>
<location>Trento.</location>
<contexts>
<context position="3416" citStr="Bordag, 2006" startWordPosition="563" endWordPosition="564">e clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the words as English, so the first step in Chinese language processing is often the Chinese word segmentation. In our system we use the FudanNLP toolkit1 to split the words. At the first stage, we split the instance of the target word and filter out the numbers, English words and stop words from it. So we get a</context>
</contexts>
<marker>Bordag, 2006</marker>
<rawString>Bordag, S. 2006. Word sense induction: Tripletbased clustering and automatic evaluation. Proceedings of EACL-06. Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>M Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3365" citStr="Brody and Lapata, 2009" startWordPosition="551" endWordPosition="555">s represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the words as English, so the first step in Chinese language processing is often the Chinese word segmentation. In our system we use the FudanNLP toolkit1 to split the words. At the first stage, we split the instance of the target word and filter out the numbers</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Brody, S. and M. Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 103– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorow</author>
<author>D Widdows</author>
</authors>
<title>Discovering corpus-specific word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 2,</booktitle>
<pages>79--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3107" citStr="Dorow and Widdows, 2003" startWordPosition="501" endWordPosition="504">s, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>Dorow, B. and D. Widdows. 2003. Discovering corpus-specific word senses. In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 2, pages 79–82. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Y Niu</author>
<author>D H Ji</author>
<author>C L Tan</author>
</authors>
<title>I2r: Three systems for word sense discrimination, chinese word sense disambiguation, and english word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>177--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2440" citStr="Niu et al., 2007" startWordPosition="380" endWordPosition="383">rganized as following: firstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the first or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottle</context>
<context position="6894" citStr="Niu et al., 2007" startWordPosition="1187" endWordPosition="1190">o 120 the result of our system is nearly the same, but outside that bound the result will become rather bad. So we use 50 as the thread. After constructing the dimension of that target word, we can get a feature vector for each instance that at each dimension the number is the frequence of that word occurs in that position. We obtain the feature vectors for the target words by employing these three stage. The following work is clustering these vector to get the classes of the word senses. 4 The Clustering Algorithm There are many classical clustering methods such as k-means, EM and so on. In (Niu et al., 2007) they applied the sIB (Slonim et al., 2002) clustering algorithm at SemEval-2007 for task 2 and it achieved a quite good result. And at first this algorithm is also introduced for the unsupervised document classification problem. So we use the sIB algorithm for clustering the feature vectors in our system. Unlike the situation in (Niu et al., 2007), the number of the sense classes is provided in CLP2010 task 4. So we can apply the sIB algorithm directly without the sense number estimation procedure in that paper. sIB algorithm is a variant of the information bottleneck method. Let d represent </context>
</contexts>
<marker>Niu, Ji, Tan, 2007</marker>
<rawString>Niu, Z.Y., D.H. Ji, and C.L. Tan. 2007. I2r: Three systems for word sense discrimination, chinese word sense disambiguation, and english word sense disambiguation. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 177–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>Distinguishing word senses in untagged text.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>2</volume>
<pages>197--207</pages>
<contexts>
<context position="2364" citStr="Pedersen and Bruce, 1997" startWordPosition="367" endWordPosition="370">sIB clustering algorithm and at last discriminated the word senses. This paper is organized as following: firstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the first or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>Pedersen, T. and R. Bruce. 1997. Distinguishing word senses in untagged text. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, volume 2, pages 197–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<contexts>
<context position="2380" citStr="Schütze, 1998" startWordPosition="371" endWordPosition="372">nd at last discriminated the word senses. This paper is organized as following: firstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the first or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequenti</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Schütze, H. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Friedman</author>
<author>N Tishby</author>
</authors>
<title>Unsupervised document classification using sequential information maximization.</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>129--136</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6937" citStr="Slonim et al., 2002" startWordPosition="1195" endWordPosition="1198"> the same, but outside that bound the result will become rather bad. So we use 50 as the thread. After constructing the dimension of that target word, we can get a feature vector for each instance that at each dimension the number is the frequence of that word occurs in that position. We obtain the feature vectors for the target words by employing these three stage. The following work is clustering these vector to get the classes of the word senses. 4 The Clustering Algorithm There are many classical clustering methods such as k-means, EM and so on. In (Niu et al., 2007) they applied the sIB (Slonim et al., 2002) clustering algorithm at SemEval-2007 for task 2 and it achieved a quite good result. And at first this algorithm is also introduced for the unsupervised document classification problem. So we use the sIB algorithm for clustering the feature vectors in our system. Unlike the situation in (Niu et al., 2007), the number of the sense classes is provided in CLP2010 task 4. So we can apply the sIB algorithm directly without the sense number estimation procedure in that paper. sIB algorithm is a variant of the information bottleneck method. Let d represent a document, and w represent a feature word,</context>
</contexts>
<marker>Slonim, Friedman, Tishby, 2002</marker>
<rawString>Slonim, N., N. Friedman, and N. Tishby. 2002. Unsupervised document classification using sequential information maximization. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 129–136. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>