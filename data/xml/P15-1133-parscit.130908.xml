<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9980595">
A convex and feature-rich discriminative approach to
dependency grammar induction
</title>
<author confidence="0.99133">
´Edouard Grave No´emie Elhadad
</author>
<affiliation confidence="0.999818">
Columbia University Columbia University
</affiliation>
<email confidence="0.993474">
edouard.grave@gmail.com noemie.elhadad@columbia.edu
</email>
<sectionHeader confidence="0.994686" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987785">
In this paper, we introduce a new method
for the problem of unsupervised depen-
dency parsing. Most current approaches
are based on generative models. Learning
the parameters of such models relies on
solving a non-convex optimization prob-
lem, thus making them sensitive to initial-
ization. We propose a new convex formu-
lation to the task of dependency grammar
induction. Our approach is discriminative,
allowing the use of different kinds of fea-
tures. We describe an efficient optimiza-
tion algorithm to learn the parameters of
our model, based on the Frank-Wolfe algo-
rithm. Our method can easily be general-
ized to other unsupervised learning prob-
lems. We evaluate our approach on ten
languages belonging to four different fam-
ilies, showing that our method is competi-
tive with other state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979685714286">
Grammar induction is an important problem in
computational linguistics. Despite having recently
received a lot of attention, it is still considered to
be an unsolved problem. In this work, we are inter-
ested in unsupervised dependency parsing. More
precisely, our goal is to induce directed depen-
dency trees, which capture binary syntactic rela-
tions between the words of a sentence. Since our
method is unsupervised, it does not have access
to such syntactic structure and only take as in-
put a corpus of words and their associated parts
of speech.
Most recent approaches to unsupervised depen-
dency parsing are based on probabilistic genera-
tive models, such as the dependency model with
valence introduced by Klein and Manning (2004).
Learning the parameters of such models is often
done by maximizing the log-likelihood of unla-
beled data, leading to a non-convex optimization
problem. Thus, the performance of those methods
rely heavily on the initialization, and practitioners
have to find good heuristics to initialize their mod-
els.
In this paper, we describe a different approach
to the problem of dependency grammar induction,
inspired by discriminative clustering. We pro-
pose to use a feature-rich discriminative parser,
and to learn the parameters of this parser us-
ing a convex quadratic objective function. In
particular, this approach also allows us to in-
duce non-projective dependency structures. Fol-
lowing the work of Naseem et al. (2010), we
use language-independent rules between pairs of
parts-of-speech to guide our parser. More pre-
cisely, we make the following contributions:
</bodyText>
<listItem confidence="0.974912307692308">
• Our method is based on a feature-rich dis-
criminative parser (section 3);
• Learning the parameters of our parser is
achieved using a convex objective, and is thus
not sensitive to initialization (section 4);
• Our method can produce non-projective de-
pendency structures (section 3.2.2);
• We propose an efficient algorithm to opti-
mize the objective, based on the Frank-Wolfe
method (section 5);
• We evaluate our approach on the universal
treebanks dataset, showing that it is competi-
tive with the state-of-the-art (section 6).
</listItem>
<figureCaption confidence="0.958399">
All languages have their own grammar
Figure 1: An example of dependency tree.
</figureCaption>
<page confidence="0.887491">
1375
</page>
<note confidence="0.980992">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1375–1384,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997467" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999956555555556">
A lot of research has been carried out in the last
decade on dependency grammar induction. We
review the dependency model with valence, on
which most unsupervised dependency parsers are
based, before presenting different extensions and
learning algorithms. Finally, we review discrimi-
native clustering, on which our method is based.
DMV. The dependency model with valence
(DMV), introduced by Klein and Manning (2004),
was the first method to outperform the baseline
consisting in attaching each token to the next one.
The DMV is a generative probabilistic model of
the dependency tree and parts-of-speech of a sen-
tence. It generates the root first, and then recur-
sively generates the tokens down the tree. The
probability of generating a new dependent for a
given token depends on the direction (left or right)
and whether a dependent was already generated in
that direction. Then, the part-of-speech of the new
dependent is generated according to a multinomial
distribution conditioned on the direction and the
head’s POS.
Extensions. Several extensions of the depen-
dency model with valence have been proposed.
Headden III et al. (2009) proposed the lexicalized
extended valence grammar (EVG), in which the
probability of generating a POS also depends on
the valence information. They rely on smooth-
ing to tackle the increased number of parame-
ters. Mareˇcek and ˇZabokrtsk`y (2012) described
an approach using a n-gram reducibility measure,
which capture which words can be deleted from
a sentence without making it syntactically incor-
rect. Cohen and Smith (2009) introduced a prior,
based on the shared logistic normal distribution.
This prior allowed to tie the grammar parameters
corresponding to different POS belonging to the
same coarse groups, such as all the POS corre-
sponding to verbs. Berg-Kirkpatrick and Klein
(2010) proposed to tie the parameters of grammars
for different languages using a prior based on a
phylogenetic tree. Naseem et al. (2010) proposed
a set of rules between parts-of-speech, encoding
syntactic universals, such as the fact that adjec-
tives are often dependents of nouns. They used
posterior regularization (Ganchev et al., 2010) to
impose that a certain amount of the infered depen-
dencies verifies one of these rules. Also using pos-
terior regularization, Gillenwater et al. (2011) im-
posed a sparsity bias on the infered dependencies,
enforcing a small number of unique dependency
types. Finally, Blunsom and Cohn (2010) refor-
mulated dependency grammar induction using tree
substitution grammars, while Bisk and Hocken-
maier (2013) proposed to use combinatory cate-
gorial grammars.
Learning. Different algorithms have been pro-
posed to improve the learning of the parameters
of the dependency model with valence. Smith
and Eisner (2005) proposed to use constrastive es-
timation to learn the parameters of a log-linear
parametrization of the DMV, while Spitkovsky et
al. (2010b) showed that using Viterbi EM instead
of classic EM leads to higher accuracy. Observing
that learning from shorter sentences is easier (be-
cause less ambiguous), Spitkovsky et al. (2010a)
presented different techniques to learn grammar
from increasingly longer sentences. Gimpel and
Smith (2012) introduced a model inspired by the
IBM1 translation model for grammar induction,
resulting in a concave log-likelihood function.
They show that initializing the DMV with the
output of their model leads to improved depen-
dency accuracies. Hsu et al. (2012) and Parikh
et al. (2014) introduced spectral methods for un-
supervised dependency and constituency parsing.
Finally, Spitkovsky et al. (2013) introduced dif-
ferent heuristics for avoiding local minima while
Gormley and Eisner (2013) proposed a method to
find the global optimum of non-convex problems,
based on branch-and-bound.
Discriminative clustering. Our unsupervised
parser is inspired by discriminative clustering, in-
troduced by Xu et al. (2004). Given a set of points,
the objective of discriminative clustering is to as-
sign labels to these points that can be easily pre-
dicted using a discriminative classifier. Xu et al.
(2004) introduced a formulation using the hinge
loss, Bach and Harchaoui (2007) proposed to use
the squared loss instead, while Joulin et al. (2010)
proposed a formulation based on the logistic loss.
Recently, a formulation based on discriminative
clustering was proposed for the problem of distant
supervision for relation extraction (Grave, 2014)
and for the problem of finding the names of char-
acters in TV series based on the corresponding
scripts (Ramanathan et al., 2014). Closest to our
approach, extensions of discriminative clustering
were used to align sequences of labels or text with
</bodyText>
<page confidence="0.987093">
1376
</page>
<bodyText confidence="0.951651">
videos (Bojanowski et al., 2014; Bojanowski et al.,
2015) or to co-localize objects in videos (Joulin et
al., 2014).
</bodyText>
<sectionHeader confidence="0.986299" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9999465">
In this section, we describe the parsing model used
in our approach and briefly review the correspond-
ing decoding algorithms. Following McDonald et
al. (2005b), we propose to cast the problem of de-
pendency parsing as a maximum weight spanning
tree problem in directed graphs.
</bodyText>
<subsectionHeader confidence="0.99499">
3.1 Edge-based factorization
</subsectionHeader>
<bodyText confidence="0.993098466666667">
Let us start by setting up some notations. An
input sentence of length n is represented by an
n−uplet x = (x1, ..., xn). The dependency tree
corresponding to that sentence is represented by a
n x (n + 1) binary matrix y, such that yij = 1 if
and only if the head of the token i is the token j
(and thus, the integer n + 1 represents the root of
the tree).
In this paper, we follow a common approach
by factoring the score of dependency tree as the
sum of the scores of the edges forming that
tree. We assume that each pair of tokens (i, j)
is represented by a high-dimensional feature vec-
tor f(x, i, j) E Rd. Then, the score sij of the
edge (i, j) is obtained using the linear model
</bodyText>
<equation confidence="0.6912175">
sij = wTf(x,
i,j),
</equation>
<bodyText confidence="0.9991025">
where w E Rd is a parameter vector. Thus the
score s corresponding to the tree y is equal to
</bodyText>
<equation confidence="0.9977505">
s = � sij
(i,j) s.t. yzj=1
�= w�f(x, i, j).
(i,j) s.t. yzj=1
</equation>
<bodyText confidence="0.9999425">
Assuming that the parameter vector w is known,
parsing a sentence reduces to finding the tree with
the highest score, which is the maximum weight
spanning tree.
</bodyText>
<subsectionHeader confidence="0.999915">
3.2 Maximum spanning trees
</subsectionHeader>
<bodyText confidence="0.9986032">
Different sets of spanning trees have been consid-
ered in the setting of supervised dependency pars-
ing. We briefly review those sets, and describe
the corresponding algorithms to compute the max-
imum weight spanning tree over those sets.
</bodyText>
<subsectionHeader confidence="0.975213">
3.2.1 Projective dependency trees
</subsectionHeader>
<bodyText confidence="0.999989904761905">
First, we consider the set of projective spanning
trees. A dependency tree is said to be projective if
the dependencies do not cross when drawn above
the words in linear order. Similarly, this means
that word and all its descendants form a contigu-
ous substring of the sentence. Projective depen-
dency trees are thus strongly related to context free
grammars, and it is possible to obtain the maxi-
mum weight spanning projective tree using a mod-
ified version of the CKY algorithm (Cocke and
Schwartz, 1970; Kasami, 1965; Younger, 1967).
The complexity of this algorithm is O(n5). This
led Eisner (1996) to propose an algorithm for pro-
jective parsing which has a complexity of O(n3).
Similarly to CKY, the Eisner algorithm is based
on dynamic programming, parsing a sentence in
a bottom-up fashion. Finally, it should be noted
that the dependency model with valence, on which
most approaches to dependency grammar induc-
tion are based, produces projective dependency
trees.
</bodyText>
<subsectionHeader confidence="0.884415">
3.2.2 Non-projective dependency trees
</subsectionHeader>
<bodyText confidence="0.999984647058824">
Second, we consider the set of non-projective
spanning trees. Indeed, many languages, such
as Czech or Dutch, have a significant number of
non-projective edges. In the context of supervised
dependency parsing, McDonald et al. (2005b)
shown that using non-projective trees improves
the accuracy of dependency parsers for those lan-
guages. The maximum weight spanning tree in
a directed graph can be computed using the Chu-
Liu/Edmonds algorithm (Chu and Liu, 1965; Ed-
monds, 1967), which has a complexity of O(n3).
Later, Tarjan (1977) proposed an improved ver-
sion of this algorithm for dense graphs, whose
complexity is O(n2), the same as for undirected
graphs using Prim’s algorithm. Thus a second ad-
vantage of using non-projective dependency trees
is the fact that it leads to more efficient parsers.
</bodyText>
<sectionHeader confidence="0.891664" genericHeader="method">
4 Learning the parameter vector
</sectionHeader>
<bodyText confidence="0.999593333333333">
In this section, we describe the loss function we
use to learn the parameter vector w from unla-
beled sentences.
</bodyText>
<subsectionHeader confidence="0.995581">
4.1 Problem formulation
</subsectionHeader>
<bodyText confidence="0.99991925">
From now on, y is a vector representing the de-
pendency trees corresponding to the whole corpus.
Thus, each index i corresponds to a potential de-
pendency between two words of a given sentence.
</bodyText>
<page confidence="0.99211">
1377
</page>
<figureCaption confidence="0.972733">
He gave a seminar yesterday about unsupervised dependency parsing
Figure 2: Example of a non-projective dependency tree in english.
</figureCaption>
<bodyText confidence="0.999963333333333">
Like before, yi = 1 if and only if there is a de-
pendency between those two words, and yi = 0
otherwise. The set of dependencies that form valid
trees is denoted by the set T .
Inspired by the discriminative clustering frame-
work introduced by Xu et al. (2004), our goal is
to jointly find the dependencies represented by the
vector y and the parameter vector w which mini-
mize the regularized empirical risk
</bodyText>
<equation confidence="0.980093">
`(yi, w&gt;xi) + λΩ(w), (1)
</equation>
<bodyText confidence="0.999727">
where ` is a loss function and Ω is a regularizer.
The intuition is that we want to find the depen-
dency trees y that can be easily predicted by a dis-
criminative parser, whose parameters are w.
Following Bach and Harchaoui (2007), we pro-
pose to use the squared loss ` defined by
</bodyText>
<equation confidence="0.988579">
`(y, ˆy) = 2(y − ˆy)2
1
</equation>
<bodyText confidence="0.9999225">
and to use the `2-norm as a regularizer. In that
case, we obtain the objective function:
</bodyText>
<equation confidence="0.833326">
2n�y − Xw�2
1 2 + λ2IIwII22. (2)
</equation>
<bodyText confidence="0.999966">
One of the main advantages of using the squared
loss is the fact that the corresponding objective
function is jointly convex in y and w. Indeed,
the objective is the composition of an affine map-
ping, defined by (y, w) H y − Xw, with a con-
vex function, defined by u H u&gt;u. Thus, the
objective function is convex (see section 3.2.2 of
Boyd and Vandenberghe (2004)). The problem (2)
is thus non-convex only because of the combinato-
rial constraints on the binary vector y, namely that
y should represents valid trees.
</bodyText>
<subsectionHeader confidence="0.997091">
4.2 Convex relaxation
</subsectionHeader>
<bodyText confidence="0.9999405">
The set T of vectors representing valid depen-
dency trees is a finite set of binary vectors. We
can thus take the convex hull of those points and
denote it by Y:
</bodyText>
<equation confidence="0.938972428571429">
Y = conv(T).
VERB H VERB NOUN H NOUN
VERB H NOUN NOUN H ADJ
VERB H PRON NOUN H DET
VERB H ADV NOUN H NUM
VERB H ADP NOUN H CONJ
ADJ H ADV ADP H NOUN
</equation>
<tableCaption confidence="0.990373">
Table 1: Set of universal rules used in our parser.
</tableCaption>
<bodyText confidence="0.999487">
By definition, this set is a convex polytope. We
then propose to replace the combinatorial con-
straints on the vector y by the fact that y should
be in the convex polytope Y. We thus obtain a
convex quadratic program, with linear constraints,
as follows:
</bodyText>
<equation confidence="0.979409">
2nIIy − XwI12
1 2 + λ2IIwII22. (3)
</equation>
<bodyText confidence="0.9999705">
We will describe how to compute the optimal so-
lution of this problem in section 5.
</bodyText>
<subsectionHeader confidence="0.998839">
4.3 Rounding
</subsectionHeader>
<bodyText confidence="0.999808">
Given a continuous solution yc E Y of the relaxed
problem, it is possible to obtain a solution of the
integer problem by finding the tree yd E T which
is closest to yc, by solving the problem
</bodyText>
<equation confidence="0.593291">
min �yd − yc�22.
yd∈T
</equation>
<bodyText confidence="0.9999135">
The solution of the previous problem can easily
be formulated is a minimum weight spanning tree
problem. Indeed, by developping the previous
expression, and using the fact that for all trees
yd E T, y&gt;d yd = n, where n is the number of
tokens, the previous problem is equivalent to:
</bodyText>
<equation confidence="0.696308">
−y&gt;d yc,
</equation>
<bodyText confidence="0.9985775">
whose solution is obtained using the minimum
weight spanning tree algorithm. It should be noted
that the rounding solution is not necessarily the
optimal solution of the integer problem.
</bodyText>
<figure confidence="0.986358222222222">
1
n
min
w
n
i=1
min
y∈T
min
y∈T
min
w
min
y∈Y
min
w
min
yd∈T
1378
Algorithm 1: Frank-Wolfe algorithm
...,
do
Compute the gradient:
9t =
Solve the li
{1,
T}
∇f(zt)
near program:
sT9t
=min
st
sED
Take the Frank-Wolfe step:
zt+1 = rytst + (1 − ryt)zt
end
</figure>
<figureCaption confidence="0.999953">
Figure 3: Illustration of a Frank-Wolfe step.
</figureCaption>
<subsectionHeader confidence="0.69166">
4.4 Prior on y
</subsectionHeader>
<bodyText confidence="0.9999303">
We now describe how to guide our unsuper-
vised parser, by using universal rules. Following
Naseem et al. (2010), we want a certain percent-
age of the infered dependencies to satisfy one of
the twelve universal syntactic rules, listed in Ta-
ble 1. Let S be the set of indices corresponding
to word pairs that satisfy one of these rules. Then,
imposing that a certain percentage c of dependen-
cies satisfy one of those rules can be obtained by
imposing the constraint:
</bodyText>
<equation confidence="0.9326465">
1 n�
iES
</equation>
<bodyText confidence="0.945829090909091">
minimum weight spanning tr
ee algorithm. Instead
we propose to use the Frank-Wolfe algorithm, that
we now describe.
set D. It is an
iterative first-order optimization
method. At each iteration t, the convex function f
is approximated by a linear function defined by its
gradient at the current point zt. Then it finds the
point st that minimizes that linear function, over
the convex set D:
</bodyText>
<equation confidence="0.546163">
yi ≥ c.
</equation>
<bodyText confidence="0.999925">
This linear constraint is equivalent to uTy ≥ c, st =min sT∇f(zt) s.t. s E D.
where the vector u is defined by s
</bodyText>
<equation confidence="0.962216">
r 1/n if i E S,
ui =Sl
0 otherwise.
</equation>
<bodyText confidence="0.9648988">
Using Lagrangian duality, we can obtain the fol-
lowing equivalent penalized problem:
deal with the penalized problem an
d we will thus
use it in the next section.
</bodyText>
<sectionHeader confidence="0.997699" genericHeader="method">
5 Optimization
</sectionHeader>
<bodyText confidence="0.946357875">
One could use a general purpose quadratic solver
to compute the solution of the previous convex
problem. However, this might be inefficient since
The point
is then defined as the weighted av-
erage between the solution st and the current point
zt:
=
</bodyText>
<equation confidence="0.714819">
st +
</equation>
<bodyText confidence="0.9887023125">
zt, where ryt is the step
size (such as 2/(t + 2)). Compared to the gradi-
ent descent algorithm, the Frank-Wolfe alogrithm
does not take a step in the direction of the gradi-
ent, but in the direction of the point that minimizes
the linear approximation of the function f over the
convex set D (see Fig 3). In particular, this ensures
that the points zt always stay inside the convex set,
and there is thus no need for a projection step.
To summarize, in order to use the Frank-Wolfe
algorithm, we need to compute the gradient of the
objective function and to minimize a linear func-
tion over our convex set. This is particularly ap-
propriate to our problem, since we can easily min-
imize alinear function over the tr
zt+1
</bodyText>
<equation confidence="0.930065666666667">
zt+1
ryt
(1−ryt)
</equation>
<bodyText confidence="0.99805675">
ee polytopes (us-
ing the minimum weight spanning tree algorithm),
while projecting on those polytopes is more ex-
pensive.
</bodyText>
<equation confidence="0.9215175">
2nIy−Xw12+λ2IwI22−µ uTy. (4)
1
min
YEY
min
W
</equation>
<bodyText confidence="0.980573">
The penalized and constrained problems are
equivalent, since for every c, there exists a µ such
that the two problems have the same optimum.
From an optimization point of view, it is easier to
for t E
it does not use the structure of the polytope and,
in particular, the fact that one can easily minimize
a linear function over the tree polytope using the
</bodyText>
<subsectionHeader confidence="0.95064">
5.1 Frank-Wolfe algorithm
</subsectionHeader>
<bodyText confidence="0.999554333333333">
The Frank-Wolfe algorithm (Frank and Wolfe,
1956; Jaggi, 2013) is used to minimize a convex
differentiable function f over a convex bounded
</bodyText>
<page confidence="0.988788">
1379
</page>
<figure confidence="0.9377419">
Algorithm 2: Optimization algorithm for our
method.
for t ∈ {1, ...,T} do
Compute the optimal w:
Solve the linear program:
st = min
sEY
Take the Frank-Wolfe step:
yt+1 = γtst + (1 − γt)yt
end
</figure>
<subsectionHeader confidence="0.935163">
5.2 Application to our problem
</subsectionHeader>
<bodyText confidence="0.9999935">
We now describe how to use the Frank-Wolfe al-
gorithm to optimize our objective function with re-
spect to y. First, let us introduce the functions f
and h defined by
</bodyText>
<equation confidence="0.9942685">
1 2 + λ
f(w, y) = 2nky − Xwk2 2 kwk2 2 − µ uTy,
h(y) = min f(w, y).
w
The original problem is equivalent to
EY
y
f (w, y) = mi
</equation>
<bodyText confidence="0.99997675">
We will use the Frank-Wolfe algorithm to optimize
the function h.
Minimizing w.r.t w. First, we need to minimize
the function f with respect to w, in order to com-
pute the function h (and its gradient). One must
note that this is an unconstrained quadratic pro-
gram, whose solution can be obtained in closed
form by solving the linear system:
</bodyText>
<equation confidence="0.9187155">
( )
XTX + λI w = XTy.
</equation>
<bodyText confidence="0.999961285714286">
However, in case of a very large feature space, this
system might be prohibitively expensive to solve
exactly. We instead propose to approximately
compute the optimal w using stochastic gradient
descent.
Computing the gradient of h. Then, the gradi-
ent of the function h at the point y is equal to
</bodyText>
<equation confidence="0.9992595">
∇h(y) = ∇yf(w*,y),
POSi × d
POSj × d
POSi × POSj × d
POSi × POSi_1 × POSj × d
POSi × POSi+1 × POSj × d
POSi × POSj × POSj_1 × d
POSi × POSj × POSj+1 × d
</equation>
<tableCaption confidence="0.926351666666667">
Table 2: Features used in our parser to describe the
dependency between tokens i and j, where i is the
head, j the dependent and d = i − j.
</tableCaption>
<bodyText confidence="0.957243">
where w* is equal to
</bodyText>
<equation confidence="0.996647">
w* = argmin f(w, y).
w
</equation>
<bodyText confidence="0.99992075">
Thus, in order to compute the gradient of h with
respect to y, we start by computing the corre-
sponding optimal value of w. Then, the gradient
with respect to y is equal to
</bodyText>
<equation confidence="0.996689666666667">
1
∇h(y) = (y − Xw*) − µ u.
n
</equation>
<bodyText confidence="0.997653222222222">
Minimizing a linear function over Y. We fi-
naly need to compute the optimal solution of the
following linear problem
∇h(y)Ts.
The optimal value of a linear function over a
bounded convex polytope is always attained on at
least one vertex of that polytope. By definition of
our polytope, those vertices correspond to span-
ning trees. Thus, computing an optimal solution
of this problem is obtained by finding a minimum
weight spanning tree.
Discussion. Similarly to the Expectation-
Maximization algorithm, our optimization
method is a two-steps iterative algorithm. In
the first step, the optimal parameter vector w is
estimated based on the previous dependency trees,
while the second step consist in re-estimating the
(relaxed) dependency trees.
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999446666666667">
In this section, we report the results of the experi-
ments we have performed to evaluate our approach
to grammar induction.
</bodyText>
<figure confidence="0.850696357142857">
wt = argmin 2nkyt − Xwk2
w 12 + λ2kwk22
Compute the gradient w.r.t. y:
n
1 (yt − Xwt) − µ u
gt =
sTgt
min
w
min
yEY
h(y).
min
sEY
</figure>
<page confidence="0.953692">
1380
</page>
<table confidence="0.9995615">
DMV PR USR OUR
DE 42.6 58.4 53.4 60.2
EN 22.4 57.5 66.2 62.3
ES 31.8 57.3 71.5 68.8
FR 56.0 66.2 54.1 72.3
ID 44.9 21.4 50.3 69.7
IT 33.3 40.4 46.5 64.3
JA 48.0 58.9 58.2 57.5
KO 35.3 50.7 48.8 59.0
PT-BR 49.6 40.7 46.4 68.3
SV 38.9 61.2 64.3 66.2
AVG 40.2 51.3 56.0 64.8
</table>
<tableCaption confidence="0.998185">
Table 3: Directed dependency accuracy, on
</tableCaption>
<bodyText confidence="0.669089333333333">
the universal treebanks with universal parts-of-
speech, on sentences of length 10 or less. PR refers
to posterior regularization, USR to universal rules.
</bodyText>
<subsectionHeader confidence="0.97108">
6.1 Features
</subsectionHeader>
<bodyText confidence="0.999913916666667">
The features used in our unsupervised parser are
based on the parts-of-speech of the head and the
dependent of the corresponding dependency, and
are given in Table 2. Following McDonald et al.
(2005a), we also include features capturing the
context of the head or the dependent. These fea-
tures are trigrams and are formed by the parts-
of-speech of the two tokens of the dependency
and one of the word appearing before/after the
head/dependent. Finally, all the features are con-
joined with the signed distance between the two
words of the dependency.
</bodyText>
<subsectionHeader confidence="0.998127">
6.2 Dataset
</subsectionHeader>
<bodyText confidence="0.9999955">
We use the universal treebanks, version 2.0, intro-
duced by McDonald et al. (2013). This dataset
contains dependency trees for ten languages be-
longing to five different families: Spanish, French,
Italian, Portuguese (Romanic family), English,
German, Swedish (Germanic family), Korean,
Japanese and Indonesian. The tokens of those
treebanks are tagged using the universal part-of-
speech tagset (Petrov et al., 2012). We focus on
inducing dependency grammars using universal
parts-of-speech, and will thus report results where
all methods use (gold) universal POS.
</bodyText>
<subsectionHeader confidence="0.999966">
6.3 Comparison with baselines
</subsectionHeader>
<bodyText confidence="0.999948333333333">
We will compare our approach to three other un-
supervised parsers. Our first baseline is the DMV
model, introduced by Klein and Manning (2004).
</bodyText>
<equation confidence="0.24911">
DMV PR USR OUR
7 min 1 h 15 h 2 min
</equation>
<bodyText confidence="0.973431529411765">
Table 4: Computational times required to learn a
grammar on the English treebank.
Our second baseline is the extended valence gram-
mar model, with posterior sparsity constraints, as
described by Gillenwater et al. (2011). Finally,
our last baseline is the model with universal rules
introduced by Naseem et al. (2010). It should
be noted that these two baselines obtain perfor-
mances that are near state-of-the-art. All methods
are trained and tested on sentences of length 10 or
less, after stripping punctuation.
Parameter selection. All the parameters were
chosen using the English development set. Our
method has two parameters, determined as:
A = 0.001 and p = 0.1. We used T = 200
iterations in all the experiments.
Discussion. We report the results in Table 3.
First, we observe that our method performs bet-
ter than the three baselines on seven out of ten
languages. Overall, our approach outperforms the
three baselines, with an absolute improvement of
13 points over the extended valence grammar with
posterior sparsity and 8 points over the model with
universal syntactic rules. We also note that the
inter-language variance is lower for our method
than the baselines (std of 4.6 for our method v.s.
8.3 for USR and 12.7 for PR). For the sake of
completeness, we also compared those methods
using the fine grained POS available in the univer-
sal treebanks. Overall, our method obtains an ac-
curacy of 68.4, while USR and PR achieve accura-
cies of 67.3 and 58.5 respectively. Finally, we re-
port computational times in Table 4, showing that
our approach is much faster than the baselines.
</bodyText>
<subsectionHeader confidence="0.999397">
6.4 Non-projective grammar induction
</subsectionHeader>
<bodyText confidence="0.9999892">
In this section, we investigate non-projective
grammar induction. With our approach, we only
have to replace the Eisner algorithm by Chu-
Liu/Edmonds. We report results in Table 5. First,
we observe that the non-projective results are
slightly worse than projective one. This is not re-
ally surprising since the amount of non-projective
gold dependencies is very small on the considered
data. Moreover, non-projective trees are much
more ambiguous than projective ones, leading to
</bodyText>
<page confidence="0.962186">
1381
</page>
<table confidence="0.999486333333333">
PROJECTIVE NON-PROJECTIVE
DE 60.2 57.2
EN 62.3 60.5
ES 68.8 66.5
FR 72.3 69.2
ID 69.7 68.4
IT 64.3 63.1
JA 57.5 59.3
KO 59.0 60.0
PT-BR 68.3 67.7
SV 66.2 65.4
AVG 64.8 63.7
</table>
<tableCaption confidence="0.731186333333333">
Table 5: Comparison between projective and non-
projective unsupervised dependency parsing using
our method.
</tableCaption>
<bodyText confidence="0.92186125">
a harder problem. We still believe those results
are interesting because the difference is small (less
than 1.5 points), while non-projective parsing is
computationaly more efficient.
</bodyText>
<subsectionHeader confidence="0.998997">
6.5 Evaluation on longer sentences
</subsectionHeader>
<bodyText confidence="0.99999">
We also evaluate our method on longer sentences
(while still training on sentences of length 10 or
less). Directed dependency accuracies are re-
ported in Figure 4. On all sentences, our method
achieve an overall accuracy of 55.8.
</bodyText>
<subsectionHeader confidence="0.997709">
6.6 Feature ablation study
</subsectionHeader>
<bodyText confidence="0.999967769230769">
In this section, we study the importance of the
different features used in our parser. We report
directed accuracies when different groups of fea-
tures are removed, one at a time, in Table 6. First,
we remove the distance information from the fea-
tures (line DISTANCE). We observe that the per-
formance of our parser is greatly affected by this
ablation, especially for long sentences. Then, we
remove the context features (line CONTEXT) and
the unigram features (line UNIGRAM) from our
model. We observe that the performance decreases
slightly due to this ablations, but the differences
are small.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9946176">
In this paper, we introduced a new framework for
the task of unsupervised dependency parsing. Our
method is a based on a feature-rich discrimina-
tive model, whose parameters are learned using a
convex objective function. We demonstrated on
</bodyText>
<table confidence="0.9967414">
|W |&lt; 10 |W |&lt; oc
DISTANCE 61.8 48.7
CONTEXT 64.2 55.1
UNIGRAM 64.0 55.3
ALL FEATURES 64.8 55.8
</table>
<tableCaption confidence="0.992605">
Table 6: Feature ablation study.
</tableCaption>
<bodyText confidence="0.9999146">
the universal treebanks that our approach leads to
competitive results, while being computationaly
very efficient. We now describe some directions
we would like to explore as future work.
Richer feature set. In our experiments, we fo-
cused on assessing the usefulness of our con-
vex, discriminative approach, and thus considered
only relatively simple features based on parts-of-
speech. Inspired by supervised dependency pars-
ing, we would like to explore the use of other fea-
tures such as Brown clusters (Brown et al., 1992)
or distributed word representations (Mikolov et
al., 2013), in order to lexicalize our parser.
Higher-order parsing. So far, our model is
lacking the notion of valency, that has proven very
useful for grammar induction. In future work,
we would thus like to replace our edge-based fac-
torization by a higher-order one, in order to cap-
ture siblings (and grandchilds) interactions. We
would then have to use a higher-order parser, such
as the ones described by McDonald and Pereira
(2006) and Koo and Collins (2010). Another po-
tential approach would be to use the linear pro-
gramming relaxed inference, described by Martins
et al. (2009).
Transfer learning. In this paper, we used uni-
versal syntactic rules, as described by Naseem et
al. (2010) to guide our parser. We would like to
explore the use of weak supervision, such as the
one considered in transfer learning (Hwa et al.,
2005). For example, projected dependencies from
a resource-rich language could be used as con-
straints in our framework.
Code. The code for our method is distributed on
the first author webpage.
</bodyText>
<sectionHeader confidence="0.998037" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.964144333333333">
This work is supported by National Science Foun-
dation award 1344668 and National Institute of
General Medical Sciences award R01 GM090187.
</bodyText>
<page confidence="0.980832">
1382
</page>
<figure confidence="0.99920235483871">
10 15 20 30 all
75
70
65
60
55
50
sv
en
de
10 15 20 30 all
75
70
65
60
55
50
es
fr
pt
it
10 15 20 30 all
75
70
65
60
55
50
id
ko
ja
</figure>
<figureCaption confidence="0.999934">
Figure 4: Directed dependency accuracies on longer sentences for our approach.
</figureCaption>
<sectionHeader confidence="0.992592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992209309859155">
Francis R Bach and Za¨ıd Harchaoui. 2007. Diffrac: a
discriminative and flexible framework for clustering.
In NIPS.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic grammar induction. In ACL.
Yonatan Bisk and Julia Hockenmaier. 2013. An hdp
model for inducing combinatory categorial gram-
mars. TACL.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In EMNLP.
Piotr Bojanowski, R´emi Lajugie, Francis Bach, Ivan
Laptev, Jean Ponce, Cordelia Schmid, and Josef
Sivic. 2014. Weakly supervised action labeling in
videos under ordering constraints. In ECCV.
Piotr Bojanowski, R´emi Lagugie, Edouard Grave,
Francis Bach, Ivan Laptev, Jean Ponce, and Cordelia
Schmid. 2015. Weakly-supervised alignment of
video with text. http://arxiv.org/abs/1505.06027.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex optimization. Cambridge university press.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On
shortest arborescence of a directed graph. Scientia
Sinica.
John Cocke and Jacob Schwartz. 1970. Programming
languages and their compilers: Preliminary notes.
Technical report.
Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In NAACL.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards.
Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING.
Marguerite Frank and Philip Wolfe. 1956. An algo-
rithm for quadratic programming. Naval research
logistics quarterly.
Kuzman Ganchev, Joao Grac¸a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. JMLR.
Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao Grac¸a,
Fernando Pereira, and Ben Taskar. 2011. Poste-
rior sparsity in unsupervised dependency parsing.
JMLR.
Kevin Gimpel and Noah A Smith. 2012. Concavity
and initialization for unsupervised dependency pars-
ing. In NAACL.
Matthew R Gormley and Jason Eisner. 2013. Noncon-
vex global optimization for latent-variable models.
In ACL.
Edouard Grave. 2014. A convex relaxation for weakly
supervised relation extraction. In EMNLP.
William P Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In
NAACL.
Daniel Hsu, Percy Liang, and Sham M Kakade. 2012.
Identifiability and unmixing of latent parse trees. In
NIPS.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural language engineering.
Martin Jaggi. 2013. Revisiting frank-wolfe:
Projection-free sparse convex optimization. In
ICML.
</reference>
<page confidence="0.576956">
1383
</page>
<reference confidence="0.999829837837838">
Armand Joulin, Jean Ponce, and Francis R Bach. 2010.
Efficient optimization for discriminative latent class
models. In NIPS.
Armand Joulin, Kevin Tang, and Li Fei-Fei. 2014. Ef-
ficient image and video co-localization with frank-
wolfe algorithm. In ECCV.
Tadao Kasami. 1965. An efficient recognition and syn-
tax analysis algorithm for context-free languages.
Technical report.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL.
David Mareˇcek and Zdenˇek ˇZabokrtsk`y. 2012. Ex-
ploiting reducibility in unsupervised dependency
parsing. In EMNLP/CoNLL.
Andr´e FT Martins, Noah A Smith, and Eric P Xing.
2009. Polyhedral outer approximations with appli-
cation to natural language parsing. In ICML.
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In EMNLP.
Ryan T McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Os-
car T¨ackstr¨om, et al. 2013. Universal dependency
annotation for multilingual parsing. In ACL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In EMNLP.
Ankur P Parikh, Shay B Cohen, and Eric P Xing.
2014. Spectral unsupervised parsing with additive
tree metrics. In ACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. LREC.
Vignesh Ramanathan, Armand Joulin, Percy Liang,
and Li Fei-Fei. 2014. Linking people with ”their”
names using coreference resolution. In ECCV.
Noah A Smith and Jason Eisner. 2005. Guiding un-
supervised grammar induction using contrastive es-
timation. In Proc. of IJCAI Workshop on Grammat-
ical Inference Applications.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From baby steps to leapfrog: How
less is more in unsupervised dependency parsing. In
NAACL.
Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010b. Viterbi train-
ing improves unsupervised dependency parsing. In
CoNLL.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In EMNLP.
Robert Endre Tarjan. 1977. Finding optimum branch-
ings. Networks.
Linli Xu, James Neufeld, Bryce Larson, and Dale
Schuurmans. 2004. Maximum margin clustering.
In NIPS.
Daniel H Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
control.
</reference>
<page confidence="0.993961">
1384
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925242">
<title confidence="0.987443">A convex and feature-rich discriminative approach dependency grammar induction</title>
<author confidence="0.977578">´Edouard Grave No´emie Elhadad</author>
<affiliation confidence="0.999877">Columbia University Columbia</affiliation>
<email confidence="0.999737">edouard.grave@gmail.comnoemie.elhadad@columbia.edu</email>
<abstract confidence="0.998298476190476">In this paper, we introduce a new method for the problem of unsupervised dependency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization problem, thus making them sensitive to initialization. We propose a new convex formulation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of features. We describe an efficient optimization algorithm to learn the parameters of our model, based on the Frank-Wolfe algorithm. Our method can easily be generalized to other unsupervised learning problems. We evaluate our approach on ten languages belonging to four different families, showing that our method is competitive with other state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis R Bach</author>
<author>Za¨ıd Harchaoui</author>
</authors>
<title>Diffrac: a discriminative and flexible framework for clustering.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7680" citStr="Bach and Harchaoui (2007)" startWordPosition="1177" endWordPosition="1180">nd constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize ob</context>
<context position="12908" citStr="Bach and Harchaoui (2007)" startWordPosition="2074" endWordPosition="2077"> there is a dependency between those two words, and yi = 0 otherwise. The set of dependencies that form valid trees is denoted by the set T . Inspired by the discriminative clustering framework introduced by Xu et al. (2004), our goal is to jointly find the dependencies represented by the vector y and the parameter vector w which minimize the regularized empirical risk `(yi, w&gt;xi) + λΩ(w), (1) where ` is a loss function and Ω is a regularizer. The intuition is that we want to find the dependency trees y that can be easily predicted by a discriminative parser, whose parameters are w. Following Bach and Harchaoui (2007), we propose to use the squared loss ` defined by `(y, ˆy) = 2(y − ˆy)2 1 and to use the `2-norm as a regularizer. In that case, we obtain the objective function: 2n�y − Xw�2 1 2 + λ2IIwII22. (2) One of the main advantages of using the squared loss is the fact that the corresponding objective function is jointly convex in y and w. Indeed, the objective is the composition of an affine mapping, defined by (y, w) H y − Xw, with a convex function, defined by u H u&gt;u. Thus, the objective function is convex (see section 3.2.2 of Boyd and Vandenberghe (2004)). The problem (2) is thus non-convex only </context>
</contexts>
<marker>Bach, Harchaoui, 2007</marker>
<rawString>Francis R Bach and Za¨ıd Harchaoui. 2007. Diffrac: a discriminative and flexible framework for clustering. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5345" citStr="Berg-Kirkpatrick and Klein (2010)" startWordPosition="820" endWordPosition="823">n which the probability of generating a POS also depends on the valence information. They rely on smoothing to tackle the increased number of parameters. Mareˇcek and ˇZabokrtsk`y (2012) described an approach using a n-gram reducibility measure, which capture which words can be deleted from a sentence without making it syntactically incorrect. Cohen and Smith (2009) introduced a prior, based on the shared logistic normal distribution. This prior allowed to tie the grammar parameters corresponding to different POS belonging to the same coarse groups, such as all the POS corresponding to verbs. Berg-Kirkpatrick and Klein (2010) proposed to tie the parameters of grammars for different languages using a prior based on a phylogenetic tree. Naseem et al. (2010) proposed a set of rules between parts-of-speech, encoding syntactic universals, such as the fact that adjectives are often dependents of nouns. They used posterior regularization (Ganchev et al., 2010) to impose that a certain amount of the infered dependencies verifies one of these rules. Also using posterior regularization, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>An hdp model for inducing combinatory categorial grammars.</title>
<date>2013</date>
<publisher>TACL.</publisher>
<contexts>
<context position="6080" citStr="Bisk and Hockenmaier (2013)" startWordPosition="933" endWordPosition="937">ee. Naseem et al. (2010) proposed a set of rules between parts-of-speech, encoding syntactic universals, such as the fact that adjectives are often dependents of nouns. They used posterior regularization (Ganchev et al., 2010) to impose that a certain amount of the infered dependencies verifies one of these rules. Also using posterior regularization, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, Blunsom and Cohn (2010) reformulated dependency grammar induction using tree substitution grammars, while Bisk and Hockenmaier (2013) proposed to use combinatory categorial grammars. Learning. Different algorithms have been proposed to improve the learning of the parameters of the dependency model with valence. Smith and Eisner (2005) proposed to use constrastive estimation to learn the parameters of a log-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly long</context>
</contexts>
<marker>Bisk, Hockenmaier, 2013</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2013. An hdp model for inducing combinatory categorial grammars. TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5970" citStr="Blunsom and Cohn (2010)" startWordPosition="919" endWordPosition="922">roposed to tie the parameters of grammars for different languages using a prior based on a phylogenetic tree. Naseem et al. (2010) proposed a set of rules between parts-of-speech, encoding syntactic universals, such as the fact that adjectives are often dependents of nouns. They used posterior regularization (Ganchev et al., 2010) to impose that a certain amount of the infered dependencies verifies one of these rules. Also using posterior regularization, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, Blunsom and Cohn (2010) reformulated dependency grammar induction using tree substitution grammars, while Bisk and Hockenmaier (2013) proposed to use combinatory categorial grammars. Learning. Different algorithms have been proposed to improve the learning of the parameters of the dependency model with valence. Smith and Eisner (2005) proposed to use constrastive estimation to learn the parameters of a log-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because les</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Bojanowski</author>
<author>R´emi Lajugie</author>
<author>Francis Bach</author>
<author>Ivan Laptev</author>
<author>Jean Ponce</author>
<author>Cordelia Schmid</author>
<author>Josef Sivic</author>
</authors>
<title>Weakly supervised action labeling in videos under ordering constraints.</title>
<date>2014</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="8233" citStr="Bojanowski et al., 2014" startWordPosition="1264" endWordPosition="1267">troduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we describe the parsing model used in our approach and briefly review the corresponding decoding algorithms. Following McDonald et al. (2005b), we propose to cast the problem of dependency parsing as a maximum weight spanning tree problem in directed graphs. 3.1 Edge-based factorization Let us start by setting up some notations. An input sentence of length n is represented by an n−uplet x = (x1, ..., xn). The dependency tree corresponding to that sentence is represented by a n x (n + </context>
</contexts>
<marker>Bojanowski, Lajugie, Bach, Laptev, Ponce, Schmid, Sivic, 2014</marker>
<rawString>Piotr Bojanowski, R´emi Lajugie, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, and Josef Sivic. 2014. Weakly supervised action labeling in videos under ordering constraints. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Bojanowski</author>
<author>R´emi Lagugie</author>
<author>Edouard Grave</author>
<author>Francis Bach</author>
<author>Ivan Laptev</author>
<author>Jean Ponce</author>
<author>Cordelia Schmid</author>
</authors>
<title>Weakly-supervised alignment of video with text.</title>
<date>2015</date>
<note>http://arxiv.org/abs/1505.06027.</note>
<contexts>
<context position="8259" citStr="Bojanowski et al., 2015" startWordPosition="1268" endWordPosition="1271">ing the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we describe the parsing model used in our approach and briefly review the corresponding decoding algorithms. Following McDonald et al. (2005b), we propose to cast the problem of dependency parsing as a maximum weight spanning tree problem in directed graphs. 3.1 Edge-based factorization Let us start by setting up some notations. An input sentence of length n is represented by an n−uplet x = (x1, ..., xn). The dependency tree corresponding to that sentence is represented by a n x (n + 1) binary matrix y, such t</context>
</contexts>
<marker>Bojanowski, Lagugie, Grave, Bach, Laptev, Ponce, Schmid, 2015</marker>
<rawString>Piotr Bojanowski, R´emi Lagugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, and Cordelia Schmid. 2015. Weakly-supervised alignment of video with text. http://arxiv.org/abs/1505.06027.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Lieven Vandenberghe</author>
</authors>
<title>Convex optimization. Cambridge university press.</title>
<date>2004</date>
<contexts>
<context position="13465" citStr="Boyd and Vandenberghe (2004)" startWordPosition="2184" endWordPosition="2187">ive parser, whose parameters are w. Following Bach and Harchaoui (2007), we propose to use the squared loss ` defined by `(y, ˆy) = 2(y − ˆy)2 1 and to use the `2-norm as a regularizer. In that case, we obtain the objective function: 2n�y − Xw�2 1 2 + λ2IIwII22. (2) One of the main advantages of using the squared loss is the fact that the corresponding objective function is jointly convex in y and w. Indeed, the objective is the composition of an affine mapping, defined by (y, w) H y − Xw, with a convex function, defined by u H u&gt;u. Thus, the objective function is convex (see section 3.2.2 of Boyd and Vandenberghe (2004)). The problem (2) is thus non-convex only because of the combinatorial constraints on the binary vector y, namely that y should represents valid trees. 4.2 Convex relaxation The set T of vectors representing valid dependency trees is a finite set of binary vectors. We can thus take the convex hull of those points and denote it by Y: Y = conv(T). VERB H VERB NOUN H NOUN VERB H NOUN NOUN H ADJ VERB H PRON NOUN H DET VERB H ADV NOUN H NUM VERB H ADP NOUN H CONJ ADJ H ADV ADP H NOUN Table 1: Set of universal rules used in our parser. By definition, this set is a convex polytope. We then propose t</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Stephen Boyd and Lieven Vandenberghe. 2004. Convex optimization. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language. Computational linguistics.</title>
<date>1992</date>
<contexts>
<context position="27174" citStr="Brown et al., 1992" startWordPosition="4614" endWordPosition="4617"> 48.7 CONTEXT 64.2 55.1 UNIGRAM 64.0 55.3 ALL FEATURES 64.8 55.8 Table 6: Feature ablation study. the universal treebanks that our approach leads to competitive results, while being computationaly very efficient. We now describe some directions we would like to explore as future work. Richer feature set. In our experiments, we focused on assessing the usefulness of our convex, discriminative approach, and thus considered only relatively simple features based on parts-ofspeech. Inspired by supervised dependency parsing, we would like to explore the use of other features such as Brown clusters (Brown et al., 1992) or distributed word representations (Mikolov et al., 2013), in order to lexicalize our parser. Higher-order parsing. So far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based factorization by a higher-order one, in order to capture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by McDonald and Pereira (2006) and Koo and Collins (2010). Another potential approach would be to use the linear programming relaxed inference, </context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoeng-Jin Chu</author>
<author>Tseng-Hong Liu</author>
</authors>
<title>On shortest arborescence of a directed graph. Scientia Sinica.</title>
<date>1965</date>
<contexts>
<context position="11412" citStr="Chu and Liu, 1965" startWordPosition="1812" endWordPosition="1815">with valence, on which most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a significant number of non-projective edges. In the context of supervised dependency parsing, McDonald et al. (2005b) shown that using non-projective trees improves the accuracy of dependency parsers for those languages. The maximum weight spanning tree in a directed graph can be computed using the ChuLiu/Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which has a complexity of O(n3). Later, Tarjan (1977) proposed an improved version of this algorithm for dense graphs, whose complexity is O(n2), the same as for undirected graphs using Prim’s algorithm. Thus a second advantage of using non-projective dependency trees is the fact that it leads to more efficient parsers. 4 Learning the parameter vector In this section, we describe the loss function we use to learn the parameter vector w from unlabeled sentences. 4.1 Problem formulation From now on, y is a vector representing the dependency trees corresponding to the whole corp</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest arborescence of a directed graph. Scientia Sinica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
<author>Jacob Schwartz</author>
</authors>
<title>Programming languages and their compilers: Preliminary notes.</title>
<date>1970</date>
<tech>Technical report.</tech>
<contexts>
<context position="10448" citStr="Cocke and Schwartz, 1970" startWordPosition="1664" endWordPosition="1667">e corresponding algorithms to compute the maximum weight spanning tree over those sets. 3.2.1 Projective dependency trees First, we consider the set of projective spanning trees. A dependency tree is said to be projective if the dependencies do not cross when drawn above the words in linear order. Similarly, this means that word and all its descendants form a contiguous substring of the sentence. Projective dependency trees are thus strongly related to context free grammars, and it is possible to obtain the maximum weight spanning projective tree using a modified version of the CKY algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967). The complexity of this algorithm is O(n5). This led Eisner (1996) to propose an algorithm for projective parsing which has a complexity of O(n3). Similarly to CKY, the Eisner algorithm is based on dynamic programming, parsing a sentence in a bottom-up fashion. Finally, it should be noted that the dependency model with valence, on which most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as C</context>
</contexts>
<marker>Cocke, Schwartz, 1970</marker>
<rawString>John Cocke and Jacob Schwartz. 1970. Programming languages and their compilers: Preliminary notes. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="5080" citStr="Cohen and Smith (2009)" startWordPosition="779" endWordPosition="782">ng to a multinomial distribution conditioned on the direction and the head’s POS. Extensions. Several extensions of the dependency model with valence have been proposed. Headden III et al. (2009) proposed the lexicalized extended valence grammar (EVG), in which the probability of generating a POS also depends on the valence information. They rely on smoothing to tackle the increased number of parameters. Mareˇcek and ˇZabokrtsk`y (2012) described an approach using a n-gram reducibility measure, which capture which words can be deleted from a sentence without making it syntactically incorrect. Cohen and Smith (2009) introduced a prior, based on the shared logistic normal distribution. This prior allowed to tie the grammar parameters corresponding to different POS belonging to the same coarse groups, such as all the POS corresponding to verbs. Berg-Kirkpatrick and Klein (2010) proposed to tie the parameters of grammars for different languages using a prior based on a phylogenetic tree. Naseem et al. (2010) proposed a set of rules between parts-of-speech, encoding syntactic universals, such as the fact that adjectives are often dependents of nouns. They used posterior regularization (Ganchev et al., 2010) </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards.</journal>
<contexts>
<context position="11428" citStr="Edmonds, 1967" startWordPosition="1816" endWordPosition="1818">ich most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a significant number of non-projective edges. In the context of supervised dependency parsing, McDonald et al. (2005b) shown that using non-projective trees improves the accuracy of dependency parsers for those languages. The maximum weight spanning tree in a directed graph can be computed using the ChuLiu/Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which has a complexity of O(n3). Later, Tarjan (1977) proposed an improved version of this algorithm for dense graphs, whose complexity is O(n2), the same as for undirected graphs using Prim’s algorithm. Thus a second advantage of using non-projective dependency trees is the fact that it leads to more efficient parsers. 4 Learning the parameter vector In this section, we describe the loss function we use to learn the parameter vector w from unlabeled sentences. 4.1 Problem formulation From now on, y is a vector representing the dependency trees corresponding to the whole corpus. Thus, each i</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>Jack Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="10545" citStr="Eisner (1996)" startWordPosition="1681" endWordPosition="1682">endency trees First, we consider the set of projective spanning trees. A dependency tree is said to be projective if the dependencies do not cross when drawn above the words in linear order. Similarly, this means that word and all its descendants form a contiguous substring of the sentence. Projective dependency trees are thus strongly related to context free grammars, and it is possible to obtain the maximum weight spanning projective tree using a modified version of the CKY algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967). The complexity of this algorithm is O(n5). This led Eisner (1996) to propose an algorithm for projective parsing which has a complexity of O(n3). Similarly to CKY, the Eisner algorithm is based on dynamic programming, parsing a sentence in a bottom-up fashion. Finally, it should be noted that the dependency model with valence, on which most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a significant number of non-projective edges. In the context of supervised de</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marguerite Frank</author>
<author>Philip Wolfe</author>
</authors>
<title>An algorithm for quadratic programming. Naval research logistics quarterly.</title>
<date>1956</date>
<contexts>
<context position="18172" citStr="Frank and Wolfe, 1956" startWordPosition="3054" endWordPosition="3057"> tr zt+1 zt+1 ryt (1−ryt) ee polytopes (using the minimum weight spanning tree algorithm), while projecting on those polytopes is more expensive. 2nIy−Xw12+λ2IwI22−µ uTy. (4) 1 min YEY min W The penalized and constrained problems are equivalent, since for every c, there exists a µ such that the two problems have the same optimum. From an optimization point of view, it is easier to for t E it does not use the structure of the polytope and, in particular, the fact that one can easily minimize a linear function over the tree polytope using the 5.1 Frank-Wolfe algorithm The Frank-Wolfe algorithm (Frank and Wolfe, 1956; Jaggi, 2013) is used to minimize a convex differentiable function f over a convex bounded 1379 Algorithm 2: Optimization algorithm for our method. for t ∈ {1, ...,T} do Compute the optimal w: Solve the linear program: st = min sEY Take the Frank-Wolfe step: yt+1 = γtst + (1 − γt)yt end 5.2 Application to our problem We now describe how to use the Frank-Wolfe algorithm to optimize our objective function with respect to y. First, let us introduce the functions f and h defined by 1 2 + λ f(w, y) = 2nky − Xwk2 2 kwk2 2 − µ uTy, h(y) = min f(w, y). w The original problem is equivalent to EY y f (</context>
</contexts>
<marker>Frank, Wolfe, 1956</marker>
<rawString>Marguerite Frank and Philip Wolfe. 1956. An algorithm for quadratic programming. Naval research logistics quarterly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<publisher>JMLR.</publisher>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Joao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Fernando Pereira</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior sparsity in unsupervised dependency parsing.</title>
<date>2011</date>
<publisher>JMLR.</publisher>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2011</marker>
<rawString>Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao Grac¸a, Fernando Pereira, and Ben Taskar. 2011. Posterior sparsity in unsupervised dependency parsing. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Concavity and initialization for unsupervised dependency parsing.</title>
<date>2012</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="6717" citStr="Gimpel and Smith (2012)" startWordPosition="1030" endWordPosition="1033">se combinatory categorial grammars. Learning. Different algorithms have been proposed to improve the learning of the parameters of the dependency model with valence. Smith and Eisner (2005) proposed to use constrastive estimation to learn the parameters of a log-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for unsupervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative cl</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A Smith. 2012. Concavity and initialization for unsupervised dependency parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew R Gormley</author>
<author>Jason Eisner</author>
</authors>
<title>Nonconvex global optimization for latent-variable models.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7203" citStr="Gormley and Eisner (2013)" startWordPosition="1103" endWordPosition="1106">ous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for unsupervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Re</context>
</contexts>
<marker>Gormley, Eisner, 2013</marker>
<rawString>Matthew R Gormley and Jason Eisner. 2013. Nonconvex global optimization for latent-variable models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edouard Grave</author>
</authors>
<title>A convex relaxation for weakly supervised relation extraction.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7949" citStr="Grave, 2014" startWordPosition="1219" endWordPosition="1220">rvised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we describe the parsing model used in our approach and briefly review the corresponding decoding algorithms. Following McDonald et al. (2005b), we propose to cast the problem of dependency parsing as a max</context>
</contexts>
<marker>Grave, 2014</marker>
<rawString>Edouard Grave. 2014. A convex relaxation for weakly supervised relation extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Johnson, McClosky, 2009</marker>
<rawString>William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Percy Liang</author>
<author>Sham M Kakade</author>
</authors>
<title>Identifiability and unmixing of latent parse trees.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6972" citStr="Hsu et al. (2012)" startWordPosition="1070" endWordPosition="1073">g-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for unsupervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discrimi</context>
</contexts>
<marker>Hsu, Liang, Kakade, 2012</marker>
<rawString>Daniel Hsu, Percy Liang, and Sham M Kakade. 2012. Identifiability and unmixing of latent parse trees. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering.</title>
<date>2005</date>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jaggi</author>
</authors>
<title>Revisiting frank-wolfe: Projection-free sparse convex optimization.</title>
<date>2013</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="18186" citStr="Jaggi, 2013" startWordPosition="3058" endWordPosition="3059">t) ee polytopes (using the minimum weight spanning tree algorithm), while projecting on those polytopes is more expensive. 2nIy−Xw12+λ2IwI22−µ uTy. (4) 1 min YEY min W The penalized and constrained problems are equivalent, since for every c, there exists a µ such that the two problems have the same optimum. From an optimization point of view, it is easier to for t E it does not use the structure of the polytope and, in particular, the fact that one can easily minimize a linear function over the tree polytope using the 5.1 Frank-Wolfe algorithm The Frank-Wolfe algorithm (Frank and Wolfe, 1956; Jaggi, 2013) is used to minimize a convex differentiable function f over a convex bounded 1379 Algorithm 2: Optimization algorithm for our method. for t ∈ {1, ...,T} do Compute the optimal w: Solve the linear program: st = min sEY Take the Frank-Wolfe step: yt+1 = γtst + (1 − γt)yt end 5.2 Application to our problem We now describe how to use the Frank-Wolfe algorithm to optimize our objective function with respect to y. First, let us introduce the functions f and h defined by 1 2 + λ f(w, y) = 2nky − Xwk2 2 kwk2 2 − µ uTy, h(y) = min f(w, y). w The original problem is equivalent to EY y f (w, y) = mi We </context>
</contexts>
<marker>Jaggi, 2013</marker>
<rawString>Martin Jaggi. 2013. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armand Joulin</author>
<author>Jean Ponce</author>
<author>Francis R Bach</author>
</authors>
<title>Efficient optimization for discriminative latent class models.</title>
<date>2010</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7749" citStr="Joulin et al. (2010)" startWordPosition="1189" endWordPosition="1192">erent heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we de</context>
</contexts>
<marker>Joulin, Ponce, Bach, 2010</marker>
<rawString>Armand Joulin, Jean Ponce, and Francis R Bach. 2010. Efficient optimization for discriminative latent class models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armand Joulin</author>
<author>Kevin Tang</author>
<author>Li Fei-Fei</author>
</authors>
<title>Efficient image and video co-localization with frankwolfe algorithm.</title>
<date>2014</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="8317" citStr="Joulin et al., 2014" startWordPosition="1278" endWordPosition="1281">the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we describe the parsing model used in our approach and briefly review the corresponding decoding algorithms. Following McDonald et al. (2005b), we propose to cast the problem of dependency parsing as a maximum weight spanning tree problem in directed graphs. 3.1 Edge-based factorization Let us start by setting up some notations. An input sentence of length n is represented by an n−uplet x = (x1, ..., xn). The dependency tree corresponding to that sentence is represented by a n x (n + 1) binary matrix y, such that yij = 1 if and only if the head of the token i is the </context>
</contexts>
<marker>Joulin, Tang, Fei-Fei, 2014</marker>
<rawString>Armand Joulin, Kevin Tang, and Li Fei-Fei. 2014. Efficient image and video co-localization with frankwolfe algorithm. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical report.</tech>
<contexts>
<context position="10462" citStr="Kasami, 1965" startWordPosition="1668" endWordPosition="1669"> to compute the maximum weight spanning tree over those sets. 3.2.1 Projective dependency trees First, we consider the set of projective spanning trees. A dependency tree is said to be projective if the dependencies do not cross when drawn above the words in linear order. Similarly, this means that word and all its descendants form a contiguous substring of the sentence. Projective dependency trees are thus strongly related to context free grammars, and it is possible to obtain the maximum weight spanning projective tree using a modified version of the CKY algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967). The complexity of this algorithm is O(n5). This led Eisner (1996) to propose an algorithm for projective parsing which has a complexity of O(n3). Similarly to CKY, the Eisner algorithm is based on dynamic programming, parsing a sentence in a bottom-up fashion. Finally, it should be noted that the dependency model with valence, on which most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch,</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Tadao Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1765" citStr="Klein and Manning (2004)" startWordPosition="266" endWordPosition="269">lot of attention, it is still considered to be an unsolved problem. In this work, we are interested in unsupervised dependency parsing. More precisely, our goal is to induce directed dependency trees, which capture binary syntactic relations between the words of a sentence. Since our method is unsupervised, it does not have access to such syntactic structure and only take as input a corpus of words and their associated parts of speech. Most recent approaches to unsupervised dependency parsing are based on probabilistic generative models, such as the dependency model with valence introduced by Klein and Manning (2004). Learning the parameters of such models is often done by maximizing the log-likelihood of unlabeled data, leading to a non-convex optimization problem. Thus, the performance of those methods rely heavily on the initialization, and practitioners have to find good heuristics to initialize their models. In this paper, we describe a different approach to the problem of dependency grammar induction, inspired by discriminative clustering. We propose to use a feature-rich discriminative parser, and to learn the parameters of this parser using a convex quadratic objective function. In particular, thi</context>
<context position="3932" citStr="Klein and Manning (2004)" startWordPosition="596" endWordPosition="599">onal Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1375–1384, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related work A lot of research has been carried out in the last decade on dependency grammar induction. We review the dependency model with valence, on which most unsupervised dependency parsers are based, before presenting different extensions and learning algorithms. Finally, we review discriminative clustering, on which our method is based. DMV. The dependency model with valence (DMV), introduced by Klein and Manning (2004), was the first method to outperform the baseline consisting in attaching each token to the next one. The DMV is a generative probabilistic model of the dependency tree and parts-of-speech of a sentence. It generates the root first, and then recursively generates the tokens down the tree. The probability of generating a new dependent for a given token depends on the direction (left or right) and whether a dependent was already generated in that direction. Then, the part-of-speech of the new dependent is generated according to a multinomial distribution conditioned on the direction and the head</context>
<context position="22771" citStr="Klein and Manning (2004)" startWordPosition="3891" endWordPosition="3894">ndency trees for ten languages belonging to five different families: Spanish, French, Italian, Portuguese (Romanic family), English, German, Swedish (Germanic family), Korean, Japanese and Indonesian. The tokens of those treebanks are tagged using the universal part-ofspeech tagset (Petrov et al., 2012). We focus on inducing dependency grammars using universal parts-of-speech, and will thus report results where all methods use (gold) universal POS. 6.3 Comparison with baselines We will compare our approach to three other unsupervised parsers. Our first baseline is the DMV model, introduced by Klein and Manning (2004). DMV PR USR OUR 7 min 1 h 15 h 2 min Table 4: Computational times required to learn a grammar on the English treebank. Our second baseline is the extended valence grammar model, with posterior sparsity constraints, as described by Gillenwater et al. (2011). Finally, our last baseline is the model with universal rules introduced by Naseem et al. (2010). It should be noted that these two baselines obtain performances that are near state-of-the-art. All methods are trained and tested on sentences of length 10 or less, after stripping punctuation. Parameter selection. All the parameters were chos</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="27687" citStr="Koo and Collins (2010)" startWordPosition="4698" endWordPosition="4701">dency parsing, we would like to explore the use of other features such as Brown clusters (Brown et al., 1992) or distributed word representations (Mikolov et al., 2013), in order to lexicalize our parser. Higher-order parsing. So far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based factorization by a higher-order one, in order to capture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by McDonald and Pereira (2006) and Koo and Collins (2010). Another potential approach would be to use the linear programming relaxed inference, described by Martins et al. (2009). Transfer learning. In this paper, we used universal syntactic rules, as described by Naseem et al. (2010) to guide our parser. We would like to explore the use of weak supervision, such as the one considered in transfer learning (Hwa et al., 2005). For example, projected dependencies from a resource-rich language could be used as constraints in our framework. Code. The code for our method is distributed on the first author webpage. Acknowledgments This work is supported by</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mareˇcek</author>
<author>Zdenˇek ˇZabokrtsk`y</author>
</authors>
<title>Exploiting reducibility in unsupervised dependency parsing.</title>
<date>2012</date>
<booktitle>In EMNLP/CoNLL.</booktitle>
<marker>Mareˇcek, ˇZabokrtsk`y, 2012</marker>
<rawString>David Mareˇcek and Zdenˇek ˇZabokrtsk`y. 2012. Exploiting reducibility in unsupervised dependency parsing. In EMNLP/CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Polyhedral outer approximations with application to natural language parsing.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="27808" citStr="Martins et al. (2009)" startWordPosition="4718" endWordPosition="4721">ed word representations (Mikolov et al., 2013), in order to lexicalize our parser. Higher-order parsing. So far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based factorization by a higher-order one, in order to capture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by McDonald and Pereira (2006) and Koo and Collins (2010). Another potential approach would be to use the linear programming relaxed inference, described by Martins et al. (2009). Transfer learning. In this paper, we used universal syntactic rules, as described by Naseem et al. (2010) to guide our parser. We would like to explore the use of weak supervision, such as the one considered in transfer learning (Hwa et al., 2005). For example, projected dependencies from a resource-rich language could be used as constraints in our framework. Code. The code for our method is distributed on the first author webpage. Acknowledgments This work is supported by National Science Foundation award 1344668 and National Institute of General Medical Sciences award R01 GM090187. 1382 10</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e FT Martins, Noah A Smith, and Eric P Xing. 2009. Polyhedral outer approximations with application to natural language parsing. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="27660" citStr="McDonald and Pereira (2006)" startWordPosition="4693" endWordPosition="4696">ch. Inspired by supervised dependency parsing, we would like to explore the use of other features such as Brown clusters (Brown et al., 1992) or distributed word representations (Mikolov et al., 2013), in order to lexicalize our parser. Higher-order parsing. So far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based factorization by a higher-order one, in order to capture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by McDonald and Pereira (2006) and Koo and Collins (2010). Another potential approach would be to use the linear programming relaxed inference, described by Martins et al. (2009). Transfer learning. In this paper, we used universal syntactic rules, as described by Naseem et al. (2010) to guide our parser. We would like to explore the use of weak supervision, such as the one considered in transfer learning (Hwa et al., 2005). For example, projected dependencies from a resource-rich language could be used as constraints in our framework. Code. The code for our method is distributed on the first author webpage. Acknowledgment</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8484" citStr="McDonald et al. (2005" startWordPosition="1305" endWordPosition="1308"> proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we describe the parsing model used in our approach and briefly review the corresponding decoding algorithms. Following McDonald et al. (2005b), we propose to cast the problem of dependency parsing as a maximum weight spanning tree problem in directed graphs. 3.1 Edge-based factorization Let us start by setting up some notations. An input sentence of length n is represented by an n−uplet x = (x1, ..., xn). The dependency tree corresponding to that sentence is represented by a n x (n + 1) binary matrix y, such that yij = 1 if and only if the head of the token i is the token j (and thus, the integer n + 1 represents the root of the tree). In this paper, we follow a common approach by factoring the score of dependency tree as the sum </context>
<context position="11184" citStr="McDonald et al. (2005" startWordPosition="1776" endWordPosition="1779">orithm for projective parsing which has a complexity of O(n3). Similarly to CKY, the Eisner algorithm is based on dynamic programming, parsing a sentence in a bottom-up fashion. Finally, it should be noted that the dependency model with valence, on which most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a significant number of non-projective edges. In the context of supervised dependency parsing, McDonald et al. (2005b) shown that using non-projective trees improves the accuracy of dependency parsers for those languages. The maximum weight spanning tree in a directed graph can be computed using the ChuLiu/Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which has a complexity of O(n3). Later, Tarjan (1977) proposed an improved version of this algorithm for dense graphs, whose complexity is O(n2), the same as for undirected graphs using Prim’s algorithm. Thus a second advantage of using non-projective dependency trees is the fact that it leads to more efficient parsers. 4 Learning the parameter vector </context>
<context position="21675" citStr="McDonald et al. (2005" startWordPosition="3721" endWordPosition="3724">2.3 ES 31.8 57.3 71.5 68.8 FR 56.0 66.2 54.1 72.3 ID 44.9 21.4 50.3 69.7 IT 33.3 40.4 46.5 64.3 JA 48.0 58.9 58.2 57.5 KO 35.3 50.7 48.8 59.0 PT-BR 49.6 40.7 46.4 68.3 SV 38.9 61.2 64.3 66.2 AVG 40.2 51.3 56.0 64.8 Table 3: Directed dependency accuracy, on the universal treebanks with universal parts-ofspeech, on sentences of length 10 or less. PR refers to posterior regularization, USR to universal rules. 6.1 Features The features used in our unsupervised parser are based on the parts-of-speech of the head and the dependent of the corresponding dependency, and are given in Table 2. Following McDonald et al. (2005a), we also include features capturing the context of the head or the dependent. These features are trigrams and are formed by the partsof-speech of the two tokens of the dependency and one of the word appearing before/after the head/dependent. Finally, all the features are conjoined with the signed distance between the two words of the dependency. 6.2 Dataset We use the universal treebanks, version 2.0, introduced by McDonald et al. (2013). This dataset contains dependency trees for ten languages belonging to five different families: Spanish, French, Italian, Portuguese (Romanic family), Engl</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith B Hall, Slav Petrov,</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<location>Hao Zhang, Oscar T¨ackstr¨om, et</location>
<marker>McDonald, Nivre, 2013</marker>
<rawString>Ryan T McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, et al. 2013. Universal dependency annotation for multilingual parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="27233" citStr="Mikolov et al., 2013" startWordPosition="4622" endWordPosition="4625">.8 55.8 Table 6: Feature ablation study. the universal treebanks that our approach leads to competitive results, while being computationaly very efficient. We now describe some directions we would like to explore as future work. Richer feature set. In our experiments, we focused on assessing the usefulness of our convex, discriminative approach, and thus considered only relatively simple features based on parts-ofspeech. Inspired by supervised dependency parsing, we would like to explore the use of other features such as Brown clusters (Brown et al., 1992) or distributed word representations (Mikolov et al., 2013), in order to lexicalize our parser. Higher-order parsing. So far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based factorization by a higher-order one, in order to capture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by McDonald and Pereira (2006) and Koo and Collins (2010). Another potential approach would be to use the linear programming relaxed inference, described by Martins et al. (2009). Transfer learning. In t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2481" citStr="Naseem et al. (2010)" startWordPosition="377" endWordPosition="380">eled data, leading to a non-convex optimization problem. Thus, the performance of those methods rely heavily on the initialization, and practitioners have to find good heuristics to initialize their models. In this paper, we describe a different approach to the problem of dependency grammar induction, inspired by discriminative clustering. We propose to use a feature-rich discriminative parser, and to learn the parameters of this parser using a convex quadratic objective function. In particular, this approach also allows us to induce non-projective dependency structures. Following the work of Naseem et al. (2010), we use language-independent rules between pairs of parts-of-speech to guide our parser. More precisely, we make the following contributions: • Our method is based on a feature-rich discriminative parser (section 3); • Learning the parameters of our parser is achieved using a convex objective, and is thus not sensitive to initialization (section 4); • Our method can produce non-projective dependency structures (section 3.2.2); • We propose an efficient algorithm to optimize the objective, based on the Frank-Wolfe method (section 5); • We evaluate our approach on the universal treebanks datase</context>
<context position="5477" citStr="Naseem et al. (2010)" startWordPosition="842" endWordPosition="845">ameters. Mareˇcek and ˇZabokrtsk`y (2012) described an approach using a n-gram reducibility measure, which capture which words can be deleted from a sentence without making it syntactically incorrect. Cohen and Smith (2009) introduced a prior, based on the shared logistic normal distribution. This prior allowed to tie the grammar parameters corresponding to different POS belonging to the same coarse groups, such as all the POS corresponding to verbs. Berg-Kirkpatrick and Klein (2010) proposed to tie the parameters of grammars for different languages using a prior based on a phylogenetic tree. Naseem et al. (2010) proposed a set of rules between parts-of-speech, encoding syntactic universals, such as the fact that adjectives are often dependents of nouns. They used posterior regularization (Ganchev et al., 2010) to impose that a certain amount of the infered dependencies verifies one of these rules. Also using posterior regularization, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, Blunsom and Cohn (2010) reformulated dependency grammar induction using tree substitution grammars, while Bisk and Hockenmaier (20</context>
<context position="15503" citStr="Naseem et al. (2010)" startWordPosition="2573" endWordPosition="2576"> −y&gt;d yc, whose solution is obtained using the minimum weight spanning tree algorithm. It should be noted that the rounding solution is not necessarily the optimal solution of the integer problem. 1 n min w n i=1 min y∈T min y∈T min w min y∈Y min w min yd∈T 1378 Algorithm 1: Frank-Wolfe algorithm ..., do Compute the gradient: 9t = Solve the li {1, T} ∇f(zt) near program: sT9t =min st sED Take the Frank-Wolfe step: zt+1 = rytst + (1 − ryt)zt end Figure 3: Illustration of a Frank-Wolfe step. 4.4 Prior on y We now describe how to guide our unsupervised parser, by using universal rules. Following Naseem et al. (2010), we want a certain percentage of the infered dependencies to satisfy one of the twelve universal syntactic rules, listed in Table 1. Let S be the set of indices corresponding to word pairs that satisfy one of these rules. Then, imposing that a certain percentage c of dependencies satisfy one of those rules can be obtained by imposing the constraint: 1 n� iES minimum weight spanning tr ee algorithm. Instead we propose to use the Frank-Wolfe algorithm, that we now describe. set D. It is an iterative first-order optimization method. At each iteration t, the convex function f is approximated by a</context>
<context position="23125" citStr="Naseem et al. (2010)" startWordPosition="3953" endWordPosition="3956">rsal parts-of-speech, and will thus report results where all methods use (gold) universal POS. 6.3 Comparison with baselines We will compare our approach to three other unsupervised parsers. Our first baseline is the DMV model, introduced by Klein and Manning (2004). DMV PR USR OUR 7 min 1 h 15 h 2 min Table 4: Computational times required to learn a grammar on the English treebank. Our second baseline is the extended valence grammar model, with posterior sparsity constraints, as described by Gillenwater et al. (2011). Finally, our last baseline is the model with universal rules introduced by Naseem et al. (2010). It should be noted that these two baselines obtain performances that are near state-of-the-art. All methods are trained and tested on sentences of length 10 or less, after stripping punctuation. Parameter selection. All the parameters were chosen using the English development set. Our method has two parameters, determined as: A = 0.001 and p = 0.1. We used T = 200 iterations in all the experiments. Discussion. We report the results in Table 3. First, we observe that our method performs better than the three baselines on seven out of ten languages. Overall, our approach outperforms the three </context>
<context position="27915" citStr="Naseem et al. (2010)" startWordPosition="4736" endWordPosition="4739">far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based factorization by a higher-order one, in order to capture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by McDonald and Pereira (2006) and Koo and Collins (2010). Another potential approach would be to use the linear programming relaxed inference, described by Martins et al. (2009). Transfer learning. In this paper, we used universal syntactic rules, as described by Naseem et al. (2010) to guide our parser. We would like to explore the use of weak supervision, such as the one considered in transfer learning (Hwa et al., 2005). For example, projected dependencies from a resource-rich language could be used as constraints in our framework. Code. The code for our method is distributed on the first author webpage. Acknowledgments This work is supported by National Science Foundation award 1344668 and National Institute of General Medical Sciences award R01 GM090187. 1382 10 15 20 30 all 75 70 65 60 55 50 sv en de 10 15 20 30 all 75 70 65 60 55 50 es fr pt it 10 15 20 30 all 75 7</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankur P Parikh</author>
<author>Shay B Cohen</author>
<author>Eric P Xing</author>
</authors>
<title>Spectral unsupervised parsing with additive tree metrics.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6997" citStr="Parikh et al. (2014)" startWordPosition="1075" endWordPosition="1078">on of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for unsupervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et </context>
</contexts>
<marker>Parikh, Cohen, Xing, 2014</marker>
<rawString>Ankur P Parikh, Shay B Cohen, and Eric P Xing. 2014. Spectral unsupervised parsing with additive tree metrics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<publisher>LREC.</publisher>
<contexts>
<context position="22451" citStr="Petrov et al., 2012" startWordPosition="3842" endWordPosition="3845"> tokens of the dependency and one of the word appearing before/after the head/dependent. Finally, all the features are conjoined with the signed distance between the two words of the dependency. 6.2 Dataset We use the universal treebanks, version 2.0, introduced by McDonald et al. (2013). This dataset contains dependency trees for ten languages belonging to five different families: Spanish, French, Italian, Portuguese (Romanic family), English, German, Swedish (Germanic family), Korean, Japanese and Indonesian. The tokens of those treebanks are tagged using the universal part-ofspeech tagset (Petrov et al., 2012). We focus on inducing dependency grammars using universal parts-of-speech, and will thus report results where all methods use (gold) universal POS. 6.3 Comparison with baselines We will compare our approach to three other unsupervised parsers. Our first baseline is the DMV model, introduced by Klein and Manning (2004). DMV PR USR OUR 7 min 1 h 15 h 2 min Table 4: Computational times required to learn a grammar on the English treebank. Our second baseline is the extended valence grammar model, with posterior sparsity constraints, as described by Gillenwater et al. (2011). Finally, our last bas</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vignesh Ramanathan</author>
<author>Armand Joulin</author>
<author>Percy Liang</author>
<author>Li Fei-Fei</author>
</authors>
<title>Linking people with ”their” names using coreference resolution.</title>
<date>2014</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="8078" citStr="Ramanathan et al., 2014" startWordPosition="1240" endWordPosition="1243">jective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based on the corresponding scripts (Ramanathan et al., 2014). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with 1376 videos (Bojanowski et al., 2014; Bojanowski et al., 2015) or to co-localize objects in videos (Joulin et al., 2014). 3 Model In this section, we describe the parsing model used in our approach and briefly review the corresponding decoding algorithms. Following McDonald et al. (2005b), we propose to cast the problem of dependency parsing as a maximum weight spanning tree problem in directed graphs. 3.1 Edge-based factorization Let us start by setting up some notations. An </context>
</contexts>
<marker>Ramanathan, Joulin, Liang, Fei-Fei, 2014</marker>
<rawString>Vignesh Ramanathan, Armand Joulin, Percy Liang, and Li Fei-Fei. 2014. Linking people with ”their” names using coreference resolution. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI Workshop on Grammatical Inference Applications.</booktitle>
<contexts>
<context position="6283" citStr="Smith and Eisner (2005)" startWordPosition="965" endWordPosition="968">nchev et al., 2010) to impose that a certain amount of the infered dependencies verifies one of these rules. Also using posterior regularization, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, Blunsom and Cohn (2010) reformulated dependency grammar induction using tree substitution grammars, while Bisk and Hockenmaier (2013) proposed to use combinatory categorial grammars. Learning. Different algorithms have been proposed to improve the learning of the parameters of the dependency model with valence. Smith and Eisner (2005) proposed to use constrastive estimation to learn the parameters of a log-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A Smith and Jason Eisner. 2005. Guiding unsupervised grammar induction using contrastive estimation. In Proc. of IJCAI Workshop on Grammatical Inference Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: How less is more in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="6421" citStr="Spitkovsky et al. (2010" startWordPosition="987" endWordPosition="990">zation, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, Blunsom and Cohn (2010) reformulated dependency grammar induction using tree substitution grammars, while Bisk and Hockenmaier (2013) proposed to use combinatory categorial grammars. Learning. Different algorithms have been proposed to improve the learning of the parameters of the dependency model with valence. Smith and Eisner (2005) proposed to use constrastive estimation to learn the parameters of a log-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral met</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010a. From baby steps to leapfrog: How less is more in unsupervised dependency parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="6421" citStr="Spitkovsky et al. (2010" startWordPosition="987" endWordPosition="990">zation, Gillenwater et al. (2011) imposed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, Blunsom and Cohn (2010) reformulated dependency grammar induction using tree substitution grammars, while Bisk and Hockenmaier (2013) proposed to use combinatory categorial grammars. Learning. Different algorithms have been proposed to improve the learning of the parameters of the dependency model with valence. Smith and Eisner (2005) proposed to use constrastive estimation to learn the parameters of a log-linear parametrization of the DMV, while Spitkovsky et al. (2010b) showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral met</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D Manning. 2010b. Viterbi training improves unsupervised dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Breaking out of local optima with count transforms and model recombination: A study in grammar induction.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7113" citStr="Spitkovsky et al. (2013)" startWordPosition="1090" endWordPosition="1093">r accuracy. Observing that learning from shorter sentences is easier (because less ambiguous), Spitkovsky et al. (2010a) presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for unsupervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2013</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking out of local optima with count transforms and model recombination: A study in grammar induction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Endre Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks.</journal>
<contexts>
<context position="11483" citStr="Tarjan (1977)" startWordPosition="1826" endWordPosition="1827">based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a significant number of non-projective edges. In the context of supervised dependency parsing, McDonald et al. (2005b) shown that using non-projective trees improves the accuracy of dependency parsers for those languages. The maximum weight spanning tree in a directed graph can be computed using the ChuLiu/Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which has a complexity of O(n3). Later, Tarjan (1977) proposed an improved version of this algorithm for dense graphs, whose complexity is O(n2), the same as for undirected graphs using Prim’s algorithm. Thus a second advantage of using non-projective dependency trees is the fact that it leads to more efficient parsers. 4 Learning the parameter vector In this section, we describe the loss function we use to learn the parameter vector w from unlabeled sentences. 4.1 Problem formulation From now on, y is a vector representing the dependency trees corresponding to the whole corpus. Thus, each index i corresponds to a potential dependency between tw</context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>Robert Endre Tarjan. 1977. Finding optimum branchings. Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linli Xu</author>
<author>James Neufeld</author>
<author>Bryce Larson</author>
<author>Dale Schuurmans</author>
</authors>
<title>Maximum margin clustering.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7423" citStr="Xu et al. (2004)" startWordPosition="1134" endWordPosition="1137">g in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved dependency accuracies. Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for unsupervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced different heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, introduced by Xu et al. (2004). Given a set of points, the objective of discriminative clustering is to assign labels to these points that can be easily predicted using a discriminative classifier. Xu et al. (2004) introduced a formulation using the hinge loss, Bach and Harchaoui (2007) proposed to use the squared loss instead, while Joulin et al. (2010) proposed a formulation based on the logistic loss. Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction (Grave, 2014) and for the problem of finding the names of characters in TV series based</context>
<context position="12507" citStr="Xu et al. (2004)" startWordPosition="2001" endWordPosition="2004">ces. 4.1 Problem formulation From now on, y is a vector representing the dependency trees corresponding to the whole corpus. Thus, each index i corresponds to a potential dependency between two words of a given sentence. 1377 He gave a seminar yesterday about unsupervised dependency parsing Figure 2: Example of a non-projective dependency tree in english. Like before, yi = 1 if and only if there is a dependency between those two words, and yi = 0 otherwise. The set of dependencies that form valid trees is denoted by the set T . Inspired by the discriminative clustering framework introduced by Xu et al. (2004), our goal is to jointly find the dependencies represented by the vector y and the parameter vector w which minimize the regularized empirical risk `(yi, w&gt;xi) + λΩ(w), (1) where ` is a loss function and Ω is a regularizer. The intuition is that we want to find the dependency trees y that can be easily predicted by a discriminative parser, whose parameters are w. Following Bach and Harchaoui (2007), we propose to use the squared loss ` defined by `(y, ˆy) = 2(y − ˆy)2 1 and to use the `2-norm as a regularizer. In that case, we obtain the objective function: 2n�y − Xw�2 1 2 + λ2IIwII22. (2) One</context>
</contexts>
<marker>Xu, Neufeld, Larson, Schuurmans, 2004</marker>
<rawString>Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. 2004. Maximum margin clustering. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3. Information and control.</title>
<date>1967</date>
<contexts>
<context position="10478" citStr="Younger, 1967" startWordPosition="1670" endWordPosition="1671">e maximum weight spanning tree over those sets. 3.2.1 Projective dependency trees First, we consider the set of projective spanning trees. A dependency tree is said to be projective if the dependencies do not cross when drawn above the words in linear order. Similarly, this means that word and all its descendants form a contiguous substring of the sentence. Projective dependency trees are thus strongly related to context free grammars, and it is possible to obtain the maximum weight spanning projective tree using a modified version of the CKY algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967). The complexity of this algorithm is O(n5). This led Eisner (1996) to propose an algorithm for projective parsing which has a complexity of O(n3). Similarly to CKY, the Eisner algorithm is based on dynamic programming, parsing a sentence in a bottom-up fashion. Finally, it should be noted that the dependency model with valence, on which most approaches to dependency grammar induction are based, produces projective dependency trees. 3.2.2 Non-projective dependency trees Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a signific</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and control.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>