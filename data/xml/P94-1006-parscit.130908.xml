<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<note confidence="0.417025">
INTENTIONS AND INFORMATION IN DISCOURSE
</note>
<author confidence="0.86454">
Nicholas Asher
</author>
<affiliation confidence="0.893264">
IRIT, Universite Paul Sabatier,
</affiliation>
<address confidence="0.911449">
118 Route de Narbonne,
31062 Toulouse, CEDEX,
France
</address>
<email confidence="0.939418">
ashereirit.fr
</email>
<sectionHeader confidence="0.996485" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903714285714">
This paper is about the flow of inference between com-
municative intentions, discourse structure and the do-
main during discourse processing. We augment a the-
ory of discourse interpretation with a theory of distinct
mental attitudes and reasoning about them, in order to
provide an account of how the attitudes interact with
reasoning about discourse structure.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99106028125">
The flow of inference between communicative intentions
and domain information is often essential to discourse
processing. It is well reflected in this discourse from
Moore and Pollack (1992):
(1)a. George Bush supports big business.
b. He&apos;s sure to veto House Bill 1711.
There are at least three different interpretations. Con-
sider Context 1: in this context the interpreter I be-
lieves that the author A wants to convince him that
(lb) is true. For example, the context is one in which
I has already uttered Bush won&apos;t veto any more bills.
I reasons that A&apos;s linguistic behavior was intentional,
and therefore that A believes that by saying (la) he
will convince I that Bush will veto the bill. Even if I
believed nothing about the bill, he now infers it&apos;s bad
for big business. So we have witnessed an inference
from premises that involve the desires and beliefs of A
(Moore and Pollack&apos;s &amp;quot;intentional structure&amp;quot;), as well
as his linguistic behavior, to a conclusion about domain
information (Moore and Pollack&apos;s &amp;quot;informational struc-
ture&amp;quot;).
Now consider Context 2: in this context I knows that
A wants to convince him of (la). As in Context 1, I
may infer that the bill is bad for big business. But now,
(lb) is used to support (la).
Finally, consider Context 3: in this context I knows
that House Bill 1711 is bad for big business, but doesn&apos;t
know A&apos;s communicative desires prior to witnessing
his linguistic behaviour. From his beliefs about the
domain, he infers that supporting big business would
cause Bush to veto this bill. So, A must have uttered
(la) to support (lb). Hence I realises that A wanted
</bodyText>
<author confidence="0.451963">
Alex Lascarides
</author>
<affiliation confidence="0.7239825">
Department of Linguistics,
Stanford University,
</affiliation>
<address confidence="0.526801666666667">
Stanford,
Ca 94305-2150,
USA,
</address>
<email confidence="0.570081">
alexecsli.stanford.edu
</email>
<bodyText confidence="0.999892586206896">
him to believe (lb). So in contrast to Contexts 1 and 2,
we have a flow of inference from informational structure
to intentional structure.
This story makes two main points. First, we agree
with Moore and Pollack that we must represent both
the intentional import and the informational import
of a discourse. As they show, this is a problem for
current formulations of Rhetorical Structure Theory
(RsT) (Thompson and Mann, 1987). Second, we go
further than Moore and Pollack, and argue that rea-
soning about beliefs and desires exploits different rules
and axioms from those used to infer rhetorical relations.
Thus, we should represent intentional structure and dis-
course structure separately. But we postulate rhetorical
relations that express the discourse function of the con-
stituents in the communicative plan of the author, and
we permit interaction between reasoning about rhetor-
ical relations and reasoning about beliefs and desires.
This paper provides the first steps towards a formal
analysis of the interaction between intentional struc-
ture and informational structure. Our framework for
discourse structure analysis is SDRT (Asher 1993). The
basic representational structures of that theory may be
used to characterise cognitive states. We will extend the
logical engine used to infer rhetorical relations—DICE
(Lascarides and Asher 1991, 1993a, 1993b, Lascarides
and Oberlander 1993)—to model inferences about in-
tentional structure and its interaction with informa-
tional structure.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="method">
BUSH&apos;S REQUIREMENTS
</sectionHeader>
<bodyText confidence="0.999963">
We must represent both the intentional import and
the informational import of a discourse simultaneously.
So we need a theory of discourse structure where dis-
course relations central to intentional import and to
informational import can hold simultaneously between
the same constituents. A logical framework in which all
those plausible relations between constituents that are
consistent with each other are inferred, such as a non-
monotonic logic like that in DICE (Lascarides and Asher,
1993a), would achieve this. So conceivably, a similar
nonmonotonic logic for RST might solve the problem
of keeping track of the intentional and informational
</bodyText>
<page confidence="0.997469">
34
</page>
<bodyText confidence="0.946880125">
structure simultaneously.
But this would work only if the various discourse rela-
tions about intentions and information could simultane-
ously hold in a consistent knowledge base (KB). Moore
and Pollack (1992) show via discourse (2) that the cur-
rent commitment to the nucleus-satellite distinction in
RST precludes this.
(2)a. Let&apos;s go home by 5.
</bodyText>
<listItem confidence="0.984902666666667">
b. Then we can get to the hardware store
before it closes.
c. That way we can finish the bookshelves tonight.
</listItem>
<bodyText confidence="0.991749882352941">
From an intentional perspective, (2b) is a satellite to
(2a) via Motivation. From an informational perspec-
tive, (2a) is a satellite to (2b) via Condition. These
two structures are incompatible. So augmenting RST
with a nonmonotonic logic for inferring rhetorical rela-
tions would not yield a representation of (2) on multiple
levels in which both intentional and informational re-
lations are represented. In SDRT, on the other hand,
not all discourse relations induce subordination, and
so there is more scope for different discourse relations
holding simultaneously in a consistent KB.
Grosz and Sidner&apos;s (1986) model of discourse inter-
pretation is one where the same discourse elements are
related simultaneously on the informational and inten-
tional levels. But using their framework to model (1) is
not straightforward. As Grosz and Sidner (1990) point
out: &amp;quot;any model (or theory) of the communication sit-
uation must distinguish among beliefs and intentions
of different agents,&amp;quot; but theirs does not. They repre-
sent intentional structure as a stack of propositions, and
different attitudes aren&apos;t distinguished. The informal
analysis of (1) above demands such distinctions, how-
ever. For example, analysing (1) under Context 3 re-
quires a representation of the following statement: since
A has provided a reason why (lb) is true, he must want
I to believe that (lb) is true. It&apos;s unclear how Grosz
and Sidner would represent this. SDRT (Asher, 1993) is
in a good position to be integrated with a theory of cog-
nitive states, because it uses the same basic structures
(discourse representation structures or DRss) that have
been used in Discourse Representation Theory (DRT)
to represent different attitudes like beliefs and desires
(Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and
Singh, 1993).
</bodyText>
<sectionHeader confidence="0.997657" genericHeader="method">
A BRIEF INTRODUCTION TO
SDRT AND DICE
</sectionHeader>
<bodyText confidence="0.97962975">
In SDRT (Asher, 1993), an NL text is represented by a
segmented DRS (SDRS), which is a pair of sets contain-
ing: the DRss or sDRss representing respectively sen-
tences and text segments, and discourse relations be-
tween them. Discourse relations, modelled after those
proposed by Hobbs (1985), Polanyi (1985) and Thomp-
son and Mann (1987), link together the constituents of
an SDRS. We will mention three: Narration, Result and
Evidence.
. SDRSS have a hierarchical configuration, and SDRT
predicts points of attachment in a discourse structure
for new information. Using DICE we infer from the
reader&apos;s knowledge resources which discourse relation
should be used to do attachment.
Lascarides and Asher (1991) introduce default rules
representing the role of Gricean pragmatic maxims and
domain knowledge in calculating the value of the up-
date function (7, a, /3), which means &amp;quot;the representation
of the current sentence is to be attached to a with a
discourse relation, where a is an open node in the repre-
sentation T of the text so far&amp;quot;. Defaults are represented
by a conditional—q&gt; tk means &apos;if 0, then normally 0.
For example, Narration says that by default Narration
relates elements in a text.
</bodyText>
<listItem confidence="0.996572">
• Narration: (7, a , )3) &gt; Narration(a, (3)
</listItem>
<bodyText confidence="0.999937619047619">
Associated axioms show how Narration affects the tem-
poral order of the events described: Narration and the
corresponding temporal axioms on Narration predict
that normally the textual order of events matches their
temporal order.
The logic on which DICE rests is Asher and Mor-
reau&apos;s (1991) Commonsense Entailment (CE). Two pat-
terns of nonmonotonic inference are particularly rele-
vant here. The first is Defeasible Modus Ponens: if one
default rule has its antecedent verified, then the con-
sequent is nonmonotonically inferred. The second is
the Penguin Principle: if there are conflicting default
rules that apply, and their antecedents are in logical
entailment relations, then the consequent of the rule
with the most specific antecedent is inferred. Lascarides
and Asher (1991) use DICE to yield the discourse struc-
tures and temporal structures for simple discourses.
But the theory has so far ignored how A&apos;s intentional
structure—or more accurately, I&apos;s model of A&apos;s inten-
tional structure—influences I&apos;s inferences about the do-
main and the discourse structure.
</bodyText>
<sectionHeader confidence="0.998241" genericHeader="method">
ADDING INTENTIONS
</sectionHeader>
<bodyText confidence="0.999947611111111">
To discuss intentional structure, we develop a language
which can express beliefs, intentions and desires. Fol-
lowing Bratman (forthcoming) and Asher and Singh
(1993), we think of the objects of attitudes either as
plans or as propositions. For example, the colloquial
intention to do something—like wash the dishes—will
be expressed as an intention toward a plan, whereas
the intention that Sue be happy is an intention toward
a proposition. Plans will just consist of sequences of ba-
sic actions ai ; a2; .;a,. Two operators-1Z for about
to do or doing, and 1, for having done—will convert ac-
tions into propositions. The attitudes we assume in our
model are believes (5,40 means &apos;A believes 0&apos;), wants
(WAO means &apos;A wants 0&apos;), and intends (IAq5 means
&apos;A intends 0&apos;). All of this takes place in a modal, dy-
namic logic, where the propositional attitudes are sup-
plied with a modal semantics. To this we add the modal
conditional operator &gt;, upon which the logic of DICE is
</bodyText>
<page confidence="0.995525">
35
</page>
<bodyText confidence="0.999710586206897">
based.
Let&apos;s take a closer look at (1) in Context 1. Let the
logical forms of the sentences (la) and (lb) be respec-
tively a and [3. In Context 1, I believes that A wants
to convince him of # and thinks that he doesn&apos;t believe
0 already. Following the DRT analysis of attitudes, we
assume I&apos;s cognitive state has embedded in it a model
of A&apos;s cognitive state, which in turn has a represen-
tation of I&apos;s cognitive state. So WA/3//3 and BA -130
hold in I&apos;s KB. Furthermore, (r, a, 0) A In fo(a , ,3) holds
in Ps Ks, where Info(a03) is a gloss for the seman-
tic content of a and # that I knows about.1 I must
now reason about what A intended by his particular
discourse action. I is thus presented with a classical
reasoning problem about attitudes: how to derive what
a person believes, from a knowledge of what he wants
and an observation of his behaviour. The classic means
of constructing such a derivation uses the practical syl-
logism, a form of reasoning about action familiar since
Aristotle. It expresses the following maxim: Act so as
to realize your goals ceteris paribus.
The practical syllogism is a rule of defeasible reason-
ing, expressible in CE by means of the nonmonotonic
consequence relation k. The consequence relation 4*0
can be stated directly in the object language of CE by
a formula which we abbreviate as 1(0, 0) (Asher 1993).
We use 1(0, 0) to state the practical syllogism. First,
we define the notion that the KB and 0, but not the KB
alone, nonmonotonically yield 0:
</bodyText>
<listItem confidence="0.998525">
• Definition:
</listItem>
<equation confidence="0.712683">
14b(0 0) 4-41(K B A 0, 0) A 1(K B 7,b)
</equation>
<bodyText confidence="0.99868975">
The Practical Syllogism says that if (a) A wants 0 but
believes it&apos;s not true, and (b) he knows that if 0 were
added to his KB it would by default make 0 true even-
tually, then by default A intends 0.
</bodyText>
<listItem confidence="0.963382">
• The Practical Syllogism:
(a) (WA (0) A BA (-10)A
(b) BA(1kb(0, eventually(0)))) &gt;
(c) IA(b)
</listItem>
<bodyText confidence="0.999174153846154">
The Practical Syllogism enables] to reason about A&apos;s
cognitive state. In Context 1, when substituting in the
Practical Syllogism 5/0 for 0, and (r, a, 0) A In fo(a , /3)
for 0, we find that clause (a) of the antecedent to the
Practical Syllogism is verified. The conclusion (c) is
also verified, because I assumes that A&apos;s discourse act
was intentional. This assumption could be expressed
explicitly as a &gt;-rule, but we will not do so here.
Now, abduction (i.e., explanatory reasoning) as well
as nonmonotonic deduction is permitted on the Prac-
tical Syllogism. So from knowing (a) and (c), I can
conclude the premise (b). We can state in CE an &apos;ab-
ductive&apos; rule based on the Practical Syllogism:
</bodyText>
<listItem confidence="0.974726333333333">
• The Abductive Practical Syllogism 1(APs1)
(WA(4,) A BA(--,4,) A IA(0)) &gt;
BA(46(0, eventually(0)))
</listItem>
<bodyText confidence="0.988194305555556">
1This doesn&apos;t necessarily include that House Bill 1711 is
bad for big business.
APs1 allows us to conclude (b) when (a) and (c) of
the Practical Syllogism hold. So, the intended action
0 must be one that A believes will eventually make 0
true.
When we make the same substitutions for 0 and
0 in APs1 as before, I will infer the conclusion of
APs1 via Defeasible Modus Ponens: BA(1kb((r, a, (3) A
Info(a, )3), eventually(Bn3))). That is, I infers that A
believes that, by uttering what he did, I will come to
believe /3.
In general, there may be a variety of alternatives that
we could use to substitute for 0 and 0 in APs1, in a
given situation. For usually, there are choices on what
can be abduced. The problem of choice is one that
Hobbs et al. (1990) address by a complex weighting
mechanism. We could adopt this approach here.
The Practical Syllogism and APS1 differ in two impor-
tant ways from the DICE axioms concerning discourse
relations. First, APs1 is motivated by an abductive
line of reasoning on a pattern of defeasible reasoning
involving cognitive states. The DICE axioms are not.
Secondly, both the Practical Syllogism and APs1 don&apos;t
include the discourse update function (r, a, g) together
with some information about the semantic content of a
and 0 in the antecedent, while this is a standard feature
of the DICE axioms for inferring discourse structure.
These two differences distinguish reasoning about in-
tentional structures and discourse structures. But dis-
course structure is linked to intentional structure in the
following way. The above reasoning with A&apos;s cognitive
state has led I to conclusions about the discourse func-
tion of a. Intuitively, a was uttered to support 0, or
a &apos;intentionally supports&apos; 0. This idea of intentional
support is defined in DICE as follows:
</bodyText>
<listItem confidence="0.94775">
• Intends to Support:
</listItem>
<bodyText confidence="0.980886">
Isupport(a, )3) (W A(Bip) A BA(-,8113) A
BA(1kbh((r, a, 13)AInfo(a, 13), eventually(B1 O))))
In words, a intentionally supports 0 if and only if A
wants I to believe # and doesn&apos;t think he does so al-
ready, and he also believes that by uttering a and )3
together, so that I is forced to reason about how they
should be attached with a rhetorical relation, I will
come to believe )3.
Isupport(a, #) defines a relationship between a and g
at the discourse structural level, in terms of I&apos;s and A&apos;s
cognitive states. With it we infer further information
about the particular discourse relation that I should
use to attach 13 to a. Isupport(a, )3) provides the link
between reasoning about cognitive states and reasoning
about discourse structure.
Let us now return to the interpretation of (1) under
Context 1. I concludes Isupport(a, )3), because the right
hand side of the 4--condition in Intends to Support is
satisfied. So I passes from a problem of reasoning about
A&apos;s intentional structure to one of reasoning about dis-
course structure. Now, I should check to see whether
a actually does lead him to believe 0. This is a check
on the coherence of discourse; in order for an SDRS r to
</bodyText>
<page confidence="0.991641">
36
</page>
<bodyText confidence="0.999337714285714">
be coherent, the discourse relations predicated of the
constituents must be satisfiable.2 Here, this amounts
to justifying A&apos;s belief that given the discourse context
and I&apos;s background beliefs of which A is aware, I will
arrive at the desired conclusion—that he believes )3. So,
I must be able to infer a particular discourse relation R
between a and )3 that has what we will call the Belief
Property: (B1a A R(a, 13)) &gt; 13113. That is, R must be
a relation that would indeed license I&apos;s concluding /3
from a.
We concentrate here for illustrative purposes on
two discourse relations with the Belief Property:
Result(a, 0) and Evidence(a, 0); or in other words, a
results in )3, or a is evidence for 13.
</bodyText>
<listItem confidence="0.998078">
• Relations with the Belief Property:
(Bra A Evidence(a, p)) &gt; 13/3
(131a A Result(a, 13)) &gt; 5/9
</listItem>
<bodyText confidence="0.86477875">
The following axiom of Cooperation captures the
above reasoning on l&apos;s part: if a Isupports )3, then it
must be possible to infer from the semantic content,
that either Result(a, 0) or Evidence(a, 13) hold:
</bodyText>
<listItem confidence="0.997311">
• Cooperation:
</listItem>
<equation confidence="0.918806666666667">
(Isupport(a, )3) A (r, a, )3))
(1kb((r, a, )3) A Info(a , Result(a, 0&apos;))V
Ikb((r, a, )3) A Info(a, )3), Evidence(a ,13)))
</equation>
<bodyText confidence="0.990273444444445">
The intentional structure of A that I has inferred has
restricted the candidate set of discourse relations that
I can use to attach 13 to a: he must use Result or Evi-
dence, or both. If I can&apos;t accommodate A&apos;s intentions
by doing this, then the discourse will be incoherent.
We&apos;ll shortly show how Cooperation contributes to the
explanation of why (3) is incoherent.
(3)a. George Bush is a weak-willed president.
b. ?He&apos;s sure to veto House Bill 1711.
</bodyText>
<sectionHeader confidence="0.993710333333333" genericHeader="method">
FROM INTENTIONS TO
INFORMATION:
CONTEXTS 1 AND 2
</sectionHeader>
<bodyText confidence="0.932600459459459">
The axioms above allow I to use his knowledge of A&apos;s
cognitive state, and the behaviour of A that he observes,
to (a) infer information about A&apos;s communicative inten-
tions, and (b) consequently to restrict the set of candi-
date discourse relations that are permitted between the
constituents. According to Cooperation, I must infer
that one of the permitted discourse relations does in-
deed hold. When clue words are lacking, the semantic
content of the constituents must be exploited. In cer-
tain cases, it&apos;s also necessary to infer further informa-
tion that wasn&apos;t explicitly mentioned in the discourse,
2Asher (1993) discusses this point in relation to Con-
trast: the discourse marker but is used coherently only if the
semantic content of the constituents it connects do indeed
form a contrast: compare Mary&apos;s hair is black but her eyes
are blue, with ?Mary&apos;s hair is black but John&apos;s hair is black.
in order to sanction the discourse relation. For exam-
ple, in (1) in Contexts 1 and 2, I infers the bill is bad
for big business.
Consider again discourse (1) in Context 1. Intu-
itively, the reason we can infer Result(a, 13) in the anal-
ysis of (1) is because (i) a entails a generic (Bush vetoes
bills that are bad for big business), and (ii) this generic
makes )3 true, as long as we assume that House Bill
1711 is bad for big business.
To define the Result Rule below that captures this
reasoning for discourse attachment, we first define this
generic-instance relationship: instance(cb, 0) holds just
in case ,(b is (V x)(A(x) &gt; B(x)) and tk is A[x I cl] A B[x I di.
For example, bird(tweety) A fly(tweety) (Tweety is a bird
and Tweety flies) is an instance of V x(bird(x) &gt; fly(x))
(Birds fly).
The Result Rule says that if (a) )3 is to be attached to
a, and a was intended to support )3, and (b) a entails a
generic, of which )3&apos; and 6 form an instance, and (c) 6 is
consistent with what A and I believe,3 then normally,
6 and Result(a, )3) are inferred.
</bodyText>
<listItem confidence="0.9152942">
• The Result Rule:
(a) ((r, a, )3) A Isupport(a, )3)A
(b) 4bAT (a, OA lkbArAb(/3,1,b) A instance(0,0)A
(c) consistent(KB U MBA U 6))
&gt; (Result(a, 0) A 6)
</listItem>
<bodyText confidence="0.987128157894737">
The Result Rule does two things. First, it allows us to
infer one discourse relation (Result) from those permit-
ted by Cooperation. Second, it allows us to infer a new
piece of information 45, in virtue of which Result(a, 0)
is true.
We might want further constraints on 6 than that in
(c); we might add that 6 shouldn&apos;t violate expectations
generated by the text. But note that the Result Rule
doesn&apos;t choose between different bs that verify clauses
(b) and (c). As we&apos;ve mentioned, the theory needs to
be extended to deal with the problem of choice, and
it may be necessary to adopt strategies for choosing
among alternatives, which take factors other than logi-
cal structure into account.
We have a similar rule for inferring Evidence(/3, a)
(&amp;quot;)3 is evidence for a&amp;quot;). The Evidence rule resembles
the Result Rule, except that the textual order of the
discourse constituents, and the direction of intentional
support changes:
</bodyText>
<listItem confidence="0.9988478">
• The Evidence Rule:
(a) ((r, a, )3) A Isupport(0, a)A
(b) IkbAr(a, akbArA6(0,0) A instance(cb,O)A
(c) consistent(KB1 U KB A U 6))
&gt; (Evidence() 3 , a) A b)
</listItem>
<bodyText confidence="0.860908833333333">
We have seen that clause (a) of the Result Rule is sat-
isfied in the analysis of (1) in Context 1. Now, let 45 be
the proposition that the House Bill 1711 is bad for big
30r, more accurately, 6 must be consistent with what I
himself believes, and what he believes that A believes. In
other words, NBA is I&apos;S model of A&apos;s KB.
</bodyText>
<page confidence="0.998948">
37
</page>
<bodyText confidence="0.979693338709677">
business (written as bad(1711)). This is consistent with
KB/ U KBA , and so clause (c) is satisfied. Clause (b)
is also satisfied, because (i) a entails Bush vetoes bills
that are bad for big business—i.e., 1KBAT (a, 0) holds,
where q5 is Vx((bill(x) A bad(x)) &gt; veto(bush, x)); (ii)
# A b is bill(1711) A veto(bush,1711) A bad(1711); and
so (iii) instance(ifi, (3 A 6) and 1KBATA,5(,3, )3 A 6) both
hold.
So, when interpreting (1) in Context 1, two rules ap-
ply: Narration and the Result Rule. But the consequent
of Narration already conflicts with what is known; that
the discourse relation between a and [3 must satisfy the
Belief Property. So the consequent of the Result Rule is
inferred: b (i.e., House Bill 1711 is bad for big business)
and Result(a, 0) .4
These rules show how (1) can make the knowledge
that the house bill is bad for big business moot; one
does not need to know that the house bill is bad for
big business prior to attempting discourse attachment.
One can infer it at the time when discourse attachment
is attempted.
Now suppose that we start from different premises, as
provided by Context 2: BAB1P, BA–,Bia and WABIa.
That is, I thinks A believes that I believes Bush will
veto the bill, and I also thinks that A wants to con-
vince him that Bush supports big business. Then
the &apos;intentional&apos; line of reasoning yields different re-
sults from the same observed behaviour—A&apos;s utter-
ance of (1). Using APs1 again, but substituting B1 a
for cb instead of BO, I concludes BA(Ikb((7, , #) A
Info(a, 0), eventually(131a)).5 So Isupports(#, a) holds.
Now the antecedent to Cooperation is verified, and so
in the monotonic component of CE, we infer that a and
,0 must be connected by a discourse relation R&apos; such
that (B1 A IV (c 13)) &gt; Blot. As before, this restricts
the set of permitted discourse relations for attaching
13 to a. But unlike before, the textual order of a and
0, and their direction of intentional support mismatch.
The rule that applies this time is the Evidence Rule.
Consequently, a different discourse relation is inferred,
although the same information 6—that House Bill 1711
is bad for big business—supports the discourse relation,
and is also be inferred.
In contrast, the antecedents of the Result and Evi-
dence Rules aren&apos;t verified in (3). Assuming I knows
about the legislative process, he knows that if George
Bush is a weak willed president, then normally, he won&apos;t
veto bills. Consequently, there is no 6 that is consis-
tent with his KB, and sanctions the Evidence or Result
relation. Since I cannot infer which of the permitted
discourse relations holds, and so by contraposing the
axiom Cooperation, a doesn&apos;t Isupport #. And so I has
failed to conclude what A intended by his discourse ac-
tion. It can no longer be a belief that it will eventually
4We could have a similar rule to the Result. Rule for
inferring Evidence(, g) in this discourse context. too.
5Given the new KB, the antecedent of APs1 would no
longer be verified if we substituted 0 with BO.
lead to I believing fl, because otherwise Isupporga, i(3)
would be true via the rule Intends To Support. Conse-
quently, I cannot infer what discourse relation to use in
attachment, yielding incoherence.
</bodyText>
<sectionHeader confidence="0.999149666666667" genericHeader="method">
FROM INFORMATION TO
INTENTIONS:
CONTEXT 3
</sectionHeader>
<bodyText confidence="0.998865571428571">
Consider the interpretation of (1) in Context 3: I has
no knowledge of A&apos;s communicative intentions prior to
witnessing his linguistic behaviour, but he does know
that the House Bill 1711 is bad for big business. I has
sufficient information about the semantic content of a
and /3 to infer Result(a, #), via a rule given in Lascarides
and Asher (1991):
</bodyText>
<listItem confidence="0.949184">
• Result
</listItem>
<equation confidence="0.578499">
((r, a, fi) A cause(cr, ,3)) &gt; Result(a, /3)
</equation>
<bodyText confidence="0.995459333333333">
Result(a, 0) has the Belief Property, and I reasons that
from believing a, he will now come to believe 0. Having
used the information structure to infer discourse struc-
ture, I must now come to some conclusions about A&apos;s
cognitive state.
Now suppose that BABI a is in /&apos;s KB. Then the
following principle of Charity allows I to assume that A
was aware that I would come to believe /3 too, through
doing the discourse attachment he did:
</bodyText>
<listItem confidence="0.999446">
• Charity: STO &gt; BAB.t0
</listItem>
<bodyText confidence="0.999912363636364">
This is because I has inferred Resuli(a, /3), and since
Result has the belief property, I will come to believe [3
through believing a; so substituting 3 for d. in Charity,
BA BO will become part of /&apos;s KB via Defeasible Modus
Ponens. So, the following is now part of /&apos;s KB:
BA(ikb((T, a, /3) A Info(a, 0)), eventually(Bi f3)). Fur-
thermore, the assumption that A&apos;s discourse behaviour
was intentional again yields the following as part of
/&apos;s KB: /A((T, a, 0) A Info(a, g)). So, substituting BO
and (7, a, /3) A Info(a, 13) respectively for 4, and tk into
the Practical Syllogism, we find that clause (b) of the
premises, and the conclusion are verified. Explanatory
reasoning on the Practical Syllogism this time permits
us to infer clause (a): A&apos;s communicative goals were to
convince I of /3, as required.
The inferential mechanisms going from discourse
structure to intentional structure are much less well
understood. One needs to be able to make some sup-
positions about the beliefs of A before one can infer
anything about his desires to communicate, and this
requires a general theory of commonsense belief attri-
bution on the basis of beliefs that one has.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="method">
IMPERATIVES AND
PLAN UPDATES
</sectionHeader>
<bodyText confidence="0.999586333333333">
The revision of intentional structures exploits modes of
speech other than the assertoric. For instance, consider
another discourse from Moore and Pollack (1992):
</bodyText>
<page confidence="0.997498">
38
</page>
<bodyText confidence="0.7551">
(2)a. Let&apos;s go home by 5.
</bodyText>
<listItem confidence="0.861417333333333">
b. Then we can get to the hardware store
before it closes.
c. That way we can finish the bookshelves tonight.
</listItem>
<bodyText confidence="0.998731111111111">
Here, one exploits how the imperative mode affects
reasoning about intentions. Sincere Ordering captures
the intuition that if A orders a, then normally he wants
a to be true; and Wanting and Doing captures the in-
tuition that if A wants a to be true, and doesn&apos;t think
that it&apos;s impossible to bring a about, then by default
he intends to ensure that a is brought about, either by
doing it himself, or getting someone else to do it (cf.
Cohen and Levesque, 1990a).
</bodyText>
<listItem confidence="0.93215725">
• Sincere Ordering:
Imp(a) &gt; WA (a).
• Wanting and Doing:
(WA a A A ---.eventually(Ra)) &gt; 1A(T)
</listItem>
<bodyText confidence="0.999933037037037">
These rules about A&apos;s intentional structure help us
analyse (2). Let the logical forms of (2a-c) be respec-
tively a, /3 and 7. Suppose that we have inferred by
the linguistic clues that Result(a, /3) holds. That is,
the action a (i.e., going home by 5pm), results in /3
(i.e., the ability to go to the hardware store before it
closes). Since a is an imperative, Defeasible Modus Po-
llens on Sincere Ordering yields the inference that WA a
is true. Now let us assume that the interpreter I be-
lieves that the author A doesn&apos;t believe that a&apos;s being
brought about is impossible. Then we may use Defea-
sible Modus Ponens again on Wanting and Doing, to
infer I A(Tta). Just how the interpreter comes to the
belief, that the author believes a is possible, is a com-
plex matter. More than likely, we would have to encode
within the extension of DICE we have made, principles
that are familiar from autoepistemic reasoning. We will
postpone this exercise, however, for another time.
Now, to connect intentions and plans with discourse
structure, we propose a rule that takes an author&apos;s use
of a particular discourse structure to be prima facie
evidence that the author has a particular intention. The
rule Plan Apprehension below, states that if a is a plan
that A intends to do, or get someone else to do, and
he states that 8 is possible as a Result of this action a,
then the interpreter may normally take the author A to
imply that he intends 6 as well.
</bodyText>
<listItem confidence="0.752719">
• Plan Apprehension:
(Result(a 05) A IA(R-a) A = can(6)) &gt; A(TZ(a ; 6))
</listItem>
<bodyText confidence="0.998738555555556">
We call this rule Plan Apprehension, to make clear that
it furnishes one way for the interpreter of a verbal mes-
sage, to form an idea of the author&apos;s intentions, on the
basis of that message&apos;s discourse structure.
Plan Apprehension uses discourse structure to at-
tribute complex plans to A. And when attaching )3 to
a, having inferred Result(a , f3), this rule&apos;s antecedent is
verified, and so we infer that 6—which in this case is to
go to the hardware store before it closes—as part of A&apos;s
plan, which he intends to bring about, either himself,
or by getting another agent to do it.
Now, we process -y That way in 7 invokes an
anaphoric reference to a complex plan. By the acces-
sibility constraints in SDRT, its antecedent must [a; 6],
because this is the only plan in the accessible discourse
context. So 7 must be the DRS below: as a result of do-
ing this plan, finishing the bookshelves (which we have
labelled c) is possible:
</bodyText>
<listItem confidence="0.401761">
(7) Result( [a; bb can(e))
</listItem>
<bodyText confidence="0.999131375">
Now, substituting [a; 6] and c for a and /3 into the
Plan Apprehension Rule, we find that the antecedent to
this rule is verified again, and so its consequent is non-
monotonically inferred: I A(IZ(a; b; 0). Again, I has
used discourse structure to attribute plans to A.
Moore and Pollack (1992) also discuss one of I&apos;s pos-
sible responses to (2):
(4)We don&apos;t need to go to the hardware store.
I borrowed a saw from Jane.
Why does I respond with (4)? I has inferred the ex-
istence of the plan [a; ; d via Plan Apprehension; so he
takes the overall goal of A to be c (to finish the book-
shelves this evening). Intuitively, he fills in A&apos;s plan
with the reason why going to the hardware store is a
subgoal: I needs a saw. So A&apos;s plan is augmented with
another subgoal C, where C is to buy a saw, as follows:
1A (R,[a; b;C; d). But since holds, he says this and
assumes that this means that A does not have to do a
and b to achieve C.. To think about this formally, we
need to not only reason about intentions but also how
agents update their intentions or revise them when pre-
sented with new information. Asher and Koons (1993)
argue that the following schema captures part of the
logic which underlies updating intentions:
</bodyText>
<listItem confidence="0.836356">
• UpdatelAMcv 1; • • -; anD,D(ai; • • -; cei)
</listItem>
<bodyText confidence="0.982206117647059">
/A (&amp;quot;TZ[aj +1; . . . ; an]) A ACR[a ; • • • ;
In other words, if you&apos;re updating your intentions to
do actions al to an, and al to aj are already done,
then the new intentions are to do aj+i to an, and you
no longer intend to do al to aj.
The question is now: how does this interact with dis-
course structure? I is attempting to be helpful to A;
he is trying to help realize A&apos;s goal. We need axioms to
model this. Some key tools for doing this have been de-
veloped in the past couple of decades—belief revision,
intention and plan revision—and the long term aim
would be to enable formal theories of discourse struc-
ture to interact with these formal theories of attitudes
and attitude revision. But since a clear understand-
ing of how intentions are revised is yet to emerge, any
speculation on the revision of intentions in a particular
discourse context seems premature.
</bodyText>
<page confidence="0.999128">
39
</page>
<sectionHeader confidence="0.9996425" genericHeader="conclusions">
CONCLUSIONS AND
FURTHER WORK
</sectionHeader>
<bodyText confidence="0.999986243902439">
We have argued that it is important to separate reason-
ing about mental states from reasoning about discourse
structure, and we have suggested how to integrate a
formal theory of discourse attachment with common-
sense reasoning about the discourse participants&apos; cog-
nitive states and actions.
We exploited a classic principle of commonsense rea-
soning about action, the Practical Syllogism, to model
/&apos;s inferences about A&apos;s cognitive state during discourse
processing. We also showed how axioms could be de-
fined, so as to enable information to mediate between
the domain, discourse structure and communicative in-
tentions.
Reasoning about intentional structure took a differ-
ent form from reasoning about discourse attachment,
in that explanatory reasoning or abduction was per-
mitted for the former but not the latter (but cf. Hobbs
et al, 1990). This, we argued, was a principled reason
for maintaining separate representations of intentional
structure and discourse structure, but preserving close
links between them via axioms like Cooperation. Coop-
eration enabled I to use A&apos;s communicative intentions
to reason about discourse relations.
This paper provides an analysis of only very simple
discourses, and we realise that although we have in-
troduced distinctions among the attitudes, which we
have exploited during discourse processing, this is only
a small part of the story.
Though DICE has used domain specific information
to infer discourse relations, the rules relate domain
structure to discourse structure in at best an indirect
way. Implicitly, the use of the discourse update function
(7, a, 0) in the DICE rules reflects the intuitively obvious
fact that domain information is filtered through the cog-
nitive state of A. To make this explicit, the discourse
community should integrate work on speech acts and
attitudes (Perrault 1990, Cohen and Levesque 1990a,
1990b) with theories of discourse structure. In future
work, we will investigate discourses where other axioms
linking the different attitudes and discourse structure
are important.
</bodyText>
<sectionHeader confidence="0.99937" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999614101449275">
Asher, Nicholas (1986) Belief in Discourse Representa-
tion Theory, Journal of Philosophical Logic, 15, 127-
189.
Asher, Nicholas (1987) A Typology for Attitude
Verbs, Linguistics and Philosophy, 10, pp125-197.
Asher, Nicholas (1993) Reference to Abstract Objects
in Discourse, Kluwer Academic Publishers, Dordrecht,
Holland.
Asher, Nicholas and Koons, Robert (1993) The Revi-
sion of Beliefs and Intentions in a Changing World, in
Precedings of the AAI Spring Symposium Series: Rea-
soning about Mental States: Formal Theories and Ap-
plications.
Asher, Nicholas and Morreau, Michael (1991) Com-
mon Sense Entailment: A Modal Theory of Nonmono-
tonic Reasoning, in Proceedings to the 12th Interna-
tional Joint Conference on Artificial Intelligence, Syd-
ney Australia, August 1991.
Asher, Nicholas and Singh, Munindar (1993) A
Logic of Intentions and Beliefs, Journal of Philosophical
Logic, 22 5, pp513-544.
Bratman, Michael (forthcoming) Intentions, Plans
and Practical Reason, Harvard University Press, Cam-
bridge, Mass.
Cohen, Phillip R. and Levesque, Hector J. (1990a)
Persistence, Intention, and Commitment, In Philip R.
Cohen, Jerry Morgan and Martha E. Pollack (editors)
Intentions in Communication, pp33-69. Cambridge,
Massachusetts: Bradford/MIT Press.
Cohen, Phillip R. and Levesque, Hector J. (1990b)
Rational Interaction and the Basis for Communica-
tion, In Philip R. Cohen, Jerry Morgan and Martha E.
Pollack (editors) Intentions in Communication, pp221—
256. Cambridge, Massachusetts: Bradford/MIT Press.
Grosz, Barbara J. and Sidner, Candice L. (1986)
Attention, Intentions and the Structure of Discourse.
Computational Linguistics, 12, 175-204.
Grosz, Barbara J. and Sidner, Candice L. (1990)
Plans for Discourse. In Philip R. Cohen, Jerry Morgan
and Martha E. Pollack (editors) Intentions in Com-
munication, pp417-444. Cambridge, Massachusetts:
Bradford/MIT Press.
Hobbs, Jerry R. (1985) On the Coherence and Struc-
ture of Discourse. Report No: CSLI-85-37, Center for
the Study of Language and Information, October 1985.
Kamp, Hans (1981) A Theory of Truth and Semantic
Representation, in Groenendijk, J. A. G., Janssen, T.
M. V., and Stokhof, M. B. J. (eds.) Formal Methods in
lhe Study of Language, 277-332.
Kamp, Hans (1991) Procedural and Cognitive As-
pects of Propositional Attitude Contexts, Lecture Notes
from the Third European Summer School in Language,
Logic and Information, Saarbriicken, Germany.
La.scarides, Alex and Asher, Nicholas (1991) Dis-
course Relations and Defeasible Knowledge, in Proceed-
ings of the 29th Annual Meeting of Computational Lin-
guistics, 55-63, Berkeley California, USA, June 1991.
Lascarides, Alex and Asher, Nicholas (1993a) Tempo-
ral Interpretation, Discourse Relations and Common-
sense Entailment, in Linguistics and Philosophy, 16,
pp437-493.
Lascarides, Alex and Asher, Nicholas (1993b) A Se-
mantics and Pragmatics for the Pluperfect, in Pro-
ceedings of the European Chapter of the Association
for Computational Linguistics (EACL93), pp250-259,
Utrecht, The Netherlands.
Lascarides, Alex, Asher, Nicholas and Oberlander,
Jon (1992) Inferring Discourse Relations in Context, in
Proceedings of the 30th Annual Meeting of the Asso-
</reference>
<page confidence="0.967237">
40
</page>
<reference confidence="0.99931308">
ciation of Computational Linguistics, pp1-8, Delaware
USA, June 1992.
Lascarides, Alex and Oberlander, Jon (1993) Tempo-
ral Connectives in a Discourse Context, in Proceedings
of the European Chapter of the Association for Com-
putational Linguistics (EACL93), pp260-268, Utrecht,
The Netherlands.
Moore, Johanna and Pollack, Martha (1992) A Prob-
lem for RST: The Need for Multi-Level Discourse Anal-
ysis Computational Linguistics, 18 4, pp537-544.
Perrault, C. Ray (1990) An Application of Default
Logic to Speech Act Theory, in Philip R. Cohen,
Jerry Morgan and Martha E. Pollack (editors) Inten-
tions in Communication, pp161-185. Cambridge, Mas-
sachusetts: Bradford/MIT Press.
Polanyi, Livia (1985) A Theory of Discourse Struc-
ture and Discourse Coherence, in Eilfor, W. H., Kroe-
ber, P. D., and Peterson, K. L., (eds), Papers from the
General Session a the Twenty-First Regional Meeting of
the Chicago Linguistics Society, Chicago, April 25-27,
1985.
Thompson, Sandra and Mann, William (1987)
Rhetorical Structure Theory: A Framework for the
Analysis of Texts. In IPRA Papers in Pragmatics, 1,
79-105.
</reference>
<page confidence="0.999446">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482077">
<title confidence="0.998632">INTENTIONS AND INFORMATION IN DISCOURSE</title>
<author confidence="0.9795415">Nicholas Asher Paul Sabatier</author>
<address confidence="0.801738666666667">118 Route de Narbonne, Toulouse, CEDEX, France</address>
<email confidence="0.995454">ashereirit.fr</email>
<abstract confidence="0.999513625">This paper is about the flow of inference between communicative intentions, discourse structure and the domain during discourse processing. We augment a theory of discourse interpretation with a theory of distinct mental attitudes and reasoning about them, in order to provide an account of how the attitudes interact with reasoning about discourse structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Belief in Discourse Representation Theory,</title>
<date>1986</date>
<journal>Journal of Philosophical Logic,</journal>
<volume>15</volume>
<pages>127--189</pages>
<contexts>
<context position="6634" citStr="Asher 1986" startWordPosition="1043" endWordPosition="1044">above demands such distinctions, however. For example, analysing (1) under Context 3 requires a representation of the following statement: since A has provided a reason why (lb) is true, he must want I to believe that (lb) is true. It&apos;s unclear how Grosz and Sidner would represent this. SDRT (Asher, 1993) is in a good position to be integrated with a theory of cognitive states, because it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in </context>
</contexts>
<marker>Asher, 1986</marker>
<rawString>Asher, Nicholas (1986) Belief in Discourse Representation Theory, Journal of Philosophical Logic, 15, 127-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>A Typology for Attitude Verbs,</title>
<date>1987</date>
<journal>Linguistics and Philosophy,</journal>
<volume>10</volume>
<pages>125--197</pages>
<marker>Asher, 1987</marker>
<rawString>Asher, Nicholas (1987) A Typology for Attitude Verbs, Linguistics and Philosophy, 10, pp125-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Reference to Abstract Objects in Discourse,</title>
<date>1993</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Holland.</location>
<contexts>
<context position="3411" citStr="Asher 1993" startWordPosition="543" endWordPosition="544">ifferent rules and axioms from those used to infer rhetorical relations. Thus, we should represent intentional structure and discourse structure separately. But we postulate rhetorical relations that express the discourse function of the constituents in the communicative plan of the author, and we permit interaction between reasoning about rhetorical relations and reasoning about beliefs and desires. This paper provides the first steps towards a formal analysis of the interaction between intentional structure and informational structure. Our framework for discourse structure analysis is SDRT (Asher 1993). The basic representational structures of that theory may be used to characterise cognitive states. We will extend the logical engine used to infer rhetorical relations—DICE (Lascarides and Asher 1991, 1993a, 1993b, Lascarides and Oberlander 1993)—to model inferences about intentional structure and its interaction with informational structure. BUSH&apos;S REQUIREMENTS We must represent both the intentional import and the informational import of a discourse simultaneously. So we need a theory of discourse structure where discourse relations central to intentional import and to informational import </context>
<context position="6330" citStr="Asher, 1993" startWordPosition="995" endWordPosition="996">dner (1990) point out: &amp;quot;any model (or theory) of the communication situation must distinguish among beliefs and intentions of different agents,&amp;quot; but theirs does not. They represent intentional structure as a stack of propositions, and different attitudes aren&apos;t distinguished. The informal analysis of (1) above demands such distinctions, however. For example, analysing (1) under Context 3 requires a representation of the following statement: since A has provided a reason why (lb) is true, he must want I to believe that (lb) is true. It&apos;s unclear how Grosz and Sidner would represent this. SDRT (Asher, 1993) is in a good position to be integrated with a theory of cognitive states, because it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations betwee</context>
<context position="11356" citStr="Asher 1993" startWordPosition="1842" endWordPosition="1843">attitudes: how to derive what a person believes, from a knowledge of what he wants and an observation of his behaviour. The classic means of constructing such a derivation uses the practical syllogism, a form of reasoning about action familiar since Aristotle. It expresses the following maxim: Act so as to realize your goals ceteris paribus. The practical syllogism is a rule of defeasible reasoning, expressible in CE by means of the nonmonotonic consequence relation k. The consequence relation 4*0 can be stated directly in the object language of CE by a formula which we abbreviate as 1(0, 0) (Asher 1993). We use 1(0, 0) to state the practical syllogism. First, we define the notion that the KB and 0, but not the KB alone, nonmonotonically yield 0: • Definition: 14b(0 0) 4-41(K B A 0, 0) A 1(K B 7,b) The Practical Syllogism says that if (a) A wants 0 but believes it&apos;s not true, and (b) he knows that if 0 were added to his KB it would by default make 0 true eventually, then by default A intends 0. • The Practical Syllogism: (a) (WA (0) A BA (-10)A (b) BA(1kb(0, eventually(0)))) &gt; (c) IA(b) The Practical Syllogism enables] to reason about A&apos;s cognitive state. In Context 1, when substituting in th</context>
<context position="17910" citStr="Asher (1993)" startWordPosition="2983" endWordPosition="2984">e axioms above allow I to use his knowledge of A&apos;s cognitive state, and the behaviour of A that he observes, to (a) infer information about A&apos;s communicative intentions, and (b) consequently to restrict the set of candidate discourse relations that are permitted between the constituents. According to Cooperation, I must infer that one of the permitted discourse relations does indeed hold. When clue words are lacking, the semantic content of the constituents must be exploited. In certain cases, it&apos;s also necessary to infer further information that wasn&apos;t explicitly mentioned in the discourse, 2Asher (1993) discusses this point in relation to Contrast: the discourse marker but is used coherently only if the semantic content of the constituents it connects do indeed form a contrast: compare Mary&apos;s hair is black but her eyes are blue, with ?Mary&apos;s hair is black but John&apos;s hair is black. in order to sanction the discourse relation. For example, in (1) in Contexts 1 and 2, I infers the bill is bad for big business. Consider again discourse (1) in Context 1. Intuitively, the reason we can infer Result(a, 13) in the analysis of (1) is because (i) a entails a generic (Bush vetoes bills that are bad for</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Asher, Nicholas (1993) Reference to Abstract Objects in Discourse, Kluwer Academic Publishers, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Robert Koons</author>
</authors>
<title>The Revision of Beliefs and Intentions in a Changing World,</title>
<date>1993</date>
<booktitle>in Precedings of the AAI Spring Symposium Series: Reasoning about Mental States: Formal Theories and Applications.</booktitle>
<contexts>
<context position="30580" citStr="Asher and Koons (1993)" startWordPosition="5265" endWordPosition="5268">ia Plan Apprehension; so he takes the overall goal of A to be c (to finish the bookshelves this evening). Intuitively, he fills in A&apos;s plan with the reason why going to the hardware store is a subgoal: I needs a saw. So A&apos;s plan is augmented with another subgoal C, where C is to buy a saw, as follows: 1A (R,[a; b;C; d). But since holds, he says this and assumes that this means that A does not have to do a and b to achieve C.. To think about this formally, we need to not only reason about intentions but also how agents update their intentions or revise them when presented with new information. Asher and Koons (1993) argue that the following schema captures part of the logic which underlies updating intentions: • UpdatelAMcv 1; • • -; anD,D(ai; • • -; cei) /A (&amp;quot;TZ[aj +1; . . . ; an]) A ACR[a ; • • • ; In other words, if you&apos;re updating your intentions to do actions al to an, and al to aj are already done, then the new intentions are to do aj+i to an, and you no longer intend to do al to aj. The question is now: how does this interact with discourse structure? I is attempting to be helpful to A; he is trying to help realize A&apos;s goal. We need axioms to model this. Some key tools for doing this have been dev</context>
</contexts>
<marker>Asher, Koons, 1993</marker>
<rawString>Asher, Nicholas and Koons, Robert (1993) The Revision of Beliefs and Intentions in a Changing World, in Precedings of the AAI Spring Symposium Series: Reasoning about Mental States: Formal Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Michael Morreau</author>
</authors>
<title>Common Sense Entailment: A Modal Theory of Nonmonotonic Reasoning,</title>
<date>1991</date>
<booktitle>in Proceedings to the 12th International Joint Conference on Artificial Intelligence,</booktitle>
<location>Sydney Australia,</location>
<marker>Asher, Morreau, 1991</marker>
<rawString>Asher, Nicholas and Morreau, Michael (1991) Common Sense Entailment: A Modal Theory of Nonmonotonic Reasoning, in Proceedings to the 12th International Joint Conference on Artificial Intelligence, Sydney Australia, August 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Munindar Singh</author>
</authors>
<title>A Logic of Intentions and Beliefs,</title>
<date>1993</date>
<journal>Journal of Philosophical Logic,</journal>
<volume>22</volume>
<pages>513--544</pages>
<contexts>
<context position="6675" citStr="Asher and Singh, 1993" startWordPosition="1048" endWordPosition="1051">ns, however. For example, analysing (1) under Context 3 requires a representation of the following statement: since A has provided a reason why (lb) is true, he must want I to believe that (lb) is true. It&apos;s unclear how Grosz and Sidner would represent this. SDRT (Asher, 1993) is in a good position to be integrated with a theory of cognitive states, because it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in a discourse structure for new information</context>
<context position="9204" citStr="Asher and Singh (1993)" startWordPosition="1451" endWordPosition="1454">tecedents are in logical entailment relations, then the consequent of the rule with the most specific antecedent is inferred. Lascarides and Asher (1991) use DICE to yield the discourse structures and temporal structures for simple discourses. But the theory has so far ignored how A&apos;s intentional structure—or more accurately, I&apos;s model of A&apos;s intentional structure—influences I&apos;s inferences about the domain and the discourse structure. ADDING INTENTIONS To discuss intentional structure, we develop a language which can express beliefs, intentions and desires. Following Bratman (forthcoming) and Asher and Singh (1993), we think of the objects of attitudes either as plans or as propositions. For example, the colloquial intention to do something—like wash the dishes—will be expressed as an intention toward a plan, whereas the intention that Sue be happy is an intention toward a proposition. Plans will just consist of sequences of basic actions ai ; a2; .;a,. Two operators-1Z for about to do or doing, and 1, for having done—will convert actions into propositions. The attitudes we assume in our model are believes (5,40 means &apos;A believes 0&apos;), wants (WAO means &apos;A wants 0&apos;), and intends (IAq5 means &apos;A intends 0&apos;)</context>
</contexts>
<marker>Asher, Singh, 1993</marker>
<rawString>Asher, Nicholas and Singh, Munindar (1993) A Logic of Intentions and Beliefs, Journal of Philosophical Logic, 22 5, pp513-544.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Bratman</author>
</authors>
<title>(forthcoming) Intentions, Plans and Practical Reason,</title>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Bratman, </marker>
<rawString>Bratman, Michael (forthcoming) Intentions, Plans and Practical Reason, Harvard University Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>33--69</pages>
<editor>Persistence, Intention, and Commitment, In Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors)</editor>
<publisher>Bradford/MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="26884" citStr="Cohen and Levesque, 1990" startWordPosition="4579" endWordPosition="4582">oore and Pollack (1992): 38 (2)a. Let&apos;s go home by 5. b. Then we can get to the hardware store before it closes. c. That way we can finish the bookshelves tonight. Here, one exploits how the imperative mode affects reasoning about intentions. Sincere Ordering captures the intuition that if A orders a, then normally he wants a to be true; and Wanting and Doing captures the intuition that if A wants a to be true, and doesn&apos;t think that it&apos;s impossible to bring a about, then by default he intends to ensure that a is brought about, either by doing it himself, or getting someone else to do it (cf. Cohen and Levesque, 1990a). • Sincere Ordering: Imp(a) &gt; WA (a). • Wanting and Doing: (WA a A A ---.eventually(Ra)) &gt; 1A(T) These rules about A&apos;s intentional structure help us analyse (2). Let the logical forms of (2a-c) be respectively a, /3 and 7. Suppose that we have inferred by the linguistic clues that Result(a, /3) holds. That is, the action a (i.e., going home by 5pm), results in /3 (i.e., the ability to go to the hardware store before it closes). Since a is an imperative, Defeasible Modus Pollens on Sincere Ordering yields the inference that WA a is true. Now let us assume that the interpreter I believes that</context>
</contexts>
<marker>Cohen, Levesque, 1990</marker>
<rawString>Cohen, Phillip R. and Levesque, Hector J. (1990a) Persistence, Intention, and Commitment, In Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors) Intentions in Communication, pp33-69. Cambridge, Massachusetts: Bradford/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<title>Rational Interaction and the Basis for Communication,</title>
<date>1990</date>
<booktitle>Intentions in Communication, pp221— 256.</booktitle>
<editor>In Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors)</editor>
<publisher>Bradford/MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="26884" citStr="Cohen and Levesque, 1990" startWordPosition="4579" endWordPosition="4582">oore and Pollack (1992): 38 (2)a. Let&apos;s go home by 5. b. Then we can get to the hardware store before it closes. c. That way we can finish the bookshelves tonight. Here, one exploits how the imperative mode affects reasoning about intentions. Sincere Ordering captures the intuition that if A orders a, then normally he wants a to be true; and Wanting and Doing captures the intuition that if A wants a to be true, and doesn&apos;t think that it&apos;s impossible to bring a about, then by default he intends to ensure that a is brought about, either by doing it himself, or getting someone else to do it (cf. Cohen and Levesque, 1990a). • Sincere Ordering: Imp(a) &gt; WA (a). • Wanting and Doing: (WA a A A ---.eventually(Ra)) &gt; 1A(T) These rules about A&apos;s intentional structure help us analyse (2). Let the logical forms of (2a-c) be respectively a, /3 and 7. Suppose that we have inferred by the linguistic clues that Result(a, /3) holds. That is, the action a (i.e., going home by 5pm), results in /3 (i.e., the ability to go to the hardware store before it closes). Since a is an imperative, Defeasible Modus Pollens on Sincere Ordering yields the inference that WA a is true. Now let us assume that the interpreter I believes that</context>
</contexts>
<marker>Cohen, Levesque, 1990</marker>
<rawString>Cohen, Phillip R. and Levesque, Hector J. (1990b) Rational Interaction and the Basis for Communication, In Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors) Intentions in Communication, pp221— 256. Cambridge, Massachusetts: Bradford/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candice L Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intentions and the Structure of Discourse. Computational Linguistics,</booktitle>
<volume>12</volume>
<pages>175--204</pages>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, Barbara J. and Sidner, Candice L. (1986) Attention, Intentions and the Structure of Discourse. Computational Linguistics, 12, 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candice L Sidner</author>
</authors>
<title>Plans for Discourse.</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>417--444</pages>
<editor>In Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors)</editor>
<publisher>Bradford/MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="5729" citStr="Grosz and Sidner (1990)" startWordPosition="896" endWordPosition="899">notonic logic for inferring rhetorical relations would not yield a representation of (2) on multiple levels in which both intentional and informational relations are represented. In SDRT, on the other hand, not all discourse relations induce subordination, and so there is more scope for different discourse relations holding simultaneously in a consistent KB. Grosz and Sidner&apos;s (1986) model of discourse interpretation is one where the same discourse elements are related simultaneously on the informational and intentional levels. But using their framework to model (1) is not straightforward. As Grosz and Sidner (1990) point out: &amp;quot;any model (or theory) of the communication situation must distinguish among beliefs and intentions of different agents,&amp;quot; but theirs does not. They represent intentional structure as a stack of propositions, and different attitudes aren&apos;t distinguished. The informal analysis of (1) above demands such distinctions, however. For example, analysing (1) under Context 3 requires a representation of the following statement: since A has provided a reason why (lb) is true, he must want I to believe that (lb) is true. It&apos;s unclear how Grosz and Sidner would represent this. SDRT (Asher, 1993</context>
</contexts>
<marker>Grosz, Sidner, 1990</marker>
<rawString>Grosz, Barbara J. and Sidner, Candice L. (1990) Plans for Discourse. In Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors) Intentions in Communication, pp417-444. Cambridge, Massachusetts: Bradford/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>On the Coherence and Structure of Discourse.</title>
<date>1985</date>
<tech>Report No: CSLI-85-37,</tech>
<institution>Center for</institution>
<contexts>
<context position="7004" citStr="Hobbs (1985)" startWordPosition="1105" endWordPosition="1106">tive states, because it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in a discourse structure for new information. Using DICE we infer from the reader&apos;s knowledge resources which discourse relation should be used to do attachment. Lascarides and Asher (1991) introduce default rules representing the role of Gricean pragmatic maxims and domain knowledge in calculating the value of the update function (7, a, /3), which means &amp;quot;the representat</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, Jerry R. (1985) On the Coherence and Structure of Discourse. Report No: CSLI-85-37, Center for the Study of Language and Information, October 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>A Theory of Truth and Semantic Representation,</title>
<date>1981</date>
<booktitle>Formal Methods in lhe Study of Language,</booktitle>
<pages>277--332</pages>
<editor>in Groenendijk, J. A. G., Janssen, T. M. V., and Stokhof, M. B. J. (eds.)</editor>
<contexts>
<context position="6622" citStr="Kamp 1981" startWordPosition="1041" endWordPosition="1042">sis of (1) above demands such distinctions, however. For example, analysing (1) under Context 3 requires a representation of the following statement: since A has provided a reason why (lb) is true, he must want I to believe that (lb) is true. It&apos;s unclear how Grosz and Sidner would represent this. SDRT (Asher, 1993) is in a good position to be integrated with a theory of cognitive states, because it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of at</context>
</contexts>
<marker>Kamp, 1981</marker>
<rawString>Kamp, Hans (1981) A Theory of Truth and Semantic Representation, in Groenendijk, J. A. G., Janssen, T. M. V., and Stokhof, M. B. J. (eds.) Formal Methods in lhe Study of Language, 277-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>Procedural and Cognitive Aspects of Propositional Attitude Contexts,</title>
<date>1991</date>
<booktitle>Lecture Notes from the Third European Summer School in Language, Logic and Information,</booktitle>
<location>Saarbriicken, Germany.</location>
<contexts>
<context position="6651" citStr="Kamp 1991" startWordPosition="1046" endWordPosition="1047"> distinctions, however. For example, analysing (1) under Context 3 requires a representation of the following statement: since A has provided a reason why (lb) is true, he must want I to believe that (lb) is true. It&apos;s unclear how Grosz and Sidner would represent this. SDRT (Asher, 1993) is in a good position to be integrated with a theory of cognitive states, because it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in a discourse struc</context>
</contexts>
<marker>Kamp, 1991</marker>
<rawString>Kamp, Hans (1991) Procedural and Cognitive Aspects of Propositional Attitude Contexts, Lecture Notes from the Third European Summer School in Language, Logic and Information, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex La scarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Discourse Relations and Defeasible Knowledge,</title>
<date>1991</date>
<booktitle>in Proceedings of the 29th Annual Meeting of Computational Linguistics,</booktitle>
<pages>55--63</pages>
<location>Berkeley California, USA,</location>
<contexts>
<context position="3612" citStr="scarides and Asher 1991" startWordPosition="570" endWordPosition="573">tions that express the discourse function of the constituents in the communicative plan of the author, and we permit interaction between reasoning about rhetorical relations and reasoning about beliefs and desires. This paper provides the first steps towards a formal analysis of the interaction between intentional structure and informational structure. Our framework for discourse structure analysis is SDRT (Asher 1993). The basic representational structures of that theory may be used to characterise cognitive states. We will extend the logical engine used to infer rhetorical relations—DICE (Lascarides and Asher 1991, 1993a, 1993b, Lascarides and Oberlander 1993)—to model inferences about intentional structure and its interaction with informational structure. BUSH&apos;S REQUIREMENTS We must represent both the intentional import and the informational import of a discourse simultaneously. So we need a theory of discourse structure where discourse relations central to intentional import and to informational import can hold simultaneously between the same constituents. A logical framework in which all those plausible relations between constituents that are consistent with each other are inferred, such as a nonmon</context>
<context position="7420" citStr="scarides and Asher (1991)" startWordPosition="1167" endWordPosition="1170">h is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in a discourse structure for new information. Using DICE we infer from the reader&apos;s knowledge resources which discourse relation should be used to do attachment. Lascarides and Asher (1991) introduce default rules representing the role of Gricean pragmatic maxims and domain knowledge in calculating the value of the update function (7, a, /3), which means &amp;quot;the representation of the current sentence is to be attached to a with a discourse relation, where a is an open node in the representation T of the text so far&amp;quot;. Defaults are represented by a conditional—q&gt; tk means &apos;if 0, then normally 0. For example, Narration says that by default Narration relates elements in a text. • Narration: (7, a , )3) &gt; Narration(a, (3) Associated axioms show how Narration affects the temporal order o</context>
<context position="8735" citStr="scarides and Asher (1991)" startWordPosition="1381" endWordPosition="1384">ct that normally the textual order of events matches their temporal order. The logic on which DICE rests is Asher and Morreau&apos;s (1991) Commonsense Entailment (CE). Two patterns of nonmonotonic inference are particularly relevant here. The first is Defeasible Modus Ponens: if one default rule has its antecedent verified, then the consequent is nonmonotonically inferred. The second is the Penguin Principle: if there are conflicting default rules that apply, and their antecedents are in logical entailment relations, then the consequent of the rule with the most specific antecedent is inferred. Lascarides and Asher (1991) use DICE to yield the discourse structures and temporal structures for simple discourses. But the theory has so far ignored how A&apos;s intentional structure—or more accurately, I&apos;s model of A&apos;s intentional structure—influences I&apos;s inferences about the domain and the discourse structure. ADDING INTENTIONS To discuss intentional structure, we develop a language which can express beliefs, intentions and desires. Following Bratman (forthcoming) and Asher and Singh (1993), we think of the objects of attitudes either as plans or as propositions. For example, the colloquial intention to do something—li</context>
<context position="24414" citStr="scarides and Asher (1991)" startWordPosition="4144" endWordPosition="4147"> if we substituted 0 with BO. lead to I believing fl, because otherwise Isupporga, i(3) would be true via the rule Intends To Support. Consequently, I cannot infer what discourse relation to use in attachment, yielding incoherence. FROM INFORMATION TO INTENTIONS: CONTEXT 3 Consider the interpretation of (1) in Context 3: I has no knowledge of A&apos;s communicative intentions prior to witnessing his linguistic behaviour, but he does know that the House Bill 1711 is bad for big business. I has sufficient information about the semantic content of a and /3 to infer Result(a, #), via a rule given in Lascarides and Asher (1991): • Result ((r, a, fi) A cause(cr, ,3)) &gt; Result(a, /3) Result(a, 0) has the Belief Property, and I reasons that from believing a, he will now come to believe 0. Having used the information structure to infer discourse structure, I must now come to some conclusions about A&apos;s cognitive state. Now suppose that BABI a is in /&apos;s KB. Then the following principle of Charity allows I to assume that A was aware that I would come to believe /3 too, through doing the discourse attachment he did: • Charity: STO &gt; BAB.t0 This is because I has inferred Resuli(a, /3), and since Result has the belief propert</context>
</contexts>
<marker>scarides, Asher, 1991</marker>
<rawString>La.scarides, Alex and Asher, Nicholas (1991) Discourse Relations and Defeasible Knowledge, in Proceedings of the 29th Annual Meeting of Computational Linguistics, 55-63, Berkeley California, USA, June 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<date>1993</date>
<booktitle>Temporal Interpretation, Discourse Relations and Commonsense Entailment, in Linguistics and Philosophy,</booktitle>
<volume>16</volume>
<pages>437--493</pages>
<contexts>
<context position="4270" citStr="Lascarides and Asher, 1993" startWordPosition="666" endWordPosition="669">berlander 1993)—to model inferences about intentional structure and its interaction with informational structure. BUSH&apos;S REQUIREMENTS We must represent both the intentional import and the informational import of a discourse simultaneously. So we need a theory of discourse structure where discourse relations central to intentional import and to informational import can hold simultaneously between the same constituents. A logical framework in which all those plausible relations between constituents that are consistent with each other are inferred, such as a nonmonotonic logic like that in DICE (Lascarides and Asher, 1993a), would achieve this. So conceivably, a similar nonmonotonic logic for RST might solve the problem of keeping track of the intentional and informational 34 structure simultaneously. But this would work only if the various discourse relations about intentions and information could simultaneously hold in a consistent knowledge base (KB). Moore and Pollack (1992) show via discourse (2) that the current commitment to the nucleus-satellite distinction in RST precludes this. (2)a. Let&apos;s go home by 5. b. Then we can get to the hardware store before it closes. c. That way we can finish the bookshelv</context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Lascarides, Alex and Asher, Nicholas (1993a) Temporal Interpretation, Discourse Relations and Commonsense Entailment, in Linguistics and Philosophy, 16, pp437-493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>A Semantics and Pragmatics for the Pluperfect,</title>
<date>1993</date>
<booktitle>in Proceedings of the European Chapter of the Association for Computational Linguistics (EACL93),</booktitle>
<pages>250--259</pages>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="4270" citStr="Lascarides and Asher, 1993" startWordPosition="666" endWordPosition="669">berlander 1993)—to model inferences about intentional structure and its interaction with informational structure. BUSH&apos;S REQUIREMENTS We must represent both the intentional import and the informational import of a discourse simultaneously. So we need a theory of discourse structure where discourse relations central to intentional import and to informational import can hold simultaneously between the same constituents. A logical framework in which all those plausible relations between constituents that are consistent with each other are inferred, such as a nonmonotonic logic like that in DICE (Lascarides and Asher, 1993a), would achieve this. So conceivably, a similar nonmonotonic logic for RST might solve the problem of keeping track of the intentional and informational 34 structure simultaneously. But this would work only if the various discourse relations about intentions and information could simultaneously hold in a consistent knowledge base (KB). Moore and Pollack (1992) show via discourse (2) that the current commitment to the nucleus-satellite distinction in RST precludes this. (2)a. Let&apos;s go home by 5. b. Then we can get to the hardware store before it closes. c. That way we can finish the bookshelv</context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Lascarides, Alex and Asher, Nicholas (1993b) A Semantics and Pragmatics for the Pluperfect, in Proceedings of the European Chapter of the Association for Computational Linguistics (EACL93), pp250-259, Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
<author>Jon Oberlander</author>
</authors>
<title>Inferring Discourse Relations in Context,</title>
<date>1992</date>
<booktitle>in Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Delaware USA,</location>
<marker>Lascarides, Asher, Oberlander, 1992</marker>
<rawString>Lascarides, Alex, Asher, Nicholas and Oberlander, Jon (1992) Inferring Discourse Relations in Context, in Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics, pp1-8, Delaware USA, June 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Jon Oberlander</author>
</authors>
<title>Temporal Connectives in a Discourse Context,</title>
<date>1993</date>
<booktitle>in Proceedings of the European Chapter of the Association for Computational Linguistics (EACL93),</booktitle>
<pages>260--268</pages>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="3659" citStr="Lascarides and Oberlander 1993" startWordPosition="576" endWordPosition="579">on of the constituents in the communicative plan of the author, and we permit interaction between reasoning about rhetorical relations and reasoning about beliefs and desires. This paper provides the first steps towards a formal analysis of the interaction between intentional structure and informational structure. Our framework for discourse structure analysis is SDRT (Asher 1993). The basic representational structures of that theory may be used to characterise cognitive states. We will extend the logical engine used to infer rhetorical relations—DICE (Lascarides and Asher 1991, 1993a, 1993b, Lascarides and Oberlander 1993)—to model inferences about intentional structure and its interaction with informational structure. BUSH&apos;S REQUIREMENTS We must represent both the intentional import and the informational import of a discourse simultaneously. So we need a theory of discourse structure where discourse relations central to intentional import and to informational import can hold simultaneously between the same constituents. A logical framework in which all those plausible relations between constituents that are consistent with each other are inferred, such as a nonmonotonic logic like that in DICE (Lascarides and </context>
</contexts>
<marker>Lascarides, Oberlander, 1993</marker>
<rawString>Lascarides, Alex and Oberlander, Jon (1993) Temporal Connectives in a Discourse Context, in Proceedings of the European Chapter of the Association for Computational Linguistics (EACL93), pp260-268, Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Martha Pollack</author>
</authors>
<title>A Problem for RST: The Need for Multi-Level Discourse Analysis Computational Linguistics,</title>
<date>1992</date>
<volume>18</volume>
<pages>537--544</pages>
<contexts>
<context position="726" citStr="Moore and Pollack (1992)" startWordPosition="102" endWordPosition="105">rbonne, 31062 Toulouse, CEDEX, France ashereirit.fr Abstract This paper is about the flow of inference between communicative intentions, discourse structure and the domain during discourse processing. We augment a theory of discourse interpretation with a theory of distinct mental attitudes and reasoning about them, in order to provide an account of how the attitudes interact with reasoning about discourse structure. INTRODUCTION The flow of inference between communicative intentions and domain information is often essential to discourse processing. It is well reflected in this discourse from Moore and Pollack (1992): (1)a. George Bush supports big business. b. He&apos;s sure to veto House Bill 1711. There are at least three different interpretations. Consider Context 1: in this context the interpreter I believes that the author A wants to convince him that (lb) is true. For example, the context is one in which I has already uttered Bush won&apos;t veto any more bills. I reasons that A&apos;s linguistic behavior was intentional, and therefore that A believes that by saying (la) he will convince I that Bush will veto the bill. Even if I believed nothing about the bill, he now infers it&apos;s bad for big business. So we have </context>
<context position="4634" citStr="Moore and Pollack (1992)" startWordPosition="721" endWordPosition="724"> can hold simultaneously between the same constituents. A logical framework in which all those plausible relations between constituents that are consistent with each other are inferred, such as a nonmonotonic logic like that in DICE (Lascarides and Asher, 1993a), would achieve this. So conceivably, a similar nonmonotonic logic for RST might solve the problem of keeping track of the intentional and informational 34 structure simultaneously. But this would work only if the various discourse relations about intentions and information could simultaneously hold in a consistent knowledge base (KB). Moore and Pollack (1992) show via discourse (2) that the current commitment to the nucleus-satellite distinction in RST precludes this. (2)a. Let&apos;s go home by 5. b. Then we can get to the hardware store before it closes. c. That way we can finish the bookshelves tonight. From an intentional perspective, (2b) is a satellite to (2a) via Motivation. From an informational perspective, (2a) is a satellite to (2b) via Condition. These two structures are incompatible. So augmenting RST with a nonmonotonic logic for inferring rhetorical relations would not yield a representation of (2) on multiple levels in which both intent</context>
<context position="26283" citStr="Moore and Pollack (1992)" startWordPosition="4467" endWordPosition="4470">fer clause (a): A&apos;s communicative goals were to convince I of /3, as required. The inferential mechanisms going from discourse structure to intentional structure are much less well understood. One needs to be able to make some suppositions about the beliefs of A before one can infer anything about his desires to communicate, and this requires a general theory of commonsense belief attribution on the basis of beliefs that one has. IMPERATIVES AND PLAN UPDATES The revision of intentional structures exploits modes of speech other than the assertoric. For instance, consider another discourse from Moore and Pollack (1992): 38 (2)a. Let&apos;s go home by 5. b. Then we can get to the hardware store before it closes. c. That way we can finish the bookshelves tonight. Here, one exploits how the imperative mode affects reasoning about intentions. Sincere Ordering captures the intuition that if A orders a, then normally he wants a to be true; and Wanting and Doing captures the intuition that if A wants a to be true, and doesn&apos;t think that it&apos;s impossible to bring a about, then by default he intends to ensure that a is brought about, either by doing it himself, or getting someone else to do it (cf. Cohen and Levesque, 199</context>
<context position="29753" citStr="Moore and Pollack (1992)" startWordPosition="5100" endWordPosition="5103">n anaphoric reference to a complex plan. By the accessibility constraints in SDRT, its antecedent must [a; 6], because this is the only plan in the accessible discourse context. So 7 must be the DRS below: as a result of doing this plan, finishing the bookshelves (which we have labelled c) is possible: (7) Result( [a; bb can(e)) Now, substituting [a; 6] and c for a and /3 into the Plan Apprehension Rule, we find that the antecedent to this rule is verified again, and so its consequent is nonmonotonically inferred: I A(IZ(a; b; 0). Again, I has used discourse structure to attribute plans to A. Moore and Pollack (1992) also discuss one of I&apos;s possible responses to (2): (4)We don&apos;t need to go to the hardware store. I borrowed a saw from Jane. Why does I respond with (4)? I has inferred the existence of the plan [a; ; d via Plan Apprehension; so he takes the overall goal of A to be c (to finish the bookshelves this evening). Intuitively, he fills in A&apos;s plan with the reason why going to the hardware store is a subgoal: I needs a saw. So A&apos;s plan is augmented with another subgoal C, where C is to buy a saw, as follows: 1A (R,[a; b;C; d). But since holds, he says this and assumes that this means that A does not</context>
</contexts>
<marker>Moore, Pollack, 1992</marker>
<rawString>Moore, Johanna and Pollack, Martha (1992) A Problem for RST: The Need for Multi-Level Discourse Analysis Computational Linguistics, 18 4, pp537-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ray Perrault</author>
</authors>
<title>An Application of Default Logic to Speech Act Theory,</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>161--185</pages>
<editor>in Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors)</editor>
<publisher>Bradford/MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<marker>Perrault, 1990</marker>
<rawString>Perrault, C. Ray (1990) An Application of Default Logic to Speech Act Theory, in Philip R. Cohen, Jerry Morgan and Martha E. Pollack (editors) Intentions in Communication, pp161-185. Cambridge, Massachusetts: Bradford/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
</authors>
<title>A Theory of Discourse Structure and Discourse Coherence,</title>
<date>1985</date>
<location>Chicago,</location>
<note>in</note>
<contexts>
<context position="7020" citStr="Polanyi (1985)" startWordPosition="1107" endWordPosition="1108">ecause it uses the same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in a discourse structure for new information. Using DICE we infer from the reader&apos;s knowledge resources which discourse relation should be used to do attachment. Lascarides and Asher (1991) introduce default rules representing the role of Gricean pragmatic maxims and domain knowledge in calculating the value of the update function (7, a, /3), which means &amp;quot;the representation of the curre</context>
</contexts>
<marker>Polanyi, 1985</marker>
<rawString>Polanyi, Livia (1985) A Theory of Discourse Structure and Discourse Coherence, in Eilfor, W. H., Kroeber, P. D., and Peterson, K. L., (eds), Papers from the General Session a the Twenty-First Regional Meeting of the Chicago Linguistics Society, Chicago, April 25-27, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Thompson</author>
<author>William Mann</author>
</authors>
<title>Rhetorical Structure Theory: A Framework for the Analysis of Texts.</title>
<date>1987</date>
<booktitle>In IPRA Papers in Pragmatics,</booktitle>
<volume>1</volume>
<pages>79--105</pages>
<contexts>
<context position="2691" citStr="Thompson and Mann, 1987" startWordPosition="434" endWordPosition="437">. So, A must have uttered (la) to support (lb). Hence I realises that A wanted Alex Lascarides Department of Linguistics, Stanford University, Stanford, Ca 94305-2150, USA, alexecsli.stanford.edu him to believe (lb). So in contrast to Contexts 1 and 2, we have a flow of inference from informational structure to intentional structure. This story makes two main points. First, we agree with Moore and Pollack that we must represent both the intentional import and the informational import of a discourse. As they show, this is a problem for current formulations of Rhetorical Structure Theory (RsT) (Thompson and Mann, 1987). Second, we go further than Moore and Pollack, and argue that reasoning about beliefs and desires exploits different rules and axioms from those used to infer rhetorical relations. Thus, we should represent intentional structure and discourse structure separately. But we postulate rhetorical relations that express the discourse function of the constituents in the communicative plan of the author, and we permit interaction between reasoning about rhetorical relations and reasoning about beliefs and desires. This paper provides the first steps towards a formal analysis of the interaction betwee</context>
<context position="7049" citStr="Thompson and Mann (1987)" startWordPosition="1110" endWordPosition="1114">same basic structures (discourse representation structures or DRss) that have been used in Discourse Representation Theory (DRT) to represent different attitudes like beliefs and desires (Kamp 1981, Asher 1986, 1987, Kamp 1991, Asher and Singh, 1993). A BRIEF INTRODUCTION TO SDRT AND DICE In SDRT (Asher, 1993), an NL text is represented by a segmented DRS (SDRS), which is a pair of sets containing: the DRss or sDRss representing respectively sentences and text segments, and discourse relations between them. Discourse relations, modelled after those proposed by Hobbs (1985), Polanyi (1985) and Thompson and Mann (1987), link together the constituents of an SDRS. We will mention three: Narration, Result and Evidence. . SDRSS have a hierarchical configuration, and SDRT predicts points of attachment in a discourse structure for new information. Using DICE we infer from the reader&apos;s knowledge resources which discourse relation should be used to do attachment. Lascarides and Asher (1991) introduce default rules representing the role of Gricean pragmatic maxims and domain knowledge in calculating the value of the update function (7, a, /3), which means &amp;quot;the representation of the current sentence is to be attached</context>
</contexts>
<marker>Thompson, Mann, 1987</marker>
<rawString>Thompson, Sandra and Mann, William (1987) Rhetorical Structure Theory: A Framework for the Analysis of Texts. In IPRA Papers in Pragmatics, 1, 79-105.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>