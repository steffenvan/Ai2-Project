<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.994669">
Learning Topic Hierarchies for Wikipedia Categories
</title>
<author confidence="0.7055595">
Linmei Hu†, Xuzhong Wang§, Mengdi Zhang†, Juanzi Li†, Xiaoli Lit,
Chao Shao†, Jie Tang†, Yongbin Liu†† Dept. of Computer Sci. and Tech. Tsinghua University, China
</author>
<affiliation confidence="0.625571">
§ State Key Laboratory of Math. Eng. and Advanced Computing, China
t Institute for Infocomm Research(I2R), A*STAR, Singapore
</affiliation>
<email confidence="0.919057">
{hulinmei1991,koodoneko,mdzhangmd,lijuanzi2008}@gmail.com
xlli@i2r.a-star.edu.sg, birdlinux@gmail.com
jietang@tsinghua.edu.cn, yongbinliu03@gmail.com
</email>
<sectionHeader confidence="0.99735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999572263157895">
Existing studies have utilized Wikipedia
for various knowledge acquisition tasks.
However, no attempts have been made to
explore multi-level topic knowledge con-
tained in Wikipedia articles’ Contents ta-
bles. The articles with similar subjects
are grouped together into Wikipedia cat-
egories. In this work, we propose novel
methods to automatically construct com-
prehensive topic hierarchies for given cat-
egories based on the structured Contents
tables as well as corresponding unstruc-
tured text descriptions. Such a hierarchy is
important for information browsing, doc-
ument organization and topic prediction.
Experimental results show our proposed
approach, incorporating both the structural
and textual information, achieves high
quality category topic hierarchies.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967176470588">
As a free-access online encyclopedia, written
collaboratively by people all over the world,
Wikipedia (abbr. to Wiki) offers a surplus of rich
information. Millions of articles cover various
concepts and instances 1. Wiki has been wide-
ly used for various knowledge discovery tasks.
Some good examples include knowledge mining
from Wiki infoboxes (Lin et al., 2011; Wang et al.,
2013), and taxonomy deriving from Wiki category
system (Zesch and Gurevych, 2007).
We observe that, in addition to Wiki’s infobox-
es and category system, Wiki articles’ Contents
tables (CT for short) also provide valuable struc-
tured topic knowledge with different levels of
granularity. For example, in the article “2010
Haiti Earthquake”, shown in Fig.1, the left Con-
tents zone is a CT formed in a topic hierarchy for-
</bodyText>
<page confidence="0.426405">
lhttp://en.wikipedia.org/wiki/Encyclopedia
</page>
<figureCaption confidence="0.986834333333333">
Figure 1: The Wiki article “2010 Haiti Earth-
quake” with structured Contents table and corre-
sponding unstructured text descriptions.
</figureCaption>
<bodyText confidence="0.973626782608696">
mat. If we view “2010 Haiti earthquake” as the
root topic, the first-level “Geology” and “Dam-
age to infrastructure” tags can be viewed as it-
s subtopics, and the second-level “Tsunami” and
“Aftershocks” tags underneath “Geology” are the
subtopics of “Geology”. Clicking any of the tags
in Contents, we can jump to the corresponding text
description. Wiki articles contain a wealth of this
kind of structured and unstructured information.
However, to our best knowledge, little work has
been done to leverage the knowledge in CT.
In Wiki, similar articles (each with their own
CT) belonging to the same subject are grouped
together into a Wiki category. We aim to inte-
grate multiple topic hierarchies represented by C-
T (from the articles under the same Wiki catego-
ry) into a comprehensive category topic hierar-
chy (CTH). While there also exist manually built
CTH represented by CT in corresponding Wik-
i articles, they are still too high-level and incom-
plete. Take the “Earthquake” category as an exam-
ple, its corresponding Wiki article 2 only contains
zhttp://en.wikipedia.org/wiki/Earthquake
</bodyText>
<page confidence="0.918016">
346
</page>
<bodyText confidence="0.962380361111111">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 346–351,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
some major and common topics. It does not in-
clude the subtopic “nuclear power plant”, which
is an important subtopic of the “2011 Japan earth-
quake”. A comprehensive CTH is believed to be
more useful for information browsing, document
organization and topic extraction in new text cor-
pus (Veeramachaneni et al., 2005). Thus, we pro-
pose to investigate the Wiki articles of the same
category to automatically build a comprehensive
CTH to enhance the manually built CTH.
Clearly, it is very challenging to learn a CTH
from multiple topic hierarchies in different articles
due to the following 3 reasons: 1) A topic can be
denoted by a variety of tags in different articles
(e.g., “foreign aids” and “aids from other coun-
tries”); 2) Structural/hierarchical information can
be inconsistent (or even opposite) across differen-
t articles (e.g., “response subtopicOf aftermath”
and “aftermath subtopicOf response” in different
earthquake event articles); 3) Intuitively, text de-
scriptions of the topics in Wiki articles are sup-
posed to be able to help determine subtopic rela-
tions between topics. However, how can we model
the textual correlation?
In this study, we propose a novel approach to
build a high-quality CTH for any given Wiki cate-
gory. We use a Bayesian network to model a CTH,
and map the CTH learning problem as a structure
learning problem. We leverage both structural and
textual information of topics in the articles to in-
duce the optimal tree structure. Experiments on 3
category data demonstrate the effectiveness of our
approach for CTH learning.
</bodyText>
<sectionHeader confidence="0.993409" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.997653547169811">
Our problem is to learn a CTH for a Wiki catego-
ry from multiple topic hierarchies represented by
CT in the Wiki articles of the category. For ex-
ample, consider the category “earthquake”. There
are a lot of Wikipedia articles about earthquake
events which are manually created by human ex-
perts. In these articles, the CTs imply hierarchi-
cal topic knowledge in the events. However, due
to crowdsourcing nature, these knowledge is het-
erogeneous across different articles. We want to
integrate these knowledge represented by CTs in
different earthquake event articles to form a com-
prehensive understanding of the category “earth-
quake” (CTH).
Specifically, our input consists of a set of Wiki
articles Ac = {a}, belonging to a Wiki category
c. As shown in Fig.1, each article a E Ac con-
tains a CT (topic tree hierarchy) Ha = {Ta, Ra},
where Ta is a set of topics, each denoted by a tag
g and associated with a text description dg, and
Ra = {(gi, gj)}, gi, gj E Ta is a set of subtopic
relations (gj is a subtopic of gi). The output is
an integrated comprehensive CTH Hc = {Tc, Rc}
where Tc = {t} is a set of topics, each denot-
ed by a set of tags t = {g} and associated by
a text description dt aggregated by {dg}gEt, and
Rc = {(ti, tj)}, ti, tj E Tc is a set of subtopic
relations (tj is a subtopic of ti).
We map the problem of learning the output Hc
from the input {Ha}, a E Ac, as a structure learn-
ing problem. We first find clusters of similar tags
Tc (each cluster represents a topic) and then derive
hierarchical relations Rc among these clusters.
Particularly, given a category c, we first col-
lect relevant Wiki articles Ac = {a}. This can
be done automatically since each Wiki article has
links to its categories. We can also manually find
the Wikipage which summarizes the links of Ac
(e.g., http://en.wikipedia.org/wiki/
Lists_of_earthquakes) and then collec-
t Ac according to the links.
Then we can get a global tag set G = {g} con-
taining all the tags including titles in the articles
Ac. We cluster the same or similar tags from dif-
ferent articles using single-pass incremental clus-
tering (Hammouda and Kamel, 2003) to construct
the topic set Tc, with cosine similarity computed
based on the names of tags g and their text de-
scriptions dg. Note that titles of all the articles be-
longing to the same cluster corresponds to a root
topic.
Next, the issue is how to induce a CTH Hc =
{Tc, Rc} from a set of topics Tc.
</bodyText>
<sectionHeader confidence="0.987199" genericHeader="method">
3 Topic Hierarchy Construction
</sectionHeader>
<bodyText confidence="0.999935">
We first present a basic method to learn Hc and
then describe a principled probabilistic model in-
corporating both structural and textual information
for CTH learning.
</bodyText>
<subsectionHeader confidence="0.999544">
3.1 Basic Method
</subsectionHeader>
<bodyText confidence="0.999935166666667">
After replacing the tags in a CT (see Fig.1) with
the topics they belong to, we can then get a top-
ic hierarchy Ha = {Ta, Ra} for each article
a. For each subtopic relation (ti, tj) E Ra, we
can calculate a count/weight n(ti,tj), represent-
ing the number of articles in Ac containing the
</bodyText>
<page confidence="0.992057">
347
</page>
<bodyText confidence="0.999954727272727">
relation. We then construct a directed complete
graph with a weight w(ti, tj) = n(ti, tj) on each
edge (ti, tj). Finally, we apply the widely used
Chu-Liu/Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967) to find an optimal tree with the
largest sum of weights from our constructed graph,
meaning that the overall subtopic relations in the
tree is best supported by all the CT/articles. The
Chu-Liu/Edmonds algorithm works as follows.
First, it selects, for each node, the maximum-
weight incoming edge. Next, it recursively breaks
cycles with the following idea: nodes in a cycle are
collapsed into a pseudo-node and the maximum-
weight edge entering the pseudo-node is selected
to replace the other incoming edge in the cycle.
During backtracking, pseudo-nodes are expanded
into an acyclic directed graph, i.e., our final cate-
gory topic hierarchy Hc.
However, the basic method has a problem.
Consider if n(“earthquake”, “damages to hos-
pitals”)=10 and n(“earthquake”) =100, while
n(“damages”, “damages to hospitals”)=5 and
n(“damages”)=5. We would prefer “damages” to
be the parent topic of “damages to hospitals” with
a higher confidence level (5/5=1 vs 10/100=0.1).
However, the above basic method will choose
“earthquake” which maximizes the weight sum.
An intuitive solution is to normalize the weights.
In Subsection 3.2, we present our proposed princi-
pled probabilistic model which can derive normal-
ized structure based weights. In addition, it can
be easily used to incorporate and combine textual
information of topics into CTH learning.
</bodyText>
<subsectionHeader confidence="0.98372">
3.2 Probabilistic Model for CTH Learning
</subsectionHeader>
<bodyText confidence="0.999442">
We first describe the principled probabilistic mod-
el for a CTH. Then we present how to encode
structural dependency and textual correlation be-
tween topics. Last, we present our final approach
combining both structural dependency and textual
correlation for CTH construction.
</bodyText>
<subsectionHeader confidence="0.975173">
3.2.1 Modeling a Category Topic Hierarchy
</subsectionHeader>
<bodyText confidence="0.9953868">
In a topic hierarchy, each node represents a top-
ic. We consider each node as a variable and the
topic hierarchy as a Bayesian network. Then the
joint probability distribution of nodes N given a
particular tree H is
</bodyText>
<equation confidence="0.7968985">
P(N|H) = P(root) � P(n|parH(n)) ,
nEN\root
</equation>
<bodyText confidence="0.9999592">
where P(n|parH(n)) is the conditional probabili-
ty of node n given its parent node parH(n) in H.
Given the nodes, this is actually the likelihood of
H. Maximizing the likelihood with respect to the
tree structure gives the optimal tree:
</bodyText>
<equation confidence="0.978662076923077">
H* = argmaxHP(N|H)
�= argmaxHP(root) P(n|parH(n))
nEN\root
�= argmaxH logP(n|parH(n))
nEN
(1)
Encoding Structural Dependency. Consider-
ing tj is a subtopic of ti, we define the structural
conditional probability:
n(ti, tj) + α
Pstru(tj I ti) =
c (2)
n(ti) + α ·|Tc — 1 |,
</equation>
<bodyText confidence="0.98796515">
where n(ti, tj) is the count of articles containing
relation (ti, tj) and n(ti) is the count of articles
containing topic ti. The parameter α = 1.0 is the
Laplace smoothing factor, and |Tc — 1 |is the to-
tal number of possible relations taking ti as their
parent topic.
Encoding Textual Correlation. Considering
a topic text description dt as a bag of words,
we use the normalized word frequencies ϕt =
1ϕt,wIwEV s.t. KwEV ϕt,w = 1 to represent a top-
ic t. To capture the subtopic relationship (ti, tj),
we prefer a model where the expectation of the
distribution for the child is exactly same with the
distribution for its parent, i.e., E(ϕtj) = ϕti.
This naturally leads to the hierarchical Dirichlet
model (Wang et al., 2014; Veeramachaneni et al.,
2005), formally, ϕtj|ϕti — Dir(βϕti) in which β
3 is the concentration parameter which determines
how concentrated the probability density is likely
to be. Thus we have:
</bodyText>
<equation confidence="0.8551068">
1 ri
Ptext(tj|ti) = Z
wEV
where Z = ∏wEV r(αϕti,w) is a normalization fac-
r(∑wEV αϕti,w)
</equation>
<bodyText confidence="0.999853333333333">
tor and 1&apos;(·) is the standard Gamma distribution.
We note that for the root node we have the uniform
prior instead of the prior coming from the parent.
</bodyText>
<subsectionHeader confidence="0.9478995">
3.2.2 Combining Structural and Textual
Information
</subsectionHeader>
<bodyText confidence="0.971486">
Substituting Eq.2 into Eq.1, we can solve
the optimal tree structure by applying Chu-
</bodyText>
<footnote confidence="0.558418">
3Experimental results are insensitive to ,Q, we set ,Q=5
</footnote>
<equation confidence="0.487248">
βϕti,w−1 , (3)
ϕtj,v
</equation>
<page confidence="0.987032">
348
</page>
<bodyText confidence="0.987846">
Liu/Edmonds algorithm to the directed com-
plete graph with structure based weights
</bodyText>
<equation confidence="0.973512">
n(ti,tj )+α
wstruc=log(Pstruc(tj  |ti) = logn(ti)+α·jT.−1j
</equation>
<bodyText confidence="0.9991382">
on the edges (ti, tj). While this solves the
problem of the basic method, it only considers
structural dependency and does not consider
textual correlation which is supposed to be useful.
Therefore, we calculate text based weights
</bodyText>
<equation confidence="0.9962435">
αoti, w−1
wtext=log(Ptext(tj|ti) = EwEV logϕtj,v
</equation>
<bodyText confidence="0.910004">
logZ similarly. Then we combine structural in-
formation and textual information by defining
the weights w(ti, tj) of the edges (ti, tj) as a
simple weighted average of wstruc(ti, tj) and
wtext(ti, tj). Specifically, we define:
</bodyText>
<equation confidence="0.769895">
w(ti, tj) = λwtext(ti, tj) + (1 − λ)wstruc(ti, tj) ,
</equation>
<bodyText confidence="0.9999818">
where λ controls the impacts of text correla-
tion and structure dependency in optimal structure
learning. Note that wtext and wstruc should be s-
caled 4 first before applying Chu-Liu/Edmonds al-
gorithm to find an optimal topic hierarchy.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999884666666667">
We evaluate the CTH automatically generated by
our proposed methods via comparing it with a
manually constructed ground-truth CTH.
</bodyText>
<subsectionHeader confidence="0.990938">
4.1 Data and Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999749736842105">
Data. We evaluate our methods on 3 categories,
i.e., English “earthquake” and “election” cate-
gories containing 293 and 60 articles, and Chi-
nese “earthquake” category containing 48 articles
5. After removing noisy tags such as “references”
and “see also”, they contain 463, 79 and 426 u-
nique tags respectively. After tag clustering 6, we
can get 176, 57 and 112 topics for each category.
Evaluation Metric. We employ the precision
measure to evaluate the performance of our meth-
ods. Let R and Rs be subtopic relation sets of
our generated result and ground-truth result re-
spectively, then precison=|R ∩ Rs|/|R|. Due to
the number of relations |R|=|Rs |= |Tc − 1|, we
have precison=recall=F1=|R ∩ Rs|/|R|.
We compare three methods, including our basic
method (Basic) which uses only non-normalized
structural information, our proposed probabilis-
tic method considering only structural information
</bodyText>
<footnote confidence="0.981841">
4We use min-max normalization x* = x−min
max−min
5We filter articles with little information in Contents.
6We use an incremental clustering algorithm
</footnote>
<bodyText confidence="0.9829805">
(λ = 0) (Pro+S), and considering both structural
and textual information (0 &lt; λ &lt; 1) (Pro+ST).
</bodyText>
<subsectionHeader confidence="0.933059">
4.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999077375">
Quantitative Analysis. From Table 1, we observe
that our approach Pro+ST (with best λ values as
shown in Fig.2) significantly outperforms Basic
and Pro+S which only utilize the structural infor-
mation (+24.3% and +5.1% on average, p &lt;0.025
with t-test). Pro+S which normalizes structural in-
formation also achieves significant higher preci-
sion than Basic (+19.2% on average, p &lt;0.025).
</bodyText>
<table confidence="0.99584425">
Earth.(En) Elect.(En) Earth.(Ch)
Basic 0.5965 0.7719 0.7143
Pro+S 0.8971 0.8596 0.9017
Pro+ST 0.9543 0.9298 0.9286
</table>
<tableCaption confidence="0.905071">
Table 1: Precision of different methods on 3 cate-
gories
</tableCaption>
<figureCaption confidence="0.990361">
Figure 2: The precision of CTH with different λ
values
</figureCaption>
<bodyText confidence="0.999948444444445">
To examine the influence of λ, we show the
performance of our approach Pro+ST with dif-
ferent λ values on 3 categories in Fig.2. All the
curves grow up first and then decrease dramatical-
ly as we emphasize more on textual information.
They can always get consistent better results when
0.2≤ λ ≤0.3. When λ approaches 1, the precision
declines fast to near 0. The reason is that the top-
ics with short (or null) text descriptions are likely
to be a parent node of all other nodes and influ-
ence the results dramatically, but if we rely mostly
on structural information and use the textual infor-
mation as auxiliary for correcting minor errors in
some ambiguous cases, we can improve the preci-
sion of the resultant topic hierarchy.
Qualitative Analysis. Due to space limitation,
we only show the topic hierarchy for “Election”
with smaller topic size in Fig.3. As we can see,
</bodyText>
<equation confidence="0.912398">
−
</equation>
<page confidence="0.996147">
349
</page>
<bodyText confidence="0.746536666666667">
fresher to quickly familiarize himself/herself with
any new category, and is very useful for informa-
tion browsing, organization and topic extraction.
</bodyText>
<figure confidence="0.89144">
6 Conclusion
</figure>
<figureCaption confidence="0.992944">
Figure 3: The category topic hierarchy for presi-
dential elections. Topics are labeled by tags sepa-
rated by “#”.
</figureCaption>
<bodyText confidence="0.998956545454545">
the root topic “presidential elections” includes
subtopics “results”, “vote”, “official candidates”,
etc. Furthermore, ‘official candidates” contain-
s subtopics “debates, “rejected candidates”, “un-
successful candidates”, etc. The above mentioned
examples are shown with red edges. However,
there are also a few (7%) mistaken relations (black
edges) such as “comparison” (should be “official
candidates” instead) —* “official candidate web-
sites”. Overall, the above hierarchy aligns well
with human knowledge.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999986029411764">
To our best knowledge, our overall problem set-
ting is novel and there is no previous work using
Wiki articles’ contents tables to learn topic hier-
archies for categories. Existing work mainly fo-
cused on learning topic hierarchies from texts only
and used traditional hierarchical clustering meth-
ods (Chuang and Chien, 2004) or topic models
such as HLDA (Griffiths and Tenenbaum, 2004),
HPAM (Mimno et al., 2007), hHDP (Zavitsanos
et al., 2011), and HETM (Hu et al., 2015). Differ-
ently, we focus on structured contents tables with
corresponding text descriptions.
Our work is also different from ontology (tax-
onomy) construction (Li et al., 2007; Tang et al.,
2009; Zhu et al., 2013; Navigli et al., 2011; Wu
et al., 2012) as their focus is concept hierarchies
(e.g. isA relation) rather than thematic topic hier-
archies. For example, given the “animals” cate-
gory, they may derive “cats” and “dogs”, etc. as
subcategories, while our work aims to derive the-
matic topics “animal protection” and “animal ex-
tinction”, etc. as subtopics. Our work enables a
In this paper, we propose an innovative problem,
i.e., to construct high quality comprehensive top-
ic hierarchies for different Wiki categories using
their associated Wiki articles. Our novel approach
is able to model a topic hierarchy and to incorpo-
rate both structural dependencies and text correla-
tions into the optimal tree learning. Experimental
results demonstrate the effectiveness of our pro-
posed approach. In future work, we will inves-
tigate how to update the category topic hierarchy
incrementally with the creation of new related ar-
ticles.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.895719888888889">
The work is supported by 973 Program
(No. 2014CB340504), NSFC-ANR (No.
61261130588), Tsinghua University Initiative
Scientific Research Program (No. 20131089256),
Science and Technology Support Program (No.
2014BAK04B00), China Postdoctoral Science
Foundation (No. 2014M550733), National Natu-
ral Science Foundation of China (No. 61309007)
and THU-NUS NExT Co-Lab.
</reference>
<sectionHeader confidence="0.978467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7756205">
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On short-
est arborescence of a directed graph. Scientia Sini-
ca, 14(10):1396.
Shui-Lung Chuang and Lee-Feng Chien. 2004. A
practical web-based approach to generating topic hi-
erarchy for text segments. In Proceedings of the
thirteenth ACM international conference on Infor-
mation and knowledge management, pages 127–
136. ACM.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(4):233–240.
DMBTL Griffiths and MIJJB Tenenbaum. 2004. Hier-
archical topic models and the nested chinese restau-
rant process. Advances in NIPS, 16:17.
Khaled M Hammouda and Mohamed S Kamel. 2003.
Incremental document clustering using cluster sim-
ilarity histograms. In Web Intelligence, 2003. WI
2003. Proceedings. IEEE/WIC International Con-
ference on, pages 597–601. IEEE.
</reference>
<page confidence="0.985605">
350
</page>
<reference confidence="0.999695603174603">
Linmei Hu, Juanzi Li, Jing Zhang, and Chao Shao.
2015. o-hetm: An online hierarchical entity topic
model for news streams. In Advances in Knowledge
Discovery and Data Mining - 19th Pacific-Asia Con-
ference, PAKDD 2015, Proceedings, Part I, pages
696–707.
Rui Li, Shenghua Bao, Yong Yu, Ben Fei, and Zhong
Su. 2007. Towards effective browsing of large s-
cale social annotations. In Proceedings of the 16th
international conference on World Wide Web, pages
943–952. ACM.
Wen-Pin Lin, Matthew Snover, and Heng Ji. 2011. Un-
supervised language-independent name translation
mining from wikipedia infoboxes. In Proceedings
of the First workshop on Unsupervised Learning in
NLP, pages 43–52. Association for Computational
Linguistics.
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko allo-
cation. In Proceedings of the 24th ICML, pages
633–640. ACM.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In IJCAI, pages 1872–
1877.
Jie Tang, Ho-fung Leung, Qiong Luo, Dewei Chen,
and Jibin Gong. 2009. Towards ontology learn-
ing from folksonomies. In IJCAI, volume 9, pages
2089–2094.
Sriharsha Veeramachaneni, Diego Sona, and Paolo
Avesani. 2005. Hierarchical dirichlet model for
document classification. In Proceedings of the 22nd
ICML, pages 928–935. ACM.
Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang, and
Jeff Z Pan. 2013. Transfer learning based cross-
lingual knowledge extraction for wikipedia. In ACL
(1), pages 641–650.
Jingjing Wang, Changsung Kang, Yi Chang, and Jiawei
Han. 2014. A hierarchical dirichlet model for tax-
onomy expansion for search engines. In Proceed-
ings of the 23rd international conference on WWW,
pages 961–970. International World Wide Web Con-
ferences Steering Committee.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q
Zhu. 2012. Probase: A probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management
of Data, pages 481–492. ACM.
Elias Zavitsanos, Georgios Paliouras, and George A
Vouros. 2011. Non-parametric estimation of top-
ic hierarchies from texts with hierarchical dirichlet
processes. The Journal of Machine Learning Re-
search, 12:2749–2775.
Torsten Zesch and Iryna Gurevych. 2007. Analysis
of the wikipedia category graph for nlp application-
s. In Proceedings of the TextGraphs-2 Workshop
(NAACL-HLT 2007), pages 1–8.
Xingwei Zhu, Zhao-Yan Ming, Xiaoyan Zhu, and Tat-
Seng Chua. 2013. Topic hierarchy construction for
the organization of multi-source user generated con-
tents. In Proceedings of the 36th international ACM
SIGIR conference on Research and development in
information retrieval, pages 233–242. ACM.
</reference>
<page confidence="0.998663">
351
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.328025">
<title confidence="0.999983">Learning Topic Hierarchies for Wikipedia Categories</title>
<author confidence="0.996853">Xuzhong Mengdi Juanzi Xiaoli</author>
<affiliation confidence="0.930909">Jie Yongbin of Computer Sci. and Tech. Tsinghua University, Key Laboratory of Math. Eng. and Advanced Computing,</affiliation>
<address confidence="0.567771">for Infocomm Research(I2R), A*STAR,</address>
<email confidence="0.719537">xlli@i2r.a-star.edu.sg,jietang@tsinghua.edu.cn,yongbinliu03@gmail.com</email>
<abstract confidence="0.99939485">Existing studies have utilized Wikipedia for various knowledge acquisition tasks. However, no attempts have been made to explore multi-level topic knowledge conin Wikipedia articles’ tables. The articles with similar subjects grouped together into Wikipedia cat- In this work, we propose novel to automatically construct comhierarchies for given catbased on the structured tables as well as corresponding unstruc- Such a hierarchy is important for information browsing, document organization and topic prediction. Experimental results show our proposed approach, incorporating both the structural and textual information, achieves high quality category topic hierarchies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>The work is supported by 973 Program (No.</title>
<booktitle>Initiative Scientific Research Program (No. 20131089256), Science and Technology Support Program (No. 2014BAK04B00), China Postdoctoral Science Foundation (No. 2014M550733), National Natural Science Foundation of China (No. 61309007) and THU-NUS NExT Co-Lab.</booktitle>
<tech>2014CB340504), NSFC-ANR (No. 61261130588),</tech>
<institution>Tsinghua University</institution>
<marker></marker>
<rawString>The work is supported by 973 Program (No. 2014CB340504), NSFC-ANR (No. 61261130588), Tsinghua University Initiative Scientific Research Program (No. 20131089256), Science and Technology Support Program (No. 2014BAK04B00), China Postdoctoral Science Foundation (No. 2014M550733), National Natural Science Foundation of China (No. 61309007) and THU-NUS NExT Co-Lab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoeng-Jin Chu</author>
<author>Tseng-Hong Liu</author>
</authors>
<title>On shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Scientia Sinica,</journal>
<volume>14</volume>
<issue>10</issue>
<contexts>
<context position="8308" citStr="Chu and Liu, 1965" startWordPosition="1339" endWordPosition="1342">then describe a principled probabilistic model incorporating both structural and textual information for CTH learning. 3.1 Basic Method After replacing the tags in a CT (see Fig.1) with the topics they belong to, we can then get a topic hierarchy Ha = {Ta, Ra} for each article a. For each subtopic relation (ti, tj) E Ra, we can calculate a count/weight n(ti,tj), representing the number of articles in Ac containing the 347 relation. We then construct a directed complete graph with a weight w(ti, tj) = n(ti, tj) on each edge (ti, tj). Finally, we apply the widely used Chu-Liu/Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to find an optimal tree with the largest sum of weights from our constructed graph, meaning that the overall subtopic relations in the tree is best supported by all the CT/articles. The Chu-Liu/Edmonds algorithm works as follows. First, it selects, for each node, the maximumweight incoming edge. Next, it recursively breaks cycles with the following idea: nodes in a cycle are collapsed into a pseudo-node and the maximumweight edge entering the pseudo-node is selected to replace the other incoming edge in the cycle. During backtracking, pseudo-nodes are expanded into an acyclic </context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest arborescence of a directed graph. Scientia Sinica, 14(10):1396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shui-Lung Chuang</author>
<author>Lee-Feng Chien</author>
</authors>
<title>A practical web-based approach to generating topic hierarchy for text segments.</title>
<date>2004</date>
<booktitle>In Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>127--136</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17042" citStr="Chuang and Chien, 2004" startWordPosition="2738" endWordPosition="2741">c. The above mentioned examples are shown with red edges. However, there are also a few (7%) mistaken relations (black edges) such as “comparison” (should be “official candidates” instead) —* “official candidate websites”. Overall, the above hierarchy aligns well with human knowledge. 5 Related Work To our best knowledge, our overall problem setting is novel and there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, wh</context>
</contexts>
<marker>Chuang, Chien, 2004</marker>
<rawString>Shui-Lung Chuang and Lee-Feng Chien. 2004. A practical web-based approach to generating topic hierarchy for text segments. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 127– 136. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards B,</journal>
<volume>71</volume>
<issue>4</issue>
<contexts>
<context position="8324" citStr="Edmonds, 1967" startWordPosition="1343" endWordPosition="1344">ncipled probabilistic model incorporating both structural and textual information for CTH learning. 3.1 Basic Method After replacing the tags in a CT (see Fig.1) with the topics they belong to, we can then get a topic hierarchy Ha = {Ta, Ra} for each article a. For each subtopic relation (ti, tj) E Ra, we can calculate a count/weight n(ti,tj), representing the number of articles in Ac containing the 347 relation. We then construct a directed complete graph with a weight w(ti, tj) = n(ti, tj) on each edge (ti, tj). Finally, we apply the widely used Chu-Liu/Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to find an optimal tree with the largest sum of weights from our constructed graph, meaning that the overall subtopic relations in the tree is best supported by all the CT/articles. The Chu-Liu/Edmonds algorithm works as follows. First, it selects, for each node, the maximumweight incoming edge. Next, it recursively breaks cycles with the following idea: nodes in a cycle are collapsed into a pseudo-node and the maximumweight edge entering the pseudo-node is selected to replace the other incoming edge in the cycle. During backtracking, pseudo-nodes are expanded into an acyclic directed graph, </context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>Jack Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards B, 71(4):233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DMBTL Griffiths</author>
<author>MIJJB Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2004</date>
<booktitle>Advances in NIPS,</booktitle>
<pages>16--17</pages>
<contexts>
<context position="17103" citStr="Griffiths and Tenenbaum, 2004" startWordPosition="2748" endWordPosition="2751">es. However, there are also a few (7%) mistaken relations (black edges) such as “comparison” (should be “official candidates” instead) —* “official candidate websites”. Overall, the above hierarchy aligns well with human knowledge. 5 Related Work To our best knowledge, our overall problem setting is novel and there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protectio</context>
</contexts>
<marker>Griffiths, Tenenbaum, 2004</marker>
<rawString>DMBTL Griffiths and MIJJB Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. Advances in NIPS, 16:17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled M Hammouda</author>
<author>Mohamed S Kamel</author>
</authors>
<title>Incremental document clustering using cluster similarity histograms.</title>
<date>2003</date>
<booktitle>In Web Intelligence, 2003. WI 2003. Proceedings. IEEE/WIC International Conference on,</booktitle>
<pages>597--601</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7313" citStr="Hammouda and Kamel, 2003" startWordPosition="1157" endWordPosition="1160">rive hierarchical relations Rc among these clusters. Particularly, given a category c, we first collect relevant Wiki articles Ac = {a}. This can be done automatically since each Wiki article has links to its categories. We can also manually find the Wikipage which summarizes the links of Ac (e.g., http://en.wikipedia.org/wiki/ Lists_of_earthquakes) and then collect Ac according to the links. Then we can get a global tag set G = {g} containing all the tags including titles in the articles Ac. We cluster the same or similar tags from different articles using single-pass incremental clustering (Hammouda and Kamel, 2003) to construct the topic set Tc, with cosine similarity computed based on the names of tags g and their text descriptions dg. Note that titles of all the articles belonging to the same cluster corresponds to a root topic. Next, the issue is how to induce a CTH Hc = {Tc, Rc} from a set of topics Tc. 3 Topic Hierarchy Construction We first present a basic method to learn Hc and then describe a principled probabilistic model incorporating both structural and textual information for CTH learning. 3.1 Basic Method After replacing the tags in a CT (see Fig.1) with the topics they belong to, we can th</context>
</contexts>
<marker>Hammouda, Kamel, 2003</marker>
<rawString>Khaled M Hammouda and Mohamed S Kamel. 2003. Incremental document clustering using cluster similarity histograms. In Web Intelligence, 2003. WI 2003. Proceedings. IEEE/WIC International Conference on, pages 597–601. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linmei Hu</author>
<author>Juanzi Li</author>
<author>Jing Zhang</author>
<author>Chao Shao</author>
</authors>
<title>o-hetm: An online hierarchical entity topic model for news streams.</title>
<date>2015</date>
<booktitle>In Advances in Knowledge Discovery and Data Mining - 19th Pacific-Asia Conference, PAKDD 2015, Proceedings, Part I,</booktitle>
<pages>696--707</pages>
<contexts>
<context position="17190" citStr="Hu et al., 2015" startWordPosition="2764" endWordPosition="2767">be “official candidates” instead) —* “official candidate websites”. Overall, the above hierarchy aligns well with human knowledge. 5 Related Work To our best knowledge, our overall problem setting is novel and there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”, etc. as subtopics. Our work enables a In this paper, we pro</context>
</contexts>
<marker>Hu, Li, Zhang, Shao, 2015</marker>
<rawString>Linmei Hu, Juanzi Li, Jing Zhang, and Chao Shao. 2015. o-hetm: An online hierarchical entity topic model for news streams. In Advances in Knowledge Discovery and Data Mining - 19th Pacific-Asia Conference, PAKDD 2015, Proceedings, Part I, pages 696–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Li</author>
<author>Shenghua Bao</author>
<author>Yong Yu</author>
<author>Ben Fei</author>
<author>Zhong Su</author>
</authors>
<title>Towards effective browsing of large scale social annotations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>943--952</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17363" citStr="Li et al., 2007" startWordPosition="2790" endWordPosition="2793">verall problem setting is novel and there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”, etc. as subtopics. Our work enables a In this paper, we propose an innovative problem, i.e., to construct high quality comprehensive topic hierarchies for different Wiki categories using their associated Wiki articles. Our novel app</context>
</contexts>
<marker>Li, Bao, Yu, Fei, Su, 2007</marker>
<rawString>Rui Li, Shenghua Bao, Yong Yu, Ben Fei, and Zhong Su. 2007. Towards effective browsing of large scale social annotations. In Proceedings of the 16th international conference on World Wide Web, pages 943–952. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Pin Lin</author>
<author>Matthew Snover</author>
<author>Heng Ji</author>
</authors>
<title>Unsupervised language-independent name translation mining from wikipedia infoboxes.</title>
<date>2011</date>
<booktitle>In Proceedings of the First workshop on Unsupervised Learning in NLP,</booktitle>
<pages>43--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1641" citStr="Lin et al., 2011" startWordPosition="215" endWordPosition="218">important for information browsing, document organization and topic prediction. Experimental results show our proposed approach, incorporating both the structural and textual information, achieves high quality category topic hierarchies. 1 Introduction As a free-access online encyclopedia, written collaboratively by people all over the world, Wikipedia (abbr. to Wiki) offers a surplus of rich information. Millions of articles cover various concepts and instances 1. Wiki has been widely used for various knowledge discovery tasks. Some good examples include knowledge mining from Wiki infoboxes (Lin et al., 2011; Wang et al., 2013), and taxonomy deriving from Wiki category system (Zesch and Gurevych, 2007). We observe that, in addition to Wiki’s infoboxes and category system, Wiki articles’ Contents tables (CT for short) also provide valuable structured topic knowledge with different levels of granularity. For example, in the article “2010 Haiti Earthquake”, shown in Fig.1, the left Contents zone is a CT formed in a topic hierarchy forlhttp://en.wikipedia.org/wiki/Encyclopedia Figure 1: The Wiki article “2010 Haiti Earthquake” with structured Contents table and corresponding unstructured text descrip</context>
</contexts>
<marker>Lin, Snover, Ji, 2011</marker>
<rawString>Wen-Pin Lin, Matthew Snover, and Heng Ji. 2011. Unsupervised language-independent name translation mining from wikipedia infoboxes. In Proceedings of the First workshop on Unsupervised Learning in NLP, pages 43–52. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with pachinko allocation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th ICML,</booktitle>
<pages>633--640</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17130" citStr="Mimno et al., 2007" startWordPosition="2753" endWordPosition="2756"> mistaken relations (black edges) such as “comparison” (should be “official candidates” instead) —* “official candidate websites”. Overall, the above hierarchy aligns well with human knowledge. 5 Related Work To our best knowledge, our overall problem setting is novel and there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”,</context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>David Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. In Proceedings of the 24th ICML, pages 633–640. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A graph-based algorithm for inducing lexical taxonomies from scratch.</title>
<date>2011</date>
<booktitle>In IJCAI,</booktitle>
<pages>1872--1877</pages>
<contexts>
<context position="17422" citStr="Navigli et al., 2011" startWordPosition="2802" endWordPosition="2805">us work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”, etc. as subtopics. Our work enables a In this paper, we propose an innovative problem, i.e., to construct high quality comprehensive topic hierarchies for different Wiki categories using their associated Wiki articles. Our novel approach is able to model a topic hierarchy and to incorporate</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A graph-based algorithm for inducing lexical taxonomies from scratch. In IJCAI, pages 1872– 1877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Tang</author>
<author>Ho-fung Leung</author>
<author>Qiong Luo</author>
<author>Dewei Chen</author>
<author>Jibin Gong</author>
</authors>
<title>Towards ontology learning from folksonomies.</title>
<date>2009</date>
<booktitle>In IJCAI,</booktitle>
<volume>9</volume>
<pages>2089--2094</pages>
<contexts>
<context position="17382" citStr="Tang et al., 2009" startWordPosition="2794" endWordPosition="2797">tting is novel and there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”, etc. as subtopics. Our work enables a In this paper, we propose an innovative problem, i.e., to construct high quality comprehensive topic hierarchies for different Wiki categories using their associated Wiki articles. Our novel approach is able to mo</context>
</contexts>
<marker>Tang, Leung, Luo, Chen, Gong, 2009</marker>
<rawString>Jie Tang, Ho-fung Leung, Qiong Luo, Dewei Chen, and Jibin Gong. 2009. Towards ontology learning from folksonomies. In IJCAI, volume 9, pages 2089–2094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriharsha Veeramachaneni</author>
<author>Diego Sona</author>
<author>Paolo Avesani</author>
</authors>
<title>Hierarchical dirichlet model for document classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd ICML,</booktitle>
<pages>928--935</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3935" citStr="Veeramachaneni et al., 2005" startWordPosition="569" endWordPosition="572">n.wikipedia.org/wiki/Earthquake 346 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 346–351, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics some major and common topics. It does not include the subtopic “nuclear power plant”, which is an important subtopic of the “2011 Japan earthquake”. A comprehensive CTH is believed to be more useful for information browsing, document organization and topic extraction in new text corpus (Veeramachaneni et al., 2005). Thus, we propose to investigate the Wiki articles of the same category to automatically build a comprehensive CTH to enhance the manually built CTH. Clearly, it is very challenging to learn a CTH from multiple topic hierarchies in different articles due to the following 3 reasons: 1) A topic can be denoted by a variety of tags in different articles (e.g., “foreign aids” and “aids from other countries”); 2) Structural/hierarchical information can be inconsistent (or even opposite) across different articles (e.g., “response subtopicOf aftermath” and “aftermath subtopicOf response” in different</context>
<context position="11541" citStr="Veeramachaneni et al., 2005" startWordPosition="1865" endWordPosition="1868">he parameter α = 1.0 is the Laplace smoothing factor, and |Tc — 1 |is the total number of possible relations taking ti as their parent topic. Encoding Textual Correlation. Considering a topic text description dt as a bag of words, we use the normalized word frequencies ϕt = 1ϕt,wIwEV s.t. KwEV ϕt,w = 1 to represent a topic t. To capture the subtopic relationship (ti, tj), we prefer a model where the expectation of the distribution for the child is exactly same with the distribution for its parent, i.e., E(ϕtj) = ϕti. This naturally leads to the hierarchical Dirichlet model (Wang et al., 2014; Veeramachaneni et al., 2005), formally, ϕtj|ϕti — Dir(βϕti) in which β 3 is the concentration parameter which determines how concentrated the probability density is likely to be. Thus we have: 1 ri Ptext(tj|ti) = Z wEV where Z = ∏wEV r(αϕti,w) is a normalization facr(∑wEV αϕti,w) tor and 1&apos;(·) is the standard Gamma distribution. We note that for the root node we have the uniform prior instead of the prior coming from the parent. 3.2.2 Combining Structural and Textual Information Substituting Eq.2 into Eq.1, we can solve the optimal tree structure by applying Chu3Experimental results are insensitive to ,Q, we set ,Q=5 βϕt</context>
</contexts>
<marker>Veeramachaneni, Sona, Avesani, 2005</marker>
<rawString>Sriharsha Veeramachaneni, Diego Sona, and Paolo Avesani. 2005. Hierarchical dirichlet model for document classification. In Proceedings of the 22nd ICML, pages 928–935. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhigang Wang</author>
<author>Zhixing Li</author>
<author>Juanzi Li</author>
<author>Jie Tang</author>
<author>Jeff Z Pan</author>
</authors>
<title>Transfer learning based crosslingual knowledge extraction for wikipedia.</title>
<date>2013</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>641--650</pages>
<contexts>
<context position="1661" citStr="Wang et al., 2013" startWordPosition="219" endWordPosition="222">rmation browsing, document organization and topic prediction. Experimental results show our proposed approach, incorporating both the structural and textual information, achieves high quality category topic hierarchies. 1 Introduction As a free-access online encyclopedia, written collaboratively by people all over the world, Wikipedia (abbr. to Wiki) offers a surplus of rich information. Millions of articles cover various concepts and instances 1. Wiki has been widely used for various knowledge discovery tasks. Some good examples include knowledge mining from Wiki infoboxes (Lin et al., 2011; Wang et al., 2013), and taxonomy deriving from Wiki category system (Zesch and Gurevych, 2007). We observe that, in addition to Wiki’s infoboxes and category system, Wiki articles’ Contents tables (CT for short) also provide valuable structured topic knowledge with different levels of granularity. For example, in the article “2010 Haiti Earthquake”, shown in Fig.1, the left Contents zone is a CT formed in a topic hierarchy forlhttp://en.wikipedia.org/wiki/Encyclopedia Figure 1: The Wiki article “2010 Haiti Earthquake” with structured Contents table and corresponding unstructured text descriptions. mat. If we vi</context>
</contexts>
<marker>Wang, Li, Li, Tang, Pan, 2013</marker>
<rawString>Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang, and Jeff Z Pan. 2013. Transfer learning based crosslingual knowledge extraction for wikipedia. In ACL (1), pages 641–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Wang</author>
<author>Changsung Kang</author>
<author>Yi Chang</author>
<author>Jiawei Han</author>
</authors>
<title>A hierarchical dirichlet model for taxonomy expansion for search engines.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd international conference on WWW,</booktitle>
<pages>961--970</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="11511" citStr="Wang et al., 2014" startWordPosition="1861" endWordPosition="1864">taining topic ti. The parameter α = 1.0 is the Laplace smoothing factor, and |Tc — 1 |is the total number of possible relations taking ti as their parent topic. Encoding Textual Correlation. Considering a topic text description dt as a bag of words, we use the normalized word frequencies ϕt = 1ϕt,wIwEV s.t. KwEV ϕt,w = 1 to represent a topic t. To capture the subtopic relationship (ti, tj), we prefer a model where the expectation of the distribution for the child is exactly same with the distribution for its parent, i.e., E(ϕtj) = ϕti. This naturally leads to the hierarchical Dirichlet model (Wang et al., 2014; Veeramachaneni et al., 2005), formally, ϕtj|ϕti — Dir(βϕti) in which β 3 is the concentration parameter which determines how concentrated the probability density is likely to be. Thus we have: 1 ri Ptext(tj|ti) = Z wEV where Z = ∏wEV r(αϕti,w) is a normalization facr(∑wEV αϕti,w) tor and 1&apos;(·) is the standard Gamma distribution. We note that for the root node we have the uniform prior instead of the prior coming from the parent. 3.2.2 Combining Structural and Textual Information Substituting Eq.2 into Eq.1, we can solve the optimal tree structure by applying Chu3Experimental results are inse</context>
</contexts>
<marker>Wang, Kang, Chang, Han, 2014</marker>
<rawString>Jingjing Wang, Changsung Kang, Yi Chang, and Jiawei Han. 2014. A hierarchical dirichlet model for taxonomy expansion for search engines. In Proceedings of the 23rd international conference on WWW, pages 961–970. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wentao Wu</author>
<author>Hongsong Li</author>
<author>Haixun Wang</author>
<author>Kenny Q Zhu</author>
</authors>
<title>Probase: A probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>481--492</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17440" citStr="Wu et al., 2012" startWordPosition="2806" endWordPosition="2809">icles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”, etc. as subtopics. Our work enables a In this paper, we propose an innovative problem, i.e., to construct high quality comprehensive topic hierarchies for different Wiki categories using their associated Wiki articles. Our novel approach is able to model a topic hierarchy and to incorporate both structural d</context>
</contexts>
<marker>Wu, Li, Wang, Zhu, 2012</marker>
<rawString>Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q Zhu. 2012. Probase: A probabilistic taxonomy for text understanding. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, pages 481–492. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Zavitsanos</author>
</authors>
<title>Georgios Paliouras, and George A Vouros.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2749</pages>
<marker>Zavitsanos, 2011</marker>
<rawString>Elias Zavitsanos, Georgios Paliouras, and George A Vouros. 2011. Non-parametric estimation of topic hierarchies from texts with hierarchical dirichlet processes. The Journal of Machine Learning Research, 12:2749–2775.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Analysis of the wikipedia category graph for nlp applications.</title>
<date>2007</date>
<booktitle>In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1737" citStr="Zesch and Gurevych, 2007" startWordPosition="230" endWordPosition="233">ental results show our proposed approach, incorporating both the structural and textual information, achieves high quality category topic hierarchies. 1 Introduction As a free-access online encyclopedia, written collaboratively by people all over the world, Wikipedia (abbr. to Wiki) offers a surplus of rich information. Millions of articles cover various concepts and instances 1. Wiki has been widely used for various knowledge discovery tasks. Some good examples include knowledge mining from Wiki infoboxes (Lin et al., 2011; Wang et al., 2013), and taxonomy deriving from Wiki category system (Zesch and Gurevych, 2007). We observe that, in addition to Wiki’s infoboxes and category system, Wiki articles’ Contents tables (CT for short) also provide valuable structured topic knowledge with different levels of granularity. For example, in the article “2010 Haiti Earthquake”, shown in Fig.1, the left Contents zone is a CT formed in a topic hierarchy forlhttp://en.wikipedia.org/wiki/Encyclopedia Figure 1: The Wiki article “2010 Haiti Earthquake” with structured Contents table and corresponding unstructured text descriptions. mat. If we view “2010 Haiti earthquake” as the root topic, the first-level “Geology” and </context>
</contexts>
<marker>Zesch, Gurevych, 2007</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT 2007), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingwei Zhu</author>
<author>Zhao-Yan Ming</author>
<author>Xiaoyan Zhu</author>
<author>TatSeng Chua</author>
</authors>
<title>Topic hierarchy construction for the organization of multi-source user generated contents.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17400" citStr="Zhu et al., 2013" startWordPosition="2798" endWordPosition="2801">there is no previous work using Wiki articles’ contents tables to learn topic hierarchies for categories. Existing work mainly focused on learning topic hierarchies from texts only and used traditional hierarchical clustering methods (Chuang and Chien, 2004) or topic models such as HLDA (Griffiths and Tenenbaum, 2004), HPAM (Mimno et al., 2007), hHDP (Zavitsanos et al., 2011), and HETM (Hu et al., 2015). Differently, we focus on structured contents tables with corresponding text descriptions. Our work is also different from ontology (taxonomy) construction (Li et al., 2007; Tang et al., 2009; Zhu et al., 2013; Navigli et al., 2011; Wu et al., 2012) as their focus is concept hierarchies (e.g. isA relation) rather than thematic topic hierarchies. For example, given the “animals” category, they may derive “cats” and “dogs”, etc. as subcategories, while our work aims to derive thematic topics “animal protection” and “animal extinction”, etc. as subtopics. Our work enables a In this paper, we propose an innovative problem, i.e., to construct high quality comprehensive topic hierarchies for different Wiki categories using their associated Wiki articles. Our novel approach is able to model a topic hierar</context>
</contexts>
<marker>Zhu, Ming, Zhu, Chua, 2013</marker>
<rawString>Xingwei Zhu, Zhao-Yan Ming, Xiaoyan Zhu, and TatSeng Chua. 2013. Topic hierarchy construction for the organization of multi-source user generated contents. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 233–242. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>