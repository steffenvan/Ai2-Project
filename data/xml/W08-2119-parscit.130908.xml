<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997303">
A Tree-to-String Phrase-based Model for Statistical Machine Translation
</title>
<author confidence="0.997681">
Thai Phuong Nguyen
</author>
<affiliation confidence="0.9921835">
College of Technology
Vietnam National University, Hanoi
</affiliation>
<email confidence="0.943474">
thainp@vnu.edu.vn
</email>
<author confidence="0.982244">
Akira Shimazu1, Tu-Bao Ho2, Minh Le Nguyen1, and Vinh Van Nguyen1
</author>
<affiliation confidence="0.997961333333333">
1School of Information Science
2School of Knowledge Science
Japan Advanced Institute of Science and Technology
</affiliation>
<email confidence="0.999141">
{shimazu,bao,nguyenml,vinhnv}@jaist.ac.jp
</email>
<sectionHeader confidence="0.994806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988841">
Though phrase-based SMT has achieved high
translation quality, it still lacks of generaliza-
tion ability to capture word order differences
between languages. In this paper we describe
a general method for tree-to-string phrase-
based SMT. We study how syntactic trans-
formation is incorporated into phrase-based
SMT and its effectiveness. We design syntac-
tic transformation models using unlexicalized
form of synchronous context-free grammars.
These models can be learned from source-
parsed bitext. Our system can naturally make
use of both constituent and non-constituent
phrasal translations in the decoding phase. We
considered various levels of syntactic analy-
sis ranging from chunking to full parsing.
Our experimental results of English-Japanese
and English-Vietnamese translation showed
a significant improvement over two baseline
phrase-based SMT systems.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895392857143">
Based on the kind of linguistic information which
is made use of, syntactic SMT can be divided into
four types: tree-to-string, string-to-tree, tree-to-tree,
and hierarchical phrase-based. The tree-to-string ap-
proach (Collins et al., 2005; Nguyen and Shimazu,
2006; Liu et al., 2006 and 2007) supposes that syn-
tax of the source language is known. This approach
can be applied when a source language parser is
available. The string-to-tree approach (Yamada and
Knight, 2001; Galley et al., 2006) focuses on syntactic
modelling of the target language in cases it has syn-
tactic resources such as treebanks and parsers. The
tree-to-tree approach models the syntax of both lan-
guages, therefore extra cost is required. The fourth
approach (Chiang, 2005) constraints phrases under
context-free grammar structure without any require-
ment of linguistic annotation.
In this paper, we present a tree-to-string phrase-
based method which is based on synchronous CFGs.
This method has two important properties: syntactic
transformation is used in the decoding phase includ-
ing a word-to-phrase tree transformation model and
a phrase reordering model; phrases are the basic unit
of translation. Since we design syntactic transforma-
tion models using un-lexicalized synchronous CFGs,
the number of rules is small1. Previous studies on
tree-to-string SMT are different from ours. Collins
et al. Collins et al. (2005) used hand crafted rules to
carry out word reordering in the preprocessing phase
but not decoding phase. Nguyen and Shimazu (2006)
presented a more general method in which lexicalized
syntactic reordering models based on PCFGs can be
learned from source-parsed bitext and then applied in
the preprocessing phase. Liu et al. (2006) changed the
translation unit from phrases to tree-to-string align-
ment templates (TATs) while we do not. TATs was
represented as xRs rules while we use synchronous
CFG rules. In order to overcome the limitation that
TATs can not capture non-constituent phrasal transla-
tions, Liu et al. (2007) proposed forest-to-string rules
while our system can naturally make use of such kind
of phrasal translation by word-to-phrase tree transfor-
mation.
We carried out experiments with two language
pairs English-Japanese and English-Vietnamese. Our
system achieved significant improvements over
Pharaoh, a state-of-the-art phrase-based SMT system.
We also analyzed the dependence of translation qual-
ity on the level of syntactic analysis (shallow or deep).
Figure 1 shows the architecture of our system. The
input of this system is a source-language tree and the
output is a target-language string. This system uses
all features of conventional phrase-based SMT as in
(Koehn et al., 2003). There are two new features in-
cluding a word-to-phrase tree transformation model
and a phrase reordering model. The decoding algo-
</bodyText>
<footnote confidence="0.940038">
1See Section 6.2.
</footnote>
<page confidence="0.888511">
143
</page>
<reference confidence="0.298676">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143–150
Manchester, August 2008
</reference>
<footnote confidence="0.574529">
rithm is a tree-based search algorithm.
</footnote>
<figureCaption confidence="0.975074">
Figure 1: A syntax-directed phrase-based SMT archi-
tecture.
</figureCaption>
<sectionHeader confidence="0.986952" genericHeader="method">
2 Translation Model
</sectionHeader>
<bodyText confidence="0.99988775">
We use an example of English-Vietnamese translation
to demonstrate the translation process as in Figure 2.
Now we describe a tree-to-string SMT model based
on synchronous CFGs. The translation process is:
</bodyText>
<figureCaption confidence="0.992191">
Figure 2: The translation process.
</figureCaption>
<equation confidence="0.977855">
T1 → T2 → T3 → T4 (1)
</equation>
<bodyText confidence="0.996435333333333">
where T1 is a source tree, T2 is a source phrase tree,
T3 is a reordered source phrase tree, and T4 is a target
phrase tree.
Using the first order chain rule, the join probability
over variables (trees) in graphical representation 1 is
approximately calculated by:
</bodyText>
<equation confidence="0.922861666666667">
P(T1, T2, T3, T4) = P(T1)xP(T2|T1)xP(T3|T2)xP(T4|T3)
(2)
P(T1) can be omitted since only one syntactic tree
</equation>
<bodyText confidence="0.9730575">
is used. P(T2|T1) is a word-to-phrase tree transfor-
mation model we describe later. P(T3|T2) is a re-
ordering model. P(T4|T3) can be calculated using a
phrase translation model and a language model. This
is the fundamental equation of our study represented
in this paper. In the next section, we will describe how
to transform a word-based CFG tree into a phrase-
based CFG tree.
</bodyText>
<sectionHeader confidence="0.853075" genericHeader="method">
3 Word-to-Phrase Tree Transformation
</sectionHeader>
<subsectionHeader confidence="0.997109">
3.1 Penn Treebank’s Tree Structure
</subsectionHeader>
<bodyText confidence="0.957565333333333">
According to this formalism, a tree is represented by
phrase structure. If we extract a CFG from a tree or
set of trees, there will be two possible rule forms:
</bodyText>
<listItem confidence="0.9552285">
• A -4 α where α is a sequence of nonterminals
(syntactic categories).
• B -4 ry where ry is a terminal symbol (or a word
in this case).
</listItem>
<bodyText confidence="0.9699385">
We consider an example of a syntactic tree and a
simple CFG extracted from that tree.
</bodyText>
<construct confidence="0.918947166666667">
Sentence: ”I am a student”
Syntactic tree: (S (NP (NN I)) (VP (VBP am) (NP (DT a) (NN
student))))
Rule set: S → NP VP; VP → VBP NP; NP → NN  |DT NN; NN
→ I  |student;
VBP → am; DT → a
</construct>
<bodyText confidence="0.999793666666667">
However, we are considering phrase-based transla-
tion. Therefore the right hand side of the second rule
form must be a sequence of terminal symbols (or a
phrase) but not a single symbol (a word). Suppose
that the phrase table contains a phrase ”am a student”
which leads to the following possible tree structure:
</bodyText>
<construct confidence="0.994635">
Phrase segmentation: ”I  |am a student”
Syntactic tree: (S (NP (NN I)) (VP (VBP am a student)))
Rule set: S → NP VP; VP → VBP; NP → NN; NN → I; VBP →
am a student
</construct>
<bodyText confidence="0.999892">
We have to find out some way to transform a CFG
tree into a tree with phrases at leaves. In the next sub-
section we propose such an algorithm.
</bodyText>
<subsectionHeader confidence="0.9915325">
3.2 An Algorithm for Word-to-Phrase Tree
Transformation
</subsectionHeader>
<bodyText confidence="0.992381333333333">
Table 1 represents our algorithm to transform a CFG
tree to a phrase CFG tree. When designing this algo-
rithm, our criterion is to preserve the original struc-
ture as much as possible. This algorithm includes two
steps. There are a number of notions concerning this
algorithm:
</bodyText>
<listItem confidence="0.992474333333333">
• A CFG rule has a head symbol on the right hand
side. Using this information, head child of a
node on a syntactic tree can be determined.
</listItem>
<page confidence="0.980672">
144
</page>
<table confidence="0.622975833333333">
+ Input: A CFG tree, a phrase segmentation
+ Output: A phrase CFG tree
+ Step 1: Allocate phrases to leaf nodes in a top-down manner: A phrase is allocated to head word of a node if the
phrase contains the head word. This head word is then considered as the phrase head.
+ Step 2: Transform the syntactic tree by replacing leaf nodes by their allocated phrase and removing all nodes whose
span is a substring of phrases.
</table>
<tableCaption confidence="0.961065">
Table 1: An algorithm to transform a CFG tree to a phrase CFG tree.
</tableCaption>
<listItem confidence="0.985200428571429">
• If a node is a pre-terminal node (containing POS
tag), its head word is itself. If a node is an in-
ner node (containing syntactic constituent tag),
its head word is retrieved through the head child.
• Word span of a node is a string of its leaves. For
instance, word span of subtree (NP (PRP$ your)
(NN class)) is ”your class”.
</listItem>
<bodyText confidence="0.999797111111111">
Now we consider an example depicted in Figure 3
and 4. Head children are tagged with functional label
H. There are two phrases: ”is a” and ”in your class”.
After the Step 1, the phrase ”is a” is attached to (VBZ
is). The phrase ”in your class” is attached to (IN in).
In Step 2, the node (V is) is replaced by (V ”is a”) and
(DT a) is removed from its father NP. Similarly, (IN
in) is replaced by (IN ”in your class”) and the subtree
NP on the right is removed.
</bodyText>
<figure confidence="0.443123">
your class
</figure>
<figureCaption confidence="0.814490333333333">
Figure 3: Tree transformation - step 1. Solid arrows
show the allocation process of ”is a”. Dotted arrows
demonstrate the allocation process of ”in your class”
</figureCaption>
<bodyText confidence="0.9995835">
The proposed algorithm has some properties. We
state these properties without presenting proof2.
</bodyText>
<listItem confidence="0.999749">
• Uniqueness: Given a CFG tree and a phrase seg-
mentation, by applying Algorithm 1, one and
only one phrase tree is generated.
</listItem>
<footnote confidence="0.580456">
2Proofs are simple.
</footnote>
<figureCaption confidence="0.89255">
Figure 4: Tree transformation - step 2.
</figureCaption>
<listItem confidence="0.940804615384615">
• Constituent subgraph: A phrase CFG tree is
a connected subgraph of input tree if leaves are
ignored.
• Flatness: A phrase CFG tree is flatter than input
tree.
• Outside head: The head of a phrase is always a
word whose head outside the phrase. If there is
more than one word satisfying this condition, the
word at the highest level is chosen.
• Dependency subgraph: Dependency graph of a
phrase CFG tree is a connected subgraph of in-
put tree’s dependency graph if there exist no de-
tached nodes.
</listItem>
<bodyText confidence="0.9996295">
The meaning of uniqueness property is that our al-
gorithm is a deterministic procedure. The constituent-
subgraph property will be employed in the next sec-
tion for an efficient decoding algorithm. When a syn-
tactic tree is transformed, a number of subtrees are
replaced by phrases. The head word of a phrase is the
contact point of that phrase with the remaining part
of a sentence. From the dependency point of view, a
head word should depend on an outer word rather than
an inner word. About dependency-subgraph property,
when there is a detached node, an indirect dependency
will become a direct one. In any cases, there is no
</bodyText>
<figure confidence="0.99552747826087">
{in your class}
S
[is]
VP-H
[is]
NP
VBZ-H
[student]
NP-H
is [student]
{is a}
DT NN-H
a student in
NP
[Fred]
PP
[in]
PRP$ NN-H
NP
IN-H
[class]
NNP-H
Fred
</figure>
<page confidence="0.992154">
145
</page>
<bodyText confidence="0.9983034">
change in dependency direction. We can observe de-
pendency trees in Figure 5. The first two trees are
source dependency tree and phrase dependency tree
of the previous example. The last one corresponds to
the case in which a detached node exists.
</bodyText>
<figure confidence="0.955045833333333">
ROOT
Fred is a student in your class
ROOT
Fred is a student inyour class
ROOT
Fred is a student inyour class
</figure>
<figureCaption confidence="0.949112">
Figure 5: Dependency trees. The third tree corre-
sponds with phrase segmentation: ”Fred  |is a student
 |in your class”
</figureCaption>
<subsectionHeader confidence="0.9748645">
3.3 Probabilistic Word-to-Phrase Tree
Transformation
</subsectionHeader>
<bodyText confidence="0.999990133333333">
We have proposed an algorithm to create a phrase
CFG tree from a pair of CFG tree and phrase seg-
mentation. Two questions naturally arise: ”is there
a way to evaluate how good a phrase tree is?” and ”is
such an evaluation valuable?” Note that phrase trees
are the means to reorder the source sentence repre-
sented as phrase segmentations. Therefore a phrase
tree is surely not good if no right order can be gen-
erated. Now the answer to the second question is
clear. We need an evaluation method to prevent our
program from generating bad phrase trees. In other
words, good phrase trees should be given a higher pri-
ority.
We define the phrase tree probability as the product
of its rule probability given the original CFG rules:
</bodyText>
<equation confidence="0.985127">
P (T &apos;) = 11 P(LHSi -4 RHS&apos;i|LHSi -4 RHSi)
i
</equation>
<bodyText confidence="0.955691857142857">
(3)
where T&apos; is a phrase tree whose CFG rules are
LHSi -4 RHS&apos;i. LHSi -4 RHSi are origi-
nal CFG rules. RHS&apos; i are subsequences of RHSi.
Since phrase tree rules should capture changes made
by the transformation from word to phrase, we use
’+’ to represent an expansion and ’-’ to show an
overlap. These symbol will be added to a nonter-
minal on the side having a change. In the previ-
ous example, since a head noun in the word tree
has been expanded on the right, the correspond-
ing symbol in phrase tree is NN-H+. A nonter-
minal X can become one of the following symbols
X, −X, +X, X−, X+, −X−, −X+, +X−, +X+.
Conditional probabilities are computed in a sepa-
rate training phase using a source-parsed and word-
aligned bitext. First, all phrase pairs consistent with
the word alignment are collected. Then using this
phrase segmentation and syntactic trees we can gener-
ate phrase trees by word-to-phrase tree transformation
and extract rules.
</bodyText>
<sectionHeader confidence="0.996438" genericHeader="method">
4 Phrase Reordering Model
</sectionHeader>
<bodyText confidence="0.999939076923077">
Reordering rules are represented as SCFG rules
which can be un-lexicalized or source-side lexicalized
(Nguyen and Shimazu, 2006). In this paper, we used
un-lexicalized rules. We used a learning algorithm
as in (Nguyen and Shimazu, 2006) to learn weighted
SCFGs. The training requirements include a bilingual
corpus, a word alignment tool, and a broad coverage
parser of the source language. The parser is a con-
stituency analyzer which can produce parse tree in
Penn Tree-bank’s style. The model is applicable to
language pairs in which the target language is poor
in resources. We used phrase reorder rules whose ’+’
and ’-’ symbols are removed.
</bodyText>
<sectionHeader confidence="0.996161" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.999980181818182">
A source sentence can have many possible phrase seg-
mentations. Each segmentation in combination with a
source tree corresponds to a phrase tree. A phrase-tree
forest is a set of those trees. A naive decoding algo-
rithm is that for each segmentation, a phrase tree is
generated and then the sentence is translated. This al-
gorithm is very slow or even intractable. Based on
the constituent-subgraph property of the tree trans-
formation algorithm, the forest of phrase trees will
be packed into a tree-structure container whose back-
bone is the original CFG tree.
</bodyText>
<subsectionHeader confidence="0.983024">
5.1 Translation Options
</subsectionHeader>
<bodyText confidence="0.997144571428571">
A translation option encodes a possibility to translate
a source phrase (at a leaf node of a phrase tree) to
another phrase in target language. Since our decoder
uses a log-linear translation model, it can exploit var-
ious features of translation options. We use the same
features as (Koehn et al., 2003). Basic information of
a translation option includes:
</bodyText>
<listItem confidence="0.999918333333333">
• source phrase
• target phrase
• phrase translation score (2)
</listItem>
<page confidence="0.900608">
146
</page>
<listItem confidence="0.9996505">
• lexical translation score (2)
• word penalty
</listItem>
<bodyText confidence="0.999651">
Translation options of an input sentence are col-
lected before any decoding takes place. This allows a
faster lookup than consulting the whole phrase trans-
lation table during decoding. Note that the entire
phrase translation table may be too big to fit into
memory.
</bodyText>
<subsectionHeader confidence="0.997031">
5.2 Translation Hypotheses
</subsectionHeader>
<bodyText confidence="0.999732142857143">
A translation hypothesis represents a partial or full
translation of an input sentence. Initial hypotheses
correspond to translation options. Each translation
hypothesis is associated with a phrase-tree node. In
other words, a phrase-tree node has a collection of
translation hypotheses. Now we consider basic infor-
mation contained in a translation hypothesis:
</bodyText>
<listItem confidence="0.9990295">
• the cost so far
• list of child hypotheses
• left language model state and right language
model state
</listItem>
<subsectionHeader confidence="0.997596">
5.3 Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.999033181818182">
First we consider structure of a syntactic tree. A tree
node contains fields such as syntactic category, child
list, and head child index. A leaf node has an ad-
ditional field of word string. In order to extend this
structure to store translation hypotheses, a new field
of hypothesis collection is appended. A hypothe-
sis collection contains translation hypotheses whose
word spans are the same. Actually, it corresponds to
a phrase-tree node. A hypothesis collection whose
word span is [i1, i2] at a node whose tag is X ex-
presses that:
</bodyText>
<listItem confidence="0.966472">
• There is a phrase-tree node (X, i1, i2).
• There exist a phrase [i1, i2] or
• There exist a subsequence of X’s child list:
(Y1,j0,j1), (Y2,j1+1,j2), ..., (Y.,j.−1+1,j.)
where j0 = i1 and j,,, = i2
• Suppose that [i, j] is X’s span, then [i1, i2] is a
valid phrase node’s span if and only if: i1 &lt;= i
or i &lt; i1 &lt;= j and there exist a phrase [i0, i1 −
1] overlapping X’s span at [i, i1 − 1]. A similar
condition is required of j.
</listItem>
<bodyText confidence="0.924220666666667">
Table 2 shows our decoding algorithm. Step 1 dis-
tributes translation options to leaf nodes using a pro-
cedure similar to Step 1 of algorithm in Table 1. Step
</bodyText>
<table confidence="0.995135">
Corpus Size Training Development Testing
Conversation 16,809 15,734 403 672
Reuters 57,778 55,757 1,000 1,021
</table>
<tableCaption confidence="0.989555">
Table 3: Corpora and data sets.
</tableCaption>
<table confidence="0.9997888">
English Vietnamese
Sentences 16,809
Average sent. len. 8.5 8.0
Words 143,373 130,043
Vocabulary 9,314 9,557
English Japanese
Sentences 57,778
Average sent. len. 26.7 33.5
Words 1,548,572 1,927,952
Vocabulary 31,702 29,406
</table>
<tableCaption confidence="0.999886">
Table 4: Corpus statistics of translation tasks.
</tableCaption>
<bodyText confidence="0.980748666666666">
2 helps check valid subsequences in Step 3 fast. Step
3 is a bottom-up procedure, a node is translated if all
of its child nodes have been translated. Step 3.1 calls
syntactic transformation models. After reordered in
Step 3.2, a subsequence will be translated in Step 3.3
using a simple monotonic decoding procedure result-
ing in new translation hypotheses. We used a beam
pruning technique to reduce the memory cost and to
accelerate the computation.
</bodyText>
<sectionHeader confidence="0.993098" genericHeader="method">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.982325">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999985909090909">
We used Reuters3, an English-Japanese bilingual cor-
pus, and Conversation, an English-Vietnamese corpus
(Table 4). These corpora were split into data sets as
shown in Table 3. Japanese sentences were analyzed
by ChaSen4, a word-segmentation tool.
A number of tools were used in our experiments.
Vietnamese sentences were segmented using a word-
segmentation program (Nguyen et al., 2003). For
learning phrase translations and decoding, we used
Pharaoh (Koehn, 2004), a state-of-the-art phrase-
based SMT system which is available for research
purpose. For word alignment, we used the GIZA++
tool (Och and Ney, 2000). For learning language
models, we used SRILM toolkit (Stolcke, 2002). For
MT evaluation, we used BLEU measure (Papineni et
al., 2001) calculated by the NIST script version 11b.
For the parsing task, we used Charniak’s parser (Char-
niak, 2000). For experiments with chunking (or shal-
low parsing), we used a CRFs-based chunking tool 5
to split a source sentence into syntactic chunks. Then
a pseudo CFG rule over chunks is built to generate a
two-level syntactic tree. This tree can be used in the
</bodyText>
<footnote confidence="0.999960333333333">
3http://www2.nict.go.jp/x/x161/members/mutiyama/index.html
4http://chasen.aist-nara.ac.jp/chasen/distribution.html.en
5http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.991844">
147
</page>
<table confidence="0.9924979">
+ Input: A source CFG tree, a translation-option collection
+ Output: The best target sentence
+ Step 1: Allocate translation options to hypothesis collections at leaf nodes.
+ Step 2: Compute overlap vector for all nodes.
+ Step 3: For each node, if all of its children have been translated, then for each valid
sub-sequence of child list, carry out the following steps:
+ Step 3.1: Retrieve transformation rules
+ Step 3.2: Reorder the sub-sequence
+ Step 3.3: Translate the reordered sub-sequence and update corresponding
hypothesis collections
</table>
<tableCaption confidence="0.83366">
Table 2: A bottom-up dynamic-programming decoding algorithm.
</tableCaption>
<table confidence="0.999817">
Corpus CFG PhraseCFG W2PTT Reorder
Conversation 2,784 2,684 8,862 2,999
Reuters 7,668 5,479 13,458 7,855
</table>
<tableCaption confidence="0.943168">
Table 5: Rule induction statistics.
</tableCaption>
<table confidence="0.99951175">
Corpus Pharaoh PB system SD system
(chunking)
Conversation 35.47 35.66 36.85
Reuters 24.41 24.20 20.60
</table>
<tableCaption confidence="0.876936">
Table 6: BLEU score comparison between phrase-
based SMT and syntax-directed SMT. PB=phrase-
based; SD=syntax-directed
</tableCaption>
<bodyText confidence="0.9950075">
same way as trees produced by Charniak’s parser.
We built a SMT system for phrase-based log-linear
translation models. This system has two decoders:
beam search and syntax-based. We implemented the
algorithm in Section 5 for the syntax-based decoder.
We also implemented a rule induction module and a
module for minimum error rate training. We used the
system for our experiments reported later.
</bodyText>
<subsectionHeader confidence="0.999091">
6.2 Rule Induction
</subsectionHeader>
<bodyText confidence="0.999978875">
In Table 5, we report statistics of CFG rules,
phrase CFG rules, word-to-phrase tree transformation
(W2PTT) rules, and reordering rules. All counted
rules were in un-lexicalized form. Those numbers are
very small in comparison with the number of phrasal
translations (up to hundreds of thousands on our cor-
pora). There were a number of ”un-seen” CFG rules
which did not have a corresponding reordering rule.
A reason is that those rules appeared once or several
times in the training corpus; however, their hierarchi-
cal alignments did not satisfy the conditions for in-
ducing a reordering rule since word alignment is not
perfect (Nguyen and Shimazu, 2006). Another reason
is that there were CFG rules which required nonlocal
reordering. This may be an issue for future research:
a Markovization technique for SCFGs.
</bodyText>
<subsectionHeader confidence="0.99595">
6.3 BLEU Scores
</subsectionHeader>
<bodyText confidence="0.991507954545454">
Table 6 shows a comparison of BLEU scores be-
tween Pharaoh, our phrase-based SMT system, and
our syntax-directed (SD) SMT system with chunking
and full parsing respectively. On both Conversation
corpus and Reuters corpus: The BLEU score of our
phrase-based SMT system is comparable to that of
Pharaoh; The BLEU score of our SD system with full
SD
tem.On Conversation corpus, our
system with
chunking has a higher performance in terms of BLEU
score than our phrase-based system. Using sign test
(Lehmann, 1986), we verified the improvements are
statistically significant. However, on Reuters corpus,
performance of the SD system with chunking is much
lower than the phrase-based system’s. The reason is
that in English-Japanese translation, chunk is a too
shallow syntactic structure to capture word order in-
formation. For example, a prepositional chunk of-
ten includes only preposition and adverb, therefore
such information does not help reordering preposi-
tional phrases.
</bodyText>
<subsectionHeader confidence="0.996286">
6.4 The Effectiveness of the W2PTT Model
</subsectionHeader>
<bodyText confidence="0.997711333333333">
Without this feature, BLEU scores decreased around
0.5 on both corpora. We now consider a linguistically
motivated example of English-Vietnamese translation
to show that phrase segmentation can be evaluated
through phrase tree scoring. This example was ex-
tracted from Conversation test set.
</bodyText>
<construct confidence="0.9059147">
English sentence: for my wife ’s mother
Vietnamese word order: for mother ’s wife my
Phrase segmentation 1: for my wife  |’s  |mother
P1=P(PP→IN+ -NP  |PP→IN NP)xP(-NP→-NP NN  |NP→NP
NN)xP(-NP→POS  |NP→PRP$ NN
POS)=log(0.00001)+log(0.14)+log(0.048)=-5-0.85-1.32=-7.17
Phrase segmentation 2: for  |my wife ’s  |mother
P2=P(PP→IN NP  |PP→IN NP)xP(NP→NP NN  |NP→NP
NN) xP(NP→POS  |NP→PRP$ NN POS)
=log(0.32)+log(0.57)+log(0.048)=-0.5-0.24-1.32=-2.06
</construct>
<bodyText confidence="0.999679125">
The first phrase segmentation is bad (or even un-
acceptable) since the right word order can not be
achieved from this segmentation by phrase reorder-
ing and word reordering within phrases. The second
phrase segmentation is much better. Source syntax
tree and phrase trees are shown in Figure 6. The first
phrase tree has a much smaller probability (P1=-7.17)
than the second (P2=-2.06).
</bodyText>
<figure confidence="0.75556875">
37.42
25.53
SD systemparsing is higher than that of our phrase-based sys-
(full-parsing)
</figure>
<page confidence="0.727561">
148
</page>
<figureCaption confidence="0.993949">
Figure 6: Two phrase trees.
</figureCaption>
<table confidence="0.997599">
Corpus Level-1 Level-2 Level-3 Level-4
Conversation 36.85 36.91 37.11 37.23
Reuters 20.60 22.76 24.49 25.12
</table>
<tableCaption confidence="0.780631">
Table 7: BLEU score with different syntactic levels.
Level-i means syntactic transformation was applied to
</tableCaption>
<bodyText confidence="0.9856685">
tree nodes whose level smaller than or equal to i. The
level of a pre-terminal node (POS tag) is 0. The level
of an inner node is the maximum of its children’s lev-
els.
</bodyText>
<subsectionHeader confidence="0.998707">
6.5 Levels of Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.999994571428572">
Since in practice, chunking and full parsing are often
used, in Table 6, we showed translation quality of the
two cases. It is interesting if we can find how syn-
tactic analysis can affect BLEU score at more inter-
mediate levels (Table 7). On the Conversation corpus,
using syntax trees of level-1 is effective in comparison
with baseline. The increase of syntactic level makes a
steady improvement in translation quality. Note that
when we carried out experiments with chunking (con-
sidered as level-1 syntax) the translation speed (in-
cluding chunking) of our tree-to-string system was
much faster than baseline systems’. This is an option
for developing applications which require high speed
such as web translation.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
7 Related Works
</sectionHeader>
<subsectionHeader confidence="0.997263">
7.1 A Comparison of Syntactic SMT Methods
</subsectionHeader>
<bodyText confidence="0.99996605">
To advance the state of the art, SMT system design-
ers have experimented with tree-structured transla-
tion models. The underlying computational models
were synchronous context-free grammars and finite-
state tree transducers which conceptually have a bet-
ter expressive power than finite-state transducers. We
create Tables 8 and 9 in order to compare syntac-
tic SMT methods including ours. The first row is a
baseline phrasal SMT approach. The second column
in Table 8 only describes input types because the out-
put is often string. Syntactic SMT methods are dif-
ferent in many aspects. Methods which make use of
phrases (in either explicit or implicit way) can beat
the baseline approach (Table 8) in terms of BLEU
metric. Two main problems these models aim to deal
with are word order and word choice. In order to ac-
complish this purpose, the underlying formal gram-
mars (including synchronous context-free grammars
and tree transducers) can be fully lexicalized or un-
lexicalized (Table 9).
</bodyText>
<subsectionHeader confidence="0.979119">
7.2 Non-constituent Phrasal Translations
</subsectionHeader>
<bodyText confidence="0.999989789473684">
Liu et al. (2007) proposed forest-to-string rules to
capture non-constituent phrasal translation while our
system can naturally make use of such kind of phrasal
translation by using word-to-phrase tree transforma-
tion. Liu et al. (2007) also discussed about how
the phenomenon of non-syntactic bilingual phrases
is dealt with in other SMT methods. Galley et al.
(2006) handled non-constituent phrasal translation by
traversing the tree upwards until reaches a node that
subsumes the phrase. Marcu et al. (2006) reported
that approximately 28% of bilingual phrases are non-
syntactic on their English-Chinese corpus. They pro-
posed using a pseudo nonterminal symbol that sub-
sumes the phrase and corresponding multi-headed
syntactic structure. One new xRs rule is required to
explain how the new nonterminal symbol can be com-
bined with others. This technique brought a signif-
icant improvement in performance to their string-to-
tree noisy channel SMT system.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999979">
We have presented a general tree-to-string phrase-
based method. This method employs a syntax-based
reordering model in the decoding phase. By word-
to-phrase tree transformation, all possible phrases
are considered in translation. Our method does
not suppose a uniform distribution over all possible
phrase segmentations as (Koehn et al., 2003) since
each phrase tree has a probability. We believe that
other kinds of translation unit such as n-gram (Jos
et al., 2006), factored phrasal translation (Koehn and
Hoang, 2007), or treelet (Quirk et al., 2005) can be
used in this method. We would like to consider this
problem as a future study. Moreover we would like to
use n-best trees as the input of our system. A number
</bodyText>
<figure confidence="0.815986">
Full
37.42
25.53
</figure>
<page confidence="0.983786">
149
</page>
<bodyText confidence="0.5550948">
Method Input Theoretical Decoding style Linguistic Phrase Performance
model information usage
Koehn et al. (2003) string FSTs beam search no yes baseline
Yamada and Knight (2001) string SCFGs parsing target no not better
Melamed (2003) string SCFGs parsing both sides no not better
Chiang (2005) string SCFGs parsing no yes better
Quirk et al. (2005) dep. tree TTs parsing source yes better
Galley et al. (2006) string TTs parsing target yes better
Liu et al. (2006) tree TTs tree transf. source yes better
Our work tree SCFGs tree transf. source yes better
</bodyText>
<tableCaption confidence="0.6566965">
Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous
Context-Free Grammar; TT=Tree Transducer.
</tableCaption>
<table confidence="0.248141">
Method Rule form Rule function Rule lexicalization level
Koehn et al. (2003) no no no
Yamada and Knight (2001) SCFG rule reorder and function-word ins./del. unlexicalized
Melamed (2003) SCFG rule reorder and word choice full
Chiang (2005) SCFG rule reorder and word choice full
Quirk et al. (2005) Treelet pair word choice full
Galley et al. (2006) xRs rule reorder and word choice full
Liu et al. (2006) xRs rule reorder and word choice full
Our work SCFG rule reorder unlexicalized
</table>
<tableCaption confidence="0.71576">
Table 9: A comparison of syntactic SMT methods (part 2). xRs is a kind of rule which maps a syntactic pattern
</tableCaption>
<bodyText confidence="0.96144725">
to a string, for example VP(AUX(does), RB(not),xo:VB) —4 ne, xo, pas. In the column Rule lexicalization
level: full=lexicalization using vocabularies of both source language and target language.
of non-local reordering phenomena such as adjunct
attachment should be handled in the future.
</bodyText>
<sectionHeader confidence="0.997716" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953618181818">
Charniak, E. 2000. A maximum entropy inspired parser.
In Proceedings ofHLT-NAACL.
Galley, M., Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, Ignacio Thayer 2006. Scal-
able Inference and Training of Context-Rich Syntactic
Translation Models. In Proceedings ofACL.
Jos B. Mario, Rafael E. Banchs, Josep M. Crego, Adri de
Gispert, Patrik Lambert, Jos A. R. Fonollosa, Marta R.
Costa-juss. 2006. N-gram-based Machine Translation.
Computational Linguistics, 32(4): 527–549.
Koehn, P. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings ofAMTA.
Koehn, P. and Hieu Hoang. 2007. Factored Translation
Models. In Proceedings ofEMNLP.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
Lehmann, E. L. 1986. Testing Statistical Hypotheses (Sec-
ond Edition). Springer-Verlag.
Liu, Y., Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings ofACL.
Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007.
Forest-to-String Statistical Translation Rules. In Pro-
ceedings ofACL.
Marcu, D., Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceed-
ings ofEMNLP.
Melamed, I. D. 2004. Statistical machine translation by
parsing. In Proceedings ofACL.
Nguyen, Thai Phuong and Akira Shimazu. 2006. Improv-
ing Phrase-Based Statistical Machine Translation with
Morphosyntactic Transformation. Machine Translation,
20(3): 147–166.
Nguyen, Thai Phuong, Nguyen Van Vinh and Le Anh
Cuong. 2003. Vietnamese Word Segmentation Using
Hidden Markov Model. In Proceedings ofInternational
Workshop for Computer, Information, and Communica-
tion Technologies in Korea and Vietnam.
Och, F. J. and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings ofACL.
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Report.
Quirk, C., A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT.
In Proceedings ofACL.
Stolcke, A. 2002. SRILM - An Extensible Language Mod-
eling Toolkit. In Proc. Intl. Conf. Spoken Language
Processing.
Yamada, K. and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proceedings ofACL.
</reference>
<page confidence="0.99832">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431601">
<title confidence="0.999958">A Tree-to-String Phrase-based Model for Statistical Machine Translation</title>
<author confidence="0.99959">Thai Phuong</author>
<affiliation confidence="0.766609333333333">College of Vietnam National University, Tu-Bao Minh Le and Vinh Van of Information of Knowledge Japan Advanced Institute of Science and</affiliation>
<abstract confidence="0.997497142857143">Though phrase-based SMT has achieved high translation quality, it still lacks of generalization ability to capture word order differences between languages. In this paper we describe a general method for tree-to-string phrasebased SMT. We study how syntactic transformation is incorporated into phrase-based SMT and its effectiveness. We design syntactic transformation models using unlexicalized form of synchronous context-free grammars. These models can be learned from sourceparsed bitext. Our system can naturally make use of both constituent and non-constituent phrasal translations in the decoding phase. We considered various levels of syntactic analysis ranging from chunking to full parsing. Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>143--150</pages>
<location>Manchester,</location>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143–150 Manchester, August 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum entropy inspired parser. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<location>Wei Wang, Ignacio Thayer</location>
<contexts>
<context position="1762" citStr="Galley et al., 2006" startWordPosition="242" endWordPosition="245">nd English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems. 1 Introduction Based on the kind of linguistic information which is made use of, syntactic SMT can be divided into four types: tree-to-string, string-to-tree, tree-to-tree, and hierarchical phrase-based. The tree-to-string approach (Collins et al., 2005; Nguyen and Shimazu, 2006; Liu et al., 2006 and 2007) supposes that syntax of the source language is known. This approach can be applied when a source language parser is available. The string-to-tree approach (Yamada and Knight, 2001; Galley et al., 2006) focuses on syntactic modelling of the target language in cases it has syntactic resources such as treebanks and parsers. The tree-to-tree approach models the syntax of both languages, therefore extra cost is required. The fourth approach (Chiang, 2005) constraints phrases under context-free grammar structure without any requirement of linguistic annotation. In this paper, we present a tree-to-string phrasebased method which is based on synchronous CFGs. This method has two important properties: syntactic transformation is used in the decoding phase including a word-to-phrase tree transformati</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, 2006</marker>
<rawString>Galley, M., Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, Ignacio Thayer 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos B Mario</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos A R Fonollosa</author>
<author>Marta R Costa-juss</author>
</authors>
<title>N-gram-based Machine Translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>527--549</pages>
<marker>Mario, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss, 2006</marker>
<rawString>Jos B. Mario, Rafael E. Banchs, Josep M. Crego, Adri de Gispert, Patrik Lambert, Jos A. R. Fonollosa, Marta R. Costa-juss. 2006. N-gram-based Machine Translation. Computational Linguistics, 32(4): 527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<marker>Koehn, 2004</marker>
<rawString>Koehn, P. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, P. and Hieu Hoang. 2007. Factored Translation Models. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Lehmann</author>
</authors>
<title>Testing Statistical Hypotheses (Second Edition).</title>
<date>1986</date>
<publisher>Springer-Verlag.</publisher>
<marker>Lehmann, 1986</marker>
<rawString>Lehmann, E. L. 1986. Testing Statistical Hypotheses (Second Edition). Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-String Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1550" citStr="Liu et al., 2006" startWordPosition="207" endWordPosition="210">tituent and non-constituent phrasal translations in the decoding phase. We considered various levels of syntactic analysis ranging from chunking to full parsing. Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems. 1 Introduction Based on the kind of linguistic information which is made use of, syntactic SMT can be divided into four types: tree-to-string, string-to-tree, tree-to-tree, and hierarchical phrase-based. The tree-to-string approach (Collins et al., 2005; Nguyen and Shimazu, 2006; Liu et al., 2006 and 2007) supposes that syntax of the source language is known. This approach can be applied when a source language parser is available. The string-to-tree approach (Yamada and Knight, 2001; Galley et al., 2006) focuses on syntactic modelling of the target language in cases it has syntactic resources such as treebanks and parsers. The tree-to-tree approach models the syntax of both languages, therefore extra cost is required. The fourth approach (Chiang, 2005) constraints phrases under context-free grammar structure without any requirement of linguistic annotation. In this paper, we present a</context>
<context position="2993" citStr="Liu et al. (2006)" startWordPosition="429" endWordPosition="432"> phrase reordering model; phrases are the basic unit of translation. Since we design syntactic transformation models using un-lexicalized synchronous CFGs, the number of rules is small1. Previous studies on tree-to-string SMT are different from ours. Collins et al. Collins et al. (2005) used hand crafted rules to carry out word reordering in the preprocessing phase but not decoding phase. Nguyen and Shimazu (2006) presented a more general method in which lexicalized syntactic reordering models based on PCFGs can be learned from source-parsed bitext and then applied in the preprocessing phase. Liu et al. (2006) changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not. TATs was represented as xRs rules while we use synchronous CFG rules. In order to overcome the limitation that TATs can not capture non-constituent phrasal translations, Liu et al. (2007) proposed forest-to-string rules while our system can naturally make use of such kind of phrasal translation by word-to-phrase tree transformation. We carried out experiments with two language pairs English-Japanese and English-Vietnamese. Our system achieved significant improvements over Pharaoh, a state-o</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Liu, Y., Qun Liu, Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3285" citStr="Liu et al. (2007)" startWordPosition="476" endWordPosition="479"> hand crafted rules to carry out word reordering in the preprocessing phase but not decoding phase. Nguyen and Shimazu (2006) presented a more general method in which lexicalized syntactic reordering models based on PCFGs can be learned from source-parsed bitext and then applied in the preprocessing phase. Liu et al. (2006) changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not. TATs was represented as xRs rules while we use synchronous CFG rules. In order to overcome the limitation that TATs can not capture non-constituent phrasal translations, Liu et al. (2007) proposed forest-to-string rules while our system can naturally make use of such kind of phrasal translation by word-to-phrase tree transformation. We carried out experiments with two language pairs English-Japanese and English-Vietnamese. Our system achieved significant improvements over Pharaoh, a state-of-the-art phrase-based SMT system. We also analyzed the dependence of translation quality on the level of syntactic analysis (shallow or deep). Figure 1 shows the architecture of our system. The input of this system is a source-language tree and the output is a target-language string. This s</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007. Forest-to-String Statistical Translation Rules. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases.</title>
<date>2006</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Marcu, D., Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Melamed, 2004</marker>
<rawString>Melamed, I. D. 2004. Statistical machine translation by parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thai Phuong Nguyen</author>
<author>Akira Shimazu</author>
</authors>
<title>Improving Phrase-Based Statistical Machine Translation with Morphosyntactic Transformation.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<pages>147--166</pages>
<contexts>
<context position="1532" citStr="Nguyen and Shimazu, 2006" startWordPosition="203" endWordPosition="206">ally make use of both constituent and non-constituent phrasal translations in the decoding phase. We considered various levels of syntactic analysis ranging from chunking to full parsing. Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems. 1 Introduction Based on the kind of linguistic information which is made use of, syntactic SMT can be divided into four types: tree-to-string, string-to-tree, tree-to-tree, and hierarchical phrase-based. The tree-to-string approach (Collins et al., 2005; Nguyen and Shimazu, 2006; Liu et al., 2006 and 2007) supposes that syntax of the source language is known. This approach can be applied when a source language parser is available. The string-to-tree approach (Yamada and Knight, 2001; Galley et al., 2006) focuses on syntactic modelling of the target language in cases it has syntactic resources such as treebanks and parsers. The tree-to-tree approach models the syntax of both languages, therefore extra cost is required. The fourth approach (Chiang, 2005) constraints phrases under context-free grammar structure without any requirement of linguistic annotation. In this p</context>
<context position="2793" citStr="Nguyen and Shimazu (2006)" startWordPosition="398" endWordPosition="401">rasebased method which is based on synchronous CFGs. This method has two important properties: syntactic transformation is used in the decoding phase including a word-to-phrase tree transformation model and a phrase reordering model; phrases are the basic unit of translation. Since we design syntactic transformation models using un-lexicalized synchronous CFGs, the number of rules is small1. Previous studies on tree-to-string SMT are different from ours. Collins et al. Collins et al. (2005) used hand crafted rules to carry out word reordering in the preprocessing phase but not decoding phase. Nguyen and Shimazu (2006) presented a more general method in which lexicalized syntactic reordering models based on PCFGs can be learned from source-parsed bitext and then applied in the preprocessing phase. Liu et al. (2006) changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not. TATs was represented as xRs rules while we use synchronous CFG rules. In order to overcome the limitation that TATs can not capture non-constituent phrasal translations, Liu et al. (2007) proposed forest-to-string rules while our system can naturally make use of such kind of phrasal translation</context>
</contexts>
<marker>Nguyen, Shimazu, 2006</marker>
<rawString>Nguyen, Thai Phuong and Akira Shimazu. 2006. Improving Phrase-Based Statistical Machine Translation with Morphosyntactic Transformation. Machine Translation, 20(3): 147–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thai Phuong Nguyen</author>
<author>Nguyen Van Vinh</author>
<author>Le Anh Cuong</author>
</authors>
<title>Vietnamese Word Segmentation Using Hidden Markov Model.</title>
<date>2003</date>
<booktitle>In Proceedings ofInternational Workshop for Computer, Information, and Communication Technologies in Korea and Vietnam.</booktitle>
<marker>Nguyen, Van Vinh, Cuong, 2003</marker>
<rawString>Nguyen, Thai Phuong, Nguyen Van Vinh and Le Anh Cuong. 2003. Vietnamese Word Segmentation Using Hidden Markov Model. In Proceedings ofInternational Workshop for Computer, Information, and Communication Technologies in Korea and Vietnam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Och, Ney, 2000</marker>
<rawString>Och, F. J. and H. Ney. 2000. Improved statistical alignment models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022), IBM Research Report.</tech>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Quirk, C., A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. Spoken Language Processing.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proc. Intl. Conf. Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1740" citStr="Yamada and Knight, 2001" startWordPosition="238" endWordPosition="241">lts of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems. 1 Introduction Based on the kind of linguistic information which is made use of, syntactic SMT can be divided into four types: tree-to-string, string-to-tree, tree-to-tree, and hierarchical phrase-based. The tree-to-string approach (Collins et al., 2005; Nguyen and Shimazu, 2006; Liu et al., 2006 and 2007) supposes that syntax of the source language is known. This approach can be applied when a source language parser is available. The string-to-tree approach (Yamada and Knight, 2001; Galley et al., 2006) focuses on syntactic modelling of the target language in cases it has syntactic resources such as treebanks and parsers. The tree-to-tree approach models the syntax of both languages, therefore extra cost is required. The fourth approach (Chiang, 2005) constraints phrases under context-free grammar structure without any requirement of linguistic annotation. In this paper, we present a tree-to-string phrasebased method which is based on synchronous CFGs. This method has two important properties: syntactic transformation is used in the decoding phase including a word-to-ph</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, K. and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>