<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001122">
<title confidence="0.998017">
Kernel Regression Framework for Machine Translation: UCL System
Description for WMT 2008 Shared Translation Task
</title>
<author confidence="0.998996">
Zhuoran Wang
</author>
<affiliation confidence="0.999602">
University College London
Dept. of Computer Science
</affiliation>
<address confidence="0.921286">
Gower Street, London, WC1E 6BT
United Kingdom
</address>
<email confidence="0.997293">
z.wang@cs.ucl.ac.uk
</email>
<author confidence="0.99664">
John Shawe-Taylor
</author>
<affiliation confidence="0.999536">
University College London
Dept. of Computer Science
</affiliation>
<address confidence="0.9217565">
Gower Street, London, WC1E 6BT
United Kingdom
</address>
<email confidence="0.997809">
jst@cs.ucl.ac.uk
</email>
<sectionHeader confidence="0.995623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999320307692308">
The novel kernel regression model for SMT
only demonstrated encouraging results on
small-scale toy data sets in previous works due
to the complexities of kernel methods. It is
the first time results based on the real-world
data from the shared translation task will be
reported at ACL 2008 Workshop on Statisti-
cal Machine Translation. This paper presents
the key modules of our system, including the
kernel ridge regression model, retrieval-based
sparse approximation, the decoding algorithm,
as well as language modeling issues under this
framework.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996805441860465">
This paper follows the work in (Wang et al., 2007;
Wang and Shawe-Taylor, 2008) which applied the
kernel regression method with high-dimensional
outputs proposed originally in (Cortes et al., 2005)
to statistical machine translation (SMT) tasks. In our
approach, the machine translation problem is viewed
as a string-to-string mapping, where both the source
and the target strings are embedded into their re-
spective kernel induced feature spaces. Then ker-
nel ridge regression is employed to learn the map-
ping from the input feature space to the output one.
As a kernel method, this model offers the potential
advantages of capturing very high-dimensional cor-
respondences among the features of the source and
target languages as well as easy integration of ad-
ditional linguistic knowledge via selecting particu-
lar kernels. However, unlike the sequence labeling
tasks such as optical character recognition in (Cortes
et al., 2005), the complexity of the SMT problem it-
self together with the computational complexities of
kernel methods significantly complicate the imple-
mentation of the regression technique in this field.
Our system is actually designed as a hybrid of
the classic phrase-based SMT model (Koehn et al.,
2003) and the kernel regression model as follows:
First, for each source sentence a small relevant set of
sentence pairs are retrieved from the large-scale par-
allel corpus. Then, the regression model is trained
on this small relevant set only as a sparse approx-
imation of the regression hyperplane trained on the
entire training set, as proposed in (Wang and Shawe-
Taylor, 2008). Finally, a beam search algorithm is
utilized to decode the target sentence from the very
noisy output feature vector we predicted, with the
support of a pre-trained phrase table to generate pos-
sible hypotheses (candidate translations). In addi-
tion, a language model trained on a monolingual cor-
pus can be integrated either directly into the regres-
sion model or during the decoding procedure as an
extra scoring function.
Before describing each key component of our sys-
tem in detail, we give a block diagram overview in
Figure 1.
</bodyText>
<sectionHeader confidence="0.965591" genericHeader="introduction">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999840571428572">
Concretely, the machine translation problem in our
method is formulated as follows. If we define a fea-
ture space H,, of our source language X, and define
the mapping 4b: X —* H,,, then a sentence x E X
can be expressed by its feature vector 4b(x) E H,;.
The definition of the feature space Hy of our target
language Y can be made in a similar way, with cor-
</bodyText>
<page confidence="0.995784">
155
</page>
<note confidence="0.476571">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 155–158,
</note>
<page confidence="0.427005">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figureCaption confidence="0.995815666666667">
Figure 1: System overview. The processes in gray blocks
are pre-performed for the whole system, while the white
blocks are online processes for each input sentence. The
two dash-line arrows represent two possible ways of lan-
guage model integration in our system described in Sec-
tion 6.
</figureCaption>
<bodyText confidence="0.999320666666667">
responding mapping Ψ : Y → Hy. Now in the ma-
chine translation task, we are trying to seek a matrix
represented linear operator W, such that:
</bodyText>
<equation confidence="0.998914">
Ψ(y) = WΦ(x) (1)
</equation>
<bodyText confidence="0.992924">
to predict the translation y for an arbitrary source
sentence x.
</bodyText>
<sectionHeader confidence="0.990207" genericHeader="method">
3 Kernel Ridge Regression
</sectionHeader>
<bodyText confidence="0.956100692307693">
Based on a set of training samples, i.e. bilingual
sentence pairs 5 = {(xi, yi) : xi E X, yi E Y, i =
1, ... , m.}, we use ridge regression to learn the W
in Equation (1), as:
min jjWMD − Myjj&apos;F + vjjWjj&apos; F (2)
where MD = [Φ(x1), ..., Φ(xm)], My =
[Ψ(y1), ..., Ψ(ym)], jj · jjF denotes the Frobenius
norm that is a matrix norm defined as the square root
of the sum of the absolute squares of the elements in
that matrix, and v is a regularization coefficient.
Differentiating the expression and setting it to
zero gives the explicit solution of the ridge regres-
sion problem:
</bodyText>
<equation confidence="0.979576333333333">
W = My(KD + vI)−1M⊤ (3)
D
where I is the identity matrix, and KD =
M⊤DMD = (KD(xi, xj)1≤i,j≤m). Note here, we use
the kernel function:
KD(xi,xj) = (Φ(xi),Φ(xj)) = Φ(xi)⊤Φ(xj) (4)
</equation>
<bodyText confidence="0.999936714285714">
to denote the inner product between two feature vec-
tors. If the feature spaces are properly defined, the
‘kernel trick’ will allow us to avoid dealing with
the very high-dimensional feature vectors explicitly
(Shawe-Taylor and Cristianini, 2004).
Inserting Equation (3) into Equation (1), we ob-
tain our prediction as:
</bodyText>
<equation confidence="0.999711">
Ψ(y) = My(KD + vI)−1kD(x) (5)
</equation>
<bodyText confidence="0.999847666666667">
where kD(x) = (KD(x, xi)1≤i≤m) is an m x 1 col-
umn matrix. Note here, we will use the exact matrix
inversion instead of iterative approximations.
</bodyText>
<subsectionHeader confidence="0.998839">
3.1 N-gram String Kernel
</subsectionHeader>
<bodyText confidence="0.999983454545454">
In the practical learning and prediction processes,
only the inner products of feature vectors are re-
quired, which can be computed with the kernel func-
tion implicitly without evaluating the explicit coor-
dinates of points in the feature spaces. Here, we de-
fine our features of a sentence as its word n-gram
counts, so that a blended n-gram string kernel can
be used. That is, if we denote by xi:j a substring
of sentence x starting with the ith word and ending
with the jth, then for two sentences x and z, the
blended n-gram string kernel is computed as:
</bodyText>
<equation confidence="0.835743">
[xi:i+p−1 = zj:j+p−1]
(6)
</equation>
<bodyText confidence="0.948068">
Here,  |·  |denotes the length of the sentence, and
[·] is the indicator function for the predicate. In our
system, the blended tri-gram kernel is used, which
means we count the n-grams of length up to 3.
</bodyText>
<sectionHeader confidence="0.99072" genericHeader="method">
4 Retrieval-based Sparse Approximation
</sectionHeader>
<bodyText confidence="0.998514">
For SMT, we are not able to use the entire training
set that contains millions of sentences to train our
regression model. Fortunately, it is not necessary ei-
ther. Wang and Shawe-Taylor (2008) suggested that
a small set of sentences whose source is relevant to
the input can be retrieved, and the regression model
can be trained on this small-scale relevant set only.
</bodyText>
<figure confidence="0.8738795">
Source Text
Retriever
Parallel
Corpus
Monolingual
Corpus
Relevant Set
Alignment
Kernel
Regression
Language
Modeling
Phrase
Extraction
N-gram
Model
Phrase Table
Target Text
Decoder
|X|−p+1
�
i=1
|Z|−p+1
�
j=1
n
K(x, z) =
p=1
</figure>
<page confidence="0.977981">
156
</page>
<bodyText confidence="0.928763857142857">
Src n’ y a-t-il pas ici deux poids , deux mesures
Rlv pourquoi y a-t-il deux poids , deux mesures
pourquoi deux poids et deux mesures
peut-ˆetre n’ y a-t-il pas d’ ´epid´emie non
plus
pourquoi n’ y a-t-il pas urgence
cette directive doit exister d’ ici deux mois
</bodyText>
<tableCaption confidence="0.9848085">
Table 1: A sample input (Src) and some of the retrieved
relevant examples (Rlv).
</tableCaption>
<bodyText confidence="0.9999485">
In our system, we take each sentence as a docu-
ment and use the tf-idf metric that is frequently used
in information retrieval tasks to retrieve the relevant
set. Preliminary experiments show that the size of
the relevant set should be properly controlled, as if
many sentences that are not very close to the source
text are involved, they will correspond to adding
noise. Hence, we use a threshold of the tf-idf score
to filter the relevant set. On average, around 1500
sentence pairs are extracted for each source sen-
tence. Table 1 shows a sample input and some of
its top relevant sentences retrieved.
</bodyText>
<sectionHeader confidence="0.995383" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.999971571428571">
After the regression, we have a prediction of the
target feature vector as in Equation (1). To ob-
tain the target sentence, a decoding algorithm is still
required to solve the pre-image problem. This is
achieved in our system by seeking the sentence y�
whose feature vector has the minimum Euclidean
distance to the prediction, as:
</bodyText>
<equation confidence="0.979772">
y� = argymin)kW-b(x) − IF(y)k (7)
Y(X
</equation>
<bodyText confidence="0.999745125">
where Y(x) ⊂ Y denotes a finite set covering all
potential translations for the given source sentence
x. To obtain a smaller search space and more re-
liable translations, Y(x) is generated with the sup-
port of a phrase table extracted from the whole train-
ing set. Then a modified beam search algorithm
is employed, in which we restricted the distortion
of the phrases by only allowing adjacent phrases to
exchange their positions, and rank the search states
in the beams according to Equation (7) but applied
directly to the partial translations and their corre-
sponding source parts. A more detailed explanation
of the decoding algorithm can be found in (Wang
et al., 2007). In addition, Wang and Shawe-Taylor
(2008) further showed that the search error rate of
this algorithm is acceptable.
</bodyText>
<sectionHeader confidence="0.987288" genericHeader="method">
6 Language Model Integration
</sectionHeader>
<bodyText confidence="0.999991">
In previous works (Wang et al., 2007; Wang and
Shawe-Taylor, 2008), there was no language model
utilized in the regression framework for SMT, as
similar function can be achieved by the correspon-
dences among the n-gram features. It was demon-
strated to work well on small-scale toy data, how-
ever, real-world data are much more sparse and
noisy, where a language model will help signifi-
cantly.
There are two ways to integrate a language model
in our framework. First, the most straightforward so-
lution is to add a weight to adjust the strength of the
regression based translation scores and the language
model score during the decoding procedure. Alter-
natively, as language model is n-gram-based which
matches the definition of our feature space, we can
add a langauge model loss to the objective function
of our regression model as follows. We define our
language score for a target sentence y as:
</bodyText>
<equation confidence="0.998139">
LM(y) = VTIF(y) (8)
</equation>
<bodyText confidence="0.999775125">
where V is a vector whose components Vy′′y′y will
typically be log-probabilities log P(y|y″y′), and y,
y′ and y″are arbitrary words. Note here, in or-
der to match our blended tri-gram induced feature
space, we can make V of the same dimension as
IF(y), while zero the components corresponding to
uni-grams and bi-grams. Then the regression prob-
lem can be defined as:
</bodyText>
<equation confidence="0.672393">
minkWMΦ−MΨk2F +v1kWk2F −v2VTWMΦ1
(9)
</equation>
<bodyText confidence="0.9994205">
where v2 is a coefficient balancing between the pre-
diction being close to the target feature vector and
being a fluent target sentence, and 1 denotes a vec-
tor with components 1. By differentiating the ex-
pression with respect to W and setting the result to
zero, we can obtain the explicit solution as:
</bodyText>
<equation confidence="0.996832">
W = (MΨ + v2V1T)(KΦ + v1I)−1MTΦ (10)
</equation>
<sectionHeader confidence="0.983066" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.96368">
Preliminary experiments are carried out on the
French-English portion of the Europarl corpus. We
</bodyText>
<page confidence="0.989842">
157
</page>
<table confidence="0.991925333333333">
System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%)
Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20
Moses 31.15 7.48 56.80 55.14 59.85 42.79
</table>
<tableCaption confidence="0.999984">
Table 3: Evaluations based on different metrics with comparison to Moses.
</tableCaption>
<bodyText confidence="0.962963192307692">
train our regression model on the training set, and
test the effects of different language models on the
development set (test2007). The results evaluated
by BLEU score (Papineni et al., 2002) is shown in
Table 2.
It can be found that integrating the language
model into the regression framework works slightly
better than just using it as an additional score com-
ponent during decoding. But language models of
higher-order than the n-gram kernel cannot be for-
mulated to the regression problem, which would be
a drawback of our system. Furthermore, the BLEU
score performance suggests that our model is not
very powerful, but some interesting hints can be
found in Table 3 when we compare our method with
a 5-gram language model to a state-of-the-art system
Moses (Koehn and Hoang, 2007) based on various
evaluation metrics, including BLEU score, NIST
score (Doddington, 2002), METEOR (Banerjee and
Lavie, 2005), TER (Snover et al., 2006), WER and
PER. It is shown that our system’s TER, WER and
PER scores are very close to Moses, though the
gaps in BLEU, NIST and METEOR are significant,
which suggests that we would be able to produce ac-
curate translations but might not be good at making
fluent sentences.
</bodyText>
<sectionHeader confidence="0.997663" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999911571428571">
This work is a novel attempt to apply the advanced
kernel method to SMT tasks. The contribution at this
stage is still preliminary. When applied to real-world
data, this approach is not as powerful as the state-of-
the-art phrase-based log-linear model. However, in-
teresting prospects can be expected from the shared
translation task.
</bodyText>
<sectionHeader confidence="0.996473" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9579485">
This work is supported by the European Commis-
sion under the IST Project SMART (FP6-033917).
</bodyText>
<table confidence="0.957146">
no-LM LM3gram LM�3gram LM�5gram
BLEU 23.27 25.19 25.66 26.59
</table>
<tableCaption confidence="0.984204">
Table 2: BLEU score performance of different language
</tableCaption>
<bodyText confidence="0.8641365">
models. LM1 denotes adding the language model dur-
ing decoding process, while LM2 represents integrating
the language model into the regression framework as de-
scribed in Problem (9).
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878027777777">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2005. A general regression technique for learning
transductions. In Proc. ofICML’05.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ofHLT’02, pages 138–145.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP-CoNLL’07.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
ofHAACL-HLT’03, pages 48–54.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proc. ofACL’02.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. ofAMTA’06.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel-
based machine translation. In Cyril Goutte, Nicola
Cancedda, Marc Dymetman, and George Foster, edi-
tors, Learning Machine Translation. MIT Press, to ap-
pear.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine transla-
tion. In Proc. ofNAACL-HLT’07, Short Paper Volume,
pages 185–188.
</reference>
<page confidence="0.997081">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.042179">
<title confidence="0.997471">Kernel Regression Framework for Machine Translation: UCL Description for WMT 2008 Shared Translation Task</title>
<author confidence="0.988037">Zhuoran</author>
<affiliation confidence="0.999324">University College Dept. of Computer</affiliation>
<address confidence="0.688248">Gower Street, London, WC1E</address>
<note confidence="0.62934">United</note>
<email confidence="0.923363">z.wang@cs.ucl.ac.uk</email>
<author confidence="0.511206">John</author>
<affiliation confidence="0.9993175">University College Dept. of Computer</affiliation>
<address confidence="0.689723">Gower Street, London, WC1E</address>
<note confidence="0.629148">United</note>
<email confidence="0.961709">jst@cs.ucl.ac.uk</email>
<abstract confidence="0.992997214285714">The novel kernel regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods. It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation. This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="11915" citStr="Banerjee and Lavie, 2005" startWordPosition="1990" endWordPosition="1993"> framework works slightly better than just using it as an additional score component during decoding. But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system. Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang, 2007) based on various evaluation metrics, including BLEU score, NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER and PER. It is shown that our system’s TER, WER and PER scores are very close to Moses, though the gaps in BLEU, NIST and METEOR are significant, which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences. 8 Conclusion This work is a novel attempt to apply the advanced kernel method to SMT tasks. The contribution at this stage is still preliminary. When applied to real-world data, this approach is not as powerful as the state-ofthe-art phrase-based log-linear model. However, interesting prospects can b</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
<author>Jason Weston</author>
</authors>
<title>A general regression technique for learning transductions.</title>
<date>2005</date>
<booktitle>In Proc. ofICML’05.</booktitle>
<contexts>
<context position="1149" citStr="Cortes et al., 2005" startWordPosition="162" endWordPosition="165">exities of kernel methods. It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation. This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework. 1 Introduction This paper follows the work in (Wang et al., 2007; Wang and Shawe-Taylor, 2008) which applied the kernel regression method with high-dimensional outputs proposed originally in (Cortes et al., 2005) to statistical machine translation (SMT) tasks. In our approach, the machine translation problem is viewed as a string-to-string mapping, where both the source and the target strings are embedded into their respective kernel induced feature spaces. Then kernel ridge regression is employed to learn the mapping from the input feature space to the output one. As a kernel method, this model offers the potential advantages of capturing very high-dimensional correspondences among the features of the source and target languages as well as easy integration of additional linguistic knowledge via selec</context>
</contexts>
<marker>Cortes, Mohri, Weston, 2005</marker>
<rawString>Corinna Cortes, Mehryar Mohri, and Jason Weston. 2005. A general regression technique for learning transductions. In Proc. ofICML’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ofHLT’02,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="11880" citStr="Doddington, 2002" startWordPosition="1987" endWordPosition="1988">e model into the regression framework works slightly better than just using it as an additional score component during decoding. But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system. Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang, 2007) based on various evaluation metrics, including BLEU score, NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER and PER. It is shown that our system’s TER, WER and PER scores are very close to Moses, though the gaps in BLEU, NIST and METEOR are significant, which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences. 8 Conclusion This work is a novel attempt to apply the advanced kernel method to SMT tasks. The contribution at this stage is still preliminary. When applied to real-world data, this approach is not as powerful as the state-ofthe-art phrase-based log-linear model. H</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ofHLT’02, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL’07.</booktitle>
<contexts>
<context position="11791" citStr="Koehn and Hoang, 2007" startWordPosition="1973" endWordPosition="1976">core (Papineni et al., 2002) is shown in Table 2. It can be found that integrating the language model into the regression framework works slightly better than just using it as an additional score component during decoding. But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system. Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang, 2007) based on various evaluation metrics, including BLEU score, NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER and PER. It is shown that our system’s TER, WER and PER scores are very close to Moses, though the gaps in BLEU, NIST and METEOR are significant, which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences. 8 Conclusion This work is a novel attempt to apply the advanced kernel method to SMT tasks. The contribution at this stage is still preliminary. When applied to real-world data,</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proc. of EMNLP-CoNLL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. ofHAACL-HLT’03,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="2175" citStr="Koehn et al., 2003" startWordPosition="322" endWordPosition="325">tial advantages of capturing very high-dimensional correspondences among the features of the source and target languages as well as easy integration of additional linguistic knowledge via selecting particular kernels. However, unlike the sequence labeling tasks such as optical character recognition in (Cortes et al., 2005), the complexity of the SMT problem itself together with the computational complexities of kernel methods significantly complicate the implementation of the regression technique in this field. Our system is actually designed as a hybrid of the classic phrase-based SMT model (Koehn et al., 2003) and the kernel regression model as follows: First, for each source sentence a small relevant set of sentence pairs are retrieved from the large-scale parallel corpus. Then, the regression model is trained on this small relevant set only as a sparse approximation of the regression hyperplane trained on the entire training set, as proposed in (Wang and ShaweTaylor, 2008). Finally, a beam search algorithm is utilized to decode the target sentence from the very noisy output feature vector we predicted, with the support of a pre-trained phrase table to generate possible hypotheses (candidate trans</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. ofHAACL-HLT’03, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL’02.</booktitle>
<contexts>
<context position="11197" citStr="Papineni et al., 2002" startWordPosition="1873" endWordPosition="1876">ero, we can obtain the explicit solution as: W = (MΨ + v2V1T)(KΦ + v1I)−1MTΦ (10) 7 Experimental Results Preliminary experiments are carried out on the French-English portion of the Europarl corpus. We 157 System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%) Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20 Moses 31.15 7.48 56.80 55.14 59.85 42.79 Table 3: Evaluations based on different metrics with comparison to Moses. train our regression model on the training set, and test the effects of different language models on the development set (test2007). The results evaluated by BLEU score (Papineni et al., 2002) is shown in Table 2. It can be found that integrating the language model into the regression framework works slightly better than just using it as an additional score component during decoding. But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system. Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang, 2007) based</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. ofACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5160" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="843" endWordPosition="846"> of the absolute squares of the elements in that matrix, and v is a regularization coefficient. Differentiating the expression and setting it to zero gives the explicit solution of the ridge regression problem: W = My(KD + vI)−1M⊤ (3) D where I is the identity matrix, and KD = M⊤DMD = (KD(xi, xj)1≤i,j≤m). Note here, we use the kernel function: KD(xi,xj) = (Φ(xi),Φ(xj)) = Φ(xi)⊤Φ(xj) (4) to denote the inner product between two feature vectors. If the feature spaces are properly defined, the ‘kernel trick’ will allow us to avoid dealing with the very high-dimensional feature vectors explicitly (Shawe-Taylor and Cristianini, 2004). Inserting Equation (3) into Equation (1), we obtain our prediction as: Ψ(y) = My(KD + vI)−1kD(x) (5) where kD(x) = (KD(x, xi)1≤i≤m) is an m x 1 column matrix. Note here, we will use the exact matrix inversion instead of iterative approximations. 3.1 N-gram String Kernel In the practical learning and prediction processes, only the inner products of feature vectors are required, which can be computed with the kernel function implicitly without evaluating the explicit coordinates of points in the feature spaces. Here, we define our features of a sentence as its word n-gram counts, so that a ble</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. ofAMTA’06.</booktitle>
<contexts>
<context position="11942" citStr="Snover et al., 2006" startWordPosition="1995" endWordPosition="1998"> than just using it as an additional score component during decoding. But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system. Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang, 2007) based on various evaluation metrics, including BLEU score, NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER and PER. It is shown that our system’s TER, WER and PER scores are very close to Moses, though the gaps in BLEU, NIST and METEOR are significant, which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences. 8 Conclusion This work is a novel attempt to apply the advanced kernel method to SMT tasks. The contribution at this stage is still preliminary. When applied to real-world data, this approach is not as powerful as the state-ofthe-art phrase-based log-linear model. However, interesting prospects can be expected from the shared </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. ofAMTA’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Kernelbased machine translation.</title>
<date>2008</date>
<booktitle>Learning Machine Translation.</booktitle>
<editor>In Cyril Goutte, Nicola Cancedda, Marc Dymetman, and George Foster, editors,</editor>
<publisher>MIT Press,</publisher>
<note>to appear.</note>
<contexts>
<context position="1031" citStr="Wang and Shawe-Taylor, 2008" startWordPosition="146" endWordPosition="149">regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods. It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation. This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework. 1 Introduction This paper follows the work in (Wang et al., 2007; Wang and Shawe-Taylor, 2008) which applied the kernel regression method with high-dimensional outputs proposed originally in (Cortes et al., 2005) to statistical machine translation (SMT) tasks. In our approach, the machine translation problem is viewed as a string-to-string mapping, where both the source and the target strings are embedded into their respective kernel induced feature spaces. Then kernel ridge regression is employed to learn the mapping from the input feature space to the output one. As a kernel method, this model offers the potential advantages of capturing very high-dimensional correspondences among th</context>
<context position="6448" citStr="Wang and Shawe-Taylor (2008)" startWordPosition="1069" endWordPosition="1072"> by xi:j a substring of sentence x starting with the ith word and ending with the jth, then for two sentences x and z, the blended n-gram string kernel is computed as: [xi:i+p−1 = zj:j+p−1] (6) Here, |· |denotes the length of the sentence, and [·] is the indicator function for the predicate. In our system, the blended tri-gram kernel is used, which means we count the n-grams of length up to 3. 4 Retrieval-based Sparse Approximation For SMT, we are not able to use the entire training set that contains millions of sentences to train our regression model. Fortunately, it is not necessary either. Wang and Shawe-Taylor (2008) suggested that a small set of sentences whose source is relevant to the input can be retrieved, and the regression model can be trained on this small-scale relevant set only. Source Text Retriever Parallel Corpus Monolingual Corpus Relevant Set Alignment Kernel Regression Language Modeling Phrase Extraction N-gram Model Phrase Table Target Text Decoder |X|−p+1 � i=1 |Z|−p+1 � j=1 n K(x, z) = p=1 156 Src n’ y a-t-il pas ici deux poids , deux mesures Rlv pourquoi y a-t-il deux poids , deux mesures pourquoi deux poids et deux mesures peut-ˆetre n’ y a-t-il pas d’ ´epid´emie non plus pourquoi n’ </context>
<context position="8894" citStr="Wang and Shawe-Taylor (2008)" startWordPosition="1487" endWordPosition="1490">source sentence x. To obtain a smaller search space and more reliable translations, Y(x) is generated with the support of a phrase table extracted from the whole training set. Then a modified beam search algorithm is employed, in which we restricted the distortion of the phrases by only allowing adjacent phrases to exchange their positions, and rank the search states in the beams according to Equation (7) but applied directly to the partial translations and their corresponding source parts. A more detailed explanation of the decoding algorithm can be found in (Wang et al., 2007). In addition, Wang and Shawe-Taylor (2008) further showed that the search error rate of this algorithm is acceptable. 6 Language Model Integration In previous works (Wang et al., 2007; Wang and Shawe-Taylor, 2008), there was no language model utilized in the regression framework for SMT, as similar function can be achieved by the correspondences among the n-gram features. It was demonstrated to work well on small-scale toy data, however, real-world data are much more sparse and noisy, where a language model will help significantly. There are two ways to integrate a language model in our framework. First, the most straightforward solut</context>
</contexts>
<marker>Wang, Shawe-Taylor, 2008</marker>
<rawString>Zhuoran Wang and John Shawe-Taylor. 2008. Kernelbased machine translation. In Cyril Goutte, Nicola Cancedda, Marc Dymetman, and George Foster, editors, Learning Machine Translation. MIT Press, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
<author>Sandor Szedmak</author>
</authors>
<title>Kernel regression based machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ofNAACL-HLT’07, Short Paper Volume,</booktitle>
<pages>185--188</pages>
<contexts>
<context position="1001" citStr="Wang et al., 2007" startWordPosition="142" endWordPosition="145">t The novel kernel regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods. It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation. This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework. 1 Introduction This paper follows the work in (Wang et al., 2007; Wang and Shawe-Taylor, 2008) which applied the kernel regression method with high-dimensional outputs proposed originally in (Cortes et al., 2005) to statistical machine translation (SMT) tasks. In our approach, the machine translation problem is viewed as a string-to-string mapping, where both the source and the target strings are embedded into their respective kernel induced feature spaces. Then kernel ridge regression is employed to learn the mapping from the input feature space to the output one. As a kernel method, this model offers the potential advantages of capturing very high-dimens</context>
<context position="8851" citStr="Wang et al., 2007" startWordPosition="1481" endWordPosition="1484">ntial translations for the given source sentence x. To obtain a smaller search space and more reliable translations, Y(x) is generated with the support of a phrase table extracted from the whole training set. Then a modified beam search algorithm is employed, in which we restricted the distortion of the phrases by only allowing adjacent phrases to exchange their positions, and rank the search states in the beams according to Equation (7) but applied directly to the partial translations and their corresponding source parts. A more detailed explanation of the decoding algorithm can be found in (Wang et al., 2007). In addition, Wang and Shawe-Taylor (2008) further showed that the search error rate of this algorithm is acceptable. 6 Language Model Integration In previous works (Wang et al., 2007; Wang and Shawe-Taylor, 2008), there was no language model utilized in the regression framework for SMT, as similar function can be achieved by the correspondences among the n-gram features. It was demonstrated to work well on small-scale toy data, however, real-world data are much more sparse and noisy, where a language model will help significantly. There are two ways to integrate a language model in our frame</context>
</contexts>
<marker>Wang, Shawe-Taylor, Szedmak, 2007</marker>
<rawString>Zhuoran Wang, John Shawe-Taylor, and Sandor Szedmak. 2007. Kernel regression based machine translation. In Proc. ofNAACL-HLT’07, Short Paper Volume, pages 185–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>