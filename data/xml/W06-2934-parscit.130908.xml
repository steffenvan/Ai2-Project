<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000196">
<title confidence="0.998169">
Multi-lingual Dependency Parsing with Incremental Integer Linear
Programming
</title>
<author confidence="0.986845">
Sebastian Riedel and Ruket C¸ akıcı and Ivan Meza-Ruiz
</author>
<affiliation confidence="0.987735666666667">
ICCS
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.987684">
Edinburgh, EH8 9LW, UK
</address>
<email confidence="0.99814">
S.R.Riedel,R.Cakici,I.V.Meza-Ruiz@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968166666667">
Our approach to dependency parsing is
based on the linear model of McDonald
et al.(McDonald et al., 2005b). Instead of
solving the linear model using the Max-
imum Spanning Tree algorithm we pro-
pose an incremental Integer Linear Pro-
gramming formulation of the problem that
allows us to enforce linguistic constraints.
Our results show only marginal improve-
ments over the non-constrained parser. In
addition to the fact that many parses did
not violate any constraints in the first place
this can be attributed to three reasons: 1)
the next best solution that fulfils the con-
straints yields equal or less accuracy, 2)
noisy POS tags and 3) occasionally our
inference algorithm was too slow and de-
coding timed out.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971111111111">
This paper presents our submission for the CoNLL
2006 shared task of multilingual dependency pars-
ing. Our parser is inspired by McDonald et
al.(2005a) which treats the task as the search for the
highest scoring Maximum Spanning Tree (MST) in
a graph. This framework is efficient for both pro-
jective and non-projective parsing and provides an
online learning algorithm which combined with a
rich feature set creates state-of-the-art performance
across multiple languages (McDonald and Pereira,
2006).
However, McDonald and Pereira (2006) mention
the restrictive nature of this parsing algorithm. In
their original framework, features are only defined
over single attachment decisions. This leads to cases
where basic linguistic constraints are not satisfied
(e.g. verbs with two subjects). In this paper we
present a novel way to implement the parsing al-
gorithms for projective and non-projective parsing
based on a more generic incremental Integer Linear
Programming (ILP) approach. This allows us to in-
clude additional global constraints that can be used
to impose linguistic information.
The rest of the paper is organised in the following
way. First we give an overview of the Integer Linear
Programming model and how we trained its param-
eters. We then describe our feature and constraint
sets for the 12 different languages of the task (Hajiˇc
et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003;
Kromann, 2003; van der Beek et al., 2002; Brants
et al., 2002; Kawata and Bartels, 2000; Afonso et
al., 2002; Dˇzeroski et al., 2006; Civit Torruella and
MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et
al., 2003; Atalay et al., 2003). Finally, our results are
discussed and error analyses for Chinese and Turk-
ish are presented.
</bodyText>
<sectionHeader confidence="0.988394" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.989159">
Our model is based on the linear model presented in
McDonald et al. (2005a),
</bodyText>
<equation confidence="0.9293135">
�
s (i, j) _ w- f (i, j)
</equation>
<bodyText confidence="0.9922985">
where x is a sentence, y a parse and s a score func-
tion over sentence-parse pairs. f (i, j) is a multidi-
</bodyText>
<equation confidence="0.8106605">
(1) s (x, y) _ �
(i,j)Ey
</equation>
<page confidence="0.964948">
226
</page>
<bodyText confidence="0.886338333333333">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 226–230, New York City, June 2006. c�2006 Association for Computational Linguistics
mensional feature vector representation of the edge
from token i to token j and w the corresponding
weight vector. Decoding in this model amounts to
finding the y for a given x that maximises s (x, y)
</bodyText>
<equation confidence="0.975359">
y� = argmaxys (x, y)
</equation>
<bodyText confidence="0.9077425">
and y contains no cycles, attaches exactly one head
to each non-root token and no head to the root node.
</bodyText>
<subsectionHeader confidence="0.988226">
2.1 Decoding
</subsectionHeader>
<bodyText confidence="0.99998254054054">
Instead of using the MST algorithm (McDonald et
al., 2005b) to maximise equation 1, we present an
equivalent ILP formulation of the problem. An ad-
vantage of a general purpose inference technique is
the addition of further linguistically motivated con-
straints. For instance, we can add constraints that
enforce that a verb can not have more than one sub-
ject argument or that coordination arguments should
have compatible types. Roth and Yih (2005) is
similarly motivated and uses ILP to deal with ad-
ditional hard constraints in a Conditional Random
Field model for Semantic Role Labelling.
There are several explicit formulations of the
MST problem as integer programs in the literature
(Williams, 2002). They are based on the concept of
eliminating subtours (cycles), cuts (disconnections)
or requiring intervertex flows (paths). However, in
practice these cause long solving times. While the
first two types yield an exponential number of con-
straints, the latter one scales cubically but produces
non-fractional solutions in its relaxed version, caus-
ing long runtime of the branch and bound algorithm.
In practice solving models of this form did not con-
verge after hours even for small sentences.
To get around this problem we followed an incre-
mental approach akin to Warme (1998). Instead of
adding constraints that forbid all possible cycles in
advance (this would result in an exponential num-
ber of constraints) we first solve the problem without
any cycle constraints. Only if the result contains cy-
cles we add constraints that forbid these cycles and
run the solver again. This process is repeated un-
til no more violated constraints are found. Figure 1
shows this algorithm.
Groetschel et al. (1981) showed that such an ap-
proach will converge after a polynomial number of
iterations with respect to the number of variables.
</bodyText>
<listItem confidence="0.910612">
1. Solve IP Pi
2. Find violated constraints C in the solution of Pi
3. if C = 0 we are done
4. Pi�1 = Pi UC
5. i = i + 1
6. goto (1)
</listItem>
<figureCaption confidence="0.994861">
Figure 1: Incremental Integer Linear Programming
</figureCaption>
<bodyText confidence="0.999225333333333">
In practice, this technique showed fast convergence
(less than 10 iterations) in most cases, yielding solv-
ing times of less than 0.5 seconds. However, for
some sentences in certain languages, such as Chi-
nese or Swedish, an optimal solution could not be
found after 500 iterations.
In the following section we present the bjective
function, variables and linear constraints that make
up the Integer Linear Program.
</bodyText>
<subsectionHeader confidence="0.559764">
2.1.1 Variables
</subsectionHeader>
<bodyText confidence="0.999813333333333">
In the implementation1 of McDonald et al.
(2005b) dependency labels are handled by finding
the best scoring label for a given token pair so that
</bodyText>
<equation confidence="0.992795">
s (i, j) = max s (i, j, label)
</equation>
<bodyText confidence="0.945029588235294">
goes into Equation 1. This is only exact as long as no
further constraints are added. Since our aim is to add
constraints our variables need to explicitly model la-
bel decisions. Therefore, we introduce binary vari-
ables
li,j,labelbi E 0..n, j E L.n, label E bestb (i, j)
where n is the number of tokens and the index 0
represents the root token. bestb (i, j) is the set of b
labels with maximal s (i, j, label). li,j,label equals 1
if there is a dependency with the label label between
token i (head) and j (child), 0 otherwise.
Furthermore, we introduce binary auxiliary vari-
ables
di,jbi E 0..n, j E L.n
representing the existence of a dependency between
tokens i and j. We connect these to the li,j,label vari-
ables by a constraint
</bodyText>
<equation confidence="0.885182333333333">
�di,j = li,j,label
label
.
</equation>
<footnote confidence="0.885874">
1Note, however, that labelled parsing is not described in the
publication.
</footnote>
<page confidence="0.987467">
227
</page>
<subsubsectionHeader confidence="0.43048">
2.1.2 Objective Function
</subsubsectionHeader>
<bodyText confidence="0.9957315">
Given the above variables our objective function
can be represented as
</bodyText>
<equation confidence="0.980243">
X X s (i, j, label) · li,j,label
i,j labelEbestk(i,j)
</equation>
<bodyText confidence="0.813729">
with a suitable k.
</bodyText>
<subsectionHeader confidence="0.649214">
2.1.3 Constraints Added in Advance
</subsectionHeader>
<bodyText confidence="0.9947255">
Only One Head In all our languages every token
has exactly one head. This yields
</bodyText>
<equation confidence="0.9743094">
X di,j = 1
i&gt;0
for non-root tokens j &gt; 0 and
X di,0 = 0
i
</equation>
<bodyText confidence="0.8710866">
for the artificial root node.
Typed Arity Constraints We might encounter so-
lutions of the basic model that contain, for instance,
verbs with two subjects. To forbid these we simply
augment our model with constraints such as
</bodyText>
<equation confidence="0.9055515">
X li,j,subject ≤ 1
j
</equation>
<bodyText confidence="0.95989">
for all verbs i in a sentence.
</bodyText>
<sectionHeader confidence="0.736438" genericHeader="method">
2.1.4 Incremental Constraints
</sectionHeader>
<bodyText confidence="0.996314666666667">
No Cycles If a solution contains one or more cy-
cles C we add the following constraints to our IP:
For every c ∈ C we add
</bodyText>
<equation confidence="0.9498075">
X di,j ≤ |c |− 1
(i,j)Ec
</equation>
<bodyText confidence="0.954735882352941">
to forbid c.
Coordination Argument Constraints In coordi-
nation conjuncts have to be of compatible types. For
example, nouns can not coordinate with verbs. We
implemented this constraint by checking the parses
for occurrences of incompatible arguments. If we
find two arguments j, k for a conjunction i: di,j and
di,k and j is a noun and k is a verb then we add
di,j + di,k ≤ 1
to forbid configurations in which both dependencies
are active.
Projective Parsing In the incremental ILP frame-
work projective parsing can be easily implemented
by checking for crossing dependencies after each it-
eration and forbidding them in the next. If we see
two dependencies that cross, di,j and dk,l, we add
the constraint
</bodyText>
<equation confidence="0.591857">
di,j + dk,l ≤ 1
</equation>
<bodyText confidence="0.9997622">
to prevent this in the next iteration. This can also
be used to prevent specific types of crossings. For
instance, in Dutch we could only allow crossing de-
pendencies as long as none of the dependencies is a
“Determiner” relation.
</bodyText>
<subsectionHeader confidence="0.990023">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.9996716">
We used single-best MIRA(Crammer and Singer,
2003).For all experiments we used 10 training iter-
ations and non-projective decoding. Note that we
used the original spanning tree algorithm for decod-
ing during training as it was faster.
</bodyText>
<sectionHeader confidence="0.989676" genericHeader="method">
3 System Summary
</sectionHeader>
<bodyText confidence="0.99995176">
We use four different feature sets. The first fea-
ture set, BASELINE, is taken from McDonald and
Pereira (2005b). It uses the FORM and the POSTAG
fields. This set also includes features that combine
the label and POS tag of head and child such as
(Label, POSHead) and (Label, POSChild_0. For
our Arabic and Japanese development sets we ob-
tained the best results with this configuration. We
also use this configuration for Chinese, German and
Portuguese because training with other configura-
tions took too much time (more than 7 days).
The BASELINE also uses pseudo-coarse-POS tag
(1st character of the POSTAG) and pseudo-lemma
tag (4 characters of the FORM when the length
is more than 3). For the next configuration we
substitute these pseudo-tags by the CPOSTAG and
LEMMA fields that were given in the data. This con-
figuration was used for Czech because for other con-
figurations training could not be finished in time.
The third feature set tries to exploit the generic
FEATS field, which can contain a list features such
as case and gender. A set of features per depen-
dency is extracted using this information. It con-
sists of cross product of the features in FEATS. We
used this configuration for Danish, Dutch, Spanish
</bodyText>
<page confidence="0.995489">
228
</page>
<bodyText confidence="0.999513">
and Turkish where it showed the best results during
development.
The fourth feature set uses the triplet of la-
bel, POS child and head as a feature such as
(Label, POSHead, POSChild). It also uses the
CPOSTAG and LEMMA fields for the head. This
configuration is used for Slovene and Swedish data
where it performed best during development.
Finally, we add constraints for Chinese, Dutch,
Japanese and Slovene. In particular, arity constraints
to Chinese and Slovene, coordination and arity con-
straints to Dutch, arity and selective projectivity
constraints for Japanese2. For all experiments b was
set to 2. We did not apply additional constraints to
any other languages due to lack of time.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99992588">
Our results on the test set are shown in Table 1.
Our results are well above the average for all lan-
guages but Czech. For Chinese we perform signif-
icantly better than all other participants (p = 0.00)
and we are in the top three entries for Dutch, Ger-
man, Danish. Although Dutch and Chinese are lan-
guages were we included additional constraints, our
scores are not a result of these. Table 2 compares the
result for the languages with additional constraints.
Adding constraints only marginally helps to improve
the system (in the case of Slovene a bug in our im-
plementation even degraded accuracy). A more de-
tailed explanation to this observation is given in the
following section. A possible explanation for our
high accuracy in Chinese could be the fact that we
were not able to optimise the feature set on the de-
velopment set (see the previous section). Maybe this
prevented us from overfitting. It should be noted that
we did use non-projective parsing for Chinese, al-
though the corpus was fully projective. Our worst
results in comparison with other participants can be
seen for Czech. We attribute this to the reduced
training set we had to use in order to produce a
model in time, even when using the original MST
algorithm.
</bodyText>
<footnote confidence="0.990549">
2This is done in order to capture the fact that crossing de-
pendencies in Japanese could only be introduced through dis-
fluencies.
</footnote>
<subsectionHeader confidence="0.983579">
4.1 Chinese
</subsectionHeader>
<bodyText confidence="0.999994575757576">
For Chinese the parser was augmented with a set of
constraints that disallowed more than one argument
of the types head, goal, nominal, range, theme, rea-
son, DUMMY, DUMMY1 and DUMMY2.
By enforcing arity constraints we could either turn
wrong labels/heads into right ones and improve ac-
curacy or turn right labels/heads into wrong ones and
degrade accuracy. For the test set the number of im-
provements (36) was higher than the number of er-
rors (22). However, this margin was outweighed by
a few sentences we could not properly process be-
cause our inference method timed out. Our overall
improvement was thus unimpressive 7 tokens.
In the context of duplicate “head” dependencies
(that is, dependencies labelled “head”) the num-
ber of sentences where accuracy dropped far out-
weighed the number of sentences where improve-
ments could be gained. Removing the arity con-
straints on “head” labels therefore should improve
our results.
This shows the importance of good second best
dependencies. If the dependency with the second
highest score is the actual gold dependency and its
score is close to the highest score, we are likely to
pick this dependency in the presence of additional
constraints. On the other hand, if the dependency
with the second highest score is not the gold one and
its score is too high, we will probably include this
dependency in order to fulfil the constraints.
There may be some further improvement to be
gained if we train our model using k-best MIRA
with k &gt; 1 since it optimises weights with respect
to the k best parses.
</bodyText>
<subsectionHeader confidence="0.921842">
4.2 Turkish
</subsectionHeader>
<bodyText confidence="0.999953454545455">
There is a considerable gap between the unlabelled
and labelled results for Turkish. And in terms of la-
bels the POS type Noun gives the worst performance
because many times a subject was classified as ob-
ject or vice a versa.
Case information in Turkish assigns argument
roles for nouns by marking different semantic roles.
Many errors in the Turkish data might have been
caused by the fact that this information was not ad-
equately used. Instead of fine-tuning our feature set
to Turkish we used the feature cross product as de-
</bodyText>
<page confidence="0.995174">
229
</page>
<table confidence="0.9543845">
Model AR CH CZ DA DU GE JP PO SL SP SW TU
OURS 66.65 89.96 67.64 83.63 78.59 86.24 90.51 84.43 71.20 77.38 80.66 58.61
AVG 59.94 78.32 67.17 78.31 70.73 78.58 85.86 80.63 65.16 73.53 76.44 55.95
TOP 66.91 89.96 80.18 84.79 79.19 87.34 91.65 87.60 73.44 82.25 84.58 65.68
</table>
<tableCaption confidence="0.993927">
Table 1: Labelled accuracy on the test sets.
</tableCaption>
<table confidence="0.999126">
Constraints DU CH SL JA
with 3927 4464 3612 4526
without 3928 4471 3563 4528
</table>
<tableCaption confidence="0.92206">
Table 2: Number of tokens correctly classified with
and without constraints.
</tableCaption>
<bodyText confidence="0.989394222222222">
scribed in Section 3. Some of the rather meaning-
less combinations might have neutralised the effect
of sensible ones. We believe that using morpho-
logical case information in a sound way would im-
prove both the unlabelled and the labelled dependen-
cies. However, we have not performed a separate ex-
periment to test if using the case information alone
would improve the system any better. This could be
the focus of future work.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999990607142857">
In this work we presented a novel way of solving the
linear model of McDonald et al. (2005a) for projec-
tive and non-projective parsing based on an incre-
mental ILP approach. This allowed us to include
additional linguistics constraints such as “a verb can
only have one subject.”
Due to time constraints we applied additional
constraints to only four languages. For each one
we gained better results than the baseline without
constraints, however, this improvement was only
marginal. This can be attributed to 4 main rea-
sons: Firstly, the next best solution that fulfils the
constraints was even worse (Chinese). Secondly,
noisy POS tags caused coordination constraints to
fail (Dutch). Thirdly, inference timed out (Chinese)
and fourthly, constraints were not violated that often
in the first place (Japanese).
However, the effect of the first problem might be
reduced by training with a higher k. The second
problem could partly be overcome by using a bet-
ter tagger or by a special treatment within the con-
straint handling for word types which are likely to
be mistagged. The third problem could be avoidable
by adding constraints during the branch and bound
algorithm, avoiding the need to resolve the full prob-
lem “from scratch” for every constraint added. With
these remedies significant improvements to the ac-
curacy for some languages might be possible.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999912333333333">
We would like to thank Beata Kouchnir, Abhishek
Arun and James Clarke for their help during the
course of this project.
</bodyText>
<sectionHeader confidence="0.999665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998638384615384">
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach. Learn.
Res., 3:951–991.
M. Groetschel, L. Lovasz, and A. Schrijver. 1981. The ellipsoid
method and its consequences in combinatorial optimization.
Combinatorica, I:169– 197.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of the 11th
Annual Meeting of the EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. of the
43rd Annual Meeting of the ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Ha-
jic. 2005b. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT/EMNLP 2005,
Vancouver, B.C., Canada.
D. Roth and W. Yih. 2005. Integer linear programming in-
ference for conditional random fields. In Proc. of the In-
ternational Conference on Machine Learning (ICML), pages
737–744.
David Michael Warme. 1998. Spanning Trees in Hypergraphs
with Application to Steiner Trees. Ph.D. thesis, University of
Virginia.
Justin C. Williams. 2002. A linear-size zero - one program-
ming model for the minimum spanning tree problem in pla-
nar graphs. Networks, 39:53–60.
</reference>
<page confidence="0.997049">
230
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685608">
<title confidence="0.998848">Multi-lingual Dependency Parsing with Incremental Integer Programming</title>
<author confidence="0.993914">Riedel akıcı</author>
<affiliation confidence="0.998981">School of University of</affiliation>
<address confidence="0.761058">Edinburgh, EH8 9LW,</address>
<email confidence="0.997346">S.R.Riedel,R.Cakici,I.V.Meza-Ruiz@sms.ed.ac.uk</email>
<abstract confidence="0.994930368421053">Our approach to dependency parsing is based on the linear model of McDonald et al.(McDonald et al., 2005b). Instead of solving the linear model using the Maximum Spanning Tree algorithm we propose an incremental Integer Linear Programming formulation of the problem that allows us to enforce linguistic constraints. Our results show only marginal improvements over the non-constrained parser. In addition to the fact that many parses did not violate any constraints in the first place this can be attributed to three reasons: 1) the next best solution that fulfils the constraints yields equal or less accuracy, 2) noisy POS tags and 3) occasionally our inference algorithm was too slow and decoding timed out.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--951</pages>
<contexts>
<context position="8754" citStr="Crammer and Singer, 2003" startWordPosition="1485" endWordPosition="1488">figurations in which both dependencies are active. Projective Parsing In the incremental ILP framework projective parsing can be easily implemented by checking for crossing dependencies after each iteration and forbidding them in the next. If we see two dependencies that cross, di,j and dk,l, we add the constraint di,j + dk,l ≤ 1 to prevent this in the next iteration. This can also be used to prevent specific types of crossings. For instance, in Dutch we could only allow crossing dependencies as long as none of the dependencies is a “Determiner” relation. 2.2 Training We used single-best MIRA(Crammer and Singer, 2003).For all experiments we used 10 training iterations and non-projective decoding. Note that we used the original spanning tree algorithm for decoding during training as it was faster. 3 System Summary We use four different feature sets. The first feature set, BASELINE, is taken from McDonald and Pereira (2005b). It uses the FORM and the POSTAG fields. This set also includes features that combine the label and POS tag of head and child such as (Label, POSHead) and (Label, POSChild_0. For our Arabic and Japanese development sets we obtained the best results with this configuration. We also use th</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Groetschel</author>
<author>L Lovasz</author>
<author>A Schrijver</author>
</authors>
<title>The ellipsoid method and its consequences in combinatorial optimization.</title>
<date>1981</date>
<journal>Combinatorica,</journal>
<volume>169</volume>
<pages>197</pages>
<contexts>
<context position="5193" citStr="Groetschel et al. (1981)" startWordPosition="842" endWordPosition="845"> and bound algorithm. In practice solving models of this form did not converge after hours even for small sentences. To get around this problem we followed an incremental approach akin to Warme (1998). Instead of adding constraints that forbid all possible cycles in advance (this would result in an exponential number of constraints) we first solve the problem without any cycle constraints. Only if the result contains cycles we add constraints that forbid these cycles and run the solver again. This process is repeated until no more violated constraints are found. Figure 1 shows this algorithm. Groetschel et al. (1981) showed that such an approach will converge after a polynomial number of iterations with respect to the number of variables. 1. Solve IP Pi 2. Find violated constraints C in the solution of Pi 3. if C = 0 we are done 4. Pi�1 = Pi UC 5. i = i + 1 6. goto (1) Figure 1: Incremental Integer Linear Programming In practice, this technique showed fast convergence (less than 10 iterations) in most cases, yielding solving times of less than 0.5 seconds. However, for some sentences in certain languages, such as Chinese or Swedish, an optimal solution could not be found after 500 iterations. In the follo</context>
</contexts>
<marker>Groetschel, Lovasz, Schrijver, 1981</marker>
<rawString>M. Groetschel, L. Lovasz, and A. Schrijver. 1981. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, I:169– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of the 11th Annual Meeting of the EACL.</booktitle>
<contexts>
<context position="1486" citStr="McDonald and Pereira, 2006" startWordPosition="223" endWordPosition="226">less accuracy, 2) noisy POS tags and 3) occasionally our inference algorithm was too slow and decoding timed out. 1 Introduction This paper presents our submission for the CoNLL 2006 shared task of multilingual dependency parsing. Our parser is inspired by McDonald et al.(2005a) which treats the task as the search for the highest scoring Maximum Spanning Tree (MST) in a graph. This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. In their original framework, features are only defined over single attachment decisions. This leads to cases where basic linguistic constraints are not satisfied (e.g. verbs with two subjects). In this paper we present a novel way to implement the parsing algorithms for projective and non-projective parsing based on a more generic incremental Integer Linear Programming (ILP) approach. This allows us to include additional global constraints that can be used to impose linguistic information. The rest</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of the 11th Annual Meeting of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2807" citStr="McDonald et al. (2005" startWordPosition="440" endWordPosition="443">gramming model and how we trained its parameters. We then describe our feature and constraint sets for the 12 different languages of the task (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003). Finally, our results are discussed and error analyses for Chinese and Turkish are presented. 2 Model Our model is based on the linear model presented in McDonald et al. (2005a), � s (i, j) _ w- f (i, j) where x is a sentence, y a parse and s a score function over sentence-parse pairs. f (i, j) is a multidi(1) s (x, y) _ � (i,j)Ey 226 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 226–230, New York City, June 2006. c�2006 Association for Computational Linguistics mensional feature vector representation of the edge from token i to token j and w the corresponding weight vector. Decoding in this model amounts to finding the y for a given x that maximises s (x, y) y� = argmaxys (x, y) and y contains no cycles, attaches ex</context>
<context position="5977" citStr="McDonald et al. (2005" startWordPosition="981" endWordPosition="984">aints C in the solution of Pi 3. if C = 0 we are done 4. Pi�1 = Pi UC 5. i = i + 1 6. goto (1) Figure 1: Incremental Integer Linear Programming In practice, this technique showed fast convergence (less than 10 iterations) in most cases, yielding solving times of less than 0.5 seconds. However, for some sentences in certain languages, such as Chinese or Swedish, an optimal solution could not be found after 500 iterations. In the following section we present the bjective function, variables and linear constraints that make up the Integer Linear Program. 2.1.1 Variables In the implementation1 of McDonald et al. (2005b) dependency labels are handled by finding the best scoring label for a given token pair so that s (i, j) = max s (i, j, label) goes into Equation 1. This is only exact as long as no further constraints are added. Since our aim is to add constraints our variables need to explicitly model label decisions. Therefore, we introduce binary variables li,j,labelbi E 0..n, j E L.n, label E bestb (i, j) where n is the number of tokens and the index 0 represents the root token. bestb (i, j) is the set of b labels with maximal s (i, j, label). li,j,label equals 1 if there is a dependency with the label </context>
<context position="15339" citStr="McDonald et al. (2005" startWordPosition="2618" endWordPosition="2621"> 4471 3563 4528 Table 2: Number of tokens correctly classified with and without constraints. scribed in Section 3. Some of the rather meaningless combinations might have neutralised the effect of sensible ones. We believe that using morphological case information in a sound way would improve both the unlabelled and the labelled dependencies. However, we have not performed a separate experiment to test if using the case information alone would improve the system any better. This could be the focus of future work. 5 Conclusion In this work we presented a novel way of solving the linear model of McDonald et al. (2005a) for projective and non-projective parsing based on an incremental ILP approach. This allowed us to include additional linguistics constraints such as “a verb can only have one subject.” Due to time constraints we applied additional constraints to only four languages. For each one we gained better results than the baseline without constraints, however, this improvement was only marginal. This can be attributed to 4 main reasons: Firstly, the next best solution that fulfils the constraints was even worse (Chinese). Secondly, noisy POS tags caused coordination constraints to fail (Dutch). Thir</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. of the 43rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP 2005,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="2807" citStr="McDonald et al. (2005" startWordPosition="440" endWordPosition="443">gramming model and how we trained its parameters. We then describe our feature and constraint sets for the 12 different languages of the task (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003). Finally, our results are discussed and error analyses for Chinese and Turkish are presented. 2 Model Our model is based on the linear model presented in McDonald et al. (2005a), � s (i, j) _ w- f (i, j) where x is a sentence, y a parse and s a score function over sentence-parse pairs. f (i, j) is a multidi(1) s (x, y) _ � (i,j)Ey 226 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 226–230, New York City, June 2006. c�2006 Association for Computational Linguistics mensional feature vector representation of the edge from token i to token j and w the corresponding weight vector. Decoding in this model amounts to finding the y for a given x that maximises s (x, y) y� = argmaxys (x, y) and y contains no cycles, attaches ex</context>
<context position="5977" citStr="McDonald et al. (2005" startWordPosition="981" endWordPosition="984">aints C in the solution of Pi 3. if C = 0 we are done 4. Pi�1 = Pi UC 5. i = i + 1 6. goto (1) Figure 1: Incremental Integer Linear Programming In practice, this technique showed fast convergence (less than 10 iterations) in most cases, yielding solving times of less than 0.5 seconds. However, for some sentences in certain languages, such as Chinese or Swedish, an optimal solution could not be found after 500 iterations. In the following section we present the bjective function, variables and linear constraints that make up the Integer Linear Program. 2.1.1 Variables In the implementation1 of McDonald et al. (2005b) dependency labels are handled by finding the best scoring label for a given token pair so that s (i, j) = max s (i, j, label) goes into Equation 1. This is only exact as long as no further constraints are added. Since our aim is to add constraints our variables need to explicitly model label decisions. Therefore, we introduce binary variables li,j,labelbi E 0..n, j E L.n, label E bestb (i, j) where n is the number of tokens and the index 0 represents the root token. bestb (i, j) is the set of b labels with maximal s (i, j, label). li,j,label equals 1 if there is a dependency with the label </context>
<context position="15339" citStr="McDonald et al. (2005" startWordPosition="2618" endWordPosition="2621"> 4471 3563 4528 Table 2: Number of tokens correctly classified with and without constraints. scribed in Section 3. Some of the rather meaningless combinations might have neutralised the effect of sensible ones. We believe that using morphological case information in a sound way would improve both the unlabelled and the labelled dependencies. However, we have not performed a separate experiment to test if using the case information alone would improve the system any better. This could be the focus of future work. 5 Conclusion In this work we presented a novel way of solving the linear model of McDonald et al. (2005a) for projective and non-projective parsing based on an incremental ILP approach. This allowed us to include additional linguistics constraints such as “a verb can only have one subject.” Due to time constraints we applied additional constraints to only four languages. For each one we gained better results than the baseline without constraints, however, this improvement was only marginal. This can be attributed to 4 main reasons: Firstly, the next best solution that fulfils the constraints was even worse (Chinese). Secondly, noisy POS tags caused coordination constraints to fail (Dutch). Thir</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT/EMNLP 2005, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML),</booktitle>
<pages>737--744</pages>
<contexts>
<context position="3934" citStr="Roth and Yih (2005)" startWordPosition="640" endWordPosition="643">a given x that maximises s (x, y) y� = argmaxys (x, y) and y contains no cycles, attaches exactly one head to each non-root token and no head to the root node. 2.1 Decoding Instead of using the MST algorithm (McDonald et al., 2005b) to maximise equation 1, we present an equivalent ILP formulation of the problem. An advantage of a general purpose inference technique is the addition of further linguistically motivated constraints. For instance, we can add constraints that enforce that a verb can not have more than one subject argument or that coordination arguments should have compatible types. Roth and Yih (2005) is similarly motivated and uses ILP to deal with additional hard constraints in a Conditional Random Field model for Semantic Role Labelling. There are several explicit formulations of the MST problem as integer programs in the literature (Williams, 2002). They are based on the concept of eliminating subtours (cycles), cuts (disconnections) or requiring intervertex flows (paths). However, in practice these cause long solving times. While the first two types yield an exponential number of constraints, the latter one scales cubically but produces non-fractional solutions in its relaxed version,</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In Proc. of the International Conference on Machine Learning (ICML), pages 737–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Michael Warme</author>
</authors>
<title>Spanning Trees in Hypergraphs with Application to Steiner Trees.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Virginia.</institution>
<contexts>
<context position="4769" citStr="Warme (1998)" startWordPosition="774" endWordPosition="775">n the literature (Williams, 2002). They are based on the concept of eliminating subtours (cycles), cuts (disconnections) or requiring intervertex flows (paths). However, in practice these cause long solving times. While the first two types yield an exponential number of constraints, the latter one scales cubically but produces non-fractional solutions in its relaxed version, causing long runtime of the branch and bound algorithm. In practice solving models of this form did not converge after hours even for small sentences. To get around this problem we followed an incremental approach akin to Warme (1998). Instead of adding constraints that forbid all possible cycles in advance (this would result in an exponential number of constraints) we first solve the problem without any cycle constraints. Only if the result contains cycles we add constraints that forbid these cycles and run the solver again. This process is repeated until no more violated constraints are found. Figure 1 shows this algorithm. Groetschel et al. (1981) showed that such an approach will converge after a polynomial number of iterations with respect to the number of variables. 1. Solve IP Pi 2. Find violated constraints C in th</context>
</contexts>
<marker>Warme, 1998</marker>
<rawString>David Michael Warme. 1998. Spanning Trees in Hypergraphs with Application to Steiner Trees. Ph.D. thesis, University of Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin C Williams</author>
</authors>
<title>A linear-size zero - one programming model for the minimum spanning tree problem in planar graphs.</title>
<date>2002</date>
<journal>Networks,</journal>
<pages>39--53</pages>
<contexts>
<context position="4190" citStr="Williams, 2002" startWordPosition="682" endWordPosition="683">present an equivalent ILP formulation of the problem. An advantage of a general purpose inference technique is the addition of further linguistically motivated constraints. For instance, we can add constraints that enforce that a verb can not have more than one subject argument or that coordination arguments should have compatible types. Roth and Yih (2005) is similarly motivated and uses ILP to deal with additional hard constraints in a Conditional Random Field model for Semantic Role Labelling. There are several explicit formulations of the MST problem as integer programs in the literature (Williams, 2002). They are based on the concept of eliminating subtours (cycles), cuts (disconnections) or requiring intervertex flows (paths). However, in practice these cause long solving times. While the first two types yield an exponential number of constraints, the latter one scales cubically but produces non-fractional solutions in its relaxed version, causing long runtime of the branch and bound algorithm. In practice solving models of this form did not converge after hours even for small sentences. To get around this problem we followed an incremental approach akin to Warme (1998). Instead of adding c</context>
</contexts>
<marker>Williams, 2002</marker>
<rawString>Justin C. Williams. 2002. A linear-size zero - one programming model for the minimum spanning tree problem in planar graphs. Networks, 39:53–60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>