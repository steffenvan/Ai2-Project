<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001685">
<title confidence="0.995584">
Associating Facial Displays with Syntactic Constituents for Generation
</title>
<author confidence="0.981687">
Mary Ellen Foster
</author>
<affiliation confidence="0.9802675">
Informatik VI: Robotics and Embedded Systems
Technical University of Munich
</affiliation>
<address confidence="0.926845">
Boltzmannstraße 3, 85748 Garching, Germany
</address>
<email confidence="0.997431">
foster@in.tum.de
</email>
<sectionHeader confidence="0.995606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984882352941">
We present an annotated corpus of conversa-
tional facial displays designed to be used for
generation. The corpus is based on a record-
ing of a single speaker reading scripted out-
put in the domain of the target generation
system. The data in the corpus consists of
the syntactic derivation tree of each sentence
annotated with the full syntactic and prag-
matic context, as well as the eye and eye-
brow displays and rigid head motion used
by the the speaker. The behaviours of the
speaker show several contextual patterns,
many of which agree with previous findings
on conversational facial displays. The cor-
pus data has been used in several studies ex-
ploring different strategies for selecting fa-
cial displays for a synthetic talking head.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999707307692308">
An increasing number of systems designed to au-
tomatically generate linguistic and multimodal out-
put now make use of corpora to help in decision-
making (cf. Belz and Varges, 2005). Some imple-
mentations use corpora to help select output that is
grammatical or fluent; for example, Langkilde and
Knight (1998) and White (2006) both used n-gram
language models to guide stochastic surface realis-
ers. In other systems, corpora are used to make
decisions based on pragmatic factors such as the
reading level of the target user (Williams and Re-
iter, 2005) or the visual features of an object be-
ing described (Cassell et al., 2007). The latter type
</bodyText>
<page confidence="0.968935">
25
</page>
<bodyText confidence="0.985696901639345">
of domain-specific contextual information is not of-
ten included in generally-available corpora. For this
reason, developers of generation systems that need
this type of information often create and make use
of application-specific corpora.
The easiest method of including the necessary
pragmatic information in a corpus is to base the cor-
pus on output generated in situations where the con-
textual factors are known; this eliminates the need to
annotate these factors explicitly. Stone et al. (2004),
for example, created a multimodal corpus based on
the voice and body language of an actor performing
scripted output in the domain of the target genera-
tion system: an animated instructor character for a
snowboarding video game. The contextual informa-
tion in the corpus scripts included the move that the
player attempted in the game and the result of that
attempt. Similarly, van Deemter et al. (2006) cre-
ated a corpus of multimodal referring expressions
produced in specific pragmatic contexts and used it
to compare several referring-expression generation
algorithms to human performance.
In this work, the task is to select facial displays
for an animated talking head to use while present-
ing output in the COMIC multimodal dialogue sys-
tem (Foster et al., 2005), which generates spoken
descriptions and comparisons of bathroom-tile op-
tions. The output of the COMIC text planner in-
cludes a range of information in addition to the text:
the syntactic derivation tree, the user’s evaluation
of the object being described, the information sta-
tus (new or old, contrastive) of each fact described,
and the predicted speech-synthesiser prosody. All of
this contextual information can be used to help select
Proceedings of the Linguistic Annotation Workshop, pages 25–32,
Prague, June 2007. c�2007 Association for Computational Linguistics
appropriate facial displays to accompany the spo-
ken presentation; however—as in the other systems
mentioned above—this requires a corpus where the
full context for every facial display is known. To cre-
ate such a corpus, we recorded a speaker performing
scripted output in the domain of COMIC.
This paper is arranged as follows. In Section 2,
we first describe how the scripts for the corpus were
created and how the recording was made. Section 3
then presents the annotation scheme and the tool that
was used to perform the annotation, while Section 4
describes the measures that were taken to ensure that
the annotation was reliable. Section 5 then sum-
marises the high-level patterns that were found in the
displays annotated in the corpus and compares them
to other findings on conversational facial displays.
At the end of the section, we use the corpus data to
test two assumptions that were made in the annota-
tion scheme. After that, in Section 6, we describe
several experiments in which different methods of
using the data in this corpus to select facial displays
for a synthetic head have been compared. Finally,
in Section 7, we summarise the contributions of this
paper and draw some conclusions about the useful-
ness of this corpus for its intended task.
</bodyText>
<sectionHeader confidence="0.955977" genericHeader="introduction">
2 Recording
</sectionHeader>
<bodyText confidence="0.993359714285714">
For this corpus, we recorded a single speaker read-
ing a set of 444 scripted sentences in the domain of
the COMIC multimodal dialogue system. The sen-
tences were generated by the full COMIC output-
generation process, which uses the OpenCCG sur-
face realiser (White, 2006) to create texts includ-
ing prosodic specifications for the speech synthe-
siser and incorporates information from the dialogue
history and a model of the user’s likes and dislikes.
Every node in the OpenCCG derivation tree for
each sentence in the script was initially annotated
with all of the available syntactic and pragmatic in-
formation from the output planner, including the fol-
lowing features:
</bodyText>
<listItem confidence="0.9957234">
• The user-model evaluation of the object being
described (positive or negative);
• Whether the fact being presented was previ-
ously mentioned in the discourse (as I said be-
fore, ... ) or is new information;
</listItem>
<subsubsectionHeader confidence="0.485535">
“Although it&apos;s in the family style, the tiles are by Alessi Tiles.”
</subsubsectionHeader>
<figureCaption confidence="0.978299">
Figure 1: Annotated OpenCCG derivation tree
</figureCaption>
<listItem confidence="0.999884333333333">
• Whether the fact is explicitly compared or con-
trasted with a feature of the previous tile design
(once again ... but here ... );
• Whether the node is in the first clause of a two-
clause sentence, in the second clause, or is an
only clause;1
• The surface string associated with the node;
• The surface string, with words replaced by se-
mantic classes or stems drawn from the gram-
mar (e.g., this design is classic becomes this
[mental-obj] be [style]); and
• Any pitch accents specified by the text planner.
</listItem>
<bodyText confidence="0.997702461538462">
Figure 1 illustrates the annotated OpenCCG
derivation tree for a sample sentence drawn from
the recording script. The annotations indicate that
every node in the first half of this sentence is associ-
ated with a negative user-model evaluation and is in
the first clause of a two-clause sentence, while every
node in the second half is linked to a positive eval-
uation and is in the second clause of the sentence.
The figure also shows the pitch accents selected by
the output planner according to Steedman’s (2000)
theory of information structure and intonation.
For the recording, the sentences in the script were
presented one at a time to the speaker; the presen-
</bodyText>
<equation confidence="0.675053285714286">
1No sentence in the script had more than two clauses.
● although it&apos;s in the family style
● although
● it&apos;s in the family style
● it
● &apos;s in the family style
● &apos;s
</equation>
<figure confidence="0.984539809523809">
● in the family style
● in
● the family style
● the
● family style
● family
● style
User model: bad
Clause: first
Accent: L+H*
● the tiles are by Alessi Tiles
● the tiles
● the
● tiles
● are by Alessi Tiles
● are
● by Alessi Tiles
● Alessi Tiles
User model: good
Clause: second
Accent: H*
</figure>
<page confidence="0.981884">
26
</page>
<bodyText confidence="0.999992777777778">
tation included both the linguistic content (with ac-
cented words highlighted) as well as the intended
pragmatic context. Each sentence was displayed in
a large font on a laptop computer directly in front
of the speaker, with the camera positioned directly
above the laptop to ensure that the speaker was look-
ing towards the camera at all times. The speaker was
instructed to read each sentence out loud as expres-
sively as possible into the camera.
</bodyText>
<sectionHeader confidence="0.991053" genericHeader="method">
3 Annotation
</sectionHeader>
<bodyText confidence="0.999921428571429">
Once all of the sentences in the script had been
recorded as described in the preceding section, the
next step was to annotate the facial displays that oc-
curred. We first used Anvil (Kipp, 2004) to split
the video into individual clips corresponding to each
sentence. This section describes how the facial dis-
plays in each of the clips were then annotated.
</bodyText>
<subsectionHeader confidence="0.999842">
3.1 Annotation scheme
</subsectionHeader>
<bodyText confidence="0.99965268">
We annotated the speaker’s facial displays by linking
each to the span of nodes in the OpenCCG derivation
tree with which it was temporally related. Making
cross-modal links at this level made it possible to
use the annotated information directly in the output-
generation process for the experiments described in
Section 6.
A display was associated with the full span of
words that it coincided with temporally, as follows.
If a single node in the derivation tree covered ex-
actly all of the relevant words, then the annotation
was placed on that node; if the words spanned by a
display did not coincide with a single node, it was
attached to the set of nodes that did span the neces-
sary words. For example, in the derivation shown in
Figure 1, the sequence the family style is associated
with a single node, so a motion temporally associ-
ated with that sequence would be attached to that
node. On the other hand, if there were a motion as-
sociated with the tiles are, it would be attached to
both the the tiles node and the are node.
The following were the features that were consid-
ered; for each feature, we note the corresponding
Action Unit (AU) from the well-known Facial Ac-
tion Coding System (Ekman et al., 2002).
</bodyText>
<listItem confidence="0.9999695">
• Eyebrows: up (AU 1+2) or down (AU 4)
• Eye squinting (AU 43)
</listItem>
<figureCaption confidence="0.500019">
Figure 2: Annotation tool
</figureCaption>
<listItem confidence="0.999878">
• Head nodding: up (AU 53) or down (AU 54)
• Head leaning: left (AU 55) or right (AU 56)
• Head turning: left (AU 57) or right (AU 58)
</listItem>
<bodyText confidence="0.9992006">
This set of displays was chosen based on a combi-
nation of three factors: the emphatic facial displays
documented in the literature, the capabilities of the
target talking head, and the actual displays of the
speaker during the recording session.
</bodyText>
<subsectionHeader confidence="0.999917">
3.2 Annotation tool
</subsectionHeader>
<bodyText confidence="0.999972105263158">
The tool for the annotation was a custom-written
program that allowed the coder to play back a
recorded sentence at full speed or slowed down, and
to associate any combination of displays with any
node or set of nodes in the OpenCCG derivation tree
of the sentence. The tool also allowed the coder to
play back a proposed annotation sequence on a syn-
thetic talking head to verify that it was as close as
possible to the actual motions. Figure 2 shows a
screenshot of the annotation tool in use on the sen-
tence from Figure 1. In the screenshot, a left turn is
attached to the entire sentence (i.e., the root node),
while a series of nods is associated with single leaf
nodes in the first half of the sentence. The annotator
has already attached a brow raise to the word are in
the second half and is in the process of adding a nod
to the same word.
The output of the annotation tool is an XML doc-
ument including the original contextually-annotated
</bodyText>
<page confidence="0.98283">
27
</page>
<bodyText confidence="0.644492166666667">
&lt;node surf=&amp;quot;although it ’s in the family style the tiles are by Alessi_Tiles&amp;quot; LEAN=&amp;quot;left&amp;quot;
sc=&amp;quot;although [pro3n] be in the [style] [abstraction] the [phys-obj] be by [manufacturer]&amp;quot;&gt;
&lt;node surf=&amp;quot;although it ’s in the family style&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot;
sc=&amp;quot;although [pro3n] be in the [style] [abstraction]&amp;quot;&gt;
&lt;node surf=&amp;quot;although&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; NOD=&amp;quot;down&amp;quot; /&gt;
&lt;node surf=&amp;quot;it ’s in the family style&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot;
</bodyText>
<figure confidence="0.90649084375">
sc=&amp;quot;[pro3n] be in the [style] [abstraction]&amp;quot;&gt;
&lt;node surf=&amp;quot;it&amp;quot; stem=&amp;quot;pro3n&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; NOD=&amp;quot;down&amp;quot; /&gt;
&lt;node surf=&amp;quot;’s in the family style&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; sc=&amp;quot;be in the [style] [abstraction]&amp;quot;&gt;
&lt;node surf=&amp;quot;’s&amp;quot; stem=&amp;quot;be&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; NOD=&amp;quot;down&amp;quot; /&gt;
&lt;node surf=&amp;quot;in the family style&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; sc=&amp;quot;in the [style] [abstraction]&amp;quot;&gt;
&lt;node surf=&amp;quot;in&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; NOD=&amp;quot;down&amp;quot; /&gt;
&lt;node surf=&amp;quot;the family style&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; sc=&amp;quot;the [style] [abstraction]&amp;quot;&gt;
&lt;node surf=&amp;quot;the&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; /&gt;
&lt;node surf=&amp;quot;family style&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; sc=&amp;quot;[style] [abstraction]&amp;quot;&gt;
&lt;node surf=&amp;quot;family&amp;quot; sc=&amp;quot;[style]&amp;quot; accent=&amp;quot;L+H*&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; NOD=&amp;quot;down&amp;quot; /&gt;
&lt;node surf=&amp;quot;style&amp;quot; sc=&amp;quot;[abstraction]&amp;quot; um=&amp;quot;b&amp;quot; first=&amp;quot;y&amp;quot; /&gt;
&lt;/node &gt;
&lt;/node&gt;
&lt;/node&gt;
&lt;/node&gt;
&lt;/node&gt;
&lt;/node&gt;
&lt;node surf=&amp;quot;the tiles are by Alessi_Tiles&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot;
sc=&amp;quot;the [phys-obj] be by [manufacturer]&amp;quot;&gt;
&lt;node surf=&amp;quot;the tiles&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; sc=&amp;quot;the [phys-obj]&amp;quot;&gt;
&lt;node surf=&amp;quot;the&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; /&gt;
&lt;node surf=&amp;quot;tiles&amp;quot; sc=&amp;quot;[phys-obj]&amp;quot; stem=&amp;quot;tile&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; /&gt;
&lt;/node&gt;
&lt;node surf=&amp;quot;are by Alessi_Tiles&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; sc=&amp;quot;be by [manufacturer]&amp;quot;&gt;
&lt;node surf=&amp;quot;are&amp;quot; stem=&amp;quot;be&amp;quot; accent=&amp;quot;H*&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; BROW=&amp;quot;up&amp;quot; NOD=&amp;quot;down&amp;quot; /&gt;
&lt;node surf=&amp;quot;by Alessi_Tiles&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; sc=&amp;quot;by [manufacturer]&amp;quot;&gt;
&lt;node surf=&amp;quot;by&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; /&gt;
&lt;node surf=&amp;quot;Alessi_Tiles&amp;quot; sc=&amp;quot;[manufacturer]&amp;quot; accent=&amp;quot;H*&amp;quot; um=&amp;quot;g&amp;quot; first=&amp;quot;n&amp;quot; /&gt;
&lt;/node&gt;
&lt;/node&gt;
&lt;/node&gt;
&lt;/node&gt;
</figure>
<figureCaption confidence="0.999985">
Figure 3: Annotated sentence from the corpus
</figureCaption>
<bodyText confidence="0.999926947368421">
OpenCCG derivation tree of each sentence, with
each node additionally labelled with a (possibly
empty) set of facial displays. Figure 3 shows the
fully-annotated version of the sentence from Fig-
ure 1. This document includes the contextual fea-
tures from the original tree, indicated by italics: ev-
ery node in the first subtree has um=&amp;quot;b&amp;quot; and first=&amp;quot;y&amp;quot;,
while every node in the second subtree has um=&amp;quot;g&amp;quot;
and first=&amp;quot;n&amp;quot;, while the accented items also have an
accent feature. Every node also specifies the string
generated by the subtree that it spans, both in its sur-
face form (surf) and with semantic-class and stem
replacement (sc). This tree also includes the facial
displays added by the coder in Figure 2, indicated
by underlining: (LEAN=&amp;quot;left&amp;quot;) attached to the root
node), a number of downward nods (NOD=&amp;quot;down&amp;quot;) on
individual words in the first half of the sentence, and
a nod accompanied by a brow raise (BROW=&amp;quot;up&amp;quot;) on
are near the end.
</bodyText>
<sectionHeader confidence="0.961229" genericHeader="method">
4 Reliability of the annotation
</sectionHeader>
<bodyText confidence="0.999971666666667">
Several measures were taken to ensure that the an-
notation process was reliable. As the first step, two
independent coders each separately processed the
same set of 20 sentences, using an initial annotation
scheme. The outputs of these two coders were com-
pared, and the coders discussed the differences and
agreed on a revised scheme. One of these coders
then used the final scheme to process the entire set
of 444 sentences. As a further test of reliability, an
</bodyText>
<page confidence="0.993058">
28
</page>
<bodyText confidence="0.999975272727272">
additional coder was instructed on the use of the an-
notation tool and scheme and used them to process
286 sentences (approximately 65% of the corpus).
To assess the degree of agreement between these
two coders, we used a version of the R agreement
coefficient proposed by Artstein and Poesio (2005).
R is designed as a coefficient that is weighted, that
applies to multiple coders, and that uses a separate
probability distribution for each coder. Weighted
coefficients like R permit degrees of agreement to
be measured, so that partial agreement is penalised
less severely than total disagreement. Like other
weighted coefficients, R is based on the ratio be-
tween the observed and expected disagreement on
the corpus.
To use this coefficient, it is necessary to define
a measure that computes the distance between two
proposed annotations. In this case, to compute the
observed disagreement Do(S) on a sentence S, we
use a measure similar to that proposed by Passon-
neau (2004) for measuring agreement on set-valued
annotations. For each display proposed by each
coder on the sentence, we search for a correspond-
ing display proposed by the other coder—one with
the same value (e.g., a brow raise) and covering a
similar span of nodes. If both proposed exactly the
same display, that indicates no disagreement (0); if
one display covers a strict subset of the nodes cov-
ered by the other, that indicates minor disagreement
(13); if the nodes covered by the two proposals over-
lap, that is a more major disagreement (23); and if no
corresponding display can be found from the second
coder, then that indicates the maximum level of dis-
agreement (1). The total observed disagreement on
a sentence is the sum of the disagreement level for
each display proposed by each coder.
The expected disagreement De(S) for a sentence
S depends on the length of that sentence, as fol-
lows. We first use the corpus counts to compute
the probability of each coder assigning each pos-
sible facial display to word spans of all possible
lengths. We then use these probabilities to estimate
the likelihood of the two coders assigning identical,
super/subset, overlapping, or disjoint annotations to
the sentence, for each possible display. The total
expected disagreement for the sentence is the sum
of these probabilities across all displays, using the
same weights as the observed disagreement above.
The overall observed disagreement in the corpus
Do is the arithmetic mean of the disagreement on
each sentence; similarly, the overall expected dis-
agreement De is the mean of the expected disagree-
ment across all of the sentences. To compute the
value of R for the output of the two coders, we sub-
tract the ratio of these two values from 1:
</bodyText>
<equation confidence="0.971955666666667">
Do
R = 1−
De
</equation>
<bodyText confidence="0.999950611111111">
As Artstein and Poesio (2005) point out, for
weighted measures such as R, there is no signif-
icance test for agreement, and the actual value is
strongly affected by the distance metric that is se-
lected. However, R values can be compared with
one another to assess degrees of agreement. The
overall R value between the two coders on the full
set of 286 sentences processed by both was 0.561,
with R values on individual facial displays ranging
from a high of 0.661 on nodding to a low of 0.285
on squinting (a very rare motion). To put these val-
ues into context, we computed R on the set of 20
sentences processed by the final coder as part of the
training process (which are not included in the set
of 286). The overall R value for these sentences is
0.231, with negative values for some of the individ-
ual displays. This demonstrates that the training pro-
cess had a positive effect on agreement.
</bodyText>
<sectionHeader confidence="0.928107" genericHeader="method">
5 Patterns in the corpus
</sectionHeader>
<bodyText confidence="0.999938777777778">
We investigated the contextual features to see which
had the most significant effect on the facial displays
occurring on a node. To determine this, we used
multinomial logit regression to select the factors and
factor interactions that had the most significant ef-
fects on the distribution of each display; this form of
regression is appropriate when, as in this case, the
response variable is categorical. In this section, we
list the most significant factors and give a qualitative
description of the impact of each.
The single most influential contextual factor was
the user-model evaluation, which had an effect on all
of the facial displays. In positive user-model con-
texts, eyebrow raising and turning to the right were
relatively more frequent (Figure 4(a)); in negative
contexts, on the other hand, the rates of eyebrow
lowering, squinting, and leaning to the left were all
higher (Figure 4(b)). Other factors also affected the
</bodyText>
<page confidence="0.995181">
29
</page>
<figure confidence="0.998809">
(a) Positive (b) Negative
</figure>
<figureCaption confidence="0.999971">
Figure 4: Characteristic facial displays for different user-model evaluations
</figureCaption>
<bodyText confidence="0.999969911764706">
distribution of facial displays. In the first half of
two-clause sentences, brow lowering was also more
frequent, as was upward nodding, while downward
nodding and right turns showed up more often in the
second clause of two-clause sentences. Nodding and
brow raising were both more frequent on nodes with
any sort of predicted pitch accent.
Several of these factors agree with previous find-
ings on conversational body language. The in-
creased frequency of nodding and brow raising on
accented words agrees with many previous stud-
ies: Ekman (1979), Cavé et al. (1996), Graf et al.
(2002), Keating et al. (2003), Krahmer and Swerts
(2004), and Flecha-García (2006) all noted similar
displays on prosodically accented parts of the sen-
tence. The speaker’s tendency to move right on pos-
itive descriptions and left on negative descriptions
is also consistent with other findings. According
to the work of Davidson and colleagues (Davidson
and Irwin, 1999), emotion and affect processing are
asymmetrically organised in the human brain. The
right hemisphere is associated with negative affect
(and withdrawal behaviours), and the left with posi-
tive affect (and approach behaviours). Because both
perceptual and motor systems are contra-laterally or-
ganised, this means that higher levels of right hemi-
sphere activity are associated with attention being
oriented towards the left, while higher levels of left
hemisphere activity are associated with attention be-
ing oriented to the right; this fits with our speaker’s
pattern of movements.
The annotation scheme described here allowed a
display to be associated with any contiguous span of
words in the sentence. Annotators were encouraged
to use syntactic constituents wherever possible, but
also had the option to select multiple nodes where a
display did not correspond with a single constituent
in the derivation tree. Earlier versions of the annota-
tion scheme did not support this degree of flexibility,
so we used the patterns in the corpus to test whether
the modifications to the scheme were useful.
In a previous study using the same video record-
ings but a different, simpler scheme (Foster and
Oberlander, 2006), facial displays could only be as-
sociated with single leaf nodes (i.e., words); that is,
in the terminology of Ekman (1979), all motions
were considered to be batons rather than underlin-
ers. Based on the data in the current corpus, that
restriction was clearly unrealistic: the mean number
of nodes spanned by a display in the full corpus was
1.95, with a maximum of 15 and a standard devia-
tion of 2. The results were similar in the sub-corpus
produced by the final coder, in which the mean num-
ber of nodes spanned by a display was 2.25.
The annotation rules for this study did not ini-
tially permit displays to be associated with more
than nodes in the derivation tree. This capability
was added following inter-coder discussions after
the initial test annotation to deal with cases where
the speaker’s displays did not correspond to syntac-
tic constituents—for example, if the speaker raised
his eyebrows on the tiles are or some other such
non-standard constituent. The data in the annotated
corpus supports this modification. Approximately
6% of the annotations in the main corpus—165 of
2826—were attached to more than one node in the
derivation tree; for the final coder, 4.5% of annota-
tions were on multiple nodes.
</bodyText>
<page confidence="0.997972">
30
</page>
<sectionHeader confidence="0.986195" genericHeader="method">
6 Generation experiments
</sectionHeader>
<bodyText confidence="0.999992105263158">
The primary reason for creating this corpus of fa-
cial displays was to use the resulting data to select
facial displays for the artificial talking head in the
COMIC multimodal dialogue system. Several dif-
ferent strategies have been implemented to use the
corpus data for this task, and a number of automated
and human evaluations have been carried out com-
paring the different implementations.
As described in the preceding section, the fac-
tor with the largest influence on the displays of
the recorded speaker was the user-model evaluation.
Two studies (Foster, 2007b) were carried out to test
the generality of the characteristic positive and neg-
ative displays (Figure 4). In the first study, users
were asked to identify the intended user-model po-
larity of a description presented by the talking head
based only on the facial displays. The participants
were generally able to recognise the characteristic
positive and negative facial displays; they also iden-
tified the displays intended to be neutral (nodding
alone) as positive, and tended to judge videos with
no facial displays to be negative. In the second study,
users’ subjective preferences were gathered between
videos in which the user-model evaluation expressed
in speech was either consistent or inconsistent with
the facial displays. In this study, the participants
generally preferred the videos that showed consis-
tent content on the two output channels.
In another study (Foster and Oberlander, 2007),
two different data-driven strategies were imple-
mented that used the corpus data to select facial dis-
plays to accompany speech. One strategy always se-
lected the highest-probability option in all contexts,
while the other made a stochastic choice among all
of the options weighted by the corpus probabili-
ties. These two strategies were compared against
each other using both automated and human eval-
uation methods: the majority strategy scored more
highly on the automated cross-validation, while the
weighted strategy was strongly preferred by human
judges. The judges also preferred resynthesised ver-
sions of the original facial displays from the corpus
to the output of either of the generation strategies.
Two further human evaluation studies compared
the weighted data-driven generation strategy from
the preceding study to a rule-based strategy that
selected the most characteristic displays based
only on the user-model evaluation (Foster, 2007a).
When users’ subjective judgements were gathered
as above, they had a mild preference for the out-
put of the weighted strategy over that of the rule-
based strategy. In a second study, videos generated
by the weighted strategy significantly decreased par-
ticipants’ ability to select descriptions that were cor-
rectly tailored to a given set of user preferences,
while videos generated by the rule-based strategy
had no such impact.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999943735294117">
We have described the collection and annotation of
an application-specific corpus of conversational fa-
cial displays. The designs of both the corpus and
the annotation scheme were driven by the needs of
a specific generation system, which makes use of a
range of pragmatic information while creating out-
put. To use this information to make corpus-based
decisions, it is necessary that the full context of ev-
ery utterance and facial display in the corpus be
available. Rather than adding this information to an
existing corpus, we chose—like Stone et al. (2004)
and van Deemter et al. (2006), for example—to cre-
ate a corpus based on known contexts so that the
full information for every sentence was known be-
fore the fact.
The final annotation scheme required each facial
display to be linked to the set of nodes in the syntac-
tic derivation tree of the sentence that exactly cov-
ered the words temporally associated with the dis-
play. Two coders separately processed the sentences
in the corpus; on the sentences processed by both
coders (about 65% of the corpus), the agreement as
measured by R was 0.561.
A number of contextual factors had an influ-
ence on the displays used by the recorded speaker.
The single most influential factor was the user-
model evaluation of the object being described.
The speaker’s characteristic side-to-side motions on
these sentences agree with findings on the relation-
ship between brain hemispheres and affect. In ad-
dition, in user studies, human judges were reliably
able to identify the intended affect based on resyn-
thesised versions of these characteristic displays.
Other patterns in the data also agree with exist-
</bodyText>
<page confidence="0.999582">
31
</page>
<bodyText confidence="0.99991925">
ing findings on facial displays: for example, the
speaker tended to nod and raise his eyebrows more
frequently on words with prosodic accents.
Several experiments have been performed in
which the annotated data from this corpus was used
to select the facial displays to accompany the out-
put of an animated talking head. These studies have
found interesting results on both the relationship be-
tween automated and human judgements of output
quality and the relative utility of rule-based and data-
driven approaches for selecting conversational facial
displays.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996876">
This research was supported by the EU projects
COMIC (IST-2001-32311) and JAST (FP6-003747-
IP). Thanks to Amy Isard, Ron Petrick, and Tom
Segler for annotation assistance, and to Jon Ober-
lander and the LAW reviewers for useful comments.
</bodyText>
<sectionHeader confidence="0.998175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887152173912">
R. Artstein and M. Poesio. 2005. Kappa3 = alpha (or beta).
Technical Report CSM-437, University of Essex Department
of Computer Science.
A. Belz and S. Varges, editors. 2005. Corpus Linguistics 2005
Workshop on Using Corpora for Natural Language Genera-
tion. http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.
J. Cassell, S. Kopp, P. Tepper, K. Ferriman, and K. Striegnitz.
2007. Trading spaces: How humans and humanoids use
speech and gesture to give directions. In T. Nishida, edi-
tor, Engineering Approaches to Conversational Informatics.
Wiley. In press.
C. Cavé, I. Guaïtella, R. Bertrand, S. Santi, F. Harlay, and R. Es-
pesser.1996. About the relationship between eyebrow move-
ments and F0 variations. In Proceedings of the 4th Interna-
tional Conference on Spoken Language Processing (ICSLP
1996).
R. J. Davidson and W. Irwin. 1999. The functional neu-
roanatomy of emotion and affective style. Trends in Cog-
nitive Sciences, 3(1):11–21. doi:10.1016/S1364-6613(98)
01265-0.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building
a semantically transparent corpus for the generation of refer-
ring expressions. In Proceedings of the Fourth International
Natural Language Generation Conference, pages 130–132.
Sydney, Australia. ACL Anthology W06-1420.
P. Ekman. 1979. About brows: Emotional and conversational
signals. In M. von Cranach, K. Foppa, W. Lepenies, and
D. Ploog, editors, Human Ethology: Claims and limits of a
new discipline. Cambridge University Press.
P. Ekman, W. V. Friesen, and J. C. Hager. 2002. Facial Action
Coding System. A Human Face, Salt Lake City.
M. L. Flecha-García. 2006. Eyebrow raising in dialogue:
Discourse structure, utterance function, and pitch accents.
Ph.D. thesis, Department of Theoretical and Applied Lin-
guistics, University of Edinburgh.
M. E. Foster. 2007a. Comparing rule-based and data-driven se-
lection of facial displays. In Proceedings of the ACL 2007
Workshop on Embodied Language Processing.
M. E. Foster. 2007b. Generating embodied descriptions tailored
to user preferences. In submission.
M. E. Foster and J. Oberlander. 2006. Data-driven generation
of emphatic facial displays. In Proceedings of the 11th Con-
ference of the European Chapter of the Association for Com-
putational Linguistics (EACL 2006), pages 353–360. Trento,
Italy. ACL Anthology E06-1045.
M. E. Foster and J. Oberlander. 2007. Corpus-based generation
of conversational facial displays. In submission.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Mul-
timodal generation in the COMIC dialogue system. In Pro-
ceedings of the ACL 2005 Demo Session. ACL Anthology
W06-1403.
H. Graf, E. Cosatto, V. Strom, and F. Huang. 2002. Visual
prosody: Facial movements accompanying speech. In Pro-
ceedings of the 5th IEEE International Conference on Auto-
matic Face and Gesture Recognition (FG 2002), pages 397–
401. doi:10.1109/AFGR.2002.1004186.
P. Keating, M. Baroni, S. Mattys, R. Scarborough, and A. Al-
wan. 2003. Optical phonetics and visual perception of lexi-
cal and phrasal stress in English. In Proceedings of the 15th
International Congress of Phonetic Sciences (ICPhS), pages
2071–2074.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
E. Krahmer and M. Swerts. 2004. More about brows: A cross-
linguistic study via analysis-by-synthesis. In C. Pelachaud
and Z. Ruttkay, editors, From Brows to Trust: Evaluating
Embodied Conversational Agents, pages 191–216. Kluwer.
doi:10.1007/1-4020-2730-3_7.
I. Langkilde and K. Knight. 1998. The practical value of n-
grams in generation. In Proceedings of the 9th International
Natural Language Generation Workshop (INLG 1998). ACL
Anthology W98-1426.
R. J. Passonneau. 2004. Computing reliability for coreference
annotation. In Proceedings, Fourth International Conference
on Language Resources and Evaluation (LREC 2004), vol-
ume 4, pages 1503–1506. Lisbon.
M. Steedman. 2000. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 31(4):649–689.
doi:10.1162/002438900554505.
M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,
and C. Bregler. 2004. Speaking with hands: Creating
animated conversational characters from recordings of hu-
man performance. ACM Transactions on Graphics (TOG),
23(3):506–513. doi:10.1145/1015706.1015753.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research on
Language and Computation, 4(1):39–75. doi:10.1007/
s11168-006-9010-2.
S. Williams and E. Reiter. 2005. Deriving content selection
rules from a corpus of non-naturally occurring documents
for a novel NLG application. In Belz and Varges (2005).
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.424366">
<title confidence="0.999967">Associating Facial Displays with Syntactic Constituents for Generation</title>
<author confidence="0.7606825">Mary Ellen Informatik Robotics</author>
<author confidence="0.7606825">Embedded</author>
<affiliation confidence="0.96204">Technical University of</affiliation>
<address confidence="0.817654">Boltzmannstraße 3, 85748 Garching,</address>
<email confidence="0.99942">foster@in.tum.de</email>
<abstract confidence="0.998243611111111">We present an annotated corpus of conversational facial displays designed to be used for generation. The corpus is based on a recording of a single speaker reading scripted output in the domain of the target generation system. The data in the corpus consists of the syntactic derivation tree of each sentence annotated with the full syntactic and pragmatic context, as well as the eye and eyebrow displays and rigid head motion used by the the speaker. The behaviours of the speaker show several contextual patterns, many of which agree with previous findings on conversational facial displays. The corpus data has been used in several studies exploring different strategies for selecting facial displays for a synthetic talking head.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Kappa3 = alpha (or beta).</title>
<date>2005</date>
<tech>Technical Report CSM-437,</tech>
<institution>University of Essex Department of Computer Science.</institution>
<contexts>
<context position="14441" citStr="Artstein and Poesio (2005)" startWordPosition="2378" endWordPosition="2381"> processed the same set of 20 sentences, using an initial annotation scheme. The outputs of these two coders were compared, and the coders discussed the differences and agreed on a revised scheme. One of these coders then used the final scheme to process the entire set of 444 sentences. As a further test of reliability, an 28 additional coder was instructed on the use of the annotation tool and scheme and used them to process 286 sentences (approximately 65% of the corpus). To assess the degree of agreement between these two coders, we used a version of the R agreement coefficient proposed by Artstein and Poesio (2005). R is designed as a coefficient that is weighted, that applies to multiple coders, and that uses a separate probability distribution for each coder. Weighted coefficients like R permit degrees of agreement to be measured, so that partial agreement is penalised less severely than total disagreement. Like other weighted coefficients, R is based on the ratio between the observed and expected disagreement on the corpus. To use this coefficient, it is necessary to define a measure that computes the distance between two proposed annotations. In this case, to compute the observed disagreement Do(S) </context>
<context position="16901" citStr="Artstein and Poesio (2005)" startWordPosition="2791" endWordPosition="2794">overlapping, or disjoint annotations to the sentence, for each possible display. The total expected disagreement for the sentence is the sum of these probabilities across all displays, using the same weights as the observed disagreement above. The overall observed disagreement in the corpus Do is the arithmetic mean of the disagreement on each sentence; similarly, the overall expected disagreement De is the mean of the expected disagreement across all of the sentences. To compute the value of R for the output of the two coders, we subtract the ratio of these two values from 1: Do R = 1− De As Artstein and Poesio (2005) point out, for weighted measures such as R, there is no significance test for agreement, and the actual value is strongly affected by the distance metric that is selected. However, R values can be compared with one another to assess degrees of agreement. The overall R value between the two coders on the full set of 286 sentences processed by both was 0.561, with R values on individual facial displays ranging from a high of 0.661 on nodding to a low of 0.285 on squinting (a very rare motion). To put these values into context, we computed R on the set of 20 sentences processed by the final code</context>
</contexts>
<marker>Artstein, Poesio, 2005</marker>
<rawString>R. Artstein and M. Poesio. 2005. Kappa3 = alpha (or beta). Technical Report CSM-437, University of Essex Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>S Varges</author>
<author>editors</author>
</authors>
<date>2005</date>
<booktitle>Corpus Linguistics 2005 Workshop on Using Corpora for Natural Language Generation. http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.</booktitle>
<marker>Belz, Varges, editors, 2005</marker>
<rawString>A. Belz and S. Varges, editors. 2005. Corpus Linguistics 2005 Workshop on Using Corpora for Natural Language Generation. http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>S Kopp</author>
<author>P Tepper</author>
<author>K Ferriman</author>
<author>K Striegnitz</author>
</authors>
<title>Trading spaces: How humans and humanoids use speech and gesture to give directions.</title>
<date>2007</date>
<editor>In T. Nishida, editor,</editor>
<publisher>Wiley. In press.</publisher>
<contexts>
<context position="1606" citStr="Cassell et al., 2007" startWordPosition="256" endWordPosition="259">n increasing number of systems designed to automatically generate linguistic and multimodal output now make use of corpora to help in decisionmaking (cf. Belz and Varges, 2005). Some implementations use corpora to help select output that is grammatical or fluent; for example, Langkilde and Knight (1998) and White (2006) both used n-gram language models to guide stochastic surface realisers. In other systems, corpora are used to make decisions based on pragmatic factors such as the reading level of the target user (Williams and Reiter, 2005) or the visual features of an object being described (Cassell et al., 2007). The latter type 25 of domain-specific contextual information is not often included in generally-available corpora. For this reason, developers of generation systems that need this type of information often create and make use of application-specific corpora. The easiest method of including the necessary pragmatic information in a corpus is to base the corpus on output generated in situations where the contextual factors are known; this eliminates the need to annotate these factors explicitly. Stone et al. (2004), for example, created a multimodal corpus based on the voice and body language o</context>
</contexts>
<marker>Cassell, Kopp, Tepper, Ferriman, Striegnitz, 2007</marker>
<rawString>J. Cassell, S. Kopp, P. Tepper, K. Ferriman, and K. Striegnitz. 2007. Trading spaces: How humans and humanoids use speech and gesture to give directions. In T. Nishida, editor, Engineering Approaches to Conversational Informatics. Wiley. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cavé</author>
<author>I Guaïtella</author>
<author>R Bertrand</author>
<author>S Santi</author>
<author>F Harlay</author>
<author>R Espesser 1996</author>
</authors>
<title>About the relationship between eyebrow movements and F0 variations.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th International Conference on Spoken Language Processing (ICSLP</booktitle>
<contexts>
<context position="19400" citStr="Cavé et al. (1996)" startWordPosition="3211" endWordPosition="3214">acial displays for different user-model evaluations distribution of facial displays. In the first half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause of two-clause sentences. Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affect (and approach beh</context>
</contexts>
<marker>Cavé, Guaïtella, Bertrand, Santi, Harlay, 1996, 1996</marker>
<rawString>C. Cavé, I. Guaïtella, R. Bertrand, S. Santi, F. Harlay, and R. Espesser.1996. About the relationship between eyebrow movements and F0 variations. In Proceedings of the 4th International Conference on Spoken Language Processing (ICSLP 1996).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Davidson</author>
<author>W Irwin</author>
</authors>
<title>The functional neuroanatomy of emotion and affective style.</title>
<date>1999</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>3</volume>
<issue>1</issue>
<pages>10--1016</pages>
<contexts>
<context position="19783" citStr="Davidson and Irwin, 1999" startWordPosition="3271" endWordPosition="3274">ted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affect (and approach behaviours). Because both perceptual and motor systems are contra-laterally organised, this means that higher levels of right hemisphere activity are associated with attention being oriented towards the left, while higher levels of left hemisphere activity are associated with attention being oriented to the right; this fits with our speaker’s pattern of movements. The annotation sche</context>
</contexts>
<marker>Davidson, Irwin, 1999</marker>
<rawString>R. J. Davidson and W. Irwin. 1999. The functional neuroanatomy of emotion and affective style. Trends in Cognitive Sciences, 3(1):11–21. doi:10.1016/S1364-6613(98) 01265-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
<author>I van der Sluis</author>
<author>A Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of the Fourth International Natural Language Generation Conference,</booktitle>
<pages>130--132</pages>
<location>Sydney,</location>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In Proceedings of the Fourth International Natural Language Generation Conference, pages 130–132. Sydney, Australia. ACL Anthology W06-1420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>About brows: Emotional and conversational signals.</title>
<date>1979</date>
<editor>In M. von Cranach, K. Foppa, W. Lepenies, and D. Ploog, editors, Human</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="19380" citStr="Ekman (1979)" startWordPosition="3209" endWordPosition="3210">aracteristic facial displays for different user-model evaluations distribution of facial displays. In the first half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause of two-clause sentences. Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affe</context>
<context position="21119" citStr="Ekman (1979)" startWordPosition="3483" endWordPosition="3484">uraged to use syntactic constituents wherever possible, but also had the option to select multiple nodes where a display did not correspond with a single constituent in the derivation tree. Earlier versions of the annotation scheme did not support this degree of flexibility, so we used the patterns in the corpus to test whether the modifications to the scheme were useful. In a previous study using the same video recordings but a different, simpler scheme (Foster and Oberlander, 2006), facial displays could only be associated with single leaf nodes (i.e., words); that is, in the terminology of Ekman (1979), all motions were considered to be batons rather than underliners. Based on the data in the current corpus, that restriction was clearly unrealistic: the mean number of nodes spanned by a display in the full corpus was 1.95, with a maximum of 15 and a standard deviation of 2. The results were similar in the sub-corpus produced by the final coder, in which the mean number of nodes spanned by a display was 2.25. The annotation rules for this study did not initially permit displays to be associated with more than nodes in the derivation tree. This capability was added following inter-coder discu</context>
</contexts>
<marker>Ekman, 1979</marker>
<rawString>P. Ekman. 1979. About brows: Emotional and conversational signals. In M. von Cranach, K. Foppa, W. Lepenies, and D. Ploog, editors, Human Ethology: Claims and limits of a new discipline. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W V Friesen</author>
<author>J C Hager</author>
</authors>
<title>Facial Action Coding System. A Human Face,</title>
<date>2002</date>
<location>Salt Lake City.</location>
<contexts>
<context position="9403" citStr="Ekman et al., 2002" startWordPosition="1587" endWordPosition="1590">incide with a single node, it was attached to the set of nodes that did span the necessary words. For example, in the derivation shown in Figure 1, the sequence the family style is associated with a single node, so a motion temporally associated with that sequence would be attached to that node. On the other hand, if there were a motion associated with the tiles are, it would be attached to both the the tiles node and the are node. The following were the features that were considered; for each feature, we note the corresponding Action Unit (AU) from the well-known Facial Action Coding System (Ekman et al., 2002). • Eyebrows: up (AU 1+2) or down (AU 4) • Eye squinting (AU 43) Figure 2: Annotation tool • Head nodding: up (AU 53) or down (AU 54) • Head leaning: left (AU 55) or right (AU 56) • Head turning: left (AU 57) or right (AU 58) This set of displays was chosen based on a combination of three factors: the emphatic facial displays documented in the literature, the capabilities of the target talking head, and the actual displays of the speaker during the recording session. 3.2 Annotation tool The tool for the annotation was a custom-written program that allowed the coder to play back a recorded sent</context>
</contexts>
<marker>Ekman, Friesen, Hager, 2002</marker>
<rawString>P. Ekman, W. V. Friesen, and J. C. Hager. 2002. Facial Action Coding System. A Human Face, Salt Lake City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Flecha-García</author>
</authors>
<title>Eyebrow raising in dialogue: Discourse structure, utterance function, and pitch accents.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Theoretical and Applied Linguistics, University of Edinburgh.</institution>
<contexts>
<context position="19496" citStr="Flecha-García (2006)" startWordPosition="3228" endWordPosition="3229">rst half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause of two-clause sentences. Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affect (and approach behaviours). Because both perceptual and motor systems are contra-laterally organised, this means t</context>
</contexts>
<marker>Flecha-García, 2006</marker>
<rawString>M. L. Flecha-García. 2006. Eyebrow raising in dialogue: Discourse structure, utterance function, and pitch accents. Ph.D. thesis, Department of Theoretical and Applied Linguistics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Comparing rule-based and data-driven selection of facial displays.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Workshop on Embodied Language Processing.</booktitle>
<contexts>
<context position="22809" citStr="Foster, 2007" startWordPosition="3765" endWordPosition="3766">on multiple nodes. 30 6 Generation experiments The primary reason for creating this corpus of facial displays was to use the resulting data to select facial displays for the artificial talking head in the COMIC multimodal dialogue system. Several different strategies have been implemented to use the corpus data for this task, and a number of automated and human evaluations have been carried out comparing the different implementations. As described in the preceding section, the factor with the largest influence on the displays of the recorded speaker was the user-model evaluation. Two studies (Foster, 2007b) were carried out to test the generality of the characteristic positive and negative displays (Figure 4). In the first study, users were asked to identify the intended user-model polarity of a description presented by the talking head based only on the facial displays. The participants were generally able to recognise the characteristic positive and negative facial displays; they also identified the displays intended to be neutral (nodding alone) as positive, and tended to judge videos with no facial displays to be negative. In the second study, users’ subjective preferences were gathered be</context>
<context position="24660" citStr="Foster, 2007" startWordPosition="4046" endWordPosition="4047">pared against each other using both automated and human evaluation methods: the majority strategy scored more highly on the automated cross-validation, while the weighted strategy was strongly preferred by human judges. The judges also preferred resynthesised versions of the original facial displays from the corpus to the output of either of the generation strategies. Two further human evaluation studies compared the weighted data-driven generation strategy from the preceding study to a rule-based strategy that selected the most characteristic displays based only on the user-model evaluation (Foster, 2007a). When users’ subjective judgements were gathered as above, they had a mild preference for the output of the weighted strategy over that of the rulebased strategy. In a second study, videos generated by the weighted strategy significantly decreased participants’ ability to select descriptions that were correctly tailored to a given set of user preferences, while videos generated by the rule-based strategy had no such impact. 7 Conclusions We have described the collection and annotation of an application-specific corpus of conversational facial displays. The designs of both the corpus and the</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>M. E. Foster. 2007a. Comparing rule-based and data-driven selection of facial displays. In Proceedings of the ACL 2007 Workshop on Embodied Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Generating embodied descriptions tailored to user preferences. In submission.</title>
<date>2007</date>
<contexts>
<context position="22809" citStr="Foster, 2007" startWordPosition="3765" endWordPosition="3766">on multiple nodes. 30 6 Generation experiments The primary reason for creating this corpus of facial displays was to use the resulting data to select facial displays for the artificial talking head in the COMIC multimodal dialogue system. Several different strategies have been implemented to use the corpus data for this task, and a number of automated and human evaluations have been carried out comparing the different implementations. As described in the preceding section, the factor with the largest influence on the displays of the recorded speaker was the user-model evaluation. Two studies (Foster, 2007b) were carried out to test the generality of the characteristic positive and negative displays (Figure 4). In the first study, users were asked to identify the intended user-model polarity of a description presented by the talking head based only on the facial displays. The participants were generally able to recognise the characteristic positive and negative facial displays; they also identified the displays intended to be neutral (nodding alone) as positive, and tended to judge videos with no facial displays to be negative. In the second study, users’ subjective preferences were gathered be</context>
<context position="24660" citStr="Foster, 2007" startWordPosition="4046" endWordPosition="4047">pared against each other using both automated and human evaluation methods: the majority strategy scored more highly on the automated cross-validation, while the weighted strategy was strongly preferred by human judges. The judges also preferred resynthesised versions of the original facial displays from the corpus to the output of either of the generation strategies. Two further human evaluation studies compared the weighted data-driven generation strategy from the preceding study to a rule-based strategy that selected the most characteristic displays based only on the user-model evaluation (Foster, 2007a). When users’ subjective judgements were gathered as above, they had a mild preference for the output of the weighted strategy over that of the rulebased strategy. In a second study, videos generated by the weighted strategy significantly decreased participants’ ability to select descriptions that were correctly tailored to a given set of user preferences, while videos generated by the rule-based strategy had no such impact. 7 Conclusions We have described the collection and annotation of an application-specific corpus of conversational facial displays. The designs of both the corpus and the</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>M. E. Foster. 2007b. Generating embodied descriptions tailored to user preferences. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>J Oberlander</author>
</authors>
<title>Data-driven generation of emphatic facial displays.</title>
<date>2006</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>353--360</pages>
<location>Trento,</location>
<contexts>
<context position="20995" citStr="Foster and Oberlander, 2006" startWordPosition="3460" endWordPosition="3463"> annotation scheme described here allowed a display to be associated with any contiguous span of words in the sentence. Annotators were encouraged to use syntactic constituents wherever possible, but also had the option to select multiple nodes where a display did not correspond with a single constituent in the derivation tree. Earlier versions of the annotation scheme did not support this degree of flexibility, so we used the patterns in the corpus to test whether the modifications to the scheme were useful. In a previous study using the same video recordings but a different, simpler scheme (Foster and Oberlander, 2006), facial displays could only be associated with single leaf nodes (i.e., words); that is, in the terminology of Ekman (1979), all motions were considered to be batons rather than underliners. Based on the data in the current corpus, that restriction was clearly unrealistic: the mean number of nodes spanned by a display in the full corpus was 1.95, with a maximum of 15 and a standard deviation of 2. The results were similar in the sub-corpus produced by the final coder, in which the mean number of nodes spanned by a display was 2.25. The annotation rules for this study did not initially permit </context>
</contexts>
<marker>Foster, Oberlander, 2006</marker>
<rawString>M. E. Foster and J. Oberlander. 2006. Data-driven generation of emphatic facial displays. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2006), pages 353–360. Trento, Italy. ACL Anthology E06-1045.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>J Oberlander</author>
</authors>
<title>Corpus-based generation of conversational facial displays. In submission.</title>
<date>2007</date>
<contexts>
<context position="23709" citStr="Foster and Oberlander, 2007" startWordPosition="3902" endWordPosition="3905">The participants were generally able to recognise the characteristic positive and negative facial displays; they also identified the displays intended to be neutral (nodding alone) as positive, and tended to judge videos with no facial displays to be negative. In the second study, users’ subjective preferences were gathered between videos in which the user-model evaluation expressed in speech was either consistent or inconsistent with the facial displays. In this study, the participants generally preferred the videos that showed consistent content on the two output channels. In another study (Foster and Oberlander, 2007), two different data-driven strategies were implemented that used the corpus data to select facial displays to accompany speech. One strategy always selected the highest-probability option in all contexts, while the other made a stochastic choice among all of the options weighted by the corpus probabilities. These two strategies were compared against each other using both automated and human evaluation methods: the majority strategy scored more highly on the automated cross-validation, while the weighted strategy was strongly preferred by human judges. The judges also preferred resynthesised v</context>
</contexts>
<marker>Foster, Oberlander, 2007</marker>
<rawString>M. E. Foster and J. Oberlander. 2007. Corpus-based generation of conversational facial displays. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>M White</author>
<author>A Setzer</author>
<author>R Catizone</author>
</authors>
<title>Multimodal generation in the COMIC dialogue system.</title>
<date>2005</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>06--1403</pages>
<contexts>
<context position="2891" citStr="Foster et al., 2005" startWordPosition="460" endWordPosition="463"> generation system: an animated instructor character for a snowboarding video game. The contextual information in the corpus scripts included the move that the player attempted in the game and the result of that attempt. Similarly, van Deemter et al. (2006) created a corpus of multimodal referring expressions produced in specific pragmatic contexts and used it to compare several referring-expression generation algorithms to human performance. In this work, the task is to select facial displays for an animated talking head to use while presenting output in the COMIC multimodal dialogue system (Foster et al., 2005), which generates spoken descriptions and comparisons of bathroom-tile options. The output of the COMIC text planner includes a range of information in addition to the text: the syntactic derivation tree, the user’s evaluation of the object being described, the information status (new or old, contrastive) of each fact described, and the predicted speech-synthesiser prosody. All of this contextual information can be used to help select Proceedings of the Linguistic Annotation Workshop, pages 25–32, Prague, June 2007. c�2007 Association for Computational Linguistics appropriate facial displays t</context>
</contexts>
<marker>Foster, White, Setzer, Catizone, 2005</marker>
<rawString>M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Multimodal generation in the COMIC dialogue system. In Proceedings of the ACL 2005 Demo Session. ACL Anthology W06-1403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Graf</author>
<author>E Cosatto</author>
<author>V Strom</author>
<author>F Huang</author>
</authors>
<title>Visual prosody: Facial movements accompanying speech.</title>
<date>2002</date>
<booktitle>In Proceedings of the 5th IEEE International Conference on Automatic Face and Gesture Recognition (FG</booktitle>
<pages>397--401</pages>
<contexts>
<context position="19420" citStr="Graf et al. (2002)" startWordPosition="3215" endWordPosition="3218">ifferent user-model evaluations distribution of facial displays. In the first half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause of two-clause sentences. Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affect (and approach behaviours). Because bo</context>
</contexts>
<marker>Graf, Cosatto, Strom, Huang, 2002</marker>
<rawString>H. Graf, E. Cosatto, V. Strom, and F. Huang. 2002. Visual prosody: Facial movements accompanying speech. In Proceedings of the 5th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2002), pages 397– 401. doi:10.1109/AFGR.2002.1004186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Keating</author>
<author>M Baroni</author>
<author>S Mattys</author>
<author>R Scarborough</author>
<author>A Alwan</author>
</authors>
<title>Optical phonetics and visual perception of lexical and phrasal stress in English.</title>
<date>2003</date>
<booktitle>In Proceedings of the 15th International Congress of Phonetic Sciences (ICPhS),</booktitle>
<pages>2071--2074</pages>
<contexts>
<context position="19443" citStr="Keating et al. (2003)" startWordPosition="3219" endWordPosition="3222">evaluations distribution of facial displays. In the first half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause of two-clause sentences. Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affect (and approach behaviours). Because both perceptual and motor</context>
</contexts>
<marker>Keating, Baroni, Mattys, Scarborough, Alwan, 2003</marker>
<rawString>P. Keating, M. Baroni, S. Mattys, R. Scarborough, and A. Alwan. 2003. Optical phonetics and visual perception of lexical and phrasal stress in English. In Proceedings of the 15th International Congress of Phonetic Sciences (ICPhS), pages 2071–2074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Gesture Generation by Imitation - From Human Behavior to Computer Character Animation.</title>
<date>2004</date>
<publisher>Dissertation.com.</publisher>
<contexts>
<context position="8004" citStr="Kipp, 2004" startWordPosition="1342" endWordPosition="1343">(with accented words highlighted) as well as the intended pragmatic context. Each sentence was displayed in a large font on a laptop computer directly in front of the speaker, with the camera positioned directly above the laptop to ensure that the speaker was looking towards the camera at all times. The speaker was instructed to read each sentence out loud as expressively as possible into the camera. 3 Annotation Once all of the sentences in the script had been recorded as described in the preceding section, the next step was to annotate the facial displays that occurred. We first used Anvil (Kipp, 2004) to split the video into individual clips corresponding to each sentence. This section describes how the facial displays in each of the clips were then annotated. 3.1 Annotation scheme We annotated the speaker’s facial displays by linking each to the span of nodes in the OpenCCG derivation tree with which it was temporally related. Making cross-modal links at this level made it possible to use the annotated information directly in the outputgeneration process for the experiments described in Section 6. A display was associated with the full span of words that it coincided with temporally, as f</context>
</contexts>
<marker>Kipp, 2004</marker>
<rawString>M. Kipp. 2004. Gesture Generation by Imitation - From Human Behavior to Computer Character Animation. Dissertation.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Swerts</author>
</authors>
<title>More about brows: A crosslinguistic study via analysis-by-synthesis.</title>
<date>2004</date>
<pages>191--216</pages>
<editor>In C. Pelachaud and Z. Ruttkay, editors, From</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="19470" citStr="Krahmer and Swerts (2004)" startWordPosition="3223" endWordPosition="3226">n of facial displays. In the first half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause of two-clause sentences. Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. Several of these factors agree with previous findings on conversational body language. The increased frequency of nodding and brow raising on accented words agrees with many previous studies: Ekman (1979), Cavé et al. (1996), Graf et al. (2002), Keating et al. (2003), Krahmer and Swerts (2004), and Flecha-García (2006) all noted similar displays on prosodically accented parts of the sentence. The speaker’s tendency to move right on positive descriptions and left on negative descriptions is also consistent with other findings. According to the work of Davidson and colleagues (Davidson and Irwin, 1999), emotion and affect processing are asymmetrically organised in the human brain. The right hemisphere is associated with negative affect (and withdrawal behaviours), and the left with positive affect (and approach behaviours). Because both perceptual and motor systems are contra-lateral</context>
</contexts>
<marker>Krahmer, Swerts, 2004</marker>
<rawString>E. Krahmer and M. Swerts. 2004. More about brows: A crosslinguistic study via analysis-by-synthesis. In C. Pelachaud and Z. Ruttkay, editors, From Brows to Trust: Evaluating Embodied Conversational Agents, pages 191–216. Kluwer. doi:10.1007/1-4020-2730-3_7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>The practical value of ngrams in generation.</title>
<date>1998</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of the 9th International Natural Language Generation Workshop (INLG</booktitle>
<pages>98--1426</pages>
<contexts>
<context position="1289" citStr="Langkilde and Knight (1998)" startWordPosition="201" endWordPosition="204">used by the the speaker. The behaviours of the speaker show several contextual patterns, many of which agree with previous findings on conversational facial displays. The corpus data has been used in several studies exploring different strategies for selecting facial displays for a synthetic talking head. 1 Introduction An increasing number of systems designed to automatically generate linguistic and multimodal output now make use of corpora to help in decisionmaking (cf. Belz and Varges, 2005). Some implementations use corpora to help select output that is grammatical or fluent; for example, Langkilde and Knight (1998) and White (2006) both used n-gram language models to guide stochastic surface realisers. In other systems, corpora are used to make decisions based on pragmatic factors such as the reading level of the target user (Williams and Reiter, 2005) or the visual features of an object being described (Cassell et al., 2007). The latter type 25 of domain-specific contextual information is not often included in generally-available corpora. For this reason, developers of generation systems that need this type of information often create and make use of application-specific corpora. The easiest method of </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. The practical value of ngrams in generation. In Proceedings of the 9th International Natural Language Generation Workshop (INLG 1998). ACL Anthology W98-1426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Passonneau</author>
</authors>
<title>Computing reliability for coreference annotation. In</title>
<date>2004</date>
<booktitle>Proceedings, Fourth International Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<volume>4</volume>
<pages>1503--1506</pages>
<location>Lisbon.</location>
<contexts>
<context position="15120" citStr="Passonneau (2004)" startWordPosition="2489" endWordPosition="2491">s to multiple coders, and that uses a separate probability distribution for each coder. Weighted coefficients like R permit degrees of agreement to be measured, so that partial agreement is penalised less severely than total disagreement. Like other weighted coefficients, R is based on the ratio between the observed and expected disagreement on the corpus. To use this coefficient, it is necessary to define a measure that computes the distance between two proposed annotations. In this case, to compute the observed disagreement Do(S) on a sentence S, we use a measure similar to that proposed by Passonneau (2004) for measuring agreement on set-valued annotations. For each display proposed by each coder on the sentence, we search for a corresponding display proposed by the other coder—one with the same value (e.g., a brow raise) and covering a similar span of nodes. If both proposed exactly the same display, that indicates no disagreement (0); if one display covers a strict subset of the nodes covered by the other, that indicates minor disagreement (13); if the nodes covered by the two proposals overlap, that is a more major disagreement (23); and if no corresponding display can be found from the secon</context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>R. J. Passonneau. 2004. Computing reliability for coreference annotation. In Proceedings, Fourth International Conference on Language Resources and Evaluation (LREC 2004), volume 4, pages 1503–1506. Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Information structure and the syntaxphonology interface.</title>
<date>2000</date>
<journal>Linguistic Inquiry,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>10--1162</pages>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. Information structure and the syntaxphonology interface. Linguistic Inquiry, 31(4):649–689. doi:10.1162/002438900554505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>D DeCarlo</author>
<author>I Oh</author>
<author>C Rodriguez</author>
<author>A Lees</author>
<author>A Stere</author>
<author>C Bregler</author>
</authors>
<title>Speaking with hands: Creating animated conversational characters from recordings of human performance.</title>
<date>2004</date>
<journal>ACM Transactions on Graphics (TOG),</journal>
<volume>23</volume>
<issue>3</issue>
<pages>10--1145</pages>
<contexts>
<context position="2125" citStr="Stone et al. (2004)" startWordPosition="336" endWordPosition="339">illiams and Reiter, 2005) or the visual features of an object being described (Cassell et al., 2007). The latter type 25 of domain-specific contextual information is not often included in generally-available corpora. For this reason, developers of generation systems that need this type of information often create and make use of application-specific corpora. The easiest method of including the necessary pragmatic information in a corpus is to base the corpus on output generated in situations where the contextual factors are known; this eliminates the need to annotate these factors explicitly. Stone et al. (2004), for example, created a multimodal corpus based on the voice and body language of an actor performing scripted output in the domain of the target generation system: an animated instructor character for a snowboarding video game. The contextual information in the corpus scripts included the move that the player attempted in the game and the result of that attempt. Similarly, van Deemter et al. (2006) created a corpus of multimodal referring expressions produced in specific pragmatic contexts and used it to compare several referring-expression generation algorithms to human performance. In this</context>
<context position="25664" citStr="Stone et al. (2004)" startWordPosition="4206" endWordPosition="4209">y the rule-based strategy had no such impact. 7 Conclusions We have described the collection and annotation of an application-specific corpus of conversational facial displays. The designs of both the corpus and the annotation scheme were driven by the needs of a specific generation system, which makes use of a range of pragmatic information while creating output. To use this information to make corpus-based decisions, it is necessary that the full context of every utterance and facial display in the corpus be available. Rather than adding this information to an existing corpus, we chose—like Stone et al. (2004) and van Deemter et al. (2006), for example—to create a corpus based on known contexts so that the full information for every sentence was known before the fact. The final annotation scheme required each facial display to be linked to the set of nodes in the syntactic derivation tree of the sentence that exactly covered the words temporally associated with the display. Two coders separately processed the sentences in the corpus; on the sentences processed by both coders (about 65% of the corpus), the agreement as measured by R was 0.561. A number of contextual factors had an influence on the d</context>
</contexts>
<marker>Stone, DeCarlo, Oh, Rodriguez, Lees, Stere, Bregler, 2004</marker>
<rawString>M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere, and C. Bregler. 2004. Speaking with hands: Creating animated conversational characters from recordings of human performance. ACM Transactions on Graphics (TOG), 23(3):506–513. doi:10.1145/1015706.1015753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Efficient realization of coordinate structures in Combinatory Categorial Grammar.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>11168--006</pages>
<contexts>
<context position="1306" citStr="White (2006)" startWordPosition="206" endWordPosition="207">aviours of the speaker show several contextual patterns, many of which agree with previous findings on conversational facial displays. The corpus data has been used in several studies exploring different strategies for selecting facial displays for a synthetic talking head. 1 Introduction An increasing number of systems designed to automatically generate linguistic and multimodal output now make use of corpora to help in decisionmaking (cf. Belz and Varges, 2005). Some implementations use corpora to help select output that is grammatical or fluent; for example, Langkilde and Knight (1998) and White (2006) both used n-gram language models to guide stochastic surface realisers. In other systems, corpora are used to make decisions based on pragmatic factors such as the reading level of the target user (Williams and Reiter, 2005) or the visual features of an object being described (Cassell et al., 2007). The latter type 25 of domain-specific contextual information is not often included in generally-available corpora. For this reason, developers of generation systems that need this type of information often create and make use of application-specific corpora. The easiest method of including the nec</context>
<context position="5006" citStr="White, 2006" startWordPosition="808" endWordPosition="809"> that, in Section 6, we describe several experiments in which different methods of using the data in this corpus to select facial displays for a synthetic head have been compared. Finally, in Section 7, we summarise the contributions of this paper and draw some conclusions about the usefulness of this corpus for its intended task. 2 Recording For this corpus, we recorded a single speaker reading a set of 444 scripted sentences in the domain of the COMIC multimodal dialogue system. The sentences were generated by the full COMIC outputgeneration process, which uses the OpenCCG surface realiser (White, 2006) to create texts including prosodic specifications for the speech synthesiser and incorporates information from the dialogue history and a model of the user’s likes and dislikes. Every node in the OpenCCG derivation tree for each sentence in the script was initially annotated with all of the available syntactic and pragmatic information from the output planner, including the following features: • The user-model evaluation of the object being described (positive or negative); • Whether the fact being presented was previously mentioned in the discourse (as I said before, ... ) or is new informat</context>
</contexts>
<marker>White, 2006</marker>
<rawString>M. White. 2006. Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation, 4(1):39–75. doi:10.1007/ s11168-006-9010-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Deriving content selection rules from a corpus of non-naturally occurring documents for a novel NLG application.</title>
<date>2005</date>
<booktitle>In Belz and Varges</booktitle>
<contexts>
<context position="1531" citStr="Williams and Reiter, 2005" startWordPosition="241" endWordPosition="245">ies for selecting facial displays for a synthetic talking head. 1 Introduction An increasing number of systems designed to automatically generate linguistic and multimodal output now make use of corpora to help in decisionmaking (cf. Belz and Varges, 2005). Some implementations use corpora to help select output that is grammatical or fluent; for example, Langkilde and Knight (1998) and White (2006) both used n-gram language models to guide stochastic surface realisers. In other systems, corpora are used to make decisions based on pragmatic factors such as the reading level of the target user (Williams and Reiter, 2005) or the visual features of an object being described (Cassell et al., 2007). The latter type 25 of domain-specific contextual information is not often included in generally-available corpora. For this reason, developers of generation systems that need this type of information often create and make use of application-specific corpora. The easiest method of including the necessary pragmatic information in a corpus is to base the corpus on output generated in situations where the contextual factors are known; this eliminates the need to annotate these factors explicitly. Stone et al. (2004), for </context>
</contexts>
<marker>Williams, Reiter, 2005</marker>
<rawString>S. Williams and E. Reiter. 2005. Deriving content selection rules from a corpus of non-naturally occurring documents for a novel NLG application. In Belz and Varges (2005).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>