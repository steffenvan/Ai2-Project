<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.982022">
LoLo: A System based on Terminology for Multilingual Extraction
</title>
<author confidence="0.992753">
Yousif Almas
</author>
<affiliation confidence="0.9899085">
Department of Computing
University of Surrey
</affiliation>
<address confidence="0.979314">
Guildford, Surrey, GU2 7XH, UK
</address>
<email confidence="0.999415">
y.almas@surrey.ac.uk
</email>
<author confidence="0.960006">
Khurshid Ahmad
</author>
<affiliation confidence="0.887865">
Department of Computer Science
Trinity College,
</affiliation>
<sectionHeader confidence="0.315255" genericHeader="abstract">
Dublin-2. IRELAND
</sectionHeader>
<email confidence="0.988034">
kahmad@cs.tcd.ie
</email>
<sectionHeader confidence="0.993598" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999802411764706">
An unsupervised learning method, based
on corpus linguistics and special lan-
guage terminology, is described that can
extract time-varying information from
text streams. The method is shown to be
,language-independent&apos; in that its use
leads to sets of regular-expressions that
can be used to extract the information in
typologically distinct languages like Eng-
lish and Arabic. The method uses the in-
formation related to the distribution of N-
grams, for automatically extracting
,meaning bearing&apos; patterns of usage in a
training corpus. The analysis of an Eng-
lish news wire corpus (1,720,142 tokens)
and Arabic news wire corpus (1,720,154
tokens) show encouraging results.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979916666667">
One of the recent trends in (adaptive) IE has
been motivated by the empirical argument that
annotated corpora, either annotated automatically
or annotated manually, can provide sufficient
information for creating the knowledge base of
an IE system (McLernon and Kushmerick,
2006). Another equally important trend is to use
manually selected seed patterns to initiate learn-
ing: In turn, active-training methods use seed
patterns to learn new related patterns from un-
annotated corpora. Many of the adaptive IE sys-
tems rely on the existing part-of-speech (POS)
taggers (Debnath and Giles, 2005) and/or syntac-
tic parsers (Stevenson and Greenwood, 2005) for
analysing and annotating text corpora. The use of
corpora in IE, especially adaptive IE, should, in
principle, alleviate the need for manually creat-
ing the rules for information extraction.
The successful use of POS/syntactic taggers is
dependent on the availability of the knowledge
of (natural) language used by the authors of
documents in a given corpus. There is a wealth
of POS taggers and parsers available for English
language, as it has been the most widely used
language in computational linguistics. However,
this is not the case for strategically important
languages like Arabic and Chinese; to start with,
in Chinese one does not have the luxury of sepa-
rating word-tokens by a white space and in Ara-
bic complex rules are required to identify mor-
phemes compared to English. The development
of segmentation programs in these languages has
certainly helped (Gao et al., 2005; Habash and
Rambow, 2005). More work is needed in under-
standing these languages such that the knowl-
edge thus derived can be used to power taggers
and parsers.
Typically, IE systems are used to analyse
news wire corpora, telephone conversations, and
more recently in bio-informatics. The first two
systems deal with language of everyday commu-
nications =the general language- whereas bio-
informatics deals with a specialist domain and
has its own ,special language&apos;. English special
languages, for example languages of law, com-
merce, finance, science &amp; technology, each have
a limited vocabulary and idiosyncratic syntactic
structures when compared with English used in
an everyday context. The same is true of Ger-
man, French, Russian, Chinese, Arabic or Hindi.
It appears that few works, if any, take advantage
of the properties of special language to build IE
systems.
Our objective is to use methods and tech-
niques of IE in the automatic analysis of special-
ist news that streams in such a way that informa-
tion extracted at an earlier period of time may be
contradicted or reinforced by information ex-
tracted at a later time. The impact of news on
financial and commodity markets is of consider-
</bodyText>
<page confidence="0.969605">
56
</page>
<note confidence="0.6926635">
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 56–65,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999874163636364">
able import and is often called sentiment analy-
sis. The prefix ,sentiment&apos; is used to distinguish
this kind of analysis from the more quantitative
analysis of assets (called fundamental analysis)
and that of price movements (called technical
analysis). There is a great deal of discussion in
financial economics, econometrics, and in the
newly emergent discipline of investor psychol-
ogy about the impact of ,good&apos; and , bad&apos; news
on the behaviour of both investors and brokers.
Three Nobel Prizes have been awarded on the
impact of market (trader and investor) sentiment
on the value of shares, currencies, derivatives
and other financial instruments (Shiller, 2000).
Financial news, in addition to e-mails and blogs,
has contributed to the catastrophic failures of
major trading institutions (Mackenzie, 2000;
Hardie &amp; Mackenzie, 2005).
One of the key proponents of news impact
analysis is the Economics Nobel Laureate Robert
Engle who has written about asymmetry of in-
formation in a market = the brokers have more
knowledge than any given individual, rumours
have different impact on different actors in the
market. Engle&apos;s statistical analysis suggests that
the , bad&apos; news has longer lasting effect than
,good&apos; news (Engle, 1993). Usually, sentiment
analysis is carried out using news proxies which
include dates/times and the names of agencies
releasing key items of financial data (Anderson
et al., 2002) or data like the age of a firm, its
number of initial public offerings, return on in-
vestment, etc. These proxies are then regressed
with share, currency or commodity prices. News
impact analysis is moving into its next phase
where the text of news is analysed albeit to a lim-
ited extent (Cutler et al., 1989; Chan, 2003). The
analysis sometimes looks at the frequency distri-
bution of pre-specified keywords =directional
metaphors like rose/fall, up/down, health meta-
phors like anaemic/healthy and animal meta-
phors like bullish/bearish. A system is trained
to correlate and to learn the changes in distribu-
tion of the prescribed metaphorical keywords,
together with names of organisations, to the
changes in the value of financial instruments
(Seo et al., 2002; Omrane et al., 2005; Koppel
and Shtrimberg 2004).
We are attempting to create a language-
informed framework for news impact analysis
using techniques of corpus linguistics and special
language analysis. The purpose is to automati-
cally extract patterns from a corpus of domain
specific texts without prescribing the metaphori-
cal keywords and organisation names. This, we
believe, can be achieved by looking at the lexical
signature of a specialist domain and extracting
collocational patterns of the individual items of
the lexical signature. The lexical signature in-
cludes key vocabulary items of the domain and
names of people, places and things in the do-
main. There are instances in the part-of-speech
tagging literature (Brill, 1993) and in IE where a
corpus is used and words within a grammatical
category help to extract rules and patterns com-
prising essential information about a domain or
topic (Wilks, 1998; Yangarber, 2003). Brill,
Wilks and Yangarber induce grammars of a uni-
versal kind: we focus on inducing a local gram-
mar that deals with the patterning of the items in
the signature. Note that in all these cases of
grammar induction the intuition of the grammar
builder plays a critical part whether it be in the
choice of syntactic transformation rules (Brill
1993), or in choosing sense taggers and implic-
itly semantic rules (Wilks, 1998; Ciravegna and
Wilks, 2003), or in choosing user supplied seed
patterns (Yangarber, 2003). Most of the work in
grammar induction is focussed on English or ty-
pologically similar languages. We have deliber-
ately chosen typologically different languages
(English and Arabic) to evaluate the extent to
which our method of ,grammar induction&apos; is lan-
guage independent.
We describe a method for building domain
specific IE systems: the patterns used to extract
domain specific information are the N-gram col-
location patterns of domain specific terms. The
patterns are extracted from un-annotated domain-
specific text corpora. We show how one can ana-
lyse the N-gram patterns and render them as
regular expressions.
The thesaurus used to identify domain specific
words is itself constructed automatically from a
(training) special-language corpus. The fre-
quency distribution of domain specific terms in a
special language corpus shows characteristic dif-
ferences from the distribution of the same terms
in a general language corpus. There is little or
no difference in the distribution of the so-called
grammatical or closed class terms in a special
and a general language corpus.
Furthermore, amongst the domain specific
terms, a few tend to dominate the frequency dis-
tribution = the so-called lexical signature of a
domain. These signature terms are used as nu-
cleates for compound terms in a domain. The
occurrence of the signature terms, either on their
own or in a compound or a phrase, is equally
idiosyncratic in that these dominant single or
</bodyText>
<page confidence="0.997318">
57
</page>
<bodyText confidence="0.999972877551021">
compound terms co-occur more frequently with
one set of words than with others. The behaviour
of signature terms appears to be governed by a
grammar that is local to the specialism and is not
elsewhere in the general language (Harris, 1991);
local grammar is used in general language for
telling times and dates in metaphorical expres-
sions (Gross, 1997), and in the lexicography for
describing the language of definitions of lemmas
in a lexicon (Barnbrook and Sinclair, 1996;
Barnbrook, 2002). The local grammar approach,
rooted in the lexical signature of a given domain
can be used to extract ,sentiment&apos; bearing sen-
tences in financial markets (Ahmad el al., 2006)
or in the description of work in a scientific labo-
ratory (Ahmad &amp; Al-Sayed, 2005).
We introduce a system that can help in build-
ing domain specific IE systems in English and
languages that are typologically distinct from
English, specifically Arabic. The development
of LoLo was inspired by Engle&apos;s pioneering
work in econometrics where news impact analy-
sis is regarded as critical to the analysis of mar-
ket movement: however much of the work in
financial economics relates to the correlation of
the timings of news announcements rather than
the content of the news stream (Ahmad et al.,
2006).
LoLo can manage a corpus and extract key
terms. Given the keyword list, the system then
identifies collocates and selects significant collo-
cates on well defined statistical criterion
(Smadja, 1994). Finally, local grammar rules are
identified and an IE system is created.
LoLo has been used to build a local grammar
to extract ,sentiment&apos; or key (changing) market
events in English and in Arabic from unseen
texts. The system can help visualise the distribu-
tion of extracted patterns synchronised with the
movement of financial markets.
IE systems need to be adaptive, as the special-
isms in particular and the world in general is
changing rapidly and this change is usually re-
flected in language use. There is an equally im-
portant need to build cross language IE systems
as information may be in different languages.
The lexically-motivated approach we describe in
this paper responds to the need for an adaptive,
cross domain and cross language IE systems.
</bodyText>
<subsectionHeader confidence="0.835831">
Method
</subsectionHeader>
<bodyText confidence="0.999867777777778">
For the extraction of local grammar from a
corpus of special language texts it is important to
focus on the keywords. The patterns in which
the keywords are embedded are assumed to com-
prise the principal elements of a subject specific
local grammar.
The manner in which we derive the local
grammar is shown in the algorithm below (Fig-
ure 1).
</bodyText>
<listItem confidence="0.731619444444445">
ALGORITHM: DISCOVER LOCAL GRAMMAR
1. SELECT a special language corpus ( L, comprising
!,,,ial words and vocabulary &amp;quot; p�dal)-
i. USE a fre$uency dist of single words from a corpus of
texts used in day-to-day communications ( % comprising
!go,—1 wards and vocabulary &amp;quot;g—ad &amp; for example, the
&apos;ritish !ational (����� ��� ��� )������ ��������*
+~~~~~~~*,-~~~~#
~~~.#
</listItem>
<bodyText confidence="0.897093285714286">
~~~/#00~~&amp;quot;~~~~~~~1
ii. CREATE a fre$uency ordered list of words in L texts is
computed
+�pcial*,-�(Wl#, f(W.#, f%#0001
iii. COMPUTE the differences in the distribution of the
same words in the two different corpora is computed us-
ing the in % and L*
</bodyText>
<figure confidence="0.26899125">
2eirdness (wd, f(wdspeciauf(wdgeneral3
!g._1!�p_al
iv. CALCULATE 4-score for the +�p_al
4f(WJ,U(WJ fv5p-aJ/6 P-w
</figure>
<bodyText confidence="0.851024869565217">
.. CREATE 7)8 a set of !9�� 9eywords ordered accord-
ing to the magnitude of the two 4-scores
7)8*,-9eyl, 9ey.� 9ey/009ey!9��#
such that 4oo,) &amp; 4(weridnesskeyi#: 1
i. ExTRACT collocates of each 7ey in L over a window
of M word neighbourhood.
ii. COMPUTE the strength of collocation using three meas-
ures due to mad;a (1&lt;&lt;=#*
&gt;��core, 9, and 4-score
iii. ExTRACT sentences in the corpus that comprise highly
collocating 9ey-words ((&gt;,9a9d:,(1?,1,1## 4
iv. FORM (orpus L@
a. +or each entencei in L@*
b. COMPUTE the fre$uency of every word in entence;.
c. REPLACE wards with fre$uency less than a threshold
value (&amp;_Iwd by a place mar9er AB
d. FOR more than one contiguous place mar9er, use3
/. GENERATE trigrams in L@B note fre$uency of each
trigram together with its position in the sentences*
i. FIND all the longest possible contiguous trigrams
across all sentences in L@ and note their fre$uency
ii. ORDER the (contiguous# trigrams according to ����
$����� �� ����������
</bodyText>
<listItem confidence="0.7290235">
iii. (CONTIGUOUS) TRIGRAMS with fre$uency above a
threshold form THE LOCAL GRAMMAR
</listItem>
<figureCaption confidence="0.929976">
Figure 1. Algorithm for the acquisition of local-
grammar patterns.
</figureCaption>
<bodyText confidence="0.9999203125">
Briefly, given a specialist corpus (SL), key-
words are identified, and collocates of the key-
words are extracted. Sentences containing key
collocates are then used to construct a sub-corpus
(SL&apos;). The sub-corpus SL&apos; is then analyzed and
trigrams above a frequency threshold in the sub-
corpus are extracted; the position of the trigrams
in each of the sentences is also noted. The sub-
corpus is searched again for contiguous trigrams
across the sentences: The sentences are ana-
lyzed for the existence of the trigrams in the cor-
rect position = if a trigram that, for example, is
noted for its frequency as a sentence initial posi-
tion, is found to co-occur with another frequent
trigram that exists at the next position, then the
two trigrams will be deemed to form a pattern.
</bodyText>
<page confidence="0.988931">
58
</page>
<bodyText confidence="0.99991125">
This process is continued until all the trigrams in
the sentence are matched with the significant
trigrams.
The local grammar then comprises significant
contiguous trigrams that are found. These do-
main specific patterns, extracted from the spe-
cialist corpus SL&apos; (and its constituent sub-corpus)
are then used to extract similar patterns and in-
formation from a test corpus to validate the pat-
terns thus found in the training corpus. Following
is a demonstration of how the algorithm works
using English and Arabic texts.
</bodyText>
<subsectionHeader confidence="0.999364">
2.1 Extracting Patterns in English
</subsectionHeader>
<bodyText confidence="0.9999730625">
We present an analysis of a corpus of financial
news wire texts: 1204 news report produced by
Reuters UK Financial News comprising 431,850
tokens. One of the frequent words in the corpus
is percent= 3622 occurrences, a relative fre-
quency of 0.0084%. When the frequency of this
keyword is looked up in the British National
Corpus (100 million words), it was found that
percent is 287 times more frequent in the finan-
cial corpus than in the British National Corpus =
this ratio is sometimes termed weirdness (of spe-
cial language); the weirdness of grammatical
words the and to is unity as these tokens are dis-
tributed with the same (relative) frequency in
Reuters Financial and the BNC. The z-score
computed using the frequency of the token in the
Reuters Financial is 12.64: the distribution of
percent is 12 standard deviations above the mean
of all words in the financial corpus. (The z-score
computed for weirdness is positive as well). The
heuristic here is this: a token is a candidate key-
word if both its z-scores are greater than a small
positive number. So percent -most frequent to-
ken with frequency and weirdness z-score over
zero- was accepted as a keyword.
The collocates of the keyword percent were
then extracted by using mutual information sta-
tistics presented by Smadja (1994). A collocate
in this terminology can be anywhere in the vicin-
ity of +/- N-words. The frequency at each
neighbourhood is calculated and then used to
compute the ,peaks&apos; in the histogram formed by
the neighbourhood frequencies and the strength
of the collocation calculated on a similar basis.
The keyword generally collocates with certain
words that have frequencies higher than itself =
the upward collocates- and collocates with cer-
tain words that have lesser frequency = the down-
wards collocates (These terms were coined by
John Sinclair). Upwards collocates are usually
grammatical words and downwards collocates
are lexical words = nouns, adjectives- and hence
the downwards collocates are treated as candi-
date compound words. There were 46 collocates
of percent in our corpus = 34 downwards collo-
cates and 12 upwards collocates. A selection of
5 downwards and upwards are shown in Table 1
and 2 respectively.
</bodyText>
<table confidence="0.998643833333333">
Collocate Frequency U-score k-score
shares 1150 1047 3.01
rose 514 2961 2.43
year 2046 396 2.40
profit 1106 263 1.65
down 486 996 1.40
</table>
<tableCaption confidence="0.9708875">
Table 1. Downward collocates of percent in a
corpus of 431,850 words.
</tableCaption>
<table confidence="0.999864166666667">
Collocate Frequency U-score k-score
the 23157 6744 14.40
to 12190 7230 10.29
in 9768 4941 8.49
a 10657 3024 8.44
of 10123 3957 8.24
</table>
<tableCaption confidence="0.92678">
Table 2. Upward collocates of percent in a cor-
pus of 431,850 words.
</tableCaption>
<bodyText confidence="0.999345428571428">
The financial texts comprise a large number of
numerals (integers and decimals) and these we
will denote as &lt;no&gt;. The numerals collocate
strongly with percent for obvious reasons. The
collocates are then used to extract trigrams com-
prising the collocates that occur at particular po-
sitions in the various sentences of our corpus:
</bodyText>
<table confidence="0.558956333333333">
Token A Token B Token C Freq Position
&lt;no&gt; percent and 16 1
rose &lt;no&gt; percent 18 1
&lt;no&gt; percent after 23 2
&lt;no&gt; percent of 47 2
&lt;no&gt; percent rise 11 2
</table>
<tableCaption confidence="0.967552">
Table 3. Trigrams of percent.
</tableCaption>
<bodyText confidence="0.999835">
There are many other frequent patterns where
the frequency of individual tokens is quite low
but at least one member of the trigram has higher
frequency: such low frequency tokens are omit-
ted and marked by the (#) symbol. All the tri-
grams containing such tokens with at least two
others are used to extract other significant tri-
grams. Sometimes more than one low frequency
tokens precede or succeed high frequency tokens
and they are denoted by the symbol (*) as shown
in Table 4. The search for contiguous trigrams
leads to larger and more complex patterns, Table
5 provides some examples.
</bodyText>
<page confidence="0.989357">
59
</page>
<table confidence="0.999535666666667">
Token A Token B Token C Freq Position
rose &lt;no&gt; percent 18 1
# &lt;no&gt; percent 29 2
# shares were 10 2
* &lt;no&gt; percent 57 2
&lt;no&gt; percent # 24 2
</table>
<tableCaption confidence="0.653239">
Table 4. Trigrams of percent with omitted low
frequency words (denoted as * for multiple to-
kens and # for a single token).
weirdness of 76 compared against our Modern
Standard Arabic Corpus (MSAC).
</tableCaption>
<bodyText confidence="0.633955142857143">
There were 31 collocates of percent; 7 up-
wards and 23 downwards. The downwards collo-
cates of percent appear to collocate with names
of instruments i.e. shares and indices (Table 6).
The upwards collocate are with the so-called
closed class words as in English like in, on and
that (Table 7).
</bodyText>
<table confidence="0.879947857142857">
Local Grammar Patterns Freq
&lt;s&gt; the * &lt;no&gt; percent 28
&lt;s&gt; * rose &lt;no&gt; percent 26
&lt;s&gt; # shares # &lt;no&gt; percent 22
&lt;s&gt; * fell &lt;no&gt; percent 20
&lt;s&gt; * &lt;no&gt; percent 18
&lt;s&gt; # shares were up &lt;no&gt; percent at 17
</table>
<tableCaption confidence="0.991877">
Table 5. Some of top patterns of percent (&lt;s&gt;
identifies a sentence boundary).
</tableCaption>
<sectionHeader confidence="0.676089" genericHeader="method">
2. 2 Extracting Patterns in Arabic
</sectionHeader>
<bodyText confidence="0.999753451612903">
Arabic is written from right to left and its writ-
ing system does not employ capitalization. The
language is highly inflected compared to Eng-
lish; words are generated using a root-and-
pattern morphology. Prefixes and suffixes can be
attached to the morphological patterns for gram-
matical purposes. For example, the grammatical
conjunction &amp;quot;and&amp;quot; in Arabic is attached to the
beginning of the following word. Words are also
sensitive to the gender and number they refer to
and their lexical structure change accordingly.
As a result, more word types can be found in
Arabic corpora compared to English of same size
and type. Short vowels which are represented as
marks in Arabic are also omitted from usual
Arabic texts resulting in some words having
same lexical structures but different semantics.
These grammatical and lexical features of
Arabic cause more complexity and ambiguity,
especially for NLP systems designed for thor-
ough processing of Arabic texts compared to
English. A shallow and statistical approach for
IE using texts of specialism can be useful to ab-
stract many complexities of Arabic texts.
Given a 431,563 word corpus comprising
2559 texts of Reuters Arabic Financial News and
the same thresholds we used with the English
corpus, percent (al-meaa, Wil) is again the most
frequent term with frequency and weirdness z-
score greater than zero. It has 3125 occurrences
(0.0072%), a frequency z-score of 19.03 and a
</bodyText>
<table confidence="0.986906692307692">
Collocate Freq U-score k-score
by-a-ratio 1257 39191 7.87
(be-nesba, d:�)
point 1167 9946 6.44
(noqta, &amp;quot;+)
the-year 1753 344 3.34
(al-aam, �-Wl)
index 1130 409 2.55
(moasher,y!_y-)
million 2281 600 2.32
(milyoon, s�)
share 705 206 1.84
(saham, f+-)
Table 6. Downward collocates of percent (al-
meaa, A ►I).
Collocate Freq U-score k-score
in 21236 434756 40.99
(fee,)
to 3339 25145 9.81
(ela, c,11)
from 10344 4682 9.58
(min, cs4)
on 5275 117 3.10
(ala,c,6 )
that 5130 260 2.65
(ann, J)
</table>
<tableCaption confidence="0.92489">
Table 7. Upward collocates of percent (almeaa,
;a.11).
</tableCaption>
<bodyText confidence="0.9985446">
Using the same thresholds the trigrams (Table
8) appear to be different from the English tri-
grams in that the words of movement are not in-
cluded here = this is because Arabic has a richer
morphological system compared to English and
Financial Arabic is not as standardised as Finan-
cial English: however, it will not be difficult to
train the system to recognise the variants of rose
and fell in Financial Arabic. Table 9 lists some
of the patterns.
</bodyText>
<page confidence="0.995866">
60
</page>
<table confidence="0.996150461538461">
To.en &apos; To.en F To.en C &gt;re8 :osition
K��L percent 0B7 0
�� /al-meaa,QRSTU3
/fee,kl3
in percent * CB 0
/fee,kl3 /al-meaa,QRSTU3
in percent to 22 2
/fee,kl3 /al-meaa,QRSTU3 /ela, mTU3
percent to KnoL 20 C
/al-meaa,QRSTU3 /ela, mTU3
M in percent 77 2
/fee�kl3 /al-meaa�QRSTU3
Ta le D&amp;quot; Trigrams of percent /almeaa, QRSTU3&amp;quot;
&amp;ocal Grammar :atterns &gt;re8
N QRSTU kl KnoL N KsL C2
percent in
KnoL mTU QRSTU kl KnoL QVWXY N KsL 2C
to percent in by-a-ratio
QRSTU kl KnoL M hij M KsL ~0
percent in share
Ks;L QZ[\ KnoL mTU QRSTU kl KnoL QVWXY ^p^Z\ qjrsU M `abc M K�L 0D
point to percent in by-ratio wider index
Ks;L QZ[\ KnoL mTU QRSTU kl KnoL tu QZ[\ KnoL N `abc 07
point to percent in namely point index
N qc QRSTU kl KnoL QVWXY M ]ev N kl 01
with percent in by-ratio day in
</table>
<bodyText confidence="0.642306">
Ta le B&amp;quot; Some patterns of percent
/almeaa, QRSTU3&amp;quot;
</bodyText>
<sectionHeader confidence="0.997517" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999936886792453">
Ee )ave argued t)at a met)od t)at is focused on
fre8uency at t)e le*ical level/s3 of linguistic de!
scription = single +ords, compounds, and (!
grams! +ill per)aps lead to patterns t)at are idio!
syncratic of a specialist domain +it)out recourse
to a t)esaurus&amp;quot; T)ere are a num er of linguistic
met)ods = t)at focus on syntactic and semantic
level of description +)ic) mig)t e of e8ual or
etter use&amp;quot;
#n order to s)o+ t)e effectiveness of our
met)od +e apply it to sentiment analysis = an
analysis t)at attempts to e*tract 8ualitative opin!
ion e*pressed a out a range of )uman and natu!
ral artefacts = films, cars, financial instruments
for instance&amp;quot; Froadly spea.ing, sentiments in
financial mar.ets relate to t)e ,rise- and ,fall- of
financial instruments /s)ares, currencies, com!
modities and energy prices39 ine*trica ly t)ese
sentiments relate to c)ange in t)e prices of t)e
instruments&amp;quot; #n ot) %nglis) and &apos;ra ic, +e
)ave found t)at percent or e8uivalent is a .ey!
+ord and trigrams and longer (!grams em ed!
ded +it) t)is .ey+ord relate to metap)orical
movement +ords= up, down, rise, fall&amp;quot; Ho+ever,
in %nglis) t)is association is furt)er conte*tual!
ised +it) ot)er .ey+ords = shares, stocks- and in
&apos;ra ic t)e conte*tualisation is +it) s)ares and
t)e principal commodity of many &apos;ra states
economies = oil&amp;quot; 5ur system ,discovered- ot)
y follo+ing a le*ical level of linguistic descrip!
tion&amp;quot;
&gt;or eac) of t)e t+o languages of interest to us,
+e )ave created 0&amp;quot;72 million to.en corpora&amp;quot;
%ac) corpus +as t)en divided into t+o /roug)ly3
e8ual si@ed su corpora9 training corpus and test!
ing corpus&lt; t)e testing corpus is su !divided into
t+o testing corpora Test0 and Test2 /Ta le 013&amp;quot;
&gt;irst, +e e*tract patterns from t)e Training Cor!
pus using t)e discover local grammar algorit)m
/&gt;igure 03 and also from Test0&amp;quot; (e*t, t)e Train!
ing0 and Test0 corpora are merged and patterns
e*tracted from t)e merged corpus&amp;quot; T)e intuition
+e )ave is t)at as t)e si@e of t)e corpus is in!
creased t)e patterns e*tracted from a smaller
si@ed corpus +ill e ela orated9 some of t)e pat!
terns t)at are idiosyncratic of t)e smaller si@ed
corpus +ill ecome statistically insignificant and
)ence +ill e ignored&amp;quot; T)e conventional +ay of
testing +ould )ave een to see )o+ many pat!
terns discovered in t)e training corpus are found
in t)e testing corpora&lt; +e are 8uantifying t)ese
results currently&amp;quot; #n t)e follo+ing +e descri e an
initial test of our met)od after introducing LoLo&amp;quot;
</bodyText>
<table confidence="0.996182571428571">
Corpus %�����) &apos;�� ��
Te*ts To.ens Te*ts To.ens
Training0 221D D70,2B2 400D D71,121
Test0 0212 2C0,D41 244B 2C0,47C
Training2 /Training0ITest03 C702 0,2BC,C22 7777 0,2BC,C22
Test2 0212 227,D11 244B 22D,470
Total 4816 197209142 109236 197209154
</table>
<bodyText confidence="0.5463895">
Ta le 01&amp;quot; Training and testing corpora used in
our e*periments&amp;quot;
</bodyText>
<subsectionHeader confidence="0.985588">
3.1 LoLo
</subsectionHeader>
<bodyText confidence="0.983926">
LoLo (stands for Local-Grammar for Learning
Terminology and means ,pearl- in &apos;ra ic) is de!
veloped using t)e &amp;quot;(%T platform&amp;quot; #t contains four
components summarised in Ta le 00&amp;quot;
</bodyText>
<table confidence="0.9514505">
Component &gt;unctionality
~5$:~~ &apos;(&apos;&amp;G~%$ �������� ������ ��������
e*traction patterns
$�&amp;%� %~#~5$ ������ �� �� ��� ��������
patterns and slots
#(&gt;5$6&apos;T#5( %xT$&apos;CT5$ %*tract information
#(&gt;5$6&apos;~#5( w#~~&apos;&amp;#~%$ w~~
~~~~~ �������� ����
time
Ta le 00&amp;quot; Summary ofLoLo&apos;s components&amp;quot;
</table>
<page confidence="0.999236">
61
</page>
<bodyText confidence="0.999861444444444">
The various components of LoLo the Ana-
lyser, Editor, Extractor and the Visualiser, can
be used to extract and present patterns; the sys-
tem has utilities to change script and the direc-
tion of writing (Arabic is right-to-left and Eng-
lish left-to-right). Table 12 is an exemplar output
from LoLo: &amp;quot;rise in profit&amp;quot; event patterns ex-
pressed similarly in English and Arabic financial
news headlines found by the Corpus Analyser.
</bodyText>
<table confidence="0.871744666666667">
English * profit up &lt;no&gt; percent
Arabic ���J� .,�&lt;no&gt; * ����� ������
percent in profit rise (up)
</table>
<tableCaption confidence="0.693008666666667">
Table 12. &amp;quot;Rise in profit&amp;quot; patterns in Arabic and
English where the * usually comprises names of
organisations or enterprises.
</tableCaption>
<bodyText confidence="0.995563342105263">
The pattern acquisition algorithm presented
earlier is implemented in the Corpus Analyser
component, which is the focus of this paper. It
can be used for discovering frequent patterns in
corpora. The user has the option to filter smaller
patterns contained in larger ones and to mine for
interrupted or non-interrupted patterns. It can
also distinguish between single word and multi
word slots.
Before mining for patterns, a corpus pre-
processor routine performs a few operations to
improve the pattern discovery. It identifies any
punctuation marks attached to the words and
separates them. it also identifies the sentences
boundaries and converts all the numerical tokens
to one tag &amp;quot;&lt;no&gt;&amp;quot; as numbers can be part of
some patterns, especially in the domain of finan-
cial news.
The Rules Editor is at its initial stages of de-
velopment, currently it can export the extraction
patterns discovered by the Corpus Analyser as
regular expressions.
A time-stamped corpus can be visualised us-
ing the Information Visualiser. The Visualiser
can display a time-series that shows how the ex-
tracted events emerge, repeat and fade over time
in relation to other events or imported time series
i.e. of financial instruments. This can be useful
for analysing any relations between different
events or detecting trends in one or more corpora
or with other time-series.
LoLo facilitates other corpus and computa-
tional linguistics tasks as well, including generat-
ing concordances and finding collocations from
texts encoded in UTF-8. This is particularly use-
ful for Arabic and languages using the Arabic
writing system like Persian and Urdu which lack
such resources.
</bodyText>
<listItem confidence="0.6384655">
3. 2 Training and Testing
3. 2.1 English
</listItem>
<bodyText confidence="0.9993715625">
We consider the English Training1 corpus first.
We extracted the significant collocates of all the
high frequency/high weirdness words, where
,high&apos; defined using the associated z-scores, in
the training corpus. Trigrams were then ex-
tracted and high frequency trigrams were chosen
and all sentences comprising the trigrams were
used to form a (training) sub corpus. The sub-
corpus was then analysed for extracting the local
grammar.
The 10 high frequency N-grams extracted
automatically from the Training1 Corpus
(861,492) are listed in Table 13. The Test1 cor-
pus has most of the trigrams in the Training1
corpus, particularly some of the larger N-grams
(Table 14).
</bodyText>
<listItem confidence="0.757631538461538">
Rank Top 10 patterns comprising ,percent&apos; Freq
1 &lt;s&gt; the * &lt;no&gt; percent 45
2 &lt;s&gt; the * was up &lt;no&gt; percent at &lt;no&gt;, &lt;no&gt; &lt;/s&gt; 33
3 &lt;s&gt; * &lt;no&gt; percent #, &lt;no&gt; &lt;/s&gt; 24
4 &lt;s&gt; * up &lt;no&gt; percent 21
5 &lt;s&gt; the * was down &lt;no&gt; percent at &lt;no&gt; , &lt;no&gt; &lt;/s&gt; 19
6 &lt;s&gt; * &lt;no&gt; percent after 18
6 &lt;s&gt; * &lt;no&gt; percent to &lt;no&gt; , &lt;no&gt; yen 18
7 &lt;s&gt;, # shares were up &lt;no&gt; percent at &lt;no&gt; 17
8 &lt;s&gt; shares in * &lt;no&gt; percent 15
9 &lt;s&gt; * rose &lt;no&gt; percent to &lt;no&gt; 14
10 &lt;s&gt; # shares rose &lt;no&gt; percent to &lt;no&gt; 13
10 &lt;s&gt; fell &lt;no&gt; percent to &lt;no&gt; 13
</listItem>
<tableCaption confidence="0.985281">
Table 13. Patterns of percent extracted from
Training1 corpus.
</tableCaption>
<table confidence="0.99838175">
Patterns Freq
&lt;s&gt; # shares # &lt;no&gt; percent 22
&lt;s&gt; shares in * &lt;no&gt; percent 13
&lt;s&gt; # shares were up &lt;no&gt; percent at 17
</table>
<tableCaption confidence="0.9923185">
Table 14. Patterns of percent extracted from
Test1 corpus found as sub-patterns in Training1.
</tableCaption>
<bodyText confidence="0.997917142857143">
We then merged the Training1 and Test1 cor-
pora together and created Training2 corpus com-
prising of 3612 texts and 1,293,342 tokens. The
Algorithm was executed on the merged corpus
and a new set of patterns were extracted, in par-
ticular the most frequent pattern in the Training1
Corpus (&lt;s&gt; the * &lt;no&gt; percent), was elabo-
</bodyText>
<page confidence="0.998429">
62
</page>
<table confidence="0.851949125">
rated y t)e &apos;lgorit)m as +ell as t)ose patterns
s)o+n in Ta le 04&amp;quot;
Training0 Corpus &gt;re8 Training2 Corpus &gt;re8
K�L �)� N +�� ��+� K��L 0B K�L �)� N index +�� ��+� ~C
������� �� K��L � K��L K;�L K��L ������� �� K��L � K��L
K;sL
KsL t)e N +as up KnoL CC KsL t)e N index +as up KnoL C2
������� �� K��L� K��L K;�L ������� �� K��L � K��L K;�L
</table>
<bodyText confidence="0.931303">
Ta le 04&amp;quot; Comparison et+een t+o patterns in
Training0 and Training2 corpora&amp;quot;
T)e patterns related to t)e collocations of
s)ares and percent from Training0 +ere pre!
served in Training2&amp;quot; T)e test on Test2 corpus
s)o+ed similar results9 t)e smaller (!grams re!
lated to t)e movement of instruments +ere simi!
lar to t)e Test0 Corpus&amp;quot; T)e analysis of &apos;ra ic
te*ts is s)o+n elo+ +it) similar results&amp;quot;
</bodyText>
<sectionHeader confidence="0.724924" genericHeader="method">
3. 2. 2 Arabic
</sectionHeader>
<bodyText confidence="0.997394833333333">
Some of fre8uent (!grams e*tracted automati!
cally from t)e Training0 &apos;ra ic corpus /D71,1213
are s)o+n in Ta le 07&amp;quot; Similar to t)e %nglis)
corpora t)e Test0 &apos;ra ic corpus )as most of t)e
trigrams in t)e Training0 Corpus and some larger
(!grams/Ta le 073&amp;quot;
</bodyText>
<table confidence="0.990849580645161">
$an. Top 01 patterns comprising ,percent- &gt;re8
0 C4
N QRSTU kl K��L N K�L
percent in
� C0
QRSTU klK��L QVWXY N kl N
percent in by-ratio in
C 2D
K�;L QZ[\ K��L mTU QRSTU kl K��L QVWXY QZ[\ K��L N
point to percent in by-ratio point
2 22
N kl QRSTU kl N K�L
in percent in
2 22
K��L mTU QRSTU kl K��L QVWXY N K�L
to percent in by-ratio
4 20
QRSTU kl K��L mTU N kl N
percent in to in
4 20
K�;LQZ[\ K��L mTU QRSTU klK��LQVWXY ^p^Z\ N `abcMK�L
point to percent in by-ratio zone index
Ta le 07&amp;quot; :atterns of percent /almeaa, QRSTU3 e*!
tracted from Training0 &apos;ra ic corpus&amp;quot;
:atterns &gt;re8
M QRSTU kl KnoL QVWXY N 01
percent in by-ratio
kl QRSTU kl KnoL QVWXY N 01
in percent in by-ratio
N QRSTU kl KnoL QVWXY N KsL 00
percent in by-ratio
</table>
<bodyText confidence="0.990366454545454">
Ta le 07&amp;quot; :atterns ofpercent /almeaa, QRSTU3 e*!
tracted from Test0 &apos;ra ic corpus found as su !
patterns in Training0&amp;quot;
&apos;fter merging t)e Training0 and Test0 &apos;ra ic
corpora toget)er into a corpus of 7777 te*ts and
0,2BC,C22 to.ens, ne+ set of patterns +ere e*!
tracted as +ell&amp;quot; Some of t)e fre8uent patterns in
t)e training corpus +ere ela orated more as +ell
li.e t)e pattern s)o+n in Ta le 0D +)ere t)e to!
.en and-rise /wa-ertifaa, qyz{Ur3 +as added to t)e
pattern&amp;quot;
</bodyText>
<table confidence="0.807367833333333">
Training, Corpus Freq Training2 Corpus Freq
^p^Z\ qjrsU M `abc M KsL 0C ^p^Z\ qjrsU M `abc #JI.3 KsL 07
KnoL mTU QRSTU kl KnoL QVWXY QZ[\ KnoL mTU QRSTU kl KnoL QVWXY
K;sL QZ[\ K;sL
Ta le 0D&amp;quot; Comparison et+een t+o patterns in
Training0 and Training2 &apos;ra ic corpora&amp;quot;
</table>
<sectionHeader confidence="0.998916" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998808923076923">
Ee )ave used t)e Rules Editor and t)e Informa-
tion Extractor to evaluate t)e patterns on a cor!
pus comprising 221D te*ts and D4D,741 to.ens
created y merging Test0 and Test2 corpora&amp;quot; T)e
&apos;ra ic evaluation corpus comprised 400D te*ts
and D71,0C2 to.ens&amp;quot; T)e (!gram pattern e*trac!
tor /+)ere ( L 23 s)o+ed considera le promise
in t)at +)o or +)at +ent up;or do+n +as unam!
iguously e*tracted from t)e %nglis) test corpus
using patterns generated t)roug) t)e training
corpus&amp;quot; #nitial results s)o+ )ig) precision +it)
t)e longer (!grams in %nglis) /Ta le 0B3 and
&apos;ra ic /Ta le 213&amp;quot;
</bodyText>
<table confidence="0.942770285714286">
:attern :recision
&lt;ORG&gt; �)���� +��� ��+� K��L ������� �� K��L 011H
/0C;0C3
&lt;Movement&gt; K��L ������� �� K��L � K��L ��� 011H
/07;073
�)� &lt;Index&gt; +�� �� K��L ������� �� K��L � K��L B�H
/00;023
&lt;ORG&gt; �)���� M �� K��L ������� �� DDH
/C1;C23
Ta le 0B&amp;quot; :atterns +it) )ig) precision /%nglis)3&amp;quot;
:������ :����!
����
QZ[\KnoL mTU QRSTU kl KnoL tu QZ[\ KnoL &lt;Index&gt; `abc 011H
point to percent in viz point index /2~;2~3
AWl i &lt;no&gt; # XI-L &lt;no&gt; # ?+-J &lt;Index&gt; -As* 011H
percent in point for-shares index /~~;~~3
QZ[\ KnoLmTU QRSTU kl KnoL QVWXY ^p^Z\ qjrsU&lt;Index&gt; `abc &lt;Movement&gt; B7H
point to percent in by-ratio zone wider index /CC;C23
XI-L &lt;no&gt; ,,1I Awl ,i &lt;no&gt; Litlsi &lt;Index&gt; _A9- &lt;Movement&gt; 77H
point to percent in zone index /~~;C43
Ta le 21&amp;quot; :atterns +it) )ig) precision /&apos;ra ic3&amp;quot;
</table>
<page confidence="0.998786">
63
</page>
<bodyText confidence="0.998215166666667">
However, some patterns return many extracted
information that require trimming. For example
many organizations names are extracted in Ara-
bic using the pattern shown in table 21 but they
usually have the word by-a-ratio (be-nesba, d:�)
attached at the end resulting in low precision.
</bodyText>
<table confidence="0.9984126">
Pattern Precision
&lt;ORG&gt; rose &lt;no&gt; percent to &lt;no&gt; 36%
(5/14)
����� ��&lt;no&gt; &lt;ORG&gt; �Syy *- &lt;Movement&gt; 30%
percent in company share (25/83)
</table>
<tableCaption confidence="0.9268045">
Table 21. Patterns with low precision in English
and Arabic
</tableCaption>
<bodyText confidence="0.999225466666667">
Because we have used the same training
thresholds for English and Arabic, the patterns in
Arabic appeared without the motion words.
However the system can extract these words
along with the org/instrument/index names be-
cause they appear frequently as slots in the pat-
terns.
The N-gram patterns (when N &lt;_ 4) show poor
results in that either such patterns found in the
training corpus are not found in the test corpus,
or the patterns retrieved from test corpora are at
semantic variance with the same pattern in the
training corpus. This suggests that there is an
optimal length of individual patterns in our local
grammar.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="conclusions">
5 Afterword
</sectionHeader>
<bodyText confidence="0.999964367346939">
The patterns extracted from the English (and
Arabic) corpora confirm to an extent the view of
the proponents of local grammar, of a special
language, that there are certain words (in our
case percent, shares, index) that appear to have a
specific grammatical category in the sense that
the neighbourhood of these words is occupied by
a small number of other words (up, down, fall,
rise, &lt;no&gt; for instance). If we were to apply the
grammars typically used in part-of-speech tag-
gers and syntactic parsing in general, the idio-
syncratic behaviour of the pivotal keywords in
specialist language does not become apparent:
the pivotal keywords are regarded as noun
phrases and the association of these phrases is
with other general categories of verb phrase, ad-
jectival phrase and adverbial phrase.
The patterns we have extracted could have
been extracted with the help of a thesaurus. And,
this is the question which is critical to us: how to
create and maintain a thesaurus within a domain.
This is illustrated in a small way by our experi-
ment on the Training1 corpus where the term in-
dex was not statistically significant for it to ap-
pear in the trigrams that populate the local
grammar. However, in Training2, the larger cor-
pus did contain significant frequency of the term
index for it to make into a pattern of its own.
Furthermore, many of the patterns in Training1
persisted in Training2. Smaller N-grams persist
as well in the various Training and Test corpora
= these patterns in themselves act like units
around which other trigrams nucleate.
The evaluation of our Algorithm is still con-
tinuing and we are in the process of setting up
experiments with human volunteers, especially
those with some knowledge of financial matters
to evaluate the output of LoLo. We intend to use
information retrieval metrics of recall and the
various F0 measures.
The local grammar movement has made er-
ratic progress since its inception in the 1960&apos;s.
Now, with the advent of accessible computers
with substantive memories, with the advent of
the Internet and the concomitant treasure of
multi-lingual text deposits and text streams, one
can explore the use of such grammars in address-
ing the major challenges in information extrac-
tion.
</bodyText>
<sectionHeader confidence="0.990445" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999780304347826">
Ahmad, Khurshid. and Al-Sayed, Rafif. (2005)
Community of Practice and the Special Language
,Ground&apos;. In (Eds.) Clarke, S and Coakes, E. En-
cyclopaedia of Knowledge Management and
Community of Practice. Hershey (PA): The Idea
Group Reference.
Ahmad, Khurshid., Cheng, David. and Almas, Yousif.
(2006) ,Multi-lingual Sentiment Analysis of Fi-
nancial News Streams.&apos; In Proc. of the 1st Interna-
tional Conference on Grid in Finance, Palermo.
Andersen, Torben., Bollerslev, Tim., Diebold, Fran-
cis, and Vega, Clara. (2002). ,Micro effects of
macro announcements: Real Time Price Discovery
in Foreign Exchange&apos;. National Bureau of Eco-
nomic Research Working Paper 8959. (Available at
http://www.nber.org/papers/w8959).
Barnbrook, Geoffrey. and Sinclair, John McH. (1996)
&apos;Parsing Cobuild Entries&apos;. In (Eds.) John McH.
Sinclair, Martin Hoelter &amp; Carol Peters. The Lan-
guages of Definition: the Formalization of Dic-
tionary Definitions for Natural Language Process-
ing: Luxembourg: Office for Official Publications
of the European Communities, pp 13-58.
</reference>
<page confidence="0.986404">
64
</page>
<reference confidence="0.999838851485149">
Barnbrook, Geoffrey. (2002) Defining Language: A
local grammar of definition sentences. Amsterdam:
John Benjamins Publishers.
Omrane, Walid., Bauwens, Luc., and Giot, Pierre.
(2005) ,News Announcements, Market Activity
and Volatility in the Euro/Dollar Foreign Exchange
Market&apos;. Journal of International Money and Fi-
nance, 24 (7), pp. 1108-1125.
Brill, Eric. (1993) ,Automatic Grammar Induction
and Parsing Free Text: A transformation-based ap-
proach&apos;. In Proc. of 31th Annual Meeting of the As-
sociation for Computational Linguistics, Ohio.
Chan, Wesley. (2003) ,Stock Price Reaction to News
and No-News. Drift and Reversal after Headlines&apos;.
Journal of Financial Economics, 70(2), pp. 223-
260.
Ciravegna, Fabio. and Wilks, Yorick. (2003) ,Design-
ing Adaptive Information Extraction for the Se-
mantic Web in Amilcare&apos;. In (Eds.) Siegfried
Handschuh and Steffen Staab, Annotation for the
Semantic Web, Frontiers in Artificial Intelligence
and Applications. US: IOS Press.
Cutler, David., Poterba, James., and Summers, Law-
rence. (1989) ,What Moves Stock Prices?&apos;. Jour-
nal of Portfolio Management, 15(3), pp. 4-12.
Debnath, Sandip. and Giles, C. Lee. (2005) ,A Learn-
ing Based Model for Headline Extraction of News
Articles to Find Explanatory Sentences for Events&apos;.
In Proc. of the 3rd international conference on
Knowledge capture, Alberta, Canada.
Engle, Robert. and K. Ng, Victor. (1993) ,Measuring
and Testing the Impact of News on Volatility&apos;,
Journal ofFinance, 48(5), pp. 1749-1777.
Gao, Jianfeng., Li, Mu., Wu, Andi. and Huang,
Chang-Ning (2005) ,Chinese Word Segmentation
and Named Entity Recognition: A Pragmatic Ap-
proach&apos;, Journal of Computational Linguistics,
31(4), Cambridge, Mass.: MIT Press, pp. 531-574
Gross, Maurice. (1997) ,The Construction of Local
Grammars&apos;. In (Eds.) Roche, E. and Schabes, Y.,
Finite-State Language Processing, Language,
Speech, and Communication, Cambridge, Mass.:
MIT Press, pp. 329-354.
Habash, Nizar. and Rambow, Owen. (2005) ,Arabic
Tokenization, Morphological Analysis, and Part-
of-Speech Tagging in One Fell Swoop&apos;. In Proc. of
the Conference of American Association for Com-
putational Linguistics (ACL&apos;05), Ann Arbour, MI.
Halliday, Michael, A. K. (1993) &apos;On the language of
Physical Sciences&apos;. In (Eds.) Halliday, Michael.
and Martin, J. R., Writing Science pp. 54-68. Lon-
don: The Falmer Press.
Hardie, Iain. and MacKenzie, Donaled. (2005) ,An
Economy of Calculation: Agencement and Distrib-
uted Cognition in a Hedge Fund&apos;. (Available at
http://www.sps.ed.ac.uk/staff/An`/`20Economy`/`2
0of`/`20Calculation.pdf).
Harris, Zellig. (1991) A Theory of Language and In-
formation: A Mathematical Approach. Oxford:
Clarendon Press.
Kittredge, Richard. and Lehrberger, John. (1982) Sub-
language: Studies of language in restricted seman-
tic domains. Berlin: Walter de Gruyter.
Koppel, Moshe and Shtrimberg, Itai. (2004) ,Good
News or Bad News? Let the Market Decide&apos;. In
AAAI Spring Symposium on Exploring Attitude and
Affect in Text, Palo Alto: AAAI Press, pp. 86-88.
Mackenzie, Donald. (2000). ,Fear in the Markets&apos;.
London Review of Books, 22(8), pp 31-32.
McLernon, Brian. and Kushmerick, Nicholas. (2006)
,Transductive Pattern Learning for Information Ex-
traction&apos;. In Proc. of EACL 2006 Workshop on
Adaptive Text Extraction and Mining, Trento.
Seo, Young-Woo., Giampapa, Joseph. and Sycara,
Katia. (2002) ,Text Classification for Intelligent
Agent Portfolio Management&apos;. In Proc. of the 1st
International Joint Conference on Autonomous
Agents and Multi-Agent Systems, pp. 802-803, Bo-
logna.
Shiller, Robert. (2000) Irrational Exuberance. Prince-
ton: Princeton University Press.
Sinclair, John McH. (1996) Collins COBUILD
Grammar Patterns 1: Verbs. HarperCollins, Glas-
gow.
Smadja, Frank. (1994) ,Retrieving Collocations from
Text: Xtract&apos;. In (Eds.) Armstrong, S., Using Large
Corpora. London: MIT Press.
Stevenson, Mark. and Greenwood, Mark A. (2005) ,A
Semantic Approach to IE Pattern Induction&apos;. In
Proc. of the 43rd Meeting of the Association for
Computational Linguistics (ACL&apos;05), pp. 379-386,
Ann Arbour, MI.
Wilks, Yorick (1998) ,Inducing Adequate Grammars
from Electronic Texts&apos;, EPSRC ROPA Grant
GR/K/66215 Final Report. (Available at
http://nlp.shef.ac.uk/research/reports/k66215.html).
Yangarber, Roman. (2003) ,Counter-Training in Dis-
covery of Semantic Patterns&apos;. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2003), pp. 343-350, Sap-
poro, Japan.
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495461">
<title confidence="0.999893">LoLo: A System based on Terminology for Multilingual Extraction</title>
<author confidence="0.993248">Yousif</author>
<affiliation confidence="0.9997425">Department of University of</affiliation>
<address confidence="0.999011">Guildford, Surrey, GU2 7XH, UK</address>
<email confidence="0.994717">y.almas@surrey.ac.uk</email>
<author confidence="0.608229">Khurshid</author>
<affiliation confidence="0.9318505">Department of Computer Trinity</affiliation>
<address confidence="0.983634">Dublin-2. IRELAND</address>
<email confidence="0.991059">kahmad@cs.tcd.ie</email>
<abstract confidence="0.998601111111111">An unsupervised learning method, based on corpus linguistics and special language terminology, is described that can extract time-varying information from text streams. The method is shown to be in that its use leads to sets of regular-expressions that can be used to extract the information in typologically distinct languages like English and Arabic. The method uses the information related to the distribution of Ngrams, for automatically extracting bearing&apos; patterns of usage in a training corpus. The analysis of an English news wire corpus (1,720,142 tokens) and Arabic news wire corpus (1,720,154 tokens) show encouraging results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>