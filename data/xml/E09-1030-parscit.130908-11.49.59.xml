<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.964509">
Reconstructing false start errors in spontaneous speech text
</title>
<author confidence="0.982644">
Erin Fitzgerald
</author>
<affiliation confidence="0.963035">
Johns Hopkins University
</affiliation>
<address confidence="0.952185">
Baltimore, MD, USA
</address>
<email confidence="0.997535">
erinf@jhu.edu
</email>
<author confidence="0.838993">
Keith Hall
</author>
<affiliation confidence="0.794204">
Google, Inc.
</affiliation>
<address confidence="0.639291">
Zurich, Switzerland
</address>
<email confidence="0.981434">
kbhall@google.com
</email>
<author confidence="0.987607">
Frederick Jelinek
</author>
<affiliation confidence="0.966296">
Johns Hopkins University
</affiliation>
<address confidence="0.952858">
Baltimore, MD, USA
</address>
<email confidence="0.99871">
jelinek@jhu.edu
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998183">
This paper presents a conditional ran-
dom field-based approach for identifying
speaker-produced disfluencies (i.e. if and
where they occur) in spontaneous speech
transcripts. We emphasize false start re-
gions, which are often missed in cur-
rent disfluency identification approaches
as they lack lexical or structural similar-
ity to the speech immediately following.
We find that combining lexical, syntac-
tic, and language model-related features
with the output of a state-of-the-art disflu-
ency identification system improves over-
all word-level identification of these and
other errors. Improvements are reinforced
under a stricter evaluation metric requiring
exact matches between cleaned sentences
annotator-produced reconstructions, and
altogether show promise for general re-
construction efforts.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948145833333">
The output of an automatic speech recognition
(ASR) system is often not what is required for sub-
sequent processing, in part because speakers them-
selves often make mistakes (e.g. stuttering, self-
correcting, or using filler words). A cleaner speech
transcript would allow for more accurate language
processing as needed for natural language process-
ing tasks such as machine translation and conver-
sation summarization which often assume a gram-
matical sentence as input.
A system would accomplish reconstruction of
its spontaneous speech input if its output were
to represent, in flawless, fluent, and content-
preserving text, the message that the speaker in-
tended to convey. Such a system could also be ap-
plied not only to spontaneous English speech, but
to correct common mistakes made by non-native
speakers (Lee and Seneff, 2006), and possibly ex-
tended to non-English speaker errors.
A key motivation for this work is the hope that a
cleaner, reconstructed speech transcript will allow
for simpler and more accurate human and natu-
ral language processing, as needed for applications
like machine translation, question answering, text
summarization, and paraphrasing which often as-
sume a grammatical sentence as input. This ben-
efit has been directly demonstrated for statistical
machine translation (SMT). Rao et al. (2007) gave
evidence that simple disfluency removal from tran-
scripts can improve BLEU (a standard SMT eval-
uation metric) up to 8% for sentences with disflu-
encies. The presence of disfluencies were found to
hurt SMT in two ways: making utterances longer
without adding semantic content (and sometimes
adding false content) and exacerbating the data
mismatch between the spontaneous input and the
clean text training data.
While full speech reconstruction would likely
require a range of string transformations and po-
tentially deep syntactic and semantic analysis of
the errorful text (Fitzgerald, 2009), in this work
we will first attempt to resolve less complex errors,
corrected by deletion alone, in a given manually-
transcribed utterance.
We build on efforts from (Johnson et al., 2004),
aiming to improve overall recall – especially of
false start or non-copy errors – while concurrently
maintaining or improving precision.
</bodyText>
<subsectionHeader confidence="0.965227">
1.1 Error classes in spontaneous speech
</subsectionHeader>
<bodyText confidence="0.999939428571428">
Common simple disfluencies in sentence-like ut-
terances (SUs) include filler words (i.e. “um”, “ah”,
and discourse markers like “you know”), as well as
speaker edits consisting of a reparandum, an inter-
ruption point (IP), an optional interregnum (like “I
mean”), and a repair region (Shriberg, 1994), as
seen in Figure 1.
</bodyText>
<note confidence="0.9230615">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 255–263,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.995814">
255
</page>
<figureCaption confidence="0.947259">
Figure 1: Typical edit region structure. In these
</figureCaption>
<bodyText confidence="0.775966">
and other examples, reparandum regions are in
brackets (’[’, ’]’), interregna are in braces (&apos;I&apos; &apos;
&apos;f&apos;,
’}’), and interruption points are marked by ’+’.
These reparanda, or edit regions, can be classified
into three main groups:
</bodyText>
<listItem confidence="0.891559285714286">
1. In a repetition (above), the repair phrase is
approximately identical to the reparandum.
2. In a revision, the repair phrase alters reparan-
dum words to correct the previously stated
thought.
EX1: but [when he] + fi mean} when she put it
that way
EX2: it helps people [that are going to quit] + that
would be quitting anyway
3. In a restart fragment (also called a false
start), an utterance is aborted and then
restarted with a new train of thought.
EX3: and [i think he’s] + he tells me he’s glad he
has one of those
</listItem>
<bodyText confidence="0.98800075">
EX4: [amazon was incorporated by] fuh} well i
only knew two people there
In simple cleanup (a precursor to full speech re-
construction), all detected filler words are deleted,
and the reparanda and interregna are deleted while
the repair region is left intact. This is a strong ini-
tial step for speech reconstruction, though more
complex and less deterministic changes are of-
ten required for generating fluent and grammatical
speech text.
In some cases, such as the repetitions men-
tioned above, simple cleanup is adequate for re-
construction. However, simply deleting the identi-
fied reparandum regions is not always optimal. We
would like to consider preserving these fragments
(for false starts in particular) if
</bodyText>
<listItem confidence="0.994834666666667">
1. the fragment contains content words, and
2. its information content is distinct from that in
surrounding utterances.
</listItem>
<bodyText confidence="0.999890181818182">
In the first restart fragment example (EX3 in Sec-
tion 1.1), the reparandum introduces no new ac-
tive verbs or new content, and thus can be safely
deleted. The second example (EX4) however
demonstrates a case when the reparandum may be
considered to have unique and preservable con-
tent of its own. Future work should address how
to most appropriately reconstruct speech in this
and similar cases; this initial work will for risk
information loss as we identify and delete these
reparandum regions.
</bodyText>
<sectionHeader confidence="0.752134" genericHeader="related work">
1.2 Related Work
</sectionHeader>
<bodyText confidence="0.999782916666667">
Stochastic approaches for simple disfluency de-
tection use features such as lexical form, acoustic
cues, and rule-based knowledge. Most state-of-
the-art methods for edit region detection such as
(Johnson and Charniak, 2004; Zhang and Weng,
2005; Liu et al., 2004; Honal and Schultz, 2005)
model speech disfluencies as a noisy channel
model. In a noisy channel model we assume that
an unknown but fluent string F has passed through
a disfluency-adding channel to produce the ob-
served disfluent string D, and we then aim to re-
cover the most likely input string F�, defined as
</bodyText>
<equation confidence="0.9894865">
F� = argmaxFP(F|D)
= argmaxFP(D|F)P(F)
</equation>
<bodyText confidence="0.999575296296296">
where P(F) represents a language model defin-
ing a probability distribution over fluent “source”
strings F, and P(D|F) is the channel model defin-
ing a conditional probability distribution of ob-
served sentences D which may contain the types
of construction errors described in the previous
subsection. The final output is a word-level tag-
ging of the error condition of each word in the se-
quence, as seen in line 2 of Figure 2.
The Johnson and Charniak (2004) approach,
referred to in this document as JC04, combines
the noisy channel paradigm with a tree-adjoining
grammar (TAG) to capture approximately re-
peated elements. The TAG approach models the
crossed word dependencies observed when the
reparandum incorporates the same or very similar
words in roughly the same word order, which JC04
refer to as a rough copy. Our version of this sys-
tem does not use external features such as prosodic
classes, as they use in Johnson et al. (2004), but
otherwise appears to produce comparable results
to those reported.
While much progress has been made in sim-
ple disfluency detection in the last decade, even
top-performing systems continue to be ineffec-
tive at identifying words in reparanda. To bet-
ter understand these problems and identify areas
</bodyText>
<figure confidence="0.54918825">
[that&apos;s] IP  |{z } a relief
 |{z } z}|{ �{uh} that&apos;s
reparandum |{z} repair
interregnum
</figure>
<page confidence="0.751266">
256
</page>
<table confidence="0.99243">
Label % of words Precision Recall F-score
Fillers 5.6% 64% 59% 61%
Edit (reparandum) 7.8% 85% 68% 75%
</table>
<tableCaption confidence="0.973214">
Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.
</tableCaption>
<table confidence="0.99799">
Label % of edits Recall
Rough copy (RC) edits 58.8% 84.8%
Non-copy (NC) edits 41.2% 43.2%
Total edits 100.0% 67.6%
</table>
<tableCaption confidence="0.998873">
Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.
</tableCaption>
<figure confidence="0.942776666666667">
1 he that ’s uh that ’s a relief
2 E E E FL - - - -
3 NC RC RC FL - - - -
</figure>
<figureCaption confidence="0.995252">
Figure 2: Example of word class and refined word
</figureCaption>
<bodyText confidence="0.952511363636364">
class labels, where - denotes a non-error, FL de-
notes a filler, E generally denotes reparanda, and
RC and NC indicate rough copy and non-copy
speaker errors, respectively.
for improvement, we used the top-performing1
JC04 noisy channel TAG edit detector to produce
edit detection analyses on the test segment of the
Spontaneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008). Table 1 demon-
strates the performance of this system for detect-
ing filled pause fillers, discourse marker fillers,
and edit words. The results of a more granular
analysis compared to a hand-refined reference (as
shown in line 3 of Figure 2) are shown in Table 2.
The reader will recall that precision P is defined
as P = |correct|
|correct|+|false |and recall R = |correct|
|correct|+|miss|.
We denote the harmonic mean of P and R as F-
score F and calculate it F = 2
1/P+1/R.
As expected given the assumptions of the TAG
approach, JC04 identifies repetitions and most
revisions in the SSR data, but less success-
fully labels false starts and other speaker self-
interruptions which do not have a cross-serial cor-
relations. These non-copy errors (with a recall of
only 43.2%), are hurting the overall edit detection
recall score. Precision (and thus F-score) cannot
be calculated for the experiment in Table 2; since
the JC04 does not explicitly label edits as rough
copies or non-copies, we have no way of knowing
whether words falsely labeled as edits would have
</bodyText>
<footnote confidence="0.615651">
1As determined in the RT04 EARS Metadata Extraction
Task
</footnote>
<bodyText confidence="0.999891">
been considered as false RCs or false NCs. This
will unfortunately hinder us from using JC04 as a
direct baseline comparison in our work targeting
false starts; however, we consider these results to
be further motivation for the work.
Surveying these results, we conclude that there
is still much room for improvement in the
field of simple disfluency identification, espe-
cially the cases of detecting non-copy reparandum
and learning how and where to implement non-
deletion reconstruction changes.
</bodyText>
<sectionHeader confidence="0.97701" genericHeader="method">
2 Approach
</sectionHeader>
<subsectionHeader confidence="0.952513">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.9998815">
We conducted our experiments on the recently re-
leased Spontaneous Speech Reconstruction (SSR)
corpus (Fitzgerald and Jelinek, 2008), a medium-
sized set of disfluency annotations atop Fisher
conversational telephone speech (CTS) data (Cieri
et al., 2004). Advantages of the SSR data include
</bodyText>
<listItem confidence="0.99223">
• aligned parallel original and cleaned sen-
tences
• several levels of error annotations, allowing
for a coarse-to-fine reconstruction approach
• multiple annotations per sentence reflecting
the occasional ambiguity of corrections
</listItem>
<bodyText confidence="0.998798">
As reconstructions are sometimes non-
deterministic (illustrated in EX6 in Section
1.1), the SSR provides two manual reconstruc-
tions for each utterance in the data. We use
these dual annotations to learn complementary
approaches in training and to allow for more
accurate evaluation.
The SSR corpus does not explicitly label all
reparandum-like regions, as defined in Section 1.1,
but only those which annotators selected to delete.
</bodyText>
<page confidence="0.976066">
257
</page>
<bodyText confidence="0.991613208333333">
Thus, for these experiments we must implicitly
attempt to replicate annotator decisions regarding
whether or not to delete reparandum regions when
labeling them as such. Fortunately, we expect this
to have a negligible effect here as we will empha-
size utterances which do not require more complex
reconstructions in this work.
The Spontaneous Speech Reconstruction cor-
pus is partitioned into three subcorpora: 17,162
training sentences (119,693 words), 2,191 sen-
tences (14,861 words) in the development set, and
2,288 sentences (15,382 words) in the test set. Ap-
proximately 17% of the total utterances contain a
reparandum-type error.
The output of the JC04 model ((Johnson and
Charniak, 2004) is included as a feature and used
as an approximate baseline in the following exper-
iments. The training of the TAG model within this
system requires a very specific data format, so this
system is trained not with SSR but with Switch-
board (SWBD) (Godfrey et al., 1992) data as de-
scribed in (Johnson and Charniak, 2004). Key dif-
ferences in these corpora, besides the form of their
annotations, include:
</bodyText>
<listItem confidence="0.946495357142857">
• SSR aims to correct speech output, while
SWBD edit annotation aims to identify
reparandum structures specifically. Thus, as
mentioned, SSR only marks those reparanda
which annotators believe must be deleted
to generate a grammatical and content-
preserving reconstruction.
• SSR considers some phenomena such as
leading conjunctions (“and i did” --+ “i did”) to
be fillers, while SWBD does not.
• SSR includes more complex error identifi-
cation and correction, though these effects
should be negligible in the experimental
setup presented herein.
</listItem>
<bodyText confidence="0.9999645">
While we hope to adapt the trained JC04 model
to SSR data in the future, for now these difference
in task, evaluation, and training data will prevent
direct comparison between JC04 and our results.
</bodyText>
<subsectionHeader confidence="0.997502">
2.2 Conditional random fields
</subsectionHeader>
<bodyText confidence="0.9993192">
Conditional random fields (Lafferty et al., 2001),
or CRFs, are undirected graphical models whose
prediction of a hidden variable sequence Y is
globally conditioned on a given observation se-
quence X, as shown in Figure 3. Each observed
</bodyText>
<figureCaption confidence="0.612945">
Figure 3: Illustration of a conditional random
</figureCaption>
<bodyText confidence="0.995026222222222">
field. For this work, x represents observable in-
puts for each word as described in Section 3.1 and
y represents the error class of each word (Section
3.2).
state xi E X is composed of the corresponding
word wi and a set of additional features Fi, de-
tailed in Section 3.1.
The conditional probability of this model can be
represented as
</bodyText>
<equation confidence="0.994614">
1
p�(Y |X) = Zλ(X) exp( AkFk(X, Y )) (1)
k
</equation>
<bodyText confidence="0.999824238095238">
where Zλ(X) is a global normalization factor and
A = (A1 ... AK) are model parameters related to
each feature function Fk(X, Y ).
CRFs have been widely applied to tasks in
natural language processing, especially those in-
volving tagging words with labels such as part-
of-speech tagging and shallow parsing (Sha and
Pereira, 2003), as well as sentence boundary
detection (Liu et al., 2005; Liu et al., 2004).
These models have the advantage that they model
sequential context (like hidden Markov models
(HMMs)) but are discriminative rather than gen-
erative and have a less restricted feature set. Ad-
ditionally, as compared to HMMs, CRFs offer
conditional (versus joint) likelihood, and directly
maximizes posterior label probabilities P(E|O).
We used the GRMM package (Sutton, 2006) to
implement our CRF models, each using a zero-
mean Gaussian prior to reduce over-fitting our
model. No feature reduction is employed, except
where indicated.
</bodyText>
<sectionHeader confidence="0.995884" genericHeader="method">
3 Word-Level ID Experiments
</sectionHeader>
<subsectionHeader confidence="0.985862">
3.1 Feature functions
</subsectionHeader>
<bodyText confidence="0.999993">
We aim to train our CRF model with sets of
features with orthogonal analyses of the errorful
text, integrating knowledge from multiple sources.
While we anticipate that repetitions and other
rough copies will be identified primarily by lexical
</bodyText>
<page confidence="0.98623">
258
</page>
<bodyText confidence="0.999931583333333">
and local context features, this will not necessarily
help for false starts with little or no lexical overlap
between reparandum and repair. To catch these er-
rors, we add both language model features (trained
with the SRILM toolkit (Stolcke, 2002) on SWBD
data with EDITED reparandum nodes removed),
and syntactic features to our model. We also in-
cluded the output of the JC04 system – which had
generally high precision on the SSR data – in the
hopes of building on these results.
Altogether, the following features F were ex-
tracted for each observation xi.
</bodyText>
<listItem confidence="0.972813222222222">
• Lexical features, including
– the lexical item and part-of-speech
(POS) for tokens ti and ti+1,
– distance from previous token to the next
matching word/POS,
– whether previous token is partial word
and the distance to the next word with
same start, and
– the token’s (normalized) position within
the sentence.
• JC04-edit: whether previous, next, or cur-
rent word is identified by the JC04 system as
an edit and/or a filler (fillers are classified as
described in (Johnson et al., 2004)).
• Language model features: the unigram log
probability of the next word (or POS) token
p(t), the token log probability conditioned on
its multi-token history h (p(t|h))2, and the
</listItem>
<bodyText confidence="0.8487952">
log ratio of the two (log p(t 1 )) to serve as
pW
an approximation for mutual information be-
tween the token and its history, as defined be-
low.
</bodyText>
<equation confidence="0.99583025">
p(h, t) log p(h, t)
p(h)p(t)
p(h, t) I log p(t  |h)1
p(t) J
</equation>
<bodyText confidence="0.999832">
This aims to capture unexpected n-grams
produced by the juxtaposition of the reparan-
dum and the repair. The mutual information
feature aims to identify when common words
are seen in uncommon context (or, alterna-
tively, penalize rare n-grams normalized for
rare words).
</bodyText>
<footnote confidence="0.847846666666667">
2In our model, word historys h encompassed the previous
two words (a 3-gram model) and POS history encompassed
the previous four POS labels (a 5-gram model)
</footnote>
<listItem confidence="0.490863111111111">
• Non-terminal (NT) ancestors: Given an au-
tomatically produced parse of the utterance
(using the Charniak (1999) parser trained on
Switchboard (SWBD) (Godfrey et al., 1992)
CTS data), we determined for each word all
NT phrases just completed (if any), all NT
phrases about to start to its right (if any), and
all NT constituents for which the word is in-
cluded.
</listItem>
<bodyText confidence="0.999838538461538">
(Ferreira and Bailey, 2004) and others have
found that false starts and repeats tend to end
at certain points of phrases, which we also
found to be generally true for the annotated
data.
Note that the syntactic and POS features we
used are extracted from the output of an automatic
parser. While we do not expect the parser to al-
ways be accurate, especially when parsing errorful
text, we hope that the parser will at least be con-
sistent in the types of structures it assigns to par-
ticular error phenomena. We use these features in
the hope of taking advantage of that consistency.
</bodyText>
<subsectionHeader confidence="0.997865">
3.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.984145333333333">
In these experiments, we attempt to label the
following word-boundary classes as annotated in
SSR corpus:
</bodyText>
<listItem confidence="0.970564363636364">
• fillers (FL), including filled pauses and dis-
course markers (-5.6% of words)
• rough copy (RC) edit (reparandum incor-
porates the same or very similar words in
roughly the same word order, including repe-
titions and some revisions) (-4.6% of words)
• non-copy (NC) edit (a speaker error where the
reparandum has no lexical or structural re-
lationship to the repair region following, as
seen in restart fragments and some revisions)
(-3.2% of words)
</listItem>
<bodyText confidence="0.999499583333333">
Other labels annotated in the SSR corpus (such
as insertions and word reorderings), have been ig-
nored for these error tagging experiments.
We approach our training of CRFs in several
ways, detailed in Table 3. In half of our exper-
iments (#1, 3, and 4), we trained a single model
to predict all three annotated classes (as defined
at the beginning of Section 3.3), and in the other
half (#2, 5, and 6), we trained the model to predict
NCs only, NCs and FLs, RCs only, or RCs and FLs
(as FLs often serve as interregnum, we predict that
these will be a valuable cue for other edits).
</bodyText>
<equation confidence="0.947785">
I(t; h) = �
h,t
�=
h,t
</equation>
<page confidence="0.995019">
259
</page>
<table confidence="0.998489714285714">
Setup Train data Test data Classes trained per model
#1 Full train Full test FL + RC + NC
#2 Full train Full test {RC,NC}, FL+{RC,NC}
#3 Errorful SUs Errorful SUs FL + RC + NC
#4 Errorful SUs Full test FL + RC + NC
#5 Errorful SUs Errorful SUs {RC,NC}, FL+{RC,NC}
#6 Errorful SUs Full test {RC,NC}, FL+{RC,NC}
</table>
<tableCaption confidence="0.999891">
Table 3: Overview of experimental setups for word-level error predictions.
</tableCaption>
<bodyText confidence="0.9999923">
We varied the subcorpus utterances used in
training. In some experiments (#1 and 2) we
trained with the entire training set3, including sen-
tences without speaker errors, and in others (#3-6)
we trained only on those sentences containing the
relevant deletion errors (and no additionally com-
plex errors) to produce a densely errorful train-
ing set. Likewise, in some experiments we pro-
duced output only for those test sentences which
we knew to contain simple errors (#3 and 5). This
was meant to emulate the ideal condition where
we could perfectly predict which sentences con-
tain errors before identifying where exactly those
errors occurred.
The JC04-edit feature was included to help us
build on previous efforts for error classification.
To confirm that the model is not simply replicating
these results and is indeed learning on its own with
the other features detailed, we also trained models
without this JC04-edit feature.
</bodyText>
<subsectionHeader confidence="0.9996055">
3.3 Evaluation of word-level experiments
3.3.1 Word class evaluation
</subsectionHeader>
<bodyText confidence="0.992232181818182">
We first evaluate edit detection accuracy on a per-
word basis. To evaluate our progress identify-
ing word-level error classes, we calculate preci-
sion, recall and F-scores for each labeled class c in
each experimental scenario. As usual, these met-
rics are calculated as ratios of correct, false, and
missed predictions. However, to take advantage of
the double reconstruction annotations provided in
SSR (and more importantly, in recognition of the
occasional ambiguities of reconstruction) we mod-
ified these calculations slightly as shown below.
</bodyText>
<equation confidence="0.996160166666667">
δ(cwi = cg1,i or cwi = cg2,i)
i:c,,,i=c
δ(cwi =� cg1,i and cwi =� cg2,i)
i:c,,,i=c
δ(cwi =� cg1,i)
i:c91,i=c
</equation>
<bodyText confidence="0.9994905">
where cwi is the hypothesized class for wi and cg1,i
and cg2,i are the two reference classes.
</bodyText>
<table confidence="0.9968625">
Setup Class labeled FL RC NC
Train and test on all SUs in the subcorpus
#1 FL+RC+NC 71.0 80.3 47.4
#2 NC - - 42.5
#2 NC+FL 70.8 - 47.5
#2 RC - 84.2 -
#2 RC+FL 67.8 84.7 -
Train and test on errorful SUs
#3 FL+RC+NC 91.6 84.1 52.2
#4 FL+RC+NC 44.1 69.3 31.6
#5 NC - - 73.8
#6 w/full test - - 39.2
#5 NC+FL 90.7 - 69.8
#6 w/full test 50.1 - 38.5
#5 RC - 88.7 -
#6 w/full test - 75.0 -
#5 RC+FL 92.3 87.4 -
#6 w/full test 62.3 73.9 -
</table>
<tableCaption confidence="0.8173222">
Table 4: Word-level error prediction F1-score re-
sults: Data variation. The first column identifies
which data setup was used for each experiment
(Table 3). The highest performing result for each
class in the first set of experiments has been high-
</tableCaption>
<equation confidence="0.88833775">
lighted.
�corr(c) =
�false(c) =
�miss(c) =
</equation>
<footnote confidence="0.8865345">
3Using both annotated SSR reference reconstructions for
each utterance
</footnote>
<bodyText confidence="0.731583">
Analysis: Experimental results can be seen in
Tables 4 and 5. Table 4 shows the impact of
</bodyText>
<page confidence="0.974827">
260
</page>
<table confidence="0.9998224">
Features FL RC NC
JC04 only 56.6 69.9-81.9 1.6-21.0
lexical only 56.5 72.7 33.4
LM only 0.0 15.0 0.0
NT bounds only 44.1 35.9 11.5
All but JC04 58.5 79.3 33.1
All but lexical 66.9 76.0 19.6
All but LM 67.9 83.1 41.0
All but NT bounds 61.8 79.4 33.6
All 71.0 80.3 47.4
</table>
<tableCaption confidence="0.988003">
Table 5: Word-level error prediction F-score re-
</tableCaption>
<bodyText confidence="0.960005857142857">
sults: Feature variation. All models were trained
with experimental setup #1 and with the set of fea-
tures identified.
training models for individual features and of con-
straining training data to contain only those ut-
terances known to contain errors. It also demon-
strates the potential impact on error classification
after prefiltering test data to those SUs with er-
rors. Table 5 demonstrates the contribution of each
group of features to our CRF models.
Our results demonstrate the impact of varying
our training data and the number of label classes
trained for. We see in Table 4 from setup #5 exper-
iments that training and testing on error-containing
utterances led to a dramatic improvement in F1-
score. On the other hand, our results for experi-
ments using setup #6 (where training data was fil-
tered to contain errorful data but test data was fully
preserved) are consistently worse than those of ei-
ther setup #2 (where both train and test data was
untouched) or setup #5 (where both train and test
data were prefiltered). The output appears to suf-
fer from sample bias, as the prior of an error oc-
curring in training is much higher than in testing.
This demonstrates that a densely errorful training
set alone cannot improve our results when testing
data conditions do not match training data condi-
tions. However, efforts to identify errorful sen-
tences before determining where errors occur in
those sentences may be worthwhile in preventing
false positives in error-less utterances.
We next consider the impact of the four feature
groups on our prediction results. The CRF model
appears competitive even without the advantage
of building on JC04 results, as seen in Table 54.
</bodyText>
<footnote confidence="0.729445">
4JC04 results are shown as a range for the reasons given in
Section 1.2: since JC04 does not on its own predict whether
an “edit” is a rough copy or non-copy, it is impossible to cal-
</footnote>
<bodyText confidence="0.999946578947368">
Interestingly and encouragingly, the NT bounds
features which indicate the linguistic phrase struc-
tures beginning and ending at each word accord-
ing to an automatic parse were also found to be
highly contribututive for both fillers and non-copy
identification. We believe that further pursuit of
syntactic features, especially those which can take
advantage of the context-free weakness of statisti-
cal parsers like (Charniak, 1999) will be promising
in future research.
It was unexpected that NC classification would
be so sensitive to the loss of lexical features while
RC labeling was generally resilient to the drop-
ping of any feature group. We hypothesize that
for rough copies, the information lost from the re-
moval of the lexical items might have been com-
pensated for by the JC04 features as JC04 per-
formed most strongly on this error type. This
should be further investigated in the future.
</bodyText>
<subsectionHeader confidence="0.848151">
3.3.2 Strict evaluation: SU matching
</subsectionHeader>
<bodyText confidence="0.9998814">
Depending on the downstream task of speech re-
construction, it could be imperative not only to
identify many of the errors in a given spoken ut-
terance, but indeed to identify all errors (and only
those errors), yielding the precise cleaned sentence
that a human annotator might provide.
In these experiments we apply simple cleanup
(as described in Section 1.1) to both JC04 out-
put and the predicted output for each experimental
setup in Table 3, deleting words when their right
boundary class is a filled pause, rough copy or
non-copy.
Taking advantage of the dual annotations for
each sentence in the SSR corpus, we can report
both single-reference and double-reference eval-
uation. Thus, we judge that if a hypothesized
cleaned sentence exactly matches either reference
sentence cleaned in the same manner, we count the
cleaned utterance as correct and otherwise assign
no credit.
Analysis: We see the outcome of this set of ex-
periments in Table 6. While the unfiltered test sets
of JC04-1, setup #1 and setup #2 appear to have
much higher sentence-level cleanup accuracy than
the other experiments, we recall that this is natu-
ral also due to the fact that the majority of these
sentences should not be cleaned at all, besides
culate precision and thus Fl score precisely. Instead, here we
show the resultant Fl for the best case and worst case preci-
sion range.
</bodyText>
<page confidence="0.990873">
261
</page>
<table confidence="0.999460333333333">
Setup Classes deleted # SUs # SUs which match gold % accuracy
Baseline only filled pauses 2288 1800 78.7%
JC04-1 E+FL 2288 1858 81.2%
CRF-#1 RC, NC, and FL 2288 1922 84.0%
CRF-#2 U {RC,NC} 2288 1901 83.1%
Baseline only filled pauses 281 5 1.8%
JC04-2 E+FL 281 126 44.8%
CRF-#3 RC, NC, and FL 281 156 55.5%
CRF-#5 U {RC,NC} 281 132 47.0%
</table>
<tableCaption confidence="0.917845">
Table 6: Word-level error predictions: exact SU match results. JC04-2 was run only on test sentences
known to contain some error to match the conditions of Setup #3 and #5 (from Table 3). For the baselines,
we delete only filled pause filler words like “eh” and “um”.
</tableCaption>
<bodyText confidence="0.997897">
occasional minor filled pause deletions. Look-
ing specifically on cleanup results for sentences
known to contain at least one error, we see, once
again, that our system outperforms our baseline
JC04 system at this task.
</bodyText>
<sectionHeader confidence="0.999862" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999969">
Our first goal in this work was to focus on an area
of disfluency detection currently weak in other
state-of-the-art speaker error detection systems –
false starts – while producing comparable classi-
fication on repetition and revision speaker errors.
Secondly, we attempted to quantify how far delet-
ing identified edits (both RC and NC) and filled
pauses could bring us to full reconstruction of
these sentences.
We’ve shown in Section 3 that by training and
testing on data prefiltered to include only utter-
ances with errors, we can dramatically improve
our results, not only by improving identification
of errors but presumably by reducing the risk of
falsely predicting errors. We would like to further
investigate to understand how well we can auto-
matically identify errorful spoken utterances in a
corpus.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="discussions">
5 Future Work
</sectionHeader>
<bodyText confidence="0.9998845">
This work has shown both achievable and demon-
strably feasible improvements in the area of iden-
tifying and cleaning simple speaker errors. We be-
lieve that improved sentence-level identification of
errorful utterances will help to improve our word-
level error identification and overall reconstruction
accuracy; we will continue to research these areas
in the future. We intend to build on these efforts,
adding prosodic and other features to our CRF and
maximum entropy models,
In addition, as we improve the word-level clas-
sification of rough copies and non-copies, we will
begin to move forward to better identify more
complex speaker errors such as missing argu-
ments, misordered or redundant phrases. We will
also work to apply these results directly to the out-
put of a speech recognition system instead of to
transcripts alone.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999984571428571">
The authors thank our anonymous reviewers for
their valuable comments. Support for this work
was provided by NSF PIRE Grant No. OISE-
0530118. Any opinions, findings, conclusions,
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the supporting agency.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990738">
J. Kathryn Bock. 1982. Toward a cognitive psy-
chology of syntax: Information processing contri-
butions to sentence formulation. Psychological Re-
view, 89(1):1–47, January.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Meeting of the North American
Association for Computational Linguistics.
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Fernanda Ferreira and Karl G. D. Bailey. 2004. Disflu-
encies and human language comprehension. Trends
in Cognitive Science, 8(5):231–237, May.
</reference>
<page confidence="0.960597">
262
</page>
<reference confidence="0.99972656060606">
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference, May.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 517–520,
San Francisco.
Matthias Honal and Tanja Schultz. 2005. Au-
tomatic disfluency removal on recognized spon-
taneous speech – rapid adaptation to speaker-
dependent disfluenices. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Rich Transcription
Fall Workshop.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282–289. Morgan Kauf-
mann, San Francisco, CA.
John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Proceedings of the International Conference on
Spoken Language Processing.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, and Mary Harper. 2004. The ICSI/UW
RT04 structural metadata extraction system. In Rich
Transcription Fall Workshop.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, pages 451–458,
Ann Arbor, MI.
Sharath Rao, Ian Lane, and Tanja Schultz. 2007. Im-
proving spoken language translation by automatic
disfluency removal: Evidence from conversational
speech transcripts. In Machine Translation Summit
XI, Copenhagen, Denmark, October.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In HLT-NAACL.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing, Denver, CO, September.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Qi Zhang and Fuliang Weng. 2005. Exploring fea-
tures for identifying edited regions in disfluent sen-
tences. In Proceedings of the International Work-
shop on Parsing Techniques, pages 179–185.
</reference>
<page confidence="0.9989">
263
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.715347">
<title confidence="0.998785">Reconstructing false start errors in spontaneous speech text</title>
<author confidence="0.999799">Erin Fitzgerald</author>
<affiliation confidence="0.999773">Johns Hopkins University</affiliation>
<address confidence="0.999747">Baltimore, MD, USA</address>
<email confidence="0.99984">erinf@jhu.edu</email>
<author confidence="0.751351">Keith Hall</author>
<affiliation confidence="0.99055">Google, Inc.</affiliation>
<address confidence="0.981809">Zurich, Switzerland</address>
<email confidence="0.9998">kbhall@google.com</email>
<author confidence="0.999949">Frederick Jelinek</author>
<affiliation confidence="0.999866">Johns Hopkins University</affiliation>
<address confidence="0.9998">Baltimore, MD, USA</address>
<email confidence="0.999878">jelinek@jhu.edu</email>
<abstract confidence="0.998712095238095">This paper presents a conditional random field-based approach for identifying speaker-produced disfluencies (i.e. if and where they occur) in spontaneous speech transcripts. We emphasize false start regions, which are often missed in current disfluency identification approaches as they lack lexical or structural similarity to the speech immediately following. We find that combining lexical, syntactic, and language model-related features with the output of a state-of-the-art disfluency identification system improves overall word-level identification of these and other errors. Improvements are reinforced under a stricter evaluation metric requiring exact matches between cleaned sentences annotator-produced reconstructions, and altogether show promise for general reconstruction efforts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Kathryn Bock</author>
</authors>
<title>Toward a cognitive psychology of syntax: Information processing contributions to sentence formulation.</title>
<date>1982</date>
<journal>Psychological Review,</journal>
<volume>89</volume>
<issue>1</issue>
<marker>Bock, 1982</marker>
<rawString>J. Kathryn Bock. 1982. Toward a cognitive psychology of syntax: Information processing contributions to sentence formulation. Psychological Review, 89(1):1–47, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>1999</date>
<booktitle>In Meeting of the North American Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17208" citStr="Charniak (1999)" startWordPosition="2776" endWordPosition="2777">as defined below. p(h, t) log p(h, t) p(h)p(t) p(h, t) I log p(t |h)1 p(t) J This aims to capture unexpected n-grams produced by the juxtaposition of the reparandum and the repair. The mutual information feature aims to identify when common words are seen in uncommon context (or, alternatively, penalize rare n-grams normalized for rare words). 2In our model, word historys h encompassed the previous two words (a 3-gram model) and POS history encompassed the previous four POS labels (a 5-gram model) • Non-terminal (NT) ancestors: Given an automatically produced parse of the utterance (using the Charniak (1999) parser trained on Switchboard (SWBD) (Godfrey et al., 1992) CTS data), we determined for each word all NT phrases just completed (if any), all NT phrases about to start to its right (if any), and all NT constituents for which the word is included. (Ferreira and Bailey, 2004) and others have found that false starts and repeats tend to end at certain points of phrases, which we also found to be generally true for the annotated data. Note that the syntactic and POS features we used are extracted from the output of an automatic parser. While we do not expect the parser to always be accurate, espe</context>
<context position="24847" citStr="Charniak, 1999" startWordPosition="4100" endWordPosition="4101">4. 4JC04 results are shown as a range for the reasons given in Section 1.2: since JC04 does not on its own predict whether an “edit” is a rough copy or non-copy, it is impossible to calInterestingly and encouragingly, the NT bounds features which indicate the linguistic phrase structures beginning and ending at each word according to an automatic parse were also found to be highly contribututive for both fillers and non-copy identification. We believe that further pursuit of syntactic features, especially those which can take advantage of the context-free weakness of statistical parsers like (Charniak, 1999) will be promising in future research. It was unexpected that NC classification would be so sensitive to the loss of lexical features while RC labeling was generally resilient to the dropping of any feature group. We hypothesize that for rough copies, the information lost from the removal of the lexical items might have been compensated for by the JC04 features as JC04 performed most strongly on this error type. This should be further investigated in the future. 3.3.2 Strict evaluation: SU matching Depending on the downstream task of speech reconstruction, it could be imperative not only to id</context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A maximum-entropyinspired parser. In Meeting of the North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>Stephanie Strassel</author>
<author>Mohamed Maamouri</author>
<author>Shudong Huang</author>
<author>James Fiumara</author>
<author>David Graff</author>
<author>Kevin Walker</author>
<author>Mark Liberman</author>
</authors>
<title>Linguistic resource creation and distribution for EARS. In Rich Transcription Fall Workshop.</title>
<date>2004</date>
<contexts>
<context position="10770" citStr="Cieri et al., 2004" startWordPosition="1726" endWordPosition="1729">e starts; however, we consider these results to be further motivation for the work. Surveying these results, we conclude that there is still much room for improvement in the field of simple disfluency identification, especially the cases of detecting non-copy reparandum and learning how and where to implement nondeletion reconstruction changes. 2 Approach 2.1 Data We conducted our experiments on the recently released Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a mediumsized set of disfluency annotations atop Fisher conversational telephone speech (CTS) data (Cieri et al., 2004). Advantages of the SSR data include • aligned parallel original and cleaned sentences • several levels of error annotations, allowing for a coarse-to-fine reconstruction approach • multiple annotations per sentence reflecting the occasional ambiguity of corrections As reconstructions are sometimes nondeterministic (illustrated in EX6 in Section 1.1), the SSR provides two manual reconstructions for each utterance in the data. We use these dual annotations to learn complementary approaches in training and to allow for more accurate evaluation. The SSR corpus does not explicitly label all repara</context>
</contexts>
<marker>Cieri, Strassel, Maamouri, Huang, Fiumara, Graff, Walker, Liberman, 2004</marker>
<rawString>Christopher Cieri, Stephanie Strassel, Mohamed Maamouri, Shudong Huang, James Fiumara, David Graff, Kevin Walker, and Mark Liberman. 2004. Linguistic resource creation and distribution for EARS. In Rich Transcription Fall Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernanda Ferreira</author>
<author>Karl G D Bailey</author>
</authors>
<title>Disfluencies and human language comprehension.</title>
<date>2004</date>
<journal>Trends in Cognitive Science,</journal>
<volume>8</volume>
<issue>5</issue>
<contexts>
<context position="17484" citStr="Ferreira and Bailey, 2004" startWordPosition="2823" endWordPosition="2826">on context (or, alternatively, penalize rare n-grams normalized for rare words). 2In our model, word historys h encompassed the previous two words (a 3-gram model) and POS history encompassed the previous four POS labels (a 5-gram model) • Non-terminal (NT) ancestors: Given an automatically produced parse of the utterance (using the Charniak (1999) parser trained on Switchboard (SWBD) (Godfrey et al., 1992) CTS data), we determined for each word all NT phrases just completed (if any), all NT phrases about to start to its right (if any), and all NT constituents for which the word is included. (Ferreira and Bailey, 2004) and others have found that false starts and repeats tend to end at certain points of phrases, which we also found to be generally true for the annotated data. Note that the syntactic and POS features we used are extracted from the output of an automatic parser. While we do not expect the parser to always be accurate, especially when parsing errorful text, we hope that the parser will at least be consistent in the types of structures it assigns to particular error phenomena. We use these features in the hope of taking advantage of that consistency. 3.2 Experimental setup In these experiments, </context>
</contexts>
<marker>Ferreira, Bailey, 2004</marker>
<rawString>Fernanda Ferreira and Karl G. D. Bailey. 2004. Disfluencies and human language comprehension. Trends in Cognitive Science, 8(5):231–237, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Fitzgerald</author>
<author>Frederick Jelinek</author>
</authors>
<title>Linguistic resources for reconstructing spontaneous speech text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference,</booktitle>
<contexts>
<context position="8882" citStr="Fitzgerald and Jelinek, 2008" startWordPosition="1417" endWordPosition="1420">100.0% 67.6% Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system. 1 he that ’s uh that ’s a relief 2 E E E FL - - - - 3 NC RC RC FL - - - - Figure 2: Example of word class and refined word class labels, where - denotes a non-error, FL denotes a filler, E generally denotes reparanda, and RC and NC indicate rough copy and non-copy speaker errors, respectively. for improvement, we used the top-performing1 JC04 noisy channel TAG edit detector to produce edit detection analyses on the test segment of the Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008). Table 1 demonstrates the performance of this system for detecting filled pause fillers, discourse marker fillers, and edit words. The results of a more granular analysis compared to a hand-refined reference (as shown in line 3 of Figure 2) are shown in Table 2. The reader will recall that precision P is defined as P = |correct| |correct|+|false |and recall R = |correct| |correct|+|miss|. We denote the harmonic mean of P and R as Fscore F and calculate it F = 2 1/P+1/R. As expected given the assumptions of the TAG approach, JC04 identifies repetitions and most revisions in the SSR data, but l</context>
<context position="10649" citStr="Fitzgerald and Jelinek, 2008" startWordPosition="1708" endWordPosition="1711">alse RCs or false NCs. This will unfortunately hinder us from using JC04 as a direct baseline comparison in our work targeting false starts; however, we consider these results to be further motivation for the work. Surveying these results, we conclude that there is still much room for improvement in the field of simple disfluency identification, especially the cases of detecting non-copy reparandum and learning how and where to implement nondeletion reconstruction changes. 2 Approach 2.1 Data We conducted our experiments on the recently released Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a mediumsized set of disfluency annotations atop Fisher conversational telephone speech (CTS) data (Cieri et al., 2004). Advantages of the SSR data include • aligned parallel original and cleaned sentences • several levels of error annotations, allowing for a coarse-to-fine reconstruction approach • multiple annotations per sentence reflecting the occasional ambiguity of corrections As reconstructions are sometimes nondeterministic (illustrated in EX6 in Section 1.1), the SSR provides two manual reconstructions for each utterance in the data. We use these dual annotations to learn complement</context>
</contexts>
<marker>Fitzgerald, Jelinek, 2008</marker>
<rawString>Erin Fitzgerald and Frederick Jelinek. 2008. Linguistic resources for reconstructing spontaneous speech text. In Proceedings of the Language Resources and Evaluation Conference, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Fitzgerald</author>
</authors>
<title>Reconstructing Spontaneous Speech.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="3009" citStr="Fitzgerald, 2009" startWordPosition="438" endWordPosition="439">MT). Rao et al. (2007) gave evidence that simple disfluency removal from transcripts can improve BLEU (a standard SMT evaluation metric) up to 8% for sentences with disfluencies. The presence of disfluencies were found to hurt SMT in two ways: making utterances longer without adding semantic content (and sometimes adding false content) and exacerbating the data mismatch between the spontaneous input and the clean text training data. While full speech reconstruction would likely require a range of string transformations and potentially deep syntactic and semantic analysis of the errorful text (Fitzgerald, 2009), in this work we will first attempt to resolve less complex errors, corrected by deletion alone, in a given manuallytranscribed utterance. We build on efforts from (Johnson et al., 2004), aiming to improve overall recall – especially of false start or non-copy errors – while concurrently maintaining or improving precision. 1.1 Error classes in spontaneous speech Common simple disfluencies in sentence-like utterances (SUs) include filler words (i.e. “um”, “ah”, and discourse markers like “you know”), as well as speaker edits consisting of a reparandum, an interruption point (IP), an optional i</context>
</contexts>
<marker>Fitzgerald, 2009</marker>
<rawString>Erin Fitzgerald. 2009. Reconstructing Spontaneous Speech. Ph.D. thesis, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>517--520</pages>
<location>San Francisco.</location>
<contexts>
<context position="12433" citStr="Godfrey et al., 1992" startWordPosition="1984" endWordPosition="1987">us Speech Reconstruction corpus is partitioned into three subcorpora: 17,162 training sentences (119,693 words), 2,191 sentences (14,861 words) in the development set, and 2,288 sentences (15,382 words) in the test set. Approximately 17% of the total utterances contain a reparandum-type error. The output of the JC04 model ((Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments. The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al., 1992) data as described in (Johnson and Charniak, 2004). Key differences in these corpora, besides the form of their annotations, include: • SSR aims to correct speech output, while SWBD edit annotation aims to identify reparandum structures specifically. Thus, as mentioned, SSR only marks those reparanda which annotators believe must be deleted to generate a grammatical and contentpreserving reconstruction. • SSR considers some phenomena such as leading conjunctions (“and i did” --+ “i did”) to be fillers, while SWBD does not. • SSR includes more complex error identification and correction, though</context>
<context position="17268" citStr="Godfrey et al., 1992" startWordPosition="2783" endWordPosition="2786"> I log p(t |h)1 p(t) J This aims to capture unexpected n-grams produced by the juxtaposition of the reparandum and the repair. The mutual information feature aims to identify when common words are seen in uncommon context (or, alternatively, penalize rare n-grams normalized for rare words). 2In our model, word historys h encompassed the previous two words (a 3-gram model) and POS history encompassed the previous four POS labels (a 5-gram model) • Non-terminal (NT) ancestors: Given an automatically produced parse of the utterance (using the Charniak (1999) parser trained on Switchboard (SWBD) (Godfrey et al., 1992) CTS data), we determined for each word all NT phrases just completed (if any), all NT phrases about to start to its right (if any), and all NT constituents for which the word is included. (Ferreira and Bailey, 2004) and others have found that false starts and repeats tend to end at certain points of phrases, which we also found to be generally true for the annotated data. Note that the syntactic and POS features we used are extracted from the output of an automatic parser. While we do not expect the parser to always be accurate, especially when parsing errorful text, we hope that the parser w</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 517–520, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Honal</author>
<author>Tanja Schultz</author>
</authors>
<title>Automatic disfluency removal on recognized spontaneous speech – rapid adaptation to speakerdependent disfluenices.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="6307" citStr="Honal and Schultz, 2005" startWordPosition="977" endWordPosition="980">X4) however demonstrates a case when the reparandum may be considered to have unique and preservable content of its own. Future work should address how to most appropriately reconstruct speech in this and similar cases; this initial work will for risk information loss as we identify and delete these reparandum regions. 1.2 Related Work Stochastic approaches for simple disfluency detection use features such as lexical form, acoustic cues, and rule-based knowledge. Most state-ofthe-art methods for edit region detection such as (Johnson and Charniak, 2004; Zhang and Weng, 2005; Liu et al., 2004; Honal and Schultz, 2005) model speech disfluencies as a noisy channel model. In a noisy channel model we assume that an unknown but fluent string F has passed through a disfluency-adding channel to produce the observed disfluent string D, and we then aim to recover the most likely input string F�, defined as F� = argmaxFP(F|D) = argmaxFP(D|F)P(F) where P(F) represents a language model defining a probability distribution over fluent “source” strings F, and P(D|F) is the channel model defining a conditional probability distribution of observed sentences D which may contain the types of construction errors described in </context>
</contexts>
<marker>Honal, Schultz, 2005</marker>
<rawString>Matthias Honal and Tanja Schultz. 2005. Automatic disfluency removal on recognized spontaneous speech – rapid adaptation to speakerdependent disfluenices. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A TAGbased noisy channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6241" citStr="Johnson and Charniak, 2004" startWordPosition="965" endWordPosition="968">r new content, and thus can be safely deleted. The second example (EX4) however demonstrates a case when the reparandum may be considered to have unique and preservable content of its own. Future work should address how to most appropriately reconstruct speech in this and similar cases; this initial work will for risk information loss as we identify and delete these reparandum regions. 1.2 Related Work Stochastic approaches for simple disfluency detection use features such as lexical form, acoustic cues, and rule-based knowledge. Most state-ofthe-art methods for edit region detection such as (Johnson and Charniak, 2004; Zhang and Weng, 2005; Liu et al., 2004; Honal and Schultz, 2005) model speech disfluencies as a noisy channel model. In a noisy channel model we assume that an unknown but fluent string F has passed through a disfluency-adding channel to produce the observed disfluent string D, and we then aim to recover the most likely input string F�, defined as F� = argmaxFP(F|D) = argmaxFP(D|F)P(F) where P(F) represents a language model defining a probability distribution over fluent “source” strings F, and P(D|F) is the channel model defining a conditional probability distribution of observed sentences </context>
<context position="12165" citStr="Johnson and Charniak, 2004" startWordPosition="1936" endWordPosition="1939">e annotator decisions regarding whether or not to delete reparandum regions when labeling them as such. Fortunately, we expect this to have a negligible effect here as we will emphasize utterances which do not require more complex reconstructions in this work. The Spontaneous Speech Reconstruction corpus is partitioned into three subcorpora: 17,162 training sentences (119,693 words), 2,191 sentences (14,861 words) in the development set, and 2,288 sentences (15,382 words) in the test set. Approximately 17% of the total utterances contain a reparandum-type error. The output of the JC04 model ((Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments. The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al., 1992) data as described in (Johnson and Charniak, 2004). Key differences in these corpora, besides the form of their annotations, include: • SSR aims to correct speech output, while SWBD edit annotation aims to identify reparandum structures specifically. Thus, as mentioned, SSR only marks those reparanda which annotators believe must </context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A TAGbased noisy channel model of speech repairs. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
<author>Matthew Lease</author>
</authors>
<title>An improved model for recognizing disfluencies in conversational speech.</title>
<date>2004</date>
<booktitle>In Rich Transcription Fall Workshop.</booktitle>
<contexts>
<context position="3196" citStr="Johnson et al., 2004" startWordPosition="467" endWordPosition="470">presence of disfluencies were found to hurt SMT in two ways: making utterances longer without adding semantic content (and sometimes adding false content) and exacerbating the data mismatch between the spontaneous input and the clean text training data. While full speech reconstruction would likely require a range of string transformations and potentially deep syntactic and semantic analysis of the errorful text (Fitzgerald, 2009), in this work we will first attempt to resolve less complex errors, corrected by deletion alone, in a given manuallytranscribed utterance. We build on efforts from (Johnson et al., 2004), aiming to improve overall recall – especially of false start or non-copy errors – while concurrently maintaining or improving precision. 1.1 Error classes in spontaneous speech Common simple disfluencies in sentence-like utterances (SUs) include filler words (i.e. “um”, “ah”, and discourse markers like “you know”), as well as speaker edits consisting of a reparandum, an interruption point (IP), an optional interregnum (like “I mean”), and a repair region (Shriberg, 1994), as seen in Figure 1. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 255–263, Athens, Greece</context>
<context position="7569" citStr="Johnson et al. (2004)" startWordPosition="1191" endWordPosition="1194"> is a word-level tagging of the error condition of each word in the sequence, as seen in line 2 of Figure 2. The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements. The TAG approach models the crossed word dependencies observed when the reparandum incorporates the same or very similar words in roughly the same word order, which JC04 refer to as a rough copy. Our version of this system does not use external features such as prosodic classes, as they use in Johnson et al. (2004), but otherwise appears to produce comparable results to those reported. While much progress has been made in simple disfluency detection in the last decade, even top-performing systems continue to be ineffective at identifying words in reparanda. To better understand these problems and identify areas [that&apos;s] IP |{z } a relief |{z } z}|{ �{uh} that&apos;s reparandum |{z} repair interregnum 256 Label % of words Precision Recall F-score Fillers 5.6% 64% 59% 61% Edit (reparandum) 7.8% 85% 68% 75% Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system. Label % of edits R</context>
<context position="16286" citStr="Johnson et al., 2004" startWordPosition="2617" endWordPosition="2620">e SSR data – in the hopes of building on these results. Altogether, the following features F were extracted for each observation xi. • Lexical features, including – the lexical item and part-of-speech (POS) for tokens ti and ti+1, – distance from previous token to the next matching word/POS, – whether previous token is partial word and the distance to the next word with same start, and – the token’s (normalized) position within the sentence. • JC04-edit: whether previous, next, or current word is identified by the JC04 system as an edit and/or a filler (fillers are classified as described in (Johnson et al., 2004)). • Language model features: the unigram log probability of the next word (or POS) token p(t), the token log probability conditioned on its multi-token history h (p(t|h))2, and the log ratio of the two (log p(t 1 )) to serve as pW an approximation for mutual information between the token and its history, as defined below. p(h, t) log p(h, t) p(h)p(t) p(h, t) I log p(t |h)1 p(t) J This aims to capture unexpected n-grams produced by the juxtaposition of the reparandum and the repair. The mutual information feature aims to identify when common words are seen in uncommon context (or, alternativel</context>
</contexts>
<marker>Johnson, Charniak, Lease, 2004</marker>
<rawString>Mark Johnson, Eugene Charniak, and Matthew Lease. 2004. An improved model for recognizing disfluencies in conversational speech. In Rich Transcription Fall Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="13390" citStr="Lafferty et al., 2001" startWordPosition="2134" endWordPosition="2137">deleted to generate a grammatical and contentpreserving reconstruction. • SSR considers some phenomena such as leading conjunctions (“and i did” --+ “i did”) to be fillers, while SWBD does not. • SSR includes more complex error identification and correction, though these effects should be negligible in the experimental setup presented herein. While we hope to adapt the trained JC04 model to SSR data in the future, for now these difference in task, evaluation, and training data will prevent direct comparison between JC04 and our results. 2.2 Conditional random fields Conditional random fields (Lafferty et al., 2001), or CRFs, are undirected graphical models whose prediction of a hidden variable sequence Y is globally conditioned on a given observation sequence X, as shown in Figure 3. Each observed Figure 3: Illustration of a conditional random field. For this work, x represents observable inputs for each word as described in Section 3.1 and y represents the error class of each word (Section 3.2). state xi E X is composed of the corresponding word wi and a set of additional features Fi, detailed in Section 3.1. The conditional probability of this model can be represented as 1 p�(Y |X) = Zλ(X) exp( AkFk(X</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Automatic grammar correction for second-language learners.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="1922" citStr="Lee and Seneff, 2006" startWordPosition="271" endWordPosition="274">r using filler words). A cleaner speech transcript would allow for more accurate language processing as needed for natural language processing tasks such as machine translation and conversation summarization which often assume a grammatical sentence as input. A system would accomplish reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and contentpreserving text, the message that the speaker intended to convey. Such a system could also be applied not only to spontaneous English speech, but to correct common mistakes made by non-native speakers (Lee and Seneff, 2006), and possibly extended to non-English speaker errors. A key motivation for this work is the hope that a cleaner, reconstructed speech transcript will allow for simpler and more accurate human and natural language processing, as needed for applications like machine translation, question answering, text summarization, and paraphrasing which often assume a grammatical sentence as input. This benefit has been directly demonstrated for statistical machine translation (SMT). Rao et al. (2007) gave evidence that simple disfluency removal from transcripts can improve BLEU (a standard SMT evaluation m</context>
</contexts>
<marker>Lee, Seneff, 2006</marker>
<rawString>John Lee and Stephanie Seneff. 2006. Automatic grammar correction for second-language learners. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Barbara Peskin</author>
<author>Mary Harper</author>
</authors>
<title>The ICSI/UW RT04 structural metadata extraction system. In Rich Transcription Fall Workshop.</title>
<date>2004</date>
<contexts>
<context position="6281" citStr="Liu et al., 2004" startWordPosition="973" endWordPosition="976"> second example (EX4) however demonstrates a case when the reparandum may be considered to have unique and preservable content of its own. Future work should address how to most appropriately reconstruct speech in this and similar cases; this initial work will for risk information loss as we identify and delete these reparandum regions. 1.2 Related Work Stochastic approaches for simple disfluency detection use features such as lexical form, acoustic cues, and rule-based knowledge. Most state-ofthe-art methods for edit region detection such as (Johnson and Charniak, 2004; Zhang and Weng, 2005; Liu et al., 2004; Honal and Schultz, 2005) model speech disfluencies as a noisy channel model. In a noisy channel model we assume that an unknown but fluent string F has passed through a disfluency-adding channel to produce the observed disfluent string D, and we then aim to recover the most likely input string F�, defined as F� = argmaxFP(F|D) = argmaxFP(D|F)P(F) where P(F) represents a language model defining a probability distribution over fluent “source” strings F, and P(D|F) is the channel model defining a conditional probability distribution of observed sentences D which may contain the types of constru</context>
<context position="14407" citStr="Liu et al., 2004" startWordPosition="2313" endWordPosition="2316"> X is composed of the corresponding word wi and a set of additional features Fi, detailed in Section 3.1. The conditional probability of this model can be represented as 1 p�(Y |X) = Zλ(X) exp( AkFk(X, Y )) (1) k where Zλ(X) is a global normalization factor and A = (A1 ... AK) are model parameters related to each feature function Fk(X, Y ). CRFs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such as partof-speech tagging and shallow parsing (Sha and Pereira, 2003), as well as sentence boundary detection (Liu et al., 2005; Liu et al., 2004). These models have the advantage that they model sequential context (like hidden Markov models (HMMs)) but are discriminative rather than generative and have a less restricted feature set. Additionally, as compared to HMMs, CRFs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities P(E|O). We used the GRMM package (Sutton, 2006) to implement our CRF models, each using a zeromean Gaussian prior to reduce over-fitting our model. No feature reduction is employed, except where indicated. 3 Word-Level ID Experiments 3.1 Feature functions We aim to train </context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Peskin, Harper, 2004</marker>
<rawString>Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Barbara Peskin, and Mary Harper. 2004. The ICSI/UW RT04 structural metadata extraction system. In Rich Transcription Fall Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Using conditional random fields for sentence boundary detection in speech.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>451--458</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="14388" citStr="Liu et al., 2005" startWordPosition="2309" endWordPosition="2312">n 3.2). state xi E X is composed of the corresponding word wi and a set of additional features Fi, detailed in Section 3.1. The conditional probability of this model can be represented as 1 p�(Y |X) = Zλ(X) exp( AkFk(X, Y )) (1) k where Zλ(X) is a global normalization factor and A = (A1 ... AK) are model parameters related to each feature function Fk(X, Y ). CRFs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such as partof-speech tagging and shallow parsing (Sha and Pereira, 2003), as well as sentence boundary detection (Liu et al., 2005; Liu et al., 2004). These models have the advantage that they model sequential context (like hidden Markov models (HMMs)) but are discriminative rather than generative and have a less restricted feature set. Additionally, as compared to HMMs, CRFs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities P(E|O). We used the GRMM package (Sutton, 2006) to implement our CRF models, each using a zeromean Gaussian prior to reduce over-fitting our model. No feature reduction is employed, except where indicated. 3 Word-Level ID Experiments 3.1 Feature functio</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 451–458, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharath Rao</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Improving spoken language translation by automatic disfluency removal: Evidence from conversational speech transcripts.</title>
<date>2007</date>
<booktitle>In Machine Translation</booktitle>
<location>Summit XI, Copenhagen, Denmark,</location>
<contexts>
<context position="2414" citStr="Rao et al. (2007)" startWordPosition="345" endWordPosition="348">ied not only to spontaneous English speech, but to correct common mistakes made by non-native speakers (Lee and Seneff, 2006), and possibly extended to non-English speaker errors. A key motivation for this work is the hope that a cleaner, reconstructed speech transcript will allow for simpler and more accurate human and natural language processing, as needed for applications like machine translation, question answering, text summarization, and paraphrasing which often assume a grammatical sentence as input. This benefit has been directly demonstrated for statistical machine translation (SMT). Rao et al. (2007) gave evidence that simple disfluency removal from transcripts can improve BLEU (a standard SMT evaluation metric) up to 8% for sentences with disfluencies. The presence of disfluencies were found to hurt SMT in two ways: making utterances longer without adding semantic content (and sometimes adding false content) and exacerbating the data mismatch between the spontaneous input and the clean text training data. While full speech reconstruction would likely require a range of string transformations and potentially deep syntactic and semantic analysis of the errorful text (Fitzgerald, 2009), in </context>
</contexts>
<marker>Rao, Lane, Schultz, 2007</marker>
<rawString>Sharath Rao, Ian Lane, and Tanja Schultz. 2007. Improving spoken language translation by automatic disfluency removal: Evidence from conversational speech transcripts. In Machine Translation Summit XI, Copenhagen, Denmark, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="14330" citStr="Sha and Pereira, 2003" startWordPosition="2299" endWordPosition="2302">ection 3.1 and y represents the error class of each word (Section 3.2). state xi E X is composed of the corresponding word wi and a set of additional features Fi, detailed in Section 3.1. The conditional probability of this model can be represented as 1 p�(Y |X) = Zλ(X) exp( AkFk(X, Y )) (1) k where Zλ(X) is a global normalization factor and A = (A1 ... AK) are model parameters related to each feature function Fk(X, Y ). CRFs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such as partof-speech tagging and shallow parsing (Sha and Pereira, 2003), as well as sentence boundary detection (Liu et al., 2005; Liu et al., 2004). These models have the advantage that they model sequential context (like hidden Markov models (HMMs)) but are discriminative rather than generative and have a less restricted feature set. Additionally, as compared to HMMs, CRFs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities P(E|O). We used the GRMM package (Sutton, 2006) to implement our CRF models, each using a zeromean Gaussian prior to reduce over-fitting our model. No feature reduction is employed, except where </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="3673" citStr="Shriberg, 1994" startWordPosition="541" endWordPosition="542">ess complex errors, corrected by deletion alone, in a given manuallytranscribed utterance. We build on efforts from (Johnson et al., 2004), aiming to improve overall recall – especially of false start or non-copy errors – while concurrently maintaining or improving precision. 1.1 Error classes in spontaneous speech Common simple disfluencies in sentence-like utterances (SUs) include filler words (i.e. “um”, “ah”, and discourse markers like “you know”), as well as speaker edits consisting of a reparandum, an interruption point (IP), an optional interregnum (like “I mean”), and a repair region (Shriberg, 1994), as seen in Figure 1. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 255–263, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 255 Figure 1: Typical edit region structure. In these and other examples, reparandum regions are in brackets (’[’, ’]’), interregna are in braces (&apos;I&apos; &apos; &apos;f&apos;, ’}’), and interruption points are marked by ’+’. These reparanda, or edit regions, can be classified into three main groups: 1. In a repetition (above), the repair phrase is approximately identical to the reparandum. 2. In a revision, the repa</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="15486" citStr="Stolcke, 2002" startWordPosition="2481" endWordPosition="2482">our model. No feature reduction is employed, except where indicated. 3 Word-Level ID Experiments 3.1 Feature functions We aim to train our CRF model with sets of features with orthogonal analyses of the errorful text, integrating knowledge from multiple sources. While we anticipate that repetitions and other rough copies will be identified primarily by lexical 258 and local context features, this will not necessarily help for false starts with little or no lexical overlap between reparandum and repair. To catch these errors, we add both language model features (trained with the SRILM toolkit (Stolcke, 2002) on SWBD data with EDITED reparandum nodes removed), and syntactic features to our model. We also included the output of the JC04 system – which had generally high precision on the SSR data – in the hopes of building on these results. Altogether, the following features F were extracted for each observation xi. • Lexical features, including – the lexical item and part-of-speech (POS) for tokens ti and ti+1, – distance from previous token to the next matching word/POS, – whether previous token is partial word and the distance to the next word with same start, and – the token’s (normalized) posit</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
</authors>
<title>GRMM: A graphical models toolkit.</title>
<date>2006</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="14782" citStr="Sutton, 2006" startWordPosition="2370" endWordPosition="2371">ks in natural language processing, especially those involving tagging words with labels such as partof-speech tagging and shallow parsing (Sha and Pereira, 2003), as well as sentence boundary detection (Liu et al., 2005; Liu et al., 2004). These models have the advantage that they model sequential context (like hidden Markov models (HMMs)) but are discriminative rather than generative and have a less restricted feature set. Additionally, as compared to HMMs, CRFs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities P(E|O). We used the GRMM package (Sutton, 2006) to implement our CRF models, each using a zeromean Gaussian prior to reduce over-fitting our model. No feature reduction is employed, except where indicated. 3 Word-Level ID Experiments 3.1 Feature functions We aim to train our CRF model with sets of features with orthogonal analyses of the errorful text, integrating knowledge from multiple sources. While we anticipate that repetitions and other rough copies will be identified primarily by lexical 258 and local context features, this will not necessarily help for false starts with little or no lexical overlap between reparandum and repair. To</context>
</contexts>
<marker>Sutton, 2006</marker>
<rawString>Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Fuliang Weng</author>
</authors>
<title>Exploring features for identifying edited regions in disfluent sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Techniques,</booktitle>
<pages>179--185</pages>
<contexts>
<context position="6263" citStr="Zhang and Weng, 2005" startWordPosition="969" endWordPosition="972">be safely deleted. The second example (EX4) however demonstrates a case when the reparandum may be considered to have unique and preservable content of its own. Future work should address how to most appropriately reconstruct speech in this and similar cases; this initial work will for risk information loss as we identify and delete these reparandum regions. 1.2 Related Work Stochastic approaches for simple disfluency detection use features such as lexical form, acoustic cues, and rule-based knowledge. Most state-ofthe-art methods for edit region detection such as (Johnson and Charniak, 2004; Zhang and Weng, 2005; Liu et al., 2004; Honal and Schultz, 2005) model speech disfluencies as a noisy channel model. In a noisy channel model we assume that an unknown but fluent string F has passed through a disfluency-adding channel to produce the observed disfluent string D, and we then aim to recover the most likely input string F�, defined as F� = argmaxFP(F|D) = argmaxFP(D|F)P(F) where P(F) represents a language model defining a probability distribution over fluent “source” strings F, and P(D|F) is the channel model defining a conditional probability distribution of observed sentences D which may contain th</context>
</contexts>
<marker>Zhang, Weng, 2005</marker>
<rawString>Qi Zhang and Fuliang Weng. 2005. Exploring features for identifying edited regions in disfluent sentences. In Proceedings of the International Workshop on Parsing Techniques, pages 179–185.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>