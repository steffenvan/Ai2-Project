<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049738">
<title confidence="0.982163">
Boosted Decision Graphs for NLP Learning Tasks
</title>
<author confidence="0.995115">
Jon D. Patrick and Ishaan Goyal
</author>
<affiliation confidence="0.99809">
Basser Department of Computer Science
University of Sydney, NSW, Australia
</affiliation>
<note confidence="0.459751">
fjonpat,ishaaq cs.usyd. edu. an
</note>
<sectionHeader confidence="0.900786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883066666667">
This paper reports the implementation of the
AdaBoost algorithm on decision graphs, op-
timized using the Minimum Message Length
Principle. The AdaBoost algorithm, which
we call 1-Stage Boosting, is shown to improve
the accuracy of decision graphs, along with
we another technique which we combine with
AdaBoost and call 2-Stage Boosting. which
shows the greater improvement. Empirical tests
demonstrate that both 1-Stage and 2-Stage
Boosting techniques perform better than the
boosted C4.5 algorithm. However the boost-
ing has not shown a significant improvement for
NLP tasks with a high disjunction of attribute
space.
</bodyText>
<sectionHeader confidence="0.998325" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999938190476191">
In a wide variety of classification problems,
boosting techniques have proven to be an ef-
fective method to significantly reduce the error
of any weak learning algorithm. While the Ad-
aBoost algorithm (Freund and Schapire, 1995)
has been used to improve the accuracy of a deci-
sion tree algorithm (Quinlan and Rivest, 1989),
which uses the Minimum Description Length
Principle (MDL), little is known about it&apos;s ef-
fectiveness on the decision graphs.
This paper examines the application of the
AdaBoost technique to the decision graph algo-
rithm (Oliver and Wallace, 1991; Wallace and
Patrick, 1993), which infers classification graphs
from data by combining the Minimum Message
Length Principle (MML) (Wallace and Boul-
ton, 1968) with the recursive partitioning algo-
rithm. In this paper we present two variants of
the boosted decision graphs, which we call as
1-Stage Boosting and 2-stage Boosting respec-
tively.
</bodyText>
<subsectionHeader confidence="0.998735">
1.1 Decision Graphs
</subsectionHeader>
<bodyText confidence="0.999958805555555">
There have been a number of attempts to ex-
tend the representational power of the decision
trees by allowing a node to have more than one
parent. Oliver and Wallace (1991), introduced
decision graphs which are generalizations of de-
cision trees, having decision nodes and leaves.
They optimize the decision graphs based on the
MML Principle. The feature that distinguishes
decision graphs from decision trees is that the
former may also contain joins. A join is rep-
resented by two nodes having a common child,
and this specifies that two subsets have some
common properties, and hence can be consid-
ered as one subset. The manner in which the
objects are allocated to leaf nodes in decision
graphs is the same as decision trees. The deci-
sion graph offers an elegant solution to the repli-
cation and fragmentation problems faced by de-
cision trees.
To discover classification graphs of short mes-
sage length, the DGRAPH program of Oliver
and Wallace (1991) starts with a single leaf at
the root and grows a graph by repeatedly apply-
ing the modification that results in the greatest
savings in the message length. Modifications
considered either replace a leaf with a decision
node or join two leaves together. The join prob-
ability, RI is taken as an argument to the al-
gorithm. The procedure halts when the best
modification fails to reduce the message length
of the resulting graph. To assist in preventing
the greedy algorithm from becoming trapped in
local message length minima, a lookahead pa-
rameter, /, is used to calculate message length
savings over the next lower / levels of the tree
under each possible modification.
</bodyText>
<sectionHeader confidence="0.910743" genericHeader="method">
2 BOOSTING DECISION
</sectionHeader>
<bodyText confidence="0.8921228">
GRAPHS - DGRAPH Parameters
The coding scheme for decision graphs has two
important parameters, one is the RI probability
of join, and the second is leaf node class purity,
alpha.
</bodyText>
<subsectionHeader confidence="0.998917">
2.1 Probability of a Join
</subsectionHeader>
<bodyText confidence="0.999964277777778">
The probability of a join, RI determines p, the
probability of leaf nodes, and, pd the probabil-
ity of decision nodes in the decision graph. The
change in the value of probability of joins de-
termines the way in which the decision graph is
constructed. A low value for the probability of
joins will encourage graphs with few join nodes,
while a high value will encourage graphs with
many join nodes.
As it is difficult to propose any fixed value
for a probability of joins that will be suitable
for a wide range of applications, we propose to
use the data itself to estimate the value of RI.
Given a data set, we grow decision graphs for
a range of values of RI, say from 0.0, ..., 0.5.
We select the graph with the smallest message
length as being the best graph, and hence derive
the estimate of RI.
</bodyText>
<subsectionHeader confidence="0.9778545">
2.2 Prior Probability of Leaf Node
Class Purity
</subsectionHeader>
<bodyText confidence="0.999979666666667">
The second parameter that effects the coding
of decision graphs is the leaf node purity prior,
alpha. The value of alpha determines how pure
the leaf nodes will be. A heterogeneous class
distribution in a leaf node has a uniform distri-
bution where the value of the prior is one, how-
ever values of prior less than one places greater
weight on more pure (homogeneous) distribu-
tions. As stated in Wallace and Patrick (1993),
the data itself is used to estimate the leaf node
purity prior. Having grown the full tree with
say, alpha = 1, we find the best pruned form
and message length for various values of alpha,
and hence the best value of alpha is the one that
generates the shortest message length.
We use both these parameters, that is, the op-
timal probability of a join, and the estimate of
the leaf node purity prior together with the Ad-
aBoost algorithm to improve the performance of
the DGRAPH algorithm in our 2-Stage Boost-
ing algorithm.
</bodyText>
<subsectionHeader confidence="0.990046">
2.3 1-Stage Boosting
</subsectionHeader>
<bodyText confidence="0.999965272727273">
In our 1-Stage Boosting algorithm, we use Ad-
aBoost to improve the prediction accuracy of
our Decision Graphs. For all our experiments
using 1-Stage Boosting, we use a fixed value of
RI= 0.2. Due to change in the distribution of
weights on the training data, at the start of each
new boosting step we re-adjust the value of al-
pha to 1 (assuming uniform distribution) and
the best value of alpha is calculated according
to the changed distribution, in the same manner
as described in section 2.2.
</bodyText>
<subsectionHeader confidence="0.964164">
2.4 2-Stage Boosting
</subsectionHeader>
<bodyText confidence="0.999991222222222">
Our 2-Stage Boosting algorithm, in each trial,
finds the best probability of join, RI and the best
alpha according to the distribution of weights on
the training data in that particular trial. In this
way it finds the optimal number of join nodes
required for data distribution in each particu-
lar boosting step and hence constructs the best
graph in each trial, using the data itself to esti-
mate the value of p, and alpha.
</bodyText>
<subsectionHeader confidence="0.998991">
2.5 Test Results
</subsectionHeader>
<bodyText confidence="0.9999795">
We have implemented our proposals and com-
pared the results between the five algorithms -
DGRAPH, 1-Stage Boosting, 2-Stage Boosting,
C4.5 and boosted C4.5. The parameter T gov-
erning the number of classifiers generated was
set at 10 as is conventional. All C4.5 param-
eters were set to their default values and we
used pruned trees. Ten complete 10-fold cross-
validations were carried out with each dataset.
All these five algorithms were evaluated on
a representative collection of datasets from the
UCI Machine Learning Repository. All the
datasets show considerable diversity in size,
number of classes, and number and type of
attributes. 2-stage Boosted DGRAPH proved
to be better than boosted C4.5 in 6 out of 7
tests by 2 to 15the seventh. 1-stage Boosted
DGRAPH was better in 5 out of 7 of the tests.
</bodyText>
<subsectionHeader confidence="0.945258">
2.6 Conclusions
</subsectionHeader>
<bodyText confidence="0.9999608">
Our trials over a diverse collection of datasets
have confirmed that boosting improves the ac-
curacy of the DGRAPH noticeably and the re-
sults also show that 2-Stage boosted DGRAPH
is more accurate than the boosted C4.5.
</bodyText>
<sectionHeader confidence="0.996922" genericHeader="method">
3 RESULTS &amp; CONCLUSIONS OF
NLP SHARED TASKS
</sectionHeader>
<bodyText confidence="0.999952828571429">
DGRAPH with 1 and 2-stage boosting and both
C4.5 versions produced identical results. It is
noticeable that they have all failed to perform
better than the baseline for the Shared Task
data sets. This is contrary to our experimental
work that has shown boosting improves clas-
sification performance, for increased errors in
data, and for increases in missing data. Like-
wise boosted DGRAPH has consistently shown
better results than boosted C4.5 in our tests.
These results confirm the extensive set of re-
sults produced by Daelemans, Van den Bosch
and Zavrel (1999) that indicate memory based
methods produce better results on NLP tasks
than decision tree methods due to a high level
of disjunctions in the attribute space.
Our experimental work on the nature of
boosting indicates that it may be having a suc-
cessful effect on classification because it com-
pensates for poor selection of the prior proba-
bilities of the data set, no matter whether that
prior is implicit or explicit to the method. This
suggestion is consistent with the known charac-
teristic of language data that notionally follows
Zipf&apos;s Law or at least a power series which at the
moment are not usable in either DGRAPH or
C4.5 in their current form. Unfortunately, usu-
ally no detailed determination of distribution
characteristics is established in published exper-
imental results for NLP tasks. We suggest that
DGRAPH and C4.5 will both perform better on
NLP tasks when they allow variation of the dis-
tribution characteristics of the classes and we
have better probability distribution knowledge
for this NLP data.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997111727272727">
W. Daelemans, A. van den Bosch, and J. Za-
vrel. 1999. Forgetting Exceptions is Harm-
ful in Language Learning. Machine Learning,
34(1).
Y. Freund and R. Schapire. 1995. A deci-
sion theoretic generalization of on-line learn-
ing and an application to boosting. In Pro-
ceedings of the Second European Conference
on Computational Learning.
J.J. Oliver and C.S. Wallace. 1991. Inferring
decision graphs. In Proceedings of Workshop
</reference>
<bodyText confidence="0.71522775">
development precision recall Fo=1
part 1 78.34% 36.82% 50.10
part 2 92.04% 48.57% 63.58
part 3 30.73% 13.57% 18.82
test precision recall Fo=1
part 1 72.81% 38.47% 50.34
part 2 89.61% 45.39% 60.26
part 3 29.54% 13.45% 18.49
</bodyText>
<tableCaption confidence="0.687361666666667">
Table 1: Results obtained for the development
and the test data set for the three parts of the
shared task.
</tableCaption>
<reference confidence="0.992409363636364">
8 - Evaluating and Changing Representation
in Machine Learning. IJCAI-91.
J.R. Quinlan and R.L. Rivest. 1989. Infer-
ring decision trees using minimum descrip-
tion length principle. Information &amp; Compu-
tation, 80:227-248.
C.S. Wallace and D.M. Boulton. 1968. An in-
formation measure for classification. Com-
puter Journal, 11:185-195.
C.S. Wallace and J.D. Patrick. 1993. Coding
Decision Trees. Machine Learning, 11:7-22.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341456">
<title confidence="0.999883">Boosted Decision Graphs for NLP Learning Tasks</title>
<author confidence="0.999608">D Patrick</author>
<affiliation confidence="0.9934685">Basser Department of Computer University of Sydney, NSW,</affiliation>
<author confidence="0.354225">edu an</author>
<abstract confidence="0.9981340625">This paper reports the implementation of the AdaBoost algorithm on decision graphs, optimized using the Minimum Message Length Principle. The AdaBoost algorithm, which we call 1-Stage Boosting, is shown to improve the accuracy of decision graphs, along with we another technique which we combine with AdaBoost and call 2-Stage Boosting. which shows the greater improvement. Empirical tests demonstrate that both 1-Stage and 2-Stage Boosting techniques perform better than the boosted C4.5 algorithm. However the boosting has not shown a significant improvement for NLP tasks with a high disjunction of attribute space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<date>1999</date>
<booktitle>Forgetting Exceptions is Harmful in Language Learning. Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting Exceptions is Harmful in Language Learning. Machine Learning, 34(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>A decision theoretic generalization of on-line learning and an application to boosting.</title>
<date>1995</date>
<booktitle>In Proceedings of the Second European Conference on Computational Learning.</booktitle>
<contexts>
<context position="1055" citStr="Freund and Schapire, 1995" startWordPosition="155" endWordPosition="158">sion graphs, along with we another technique which we combine with AdaBoost and call 2-Stage Boosting. which shows the greater improvement. Empirical tests demonstrate that both 1-Stage and 2-Stage Boosting techniques perform better than the boosted C4.5 algorithm. However the boosting has not shown a significant improvement for NLP tasks with a high disjunction of attribute space. 1 INTRODUCTION In a wide variety of classification problems, boosting techniques have proven to be an effective method to significantly reduce the error of any weak learning algorithm. While the AdaBoost algorithm (Freund and Schapire, 1995) has been used to improve the accuracy of a decision tree algorithm (Quinlan and Rivest, 1989), which uses the Minimum Description Length Principle (MDL), little is known about it&apos;s effectiveness on the decision graphs. This paper examines the application of the AdaBoost technique to the decision graph algorithm (Oliver and Wallace, 1991; Wallace and Patrick, 1993), which infers classification graphs from data by combining the Minimum Message Length Principle (MML) (Wallace and Boulton, 1968) with the recursive partitioning algorithm. In this paper we present two variants of the boosted decisi</context>
</contexts>
<marker>Freund, Schapire, 1995</marker>
<rawString>Y. Freund and R. Schapire. 1995. A decision theoretic generalization of on-line learning and an application to boosting. In Proceedings of the Second European Conference on Computational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Oliver</author>
<author>C S Wallace</author>
</authors>
<title>Inferring decision graphs.</title>
<date>1991</date>
<booktitle>In Proceedings of Workshop 8 - Evaluating and Changing Representation in Machine Learning. IJCAI-91.</booktitle>
<contexts>
<context position="1394" citStr="Oliver and Wallace, 1991" startWordPosition="210" endWordPosition="213">with a high disjunction of attribute space. 1 INTRODUCTION In a wide variety of classification problems, boosting techniques have proven to be an effective method to significantly reduce the error of any weak learning algorithm. While the AdaBoost algorithm (Freund and Schapire, 1995) has been used to improve the accuracy of a decision tree algorithm (Quinlan and Rivest, 1989), which uses the Minimum Description Length Principle (MDL), little is known about it&apos;s effectiveness on the decision graphs. This paper examines the application of the AdaBoost technique to the decision graph algorithm (Oliver and Wallace, 1991; Wallace and Patrick, 1993), which infers classification graphs from data by combining the Minimum Message Length Principle (MML) (Wallace and Boulton, 1968) with the recursive partitioning algorithm. In this paper we present two variants of the boosted decision graphs, which we call as 1-Stage Boosting and 2-stage Boosting respectively. 1.1 Decision Graphs There have been a number of attempts to extend the representational power of the decision trees by allowing a node to have more than one parent. Oliver and Wallace (1991), introduced decision graphs which are generalizations of decision tr</context>
<context position="2704" citStr="Oliver and Wallace (1991)" startWordPosition="426" endWordPosition="429"> MML Principle. The feature that distinguishes decision graphs from decision trees is that the former may also contain joins. A join is represented by two nodes having a common child, and this specifies that two subsets have some common properties, and hence can be considered as one subset. The manner in which the objects are allocated to leaf nodes in decision graphs is the same as decision trees. The decision graph offers an elegant solution to the replication and fragmentation problems faced by decision trees. To discover classification graphs of short message length, the DGRAPH program of Oliver and Wallace (1991) starts with a single leaf at the root and grows a graph by repeatedly applying the modification that results in the greatest savings in the message length. Modifications considered either replace a leaf with a decision node or join two leaves together. The join probability, RI is taken as an argument to the algorithm. The procedure halts when the best modification fails to reduce the message length of the resulting graph. To assist in preventing the greedy algorithm from becoming trapped in local message length minima, a lookahead parameter, /, is used to calculate message length savings over</context>
</contexts>
<marker>Oliver, Wallace, 1991</marker>
<rawString>J.J. Oliver and C.S. Wallace. 1991. Inferring decision graphs. In Proceedings of Workshop 8 - Evaluating and Changing Representation in Machine Learning. IJCAI-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
<author>R L Rivest</author>
</authors>
<title>Inferring decision trees using minimum description length principle.</title>
<date>1989</date>
<journal>Information &amp; Computation,</journal>
<pages>80--227</pages>
<contexts>
<context position="1149" citStr="Quinlan and Rivest, 1989" startWordPosition="172" endWordPosition="175">osting. which shows the greater improvement. Empirical tests demonstrate that both 1-Stage and 2-Stage Boosting techniques perform better than the boosted C4.5 algorithm. However the boosting has not shown a significant improvement for NLP tasks with a high disjunction of attribute space. 1 INTRODUCTION In a wide variety of classification problems, boosting techniques have proven to be an effective method to significantly reduce the error of any weak learning algorithm. While the AdaBoost algorithm (Freund and Schapire, 1995) has been used to improve the accuracy of a decision tree algorithm (Quinlan and Rivest, 1989), which uses the Minimum Description Length Principle (MDL), little is known about it&apos;s effectiveness on the decision graphs. This paper examines the application of the AdaBoost technique to the decision graph algorithm (Oliver and Wallace, 1991; Wallace and Patrick, 1993), which infers classification graphs from data by combining the Minimum Message Length Principle (MML) (Wallace and Boulton, 1968) with the recursive partitioning algorithm. In this paper we present two variants of the boosted decision graphs, which we call as 1-Stage Boosting and 2-stage Boosting respectively. 1.1 Decision G</context>
</contexts>
<marker>Quinlan, Rivest, 1989</marker>
<rawString>J.R. Quinlan and R.L. Rivest. 1989. Inferring decision trees using minimum description length principle. Information &amp; Computation, 80:227-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wallace</author>
<author>D M Boulton</author>
</authors>
<title>An information measure for classification.</title>
<date>1968</date>
<journal>Computer Journal,</journal>
<pages>11--185</pages>
<contexts>
<context position="1552" citStr="Wallace and Boulton, 1968" startWordPosition="232" endWordPosition="236">method to significantly reduce the error of any weak learning algorithm. While the AdaBoost algorithm (Freund and Schapire, 1995) has been used to improve the accuracy of a decision tree algorithm (Quinlan and Rivest, 1989), which uses the Minimum Description Length Principle (MDL), little is known about it&apos;s effectiveness on the decision graphs. This paper examines the application of the AdaBoost technique to the decision graph algorithm (Oliver and Wallace, 1991; Wallace and Patrick, 1993), which infers classification graphs from data by combining the Minimum Message Length Principle (MML) (Wallace and Boulton, 1968) with the recursive partitioning algorithm. In this paper we present two variants of the boosted decision graphs, which we call as 1-Stage Boosting and 2-stage Boosting respectively. 1.1 Decision Graphs There have been a number of attempts to extend the representational power of the decision trees by allowing a node to have more than one parent. Oliver and Wallace (1991), introduced decision graphs which are generalizations of decision trees, having decision nodes and leaves. They optimize the decision graphs based on the MML Principle. The feature that distinguishes decision graphs from decis</context>
</contexts>
<marker>Wallace, Boulton, 1968</marker>
<rawString>C.S. Wallace and D.M. Boulton. 1968. An information measure for classification. Computer Journal, 11:185-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wallace</author>
<author>J D Patrick</author>
</authors>
<title>Coding Decision Trees.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<pages>11--7</pages>
<contexts>
<context position="1422" citStr="Wallace and Patrick, 1993" startWordPosition="214" endWordPosition="217"> attribute space. 1 INTRODUCTION In a wide variety of classification problems, boosting techniques have proven to be an effective method to significantly reduce the error of any weak learning algorithm. While the AdaBoost algorithm (Freund and Schapire, 1995) has been used to improve the accuracy of a decision tree algorithm (Quinlan and Rivest, 1989), which uses the Minimum Description Length Principle (MDL), little is known about it&apos;s effectiveness on the decision graphs. This paper examines the application of the AdaBoost technique to the decision graph algorithm (Oliver and Wallace, 1991; Wallace and Patrick, 1993), which infers classification graphs from data by combining the Minimum Message Length Principle (MML) (Wallace and Boulton, 1968) with the recursive partitioning algorithm. In this paper we present two variants of the boosted decision graphs, which we call as 1-Stage Boosting and 2-stage Boosting respectively. 1.1 Decision Graphs There have been a number of attempts to extend the representational power of the decision trees by allowing a node to have more than one parent. Oliver and Wallace (1991), introduced decision graphs which are generalizations of decision trees, having decision nodes a</context>
<context position="4868" citStr="Wallace and Patrick (1993)" startWordPosition="809" endWordPosition="812">e of values of RI, say from 0.0, ..., 0.5. We select the graph with the smallest message length as being the best graph, and hence derive the estimate of RI. 2.2 Prior Probability of Leaf Node Class Purity The second parameter that effects the coding of decision graphs is the leaf node purity prior, alpha. The value of alpha determines how pure the leaf nodes will be. A heterogeneous class distribution in a leaf node has a uniform distribution where the value of the prior is one, however values of prior less than one places greater weight on more pure (homogeneous) distributions. As stated in Wallace and Patrick (1993), the data itself is used to estimate the leaf node purity prior. Having grown the full tree with say, alpha = 1, we find the best pruned form and message length for various values of alpha, and hence the best value of alpha is the one that generates the shortest message length. We use both these parameters, that is, the optimal probability of a join, and the estimate of the leaf node purity prior together with the AdaBoost algorithm to improve the performance of the DGRAPH algorithm in our 2-Stage Boosting algorithm. 2.3 1-Stage Boosting In our 1-Stage Boosting algorithm, we use AdaBoost to i</context>
</contexts>
<marker>Wallace, Patrick, 1993</marker>
<rawString>C.S. Wallace and J.D. Patrick. 1993. Coding Decision Trees. Machine Learning, 11:7-22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>