<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001606">
<title confidence="0.9997335">
A Tightly-coupled Unsupervised Clustering and
Bilingual Alignment Model for Transliteration
</title>
<author confidence="0.999163">
Tingting Li&apos;, Tiejun Zhao&apos;, Andrew Finch2, Chunyue Zhang&apos;
</author>
<affiliation confidence="0.99743">
&apos;Harbin Institute of Technology, Harbin, China
</affiliation>
<address confidence="0.496118">
2NICT, Japan
</address>
<email confidence="0.91284">
&apos;{ttli, tjzhao, cyzhang}@mtlab.hit.edu.cn
2andrew.finch@nict.go.jp
</email>
<sectionHeader confidence="0.993764" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883958333333">
Machine Transliteration is an essential
task for many NLP applications. Howev-
er, names and loan words typically orig-
inate from various languages, obey dif-
ferent transliteration rules, and therefore
may benefit from being modeled inde-
pendently. Recently, transliteration mod-
els based on Bayesian learning have over-
come issues with over-fitting allowing for
many-to-many alignment in the training of
transliteration models. We propose a nov-
el coupled Dirichlet process mixture mod-
el (cDPMM) that simultaneously clusters
and bilingually aligns transliteration data
within a single unified model. The un-
ified model decomposes into two class-
es of non-parametric Bayesian component
models: a Dirichlet process mixture mod-
el for clustering, and a set of multino-
mial Dirichlet process models that perf-
orm bilingual alignment independently for
each cluster. The experimental results
show that our method considerably outper-
forms conventional alignment models.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.946884962264151">
Machine transliteration methods can be catego-
rized into phonetic-based models (Knight et al.,
1998), spelling-based models (Brill et al., 2000),
and hybrid models which utilize both phonetic
and spelling information (Oh et al., 2005; Oh et
al., 2006). Among them, statistical spelling-based
models which directly align characters in the train-
ing corpus have become popular because they
are language-independent, do not require phonet-
ic knowledge, and are capable of achieving state-
of-the-art performance (Zhang et al., 2012b). A
major problem with real-word transliteration cor-
pora is that they are usually not clean, may con-
tain name pairs with various linguistic origins and
this can hinder the performance of spelling-based
models because names from different origins obey
different pronunciation rules, for example:
“Kim Jong-il/�1l,0” (Korea),
“Kana Gaski/�llq.” (Japan),
“Haw King/ 4�” (England),
“Jin yong/�) ’ (China).
The same Chinese character “4�” should be
aligned to different romanized character se-
quences: “Kim”, “Kana”, “King”, “Jin”. To ad-
dress this issue, many name classification metho-
ds have been proposed, such as the supervised lan-
guage model-based approach of (Li et al., 2007),
and the unsupervised approach of (Huang et al.,
2005) that used a bottom-up clustering algorithm.
(Li et al., 2007) proposed a supervised translitera-
tion model which classifies names based on their
origins and genders using a language model; it
switches between transliteration models based on
the input. (Hagiwara et al., 2011) tackled the is-
sue by using an unsupervised method based on the
EM algorithm to perform a soft classification.
Recently, non-parametric Bayesian
models (Finch et al., 2010; Huang et al.,
2011; Hagiwara et al., 2012) have attracted
much attention in the transliteration field. In
comparison to many of the previous alignment
models (Li et al., 2004; Jiampojamarn et al.,
2007; Berg-Kirkpatrick et al., 2011), the non-
parametric Bayesian models allow unconstrained
monotonic many-to-many alignment and are able
to overcome the inherent over-fitting problem.
Until now most of the previous work (Li et al.,
2007; Hagiwara et al., 2011) is either affected by
the multi-origins factor, or has issues with over-
fitting. (Hagiwara et al., 2012) took these two fac-
tors into consideration, but their approach still op-
erates within an EM framework and model order
selection by hand is necessary prior to training.
</bodyText>
<page confidence="0.99062">
393
</page>
<bodyText confidence="0.922075055555555">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393–398,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
We propose a simple, elegant, fully-
unsupervised solution based on a single generative
model able to both cluster and align simultaneous-
ly. The coupled Dirichlet Process Mixture Model
(cDPMM) integrates a Dirichlet process mixture
model (DPMM) (Antoniak, 1974) and a Bayesian
Bilingual Alignment Model (BBAM) (Finch et
al., 2010). The two component models work
synergistically to support one another: the clus-
tering model sorts the data into classes so that
self-consistent alignment models can be built
using data of the same type, and at the same time
the alignment probabilities from the alignment
models drive the clustering process.
In summary, the key advantages of our model
are as follows:
</bodyText>
<listItem confidence="0.9997625">
• it is based on a single, unified generative
model;
• it is fully unsupervised;
• it is an infinite mixture model, and does not
require model order selection – it is effec-
tively capable of discovering an appropriate
number of clusters from the data;
• it is able to handle data from multiple origins;
• it can perform many-to-many alignment
without over-fitting.
</listItem>
<sectionHeader confidence="0.878073" genericHeader="method">
2 Model Description
</sectionHeader>
<bodyText confidence="0.9997155">
In this section we describe the methodology and
realization of the proposed cDPMM in detail.
</bodyText>
<subsectionHeader confidence="0.885671">
2.1 Terminology
</subsectionHeader>
<bodyText confidence="0.999256555555556">
In this paper, we concentrate on the alignment
process for transliteration. The proposed cDP-
MM segments a bilingual corpus of transliteration
pairs into bilingual character sequence-pairs. We
will call these sequence-pairs Transliteration U-
nits (TUs). We denote the source and target of
a TU as sm1 = (s1,..., sm) and tn1 =(t1, ..., tn)
respectively, where si (ti) is a single character in
source (target) language. We use the same no-
tation (s, t) = ((s1, ..., sm), (t1, ..., tn)) to de-
note a transliteration pair, which we can write as
x = (sm1 , tn1) for simplicity. Finally, we express
the training set itself as a set of sequence pairs:
D = {xi}Ii=1. Our aim is to obtain a bilingual
alignment ((s1, t1), ..., (sl, tl)) for each transliter-
ation pair xi, where each (sj, tj) is a segment of
the whole pair (a TU) and l is the number of seg-
ments used to segment xi.
</bodyText>
<subsectionHeader confidence="0.995047">
2.2 Methodology
</subsectionHeader>
<bodyText confidence="0.9996961">
Our cDPMM integrates two Dirichlet process
models: the DPMM clustering model, and the
BBAM alignment model which is a multinomial
Dirichlet process.
A Dirichlet process mixture model, models the
data as a mixture of distributions – one for each
cluster. It is an infinite mixture model, and the
number of components is not fixed prior to train-
ing. Equation 1 expresses the DPMM hierarchi-
cally.
</bodyText>
<equation confidence="0.997357666666667">
GcJac, G0c — DP(ac, G0c)
BkJGc — Gc
xiJBk — f(xiJBk) (1)
</equation>
<bodyText confidence="0.999972909090909">
where G0c is the base measure and ac &gt; 0 is the
concentration parameter for the distribution Gc.
xi is a name pair in training data, and Bk repre-
sents the parameters of a candidate cluster k for
xi. Specifically Bk contains the probabilities of all
the TUs in cluster k. f(xiJBk) (defined in Equa-
tion 7) is the probability that mixture component
k parameterized by Bk will generate xi.
The alignment component of our cDPMM is
a multinomial Dirichlet process and is defined as
follows:
</bodyText>
<equation confidence="0.868656">
GaJaa, G0a — DP(aa, G0a)
(sj, tj)JGa — Ga (2)
</equation>
<bodyText confidence="0.999949888888889">
The subscripts ‘c’ and ‘a’ in Equations 1 and 2
indicate whether the terms belong to the clustering
or alignment model respectively.
The generative story for the cDPMM is sim-
ple: first generate an infinite number of clusters,
choose one, then generate a transliteration pair us-
ing the parameters that describe the cluster. The
basic sampling unit of the cDPMM for the cluster-
ing process is a transliteration pair, but the basic
sampling unit for BBAM is a TU. In order to inte-
grate the two processes in a single model we treat
a transliteration pair as a sequence of TUs gener-
ated by a BBAM model. The BBAM generates a
sequence (a transliteration pair) based on the joint
source-channel model (Li et al., 2004). We use a
blocked version of a Gibbs sampler to train each
BBAM (see (Mochihashi et al., 2009) for details
of this process).
</bodyText>
<subsectionHeader confidence="0.977191">
2.3 The Alignment Model
</subsectionHeader>
<bodyText confidence="0.982559">
This model is a multinomial DP model. Under the
Chinese restaurant process (CRP) (Aldous, 1985)
</bodyText>
<page confidence="0.994344">
394
</page>
<bodyText confidence="0.999657333333333">
interpretation, each unique TU corresponds to a
dish served at a table, and the number of customers
in each table represents the count of a particular
TU in the model.
The probability of generating the jth TU (sj, tj)
is,
</bodyText>
<equation confidence="0.99789525">
( ) ( )
N (sj, tj) + αaG0a (sj, tj)
N + αa
(3)
</equation>
<bodyText confidence="0.991466333333333">
where N is the total number of TUs generated
( )
so far, and N (sj,tj) is the count of (sj,tj).
(s−j, t−j) are all the TUs generated so far except
(sj, tj). The base measure G0a is a joint spelling
model:
</bodyText>
<equation confidence="0.906527">
( )
G0a (s,t) = P(|s|)P(s||s|)P(|t|)P(t||t|)
−λtv−|t|
t
(4)
</equation>
<bodyText confidence="0.99996675">
where |s |(|t|) is the length of the source (target)
sequence, vs (vt) is the vocabulary (alphabet) size
of the source (target) language, and As (At) is the
expected length of source (target) side.
</bodyText>
<subsectionHeader confidence="0.988476">
2.4 The Clustering Model
</subsectionHeader>
<bodyText confidence="0.999968666666667">
This model is a DPMM. Under the CRP interpre-
tation, a transliteration pair corresponds to a cus-
tomer, the dish served on each table corresponds
to an origin of names.
We use z = (z1, ..., zI), zi E 11, ..., K} to in-
dicate the cluster of each transliteration pair xi in
the training set and B = (B1, ..., BK) to represent
the parameters of the component associated with
each cluster.
In our model, each mixture component is a
multinomial DP model, and since Bk contains the
probabilities of all the TUs in cluster k, the num-
ber of parameters in each Bk is uncertain and
changes with the transliteration pairs that belong
to the cluster. For a new cluster (the K + 1th clus-
ter), we use Equation 4 to calculate the probability
of each TU. The cluster membership probability
of a transliteration pair xi is calculated as follows,
</bodyText>
<equation confidence="0.91295825">
αc
P(zi = K + 1|D, θ, z−i) a P(xi|z, θK+1)
n − 1 + αc
(6)
</equation>
<bodyText confidence="0.999857066666667">
where nk is the number of transliteration pairs in
the existing cluster k E 11, ..., K} (cluster K + 1
is a newly created cluster), zi is the cluster indi-
cator for xi, and z−i is the sequence of observed
clusters up to xi. As mentioned earlier, basic sam-
pling units are inconsistent for the clustering and
alignment model, therefore to couple the models
the BBAM generates transliteration pairs as a se-
quence of TUs, these pairs are then used directly
in the DPMM.
Let -y = ((s1, t1),..., (sl, tl)) be a derivation of
a transliteration pair xi. To make the model inte-
gration process explicit, we use function f to cal-
culate the probability P(xi|z, Bk), where f is de-
fined as follows,
</bodyText>
<equation confidence="0.999707166666667">
{ ∑ ∏
γ∈R
f(xi|θk) = (s
∑γ∈R ∏(s,t)∈γ G0c(s|tj) k E {K K}
,+.1
J (7)
</equation>
<bodyText confidence="0.999608125">
where R denotes the set of all derivations of xi,
G0c is the same as Equation 4.
The cluster membership zi is sampled together
with the derivation -y in a single step according to
P(zi = k|D, B, z−i) and f(xi|Bk). Following the
method of (Mochihashi et al., 2009), first f(xi|Bk)
is calculated by forward filtering, and then a sam-
ple -y is taken by backward sampling.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997816">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999986611111111">
To empirically validate our approach, we investi-
gate the effectiveness of our model by conduct-
ing English-Chinese name transliteration genera-
tion on three corpora containing name pairs of
varying degrees of mixed origin. The first two cor-
pora were drawn from the “Names of The World’s
Peoples” dictionary published by Xin Hua Pub-
lishing House. The first corpus was construct-
ed with names only originating from English lan-
guage (EO), and the second with names originat-
ing from English, Chinese, Japanese evenly (ECJ-
O). The third corpus was created by extracting
name pairs from LDC (Linguistic Data Consor-
tium) Named Entity List, which contains names
from all over the world (Multi-O). We divided the
datasets into training, development and test sets
for each corpus with a ratio of 10:1:1. The details
of the division are displayed in Table 2.
</bodyText>
<equation confidence="0.992842230769231">
( )
P (sj, tj)|(s−j, t−j) =
−λsv−|s |X
s
A|s|
s
|s|! e
Att |
|t|! e
nk
P(zi = k|D, θ, z−i) a
P(xi|z, θk) (5)
n − 1 + αc
</equation>
<page confidence="0.996646">
395
</page>
<table confidence="0.9998988">
cDPMM Alignment BBAM Alignment
mun|蒙 din|丁 ger|格(0, English) mun|蒙 din|丁 ger|格
ding|丁 guo|果(2, Chinese) din|丁 g |guo|果
tei|丁 be|部(3, Japanese) t ||丁 e |ibe|部
fan|范 chun|纯 yi|一(2, Chinese) fan|范 chun|纯 y |i|一
hong|洪 il|一 sik|植(5, Korea) hong|洪 i|一 l |si|植 k|
sei|静 ichi|一 ro|郎(4, Japanese) seii|静 ch |i|一 ro|郎
dom|东 b|布 ro|罗 w|夫 s|斯 ki|基(0, Russian) do|东 mb|布 ro|罗 w|夫 s|斯 ki|基
he|何 dong|东 chang|昌(2, Chinese) he|何 don|东 gchang|昌
b|布 ran|兰 don|东(0, English) b|布 ran|兰 don|东
</table>
<tableCaption confidence="0.999745">
Table 1: Typical alignments from the BBAM and cDPMM.
</tableCaption>
<subsectionHeader confidence="0.99868">
3.2 Baselines
</subsectionHeader>
<bodyText confidence="0.997452307692308">
We compare our alignment model with
GIZA++ (Och et al., 2003) and the Bayesian
bilingual alignment model (BBAM). We employ
two decoding models: a phrase-based machine
translation decoder (specifically Moses (Koehn
et al., 2007)), and the DirecTL decoder (Jiampo-
jamarn et al., 2009). They are based on different
decoding strategies and optimization targets, and
therefore make the comparison more compre-
hensive. For the Moses decoder, we applied the
grow-diag-final-and heuristic algorithm to extract
the phrase table, and tuned the parameters using
the BLEU metric.
</bodyText>
<table confidence="0.9969636">
Corpora Corpus Scale
Training Development Testing
EO 32,681 3,267 3,267
ECJ-O 32,500 3,250 3,250
Multi-O 33,291 3,328 3,328
</table>
<tableCaption confidence="0.999448">
Table 2: Statistics of the experimental corpora.
</tableCaption>
<bodyText confidence="0.9995374">
To evaluate the experimental results, we uti-
lized 3 metrics from the Named Entities Workshop
(NEWS) (Zhang et al., 2012a): word accuracy in
top-1 (ACC), fuzziness in top-1 (Mean F-score)
and mean reciprocal rank (MRR).
</bodyText>
<subsectionHeader confidence="0.998461">
3.3 Parameter Setting
</subsectionHeader>
<bodyText confidence="0.999753846153846">
In our model, there are several important parame-
ters: 1) max s, the maximum length of the source
sequences of the alignment tokens; 2) max t, the
maximum length of the target sequences of the
alignment tokens; and 3) nc, the initial number of
classes for the training data. We set max s = 6,
max t = 1 and nc = 5 empirically based on a
small pilot experiment. The Moses decoder was
used with default settings except for the distortion-
limit which was set to 0 to ensure monotonic de-
coding. For the DirecTL decoder the following
settings were used: cs = 4, ng = 9 and nBest =
5. cs denotes the size of context window for fea-
tures, ng indicates the size of n-gram features and
nBest is the size of transliteration candidate list
for updating the model in each iteration. The con-
centration parameter α, αs of the clustering mod-
el and the BBAM was learned by sampling its val-
ue. Following (Blunsom et al., 2009) we used
a vague gamma prior F(10−4,104), and sampled
new values from a log-normal distribution whose
mean was the value of the parameter, and variance
was 0.3. We used the Metropolis-Hastings algo-
rithm to determine whether this new sample would
be accepted. The parameters A, and At in Equa-
tion 4 were set to A, = 4 and At = 1.
</bodyText>
<table confidence="0.9995988">
Model EO ECJ-O Multi-O
#(Clusters) cDPMM 5.8 9.5 14.3
#(Targets) GIZA++ 14.43 5.35 6.62
BBAM 6.06 2.45 2.91
cDPMM 9.32 3.45 4.28
</table>
<tableCaption confidence="0.99977">
Table 3: Alignment statistics.
</tableCaption>
<subsectionHeader confidence="0.980808">
3.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9998658">
Table 3 shows some details of the alignment re-
sults. The #(Clusters) represents the average num-
ber of clusters from the cDPMM. It is averaged
over the final 50 iterations, and the classes which
contain less than 10 name pairs are excluded. The
#(Targets) represents the average number of En-
glish character sequences that are aligned to each
Chinese sequence. From the results we can see
that in terms of the number of alignment targe-
ts: GIZA++ &gt; cDPMM &gt; BBAM. GIZA++ has
considerably more targets than the other approach-
es, and this is likely to be a symptom of it over-
fitting the data. cDPMM can alleviate the over-
fitting through its BBAM component, and at the
same time effectively model the diversity in Chi-
nese character sequences caused by multi-origin.
Table 1 shows some typical TUs from the align-
ments produced by BBAM and cDPMM on cor-
pus Multi-O. The information in brackets in Ta-
ble 1, represents the ID of the class and origin of
</bodyText>
<page confidence="0.996495">
396
</page>
<table confidence="0.999493272727273">
Corpora Model Evaluation
ACC M-Fscore MRR
EO GIZA 0.7241 0.8881 0.8061
BBAM 0.7286 0.8920 0.8043
cDPMM 0.7398 0.8983 0.8126
ECJ-O GIZA 0.5471 0.7278 0.6268
BBAM 0.5522 0.7370 0.6344
cDPMM 0.5643 0.7420 0.6446
Multi-O GIZA 0.4993 0.7587 0.5986
BBAM 0.5163 0.7769 0.6123
cDPMM 0.5237 0.7796 0.6188
</table>
<tableCaption confidence="0.988301">
Table 4: Comparison of different methods using
the Moses phrase-based decoder.
</tableCaption>
<bodyText confidence="0.999977176470589">
the name pair; the symbol ‘ ’ indicates a “NUL-
L” alignment. We can see the Chinese characters
“丁(ding) 一(yi) 东(dong)” have different align-
ments in different origins, and that the cDPMM
has provided the correct alignments for them.
We used the sampled alignment from running
the BBAM and cDPMM models for 100 iterations,
and combined the alignment tables of each class
together. The experiments are therefore investigat-
ing whether the alignment has been meaningfully
improved by the clustering process. We would ex-
pect further gains from exploiting the class infor-
mation in the decoding process (as in (Li et al.,
2007)), but this remains future research. The top-
10 transliteration candidates were used for testing.
The detailed experimental results are shown in Ta-
bles 4 and 5.
Our proposed model obtained the highest per-
formance on all three datasets for all evaluation
metrics by a considerable margin. Surprisingly,
for dataset EO although there is no multi-origin
factor, we still observed a respectable improve-
ment in every metric. This shows that although
names may have monolingual origin, there are hid-
den factors which can allow our model to succeed,
possibly related to gender or convention. Other
models based on supervised classification or clus-
tering with fixed classes may fail to capture these
characteristics.
To guarantee the reliability of the compara-
tive results, we performed significance testing
based on paired bootstrap resampling (Efron et al.,
1993). We found all differences to be significant
(p &lt; 0.05).
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.983706">
In this paper we propose an elegant unsupervised
technique for monotonic sequence alignment
based on a single generative model. The key ben-
</bodyText>
<table confidence="0.998979454545455">
Corpora Model Evaluation
ACC M-Fscore MRR
EO GIZA 0.6950 0.8812 0.7632
BBAM 0.7152 0.8899 0.7839
cDPMM 0.7231 0.8933 0.7941
ECJ-O GIZA 0.3325 0.6208 0.4064
BBAM 0.3427 0.6259 0.4192
cDPMM 0.3521 0.6302 0.4316
Multi-O GIZA 0.3815 0.7053 0.4592
BBAM 0.3934 0.7146 0.4799
cDPMM 0.3970 0.7179 0.4833
</table>
<tableCaption confidence="0.996943">
Table 5: Comparison of different methods using
</tableCaption>
<bodyText confidence="0.965967615384615">
the DirecTL decoder.
efits of our model are that it can handle data from
multiple origins, and model using many-to-many
alignment without over-fitting. The model oper-
ates by clustering the data into classes while si-
multaneously aligning it, and is able to discover
an appropriate number of classes from the data.
Our results show that our alignment model can im-
prove the performance of a transliteration gener-
ation system relative to two other state-of-the-art
aligners. Furthermore, the system produced gains
even on data of monolingual origin, where no ob-
vious clusters in the data were expected.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999763125">
We thank the anonymous reviewers for their valu-
able comments and helpful suggestions.We also
thank Chonghui Zhu, Mo Yu, and Wenwen Zhang
for insightful discussions. This work was support-
ed by National Natural Science Foundation of Chi-
na (61173073), and the Key Project of the Nation-
al High Technology Research and Development
Program of China (2011AA01A207).
</bodyText>
<sectionHeader confidence="0.998349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993274733333333">
D.J. Aldous. 1985. Exchangeability and Related Top-
ics. Ecole d’ Ete St Flour 1983. Springer, 1985,
1117:1–198.
C.E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics. 2:1152, 174.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proc. of EMNLP, pages 313–321.
P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. of ACL, pages 782–790.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
In Proc. of ACL, pages 286–293.
</reference>
<page confidence="0.991008">
397
</page>
<reference confidence="0.999593675675676">
B. Efron and R. J. Tibshirani 1993. An Introduction to
the Bootstrap. Chapman &amp; Hall, New York, NY.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Translitera-
tion. In Proc. of the 7th International Workshop on
Spoken Language Translation, pages 259–266.
Masato Hagiwara and Satoshi Sekine. 2011. Latent
Class Transliteration based on Source Language O-
rigin. In Proc. of ACL (Short Papers), pages 53-57.
Masato Hagiwara and Satoshi Sekine. 2012. Latent
semantic transliteration using dirichlet mixture. In
Proc. of the 4th Named Entity Workshop, pages 30–
37.
Fei Huang, Stephan Vogel, and Alex Waibel. 2005.
Clustering and Classifying Person Names by Origin.
In Proc. of AAAI, pages 1056–1061.
Yun Huang, Min Zhang and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration
with Synchronous Adaptor Grammars. In Proc. of
ACL, pages 534–539.
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek
Sherif. 2007. Applying Many-to-Many Alignments
and Hidden Markov Models to Letter-to-Phoneme
Conversion. In Proc. of NAACL, pages 372–379.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer and Grzegorz Kondrak. 2009.
DirecTL: a Language Independent Approach to
Transliteration. In Proc. of the 2009 Named Entities
Workshop: Shared Task on Transliteration (NEWS
2009), pages 1056–1061.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Journal of Computational Linguis-
tics, pages 28–31.
Philipp Koehn and Hieu Hoang and Alexandra Birch
and Chris Callison-Burch and Marcello Federico
and Nicola Bertoldi and Brooke Cowan and Wade
Shen and Christine Moran and Richard Zens and
Chris Dyer and Ondrej Bojar and Alexandra Con-
stantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL.
Haizou Li, Min Zhang, and Jian Su 2004. A join-
t source-channel model for machine transliteration.
In ACL ’04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics.
Association for Computational Linguistics, Morris-
town, NJ, USA, 159.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic Transliteration of Personal
Names. In Proc. of ACL, pages 120–127.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ue-
da. 2009. Bayesian Unsupervised Word Segmen-
tation with Nested Pitman-Yor Language Modeling.
In Proc. of ACL/IJCNLP, pages 100–108.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Journal of Comput. Linguist., 29(1):19-51.
Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine
Learning Based English-to-Korean Transliteration
Using Grapheme and Phoneme Information. Jour-
nal of IEICE Transactions, 88-D(7):1737-1748.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A machine transliteration model based on
correspondence between graphemes and phonemes.
Journal of ACM Trans. Asian Lang. Inf. Process.,
5(3):185-208.
Min Zhang, Haizhou Li, Ming Liu and A Kumaran.
2012a. Whitepaper of NEWS 2012 shared task on
machine transliteration. In Proc. of the 4th Named
Entity Workshop (NEWS 2012), pages 1–9.
Min Zhang, Haizhou Li, A Kumaran and Ming Liu.
2012b. Report of NEWS 2012 Machine Translitera-
tion Shared Task. In Proc. of the 4th Named Entity
Workshop (NEWS 2012), pages 10–20.
</reference>
<page confidence="0.998266">
398
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934064">
<title confidence="0.999206">A Tightly-coupled Unsupervised Clustering Bilingual Alignment Model for Transliteration</title>
<author confidence="0.995222">Tiejun Andrew Chunyue</author>
<affiliation confidence="0.998409">Institute of Technology, Harbin,</affiliation>
<email confidence="0.983378">tjzhao,</email>
<abstract confidence="0.99828336">Machine Transliteration is an essential task for many NLP applications. However, names and loan words typically originate from various languages, obey different transliteration rules, and therefore may benefit from being modeled independently. Recently, transliteration models based on Bayesian learning have overcome issues with over-fitting allowing for many-to-many alignment in the training of transliteration models. We propose a novel coupled Dirichlet process mixture model (cDPMM) that simultaneously clusters and bilingually aligns transliteration data within a single unified model. The unified model decomposes into two classes of non-parametric Bayesian component models: a Dirichlet process mixture model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D J Aldous</author>
</authors>
<title>Exchangeability and Related Topics. Ecole d’ Ete St Flour</title>
<date>1985</date>
<pages>1117--1</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="7886" citStr="Aldous, 1985" startWordPosition="1260" endWordPosition="1261">unit of the cDPMM for the clustering process is a transliteration pair, but the basic sampling unit for BBAM is a TU. In order to integrate the two processes in a single model we treat a transliteration pair as a sequence of TUs generated by a BBAM model. The BBAM generates a sequence (a transliteration pair) based on the joint source-channel model (Li et al., 2004). We use a blocked version of a Gibbs sampler to train each BBAM (see (Mochihashi et al., 2009) for details of this process). 2.3 The Alignment Model This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394 interpretation, each unique TU corresponds to a dish served at a table, and the number of customers in each table represents the count of a particular TU in the model. The probability of generating the jth TU (sj, tj) is, ( ) ( ) N (sj, tj) + αaG0a (sj, tj) N + αa (3) where N is the total number of TUs generated ( ) so far, and N (sj,tj) is the count of (sj,tj). (s−j, t−j) are all the TUs generated so far except (sj, tj). The base measure G0a is a joint spelling model: ( ) G0a (s,t) = P(|s|)P(s||s|)P(|t|)P(t||t|) −λtv−|t| t (4) where |s |(|t|) is the length of the source (target) sequence</context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>D.J. Aldous. 1985. Exchangeability and Related Topics. Ecole d’ Ete St Flour 1983. Springer, 1985, 1117:1–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Antoniak</author>
</authors>
<title>Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.</title>
<date>1974</date>
<journal>Annals of Statistics.</journal>
<volume>2</volume>
<pages>174</pages>
<contexts>
<context position="4134" citStr="Antoniak, 1974" startWordPosition="605" endWordPosition="606">ook these two factors into consideration, but their approach still operates within an EM framework and model order selection by hand is necessary prior to training. 393 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393–398, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We propose a simple, elegant, fullyunsupervised solution based on a single generative model able to both cluster and align simultaneously. The coupled Dirichlet Process Mixture Model (cDPMM) integrates a Dirichlet process mixture model (DPMM) (Antoniak, 1974) and a Bayesian Bilingual Alignment Model (BBAM) (Finch et al., 2010). The two component models work synergistically to support one another: the clustering model sorts the data into classes so that self-consistent alignment models can be built using data of the same type, and at the same time the alignment probabilities from the alignment models drive the clustering process. In summary, the key advantages of our model are as follows: • it is based on a single, unified generative model; • it is fully unsupervised; • it is an infinite mixture model, and does not require model order selection – i</context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C.E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics. 2:1152, 174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Simple effective decipherment via combinatorial optimization.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>313--321</pages>
<marker>Berg-Kirkpatrick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In Proc. of EMNLP, pages 313–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>C Dyer</author>
<author>M Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>782--790</pages>
<contexts>
<context position="14073" citStr="Blunsom et al., 2009" startWordPosition="2364" endWordPosition="2367">s = 6, max t = 1 and nc = 5 empirically based on a small pilot experiment. The Moses decoder was used with default settings except for the distortionlimit which was set to 0 to ensure monotonic decoding. For the DirecTL decoder the following settings were used: cs = 4, ng = 9 and nBest = 5. cs denotes the size of context window for features, ng indicates the size of n-gram features and nBest is the size of transliteration candidate list for updating the model in each iteration. The concentration parameter α, αs of the clustering model and the BBAM was learned by sampling its value. Following (Blunsom et al., 2009) we used a vague gamma prior F(10−4,104), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. We used the Metropolis-Hastings algorithm to determine whether this new sample would be accepted. The parameters A, and At in Equation 4 were set to A, = 4 and At = 1. Model EO ECJ-O Multi-O #(Clusters) cDPMM 5.8 9.5 14.3 #(Targets) GIZA++ 14.43 5.35 6.62 BBAM 6.06 2.45 2.91 cDPMM 9.32 3.45 4.28 Table 3: Alignment statistics. 3.4 Experimental Results Table 3 shows some details of the alignment results. The #(Clusters) represents the av</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACL, pages 782–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An Improved Error Model for Noisy Channel Spelling Correction.</title>
<date>2000</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>286--293</pages>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction. In Proc. of ACL, pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>New York, NY.</location>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. J. Tibshirani 1993. An Introduction to the Bootstrap. Chapman &amp; Hall, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>A Bayesian Model of Bilingual Segmentation for Transliteration.</title>
<date>2010</date>
<booktitle>In Proc. of the 7th International Workshop on Spoken Language Translation,</booktitle>
<pages>259--266</pages>
<marker>Finch, Sumita, 2010</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2010. A Bayesian Model of Bilingual Segmentation for Transliteration. In Proc. of the 7th International Workshop on Spoken Language Translation, pages 259–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Satoshi Sekine</author>
</authors>
<title>Latent Class Transliteration based on Source Language Origin.</title>
<date>2011</date>
<booktitle>In Proc. of ACL (Short Papers),</booktitle>
<pages>53--57</pages>
<marker>Hagiwara, Sekine, 2011</marker>
<rawString>Masato Hagiwara and Satoshi Sekine. 2011. Latent Class Transliteration based on Source Language Origin. In Proc. of ACL (Short Papers), pages 53-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Satoshi Sekine</author>
</authors>
<title>Latent semantic transliteration using dirichlet mixture.</title>
<date>2012</date>
<booktitle>In Proc. of the 4th Named Entity Workshop,</booktitle>
<pages>30--37</pages>
<marker>Hagiwara, Sekine, 2012</marker>
<rawString>Masato Hagiwara and Satoshi Sekine. 2012. Latent semantic transliteration using dirichlet mixture. In Proc. of the 4th Named Entity Workshop, pages 30– 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Clustering and Classifying Person Names by Origin.</title>
<date>2005</date>
<booktitle>In Proc. of AAAI,</booktitle>
<pages>1056--1061</pages>
<contexts>
<context position="2507" citStr="Huang et al., 2005" startWordPosition="355" endWordPosition="358">airs with various linguistic origins and this can hinder the performance of spelling-based models because names from different origins obey different pronunciation rules, for example: “Kim Jong-il/�1l,0” (Korea), “Kana Gaski/�llq.” (Japan), “Haw King/ 4�” (England), “Jin yong/�) ’ (China). The same Chinese character “4�” should be aligned to different romanized character sequences: “Kim”, “Kana”, “King”, “Jin”. To address this issue, many name classification methods have been proposed, such as the supervised language model-based approach of (Li et al., 2007), and the unsupervised approach of (Huang et al., 2005) that used a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration field. In comparison to many of the previous alignment m</context>
</contexts>
<marker>Huang, Vogel, Waibel, 2005</marker>
<rawString>Fei Huang, Stephan Vogel, and Alex Waibel. 2005. Clustering and Classifying Person Names by Origin. In Proc. of AAAI, pages 1056–1061.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Huang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>534--539</pages>
<contexts>
<context position="2973" citStr="Huang et al., 2011" startWordPosition="428" endWordPosition="431">ods have been proposed, such as the supervised language model-based approach of (Li et al., 2007), and the unsupervised approach of (Huang et al., 2005) that used a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration field. In comparison to many of the previous alignment models (Li et al., 2004; Jiampojamarn et al., 2007; Berg-Kirkpatrick et al., 2011), the nonparametric Bayesian models allow unconstrained monotonic many-to-many alignment and are able to overcome the inherent over-fitting problem. Until now most of the previous work (Li et al., 2007; Hagiwara et al., 2011) is either affected by the multi-origins factor, or has issues with overfitting. (Hagiwara et al., 2012) took these two factors into consideration, but their ap</context>
</contexts>
<marker>Huang, Zhang, Tan, 2011</marker>
<rawString>Yun Huang, Min Zhang and Chew Lim Tan. 2011. Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars. In Proc. of ACL, pages 534–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
</authors>
<title>Grzegorz Kondrak and Tarek Sherif.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>372--379</pages>
<marker>Jiampojamarn, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek Sherif. 2007. Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion. In Proc. of NAACL, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Aditya Bhargava</author>
<author>Qing Dou</author>
<author>Kenneth Dwyer</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>DirecTL: a Language Independent Approach to Transliteration.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>1056--1061</pages>
<contexts>
<context position="12468" citStr="Jiampojamarn et al., 2009" startWordPosition="2088" endWordPosition="2092">hun|纯 y |i|一 hong|洪 il|一 sik|植(5, Korea) hong|洪 i|一 l |si|植 k| sei|静 ichi|一 ro|郎(4, Japanese) seii|静 ch |i|一 ro|郎 dom|东 b|布 ro|罗 w|夫 s|斯 ki|基(0, Russian) do|东 mb|布 ro|罗 w|夫 s|斯 ki|基 he|何 dong|东 chang|昌(2, Chinese) he|何 don|东 gchang|昌 b|布 ran|兰 don|东(0, English) b|布 ran|兰 don|东 Table 1: Typical alignments from the BBAM and cDPMM. 3.2 Baselines We compare our alignment model with GIZA++ (Och et al., 2003) and the Bayesian bilingual alignment model (BBAM). We employ two decoding models: a phrase-based machine translation decoder (specifically Moses (Koehn et al., 2007)), and the DirecTL decoder (Jiampojamarn et al., 2009). They are based on different decoding strategies and optimization targets, and therefore make the comparison more comprehensive. For the Moses decoder, we applied the grow-diag-final-and heuristic algorithm to extract the phrase table, and tuned the parameters using the BLEU metric. Corpora Corpus Scale Training Development Testing EO 32,681 3,267 3,267 ECJ-O 32,500 3,250 3,250 Multi-O 33,291 3,328 3,328 Table 2: Statistics of the experimental corpora. To evaluate the experimental results, we utilized 3 metrics from the Named Entities Workshop (NEWS) (Zhang et al., 2012a): word accuracy in to</context>
</contexts>
<marker>Jiampojamarn, Bhargava, Dou, Dwyer, Kondrak, 2009</marker>
<rawString>Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer and Grzegorz Kondrak. 2009. DirecTL: a Language Independent Approach to Transliteration. In Proc. of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1056–1061.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Machine transliteration.</title>
<date>1998</date>
<journal>Journal of Computational Linguistics,</journal>
<pages>28--31</pages>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Journal of Computational Linguistics, pages 28–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="12414" citStr="Koehn et al., 2007" startWordPosition="2080" endWordPosition="2083"> e |ibe|部 fan|范 chun|纯 yi|一(2, Chinese) fan|范 chun|纯 y |i|一 hong|洪 il|一 sik|植(5, Korea) hong|洪 i|一 l |si|植 k| sei|静 ichi|一 ro|郎(4, Japanese) seii|静 ch |i|一 ro|郎 dom|东 b|布 ro|罗 w|夫 s|斯 ki|基(0, Russian) do|东 mb|布 ro|罗 w|夫 s|斯 ki|基 he|何 dong|东 chang|昌(2, Chinese) he|何 don|东 gchang|昌 b|布 ran|兰 don|东(0, English) b|布 ran|兰 don|东 Table 1: Typical alignments from the BBAM and cDPMM. 3.2 Baselines We compare our alignment model with GIZA++ (Och et al., 2003) and the Bayesian bilingual alignment model (BBAM). We employ two decoding models: a phrase-based machine translation decoder (specifically Moses (Koehn et al., 2007)), and the DirecTL decoder (Jiampojamarn et al., 2009). They are based on different decoding strategies and optimization targets, and therefore make the comparison more comprehensive. For the Moses decoder, we applied the grow-diag-final-and heuristic algorithm to extract the phrase table, and tuned the parameters using the BLEU metric. Corpora Corpus Scale Training Development Testing EO 32,681 3,267 3,267 ECJ-O 32,500 3,250 3,250 Multi-O 33,291 3,328 3,328 Table 2: Statistics of the experimental corpora. To evaluate the experimental results, we utilized 3 metrics from the Named Entities Work</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang and Alexandra Birch and Chris Callison-Burch and Marcello Federico and Nicola Bertoldi and Brooke Cowan and Wade Shen and Christine Moran and Richard Zens and Chris Dyer and Ondrej Bojar and Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>159</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="3129" citStr="Li et al., 2004" startWordPosition="453" endWordPosition="456">sed a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration field. In comparison to many of the previous alignment models (Li et al., 2004; Jiampojamarn et al., 2007; Berg-Kirkpatrick et al., 2011), the nonparametric Bayesian models allow unconstrained monotonic many-to-many alignment and are able to overcome the inherent over-fitting problem. Until now most of the previous work (Li et al., 2007; Hagiwara et al., 2011) is either affected by the multi-origins factor, or has issues with overfitting. (Hagiwara et al., 2012) took these two factors into consideration, but their approach still operates within an EM framework and model order selection by hand is necessary prior to training. 393 Proceedings of the 51st Annual Meeting of</context>
<context position="7641" citStr="Li et al., 2004" startWordPosition="1216" endWordPosition="1219">ering or alignment model respectively. The generative story for the cDPMM is simple: first generate an infinite number of clusters, choose one, then generate a transliteration pair using the parameters that describe the cluster. The basic sampling unit of the cDPMM for the clustering process is a transliteration pair, but the basic sampling unit for BBAM is a TU. In order to integrate the two processes in a single model we treat a transliteration pair as a sequence of TUs generated by a BBAM model. The BBAM generates a sequence (a transliteration pair) based on the joint source-channel model (Li et al., 2004). We use a blocked version of a Gibbs sampler to train each BBAM (see (Mochihashi et al., 2009) for details of this process). 2.3 The Alignment Model This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394 interpretation, each unique TU corresponds to a dish served at a table, and the number of customers in each table represents the count of a particular TU in the model. The probability of generating the jth TU (sj, tj) is, ( ) ( ) N (sj, tj) + αaG0a (sj, tj) N + αa (3) where N is the total number of TUs generated ( ) so far, and N (sj,tj) is the cou</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizou Li, Min Zhang, and Jian Su 2004. A joint source-channel model for machine transliteration. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Morristown, NJ, USA, 159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Khe Chai Sim</author>
<author>Jin-Shea Kuo</author>
<author>Minghui Dong</author>
</authors>
<title>Semantic Transliteration of Personal Names.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="2452" citStr="Li et al., 2007" startWordPosition="346" endWordPosition="349"> that they are usually not clean, may contain name pairs with various linguistic origins and this can hinder the performance of spelling-based models because names from different origins obey different pronunciation rules, for example: “Kim Jong-il/�1l,0” (Korea), “Kana Gaski/�llq.” (Japan), “Haw King/ 4�” (England), “Jin yong/�) ’ (China). The same Chinese character “4�” should be aligned to different romanized character sequences: “Kim”, “Kana”, “King”, “Jin”. To address this issue, many name classification methods have been proposed, such as the supervised language model-based approach of (Li et al., 2007), and the unsupervised approach of (Huang et al., 2005) that used a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration f</context>
<context position="16525" citStr="Li et al., 2007" startWordPosition="2781" endWordPosition="2784">e name pair; the symbol ‘ ’ indicates a “NULL” alignment. We can see the Chinese characters “丁(ding) 一(yi) 东(dong)” have different alignments in different origins, and that the cDPMM has provided the correct alignments for them. We used the sampled alignment from running the BBAM and cDPMM models for 100 iterations, and combined the alignment tables of each class together. The experiments are therefore investigating whether the alignment has been meaningfully improved by the clustering process. We would expect further gains from exploiting the class information in the decoding process (as in (Li et al., 2007)), but this remains future research. The top10 transliteration candidates were used for testing. The detailed experimental results are shown in Tables 4 and 5. Our proposed model obtained the highest performance on all three datasets for all evaluation metrics by a considerable margin. Surprisingly, for dataset EO although there is no multi-origin factor, we still observed a respectable improvement in every metric. This shows that although names may have monolingual origin, there are hidden factors which can allow our model to succeed, possibly related to gender or convention. Other models bas</context>
</contexts>
<marker>Li, Sim, Kuo, Dong, 2007</marker>
<rawString>Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong. 2007. Semantic Transliteration of Personal Names. In Proc. of ACL, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling.</title>
<date>2009</date>
<booktitle>In Proc. of ACL/IJCNLP,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="7736" citStr="Mochihashi et al., 2009" startWordPosition="1234" endWordPosition="1237">rst generate an infinite number of clusters, choose one, then generate a transliteration pair using the parameters that describe the cluster. The basic sampling unit of the cDPMM for the clustering process is a transliteration pair, but the basic sampling unit for BBAM is a TU. In order to integrate the two processes in a single model we treat a transliteration pair as a sequence of TUs generated by a BBAM model. The BBAM generates a sequence (a transliteration pair) based on the joint source-channel model (Li et al., 2004). We use a blocked version of a Gibbs sampler to train each BBAM (see (Mochihashi et al., 2009) for details of this process). 2.3 The Alignment Model This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394 interpretation, each unique TU corresponds to a dish served at a table, and the number of customers in each table represents the count of a particular TU in the model. The probability of generating the jth TU (sj, tj) is, ( ) ( ) N (sj, tj) + αaG0a (sj, tj) N + αa (3) where N is the total number of TUs generated ( ) so far, and N (sj,tj) is the count of (sj,tj). (s−j, t−j) are all the TUs generated so far except (sj, tj). The base measure G0</context>
<context position="10548" citStr="Mochihashi et al., 2009" startWordPosition="1764" endWordPosition="1767">rs as a sequence of TUs, these pairs are then used directly in the DPMM. Let -y = ((s1, t1),..., (sl, tl)) be a derivation of a transliteration pair xi. To make the model integration process explicit, we use function f to calculate the probability P(xi|z, Bk), where f is defined as follows, { ∑ ∏ γ∈R f(xi|θk) = (s ∑γ∈R ∏(s,t)∈γ G0c(s|tj) k E {K K} ,+.1 J (7) where R denotes the set of all derivations of xi, G0c is the same as Equation 4. The cluster membership zi is sampled together with the derivation -y in a single step according to P(zi = k|D, B, z−i) and f(xi|Bk). Following the method of (Mochihashi et al., 2009), first f(xi|Bk) is calculated by forward filtering, and then a sample -y is taken by backward sampling. 3 Experiments 3.1 Corpora To empirically validate our approach, we investigate the effectiveness of our model by conducting English-Chinese name transliteration generation on three corpora containing name pairs of varying degrees of mixed origin. The first two corpora were drawn from the “Names of The World’s Peoples” dictionary published by Xin Hua Publishing House. The first corpus was constructed with names only originating from English language (EO), and the second with names originatin</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling. In Proc. of ACL/IJCNLP, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Journal of Comput. Linguist.,</journal>
<pages>29--1</pages>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Journal of Comput. Linguist., 29(1):19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>Machine Learning Based English-to-Korean Transliteration Using Grapheme and Phoneme Information.</title>
<date>2005</date>
<journal>Journal of IEICE Transactions,</journal>
<pages>88--7</pages>
<marker>Oh, Choi, 2005</marker>
<rawString>Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine Learning Based English-to-Korean Transliteration Using Grapheme and Phoneme Information. Journal of IEICE Transactions, 88-D(7):1737-1748.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A machine transliteration model based on correspondence between graphemes and phonemes.</title>
<date>2006</date>
<journal>Journal of ACM Trans. Asian Lang. Inf. Process.,</journal>
<pages>5--3</pages>
<contexts>
<context position="1502" citStr="Oh et al., 2006" startWordPosition="205" endWordPosition="208">fied model decomposes into two classes of non-parametric Bayesian component models: a Dirichlet process mixture model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models. 1 Introduction Machine transliteration methods can be categorized into phonetic-based models (Knight et al., 1998), spelling-based models (Brill et al., 2000), and hybrid models which utilize both phonetic and spelling information (Oh et al., 2005; Oh et al., 2006). Among them, statistical spelling-based models which directly align characters in the training corpus have become popular because they are language-independent, do not require phonetic knowledge, and are capable of achieving stateof-the-art performance (Zhang et al., 2012b). A major problem with real-word transliteration corpora is that they are usually not clean, may contain name pairs with various linguistic origins and this can hinder the performance of spelling-based models because names from different origins obey different pronunciation rules, for example: “Kim Jong-il/�1l,0” (Korea), “</context>
</contexts>
<marker>Oh, Choi, Isahara, 2006</marker>
<rawString>Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara. 2006. A machine transliteration model based on correspondence between graphemes and phonemes. Journal of ACM Trans. Asian Lang. Inf. Process., 5(3):185-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Ming Liu and A Kumaran.</title>
<date>2012</date>
<booktitle>2012a. Whitepaper of NEWS</booktitle>
<pages>1--9</pages>
<marker>Zhang, Li, 2012</marker>
<rawString>Min Zhang, Haizhou Li, Ming Liu and A Kumaran. 2012a. Whitepaper of NEWS 2012 shared task on machine transliteration. In Proc. of the 4th Named Entity Workshop (NEWS 2012), pages 1–9.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>A Kumaran and Ming Liu.</title>
<booktitle>2012b. Report of NEWS 2012 Machine Transliteration Shared Task. In Proc. of the 4th Named Entity Workshop (NEWS 2012),</booktitle>
<pages>10--20</pages>
<marker>Zhang, Li, </marker>
<rawString>Min Zhang, Haizhou Li, A Kumaran and Ming Liu. 2012b. Report of NEWS 2012 Machine Transliteration Shared Task. In Proc. of the 4th Named Entity Workshop (NEWS 2012), pages 10–20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>