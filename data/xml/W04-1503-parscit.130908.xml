<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003413">
<title confidence="0.990466">
A Simple String-Rewriting Formalism for Dependency Grammar
</title>
<author confidence="0.655347">
Alexis NASR
</author>
<affiliation confidence="0.332016">
Lattice, UFRL
</affiliation>
<address confidence="0.587218">
Universit´e Paris 7
F-75005 Paris
France
</address>
<email confidence="0.997169">
alexis.nasr@linguist.jussieu.fr
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928315789474">
Recently, dependency grammar has gained renewed
attention as empirical methods in parsing have
emphasized the importance of relations between
words, which is what dependency grammars model
explicitly, but context-free phrase-structure gram-
mars do not. While there has been much work on
formalizing dependency grammar and on parsing
algorithms for dependency grammars in the past,
there is not a complete generative formalization of
dependency grammar based on string-rewriting in
which the derivation structure is the desired depen-
dency structure. Such a system allows for the defi-
nition of a compact parse forest in a straightforward
manner. In this paper, we present a simple gen-
erative formalism for dependency grammars based
on Extended Context-Free Grammar, along with
a parser; the formalism captures the intuitions of
previous formalizations while deviating minimally
from the much-used Context-Free Grammar.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993435">
Dependency grammar has a long tradition in syn-
tactic theory, dating back to at least Tesni`ere’s work
from the thirties. Recently, it has gained renewed
attention as empirical methods in parsing have em-
phasized the importance of relations between words
(see, e.g., (Collins, 1997)), which is what depen-
dency grammars model explicitly, but context-free
phrase-structure grammars do not. In this paper, we
address an important issue in using grammar for-
malisms: the compact representation of the parse
forest. Why is this an important issue? It is well
known that for non-toy grammars and non-toy ex-
amples, a sentence can have a staggeringly large
number of analyses (for example, using a context-
free grammar (CFG) extracted from the Penn Tree-
bank, a sentence of 25 words may easily have
1,000,000 or more analyses). By way of an exam-
ple of an ambiguous sentence (though with only two
readings), the two dependency representations for
the ambiguous sentence (1) are given in Figure 1.
</bodyText>
<author confidence="0.6717">
Owen RAMBOW
</author>
<affiliation confidence="0.996672">
Columbia University
Department of Computer Science
</affiliation>
<address confidence="0.8752205">
1214 Amsterdam Avenue
New York, NY 10027-7003, USA
</address>
<email confidence="0.993705">
rambow@cs.columbia.edu
</email>
<listItem confidence="0.805914">
(1) Pilar saw a man with a telescope
</listItem>
<bodyText confidence="0.999967444444444">
It is clear that if we want to evaluate each possi-
ble analysis (be it using a probabilistic model or a
different method, for example a semantic checker),
we cannot efficiently do so if we enumerate all
cases.&apos; We have two options: we can either use a
greedy heuristic method for checking which does
not examine all possible solutions, which entails
we may miss the optimal solution, or we perform
our checking operation on a representation which
encodes all options in a compact representation.
This is possible because the exponential number of
possible analyses (exponential in the length of the
input sentence) share subanalyses, thus making a
polynomial-size representation possible. This rep-
resentation is called the shared parse forest and it
has been extensively studied for CFGs (see, for ex-
ample, (Lang, 1991)). To our knowledge, there has
been no description of the notion of shared parse
forest for dependency trees to date. In this paper, we
propose a formalization which is very closely based
on the shared parse forest for CFG. We achieve this
by defining a generative string-rewriting formal-
ism whose derivation trees are dependency trees.
The formalism, and the corresponding shared parse
forests, are used in a probabilistic chart parser for
dependency grammar, which is described in (Nasr
and Rambow, 2004b).
While there has been much work on formalizing
dependency grammar and on parsing algorithms for
dependency grammars in the past, we observe that
there is not, to our knowledge, a complete gener-
ative formalization2 of dependency grammar based
on string-rewriting in which the derivation structure
is exactly the desired dependency structure. The
most salient reason for the lack of such a gener-
ative dependency grammar is the absence of non-
</bodyText>
<footnote confidence="0.9762004">
1We would like to thank two anonymous reviewers for use-
ful comments.
2While the notions are related historically and conceptually,
we refer to a type of mathematical formalization, not to the
school of linguistics known as “Generative Grammar”.
</footnote>
<bodyText confidence="0.999784590909091">
terminal symbols in a dependency tree, which pre-
vents us from interpreting it as a derivation struc-
ture in a system that distinguishes between termi-
nal and nonterminal symbols. The standard solu-
tion to this problem, proposed by Gaifman (1965),
is to introduce nonterminal symbols denoting lex-
ical categories, as depicted in figure 2 (called the
“labelled phrase-structure trees induced by a depen-
dency tree” by Gaifman (1965)). Clearly, the “pure”
dependency tree can be derived in a straightforward
manner. The string rewriting system described in
(Gaifman, 1965) generates as derivation structures
this kind of trees.
There is however a deeper problem when con-
sidering dependency trees as derivation structures,
following from the fact that in a dependency tree,
modifiers3 are direct dependents of the head they
modify, and (in certain syntactic contexts) the num-
ber of modifiers is unbounded. Thus, if we wish to
obtain a tree as shown in Figure 2, we need to have
productions whose right-hand side is of unbounded
size, which is not possible in a context-free gram-
mar. Indeed, the formalization of dependency gram-
mar proposed by Gaifman (1965) is unsatisfactory
in that it does not allow for an unbounded number
of modifiers!
In this paper, we follow a suggestion made by
Abney (1996) and worked out in some detail in
(Lombardo, 1996)4 to extend Gaifman’s notation
with regular expressions, similar to the approach
used in extended context-free grammars. The re-
sult is a simple generative formalism which has
the property that the derivation structures are de-
pendency trees, except for the introduction of pre-
terminal nodes as shown in Figure 2. We do not
mean to imply that our formalism is substantially
different from previous formalizations of depen-
dency grammar; the goal of this paper is to present
a clean and easy-to-use generative formalism with
a straightforward notion of parse forest. In partic-
ular, our formalism, Generative Dependency Gram-
mar, allows for an unbounded number of daughter
nodes in the derivation tree through the use of reg-
ular expressions in its rules. The parser uses the
</bodyText>
<footnote confidence="0.972110181818182">
3We use the term modifier in its linguistic sense as a type of
syntactic dependency (another type being argument). We use
head (or mother) and dependent (or daughter) to refer to nodes
in a tree. Sometimes, in the formal and parsing literature, mod-
ifier is used to designate any dependent node, but we consider
that usage confusing because of the related but different mean-
ing of the term modifier that is well-established in the linguistic
literature.
4In fact, much of our formalism is very similar to (Lom-
bardo, 1996), who however does not discuss parsing (only
recognition), nor the representation of the parse forest.
</footnote>
<bodyText confidence="0.999465842105263">
corresponding finite-state machines which straight-
forwardly allows for a binary-branching representa-
tion of the derivation structure for the purpose of
parsing, and thus for a compact (polynomial and
not exponential) representation of the parse forest.
This formalism is based on previous work presented
in (Kahane et al., 1998), which has been substan-
tially reformulated in order to simplify it.5 In par-
ticular, we do not address non-projectivity here, but
acknowledge that for certain languages it is a cru-
cial issue. We will extend our basic approach in the
spirit of (Kahane et al., 1998) in future work.
The paper is structured as follows. We start
out by surveying previous formalizations of depen-
dency grammar in Section 2. In Section 3, we intro-
duce several formalisms, including Generative De-
pendency Grammar. We present a parsing algorithm
in Section 4, and mention empirical results in Sec-
tion 5. We then conclude.
</bodyText>
<sectionHeader confidence="0.9559635" genericHeader="method">
2 Previous Formalizations of Dependency
Grammar
</sectionHeader>
<bodyText confidence="0.999969636363636">
We start out by observing that “dependency gram-
mar” should be contrasted with “phrase structure
grammar”, not “CFG”, which is a particular formal-
ization of phrase structure grammar. Thus, just as it
only makes sense to study the formal properties of
a particular formalization of phrase structure gram-
mar, the question about the formal properties of de-
pendency grammar in general is not well defined,
nor the question of a comparison of a dependency
formalism with dependency grammar.
There have been (at least) four types of formal-
izations of dependency grammars in the past.6 None
of these approaches, to our knowledge, discuss the
notion of shared parse forest. The first approach
(for example, (Lombardo and Lesmo, 2000)) fol-
lows Gaifman (1965) in proposing traditional string
rewriting rules, which however do not allow for an
unbounded number of adjuncts.
In the second approach, the dependency structure
is constructed in reference to a parallel (“deeper”)
structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Be-
cause the rules make reference to other struc-
</bodyText>
<footnote confidence="0.988193181818182">
5Kahane et al. (1998) present three different types of rules,
for subcategorization, modification, and linear precedence. In
the formalism presented in this paper, they have been collapsed
into one.
6We leave aside here work on tree rewriting systems such
as Tree Adjoining Grammar, which, when lexicalized, have
derivation structures which are very similar to dependency
trees. See (Rambow and Joshi, 1997) for a discussion related
to TAG, and see (Rambow et al., 2001) for the definition of a
tree-rewriting system which can be used to develop grammars
whose derivations faithfully mirror syntactic dependency.
</footnote>
<figure confidence="0.989843428571428">
man
�� � �
a with
telescope
saw
�� �
�
Pilar
saw
���� � � � �
Pilar man with
a telescope
a
a
</figure>
<figureCaption confidence="0.999931">
Figure 1: Two dependency trees
Figure 2: Two dependency trees with lexical categories
</figureCaption>
<figure confidence="0.997433652173913">
V
V
N
N
saw
saw
N
with
a
N P
Pilar
Pilar
P
N
D
telesope
a
a
D telesope
a
D man
D man
with N
</figure>
<bodyText confidence="0.999110897959184">
tures, these approaches cannot be formalized in
a straightforward manner as context-free rewriting
formalisms.
In the third approach, which includes formaliza-
tions of dependency structure such as Dependency
Tree Grammar of Modina (see (Dikovsky and Mod-
ina, 2000) for an overview), Link Grammar (Sleator
and Temperley, 1993) or the tree-composition ap-
proach of Nasr (1996), rules construct the depen-
dency tree incrementally; in these approaches, the
grammar licenses dependency relations which, in a
derivation, are added to the tree one by one, or in
groups. In contrast, we are interested in a string-
rewriting system; in such a system, we cannot add
dependency relations incrementally: all daughters
of a node must be added at once to represent a sin-
gle rewrite step.
In the fourth approach, the dependency grammar
is converted into a headed context-free grammar
(Abney, 1996; Holan et al., 1998), also the Basic
Dependency Grammar of Beletskij (1967) as cited
in (Dikovsky and Modina, 2000). This approach al-
lows for the recovery of the dependency structure
both from the derivation tree and from a parse for-
est represented in polynomial space. (In fact, our
parsing algorithm draws on this work.) However,
the approach of course requires the introduction of
additional nonterminal nodes. Finally, we observe
that Recursive Transition Networks (Woods, 1970)
can be used to encode a grammar whose deriva-
tion trees are dependency trees. However, they are
more a general framework for encoding grammars
than a specific type of grammar (for example, we
can also use them to encode CFGs). In a some-
what related manner, Alshawi et al. (2000) use cas-
caded head automata to derive dependency trees, but
leave the nature of the cascading under-formalized.
Eisner (2000) provides a formalization of a system
that uses two different automata to generate left and
right children of a head. His formalism is very close
to the one we present, but it is not a string-rewriting
formalism (and not really generative at all). We
are looking for a precise formulation of a genera-
tive dependency grammar, and the question has re-
mained open whether there is an alternate formal-
ism which allows for an unbounded number of ad-
juncts, introduces all daughter nodes at once in a
string-rewriting step, and avoids the introduction of
additional nonterminal nodes.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="method">
3 Formalism
</sectionHeader>
<bodyText confidence="0.996102086956522">
In this section we first review the definition of Ex-
tended Context-Free Grammar and then show how
we use it to model dependency derivations. An Ex-
tended Context-Free Grammar (or ECFG for short)
is like a context-free grammar (CFG), except that
the right-hand side is a regular expression over the
terminal and nonterminal symbols of the grammar.
At each step in a derivation, we first choose a rewrite
rule (as we do in CFG), and then we choose a string
which is in the language denoted by the regular ex-
pression associated with the rule. This string is then
treated like the right-hand side of a CFG rewrite
rule.
In the following, if G is a grammar and R a regu-
lar expression, then L(G) denotes the language gen-
erated by the grammar and L(R) the language de-
noted by the regular expression. If F is a class of
grammars (such as CFG), then L(F) denote the class
of languages generated by the grammars in F. We
now give a formal definition, which closely follows
that given by Albert et al. (1999).7
A Extended Context-Free Grammar is a 4-
tuple (VN, VT, P, 5), where:
</bodyText>
<listItem confidence="0.992125625">
• VN is a finite set of nonterminal symbols,
• VT is a finite set of terminal symbols (disjoint
from VN),
• P is a finite set of rules, which are ordered
pairs consisting of an element of VN and a reg-
ular expression over VN U VT,
• 5, a subset of VN, contains the possible start
symbols.
</listItem>
<bodyText confidence="0.993944206349207">
We will use the traditional arrow notation (−�)
to write rules.
For A E VN and u, v E (VN U VT) we say that
uAv yields uwv (written uAv ==�- uwv) if A −�
R is in P and w is in L(R). The transitive closure
of the yield relation (denoted  =� ) is defined in the
usual manner.
The language generated by a Extended Context-
Free Grammar is the set {w E VT  |A ===&gt; w, A E
5}.
We now define a restricted version of ECFG
which we will use for defining dependency gram-
mars. The only new formal requirement is that
the rules be lexicalized in the sense of (Joshi and
Schabes, 1991). For our formalism, this means
that the regular expression in a production is such
that each string in its denoted language contains at
least one terminal symbol. Linguistically speaking,
this means that each rule is associated with exactly
7ECFG has been around informally since the sixties (e.g.,
the Backus-Naur form); for a slightly different formalization,
see (Madsen and Kristensen, 1976), whose definition allows
for an infinite rule set.
one lexical item (which may be multi-word). We
will call this particular type of Extended Context-
Free Grammar a lexicalized Extended Context-
Free Grammar or, for obvious reasons, a Genera-
tive Dependency Grammar (GDG for short). When
we use a GDG for linguistic description, its left-
hand side nonterminal will be interpreted as the lex-
ical category of the lexical item and will represent
its maximal projection.$
A Generative Dependency Grammar is a lexi-
calized ECFG.
It is sometimes useful to have dependency repre-
sentations with labeled arcs (typically labeled with
syntactic functions such as SUBJ for subject or ADJ
for adjunct). There are different ways of achieving
this goal; here, we discuss the use of feature struc-
tures in conjunction with the nonterminal symbols,
for example N[gf:subj] instead of just N. Fea-
ture structures are of course useful for other reasons
as well, such as expressing morphological features.
In terms of our formalism, the use of bounded fea-
ture structures can be seen as a shorthand notation
for an increased set of nonterminal symbols. The
use of feature structures (rather than simple nonter-
minal symbols) allows for a more perspicuous rep-
resentation of linguistic relations through the use of
underspecification. Note that the use of underspec-
ified feature structures in rules can potentially lead
to an exponential increase (exponential in the size
of the set of feature values) of the size of the set of
rules if rules contain underspecified feature struc-
tures on the right-hand side. However, we note that
the feature representing, grammatical function will
presumably always be fully specified on the right-
hand side of a rule (the head determines the func-
tion of its dependents). Underspecification in the
left-hand side of a rule only leads to linear compact-
ification of the rule set.
We now give a toy linguistic example. We let
GLing be (VN, VT, P, 5) as follows:
</bodyText>
<listItem confidence="0.9998135">
• VN = {V, N, D, A, Adv}
• VT = { Pilar, saw, man, a, telescope, with, tall,
very }
• P consists of the following rules:
</listItem>
<equation confidence="0.976074333333333">
p1 : V −� N saw N P
p2 : N −�
(Pilar  |D (A) (man  |telescope) P )
</equation>
<footnote confidence="0.984149">
8For practical purposes, we can separate the lexicon (which
assigns lexical categories to lexemes) from the syntactic rules
(which hold for all lexemes of a class), as does Gaifman (1965),
resulting in a straightforward notational extension to our for-
malism.
</footnote>
<figure confidence="0.984896333333333">
V
= N saw N P
p1
p2p2p3
= Pilar saw D man with N
p6p2
= Pilar saw a man with D telescope
= Pilar saw a man with a telescope
p6
</figure>
<figureCaption confidence="0.999423">
Figure 3: A sample GDG derivation
</figureCaption>
<equation confidence="0.99262375">
p3 : P − with N
p4 : A − Adv* tall
p5 : Adv − very
p6 : D − a
</equation>
<listItem confidence="0.721048">
• S = {V }
</listItem>
<bodyText confidence="0.995746875">
A derivation is shown in Figure 3; the corre-
sponding derivation tree is shown in the right part
of Figure 2. As can be seen, the derivation structure
is a dependency tree, except for the use of pretermi-
nals, as we desired.
The first part of the following theorem follows
from the existence of a Greibach Normal Form for
ECFG (Albert et al., 1999). The second part follows
immediately from the closure of CFG under regular
substitution.
L(GDG) = L(ECFG) = L(CFG).
Of course, ECFG, GDG and CFG are not strongly
equivalent in the standard sense for string rewrit-
ing systems of having the same sets of derivation
trees. Clearly, ECFG can generate all sets of deriva-
tion trees that GDG can, while CFG cannot (because
of the unbounded branching factor of ECFG and of
GDG); ECFG can also generate all sets of deriva-
tion trees that CFG can, while GDG cannot (because
of the lexicalization requirement). ECFG thus has
a greater strong generative capacity than CFG and
GDG, while those of GDG and CFG are incompa-
rable.
It is interesting to notice the difference between
the rewriting operation of a nonterminal symbol as
defined for a ECFG or a GDG and the equivalent
rewriting steps with a weakly equivalent CFG. A
GDG rewriting operation of a symbol X using a
rule r is decomposed in two stages, the first stage
consists in choosing a string w which belongs to the
set denoted by the right-hand side of r. During the
second stage, X is replaced by w. These two stages
are of a different nature, the first concerns the gener-
ation of CFG rules (and hence a CFG) using a GDG
while the second concerns the generation of a string
using the generated CFG. The equivalent rewriting
operation (X  w) with a CFG does not distin-
guish the same two stages, both the selection of w
and the rewriting of X as w are done at the same
time.
</bodyText>
<figureCaption confidence="0.997617">
Figure 4: A packed parse forest
</figureCaption>
<sectionHeader confidence="0.986325" genericHeader="method">
4 Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.990996944444445">
The parsing algorithm given here is a simple exten-
sion of the CKY algorithm. The difference is in the
use of finite state machines in the items in the chart
to represent the right-hand sides of the rules of the
ECFG.9 A rule with category C as its left-hand side
will give rise to a finite state machine which we call
a C-rule FSM; its final states mark the completed
recognition of a constituent of label C.
CKY-Style parsing algorithm for Extended
Context-Free Grammars.
Input. A ECFG G and an input string W =
w1 ··· wn.
Output. The parse table T for W such that ti,j
contains (M, q) iff M is a C-rule-FSM, q is one
of the final states of M, and we have a derivation
C += wi · · · wj. If i = j, ti,j also contains the in-
put symbol wi.
Method.
</bodyText>
<listItem confidence="0.9898507">
• Initialization: For each i, 1  i  n, add wi
to ti,i.
• Completion: If ti,j contains either the input
symbol w or an item (M, q) such that q is
a final state of M, and M is a C-rule-FSM,
then add to ti,j all (M&apos;, q&apos;) such that M&apos; is a
rule-FSM which transitions from a start state
to state q&apos; on input w or C. Add a single back-
pointer from (M&apos;, q&apos;) in ti,j to (M, q) or w in
ti,j.
</listItem>
<footnote confidence="0.9784492">
9Recent work in the context of using ECFG for pars-
ing SGML and XML proposes an LL-type parser for ECFG
(Br¨uggemann-Klein and Wood, 2003); their approach also ex-
ploits the automaton representation of the right-hand side of
rules, as is natural for an algorithm dealing with ECFG.
</footnote>
<figure confidence="0.997965222222222">
Pilar a
a
(m1,4) (m1,4)
(m2,4)
(m1,4)
(m3,3)
(m1,3)
(m1,2)
(m2,4)
(m3,2) (m2,4)
with
man
saw
(m2,3) telescope
(m2,3)
(m2,4)
(13,2)
(13,2)
</figure>
<figureCaption confidence="0.999120666666667">
Figure 5: Three rules FSM m1, m2 and m3. m1 is a V-rule-FSM corresponding to rule p1, m2 is an
N-rule-FSM which corresponds to rule p2 and m3 is a P-rule-FSM which corresponds to rule p3
Figure 6: A parse forest
</figureCaption>
<figure confidence="0.975192066666667">
m1
1 2 3
N saw N
4 P
Pilar
D A man
1 2 3 4 P
telescope
m2
man
telescope
m3
1
with N
2 3
</figure>
<equation confidence="0.951015714285714">
V(3)
N(4)
Pilar
V(4)
saw
N(3)
D(2)
a
V(5)
N(4)
man P(2) N(4)
N(4)
with
P(3)
N(3) telescope
D(2)
a
V(3)
N(4)
Pilar
V(4)
saw
V(5)
N(3)
D(2)
a
N(4)
V(5)
man
with
P(2) N(4)
P(3)
N(3) telescope
D(2)
a
</equation>
<listItem confidence="0.813805666666667">
• Scanning: If (M1, q1) is in ti,k, and tk+1,j
contains either the input symbol w or the item
(M2, q2) where q2 is a final state and M2 is a
C-rule-FSM, then add (M1, q) to ti,j (if not al-
ready present) if M1 transitions from q1 to q on
either w or C. Add a double backpointer from
(M1, q) in ti,j to (M1, q1) in ti,k (left back-
pointer) and to either w or (M2, q2) in tk+1,j
(right backpointer).
</listItem>
<bodyText confidence="0.99992585">
At the end of the parsing process, a packed parse
forest has been built. The packed forest correspond-
ing to the parse of sentence Pilar saw a man with a
telescope, using the grammar of Section 3 is repre-
sented in Figure 4. The nonterminal nodes are la-
beled with pairs (M, q) where M is an rule-FSM
and q a state of this FSM. Three rule-FSMs corre-
sponding to rules p1, p2 and p3 have been repre-
sented in Figure 5.
Obtaining the dependency trees from the packed
parse forest is performed in two stages. In a first
stage, a forest of binary syntagmatic trees is ob-
tained from the packed forest and in a second stage,
each syntagmatic tree is transformed into a depen-
dency tree. We shall not give the details of these
processes. The two trees resulting from de-packing
of Figure 4 are represented in Figure 6. The dif-
ferent nodes of the syntagmatic tree that will be
grouped in a single node of the dependency trees
have been circled.
</bodyText>
<sectionHeader confidence="0.975321" genericHeader="method">
5 Empirical Results
</sectionHeader>
<bodyText confidence="0.99993361971831">
While the presentation of empirical results is not the
object of this paper, we give an overview of some
empirical work using ECFG for natural language
processing in this section. For full details, we refer
to (Nasr and Rambow, 2004a; Nasr and Rambow,
2004b; Nasr, 2004).
The parser presented in Section 4 above has been
implemented. We have investigated the use the
parser in a two-step probabilistic framework. In a
first step, we determine which rules of the ECFG
should be used for each word in the input sentence.
(Recall that a grammar rule encodes the active and
passive valency, as well as how any arguments are
realized, for example, fronted or in canonical posi-
tion.) This step is called supertagging and has been
suggested and studied in the context of Tree Adjoin-
ing Grammar by Bangalore and Joshi (1999). In
a second step, we use a probabilistic ECFG where
the probabilities are non-lexical and are based en-
tirely on the grammar rules. We extract the most
probable derivation from the compact parse forest
using dynamic programming in the usual manner.
This non-lexical probability model is used because
the supertagging step already takes the words in the
sentence into account. The probabilities can be en-
coded directly as weights on the transitions in the
rule-FSMs used by the parser.
The ECFG grammar we use has been automati-
cally extracted from the Penn Treebank (PTB). In
fact, we first extract a Tree Insertion Grammar fol-
lowing the work of (Xia et al., 2000; Chen, 2001;
Chiang, 2000), and then directly convert the trees
of the obtained TAG into automata for the parser.
It is clear that one could also derive an explicit
ECFG in the same manner. The extracted gram-
mar has about 4.800 rules. The probabilities are
estimated from the corpus during extraction. Note
that there are many different ways of extracting an
ECFG from the PTB, corresponding to different the-
ories of syntactic dependency. We have chosen to
directly model predicate-argument relations rather
than more surface-oriented syntactic relations such
as agreement, so that all function words (determin-
ers, auxiliaries, and so on) depend on the lexical
word. Strongly governed prepositions are treated as
part of a lexeme rather than as full prepositions.
We have investigated several different ways of
modeling the probability of attaching a sequence of
modifiers at a certain point in the derivation (con-
ditioning on the position of the modifier in the se-
quence or conditioning on the previous modifier
used). We found that using position or context im-
proves on using neither.
We have performed two types of experiments: us-
ing the correct ECFG rule for each word, and as-
signing ECFG rules automatically using supertag-
ging. In the case of using the correct supertag, we
obtain unlabeled dependency accuracies of about
98% (i.e., in about 2% of cases a word is assigned
a wrong governor). Automatic supertagging (us-
ing standard n-gram tagging methods) for a gram-
mar our size has an accuracy of about 80%. This
is also approximately the dependency accuracy ob-
tained when parsing the output of a supertagger. We
conclude from this performance that if we can in-
crease the performance of the supertagger, we can
also directly increase the performance of the parser.
Current work includes examining which grammati-
cal distinctions the grammar should make in order to
optimize both supertagging and parsing (Toussenel,
2004).
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969666666667">
We have presented a generative string-rewriting
system, Extended Context-Free Grammar, whose
derivation trees are dependency trees with un-
bounded branching factor. We have shown how we
can reuse the representation of shared parse forests
well-known from CFGs for Extended Context-Free
Grammar. The question arises whether we can rep-
resent the shared parse forest in a manner more di-
rectly in the spirit of dependency. This question was
investigated by (Nasr, 2003). He shows that the fac-
toring realized in the shared forest presented here
and which is the key to the polynomial represen-
tation of a potentially exponential set, can be done
directly on the dependency trees by introducing the
notion of dependency sets.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998618717557252">
Abney, Steven (1996). A grammar of projections.
Unpublished manuscript, Universit¨at T¨ubingen.
Albert, J¨urgen; Giammarresi, Dora; and Wood, Der-
ick (1999). Extended context-free grammars and
normal form algorithms. In Champarnaud, Jean-
Marc; Maurel, Denis; and Ziadi, Djelloul, ed-
itors, Automata Implementations: Third Inter-
national Workshop on Implementing Automata
(WIA’98), volume 1660 of LNCS, pages 1–12.
Springer Verlag.
Alshawi, Hiyan; Bangalore, Srinivas; and Douglas,
Shona (2000). Learning dependency translation
models as collections of finite-state head trans-
ducers. cl, 26(1):45–60.
Bangalore, Srinivas and Joshi, Aravind (1999).
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237–266.
Br¨uggemann-Klein, Anne and Wood, Derick
(2003). The parsing of extended context-free
grammars. Unpublished manuscript, Technische
Universit¨at M¨unchen and Hong Kong University
of Science &amp; Technology.
Chen, John (2001). Towards Efficient Statistical
Parsing Using Lexicalized Grammatical Infor-
mation. PhD thesis, University of Delaware.
Chiang, David (2000). Statistical parsing with an
automatically-extracted tree adjoining grammar.
In 38th Meeting of the Association for Compu-
tational Linguistics (ACL’00), pages 456–463,
Hong Kong, China.
Collins, Michael (1997). Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics, Madrid,
Spain.
Dikovsky, Alexander and Modina, Larissa (2000).
Dependencies on the other side of the curtain.
Traitement Automatique des Langues, 41(1):79–
111.
Eisner, Jason (2000). Bilexical grammars and their
cubic-time parsing algorithms. In Bunt, Harry C.
and Nijholt, Anton, editors, New Developments
in Natural Language Parsing. Kluwer Academic
Publishers.
Gaifman, Haim (1965). Dependency systems and
phrase-structure systems. Information and Con-
trol, 8:304–337.
Holan, Tom´aˇs; Kuboˇn, Vladislav; Oliva, Karel; and
Pl´atek, Martin (1998). Two useful measures of
word order complexity. In Kahane, Sylvain and
Polgu`ere, Alain, editors, Processing of Depen-
dency Grammars: Proceeding of the Workshop,
pages 21–28, Montr´eal, Canada. ACL/COLING.
Joshi, Aravind K. and Schabes, Yves (1991). Tree-
adjoining grammars and lexicalized grammars.
In Nivat, Maurice and Podelski, Andreas, editors,
Definability and Recognizability of Sets of Trees.
Elsevier.
Kahane, Sylvain; Nasr, Alexis; and Rambow, Owen
(1998). Pseudo-projectivity: A polynomially
parsable non-projective dependency grammar. In
36th Meeting of the Association for Computa-
tional Linguistics and 17th International Con-
ference on Computational Linguistics (COLING-
ACL’98), pages 646–652, Montr´eal, Canada.
Lang, Bernard (1991). Towards a uniform formal
framework for parsing. In Tomita, M., editor,
Current Issues in Parsing technology, chapter 11,
pages 153–171. Kluwer Academic Publishers.
Lombardo, Vincenzo (1996). An Earley-style
parser for dependency grammars. In Proceedings
of the 16th International Conference on Compu-
tational Linguistics (COLING’96), Copenhagen.
Lombardo, Vincenzo and Lesmo, Leonardo (2000).
A formal theory of dependency syntax with
empty units. Traitement automatque des langues,
41(1):179–209.
Madsen, O.L. and Kristensen, B.B. (1976). LR-
parsing of extended context-free grammars. Acta
Informatica, 7:61–73.
Mel’ˇcuk, Igor A. (1988). Dependency Syntax: The-
ory and Practice. State University of New York
Press, New York.
Nasr, Alexis (1996). Un syst`eme de reformula-
tion automatique de phrases fond´e sur la Th´eorie
Sens-Texte : application aux langues contrˆol´ees.
PhD thesis, Universit´e Paris 7.
Nasr, Alexis (2003). Factoring sufrace syntac-
tic structures. In First International Conference
on Meaning-Text Theory, pages 249–258, Paris,
France.
Nasr, Alexis (2004). Grammaires de d´ependances
g´en´eratives: exp´eriences sur le Corpus Paris 7.
Unpublished manuscript, Universit´e Paris 7.
Nasr, Alexis and Rambow, Owen (2004a). Depen-
dency parsing based on n-best-path supertagging.
Unpublished manuscript, Universit´e Paris 7 and
COlumbia University.
Nasr, Alexis and Rambow, Owen (2004b). Su-
pertagging and full parsing. In Proceedings of
the Workshop on Tree Adjoining Grammar and
Related Formalisms (TAG+7), Vancouver, BC,
Canada.
Rambow, Owen and Joshi, Aravind (1997). A for-
mal look at dependency grammars and phrase-
structure grammars, with special consideration
of word-order phenomena. In Wanner, Leo,
editor, Recent Trends in Meaning-Text Theory,
pages 167–190. John Benjamins, Amsterdam and
Philadelphia.
Rambow, Owen; Vijay-Shanker, K.; and Weir,
David (2001). D-Tree Substitution Grammars.
Computational Linguistics, 27(1).
Sgall, P.; Hajiˇcov´a, E.; and Panevov´a, J. (1986).
The meaning of the sentence and its semantic and
pragmatic aspects. Reidel, Dordrecht.
Sleator, Daniel and Temperley, Davy (1993). Pars-
ing english with a link grammar,. In Proceedings
of the Third International Workshop on Parsing
Technologies IIWPT’93).
Toussenel, Franc¸ois (2004). Why supertagging is
hard. In Proceedings of the Workshop on Tree
Adjoining Grammar and Related Formalisms
(TAG+7), Vancouver, BC, Canada.
Woods, William A. (1970). Transition network
grammars for natural language analysis. Com-
mun. ACM, 3(10):591–606.
Xia, Fei; Palmer, Martha; and Joshi, Aravind
(2000). A uniform method of grammar extrac-
tion and its applications. In Proc. of the EMNLP
2000, Hong Kong.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696232">
<title confidence="0.997974">A Simple String-Rewriting Formalism for Dependency Grammar</title>
<author confidence="0.94611">Alexis</author>
<affiliation confidence="0.954133">Lattice, Universit´e Paris</affiliation>
<address confidence="0.891889">F-75005</address>
<email confidence="0.992762">alexis.nasr@linguist.jussieu.fr</email>
<abstract confidence="0.99475305">Recently, dependency grammar has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words, which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not. While there has been much work on formalizing dependency grammar and on parsing algorithms for dependency grammars in the past, there is not a complete generative formalization of dependency grammar based on string-rewriting in which the derivation structure is the desired dependency structure. Such a system allows for the definition of a compact parse forest in a straightforward manner. In this paper, we present a simple generative formalism for dependency grammars based on Extended Context-Free Grammar, along with a parser; the formalism captures the intuitions of previous formalizations while deviating minimally from the much-used Context-Free Grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>A grammar of projections.</title>
<date>1996</date>
<institution>Universit¨at T¨ubingen.</institution>
<note>Unpublished manuscript,</note>
<contexts>
<context position="5554" citStr="Abney (1996)" startWordPosition="870" endWordPosition="871">ency trees as derivation structures, following from the fact that in a dependency tree, modifiers3 are direct dependents of the head they modify, and (in certain syntactic contexts) the number of modifiers is unbounded. Thus, if we wish to obtain a tree as shown in Figure 2, we need to have productions whose right-hand side is of unbounded size, which is not possible in a context-free grammar. Indeed, the formalization of dependency grammar proposed by Gaifman (1965) is unsatisfactory in that it does not allow for an unbounded number of modifiers! In this paper, we follow a suggestion made by Abney (1996) and worked out in some detail in (Lombardo, 1996)4 to extend Gaifman’s notation with regular expressions, similar to the approach used in extended context-free grammars. The result is a simple generative formalism which has the property that the derivation structures are dependency trees, except for the introduction of preterminal nodes as shown in Figure 2. We do not mean to imply that our formalism is substantially different from previous formalizations of dependency grammar; the goal of this paper is to present a clean and easy-to-use generative formalism with a straightforward notion of p</context>
<context position="10789" citStr="Abney, 1996" startWordPosition="1728" endWordPosition="1729">r an overview), Link Grammar (Sleator and Temperley, 1993) or the tree-composition approach of Nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in a derivation, are added to the tree one by one, or in groups. In contrast, we are interested in a stringrewriting system; in such a system, we cannot add dependency relations incrementally: all daughters of a node must be added at once to represent a single rewrite step. In the fourth approach, the dependency grammar is converted into a headed context-free grammar (Abney, 1996; Holan et al., 1998), also the Basic Dependency Grammar of Beletskij (1967) as cited in (Dikovsky and Modina, 2000). This approach allows for the recovery of the dependency structure both from the derivation tree and from a parse forest represented in polynomial space. (In fact, our parsing algorithm draws on this work.) However, the approach of course requires the introduction of additional nonterminal nodes. Finally, we observe that Recursive Transition Networks (Woods, 1970) can be used to encode a grammar whose derivation trees are dependency trees. However, they are more a general framew</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Abney, Steven (1996). A grammar of projections. Unpublished manuscript, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨urgen Albert</author>
<author>Dora Giammarresi</author>
<author>Derick Wood</author>
</authors>
<title>Extended context-free grammars and normal form algorithms.</title>
<date>1999</date>
<booktitle>Automata Implementations: Third International Workshop on Implementing Automata (WIA’98),</booktitle>
<volume>1660</volume>
<pages>1--12</pages>
<editor>In Champarnaud, JeanMarc; Maurel, Denis; and Ziadi, Djelloul, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="13260" citStr="Albert et al. (1999)" startWordPosition="2151" endWordPosition="2154">we first choose a rewrite rule (as we do in CFG), and then we choose a string which is in the language denoted by the regular expression associated with the rule. This string is then treated like the right-hand side of a CFG rewrite rule. In the following, if G is a grammar and R a regular expression, then L(G) denotes the language generated by the grammar and L(R) the language denoted by the regular expression. If F is a class of grammars (such as CFG), then L(F) denote the class of languages generated by the grammars in F. We now give a formal definition, which closely follows that given by Albert et al. (1999).7 A Extended Context-Free Grammar is a 4- tuple (VN, VT, P, 5), where: • VN is a finite set of nonterminal symbols, • VT is a finite set of terminal symbols (disjoint from VN), • P is a finite set of rules, which are ordered pairs consisting of an element of VN and a regular expression over VN U VT, • 5, a subset of VN, contains the possible start symbols. We will use the traditional arrow notation (−�) to write rules. For A E VN and u, v E (VN U VT) we say that uAv yields uwv (written uAv ==�- uwv) if A −� R is in P and w is in L(R). The transitive closure of the yield relation (denoted  =</context>
<context position="17585" citStr="Albert et al., 1999" startWordPosition="2941" endWordPosition="2944">nal extension to our formalism. V = N saw N P p1 p2p2p3 = Pilar saw D man with N p6p2 = Pilar saw a man with D telescope = Pilar saw a man with a telescope p6 Figure 3: A sample GDG derivation p3 : P − with N p4 : A − Adv* tall p5 : Adv − very p6 : D − a • S = {V } A derivation is shown in Figure 3; the corresponding derivation tree is shown in the right part of Figure 2. As can be seen, the derivation structure is a dependency tree, except for the use of preterminals, as we desired. The first part of the following theorem follows from the existence of a Greibach Normal Form for ECFG (Albert et al., 1999). The second part follows immediately from the closure of CFG under regular substitution. L(GDG) = L(ECFG) = L(CFG). Of course, ECFG, GDG and CFG are not strongly equivalent in the standard sense for string rewriting systems of having the same sets of derivation trees. Clearly, ECFG can generate all sets of derivation trees that GDG can, while CFG cannot (because of the unbounded branching factor of ECFG and of GDG); ECFG can also generate all sets of derivation trees that CFG can, while GDG cannot (because of the lexicalization requirement). ECFG thus has a greater strong generative capacity </context>
</contexts>
<marker>Albert, Giammarresi, Wood, 1999</marker>
<rawString>Albert, J¨urgen; Giammarresi, Dora; and Wood, Derick (1999). Extended context-free grammars and normal form algorithms. In Champarnaud, JeanMarc; Maurel, Denis; and Ziadi, Djelloul, editors, Automata Implementations: Third International Workshop on Implementing Automata (WIA’98), volume 1660 of LNCS, pages 1–12. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite-state head transducers. cl,</title>
<date>2000</date>
<pages>26--1</pages>
<contexts>
<context position="11550" citStr="Alshawi et al. (2000)" startWordPosition="1851" endWordPosition="1854">or the recovery of the dependency structure both from the derivation tree and from a parse forest represented in polynomial space. (In fact, our parsing algorithm draws on this work.) However, the approach of course requires the introduction of additional nonterminal nodes. Finally, we observe that Recursive Transition Networks (Woods, 1970) can be used to encode a grammar whose derivation trees are dependency trees. However, they are more a general framework for encoding grammars than a specific type of grammar (for example, we can also use them to encode CFGs). In a somewhat related manner, Alshawi et al. (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head. His formalism is very close to the one we present, but it is not a string-rewriting formalism (and not really generative at all). We are looking for a precise formulation of a generative dependency grammar, and the question has remained open whether there is an alternate formalism which allows for an unbounded number of adjuncts, introduces all daughter n</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Alshawi, Hiyan; Bangalore, Srinivas; and Douglas, Shona (2000). Learning dependency translation models as collections of finite-state head transducers. cl, 26(1):45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="23316" citStr="Bangalore and Joshi (1999)" startWordPosition="4031" endWordPosition="4034">etails, we refer to (Nasr and Rambow, 2004a; Nasr and Rambow, 2004b; Nasr, 2004). The parser presented in Section 4 above has been implemented. We have investigated the use the parser in a two-step probabilistic framework. In a first step, we determine which rules of the ECFG should be used for each word in the input sentence. (Recall that a grammar rule encodes the active and passive valency, as well as how any arguments are realized, for example, fronted or in canonical position.) This step is called supertagging and has been suggested and studied in the context of Tree Adjoining Grammar by Bangalore and Joshi (1999). In a second step, we use a probabilistic ECFG where the probabilities are non-lexical and are based entirely on the grammar rules. We extract the most probable derivation from the compact parse forest using dynamic programming in the usual manner. This non-lexical probability model is used because the supertagging step already takes the words in the sentence into account. The probabilities can be encoded directly as weights on the transitions in the rule-FSMs used by the parser. The ECFG grammar we use has been automatically extracted from the Penn Treebank (PTB). In fact, we first extract a</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Bangalore, Srinivas and Joshi, Aravind (1999). Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Br¨uggemann-Klein</author>
<author>Derick Wood</author>
</authors>
<title>The parsing of extended context-free grammars.</title>
<date>2003</date>
<institution>Technische Universit¨at M¨unchen and Hong Kong University of Science &amp; Technology.</institution>
<note>Unpublished manuscript,</note>
<marker>Br¨uggemann-Klein, Wood, 2003</marker>
<rawString>Br¨uggemann-Klein, Anne and Wood, Derick (2003). The parsing of extended context-free grammars. Unpublished manuscript, Technische Universit¨at M¨unchen and Hong Kong University of Science &amp; Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
</authors>
<title>Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>University of Delaware.</institution>
<contexts>
<context position="23991" citStr="Chen, 2001" startWordPosition="4147" endWordPosition="4148">lities are non-lexical and are based entirely on the grammar rules. We extract the most probable derivation from the compact parse forest using dynamic programming in the usual manner. This non-lexical probability model is used because the supertagging step already takes the words in the sentence into account. The probabilities can be encoded directly as weights on the transitions in the rule-FSMs used by the parser. The ECFG grammar we use has been automatically extracted from the Penn Treebank (PTB). In fact, we first extract a Tree Insertion Grammar following the work of (Xia et al., 2000; Chen, 2001; Chiang, 2000), and then directly convert the trees of the obtained TAG into automata for the parser. It is clear that one could also derive an explicit ECFG in the same manner. The extracted grammar has about 4.800 rules. The probabilities are estimated from the corpus during extraction. Note that there are many different ways of extracting an ECFG from the PTB, corresponding to different theories of syntactic dependency. We have chosen to directly model predicate-argument relations rather than more surface-oriented syntactic relations such as agreement, so that all function words (determine</context>
</contexts>
<marker>Chen, 2001</marker>
<rawString>Chen, John (2001). Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information. PhD thesis, University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In 38th Meeting of the Association for Computational Linguistics (ACL’00),</booktitle>
<pages>456--463</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="24006" citStr="Chiang, 2000" startWordPosition="4149" endWordPosition="4150">on-lexical and are based entirely on the grammar rules. We extract the most probable derivation from the compact parse forest using dynamic programming in the usual manner. This non-lexical probability model is used because the supertagging step already takes the words in the sentence into account. The probabilities can be encoded directly as weights on the transitions in the rule-FSMs used by the parser. The ECFG grammar we use has been automatically extracted from the Penn Treebank (PTB). In fact, we first extract a Tree Insertion Grammar following the work of (Xia et al., 2000; Chen, 2001; Chiang, 2000), and then directly convert the trees of the obtained TAG into automata for the parser. It is clear that one could also derive an explicit ECFG in the same manner. The extracted grammar has about 4.800 rules. The probabilities are estimated from the corpus during extraction. Note that there are many different ways of extracting an ECFG from the PTB, corresponding to different theories of syntactic dependency. We have chosen to directly model predicate-argument relations rather than more surface-oriented syntactic relations such as agreement, so that all function words (determiners, auxiliaries</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>Chiang, David (2000). Statistical parsing with an automatically-extracted tree adjoining grammar. In 38th Meeting of the Association for Computational Linguistics (ACL’00), pages 456–463, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1378" citStr="Collins, 1997" startWordPosition="196" endWordPosition="197">act parse forest in a straightforward manner. In this paper, we present a simple generative formalism for dependency grammars based on Extended Context-Free Grammar, along with a parser; the formalism captures the intuitions of previous formalizations while deviating minimally from the much-used Context-Free Grammar. 1 Introduction Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni`ere’s work from the thirties. Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not. In this paper, we address an important issue in using grammar formalisms: the compact representation of the parse forest. Why is this an important issue? It is well known that for non-toy grammars and non-toy examples, a sentence can have a staggeringly large number of analyses (for example, using a contextfree grammar (CFG) extracted from the Penn Treebank, a sentence of 25 words may easily have 1,000,000 or more analyses). By way of an example of an ambiguous sentence (though with only t</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael (1997). Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Dikovsky</author>
<author>Larissa Modina</author>
</authors>
<title>Dependencies on the other side of the curtain. Traitement Automatique des Langues,</title>
<date>2000</date>
<volume>41</volume>
<issue>1</issue>
<pages>111</pages>
<contexts>
<context position="10175" citStr="Dikovsky and Modina, 2000" startWordPosition="1625" endWordPosition="1629">ch can be used to develop grammars whose derivations faithfully mirror syntactic dependency. man �� � � a with telescope saw �� � � Pilar saw ���� � � � � Pilar man with a telescope a a Figure 1: Two dependency trees Figure 2: Two dependency trees with lexical categories V V N N saw saw N with a N P Pilar Pilar P N D telesope a a D telesope a D man D man with N tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms. In the third approach, which includes formalizations of dependency structure such as Dependency Tree Grammar of Modina (see (Dikovsky and Modina, 2000) for an overview), Link Grammar (Sleator and Temperley, 1993) or the tree-composition approach of Nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in a derivation, are added to the tree one by one, or in groups. In contrast, we are interested in a stringrewriting system; in such a system, we cannot add dependency relations incrementally: all daughters of a node must be added at once to represent a single rewrite step. In the fourth approach, the dependency grammar is converted into a headed context-free gramma</context>
</contexts>
<marker>Dikovsky, Modina, 2000</marker>
<rawString>Dikovsky, Alexander and Modina, Larissa (2000). Dependencies on the other side of the curtain. Traitement Automatique des Langues, 41(1):79– 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>New Developments in Natural Language Parsing.</booktitle>
<editor>In Bunt, Harry C. and Nijholt, Anton, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="11675" citStr="Eisner (2000)" startWordPosition="1872" endWordPosition="1873">n fact, our parsing algorithm draws on this work.) However, the approach of course requires the introduction of additional nonterminal nodes. Finally, we observe that Recursive Transition Networks (Woods, 1970) can be used to encode a grammar whose derivation trees are dependency trees. However, they are more a general framework for encoding grammars than a specific type of grammar (for example, we can also use them to encode CFGs). In a somewhat related manner, Alshawi et al. (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head. His formalism is very close to the one we present, but it is not a string-rewriting formalism (and not really generative at all). We are looking for a precise formulation of a generative dependency grammar, and the question has remained open whether there is an alternate formalism which allows for an unbounded number of adjuncts, introduces all daughter nodes at once in a string-rewriting step, and avoids the introduction of additional nonterminal nodes. 3 Formalism In this sec</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Eisner, Jason (2000). Bilexical grammars and their cubic-time parsing algorithms. In Bunt, Harry C. and Nijholt, Anton, editors, New Developments in Natural Language Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<pages>8--304</pages>
<contexts>
<context position="4507" citStr="Gaifman (1965)" startWordPosition="700" endWordPosition="701">esired dependency structure. The most salient reason for the lack of such a generative dependency grammar is the absence of non1We would like to thank two anonymous reviewers for useful comments. 2While the notions are related historically and conceptually, we refer to a type of mathematical formalization, not to the school of linguistics known as “Generative Grammar”. terminal symbols in a dependency tree, which prevents us from interpreting it as a derivation structure in a system that distinguishes between terminal and nonterminal symbols. The standard solution to this problem, proposed by Gaifman (1965), is to introduce nonterminal symbols denoting lexical categories, as depicted in figure 2 (called the “labelled phrase-structure trees induced by a dependency tree” by Gaifman (1965)). Clearly, the “pure” dependency tree can be derived in a straightforward manner. The string rewriting system described in (Gaifman, 1965) generates as derivation structures this kind of trees. There is however a deeper problem when considering dependency trees as derivation structures, following from the fact that in a dependency tree, modifiers3 are direct dependents of the head they modify, and (in certain syn</context>
<context position="8718" citStr="Gaifman (1965)" startWordPosition="1383" endWordPosition="1384">ular formalization of phrase structure grammar. Thus, just as it only makes sense to study the formal properties of a particular formalization of phrase structure grammar, the question about the formal properties of dependency grammar in general is not well defined, nor the question of a comparison of a dependency formalism with dependency grammar. There have been (at least) four types of formalizations of dependency grammars in the past.6 None of these approaches, to our knowledge, discuss the notion of shared parse forest. The first approach (for example, (Lombardo and Lesmo, 2000)) follows Gaifman (1965) in proposing traditional string rewriting rules, which however do not allow for an unbounded number of adjuncts. In the second approach, the dependency structure is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Because the rules make reference to other struc5Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which,</context>
<context position="16925" citStr="Gaifman (1965)" startWordPosition="2803" endWordPosition="2804">he head determines the function of its dependents). Underspecification in the left-hand side of a rule only leads to linear compactification of the rule set. We now give a toy linguistic example. We let GLing be (VN, VT, P, 5) as follows: • VN = {V, N, D, A, Adv} • VT = { Pilar, saw, man, a, telescope, with, tall, very } • P consists of the following rules: p1 : V −� N saw N P p2 : N −� (Pilar |D (A) (man |telescope) P ) 8For practical purposes, we can separate the lexicon (which assigns lexical categories to lexemes) from the syntactic rules (which hold for all lexemes of a class), as does Gaifman (1965), resulting in a straightforward notational extension to our formalism. V = N saw N P p1 p2p2p3 = Pilar saw D man with N p6p2 = Pilar saw a man with D telescope = Pilar saw a man with a telescope p6 Figure 3: A sample GDG derivation p3 : P − with N p4 : A − Adv* tall p5 : Adv − very p6 : D − a • S = {V } A derivation is shown in Figure 3; the corresponding derivation tree is shown in the right part of Figure 2. As can be seen, the derivation structure is a dependency tree, except for the use of preterminals, as we desired. The first part of the following theorem follows from the existe</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, Haim (1965). Dependency systems and phrase-structure systems. Information and Control, 8:304–337.</rawString>
</citation>
<citation valid="true">
<title>Two useful measures of word order complexity.</title>
<date>1998</date>
<booktitle>Processing of Dependency Grammars: Proceeding of the Workshop,</booktitle>
<pages>21--28</pages>
<editor>Holan, Tom´aˇs; Kuboˇn, Vladislav; Oliva, Karel; and Pl´atek,</editor>
<publisher>ACL/COLING.</publisher>
<location>Martin</location>
<contexts>
<context position="9050" citStr="(1998)" startWordPosition="1436" endWordPosition="1436">y grammar. There have been (at least) four types of formalizations of dependency grammars in the past.6 None of these approaches, to our knowledge, discuss the notion of shared parse forest. The first approach (for example, (Lombardo and Lesmo, 2000)) follows Gaifman (1965) in proposing traditional string rewriting rules, which however do not allow for an unbounded number of adjuncts. In the second approach, the dependency structure is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Because the rules make reference to other struc5Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees. See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency. man �� �</context>
</contexts>
<marker>1998</marker>
<rawString>Holan, Tom´aˇs; Kuboˇn, Vladislav; Oliva, Karel; and Pl´atek, Martin (1998). Two useful measures of word order complexity. In Kahane, Sylvain and Polgu`ere, Alain, editors, Processing of Dependency Grammars: Proceeding of the Workshop, pages 21–28, Montr´eal, Canada. ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars and lexicalized grammars.</title>
<date>1991</date>
<booktitle>Definability and Recognizability of Sets of Trees.</booktitle>
<editor>In Nivat, Maurice and Podelski, Andreas, editors,</editor>
<publisher>Elsevier.</publisher>
<contexts>
<context position="14194" citStr="Joshi and Schabes, 1991" startWordPosition="2341" endWordPosition="2344">ubset of VN, contains the possible start symbols. We will use the traditional arrow notation (−�) to write rules. For A E VN and u, v E (VN U VT) we say that uAv yields uwv (written uAv ==�- uwv) if A −� R is in P and w is in L(R). The transitive closure of the yield relation (denoted  =� ) is defined in the usual manner. The language generated by a Extended ContextFree Grammar is the set {w E VT |A ===&gt; w, A E 5}. We now define a restricted version of ECFG which we will use for defining dependency grammars. The only new formal requirement is that the rules be lexicalized in the sense of (Joshi and Schabes, 1991). For our formalism, this means that the regular expression in a production is such that each string in its denoted language contains at least one terminal symbol. Linguistically speaking, this means that each rule is associated with exactly 7ECFG has been around informally since the sixties (e.g., the Backus-Naur form); for a slightly different formalization, see (Madsen and Kristensen, 1976), whose definition allows for an infinite rule set. one lexical item (which may be multi-word). We will call this particular type of Extended ContextFree Grammar a lexicalized Extended ContextFree Grammar</context>
</contexts>
<marker>Joshi, Schabes, 1991</marker>
<rawString>Joshi, Aravind K. and Schabes, Yves (1991). Treeadjoining grammars and lexicalized grammars. In Nivat, Maurice and Podelski, Andreas, editors, Definability and Recognizability of Sets of Trees. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity: A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLINGACL’98),</booktitle>
<pages>646--652</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7324" citStr="Kahane et al., 1998" startWordPosition="1153" endWordPosition="1156"> because of the related but different meaning of the term modifier that is well-established in the linguistic literature. 4In fact, much of our formalism is very similar to (Lombardo, 1996), who however does not discuss parsing (only recognition), nor the representation of the parse forest. corresponding finite-state machines which straightforwardly allows for a binary-branching representation of the derivation structure for the purpose of parsing, and thus for a compact (polynomial and not exponential) representation of the parse forest. This formalism is based on previous work presented in (Kahane et al., 1998), which has been substantially reformulated in order to simplify it.5 In particular, we do not address non-projectivity here, but acknowledge that for certain languages it is a crucial issue. We will extend our basic approach in the spirit of (Kahane et al., 1998) in future work. The paper is structured as follows. We start out by surveying previous formalizations of dependency grammar in Section 2. In Section 3, we introduce several formalisms, including Generative Dependency Grammar. We present a parsing algorithm in Section 4, and mention empirical results in Section 5. We then conclude. 2 </context>
<context position="9050" citStr="Kahane et al. (1998)" startWordPosition="1432" endWordPosition="1436">with dependency grammar. There have been (at least) four types of formalizations of dependency grammars in the past.6 None of these approaches, to our knowledge, discuss the notion of shared parse forest. The first approach (for example, (Lombardo and Lesmo, 2000)) follows Gaifman (1965) in proposing traditional string rewriting rules, which however do not allow for an unbounded number of adjuncts. In the second approach, the dependency structure is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Because the rules make reference to other struc5Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees. See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency. man �� �</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Kahane, Sylvain; Nasr, Alexis; and Rambow, Owen (1998). Pseudo-projectivity: A polynomially parsable non-projective dependency grammar. In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLINGACL’98), pages 646–652, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Towards a uniform formal framework for parsing.</title>
<date>1991</date>
<booktitle>Current Issues in Parsing technology, chapter 11,</booktitle>
<pages>153--171</pages>
<editor>In Tomita, M., editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3073" citStr="Lang, 1991" startWordPosition="471" endWordPosition="472">.&apos; We have two options: we can either use a greedy heuristic method for checking which does not examine all possible solutions, which entails we may miss the optimal solution, or we perform our checking operation on a representation which encodes all options in a compact representation. This is possible because the exponential number of possible analyses (exponential in the length of the input sentence) share subanalyses, thus making a polynomial-size representation possible. This representation is called the shared parse forest and it has been extensively studied for CFGs (see, for example, (Lang, 1991)). To our knowledge, there has been no description of the notion of shared parse forest for dependency trees to date. In this paper, we propose a formalization which is very closely based on the shared parse forest for CFG. We achieve this by defining a generative string-rewriting formalism whose derivation trees are dependency trees. The formalism, and the corresponding shared parse forests, are used in a probabilistic chart parser for dependency grammar, which is described in (Nasr and Rambow, 2004b). While there has been much work on formalizing dependency grammar and on parsing algorithms </context>
</contexts>
<marker>Lang, 1991</marker>
<rawString>Lang, Bernard (1991). Towards a uniform formal framework for parsing. In Tomita, M., editor, Current Issues in Parsing technology, chapter 11, pages 153–171. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Lombardo</author>
</authors>
<title>An Earley-style parser for dependency grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96),</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="5604" citStr="Lombardo, 1996" startWordPosition="879" endWordPosition="880">from the fact that in a dependency tree, modifiers3 are direct dependents of the head they modify, and (in certain syntactic contexts) the number of modifiers is unbounded. Thus, if we wish to obtain a tree as shown in Figure 2, we need to have productions whose right-hand side is of unbounded size, which is not possible in a context-free grammar. Indeed, the formalization of dependency grammar proposed by Gaifman (1965) is unsatisfactory in that it does not allow for an unbounded number of modifiers! In this paper, we follow a suggestion made by Abney (1996) and worked out in some detail in (Lombardo, 1996)4 to extend Gaifman’s notation with regular expressions, similar to the approach used in extended context-free grammars. The result is a simple generative formalism which has the property that the derivation structures are dependency trees, except for the introduction of preterminal nodes as shown in Figure 2. We do not mean to imply that our formalism is substantially different from previous formalizations of dependency grammar; the goal of this paper is to present a clean and easy-to-use generative formalism with a straightforward notion of parse forest. In particular, our formalism, Generat</context>
<context position="6893" citStr="Lombardo, 1996" startWordPosition="1091" endWordPosition="1093">n the derivation tree through the use of regular expressions in its rules. The parser uses the 3We use the term modifier in its linguistic sense as a type of syntactic dependency (another type being argument). We use head (or mother) and dependent (or daughter) to refer to nodes in a tree. Sometimes, in the formal and parsing literature, modifier is used to designate any dependent node, but we consider that usage confusing because of the related but different meaning of the term modifier that is well-established in the linguistic literature. 4In fact, much of our formalism is very similar to (Lombardo, 1996), who however does not discuss parsing (only recognition), nor the representation of the parse forest. corresponding finite-state machines which straightforwardly allows for a binary-branching representation of the derivation structure for the purpose of parsing, and thus for a compact (polynomial and not exponential) representation of the parse forest. This formalism is based on previous work presented in (Kahane et al., 1998), which has been substantially reformulated in order to simplify it.5 In particular, we do not address non-projectivity here, but acknowledge that for certain languages </context>
</contexts>
<marker>Lombardo, 1996</marker>
<rawString>Lombardo, Vincenzo (1996). An Earley-style parser for dependency grammars. In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96), Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Lombardo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>A formal theory of dependency syntax with empty units. Traitement automatque des langues,</title>
<date>2000</date>
<pages>41--1</pages>
<contexts>
<context position="8694" citStr="Lombardo and Lesmo, 2000" startWordPosition="1377" endWordPosition="1380">mmar”, not “CFG”, which is a particular formalization of phrase structure grammar. Thus, just as it only makes sense to study the formal properties of a particular formalization of phrase structure grammar, the question about the formal properties of dependency grammar in general is not well defined, nor the question of a comparison of a dependency formalism with dependency grammar. There have been (at least) four types of formalizations of dependency grammars in the past.6 None of these approaches, to our knowledge, discuss the notion of shared parse forest. The first approach (for example, (Lombardo and Lesmo, 2000)) follows Gaifman (1965) in proposing traditional string rewriting rules, which however do not allow for an unbounded number of adjuncts. In the second approach, the dependency structure is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Because the rules make reference to other struc5Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6We leave aside here work on tree rewriting systems such as Tree A</context>
</contexts>
<marker>Lombardo, Lesmo, 2000</marker>
<rawString>Lombardo, Vincenzo and Lesmo, Leonardo (2000). A formal theory of dependency syntax with empty units. Traitement automatque des langues, 41(1):179–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O L Madsen</author>
<author>B B Kristensen</author>
</authors>
<title>LRparsing of extended context-free grammars.</title>
<date>1976</date>
<journal>Acta Informatica,</journal>
<pages>7--61</pages>
<contexts>
<context position="14590" citStr="Madsen and Kristensen, 1976" startWordPosition="2401" endWordPosition="2404">t {w E VT |A ===&gt; w, A E 5}. We now define a restricted version of ECFG which we will use for defining dependency grammars. The only new formal requirement is that the rules be lexicalized in the sense of (Joshi and Schabes, 1991). For our formalism, this means that the regular expression in a production is such that each string in its denoted language contains at least one terminal symbol. Linguistically speaking, this means that each rule is associated with exactly 7ECFG has been around informally since the sixties (e.g., the Backus-Naur form); for a slightly different formalization, see (Madsen and Kristensen, 1976), whose definition allows for an infinite rule set. one lexical item (which may be multi-word). We will call this particular type of Extended ContextFree Grammar a lexicalized Extended ContextFree Grammar or, for obvious reasons, a Generative Dependency Grammar (GDG for short). When we use a GDG for linguistic description, its lefthand side nonterminal will be interpreted as the lexical category of the lexical item and will represent its maximal projection.$ A Generative Dependency Grammar is a lexicalized ECFG. It is sometimes useful to have dependency representations with labeled arcs (typic</context>
</contexts>
<marker>Madsen, Kristensen, 1976</marker>
<rawString>Madsen, O.L. and Kristensen, B.B. (1976). LRparsing of extended context-free grammars. Acta Informatica, 7:61–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>New York.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Mel’ˇcuk, Igor A. (1988). Dependency Syntax: Theory and Practice. State University of New York Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>Un syst`eme de reformulation automatique de phrases fond´e sur la Th´eorie Sens-Texte : application aux langues contrˆol´ees.</title>
<date>1996</date>
<tech>PhD thesis,</tech>
<institution>Universit´e Paris</institution>
<contexts>
<context position="10284" citStr="Nasr (1996)" startWordPosition="1645" endWordPosition="1646"> �� � � Pilar saw ���� � � � � Pilar man with a telescope a a Figure 1: Two dependency trees Figure 2: Two dependency trees with lexical categories V V N N saw saw N with a N P Pilar Pilar P N D telesope a a D telesope a D man D man with N tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms. In the third approach, which includes formalizations of dependency structure such as Dependency Tree Grammar of Modina (see (Dikovsky and Modina, 2000) for an overview), Link Grammar (Sleator and Temperley, 1993) or the tree-composition approach of Nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in a derivation, are added to the tree one by one, or in groups. In contrast, we are interested in a stringrewriting system; in such a system, we cannot add dependency relations incrementally: all daughters of a node must be added at once to represent a single rewrite step. In the fourth approach, the dependency grammar is converted into a headed context-free grammar (Abney, 1996; Holan et al., 1998), also the Basic Dependency Grammar of Beletskij (1967) as cited in (Dikov</context>
</contexts>
<marker>Nasr, 1996</marker>
<rawString>Nasr, Alexis (1996). Un syst`eme de reformulation automatique de phrases fond´e sur la Th´eorie Sens-Texte : application aux langues contrˆol´ees. PhD thesis, Universit´e Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>Factoring sufrace syntactic structures.</title>
<date>2003</date>
<booktitle>In First International Conference on Meaning-Text Theory,</booktitle>
<pages>249--258</pages>
<location>Paris, France.</location>
<marker>Nasr, 2003</marker>
<rawString>Nasr, Alexis (2003). Factoring sufrace syntactic structures. In First International Conference on Meaning-Text Theory, pages 249–258, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>Grammaires de d´ependances g´en´eratives: exp´eriences sur le Corpus Paris 7.</title>
<date>2004</date>
<institution>Universit´e Paris</institution>
<note>Unpublished manuscript,</note>
<contexts>
<context position="22770" citStr="Nasr, 2004" startWordPosition="3939" endWordPosition="3940">ge, each syntagmatic tree is transformed into a dependency tree. We shall not give the details of these processes. The two trees resulting from de-packing of Figure 4 are represented in Figure 6. The different nodes of the syntagmatic tree that will be grouped in a single node of the dependency trees have been circled. 5 Empirical Results While the presentation of empirical results is not the object of this paper, we give an overview of some empirical work using ECFG for natural language processing in this section. For full details, we refer to (Nasr and Rambow, 2004a; Nasr and Rambow, 2004b; Nasr, 2004). The parser presented in Section 4 above has been implemented. We have investigated the use the parser in a two-step probabilistic framework. In a first step, we determine which rules of the ECFG should be used for each word in the input sentence. (Recall that a grammar rule encodes the active and passive valency, as well as how any arguments are realized, for example, fronted or in canonical position.) This step is called supertagging and has been suggested and studied in the context of Tree Adjoining Grammar by Bangalore and Joshi (1999). In a second step, we use a probabilistic ECFG where </context>
</contexts>
<marker>Nasr, 2004</marker>
<rawString>Nasr, Alexis (2004). Grammaires de d´ependances g´en´eratives: exp´eriences sur le Corpus Paris 7. Unpublished manuscript, Universit´e Paris 7.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>(2004a). Dependency parsing based on n-best-path supertagging.</title>
<institution>Universit´e Paris 7 and COlumbia University.</institution>
<note>Unpublished manuscript,</note>
<marker>Nasr, Rambow, </marker>
<rawString>Nasr, Alexis and Rambow, Owen (2004a). Dependency parsing based on n-best-path supertagging. Unpublished manuscript, Universit´e Paris 7 and COlumbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>(2004b). Supertagging and full parsing.</title>
<date></date>
<booktitle>In Proceedings of the Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7),</booktitle>
<location>Vancouver, BC,</location>
<marker>Nasr, Rambow, </marker>
<rawString>Nasr, Alexis and Rambow, Owen (2004b). Supertagging and full parsing. In Proceedings of the Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7), Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Aravind Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrasestructure grammars, with special consideration of word-order phenomena.</title>
<date>1997</date>
<booktitle>Recent Trends in Meaning-Text Theory,</booktitle>
<pages>167--190</pages>
<editor>In Wanner, Leo, editor,</editor>
<location>Amsterdam and Philadelphia.</location>
<contexts>
<context position="9436" citStr="Rambow and Joshi, 1997" startWordPosition="1490" endWordPosition="1493">umber of adjuncts. In the second approach, the dependency structure is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Because the rules make reference to other struc5Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees. See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency. man �� � � a with telescope saw �� � � Pilar saw ���� � � � � Pilar man with a telescope a a Figure 1: Two dependency trees Figure 2: Two dependency trees with lexical categories V V N N saw saw N with a N P Pilar Pilar P N D telesope a a D telesope a D man D man with N tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms. In the third</context>
</contexts>
<marker>Rambow, Joshi, 1997</marker>
<rawString>Rambow, Owen and Joshi, Aravind (1997). A formal look at dependency grammars and phrasestructure grammars, with special consideration of word-order phenomena. In Wanner, Leo, editor, Recent Trends in Meaning-Text Theory, pages 167–190. John Benjamins, Amsterdam and Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-Tree Substitution Grammars.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="9499" citStr="Rambow et al., 2001" startWordPosition="1502" endWordPosition="1505">e is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988). Because the rules make reference to other struc5Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees. See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency. man �� � � a with telescope saw �� � � Pilar saw ���� � � � � Pilar man with a telescope a a Figure 1: Two dependency trees Figure 2: Two dependency trees with lexical categories V V N N saw saw N with a N P Pilar Pilar P N D telesope a a D telesope a D man D man with N tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms. In the third approach, which includes formalizations of dependency structur</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 2001</marker>
<rawString>Rambow, Owen; Vijay-Shanker, K.; and Weir, David (2001). D-Tree Substitution Grammars. Computational Linguistics, 27(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sgall</author>
<author>E Hajiˇcov´a</author>
<author>J Panevov´a</author>
</authors>
<title>The meaning of the sentence and its semantic and pragmatic aspects.</title>
<date>1986</date>
<location>Reidel, Dordrecht.</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Sgall, P.; Hajiˇcov´a, E.; and Panevov´a, J. (1986). The meaning of the sentence and its semantic and pragmatic aspects. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing english with a link grammar,.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third International Workshop on Parsing Technologies IIWPT’93).</booktitle>
<contexts>
<context position="10236" citStr="Sleator and Temperley, 1993" startWordPosition="1635" endWordPosition="1638">ully mirror syntactic dependency. man �� � � a with telescope saw �� � � Pilar saw ���� � � � � Pilar man with a telescope a a Figure 1: Two dependency trees Figure 2: Two dependency trees with lexical categories V V N N saw saw N with a N P Pilar Pilar P N D telesope a a D telesope a D man D man with N tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms. In the third approach, which includes formalizations of dependency structure such as Dependency Tree Grammar of Modina (see (Dikovsky and Modina, 2000) for an overview), Link Grammar (Sleator and Temperley, 1993) or the tree-composition approach of Nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in a derivation, are added to the tree one by one, or in groups. In contrast, we are interested in a stringrewriting system; in such a system, we cannot add dependency relations incrementally: all daughters of a node must be added at once to represent a single rewrite step. In the fourth approach, the dependency grammar is converted into a headed context-free grammar (Abney, 1996; Holan et al., 1998), also the Basic Dependenc</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Sleator, Daniel and Temperley, Davy (1993). Parsing english with a link grammar,. In Proceedings of the Third International Workshop on Parsing Technologies IIWPT’93).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Toussenel</author>
</authors>
<title>Why supertagging is hard.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7),</booktitle>
<location>Vancouver, BC,</location>
<contexts>
<context position="25905" citStr="Toussenel, 2004" startWordPosition="4458" endWordPosition="4459">curacies of about 98% (i.e., in about 2% of cases a word is assigned a wrong governor). Automatic supertagging (using standard n-gram tagging methods) for a grammar our size has an accuracy of about 80%. This is also approximately the dependency accuracy obtained when parsing the output of a supertagger. We conclude from this performance that if we can increase the performance of the supertagger, we can also directly increase the performance of the parser. Current work includes examining which grammatical distinctions the grammar should make in order to optimize both supertagging and parsing (Toussenel, 2004). 6 Conclusion We have presented a generative string-rewriting system, Extended Context-Free Grammar, whose derivation trees are dependency trees with unbounded branching factor. We have shown how we can reuse the representation of shared parse forests well-known from CFGs for Extended Context-Free Grammar. The question arises whether we can represent the shared parse forest in a manner more directly in the spirit of dependency. This question was investigated by (Nasr, 2003). He shows that the factoring realized in the shared forest presented here and which is the key to the polynomial represe</context>
</contexts>
<marker>Toussenel, 2004</marker>
<rawString>Toussenel, Franc¸ois (2004). Why supertagging is hard. In Proceedings of the Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7), Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<volume>3</volume>
<issue>10</issue>
<contexts>
<context position="11272" citStr="Woods, 1970" startWordPosition="1803" endWordPosition="1804"> single rewrite step. In the fourth approach, the dependency grammar is converted into a headed context-free grammar (Abney, 1996; Holan et al., 1998), also the Basic Dependency Grammar of Beletskij (1967) as cited in (Dikovsky and Modina, 2000). This approach allows for the recovery of the dependency structure both from the derivation tree and from a parse forest represented in polynomial space. (In fact, our parsing algorithm draws on this work.) However, the approach of course requires the introduction of additional nonterminal nodes. Finally, we observe that Recursive Transition Networks (Woods, 1970) can be used to encode a grammar whose derivation trees are dependency trees. However, they are more a general framework for encoding grammars than a specific type of grammar (for example, we can also use them to encode CFGs). In a somewhat related manner, Alshawi et al. (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head. His formalism is very close to the one we present, but it is not a string-rewri</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>Woods, William A. (1970). Transition network grammars for natural language analysis. Commun. ACM, 3(10):591–606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
<author>Aravind Joshi</author>
</authors>
<title>A uniform method of grammar extraction and its applications.</title>
<date>2000</date>
<booktitle>In Proc. of the EMNLP</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="23979" citStr="Xia et al., 2000" startWordPosition="4143" endWordPosition="4146"> where the probabilities are non-lexical and are based entirely on the grammar rules. We extract the most probable derivation from the compact parse forest using dynamic programming in the usual manner. This non-lexical probability model is used because the supertagging step already takes the words in the sentence into account. The probabilities can be encoded directly as weights on the transitions in the rule-FSMs used by the parser. The ECFG grammar we use has been automatically extracted from the Penn Treebank (PTB). In fact, we first extract a Tree Insertion Grammar following the work of (Xia et al., 2000; Chen, 2001; Chiang, 2000), and then directly convert the trees of the obtained TAG into automata for the parser. It is clear that one could also derive an explicit ECFG in the same manner. The extracted grammar has about 4.800 rules. The probabilities are estimated from the corpus during extraction. Note that there are many different ways of extracting an ECFG from the PTB, corresponding to different theories of syntactic dependency. We have chosen to directly model predicate-argument relations rather than more surface-oriented syntactic relations such as agreement, so that all function word</context>
</contexts>
<marker>Xia, Palmer, Joshi, 2000</marker>
<rawString>Xia, Fei; Palmer, Martha; and Joshi, Aravind (2000). A uniform method of grammar extraction and its applications. In Proc. of the EMNLP 2000, Hong Kong.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>