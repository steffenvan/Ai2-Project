<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.9909075">
Ontology driven content extraction using interlingual annotation of
texts in the OMNIA project
</title>
<author confidence="0.922526">
Achille Falaise, David Rouquet, Didier Schwab, Herv´e Blanchon, Christian Boitet
</author>
<affiliation confidence="0.938429">
LIG-GETALP, University of Grenoble
</affiliation>
<email confidence="0.997363">
{Firstname}.{Lastname}@imag.fr
</email>
<sectionHeader confidence="0.994749" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868846153846">
OMNIA is an on-going project that aims
to retrieve images accompanied with
multilingual texts. In this paper, we pro-
pose a generic method (language and do-
main independent) to extract conceptual
information from such texts and sponta-
neous user requests. First, texts are la-
belled with interlingual annotation, then
a generic extractor taking a domain on-
tology as a parameter extract relevant
conceptual information. Implementation
is also presented with a first experiment
and preliminary results.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999617">
The OMNIA project (Luca Marchesotti et al.,
2010) aims to retrieve images that are described
with multilingual free companion texts (cap-
tions, comments, etc.) in large Web datasets.
Images are first classified with formal descrip-
tors in a lightweight ontology using automatic
textual and visual analysis. Then, users may ex-
press spontaneous queries in their mother tongue
to retrieve images. In order to build both formal
descriptors and queries for the ontology, a con-
tent extraction in multilingual texts is required.
Multilingual content extraction does not im-
ply translation. It has been shown in (Daoud,
2006) that annotating words or chunks with in-
terlingual lexemes is a valid approach to initiate
a content extraction. We thus skip syntactical
analysis, an expensive and low quality process,
and get language-independent data early in our
flow, allowing further treatments to be language-
independent. We use the lightweight ontology
for image classifications as the formal knowl-
edge representation tha determines relevant in-
formation to extract. This ontology is considered
as a domain parameter for the content extractor.
We are testing this method on a database pro-
vided for the image retrieval challenge CLEF09
by the Belgium press agency Belga. The
database contains 500K images with free com-
panion texts of about 50 words (about 25M
words in total). The texts in the database are in
English only, and we ”simulate” multilinguism
with partially post-edited machine translation.
The rest of the paper is organized as fol-
low. We first depict our general architecture de-
ployed for CLIA and then detail the various pro-
cesses involved : interlingual annotation, con-
ceptual vector based disambiguation and ontol-
ogy driven content extraction. We conclude
with the first results of experimentations on the
CLEF09 data.
</bodyText>
<sectionHeader confidence="0.977814" genericHeader="method">
2 General architecture
</sectionHeader>
<subsectionHeader confidence="0.733344">
2.1 General process
</subsectionHeader>
<bodyText confidence="0.985234571428571">
In our scenario, there are two types of tex-
tual data to deal with : companion texts in the
database (captions), but also user requests. The
two are processed in a very similar way.
The general architecture is depicted in figure
1. The main components, that will be described
in detail, may be summarized as follows:
</bodyText>
<listItem confidence="0.9813084">
• Texts (both companions and requests) are
first lemmatised with a language-dependent
piece of software. Ambiguities are pre-
served in a Q-graph structure presented in
section 3.1.2.
</listItem>
<page confidence="0.981909">
52
</page>
<note confidence="0.795831">
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 52–60,
Beijing, August 2010
</note>
<figureCaption confidence="0.999559">
Figure 1: General architecture of CLIA in the OMNIA project
</figureCaption>
<figure confidence="0.997442266666667">
Lemmatisation
NL-UW
dictionnary
Interlingual
annotation
Q-Graph
UW-Concept Ontology
Map
Disamb
Concept
extraction
Concepts
Companion
texts
NL Requests
</figure>
<listItem confidence="0.991891307692308">
• Then, the lemmatised texts are annotated
with interlingual (ideally unambiguous)
lexemes, namely Universal Words (UW)
presented in section 3.1.1. This adds a lot
of ambiguities to the structure, as an ac-
tual lemma may refer to several semanti-
cally different lexemes.
• The possible meanings for lemmas are then
weighted in the Q-graph through a disam-
biguation process.
• Finally, relevant conceptual information is
extracted using an alignment between a do-
main ontology and the interlingual lexemes.
</listItem>
<bodyText confidence="0.9997786">
The conceptual information in the output may
adopt different shapes, such as a weighted con-
ceptual vector, statements in the A-Box of the
ontology or annotations in the original text, etc.
In the case of OMNIA, conceptual informa-
tion extracted from companion texts is stored
in a database, while conceptual information ex-
tracted from users requests are transformed into
formal requests for the database (such as SQL,
SPARQL, etc.).
</bodyText>
<subsectionHeader confidence="0.99697">
2.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999959875">
The general process is implemented following a
Service Oriented Architecture (SOA). Each part
of the process corresponds to a service.
This allowed us to reuse part of existing re-
sources developed on heterogeneous platforms
using web interfaces (in the best case REST in-
terfaces (Fielding, 2000), but frequently only
HTML form-based interfaces). A service su-
pervisor has been built to deal with such an
heterogeneity and address normalization issues
(e.g. line-breaks, encoding, identification, cook-
ies, page forwarding, etc.).
This architecture is able to process multiple
tasks concurrently, allowing to deal with users
requests in real time while processing compan-
ion texts in the background.
</bodyText>
<sectionHeader confidence="0.997342" genericHeader="method">
3 Interlingual annotation
</sectionHeader>
<bodyText confidence="0.999841230769231">
We present in this section the preliminary treat-
ments of multilingual texts (image companion
texts or user requests) that are required for
our content extraction process (Rouquet and
Nguyen, 2009a).
In order to allow a content extraction in multi-
lingual texts, we propose to represent texts with
the internal formalism of the Q-Systems and
to annotate chunks with UNL interlingual lex-
emes (UW) . Roughly, we are making an inter-
lingual lemmatisation, containing more informa-
tion than simple tagging, that is not currently
proposed by any lemmatisation software.
</bodyText>
<subsectionHeader confidence="0.977438">
3.1 Resources and data structures
3.1.1 The Universal Network Language
</subsectionHeader>
<bodyText confidence="0.996302142857143">
UNL (Boitet et al., 2009; Uchida Hiroshi et
al., 2009) is a pivot language that represents the
meaning of a sentence with a semantic abstract
structure (an hyper-graph) of an equivalent En-
glish sentence.
The vocabulary of UNL consists in a set of
Universal Words (UW). An UW consists of:
</bodyText>
<listItem confidence="0.947765625">
1. a headword, if possible derived from En-
glish, that can be a word, initials, an expres-
sion or even an entire sentence. It is a label
for the concepts it represents in its original
language;
2. a list of restrictions that aims to precisely
specify the concept the UW refers to. Re-
strictions are semantic relations with other
</listItem>
<page confidence="0.998065">
53
</page>
<bodyText confidence="0.996904">
UW. The most used is the “icl” relation that
points to a more general UW.
</bodyText>
<listItem confidence="0.878471875">
Examples :
• book(icl&gt;do, agt&gt;human, obj&gt;thing)
and book(icl&gt;thing).
Here, the sense of the headword is focused
by the attributes.
• ikebana(icl&gt;flower arrangement).
Here, the headword comes from Japanese.
• go down.
</listItem>
<bodyText confidence="0.99925605">
Here, the headword does not need any re-
finement.
Ideally, an UW refers unambiguously to a con-
cept, shared among several languages. However,
UW are designed to represent acceptions in a
language ; we therefore find distinct UW that
refer to the same concept as for “affection” and
“disease”.
We are mainly using the 207k UW built by the
U++ Consortium (Jesus Carde˜nosa et al., 2009)
from the synsets of the Princeton WordNet, that
are linked to natural languages via bilingual dic-
tionaries. The storage of these dictionaries can
be supported by a suitable platform like PIVAX
(Nguyen et al., 2007) or a dedicated database.
The gain of a pivot language is illustrated in fig-
ure 2. If we want to add a new language in the
multilingual system, we just need to create the
links with the pivot but not with all the other lan-
guages.
</bodyText>
<subsectionHeader confidence="0.67213">
3.1.2 The Q-Systems
</subsectionHeader>
<bodyText confidence="0.999934384615385">
We can think of inserting the UW annotations
with tags (e.g. XML) directly along the source
text as in table 1. However, this naive approach is
not adequate to represent the segmentation am-
biguities that can occur in the text interpretation
(in the example of table 1, we list the different
possible meanings for “in”, but cannot represent
“waiting”, “room” and “waiting room” as three
possible lexical units).
In order to allow the representation of segmen-
tation and other ambiguities, that can occur in
a text interpretation, we propose to use the Q-
Systems. They represent texts in an adequate
</bodyText>
<figure confidence="0.934944">
Chinese
volume
</figure>
<figureCaption confidence="0.998432">
Figure 2: Multilingual architecture with a pivot
</figureCaption>
<bodyText confidence="0.985397555555556">
in a waiting room
&lt;tag uw=’in(icl-sup-how),
in(icl-sup-adj),
in(icl-sup-linear unit,
equ-sup-inch)’&gt;in&lt;/tag&gt;
&lt;tag uw=’unk’&gt;a&lt;/tag&gt; &lt;tag
uw=’waiting room(icl-sup-room,
equ-sup-lounge)’&gt;waiting
room&lt;/tag&gt;
</bodyText>
<tableCaption confidence="0.989827">
Table 1: Naive annotation of a text fragment
</tableCaption>
<bodyText confidence="0.996073642857143">
graph structure decorated with bracketed expres-
sions (trees) and, moreover, allow processing on
this structure via graph rewriting rules (a set of
such rewriting rules is a so called Q-System).
An example of the Q-System formalism is
given in figure 3 of section 3.2.3. It presents
successively : the textual input representing a Q-
graph, a rewriting rule and a graphical view of
the Q-graph obtained after the application of the
rule (and others).
The Q-Systems were proposed by Alain
Colmeraurer at Montreal University (Colmer-
auer, 1970). For our goal, they have three main
advantages :
</bodyText>
<listItem confidence="0.9956">
• they provide the formalized internal struc-
ture for linguistic portability that we men-
tioned in the introduction (Hajlaoui and
Boitet, 2007) ;
• they unify text processing with powerful
graph rewriting systems ;
</listItem>
<figure confidence="0.998761833333333">
English
volume
French
volume
Interlingual
UW volume
</figure>
<page confidence="0.963011">
54
</page>
<listItem confidence="0.654518">
• they allow the creation or the edition of
a process by non-programmers (e.g. lin-
guists) using SLLP (Specialized Language
for Linguistic Programming).
</listItem>
<bodyText confidence="0.99927675">
We are actually using a reimplementation of
the Q-Systems made in 2007 by Hong-Thai
Nguyen during his PhD in the LIG-GETALP
team (Nguyen, 2009).
</bodyText>
<subsectionHeader confidence="0.998917">
3.2 Framework of the annotation process
3.2.1 Overview
</subsectionHeader>
<bodyText confidence="0.9997595">
The annotation process is composed by the
following steps :
</bodyText>
<listItem confidence="0.999058375">
1. splitting the text in fragments if too long ;
2. lemmatisation with a specialized software;
3. transcription to the Q-Systems format;
4. creation of local bilingual dictionaries
(source language - UW) for each fragment
with PIVAX ;
5. execution of those dictionaries on the frag-
ments ;
</listItem>
<subsectionHeader confidence="0.879477">
3.2.2 Lemmatisation
</subsectionHeader>
<bodyText confidence="0.9999066">
As we want to use dictionaries where entries
are lemmas, the first step is to lemmatise the in-
put text (i.e. to annotate occurrences with possi-
ble lemmas). This step is very important because
it although gives the possible segmentations of
the text in lexical units. It brings two kinds of
ambiguities into play : on one hand, an occur-
rence can be interpreted as different lemmas, on
the other, there can be several possible segmen-
tations (eventually overlapping) to determine the
lexical units.
For content extraction or information retrieval
purpose, it is better to preserve an ambiguity than
to badly resolve it. Therefore we expect from a
lemmatiser to keep all ambiguities and to repre-
sent them in a confusion network (a simple tag-
ger is not suitable). Several lemmatiser can be
used to cover different languages. For each of
them, we propose to use a dedicated ANTLR
grammar (Terence Parr et al., 2009) in order to
soundly transform the output in a Q-graph.
To process the Belga corpus, we developed a
lemmatiser that produce natively Q-graphs. It
is based on the morphologic dictionary DELA1
available under LGPL licence.
</bodyText>
<subsectionHeader confidence="0.64376">
3.2.3 Local dictionaries as Q-Systems
</subsectionHeader>
<bodyText confidence="0.9995995">
Having the input text annotated with lemmas,
with the Q-System formalism, we want to use the
graph rewriting possibilities to annotate it with
UW. To do so, we use PIVAX export features to
produce rules that rewrite a lemma in an UW (see
figure 3). Each rule correspond to an entry in
the bilingual dictionary. To obtain a tractable Q-
Systems (sets of rules), we built local dictionar-
ies that contain the entries for fragments of the
text (about 250 words in the first experiment).
</bodyText>
<figureCaption confidence="0.999149">
Figure 3: Creation and execution of a Q-System
</figureCaption>
<bodyText confidence="0.999898666666667">
Considering the significant quantity of ambi-
guities generated by this approach (up to a dozen
UW for a single word), we need to include a
disambiguation process. This process, based on
conceptual vectors, is presented in the next sec-
tion.
</bodyText>
<sectionHeader confidence="0.8158475" genericHeader="method">
4 Conceptual vector based
disambiguation
</sectionHeader>
<bodyText confidence="0.9999052">
Vectors have been used in NLP for over 40 years.
For information retrieval, the standard vector
model (SVM) was invented by Salton (Salton,
1991) during the late 60’s, while for meaning
representation, latent semantic analysis (LSA)
</bodyText>
<footnote confidence="0.979663">
1http://infolingu.univ-mlv.fr/DonneesLinguistiques/
Dictionnaires/telechargement.html
</footnote>
<page confidence="0.998079">
55
</page>
<bodyText confidence="0.999960659090909">
was developed during the late 80’s (Deerwester
et al., 1990). These approaches are inspired
by distributional semantics (Harris et al., 1989)
which hypothesises that a word meaning can be
defined by its co-text. For example, the mean-
ing of ,milk- could be described by {,cow-, ,cat-,
,white-, ,cheese-, ,mammal-, ... }. Hence, distribu-
tional vector elements correspond directly (for
SVM) or indirectly (for LSA) to lexical items
from utterances.
The conceptual vector model is different as it
is inspired by componential linguistics (Hjelm-
lev, 1968) which holds that the meaning of words
can be described with semantic components.
These can be considered as atoms of meaning
(known as primitives (Wierzbicka, 1996)), or
also only as constituents of the meaning (known
as semes, features (Greimas, 1984), concepts,
ideas). For example, the meaning of ,milk-
could be described by {LIQUID, DAIRYPRODUCT, WHITE,
FOOD, ... }. Conceptual vectors model a formal-
ism for the projection of this notion in a vectorial
space. Hence, conceptual vector elements corre-
spond to concepts indirectly, as we will see later.
For textual purposes2, conceptual vectors can
be associated to all levels of a text (word, phrase,
sentence, paragraph, whole texts, etc.). As they
represent ideas, they correspond to the notion of
semantic field3 at the lexical level, and to the
overall thematic aspects at the level of the entire
text.
Conceptual vectors can also be applied to
lexical meanings. They have been studied in
word sense disambiguation (WSD) using iso-
topic properties in a text, i.e. redundancy of ideas
(Greimas, 1984). The basic idea is to maximise
the overlap of shared ideas between senses of
lexical items. This can be done by computing the
angular distance between two conceptual vectors
(Schwab and Lafourcade, 2007).
In our case, conceptual vectors are used for
automatic disambiguation of texts. Using this
method, we calculate confidence score for each
UW hypothesis appearing in the Q-Graph.
</bodyText>
<footnote confidence="0.88881975">
2Conceptual vectors can be associated with any content,
not only text: images, videos, multimedia, Web pages, etc.
3The semantic field is the set of ideas conveyed by a
term.
</footnote>
<subsubsectionHeader confidence="0.366203">
5 Ontology driven content extraction
</subsubsectionHeader>
<bodyText confidence="0.999844">
The content extraction has to be leaded by a
“knowledge base” containing the informations
we want to retrieve.
</bodyText>
<subsectionHeader confidence="0.972523">
5.1 Previous works in content extraction
</subsectionHeader>
<bodyText confidence="0.99931585">
This approach has its roots in machine trans-
lation projects such as C-Star II (1993-1999)
(Blanchon and Boitet, 2000) and Nespole!
(2000-2002) (Metze et al., 2002), for on the fly
translation of oral speech acts in the domain of
tourism. In these projects, semantic transfer was
achieved through an IF (Inter-exchange Format),
that is a semantic pivot dedicated to the domain.
This IF allows to store information extracted
from texts but is although used to lead the con-
tent extraction process by giving a formal repre-
sentation of the relevant informations to extract,
according to the domain.
The Nespole! IF consists of 123 concepts
from the tourism domain, associated with sev-
eral arguments and associable with speech acts
markers. The extraction process is based on pat-
terns. As an example, the statement ”I wish a
single room from September 10th to 15th” may
be represented as follows:
</bodyText>
<equation confidence="0.961589">
{ c:give-information+disposition+room
( disposition=(desire, who=i),
room-spec=
( identifiability=no,single_room ),
time=
( start-time=(md=10),
end-time(md=15, month=9)
)
)
}
</equation>
<subsectionHeader confidence="0.98709">
5.2 Ontologies as parameter for the domain
</subsectionHeader>
<bodyText confidence="0.99818225">
In the project OMNIA, the knowledge base has
the form of a lightweight ontology for image
classification 4. This ontology contains 732 con-
cepts in the following domains : animals, pol-
itics, religion, army, sports, monuments, trans-
ports, games, entertainment, emotions, etc. To
us, using an ontology has the following advan-
tages :
</bodyText>
<listItem confidence="0.964073">
• Ontologies give an axiomatic description
of a domain, based on formal logics (usu-
</listItem>
<footnote confidence="0.98883">
4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl
</footnote>
<page confidence="0.998503">
56
</page>
<bodyText confidence="0.9928245">
ally description logics (Baader et al., 2003))
with an explicit semantic. Thus, the knowl-
edge stored in them can be used soundly by
software agents;
</bodyText>
<listItem confidence="0.8598211">
• Ontological structures are close to the or-
ganisation of ideas as semantic networks in
human mind (Aitchenson, 2003) and are la-
beled with strings derived from natural lan-
guages. Thus humans can use them (brows-
ing or contributing) in a pretty natural way;
• Finally, with the advent of the Semantic
Web and normative initiatives such as the
W3C5, ontologies come with a lot of shared
tools for editing, querying, merging, etc.
</listItem>
<bodyText confidence="0.998736925925926">
As the content extractor might only process
UW annotations, it is necessary that the knowl-
edge base is whether expressed using UW or
linked to UW. The ontology is here considered
as a domain parameter of content extraction
and can be changed to improve preformances
on specific data collections. Therefore, given
any OWL ontology6, we must be able to link it
with a volume of UW considering the following
constraints :
Creating manually such correspondences
is costly due to the size of resources so an
automatic process is requiered.
Ontologies and lexicons evolve over the time
so an alignment must be adaptable to incremen-
tal evolutions of resources.
The correspondences must be easily manip-
ulated by users so they can manually improve
the quality of automatically created alignments
with post-edition.
Constructing and maintaining an alignment
between an ontology and an UW lexicon is a
challenging task (Rouquet and Nguyen, 2009b).
Basically, any lexical resource can be repre-
sented in an ontology language as a graph. We
propose to use an OWL version of the UW vol-
ume available on Kaiko website 7. It allows us
</bodyText>
<footnote confidence="0.999479333333333">
5http://www.w3.org/
6http://www.w3.org/2004/OWL/
7http://kaiko.getalp.org
</footnote>
<bodyText confidence="0.998851666666667">
to benefit of classical ontology matching tech-
niques and tools (Euzenat and Shvaiko, 2007)
to represent, compute and manipulate the align-
ment. We implemented two string based match-
ing techniques on top of the alignment API (Eu-
zenat, 2004). Specific disambiguation methods
are in development to improve the alignment
precision. Some of them are based on conceptual
vectors presented in section 4, others will adapt
structural ontology matching techniques. This
approach to match an ontology with a lexical re-
source is detailled in (Rouquet et al., 2010).
</bodyText>
<subsectionHeader confidence="0.995135">
5.3 The generic extractor
</subsectionHeader>
<bodyText confidence="0.999905571428571">
In the case of the OMNIA project, the system
output format is constraint by the goal of an in-
tegration with visual analysis results, in a larger
multimodal system. The visual analysis systems
are also based on concept extraction, but does not
need an ontology to organise concepts. There-
fore, our results has to remain autonaumous,
which means without references to the ontology
used to extract concepts. So, we use a simple
concept vector as output, with intensity weights;
practically, a simple data-value pairs sequence
formatted in XML.
Concept extraction is achieved through a 3
steps process, has shown in figure 4.
</bodyText>
<listItem confidence="0.991084764705882">
1. Concept matching: each UW in the Q-
Graph, that matches a concept according to
the UW-concept map, is labelled with this
concept.
2. Confidence calculation: each concept la-
bel is given a confidence score, in accor-
dance with the score of the UW carrying the
concept, obtained after disambiguation, and
pondered according to the number of UWs
in the Q-Graph. It is planed to take into ac-
count a few linguistics hints here, such as
negations, and intensity adverbs.
3. Score propagation: because we need au-
tonomous results, we have to perform all
ontology-based calculation before releasing
them. The confidence scores are propagated
in the ontology concept hierarchy: for each
</listItem>
<page confidence="0.997596">
57
</page>
<bodyText confidence="0.999537714285714">
labelled concept, its score is added to the
super-concept, and so on.
The ontology and the derivated UW-concept
map are considered as parameters for the treat-
ments, and may be replaced in accordance with
the domain, and the relevance of the concepts
and their hierarchy, according to the task.
</bodyText>
<figureCaption confidence="0.994723">
Figure 4: Detail of concept extraction.
</figureCaption>
<sectionHeader confidence="0.995755" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9861325">
For a first experiment, we used a small dataset,
containing:
</bodyText>
<listItem confidence="0.9995635">
• a sub-corpus of 1046 English companion
texts from CLEF09 corpus (press pictures
and captions of about 50 words),
• a 159 concepts ontology, designed for pic-
ture and emotions depiction,
• a UW-concept map comprising 3099 UW.
</listItem>
<bodyText confidence="0.9998974">
It appeared that, with this parameters, con-
cepts where extracted for only 25% of the texts.
This preliminary result stressed the importance
of recall for such short texts. However, there
were many ways to improve recall in the system:
</bodyText>
<listItem confidence="0.873132">
• improve the ontology, in order to better
cover the press domain;
• significantly increase the quantity of UW
linked to concepts (only 3099 obtained for
this experiment), by considering synonyms
during the linking process;
• using UW restrictions during concept
matching for UW that are not directly
linked to a concept, as these restrictions are
a rich source of refined semantic informa-
tion.
</listItem>
<bodyText confidence="0.999960444444444">
A second experiment with an improved on-
tology, including 732 concepts, and the use of
UW restrictions, showed very promising results.
Concepts were retrieved from 77% of texts. The
remaining texts were very short (less than 10
words, sometime just date or name).
For example, we extracted the following con-
cepts from the picture and companion text repro-
duced in figure 5.
</bodyText>
<figureCaption confidence="0.8415955">
AWA05 - 20020924 - BAGHDAD,
IRAQ : Iraqi women sit under a
portrait of Iraqi President Saddam
Hussein in a waiting room in
Baghdad&apos;s al-Mansur hospital 24
September 2002. Saddam Hussein
is doggedly pursuing the
development of weapons of mass
destruction and will do his best to
hide them from UN inspectors, the
British government claimed in a 55-
page dossier made public just
hours before a special House of
Commons debate on Iraq. Iraqi
Culture Minister Hamad Yussef
Hammadi called the British
allegations &amp;quot;baseless.&amp;quot; EPA
PHOTO AFPI AWAD AWAD
Figure 5: Picture document and companion text
example.
</figureCaption>
<table confidence="0.981975083333333">
CONCEPT WEIGHT
BUILDING 0.098
HOSPITAL 0.005
HOUSE 0.043
MINISTER 0.016
OTHER BUILDING 0.005
PEOPLE 0.142
PERSON 0.038
POLITICS 0.032
PRESIDENT 0.016
RESIDENTIAL BUILDING 0.043
WOMAN 0.005
</table>
<bodyText confidence="0.893197111111111">
As this results were more consistent, we could
have a preliminary survey about precision, on a
30 texts sample. While disambiguation imple-
mentation is still at an early stage, weights were
not yet taken into account. A concept match can
be considered correct following two criterons :
1. Visual relevance considers a concept as
correct if carried by an element of the pic-
ture; for instance, the match of concept
</bodyText>
<figure confidence="0.998901384615385">
Confidence
calculation
Score
propagation
Concepts
Ontology
cp
Content
extraction
matching
Q-Graph
UW-Concept
Map
</figure>
<page confidence="0.996541">
58
</page>
<bodyText confidence="0.996671272727273">
”SPORT” is regarded as correct for a pic-
ture containing a minister of sports, even if
not actually performing any sport.
2. Textual relevance considers a concept as
correct if carried by a word of the text,
as parts of texts may involve concepts that
are not actually present in the picture, such
as contextual information, previous events,
etc.
124 concepts were found in 23 texts (7 texts had
no concept match):
</bodyText>
<listItem confidence="0.8952092">
1. 99 concepts were correct according to the
visual relevance,
2. 110 were correct according to the textual
relevance,
3. 14 were totally incorrect.
</listItem>
<bodyText confidence="0.999966">
We thus have an overall precision score of 0.798
according to the visual relevance and 0.895 ac-
cording to the textual relevance. Most of the er-
rors where caused by ambiguity problems, and
may be addressed with disambiguation process
that are not fully implemented yet.
</bodyText>
<sectionHeader confidence="0.964147" genericHeader="conclusions">
7 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.9109199375">
We exposed a generic system designed to extract
content (in the form of concepts) from multi-
lingual texts. Our content extraction process is
generic regarding to two aspects :
• it is language independent, as it process an
interlingual representation of the texts
• the content to be extracted can be specified
using a domain ontology as a parameter
This is an ongoing work, and disambiguation
through conceptual vectors is expected to im-
prove accuracy, giving significant weights to the
hypothetical meanings of words.
In the long run, we will focus on integration
with visual content extractors, speed optimiza-
tion to achieve a real-time demonstrator and de-
tailled evaluation of the method.
</bodyText>
<sectionHeader confidence="0.980121" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998906354166667">
Aitchenson, J. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell Publish-
ers.
Baader, De Franz, Diego Calvanese, Deborah
McGuinness, Peter Patel-Schneider, and Daniele
Nardi. 2003. The Description Logic Handbook.
Cambridge University Press.
Blanchon, H. and C. Boitet. 2000. Speech translation
for french within the C-STAR II consortium and
future perspectives. In Proc. ICSLP 2000, pages
412–417, Beijing, China.
Boitet, Christian, Igor Boguslavskij, and Jesus
Carde˜nosa. 2009. An evaluation of UNL usabil-
ity for high quality multilingualization and projec-
tions for a future UNL++ language. In Computa-
tional Linguistics and Intelligent Text Processing,
pages 361–373.
Colmerauer, A. 1970. Les syst`emes-q ou un for-
malisme pour analyser et synth´etiser des phrases
sur ordinateur. d´epartement d’informatique de
l’Universit´e de Montr´eal, publication interne, 43,
September.
Daoud, Daoud. 2006. Il faut et on peut constru-
ire des syst`emes de commerce ´electronique a` inter-
face en langue naturelle restreints (et multilingues)
en utilisant des m´ethodes orient´ees vers les sous-
langages et le contenu. Ph.D. thesis, UJF, Septem-
ber.
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A.
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society of In-
formation Science, 41(6).
Euzenat, J´erˆome and Pavel Shvaiko. 2007. Ontology
matching. Springer, Heidelberg (DE).
Euzenat, J´erˆome. 2004. An API for ontology align-
ment. In Proceedings of the 3rd International
Semantic Web Conference, pages 698–7112, Hi-
roshima, Japan.
Fielding, Roy T. 2000. Architectural styles and the
design of network-based software architectures.
Ph.D. thesis, University of California.
Greimas, Algirdas Julien. 1984. Structural Seman-
tics: An Attempt at a Method. University of Ne-
braska Press.
Hajlaoui, Najeh and Christian Boitet. 2007. Portage
linguistique d’applications de gestion de contenu.
In TOTh07, Annecy.
</reference>
<page confidence="0.982825">
59
</page>
<reference confidence="0.999666296875">
Harris, Zellig S., Michael Gottfried, Thomas Ryck-
man, Paul Mattick Jr., Anne Daladier, T.N. Har-
ris, and S. Harris. 1989. The form of Information
in Science, Analysis of Immunology Sublanguage,
volume 104 of Boston Studies in the Philosophy of
Science. Kluwer Academic Publisher, Dordrecht.
Hjelmlev, Louis. 1968. Prol´egol`eme a` une th´eorie
du langage. ´editions de minuit.
Schwab, Didier and Mathieu Lafourcade. 2007. Lex-
ical functions for ants based semantic analysis. In
ICAI’07- The 2007 International Conference on
Artificial Intelligence, Las Vegas, Nevada, USA,
juin.
Terence Parr et al. 2009. ANTLR parser
generator (accessed on september 2009).
http://www.antlr.org/, September.
Jesus Carde˜nosa et al. 2009. The U++ con-
sortium (accessed on september 2009).
http://www.unl.fi.upm.es/consorcio/index.php,
September.
Luca Marchesotti et al. 2010. The Omnia project
(accessed on may 2010). http://www.omnia-
project.org, May.
Max Silberztein. 2009. NooJ linguistic
software (accessed on september 2009).
http://www.nooj4nlp.net/pages/nooj.html,
September.
Metze, F., J. McDonough, H. Soltau, A. Waibel,
A. Lavie, S. Burger, C. Langley, L. Levin,
T. Schultz, F. Pianesi, R. Cattoni, G. Lazzari,
N. Mana, and E. Pianta. 2002. The Nespole!
speech-to-speech translation system. In Proceed-
ings of HLT-2002 Human Language Technology
Conference, San Diego, USA, march.
Nguyen, H.T., C. Boitet, and G. S´erasset. 2007. PI-
VAX, an online contributive lexical data base for
heterogeneous MT systems using a lexical pivot.
In SNLP, Bangkok, Thailand.
Nguyen, Hong-Thai. 2009. EMEU w,
a simple interface to test the Q-
Systems (accessed on september 2009).
http://sway.imag.fr/unldeco/SystemsQ.po?localhost=
/home/nguyenht/SYS-Q/MONITEUR/, Septem-
ber.
Uchida Hiroshi et al. 2009. The UNDL
foundation (accessed on september 2009).
http://www.undl.org/, September.
Wierzbicka, Anna. 1996. Semantics: Primes and
Universals. Oxford University Press.
Rouquet, David and Hong-Thai Nguyen. 2009a.
Interlingual annotation of texts in the OMNIA
project. Poznan, Poland.
Rouquet, David and Hong-Thai Nguyen. 2009b.
Multilinguisation d’une ontologie par des core-
spondances avec un lexique pivot. In TOTh09, An-
necy, France, May.
Rouquet, David, Cassia Trojahn, Didier Scwab, and
Gilles S´erasset. 2010. Building correspondences
between ontologies and lexical resources. In to be
published.
Salton, Gerard. 1991. The Smart document re-
trieval project. In Proc. of the 14th Annual Int’l
ACM/SIGIR Conf. on Research and Development
in Information Retrieval, Chicago.
</reference>
<page confidence="0.998409">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911544">
<title confidence="0.954059">Ontology driven content extraction using interlingual annotation of texts in the OMNIA project</title>
<author confidence="0.990014">Achille Falaise</author>
<author confidence="0.990014">David Rouquet</author>
<author confidence="0.990014">Didier Schwab</author>
<author confidence="0.990014">Herv´e Blanchon</author>
<author confidence="0.990014">Christian Boitet</author>
<affiliation confidence="0.994635">LIG-GETALP, University of Grenoble</affiliation>
<abstract confidence="0.999475285714286">OMNIA is an on-going project that aims to retrieve images accompanied with multilingual texts. In this paper, we propose a generic method (language and domain independent) to extract conceptual information from such texts and spontaneous user requests. First, texts are labelled with interlingual annotation, then a generic extractor taking a domain ontology as a parameter extract relevant conceptual information. Implementation is also presented with a first experiment and preliminary results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aitchenson</author>
</authors>
<title>Words in the Mind. An Introduction to the Mental Lexicon.</title>
<date>2003</date>
<publisher>Blackwell Publishers.</publisher>
<contexts>
<context position="16524" citStr="Aitchenson, 2003" startWordPosition="2602" endWordPosition="2603">s 732 concepts in the following domains : animals, politics, religion, army, sports, monuments, transports, games, entertainment, emotions, etc. To us, using an ontology has the following advantages : • Ontologies give an axiomatic description of a domain, based on formal logics (usu4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl 56 ally description logics (Baader et al., 2003)) with an explicit semantic. Thus, the knowledge stored in them can be used soundly by software agents; • Ontological structures are close to the organisation of ideas as semantic networks in human mind (Aitchenson, 2003) and are labeled with strings derived from natural languages. Thus humans can use them (browsing or contributing) in a pretty natural way; • Finally, with the advent of the Semantic Web and normative initiatives such as the W3C5, ontologies come with a lot of shared tools for editing, querying, merging, etc. As the content extractor might only process UW annotations, it is necessary that the knowledge base is whether expressed using UW or linked to UW. The ontology is here considered as a domain parameter of content extraction and can be changed to improve preformances on specific data collect</context>
</contexts>
<marker>Aitchenson, 2003</marker>
<rawString>Aitchenson, J. 2003. Words in the Mind. An Introduction to the Mental Lexicon. Blackwell Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>De Franz Baader</author>
<author>Diego Calvanese</author>
<author>Deborah McGuinness</author>
<author>Peter Patel-Schneider</author>
<author>Daniele Nardi</author>
</authors>
<title>The Description Logic Handbook.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16303" citStr="Baader et al., 2003" startWordPosition="2563" endWordPosition="2566"> start-time=(md=10), end-time(md=15, month=9) ) ) } 5.2 Ontologies as parameter for the domain In the project OMNIA, the knowledge base has the form of a lightweight ontology for image classification 4. This ontology contains 732 concepts in the following domains : animals, politics, religion, army, sports, monuments, transports, games, entertainment, emotions, etc. To us, using an ontology has the following advantages : • Ontologies give an axiomatic description of a domain, based on formal logics (usu4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl 56 ally description logics (Baader et al., 2003)) with an explicit semantic. Thus, the knowledge stored in them can be used soundly by software agents; • Ontological structures are close to the organisation of ideas as semantic networks in human mind (Aitchenson, 2003) and are labeled with strings derived from natural languages. Thus humans can use them (browsing or contributing) in a pretty natural way; • Finally, with the advent of the Semantic Web and normative initiatives such as the W3C5, ontologies come with a lot of shared tools for editing, querying, merging, etc. As the content extractor might only process UW annotations, it is nec</context>
</contexts>
<marker>Baader, Calvanese, McGuinness, Patel-Schneider, Nardi, 2003</marker>
<rawString>Baader, De Franz, Diego Calvanese, Deborah McGuinness, Peter Patel-Schneider, and Daniele Nardi. 2003. The Description Logic Handbook. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Blanchon</author>
<author>C Boitet</author>
</authors>
<title>Speech translation for french within the C-STAR II consortium and future perspectives.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP</booktitle>
<pages>412--417</pages>
<location>Beijing, China.</location>
<contexts>
<context position="14786" citStr="Blanchon and Boitet, 2000" startWordPosition="2334" endWordPosition="2337">re used for automatic disambiguation of texts. Using this method, we calculate confidence score for each UW hypothesis appearing in the Q-Graph. 2Conceptual vectors can be associated with any content, not only text: images, videos, multimedia, Web pages, etc. 3The semantic field is the set of ideas conveyed by a term. 5 Ontology driven content extraction The content extraction has to be leaded by a “knowledge base” containing the informations we want to retrieve. 5.1 Previous works in content extraction This approach has its roots in machine translation projects such as C-Star II (1993-1999) (Blanchon and Boitet, 2000) and Nespole! (2000-2002) (Metze et al., 2002), for on the fly translation of oral speech acts in the domain of tourism. In these projects, semantic transfer was achieved through an IF (Inter-exchange Format), that is a semantic pivot dedicated to the domain. This IF allows to store information extracted from texts but is although used to lead the content extraction process by giving a formal representation of the relevant informations to extract, according to the domain. The Nespole! IF consists of 123 concepts from the tourism domain, associated with several arguments and associable with spe</context>
</contexts>
<marker>Blanchon, Boitet, 2000</marker>
<rawString>Blanchon, H. and C. Boitet. 2000. Speech translation for french within the C-STAR II consortium and future perspectives. In Proc. ICSLP 2000, pages 412–417, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Boitet</author>
<author>Igor Boguslavskij</author>
<author>Jesus Carde˜nosa</author>
</authors>
<title>An evaluation of UNL usability for high quality multilingualization and projections for a future UNL++ language.</title>
<date>2009</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>361--373</pages>
<marker>Boitet, Boguslavskij, Carde˜nosa, 2009</marker>
<rawString>Boitet, Christian, Igor Boguslavskij, and Jesus Carde˜nosa. 2009. An evaluation of UNL usability for high quality multilingualization and projections for a future UNL++ language. In Computational Linguistics and Intelligent Text Processing, pages 361–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Colmerauer</author>
</authors>
<title>Les syst`emes-q ou un formalisme pour analyser et synth´etiser des phrases sur ordinateur. d´epartement d’informatique de l’Universit´e de Montr´eal, publication interne,</title>
<date>1970</date>
<pages>43</pages>
<contexts>
<context position="8980" citStr="Colmerauer, 1970" startWordPosition="1406" endWordPosition="1408">u-sup-lounge)’&gt;waiting room&lt;/tag&gt; Table 1: Naive annotation of a text fragment graph structure decorated with bracketed expressions (trees) and, moreover, allow processing on this structure via graph rewriting rules (a set of such rewriting rules is a so called Q-System). An example of the Q-System formalism is given in figure 3 of section 3.2.3. It presents successively : the textual input representing a Qgraph, a rewriting rule and a graphical view of the Q-graph obtained after the application of the rule (and others). The Q-Systems were proposed by Alain Colmeraurer at Montreal University (Colmerauer, 1970). For our goal, they have three main advantages : • they provide the formalized internal structure for linguistic portability that we mentioned in the introduction (Hajlaoui and Boitet, 2007) ; • they unify text processing with powerful graph rewriting systems ; English volume French volume Interlingual UW volume 54 • they allow the creation or the edition of a process by non-programmers (e.g. linguists) using SLLP (Specialized Language for Linguistic Programming). We are actually using a reimplementation of the Q-Systems made in 2007 by Hong-Thai Nguyen during his PhD in the LIG-GETALP team (</context>
</contexts>
<marker>Colmerauer, 1970</marker>
<rawString>Colmerauer, A. 1970. Les syst`emes-q ou un formalisme pour analyser et synth´etiser des phrases sur ordinateur. d´epartement d’informatique de l’Universit´e de Montr´eal, publication interne, 43, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Daoud</author>
</authors>
<title>Il faut et on peut construire des syst`emes de commerce ´electronique a` interface en langue naturelle restreints (et multilingues) en utilisant des m´ethodes orient´ees vers les souslangages et le contenu.</title>
<date>2006</date>
<tech>Ph.D. thesis, UJF,</tech>
<contexts>
<context position="1377" citStr="Daoud, 2006" startWordPosition="200" endWordPosition="201">e OMNIA project (Luca Marchesotti et al., 2010) aims to retrieve images that are described with multilingual free companion texts (captions, comments, etc.) in large Web datasets. Images are first classified with formal descriptors in a lightweight ontology using automatic textual and visual analysis. Then, users may express spontaneous queries in their mother tongue to retrieve images. In order to build both formal descriptors and queries for the ontology, a content extraction in multilingual texts is required. Multilingual content extraction does not imply translation. It has been shown in (Daoud, 2006) that annotating words or chunks with interlingual lexemes is a valid approach to initiate a content extraction. We thus skip syntactical analysis, an expensive and low quality process, and get language-independent data early in our flow, allowing further treatments to be languageindependent. We use the lightweight ontology for image classifications as the formal knowledge representation tha determines relevant information to extract. This ontology is considered as a domain parameter for the content extractor. We are testing this method on a database provided for the image retrieval challenge </context>
</contexts>
<marker>Daoud, 2006</marker>
<rawString>Daoud, Daoud. 2006. Il faut et on peut construire des syst`emes de commerce ´electronique a` interface en langue naturelle restreints (et multilingues) en utilisant des m´ethodes orient´ees vers les souslangages et le contenu. Ph.D. thesis, UJF, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="12377" citStr="Deerwester et al., 1990" startWordPosition="1953" endWordPosition="1956">f ambiguities generated by this approach (up to a dozen UW for a single word), we need to include a disambiguation process. This process, based on conceptual vectors, is presented in the next section. 4 Conceptual vector based disambiguation Vectors have been used in NLP for over 40 years. For information retrieval, the standard vector model (SVM) was invented by Salton (Salton, 1991) during the late 60’s, while for meaning representation, latent semantic analysis (LSA) 1http://infolingu.univ-mlv.fr/DonneesLinguistiques/ Dictionnaires/telechargement.html 55 was developed during the late 80’s (Deerwester et al., 1990). These approaches are inspired by distributional semantics (Harris et al., 1989) which hypothesises that a word meaning can be defined by its co-text. For example, the meaning of ,milk- could be described by {,cow-, ,cat-, ,white-, ,cheese-, ,mammal-, ... }. Hence, distributional vector elements correspond directly (for SVM) or indirectly (for LSA) to lexical items from utterances. The conceptual vector model is different as it is inspired by componential linguistics (Hjelmlev, 1968) which holds that the meaning of words can be described with semantic components. These can be considered as at</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Deerwester, Scott C., Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J´erˆome Euzenat</author>
<author>Pavel Shvaiko</author>
</authors>
<date>2007</date>
<booktitle>Ontology matching.</booktitle>
<publisher>Springer,</publisher>
<location>Heidelberg (DE).</location>
<contexts>
<context position="18115" citStr="Euzenat and Shvaiko, 2007" startWordPosition="2856" endWordPosition="2859">he correspondences must be easily manipulated by users so they can manually improve the quality of automatically created alignments with post-edition. Constructing and maintaining an alignment between an ontology and an UW lexicon is a challenging task (Rouquet and Nguyen, 2009b). Basically, any lexical resource can be represented in an ontology language as a graph. We propose to use an OWL version of the UW volume available on Kaiko website 7. It allows us 5http://www.w3.org/ 6http://www.w3.org/2004/OWL/ 7http://kaiko.getalp.org to benefit of classical ontology matching techniques and tools (Euzenat and Shvaiko, 2007) to represent, compute and manipulate the alignment. We implemented two string based matching techniques on top of the alignment API (Euzenat, 2004). Specific disambiguation methods are in development to improve the alignment precision. Some of them are based on conceptual vectors presented in section 4, others will adapt structural ontology matching techniques. This approach to match an ontology with a lexical resource is detailled in (Rouquet et al., 2010). 5.3 The generic extractor In the case of the OMNIA project, the system output format is constraint by the goal of an integration with vi</context>
</contexts>
<marker>Euzenat, Shvaiko, 2007</marker>
<rawString>Euzenat, J´erˆome and Pavel Shvaiko. 2007. Ontology matching. Springer, Heidelberg (DE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J´erˆome Euzenat</author>
</authors>
<title>An API for ontology alignment.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Semantic Web Conference,</booktitle>
<pages>698--7112</pages>
<location>Hiroshima, Japan.</location>
<contexts>
<context position="18263" citStr="Euzenat, 2004" startWordPosition="2882" endWordPosition="2884">ing and maintaining an alignment between an ontology and an UW lexicon is a challenging task (Rouquet and Nguyen, 2009b). Basically, any lexical resource can be represented in an ontology language as a graph. We propose to use an OWL version of the UW volume available on Kaiko website 7. It allows us 5http://www.w3.org/ 6http://www.w3.org/2004/OWL/ 7http://kaiko.getalp.org to benefit of classical ontology matching techniques and tools (Euzenat and Shvaiko, 2007) to represent, compute and manipulate the alignment. We implemented two string based matching techniques on top of the alignment API (Euzenat, 2004). Specific disambiguation methods are in development to improve the alignment precision. Some of them are based on conceptual vectors presented in section 4, others will adapt structural ontology matching techniques. This approach to match an ontology with a lexical resource is detailled in (Rouquet et al., 2010). 5.3 The generic extractor In the case of the OMNIA project, the system output format is constraint by the goal of an integration with visual analysis results, in a larger multimodal system. The visual analysis systems are also based on concept extraction, but does not need an ontolog</context>
</contexts>
<marker>Euzenat, 2004</marker>
<rawString>Euzenat, J´erˆome. 2004. An API for ontology alignment. In Proceedings of the 3rd International Semantic Web Conference, pages 698–7112, Hiroshima, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy T Fielding</author>
</authors>
<title>Architectural styles and the design of network-based software architectures.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California.</institution>
<contexts>
<context position="4720" citStr="Fielding, 2000" startWordPosition="721" endWordPosition="722">of the ontology or annotations in the original text, etc. In the case of OMNIA, conceptual information extracted from companion texts is stored in a database, while conceptual information extracted from users requests are transformed into formal requests for the database (such as SQL, SPARQL, etc.). 2.2 Implementation The general process is implemented following a Service Oriented Architecture (SOA). Each part of the process corresponds to a service. This allowed us to reuse part of existing resources developed on heterogeneous platforms using web interfaces (in the best case REST interfaces (Fielding, 2000), but frequently only HTML form-based interfaces). A service supervisor has been built to deal with such an heterogeneity and address normalization issues (e.g. line-breaks, encoding, identification, cookies, page forwarding, etc.). This architecture is able to process multiple tasks concurrently, allowing to deal with users requests in real time while processing companion texts in the background. 3 Interlingual annotation We present in this section the preliminary treatments of multilingual texts (image companion texts or user requests) that are required for our content extraction process (Ro</context>
</contexts>
<marker>Fielding, 2000</marker>
<rawString>Fielding, Roy T. 2000. Architectural styles and the design of network-based software architectures. Ph.D. thesis, University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Algirdas Julien Greimas</author>
</authors>
<title>Structural Semantics: An Attempt at a Method.</title>
<date>1984</date>
<publisher>University of Nebraska Press.</publisher>
<contexts>
<context position="13119" citStr="Greimas, 1984" startWordPosition="2070" endWordPosition="2071">defined by its co-text. For example, the meaning of ,milk- could be described by {,cow-, ,cat-, ,white-, ,cheese-, ,mammal-, ... }. Hence, distributional vector elements correspond directly (for SVM) or indirectly (for LSA) to lexical items from utterances. The conceptual vector model is different as it is inspired by componential linguistics (Hjelmlev, 1968) which holds that the meaning of words can be described with semantic components. These can be considered as atoms of meaning (known as primitives (Wierzbicka, 1996)), or also only as constituents of the meaning (known as semes, features (Greimas, 1984), concepts, ideas). For example, the meaning of ,milkcould be described by {LIQUID, DAIRYPRODUCT, WHITE, FOOD, ... }. Conceptual vectors model a formalism for the projection of this notion in a vectorial space. Hence, conceptual vector elements correspond to concepts indirectly, as we will see later. For textual purposes2, conceptual vectors can be associated to all levels of a text (word, phrase, sentence, paragraph, whole texts, etc.). As they represent ideas, they correspond to the notion of semantic field3 at the lexical level, and to the overall thematic aspects at the level of the entire</context>
</contexts>
<marker>Greimas, 1984</marker>
<rawString>Greimas, Algirdas Julien. 1984. Structural Semantics: An Attempt at a Method. University of Nebraska Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Najeh Hajlaoui</author>
<author>Christian Boitet</author>
</authors>
<title>Portage linguistique d’applications de gestion de contenu.</title>
<date>2007</date>
<booktitle>In TOTh07,</booktitle>
<location>Annecy.</location>
<contexts>
<context position="9171" citStr="Hajlaoui and Boitet, 2007" startWordPosition="1436" endWordPosition="1439">ructure via graph rewriting rules (a set of such rewriting rules is a so called Q-System). An example of the Q-System formalism is given in figure 3 of section 3.2.3. It presents successively : the textual input representing a Qgraph, a rewriting rule and a graphical view of the Q-graph obtained after the application of the rule (and others). The Q-Systems were proposed by Alain Colmeraurer at Montreal University (Colmerauer, 1970). For our goal, they have three main advantages : • they provide the formalized internal structure for linguistic portability that we mentioned in the introduction (Hajlaoui and Boitet, 2007) ; • they unify text processing with powerful graph rewriting systems ; English volume French volume Interlingual UW volume 54 • they allow the creation or the edition of a process by non-programmers (e.g. linguists) using SLLP (Specialized Language for Linguistic Programming). We are actually using a reimplementation of the Q-Systems made in 2007 by Hong-Thai Nguyen during his PhD in the LIG-GETALP team (Nguyen, 2009). 3.2 Framework of the annotation process 3.2.1 Overview The annotation process is composed by the following steps : 1. splitting the text in fragments if too long ; 2. lemmatisa</context>
</contexts>
<marker>Hajlaoui, Boitet, 2007</marker>
<rawString>Hajlaoui, Najeh and Christian Boitet. 2007. Portage linguistique d’applications de gestion de contenu. In TOTh07, Annecy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
<author>Michael Gottfried</author>
<author>Thomas Ryckman</author>
<author>Paul Mattick Jr</author>
<author>Anne Daladier</author>
<author>T N Harris</author>
<author>S Harris</author>
</authors>
<title>The form of Information in</title>
<date>1989</date>
<journal>Science, Analysis of Immunology Sublanguage,</journal>
<booktitle>of Boston Studies in the Philosophy of Science.</booktitle>
<volume>104</volume>
<publisher>Kluwer Academic Publisher,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="12458" citStr="Harris et al., 1989" startWordPosition="1964" endWordPosition="1967">d to include a disambiguation process. This process, based on conceptual vectors, is presented in the next section. 4 Conceptual vector based disambiguation Vectors have been used in NLP for over 40 years. For information retrieval, the standard vector model (SVM) was invented by Salton (Salton, 1991) during the late 60’s, while for meaning representation, latent semantic analysis (LSA) 1http://infolingu.univ-mlv.fr/DonneesLinguistiques/ Dictionnaires/telechargement.html 55 was developed during the late 80’s (Deerwester et al., 1990). These approaches are inspired by distributional semantics (Harris et al., 1989) which hypothesises that a word meaning can be defined by its co-text. For example, the meaning of ,milk- could be described by {,cow-, ,cat-, ,white-, ,cheese-, ,mammal-, ... }. Hence, distributional vector elements correspond directly (for SVM) or indirectly (for LSA) to lexical items from utterances. The conceptual vector model is different as it is inspired by componential linguistics (Hjelmlev, 1968) which holds that the meaning of words can be described with semantic components. These can be considered as atoms of meaning (known as primitives (Wierzbicka, 1996)), or also only as constitu</context>
</contexts>
<marker>Harris, Gottfried, Ryckman, Jr, Daladier, Harris, Harris, 1989</marker>
<rawString>Harris, Zellig S., Michael Gottfried, Thomas Ryckman, Paul Mattick Jr., Anne Daladier, T.N. Harris, and S. Harris. 1989. The form of Information in Science, Analysis of Immunology Sublanguage, volume 104 of Boston Studies in the Philosophy of Science. Kluwer Academic Publisher, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis Hjelmlev</author>
</authors>
<title>Prol´egol`eme a` une th´eorie du langage. ´editions de minuit.</title>
<date>1968</date>
<contexts>
<context position="12866" citStr="Hjelmlev, 1968" startWordPosition="2029" endWordPosition="2031">mlv.fr/DonneesLinguistiques/ Dictionnaires/telechargement.html 55 was developed during the late 80’s (Deerwester et al., 1990). These approaches are inspired by distributional semantics (Harris et al., 1989) which hypothesises that a word meaning can be defined by its co-text. For example, the meaning of ,milk- could be described by {,cow-, ,cat-, ,white-, ,cheese-, ,mammal-, ... }. Hence, distributional vector elements correspond directly (for SVM) or indirectly (for LSA) to lexical items from utterances. The conceptual vector model is different as it is inspired by componential linguistics (Hjelmlev, 1968) which holds that the meaning of words can be described with semantic components. These can be considered as atoms of meaning (known as primitives (Wierzbicka, 1996)), or also only as constituents of the meaning (known as semes, features (Greimas, 1984), concepts, ideas). For example, the meaning of ,milkcould be described by {LIQUID, DAIRYPRODUCT, WHITE, FOOD, ... }. Conceptual vectors model a formalism for the projection of this notion in a vectorial space. Hence, conceptual vector elements correspond to concepts indirectly, as we will see later. For textual purposes2, conceptual vectors can</context>
</contexts>
<marker>Hjelmlev, 1968</marker>
<rawString>Hjelmlev, Louis. 1968. Prol´egol`eme a` une th´eorie du langage. ´editions de minuit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Schwab</author>
<author>Mathieu Lafourcade</author>
</authors>
<title>Lexical functions for ants based semantic analysis.</title>
<date>2007</date>
<booktitle>In ICAI’07- The 2007 International Conference on Artificial Intelligence,</booktitle>
<location>Las Vegas, Nevada, USA, juin.</location>
<contexts>
<context position="14125" citStr="Schwab and Lafourcade, 2007" startWordPosition="2230" endWordPosition="2233"> a text (word, phrase, sentence, paragraph, whole texts, etc.). As they represent ideas, they correspond to the notion of semantic field3 at the lexical level, and to the overall thematic aspects at the level of the entire text. Conceptual vectors can also be applied to lexical meanings. They have been studied in word sense disambiguation (WSD) using isotopic properties in a text, i.e. redundancy of ideas (Greimas, 1984). The basic idea is to maximise the overlap of shared ideas between senses of lexical items. This can be done by computing the angular distance between two conceptual vectors (Schwab and Lafourcade, 2007). In our case, conceptual vectors are used for automatic disambiguation of texts. Using this method, we calculate confidence score for each UW hypothesis appearing in the Q-Graph. 2Conceptual vectors can be associated with any content, not only text: images, videos, multimedia, Web pages, etc. 3The semantic field is the set of ideas conveyed by a term. 5 Ontology driven content extraction The content extraction has to be leaded by a “knowledge base” containing the informations we want to retrieve. 5.1 Previous works in content extraction This approach has its roots in machine translation proje</context>
</contexts>
<marker>Schwab, Lafourcade, 2007</marker>
<rawString>Schwab, Didier and Mathieu Lafourcade. 2007. Lexical functions for ants based semantic analysis. In ICAI’07- The 2007 International Conference on Artificial Intelligence, Las Vegas, Nevada, USA, juin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Parr</author>
</authors>
<date>2009</date>
<note>ANTLR parser generator (accessed on</note>
<marker>Parr, 2009</marker>
<rawString>Terence Parr et al. 2009. ANTLR parser generator (accessed on september 2009). http://www.antlr.org/, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Carde˜nosa</author>
</authors>
<title>The U++ consortium (accessed on</title>
<date>2009</date>
<note>http://www.unl.fi.upm.es/consorcio/index.php,</note>
<marker>Carde˜nosa, 2009</marker>
<rawString>Jesus Carde˜nosa et al. 2009. The U++ consortium (accessed on september 2009). http://www.unl.fi.upm.es/consorcio/index.php, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luca Marchesotti</author>
</authors>
<title>The Omnia project (accessed on</title>
<date>2010</date>
<location>http://www.omniaproject.org,</location>
<marker>Marchesotti, 2010</marker>
<rawString>Luca Marchesotti et al. 2010. The Omnia project (accessed on may 2010). http://www.omniaproject.org, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Silberztein</author>
</authors>
<title>NooJ linguistic software (accessed on</title>
<date>2009</date>
<note>http://www.nooj4nlp.net/pages/nooj.html,</note>
<marker>Silberztein, 2009</marker>
<rawString>Max Silberztein. 2009. NooJ linguistic software (accessed on september 2009). http://www.nooj4nlp.net/pages/nooj.html, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Metze</author>
<author>J McDonough</author>
<author>H Soltau</author>
<author>A Waibel</author>
<author>A Lavie</author>
<author>S Burger</author>
<author>C Langley</author>
<author>L Levin</author>
<author>T Schultz</author>
<author>F Pianesi</author>
<author>R Cattoni</author>
<author>G Lazzari</author>
<author>N Mana</author>
<author>E Pianta</author>
</authors>
<title>The Nespole! speech-to-speech translation system.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT-2002 Human Language Technology Conference,</booktitle>
<location>San Diego, USA,</location>
<contexts>
<context position="14832" citStr="Metze et al., 2002" startWordPosition="2341" endWordPosition="2344">this method, we calculate confidence score for each UW hypothesis appearing in the Q-Graph. 2Conceptual vectors can be associated with any content, not only text: images, videos, multimedia, Web pages, etc. 3The semantic field is the set of ideas conveyed by a term. 5 Ontology driven content extraction The content extraction has to be leaded by a “knowledge base” containing the informations we want to retrieve. 5.1 Previous works in content extraction This approach has its roots in machine translation projects such as C-Star II (1993-1999) (Blanchon and Boitet, 2000) and Nespole! (2000-2002) (Metze et al., 2002), for on the fly translation of oral speech acts in the domain of tourism. In these projects, semantic transfer was achieved through an IF (Inter-exchange Format), that is a semantic pivot dedicated to the domain. This IF allows to store information extracted from texts but is although used to lead the content extraction process by giving a formal representation of the relevant informations to extract, according to the domain. The Nespole! IF consists of 123 concepts from the tourism domain, associated with several arguments and associable with speech acts markers. The extraction process is ba</context>
</contexts>
<marker>Metze, McDonough, Soltau, Waibel, Lavie, Burger, Langley, Levin, Schultz, Pianesi, Cattoni, Lazzari, Mana, Pianta, 2002</marker>
<rawString>Metze, F., J. McDonough, H. Soltau, A. Waibel, A. Lavie, S. Burger, C. Langley, L. Levin, T. Schultz, F. Pianesi, R. Cattoni, G. Lazzari, N. Mana, and E. Pianta. 2002. The Nespole! speech-to-speech translation system. In Proceedings of HLT-2002 Human Language Technology Conference, San Diego, USA, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Nguyen</author>
<author>C Boitet</author>
<author>G S´erasset</author>
</authors>
<title>PIVAX, an online contributive lexical data base for heterogeneous MT systems using a lexical pivot. In SNLP,</title>
<date>2007</date>
<location>Bangkok, Thailand.</location>
<marker>Nguyen, Boitet, S´erasset, 2007</marker>
<rawString>Nguyen, H.T., C. Boitet, and G. S´erasset. 2007. PIVAX, an online contributive lexical data base for heterogeneous MT systems using a lexical pivot. In SNLP, Bangkok, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Thai Nguyen</author>
</authors>
<date>2009</date>
<publisher>EMEU w,</publisher>
<contexts>
<context position="5342" citStr="Nguyen, 2009" startWordPosition="812" endWordPosition="813">uently only HTML form-based interfaces). A service supervisor has been built to deal with such an heterogeneity and address normalization issues (e.g. line-breaks, encoding, identification, cookies, page forwarding, etc.). This architecture is able to process multiple tasks concurrently, allowing to deal with users requests in real time while processing companion texts in the background. 3 Interlingual annotation We present in this section the preliminary treatments of multilingual texts (image companion texts or user requests) that are required for our content extraction process (Rouquet and Nguyen, 2009a). In order to allow a content extraction in multilingual texts, we propose to represent texts with the internal formalism of the Q-Systems and to annotate chunks with UNL interlingual lexemes (UW) . Roughly, we are making an interlingual lemmatisation, containing more information than simple tagging, that is not currently proposed by any lemmatisation software. 3.1 Resources and data structures 3.1.1 The Universal Network Language UNL (Boitet et al., 2009; Uchida Hiroshi et al., 2009) is a pivot language that represents the meaning of a sentence with a semantic abstract structure (an hyper-g</context>
<context position="9593" citStr="Nguyen, 2009" startWordPosition="1505" endWordPosition="1506">. For our goal, they have three main advantages : • they provide the formalized internal structure for linguistic portability that we mentioned in the introduction (Hajlaoui and Boitet, 2007) ; • they unify text processing with powerful graph rewriting systems ; English volume French volume Interlingual UW volume 54 • they allow the creation or the edition of a process by non-programmers (e.g. linguists) using SLLP (Specialized Language for Linguistic Programming). We are actually using a reimplementation of the Q-Systems made in 2007 by Hong-Thai Nguyen during his PhD in the LIG-GETALP team (Nguyen, 2009). 3.2 Framework of the annotation process 3.2.1 Overview The annotation process is composed by the following steps : 1. splitting the text in fragments if too long ; 2. lemmatisation with a specialized software; 3. transcription to the Q-Systems format; 4. creation of local bilingual dictionaries (source language - UW) for each fragment with PIVAX ; 5. execution of those dictionaries on the fragments ; 3.2.2 Lemmatisation As we want to use dictionaries where entries are lemmas, the first step is to lemmatise the input text (i.e. to annotate occurrences with possible lemmas). This step is very </context>
<context position="17767" citStr="Nguyen, 2009" startWordPosition="2806" endWordPosition="2807">ontology6, we must be able to link it with a volume of UW considering the following constraints : Creating manually such correspondences is costly due to the size of resources so an automatic process is requiered. Ontologies and lexicons evolve over the time so an alignment must be adaptable to incremental evolutions of resources. The correspondences must be easily manipulated by users so they can manually improve the quality of automatically created alignments with post-edition. Constructing and maintaining an alignment between an ontology and an UW lexicon is a challenging task (Rouquet and Nguyen, 2009b). Basically, any lexical resource can be represented in an ontology language as a graph. We propose to use an OWL version of the UW volume available on Kaiko website 7. It allows us 5http://www.w3.org/ 6http://www.w3.org/2004/OWL/ 7http://kaiko.getalp.org to benefit of classical ontology matching techniques and tools (Euzenat and Shvaiko, 2007) to represent, compute and manipulate the alignment. We implemented two string based matching techniques on top of the alignment API (Euzenat, 2004). Specific disambiguation methods are in development to improve the alignment precision. Some of them ar</context>
</contexts>
<marker>Nguyen, 2009</marker>
<rawString>Nguyen, Hong-Thai. 2009. EMEU w,</rawString>
</citation>
<citation valid="true">
<title>a simple interface to test the QSystems (accessed on</title>
<date>2009</date>
<note>http://sway.imag.fr/unldeco/SystemsQ.po?localhost= /home/nguyenht/SYS-Q/MONITEUR/,</note>
<marker>2009</marker>
<rawString>a simple interface to test the QSystems (accessed on september 2009). http://sway.imag.fr/unldeco/SystemsQ.po?localhost= /home/nguyenht/SYS-Q/MONITEUR/, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uchida Hiroshi</author>
</authors>
<title>The UNDL foundation (accessed on</title>
<date>2009</date>
<note>http://www.undl.org/,</note>
<marker>Hiroshi, 2009</marker>
<rawString>Uchida Hiroshi et al. 2009. The UNDL foundation (accessed on september 2009). http://www.undl.org/, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Wierzbicka</author>
</authors>
<title>Semantics: Primes and Universals.</title>
<date>1996</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="13031" citStr="Wierzbicka, 1996" startWordPosition="2056" endWordPosition="2057">stributional semantics (Harris et al., 1989) which hypothesises that a word meaning can be defined by its co-text. For example, the meaning of ,milk- could be described by {,cow-, ,cat-, ,white-, ,cheese-, ,mammal-, ... }. Hence, distributional vector elements correspond directly (for SVM) or indirectly (for LSA) to lexical items from utterances. The conceptual vector model is different as it is inspired by componential linguistics (Hjelmlev, 1968) which holds that the meaning of words can be described with semantic components. These can be considered as atoms of meaning (known as primitives (Wierzbicka, 1996)), or also only as constituents of the meaning (known as semes, features (Greimas, 1984), concepts, ideas). For example, the meaning of ,milkcould be described by {LIQUID, DAIRYPRODUCT, WHITE, FOOD, ... }. Conceptual vectors model a formalism for the projection of this notion in a vectorial space. Hence, conceptual vector elements correspond to concepts indirectly, as we will see later. For textual purposes2, conceptual vectors can be associated to all levels of a text (word, phrase, sentence, paragraph, whole texts, etc.). As they represent ideas, they correspond to the notion of semantic fie</context>
</contexts>
<marker>Wierzbicka, 1996</marker>
<rawString>Wierzbicka, Anna. 1996. Semantics: Primes and Universals. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rouquet</author>
<author>Hong-Thai Nguyen</author>
</authors>
<title>Interlingual annotation of texts in the OMNIA project.</title>
<date>2009</date>
<location>Poznan, Poland.</location>
<contexts>
<context position="5342" citStr="Rouquet and Nguyen, 2009" startWordPosition="810" endWordPosition="813">0), but frequently only HTML form-based interfaces). A service supervisor has been built to deal with such an heterogeneity and address normalization issues (e.g. line-breaks, encoding, identification, cookies, page forwarding, etc.). This architecture is able to process multiple tasks concurrently, allowing to deal with users requests in real time while processing companion texts in the background. 3 Interlingual annotation We present in this section the preliminary treatments of multilingual texts (image companion texts or user requests) that are required for our content extraction process (Rouquet and Nguyen, 2009a). In order to allow a content extraction in multilingual texts, we propose to represent texts with the internal formalism of the Q-Systems and to annotate chunks with UNL interlingual lexemes (UW) . Roughly, we are making an interlingual lemmatisation, containing more information than simple tagging, that is not currently proposed by any lemmatisation software. 3.1 Resources and data structures 3.1.1 The Universal Network Language UNL (Boitet et al., 2009; Uchida Hiroshi et al., 2009) is a pivot language that represents the meaning of a sentence with a semantic abstract structure (an hyper-g</context>
<context position="17767" citStr="Rouquet and Nguyen, 2009" startWordPosition="2804" endWordPosition="2807">ven any OWL ontology6, we must be able to link it with a volume of UW considering the following constraints : Creating manually such correspondences is costly due to the size of resources so an automatic process is requiered. Ontologies and lexicons evolve over the time so an alignment must be adaptable to incremental evolutions of resources. The correspondences must be easily manipulated by users so they can manually improve the quality of automatically created alignments with post-edition. Constructing and maintaining an alignment between an ontology and an UW lexicon is a challenging task (Rouquet and Nguyen, 2009b). Basically, any lexical resource can be represented in an ontology language as a graph. We propose to use an OWL version of the UW volume available on Kaiko website 7. It allows us 5http://www.w3.org/ 6http://www.w3.org/2004/OWL/ 7http://kaiko.getalp.org to benefit of classical ontology matching techniques and tools (Euzenat and Shvaiko, 2007) to represent, compute and manipulate the alignment. We implemented two string based matching techniques on top of the alignment API (Euzenat, 2004). Specific disambiguation methods are in development to improve the alignment precision. Some of them ar</context>
</contexts>
<marker>Rouquet, Nguyen, 2009</marker>
<rawString>Rouquet, David and Hong-Thai Nguyen. 2009a. Interlingual annotation of texts in the OMNIA project. Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rouquet</author>
<author>Hong-Thai Nguyen</author>
</authors>
<title>Multilinguisation d’une ontologie par des corespondances avec un lexique pivot.</title>
<date>2009</date>
<booktitle>In TOTh09,</booktitle>
<location>Annecy, France,</location>
<contexts>
<context position="5342" citStr="Rouquet and Nguyen, 2009" startWordPosition="810" endWordPosition="813">0), but frequently only HTML form-based interfaces). A service supervisor has been built to deal with such an heterogeneity and address normalization issues (e.g. line-breaks, encoding, identification, cookies, page forwarding, etc.). This architecture is able to process multiple tasks concurrently, allowing to deal with users requests in real time while processing companion texts in the background. 3 Interlingual annotation We present in this section the preliminary treatments of multilingual texts (image companion texts or user requests) that are required for our content extraction process (Rouquet and Nguyen, 2009a). In order to allow a content extraction in multilingual texts, we propose to represent texts with the internal formalism of the Q-Systems and to annotate chunks with UNL interlingual lexemes (UW) . Roughly, we are making an interlingual lemmatisation, containing more information than simple tagging, that is not currently proposed by any lemmatisation software. 3.1 Resources and data structures 3.1.1 The Universal Network Language UNL (Boitet et al., 2009; Uchida Hiroshi et al., 2009) is a pivot language that represents the meaning of a sentence with a semantic abstract structure (an hyper-g</context>
<context position="17767" citStr="Rouquet and Nguyen, 2009" startWordPosition="2804" endWordPosition="2807">ven any OWL ontology6, we must be able to link it with a volume of UW considering the following constraints : Creating manually such correspondences is costly due to the size of resources so an automatic process is requiered. Ontologies and lexicons evolve over the time so an alignment must be adaptable to incremental evolutions of resources. The correspondences must be easily manipulated by users so they can manually improve the quality of automatically created alignments with post-edition. Constructing and maintaining an alignment between an ontology and an UW lexicon is a challenging task (Rouquet and Nguyen, 2009b). Basically, any lexical resource can be represented in an ontology language as a graph. We propose to use an OWL version of the UW volume available on Kaiko website 7. It allows us 5http://www.w3.org/ 6http://www.w3.org/2004/OWL/ 7http://kaiko.getalp.org to benefit of classical ontology matching techniques and tools (Euzenat and Shvaiko, 2007) to represent, compute and manipulate the alignment. We implemented two string based matching techniques on top of the alignment API (Euzenat, 2004). Specific disambiguation methods are in development to improve the alignment precision. Some of them ar</context>
</contexts>
<marker>Rouquet, Nguyen, 2009</marker>
<rawString>Rouquet, David and Hong-Thai Nguyen. 2009b. Multilinguisation d’une ontologie par des corespondances avec un lexique pivot. In TOTh09, Annecy, France, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rouquet</author>
<author>Cassia Trojahn</author>
<author>Didier Scwab</author>
<author>Gilles S´erasset</author>
</authors>
<title>Building correspondences between ontologies and lexical resources. In</title>
<date>2010</date>
<note>to be published.</note>
<marker>Rouquet, Trojahn, Scwab, S´erasset, 2010</marker>
<rawString>Rouquet, David, Cassia Trojahn, Didier Scwab, and Gilles S´erasset. 2010. Building correspondences between ontologies and lexical resources. In to be published.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>The Smart document retrieval project.</title>
<date>1991</date>
<booktitle>In Proc. of the 14th Annual Int’l ACM/SIGIR Conf. on Research and Development in Information Retrieval,</booktitle>
<location>Chicago.</location>
<contexts>
<context position="12140" citStr="Salton, 1991" startWordPosition="1930" endWordPosition="1931">s (sets of rules), we built local dictionaries that contain the entries for fragments of the text (about 250 words in the first experiment). Figure 3: Creation and execution of a Q-System Considering the significant quantity of ambiguities generated by this approach (up to a dozen UW for a single word), we need to include a disambiguation process. This process, based on conceptual vectors, is presented in the next section. 4 Conceptual vector based disambiguation Vectors have been used in NLP for over 40 years. For information retrieval, the standard vector model (SVM) was invented by Salton (Salton, 1991) during the late 60’s, while for meaning representation, latent semantic analysis (LSA) 1http://infolingu.univ-mlv.fr/DonneesLinguistiques/ Dictionnaires/telechargement.html 55 was developed during the late 80’s (Deerwester et al., 1990). These approaches are inspired by distributional semantics (Harris et al., 1989) which hypothesises that a word meaning can be defined by its co-text. For example, the meaning of ,milk- could be described by {,cow-, ,cat-, ,white-, ,cheese-, ,mammal-, ... }. Hence, distributional vector elements correspond directly (for SVM) or indirectly (for LSA) to lexical </context>
</contexts>
<marker>Salton, 1991</marker>
<rawString>Salton, Gerard. 1991. The Smart document retrieval project. In Proc. of the 14th Annual Int’l ACM/SIGIR Conf. on Research and Development in Information Retrieval, Chicago.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>