<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.999296">
Bootstrapping Coreference Classifiers with
Multiple Machine Learning Algorithms
</title>
<author confidence="0.998952">
Vincent Ng and Claire Cardie
</author>
<affiliation confidence="0.9964985">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.577778">
Ithaca, NY 14853-7501
</address>
<email confidence="0.998913">
{yung,cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892444444444">
Successful application of multi-view co-
training algorithms relies on the ability to
factor the available features into views that
are compatible and uncorrelated. This can
potentially preclude their use on problems
such as coreference resolution that lack an
obvious feature split. To bootstrap coref-
erence classifiers, we propose and eval-
uate a single-view weakly supervised al-
gorithm that relies on two different learn-
ing algorithms in lieu of the two different
views required by co-training. In addition,
we investigate a method for ranking un-
labeled instances to be fed back into the
bootstrapping loop as labeled data, aiming
to alleviate the problem of performance
deterioration that is commonly observed
in the course of bootstrapping.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868857142857">
Co-training (Blum and Mitchell, 1998) is a weakly
supervised paradigm that learns a task from a small
set of labeled data and a large pool of unlabeled
data using separate, but redundant views of the data
(i.e. using disjoint feature subsets to represent the
data). To ensure provable performance guaran-
tees, the co-training algorithm assumes as input a
set of views that satisfies two fairly strict condi-
tions. First, each view must be sufficient for learn-
ing the target concept. Second, the views must be
conditionally independent of each other given the
class. Empirical results on artificial data sets by
Muslea et al. (2002) and Nigam and Ghani (2000)
confirm that co-training is sensitive to these assump-
tions. Indeed, although the algorithm has been ap-
plied successfully to natural language processing
(NLP) tasks that have a natural view factorization
(e.g. web page classification (Blum and Mitchell,
1998) and named entity classification (Collins and
Singer, 1999)), there has been little success, and a
number of reported problems, when applying co-
training to NLP data sets for which no natural fea-
ture split has been found (e.g. anaphora resolution
(Mueller et al., 2002)).
As a result, researchers have begun to investigate
co-training procedures that do not require explicit
view factorization. Goldman and Zhou (2000) and
Steedman et al. (2003b) use two different learning
algorithms in lieu of the multiple views required by
standard co-training.1 The intuition is that the two
learning algorithms can potentially substitute for the
two views: different learners have different rep-
resentation and search biases and can complement
each other by inducing different hypotheses from the
data. Despite their similarities, the principles under-
lying the Goldman and Zhou and Steedman et al.
co-training algorithms are fundamentally different.
In particular, Goldman and Zhou rely on hypothesis
testing to select new instances to add to the labeled
data. On the other hand, Steedman et al. use two
learning algorithms that correspond to coarsely dif-
ferent features, thus retaining in spirit the advantages
</bodyText>
<footnote confidence="0.856890333333333">
1Steedman et al. (2003b) bootstrap two parsers that use dif-
ferent statistical models via co-training. Hence, the two parsers
can effectively be viewed as two different learning algorithms.
</footnote>
<bodyText confidence="0.998930868421053">
provided by conditionally independent feature splits
in the Blum and Mitchell algorithm.
The goal of this paper is two-fold. First, we
propose a single-view algorithm for bootstrapping
coreference classifiers. Like anaphora resolution,
noun phrase coreference resolution is a problem for
which a natural feature split is not readily available.
In related work (Ng and Cardie, 2003), we com-
pare the performance of the Blum and Mitchell co-
training algorithm with that of two existing single-
view bootstrapping algorithms — self-training with
bagging (Banko and Brill, 2001) and EM (Nigam et
al., 2000) — on coreference resolution, and show
that single-view weakly supervised learners are a vi-
able alternative to co-training for the task. This pa-
per instead focuses on developing a single-view al-
gorithm that combines aspects of each of the Gold-
man and Zhou and Steedman et al. algorithms.
Second, we investigate a new method that, in-
spired by Steedman et al. (2003a), ranks unlabeled
instances to be added to the labeled data in an at-
tempt to alleviate a problem commonly observed in
bootstrapping experiments — performance deterio-
ration due to the degradation in the quality of the
labeled data as bootstrapping progresses (Pierce and
Cardie, 2001; Riloff and Jones, 1999).
In a set of baseline experiments, we first demon-
strate that multi-view co-training fails to boost the
performance of the coreference system under var-
ious parameter settings. We then show that our
single-view weakly supervised algorithm success-
fully bootstraps the coreference classifiers, boost-
ing the F-measure score by 9-12% on two standard
coreference data sets. Finally, we present experi-
mental results that suggest that our method for rank-
ing instances is more resistant to performance dete-
rioration in the bootstrapping process than Blum and
Mitchell’s “rank-by-confidence” method.
</bodyText>
<sectionHeader confidence="0.83635" genericHeader="introduction">
2 Noun Phrase Coreference Resolution
</sectionHeader>
<footnote confidence="0.793637428571429">
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument.2 In this section, we give an overview of
the coreference resolution system to which the boot-
2Concrete examples of the coreference task can be found in
MUC-6 (1995) and MUC-7 (1998).
</footnote>
<bodyText confidence="0.99972705882353">
strapping algorithms will be applied.
The framework underlying the coreference sys-
tem is a standard combination of classification and
clustering (see Ng and Cardie (2002) for details).
Coreference resolution is first recast as a classifica-
tion task, in which a pair of NPs is classified as co-
referring or not based on constraints that are learned
from an annotated corpus. A separate clustering
mechanism then coordinates the possibly contradic-
tory pairwise classifications and constructs a parti-
tion on the set of NPs. When the system operates
within the weakly supervised setting, a weakly su-
pervised algorithm bootstraps the coreference classi-
fier from the given labeled and unlabeled data rather
than from a much larger set of labeled instances. The
clustering algorithm, however, is not manipulated by
the bootstrapping procedure.
</bodyText>
<sectionHeader confidence="0.964079" genericHeader="method">
3 Learning Algorithms
</sectionHeader>
<bodyText confidence="0.9999145">
We employ naive Bayes and decision list learners
in our single-view, multiple-learner framework for
bootstrapping coreference classifiers. This section
gives an overview of the two learners.
</bodyText>
<subsectionHeader confidence="0.996167">
3.1 Naive Bayes
</subsectionHeader>
<bodyText confidence="0.99655225">
A naive Bayes (NB) classifier is a generative classi-
fier that assigns to a test instance i with feature val-
ues &lt;x1, ..., xm&gt; the maximum a posteriori (MAP)
label y*, which is determined as follows:
</bodyText>
<equation confidence="0.9967515">
y* = arg max
y
= arg max
y
= arg max
y
</equation>
<bodyText confidence="0.999955666666667">
The first equality above follows from the definition
of MAP, the second one from Bayes rule, and the last
one from the conditional independence assumption
of the feature values. We determine the class priors
P(y) and the class densities P(xi I y) directly from
the training data using add-one smoothing.
</bodyText>
<subsectionHeader confidence="0.997835">
3.2 Decision Lists
</subsectionHeader>
<bodyText confidence="0.99984375">
Our decision list (DL) algorithm is based on that de-
scribed in Collins and Singer (1999). For each avail-
able feature fi and each possible value vj of fi in the
training data, the learner induces an element of the
</bodyText>
<equation confidence="0.9977632">
P(y  |i)
P(y)P(i I y)
m
P(y) P(xi I y)
i =1
</equation>
<bodyText confidence="0.9947269375">
Observations Justifications
Many feature-value pairs alone can de- Decision lists draw a decision boundary based on a single feature-value pair
termine the class value.3 For example, and can take advantage of this observation directly. On the other hand, naive
two NPs cannot be coreferent if they differ Bayes classifiers make a decision based on a combination of features and
in gender or semantic class. thus cannot take advantage of this observation directly.
The class distributions in coreference Naive Bayes classifiers are fairly resistant to class skewness, which can
data sets are skewed. Specifically, the only exert its influence on classifier prediction via the class priors. On the
fact that most NP pairs in a document are other hand, decision lists suffer from skewed class distributions. Elements
not coreferent implies that the negative in- corresponding to the negative class tend to aggregate towards the beginning
stances grossly outnumber the positives. of the list, causing the classifier to perform poorly on the minority class.
Many instances contain redundant in- Both naive Bayes classifiers and decision lists can take advantage of data
formation as far as classification is con- redundancy. Frequency counts of feature-value pairs in these classifiers are
cerned. For example, two NPs may dif- updated independently, and thus a single instance can possibly contribute to
fer in both gender and semantic class, but the discovery of more than one useful feature-value pair. On the other hand,
knowing one of these two differences is suf- some classifiers such as decision trees are not able to take advantage of this
ficient for determining the class value. redundancy because of their intrinsic nature of recursive data partitioning.
</bodyText>
<tableCaption confidence="0.741598">
Table 1: The justifications (shown in the right column) for using naive Bayes and decision list learner as
</tableCaption>
<bodyText confidence="0.889891571428571">
the underlying learning algorithms for bootstrapping coreference classifiers are based on the corresponding
observations on the coreference task and the features used by the coreference system in the left column.
decision list for each class y. The elements in the list
are sorted in decreasing order of the strength associ-
ated with each element, which is defined as the con-
ditional probability P(y I fi = vj) and is estimated
based on the training data as follows:
</bodyText>
<equation confidence="0.993267">
N(fi = vj, y) + α
P(y � fi = vj) = N(fi = vj) + kα
</equation>
<bodyText confidence="0.99989125">
N(x) is the frequency of event x in the training
data, α a smoothing parameter, and k the number
of classes. In this paper, k = 2 and we set α to 0.01.
A test instance is assigned the class associated with
the first element of the list whose predicate is satis-
fied by the description of the instance.
While generative classifiers estimate class densi-
ties, discriminative classifiers like decision lists fo-
cus on approximating class boundaries. Table 1 pro-
vides the justifications for choosing these two learn-
ers as components in our single-view, multi-learner
bootstrapping algorithm. Based on observations of
the coreference task and the features employed by
our coreference system, the justifications suggest
that the two learners can potentially compensate for
each other’s weaknesses.
</bodyText>
<sectionHeader confidence="0.966787" genericHeader="method">
4 Multi-View Co-Training
</sectionHeader>
<bodyText confidence="0.8498216">
In this section, we describe the Blum and Mitchell
(B&amp;M) multi-view co-training algorithm and apply
it to coreference resolution.
3This justifies the use of a decision list as a potential classi-
fier for bootstrapping. See Yarowsky (1995) for details.
</bodyText>
<subsectionHeader confidence="0.994569">
4.1 The Multi-View Co-Training Algorithm
</subsectionHeader>
<bodyText confidence="0.999996681818182">
The intuition behind the B&amp;M co-training algorithm
is to train two classifiers that can help augment each
other’s labeled data by exploiting two separate but
redundant views of the data. Specifically, each clas-
sifier is trained using one view of the labeled data
and predicts labels for all instances in the data pool,
which consists of a randomly chosen subset of the
unlabeled data. Each then selects its most confident
predictions, and adds the corresponding instances
with their predicted labels to the labeled data while
maintaining the class distribution in the labeled data.
The number of instances to be added to the la-
beled data by each classifier at each iteration is lim-
ited by a pre-specified growth size to ensure that
only the instances that have a high probability of be-
ing assigned the correct label are incorporated. The
data pool is replenished with instances from the un-
labeled data and the process is repeated.
During testing, each classifier makes an indepen-
dent decision for a test instance. In this paper, the
decision associated with the higher confidence is
taken to be the final prediction for the instance.
</bodyText>
<subsectionHeader confidence="0.953231">
4.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9997444">
One of the goals of the experiments is to enable a
fair comparison of the multi-view algorithm with
our single-view bootstrapping algorithm. Since the
B&amp;M co-training algorithm is sensitive not only to
the views employed but also to other input parame-
</bodyText>
<table confidence="0.916024714285714">
MUC-6 MUC-7
Naive Bayes Decision List Naive Bayes Decision List
Experiments R P F R P F R P F R P F
Baseline 50.7 52.6 51.6 17.9 72.0 28.7 40.1 40.2 40.1 32.4 78.3 45.8
Multi-view Co-Training 33.3 90.7 48.7 19.5 71.2 30.6 32.9 76.3 46.0 32.4 78.3 45.8
Single-view Bootstrapping 53.6 79.0 63.9 40.1 83.1 54.1 43.5 73.2 54.6 38.3 75.4 50.8
Self-Training 48.3 63.5 54.9 18.7 70.8 29.6 40.1 40.2 40.1 32.9 78.1 46.3
</table>
<tableCaption confidence="0.7922665">
Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and
F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown.
</tableCaption>
<bodyText confidence="0.99943">
ters such as the pool size and the growth size (Pierce
and Cardie, 2001), we evaluate the algorithm under
different parameter settings, as described below.
Evaluation. We use the MUC-6 (1995) and MUC-
7 (1998) coreference data sets for evaluation. The
training set is composed of 30 “dry run” texts, from
which 491659 and 482125 NP pair instances are
generated for the MUC-6 and MUC-7 data sets, re-
spectively. Unlike Ng and Cardie (2003) where we
choose one of the dryrun texts (contributing ap-
proximately 3500–3700 instances) form the labeled
data set, however, here we randomly select 1000 in-
stances. The remaining instances are used as un-
labeled data. Testing is performed by applying the
bootstrapped coreference classifier and the cluster-
ing algorithm described in section 2 on the 20–30
“formal evaluation” texts for each of the MUC-6 and
MUC-7 data sets.
Two sets of experiments are conducted, one using
naive Bayes as the underlying supervised learning
algorithm and the other the decision list learner. All
results reported are averages across five runs.
Co-training parameters. The co-training param-
eters are set as follows.
Views. We used three methods to generate the
views from the 25 features used by the coreference
system: Mueller et al.’s (2002) greedy method, ran-
dom splitting of features into views, and splitting
of features according to the feature type (i.e. lexico-
syntactic vs. non-lexico-syntactic features).4
Pool size. We tested values of 500, 1000, 5000.
Growth size. We tested values of 10, 50, 100, 200.
</bodyText>
<footnote confidence="0.6479494">
4.3 Results and Discussion
Results are shown in Table 2, where performance is
reported in terms of recall, precision, and F-measure
4Space limitation precludes a detailed description of these
methods. See Ng and Cardie (2003) for details.
</footnote>
<figure confidence="0.904421692307692">
100
Baseline
Recall
Precision
F−measure
80
70
60
50
40
30
50 100 150 200 250 300 350 400 450 500
Number of Co−Training Iterations
</figure>
<figureCaption confidence="0.999608">
Figure 1: Learning curve for co-training (pool size
</figureCaption>
<bodyText confidence="0.982988090909091">
= 500, growth size = 50, views formed by randomly
splitting the features) for MUC-6.
using the model-theoretic MUC scoring program
(Vilain et al., 1995). The baseline coreference sys-
tem, which is trained only on the initially labeled
data using all of the features, achieves an F-measure
of 51.6 (NB) and 28.7 (DL) on the MUC-6 data set
and 40.1 (NB) and 45.8 (DL) on MUC-7.
The results shown in row 2 of Table 2 correspond
to the best F-measure scores achieved by co-training
across all of the parameter combinations described
in the previous subsection. In comparison to the
baseline, co-training is able to improve system per-
formance in only two of the four classifier/data set
combinations: F-measure increases by 2% and 6%
for MUC-6/DL and MUC-7/NB, respectively. Nev-
ertheless, co-training produces high-precision clas-
sifiers in all four cases (at the expense of recall). In
practical applications in which precision is critical,
the co-training classifiers may be preferable to the
baseline classifiers despite the fact that they achieve
similar F-measure scores.
</bodyText>
<figureCaption confidence="0.5267525">
Figure 1 depicts the learning curve for the co-
training run that gives rise to the best F-measure for
</figureCaption>
<figure confidence="0.919466666666667">
Score
90
200
</figure>
<bodyText confidence="0.998659333333333">
the MUC-6 data set using naive Bayes. The hor-
izontal (dotted) line shows the performance of the
baseline system, as described above. As co-training
progresses, F-measure rises to 48.7 at iteration ten
and gradually drops to and stabilizes at 42.9. We ob-
serve similar performance trends for the other clas-
sifier/data set combinations. The drop in F-measure
is potentially due to the pollution of the labeled data
by mislabeled instances (Pierce and Cardie, 2001).
</bodyText>
<sectionHeader confidence="0.995011" genericHeader="method">
5 Single-View Bootstrapping
</sectionHeader>
<bodyText confidence="0.9999246">
In this section, we describe and evaluate our single-
view, multi-learner bootstrapping algorithm, which
combines ideas from Goldman and Zhou (2000) and
Steedman et al. (2003b). We will start by giving an
overview of these two co-training algorithms.
</bodyText>
<sectionHeader confidence="0.9259" genericHeader="method">
5.1 Related Work
</sectionHeader>
<bodyText confidence="0.997964935483871">
The Goldman and Zhou (G&amp;Z) Algorithm.
This single-view algorithm begins by training two
classifiers on the initially labeled data using two
different learning algorithms; it requires that each
classifier partition the instance space into a set of
equivalence classes (e.g. in a decision tree, each leaf
node defines an equivalence class). Each classi-
fier then considers each equivalence class and uses
hypothesis testing to determine if adding all unla-
beled instances within the equivalence class to the
other classifier’s labeled data will improve the per-
formance of its counterparts. The process is then
repeated until no more instances can be labeled.
The Steedman et al. (Ste) Algorithm. This algo-
rithm is a variation of B&amp;M applied to two diverse
statistical parsers. Initially, each parser is trained on
the labeled data. Each then parses and scores all
sentences in the data pool, and then adds the most
confidently parsed sentences to the training data of
the other parser. The parsers are retrained, and the
process is repeated for several iterations.
The algorithm differs from B&amp;M in three main
respects. First, the training data of the two parsers
diverge after the first co-training iteration. Second,
the data pool is flushed and refilled entirely with in-
stances from the unlabeled data after each iteration.
This reduces the possibility of having unreliably la-
beled sentences accumulating in the pool. Finally,
the two parsers, each of which is assumed to hold a
unique “view” of the data, are effectively two differ-
ent learning algorithms.
</bodyText>
<subsectionHeader confidence="0.998728">
5.2 Our Single-View Bootstrapping Algorithm
</subsectionHeader>
<bodyText confidence="0.999996363636364">
As mentioned before, our algorithm uses two dif-
ferent learning algorithms to train two classifiers on
the same set of features (i.e. the full feature set).
At each bootstrapping iteration, each classifier la-
bels and scores all instances in the data pool. The
highest scored instances labeled by one classifier are
added to the training data of the other classifier and
vice versa. Since the two classifiers are trained on
the same view, it is important to maintain a separate
training set for each classifier: this reduces the prob-
ability that the two classifiers converge to the same
hypothesis at an early stage and hence implicitly in-
creases the ability to bootstrap. Like Ste, the entire
data pool is replenished with instances drawn from
the unlabeled data after each iteration, and the pro-
cess is repeated. So our algorithm is effectively Ste
applied to coreference resolution — instead of two
parsing algorithms that correspond to different fea-
tures, we use two learning algorithms, each of which
relies on the same set of features as in G&amp;Z. The
similarities and differences among B&amp;M, G&amp;Z, Ste,
and our algorithm are summarized in Table 3.
</bodyText>
<subsectionHeader confidence="0.841612">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9994343">
We tested different pool sizes and growth sizes as
specified in section 4.2 to determine the best pa-
rameter setting for our algorithm. For both data
sets, the best F-measure score is achieved using a
pool size of 5000 and a growth size of 50. The re-
sults under this parameter setting are given in row
3 of Table 2. In comparison to the baseline, we see
dramatic improvement in F-measure for both clas-
sifiers and both data sets. In addition, we see si-
multaneous gains in recall and precision in all cases
except MUC-7/DL. Furthermore, single-view boot-
strapping beats co-training (in terms of F-measure
scores) by a large margin in all four cases. These
results provide suggestive evidence that single-view,
multi-learner bootstrapping might be a better alter-
native to its multi-view, single-learner counterparts
for coreference resolution.
The bootstrapping run that corresponds to this pa-
rameter setting for the MUC-6 data set using naive
Bayes is shown in Figure 2. Again, we see a “typi-
</bodyText>
<table confidence="0.93508975">
Blum and Mitchell Goldman and Zhou Steedman et al. Ours
Bootstrapping basis Use different views Use different learners Use different parsers Use different learners
Number of instances Fixed Variable Fixed Fixed
added per iteration
Training sets for the Same Different Different Different
two learners/parsers
Data pool flushed af- No N/A (No data pool is Yes Yes
ter each iteration used)
Example selection Highest scored in- Instances in all Highest scored sen- Highest scored in-
method stances equivalance classes tences stances
that are expected to
improve a classifier
</table>
<tableCaption confidence="0.962035">
Table 3: Summary of the major similarities and differences among four bootstrapping schemes: Blum and
Mitchell, Goldman and Zhou, Steedman et al., and ours. Only the relevant dimensions are discussed here.
</tableCaption>
<figure confidence="0.797786">
Number of Co−Training Iterations
</figure>
<figureCaption confidence="0.713301666666667">
Figure 2: Learning curve for our single-view boot-
strapping algorithm (pool size = 5000, growth size =
50) for MUC-6.
</figureCaption>
<bodyText confidence="0.999845976190476">
cal” bootstrapping curve: an initial rise in F-measure
followed by a gradual deterioration. In comparison
to Figure 1, the recall level achieved by co-training is
much lower than that of single-view bootstrapping.
This appears to indicate that each co-training view is
insufficient for learning the target concept: the fea-
ture split limits any interaction of features that can
produce better recall.
Finally, Figure 2 shows that performance in-
creases most rapidly in the first 200 iterations. This
provides indirect evidence that the two classifiers
have acquired different hypotheses from the ini-
tial data and are exchanging information with each
other. To ensure that the classifiers are indeed bene-
fiting from each other, we conducted a self-training
experiment for each classifier separately: at each
self-training iteration, each classifier labels all 5000
instances in the data pool using all available features
and selects the most confidently labeled 50 instances
for addition to its labeled data.5 The best F-measure
scores achieved by self-training are shown in the last
row of Table 2. Overall, self-training only yields
marginal performance gains over the baseline.
Nevertheless, self-training outperforms co-
training in both cases where naive Bayes is used.
While these results seem to suggest that co-training
is inherently handicapped for coreference resolu-
tion, there are two plausible explanations against
this conclusion. First, the fact that self-training has
access to all of the available features may account
for its superior performance to co-training. This is
again partially supported by the fact that the recall
level achieved by co-training is lower than that of
self-training in both cases in which self-training
outperforms co-training. Second, 1000 instances
may simply not be sufficient for co-training to be
effective for this task: in related work (Ng and
Cardie, 2003), we find that starting with 3500–3700
labeled instances instead of 1000 allows co-training
to improve the baseline by 4.6% and 9.5% in
F-measure using naive Bayes classifiers for the
MUC-6 and MUC-7 data sets, respectively.
</bodyText>
<sectionHeader confidence="0.947675" genericHeader="method">
6 An Alternative Ranking Method
</sectionHeader>
<bodyText confidence="0.99972425">
As we have seen before, F-measure scores ulti-
mately decrease as bootstrapping progresses. If the
drop were caused by the degradation in the quality
of the bootstrapped data, then a more “conservative”
instance selection method than that of B&amp;M would
help alleviate this problem. Our hypothesis is that
selection methods that are based solely on the con-
fidence assigned to an instance by a single classifier
</bodyText>
<footnote confidence="0.7624445">
5Note that this is self-training without bagging, unlike the
self-training algorithm discussed in Ng and Cardie (2003).
</footnote>
<figure confidence="0.940542863636364">
Score
85
80
75
70
65
60
55
50
45
40
0
500 1000 1500 2000
2500 3000 3500 4000
Baseline
Recall
Precision
F−measure
i1 &gt; i2 if any of the following is true:
[µ(C1(i1)) = µ(C2(i1))] n [µ(C1(i2)) =� µ(C2(i2))]
[µ(C1(i1)) = µ(C2(i1))] n [µ(C1(i2)) = µ(C2(i2))] n [|C1(i1) − C2(i1) |&gt; |C1(i2) − C2(i2)|]
[µ(C1(i1)) =� µ(C2(i1))] n [µ(C1(i2)) =� µ(C2(i2))] n [max(C1(i1),1 − C1(i1)) &gt; max(C1(i2),1 − C1(i2))]
</figure>
<figureCaption confidence="0.945768333333333">
Figure 3: The ranking method that a binary classifier C1 uses to impose a partial ordering on the instances
to be selected and added to the training set of binary classifier C2. i1 and i2 are arbitrary instances, and p is
a function that rounds a number to its closest integer.
</figureCaption>
<bodyText confidence="0.965217771428572">
may be too liberal. In particular, these methods al-
low the addition of instances with opposing labels
to the labeled data; this can potentially result in in-
creased incompatibility between the classifiers.
Consequently, we develop a new procedure for
ranking instances in the data pool. The bootstrap-
ping algorithm then selects the highest ranked in-
stances to add to the labeled data in each iteration.
The method favors instances whose label is agreed
upon by both classifiers (Preference 1). However,
incorporating instances that are confidently labeled
by both classifiers may reduce the probability of
acquiring new information from the data. There-
fore, the method imposes an additional preference
for instances that are confidently labeled by one but
not both (Preference 2). If none of the instances
receives the same label from the classifiers, the
method resorts to the “rank-by-confidence” method
used by B&amp;M (Preference 3).
More formally, define a binary classifier as a func-
tion that maps an instance to a value that indicates
the probability that it is labeled as positive. Now,
let p be a function that rounds a number to its near-
est integer. Given two binary classifiers C1 and C2
and instances i1 and i2, the ranking method shown in
Figure 3 uses the three preferences described above
to impose a partial ordering on the given instances
for incorporation into C2’s labeled data. The method
similarly ranks instances to be added to C1’s labeled
data, with the roles of C1 and C2 reversed.
Steedman et al. (2003a) also investigate instance
selection methods for co-training, but their goal is
primarily to use selection methods as a means to
explore the trade-off between maximizing coverage
and maximizing accuracy.6 In contrast, our focus
</bodyText>
<figure confidence="0.535176">
Number of Co−Training Iterations
</figure>
<figureCaption confidence="0.810933">
Figure 4: F-measure curves for our single-view
</figureCaption>
<bodyText confidence="0.954769238095238">
bootstrapping algorithm with different ranking
methods (pool size = 5000, growth size = 50) for
MUC-6.
here is on examining whether a more conservative
ranking method can alleviate the problem of perfor-
mance deterioration. Nevertheless, Preference 2 is
inspired by their Sint-n selection method, which se-
lects an instance if it belongs to the intersection of
the set of the n percent highest scoring instances
of one classifier and the set of the n percent lowest
scoring instances of the other. To our knowledge, no
previous work has examined a ranking method that
combines the three preferences described above.
To compare our ranking procedure with B&amp;M’s
rank-by-confidence method, we repeat the boot-
strapping experiment shown in Figure 2 except that
we replace B&amp;M’s ranking method with ours. The
learning curves generated using the two ranking
methods with naive Bayes for the MUC-6 data set
are shown in Figure 4. The results are consistent
with our intuition regarding the two ranking meth-
</bodyText>
<figure confidence="0.991312">
F−measure
64
62
60
58
56
54
52
50
48
46
44
0
500 1000 1500 2000
2500 3000 3500 4000
Baseline
Using the B&amp;M ranking method
Using our ranking method
</figure>
<footnote confidence="0.388833">
6McCallum and Nigam (1998) tackle this idea of balancing
</footnote>
<bodyText confidence="0.996798833333333">
accuracy and coverage by combining EM and active learning.
ods. The B&amp;M ranking method is more liberal.
In particular, each classifier always selects the most
confidently labeled instances to add to the other’s la-
beled data at each iteration. If the underlying learn-
ers have indeed induced two different hypotheses
from the data, then each classifier can potentially ac-
quire informative instances from the other and yield
performance improvements very rapidly.
In contrast, our ranking method is more conserva-
tive in that it places more emphasis on maintaining
labeled data accuracy than the B&amp;M method. As
a result, the classifier learns at a slower rate when
compared to that in the B&amp;M case: it is not until iter-
ation 600 that we see a sharp rise in F-measure. Due
to the “liberal” nature of the B&amp;M method, however,
its performance drops dramatically as bootstrapping
progresses, whereas ours just dips temporarily. This
can potentially be attributed to the more rapid injec-
tion of mislabeled instances into the labeled data in
the B&amp;M case. At iteration 2800, our method starts
to outperform B&amp;M’s. Overall, our ranking method
does not exhibit the performance trend observed
with the B&amp;M method: except for the spike between
iterations 0 and 100, F-measure does not deteriorate
as bootstrapping progresses. Since it is hard to deter-
mine a “good” stopping point for bootstrapping due
to the paucity of labeled data in a weakly supervised
setting, our ranking method can potentially serve as
an alternative to the B&amp;M method.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999959">
We have proposed a single-view, multi-learner boot-
strapping algorithm for coreference resolution and
shown empirically that the algorithm is a better al-
ternative to the Blum and Mitchell co-training al-
gorithm for this task for which no natural feature
split has been found. In addition, we have investi-
gated an example ranking method for bootstrapping
that, unlike Blum and Mitchell’s rank-by-confidence
method, can potentially alleviate the problem of per-
formance deterioration due to the pollution of the la-
beled data in the course of bootstrapping.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999783333333333">
We thank the anonymous reviewers for their invalu-
able and insightful comments. This work was sup-
ported in part by NSF Grant IIS–0208028.
</bodyText>
<sectionHeader confidence="0.996413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999733">
Michele Banko and Eric Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proceed-
ings of the ACL/EACL, pages 26–33.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of COLT,
pages 92–100.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings of
EMNLP/VLC, pages 100–110.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings ofICML, pages
327–334.
Andrew McCallum and Kamal Nigam. 1998. Employing EM
and pool-based active learning for text classification. In Pro-
ceedings ofICML, pages 359–367.
MUC-6. 1995. Proceedings of the Sixth Message Understand-
ing Conference (MUC-6).
MUC-7. 1998. Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Christoph Mueller, Stefan Rapp, and Michael Strube. 2002.
Applying co-training to reference resolution. In Proceedings
of the ACL, pages 352–359.
Ion Muslea, Steven Minton, and Craig Knoblock. 2002. Active
+ Semi-Supervised Learning = Robust Multi-View Learning.
In Proceedings ofICML.
Vincent Ng and Claire Cardie. 2002. Combining sample selec-
tion and error-driven pruning for machine learning of coref-
erence rules. In Proceedings ofEMNLP, pages 55–62.
Vincent Ng and Claire Cardie. 2003. Weakly supervised natu-
ral language learning without redundant views. In Proceed-
ings ofHLT-NAACL.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings of
CIKM, pages 86–93.
Kamal Nigam, Andrew McCallum, Sabastian Thrun, and
Tom Mitchell. 2000. Text classification from labeled
and unlabeled documents using EM. Machine Learning,
39(2/3):103–134.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings ofEMNLP, pages 1–9.
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping. In Pro-
ceedings ofAAAI, pages 474–479.
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003a.
Example selection for bootstrapping statistical parsers. In
Proceedings of HLT-NAACL.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003b.
Bootstrapping statistical parsers from small datasets. In Pro-
ceedings of the EACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings ofthe Sixth MessageUnderstanding
Conference (MUC-6), pages 45–52.
David Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proceedingsofthe ACL,
pages 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867682">
<title confidence="0.9990765">Bootstrapping Coreference Classifiers Multiple Machine Learning Algorithms</title>
<author confidence="0.980732">Ng</author>
<affiliation confidence="0.959421">Department of Computer Cornell</affiliation>
<address confidence="0.977716">Ithaca, NY</address>
<abstract confidence="0.999265684210526">Successful application of multi-view cotraining algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated. This can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split. To bootstrap coreference classifiers, we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training. In addition, we investigate a method for ranking unlabeled instances to be fed back into the bootstrapping loop as labeled data, aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL/EACL,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="3850" citStr="Banko and Brill, 2001" startWordPosition="581" endWordPosition="584">ely be viewed as two different learning algorithms. provided by conditionally independent feature splits in the Blum and Mitchell algorithm. The goal of this paper is two-fold. First, we propose a single-view algorithm for bootstrapping coreference classifiers. Like anaphora resolution, noun phrase coreference resolution is a problem for which a natural feature split is not readily available. In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms — self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) — on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterioration due to the degradation in the quali</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the ACL/EACL, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="1014" citStr="Blum and Mitchell, 1998" startWordPosition="141" endWordPosition="144"> potentially preclude their use on problems such as coreference resolution that lack an obvious feature split. To bootstrap coreference classifiers, we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training. In addition, we investigate a method for ranking unlabeled instances to be fed back into the bootstrapping loop as labeled data, aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping. 1 Introduction Co-training (Blum and Mitchell, 1998) is a weakly supervised paradigm that learns a task from a small set of labeled data and a large pool of unlabeled data using separate, but redundant views of the data (i.e. using disjoint feature subsets to represent the data). To ensure provable performance guarantees, the co-training algorithm assumes as input a set of views that satisfies two fairly strict conditions. First, each view must be sufficient for learning the target concept. Second, the views must be conditionally independent of each other given the class. Empirical results on artificial data sets by Muslea et al. (2002) and Nig</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="1951" citStr="Collins and Singer, 1999" startWordPosition="291" endWordPosition="294">ut a set of views that satisfies two fairly strict conditions. First, each view must be sufficient for learning the target concept. Second, the views must be conditionally independent of each other given the class. Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training.1 The intuition is that the two learning algorithms can potentially substitute for the two views: different lear</context>
<context position="7226" citStr="Collins and Singer (1999)" startWordPosition="1124" endWordPosition="1127">sifier is a generative classifier that assigns to a test instance i with feature values &lt;x1, ..., xm&gt; the maximum a posteriori (MAP) label y*, which is determined as follows: y* = arg max y = arg max y = arg max y The first equality above follows from the definition of MAP, the second one from Bayes rule, and the last one from the conditional independence assumption of the feature values. We determine the class priors P(y) and the class densities P(xi I y) directly from the training data using add-one smoothing. 3.2 Decision Lists Our decision list (DL) algorithm is based on that described in Collins and Singer (1999). For each available feature fi and each possible value vj of fi in the training data, the learner induces an element of the P(y |i) P(y)P(i I y) m P(y) P(xi I y) i =1 Observations Justifications Many feature-value pairs alone can de- Decision lists draw a decision boundary based on a single feature-value pair termine the class value.3 For example, and can take advantage of this observation directly. On the other hand, naive two NPs cannot be coreferent if they differ Bayes classifiers make a decision based on a combination of features and in gender or semantic class. thus cannot take advantag</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP/VLC, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Goldman</author>
<author>Yan Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>327--334</pages>
<contexts>
<context position="2309" citStr="Goldman and Zhou (2000)" startWordPosition="348" endWordPosition="351">ptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training.1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instanc</context>
<context position="16709" citStr="Goldman and Zhou (2000)" startWordPosition="2667" endWordPosition="2670"> set using naive Bayes. The horizontal (dotted) line shows the performance of the baseline system, as described above. As co-training progresses, F-measure rises to 48.7 at iteration ten and gradually drops to and stabilizes at 42.9. We observe similar performance trends for the other classifier/data set combinations. The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). 5 Single-View Bootstrapping In this section, we describe and evaluate our singleview, multi-learner bootstrapping algorithm, which combines ideas from Goldman and Zhou (2000) and Steedman et al. (2003b). We will start by giving an overview of these two co-training algorithms. 5.1 Related Work The Goldman and Zhou (G&amp;Z) Algorithm. This single-view algorithm begins by training two classifiers on the initially labeled data using two different learning algorithms; it requires that each classifier partition the instance space into a set of equivalence classes (e.g. in a decision tree, each leaf node defines an equivalence class). Each classifier then considers each equivalence class and uses hypothesis testing to determine if adding all unlabeled instances within the e</context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>Sally Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. In Proceedings ofICML, pages 327–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing EM and pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="27886" citStr="McCallum and Nigam (1998)" startWordPosition="4468" endWordPosition="4471">a ranking method that combines the three preferences described above. To compare our ranking procedure with B&amp;M’s rank-by-confidence method, we repeat the bootstrapping experiment shown in Figure 2 except that we replace B&amp;M’s ranking method with ours. The learning curves generated using the two ranking methods with naive Bayes for the MUC-6 data set are shown in Figure 4. The results are consistent with our intuition regarding the two ranking methF−measure 64 62 60 58 56 54 52 50 48 46 44 0 500 1000 1500 2000 2500 3000 3500 4000 Baseline Using the B&amp;M ranking method Using our ranking method 6McCallum and Nigam (1998) tackle this idea of balancing accuracy and coverage by combining EM and active learning. ods. The B&amp;M ranking method is more liberal. In particular, each classifier always selects the most confidently labeled instances to add to the other’s labeled data at each iteration. If the underlying learners have indeed induced two different hypotheses from the data, then each classifier can potentially acquire informative instances from the other and yield performance improvements very rapidly. In contrast, our ranking method is more conservative in that it places more emphasis on maintaining labeled </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. Employing EM and pool-based active learning for text classification. In Proceedings ofICML, pages 359–367.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<contexts>
<context position="5494" citStr="(1995)" startWordPosition="845" endWordPosition="845">andard coreference data sets. Finally, we present experimental results that suggest that our method for ranking instances is more resistant to performance deterioration in the bootstrapping process than Blum and Mitchell’s “rank-by-confidence” method. 2 Noun Phrase Coreference Resolution Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.2 In this section, we give an overview of the coreference resolution system to which the boot2Concrete examples of the coreference task can be found in MUC-6 (1995) and MUC-7 (1998). strapping algorithms will be applied. The framework underlying the coreference system is a standard combination of classification and clustering (see Ng and Cardie (2002) for details). Coreference resolution is first recast as a classification task, in which a pair of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs. When the system operates within the weakly supervised setting</context>
<context position="10831" citStr="(1995)" startWordPosition="1714" endWordPosition="1714">able 1 provides the justifications for choosing these two learners as components in our single-view, multi-learner bootstrapping algorithm. Based on observations of the coreference task and the features employed by our coreference system, the justifications suggest that the two learners can potentially compensate for each other’s weaknesses. 4 Multi-View Co-Training In this section, we describe the Blum and Mitchell (B&amp;M) multi-view co-training algorithm and apply it to coreference resolution. 3This justifies the use of a decision list as a potential classifier for bootstrapping. See Yarowsky (1995) for details. 4.1 The Multi-View Co-Training Algorithm The intuition behind the B&amp;M co-training algorithm is to train two classifiers that can help augment each other’s labeled data by exploiting two separate but redundant views of the data. Specifically, each classifier is trained using one view of the labeled data and predicts labels for all instances in the data pool, which consists of a randomly chosen subset of the unlabeled data. Each then selects its most confident predictions, and adds the corresponding instances with their predicted labels to the labeled data while maintaining the cla</context>
<context position="13127" citStr="(1995)" startWordPosition="2092" endWordPosition="2092">76.3 46.0 32.4 78.3 45.8 Single-view Bootstrapping 53.6 79.0 63.9 40.1 83.1 54.1 43.5 73.2 54.6 38.3 75.4 50.8 Self-Training 48.3 63.5 54.9 18.7 70.8 29.6 40.1 40.2 40.1 32.9 78.1 46.3 Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 2 on the 20–30 </context>
</contexts>
<marker>1995</marker>
<rawString>MUC-6. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Mueller</author>
<author>Stefan Rapp</author>
<author>Michael Strube</author>
</authors>
<title>Applying co-training to reference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>352--359</pages>
<contexts>
<context position="2160" citStr="Mueller et al., 2002" startWordPosition="327" endWordPosition="330">. Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training.1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and</context>
</contexts>
<marker>Mueller, Rapp, Strube, 2002</marker>
<rawString>Christoph Mueller, Stefan Rapp, and Michael Strube. 2002. Applying co-training to reference resolution. In Proceedings of the ACL, pages 352–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Muslea</author>
<author>Steven Minton</author>
<author>Craig Knoblock</author>
</authors>
<title>Active + Semi-Supervised Learning = Robust Multi-View Learning.</title>
<date>2002</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="1606" citStr="Muslea et al. (2002)" startWordPosition="240" endWordPosition="243">ng (Blum and Mitchell, 1998) is a weakly supervised paradigm that learns a task from a small set of labeled data and a large pool of unlabeled data using separate, but redundant views of the data (i.e. using disjoint feature subsets to represent the data). To ensure provable performance guarantees, the co-training algorithm assumes as input a set of views that satisfies two fairly strict conditions. First, each view must be sufficient for learning the target concept. Second, the views must be conditionally independent of each other given the class. Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to inve</context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2002</marker>
<rawString>Ion Muslea, Steven Minton, and Craig Knoblock. 2002. Active + Semi-Supervised Learning = Robust Multi-View Learning. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Combining sample selection and error-driven pruning for machine learning of coreference rules.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="5683" citStr="Ng and Cardie (2002)" startWordPosition="870" endWordPosition="873"> bootstrapping process than Blum and Mitchell’s “rank-by-confidence” method. 2 Noun Phrase Coreference Resolution Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.2 In this section, we give an overview of the coreference resolution system to which the boot2Concrete examples of the coreference task can be found in MUC-6 (1995) and MUC-7 (1998). strapping algorithms will be applied. The framework underlying the coreference system is a standard combination of classification and clustering (see Ng and Cardie (2002) for details). Coreference resolution is first recast as a classification task, in which a pair of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs. When the system operates within the weakly supervised setting, a weakly supervised algorithm bootstraps the coreference classifier from the given labeled and unlabeled data rather than from a much larger set of labeled instances. The clustering algor</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Combining sample selection and error-driven pruning for machine learning of coreference rules. In Proceedings ofEMNLP, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="3661" citStr="Ng and Cardie, 2003" startWordPosition="551" endWordPosition="554">features, thus retaining in spirit the advantages 1Steedman et al. (2003b) bootstrap two parsers that use different statistical models via co-training. Hence, the two parsers can effectively be viewed as two different learning algorithms. provided by conditionally independent feature splits in the Blum and Mitchell algorithm. The goal of this paper is two-fold. First, we propose a single-view algorithm for bootstrapping coreference classifiers. Like anaphora resolution, noun phrase coreference resolution is a problem for which a natural feature split is not readily available. In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms — self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) — on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled </context>
<context position="13371" citStr="Ng and Cardie (2003)" startWordPosition="2132" endWordPosition="2135">ingle-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 2 on the 20–30 “formal evaluation” texts for each of the MUC-6 and MUC-7 data sets. Two sets of experiments are conducted, one using naive Bayes as the underlying supervised learning algorithm and the other the decision list learner. All results reported are </context>
<context position="14692" citStr="Ng and Cardie (2003)" startWordPosition="2341" endWordPosition="2344">ws. We used three methods to generate the views from the 25 features used by the coreference system: Mueller et al.’s (2002) greedy method, random splitting of features into views, and splitting of features according to the feature type (i.e. lexicosyntactic vs. non-lexico-syntactic features).4 Pool size. We tested values of 500, 1000, 5000. Growth size. We tested values of 10, 50, 100, 200. 4.3 Results and Discussion Results are shown in Table 2, where performance is reported in terms of recall, precision, and F-measure 4Space limitation precludes a detailed description of these methods. See Ng and Cardie (2003) for details. 100 Baseline Recall Precision F−measure 80 70 60 50 40 30 50 100 150 200 250 300 350 400 450 500 Number of Co−Training Iterations Figure 1: Learning curve for co-training (pool size = 500, growth size = 50, views formed by randomly splitting the features) for MUC-6. using the model-theoretic MUC scoring program (Vilain et al., 1995). The baseline coreference system, which is trained only on the initially labeled data using all of the features, achieves an F-measure of 51.6 (NB) and 28.7 (DL) on the MUC-6 data set and 40.1 (NB) and 45.8 (DL) on MUC-7. The results shown in row 2 of</context>
<context position="23421" citStr="Ng and Cardie, 2003" startWordPosition="3732" endWordPosition="3735"> results seem to suggest that co-training is inherently handicapped for coreference resolution, there are two plausible explanations against this conclusion. First, the fact that self-training has access to all of the available features may account for its superior performance to co-training. This is again partially supported by the fact that the recall level achieved by co-training is lower than that of self-training in both cases in which self-training outperforms co-training. Second, 1000 instances may simply not be sufficient for co-training to be effective for this task: in related work (Ng and Cardie, 2003), we find that starting with 3500–3700 labeled instances instead of 1000 allows co-training to improve the baseline by 4.6% and 9.5% in F-measure using naive Bayes classifiers for the MUC-6 and MUC-7 data sets, respectively. 6 An Alternative Ranking Method As we have seen before, F-measure scores ultimately decrease as bootstrapping progresses. If the drop were caused by the degradation in the quality of the bootstrapped data, then a more “conservative” instance selection method than that of B&amp;M would help alleviate this problem. Our hypothesis is that selection methods that are based solely o</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincent Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="1633" citStr="Nigam and Ghani (2000)" startWordPosition="245" endWordPosition="248">98) is a weakly supervised paradigm that learns a task from a small set of labeled data and a large pool of unlabeled data using separate, but redundant views of the data (i.e. using disjoint feature subsets to represent the data). To ensure provable performance guarantees, the co-training algorithm assumes as input a set of views that satisfies two fairly strict conditions. First, each view must be sufficient for learning the target concept. Second, the views must be conditionally independent of each other given the class. Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedu</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proceedings of CIKM, pages 86–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sabastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="3878" citStr="Nigam et al., 2000" startWordPosition="587" endWordPosition="590">learning algorithms. provided by conditionally independent feature splits in the Blum and Mitchell algorithm. The goal of this paper is two-fold. First, we propose a single-view algorithm for bootstrapping coreference classifiers. Like anaphora resolution, noun phrase coreference resolution is a problem for which a natural feature split is not readily available. In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms — self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) — on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterioration due to the degradation in the quality of the labeled data as bo</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sabastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of cotraining for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="4525" citStr="Pierce and Cardie, 2001" startWordPosition="694" endWordPosition="697">tion, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterioration due to the degradation in the quality of the labeled data as bootstrapping progresses (Pierce and Cardie, 2001; Riloff and Jones, 1999). In a set of baseline experiments, we first demonstrate that multi-view co-training fails to boost the performance of the coreference system under various parameter settings. We then show that our single-view weakly supervised algorithm successfully bootstraps the coreference classifiers, boosting the F-measure score by 9-12% on two standard coreference data sets. Finally, we present experimental results that suggest that our method for ranking instances is more resistant to performance deterioration in the bootstrapping process than Blum and Mitchell’s “rank-by-confi</context>
<context position="13008" citStr="Pierce and Cardie, 2001" startWordPosition="2072" endWordPosition="2075">P F R P F Baseline 50.7 52.6 51.6 17.9 72.0 28.7 40.1 40.2 40.1 32.4 78.3 45.8 Multi-view Co-Training 33.3 90.7 48.7 19.5 71.2 30.6 32.9 76.3 46.0 32.4 78.3 45.8 Single-view Bootstrapping 53.6 79.0 63.9 40.1 83.1 54.1 43.5 73.2 54.6 38.3 75.4 50.8 Self-Training 48.3 63.5 54.9 18.7 70.8 29.6 40.1 40.2 40.1 32.9 78.1 46.3 Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is performe</context>
<context position="16533" citStr="Pierce and Cardie, 2001" startWordPosition="2643" endWordPosition="2646">fact that they achieve similar F-measure scores. Figure 1 depicts the learning curve for the cotraining run that gives rise to the best F-measure for Score 90 200 the MUC-6 data set using naive Bayes. The horizontal (dotted) line shows the performance of the baseline system, as described above. As co-training progresses, F-measure rises to 48.7 at iteration ten and gradually drops to and stabilizes at 42.9. We observe similar performance trends for the other classifier/data set combinations. The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). 5 Single-View Bootstrapping In this section, we describe and evaluate our singleview, multi-learner bootstrapping algorithm, which combines ideas from Goldman and Zhou (2000) and Steedman et al. (2003b). We will start by giving an overview of these two co-training algorithms. 5.1 Related Work The Goldman and Zhou (G&amp;Z) Algorithm. This single-view algorithm begins by training two classifiers on the initially labeled data using two different learning algorithms; it requires that each classifier partition the instance space into a set of equivalence classes (e.g. in a decision tree, each leaf n</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of cotraining for natural language learning from large datasets. In Proceedings ofEMNLP, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>474--479</pages>
<contexts>
<context position="4550" citStr="Riloff and Jones, 1999" startWordPosition="698" endWordPosition="701">e-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterioration due to the degradation in the quality of the labeled data as bootstrapping progresses (Pierce and Cardie, 2001; Riloff and Jones, 1999). In a set of baseline experiments, we first demonstrate that multi-view co-training fails to boost the performance of the coreference system under various parameter settings. We then show that our single-view weakly supervised algorithm successfully bootstraps the coreference classifiers, boosting the F-measure score by 9-12% on two standard coreference data sets. Finally, we present experimental results that suggest that our method for ranking instances is more resistant to performance deterioration in the bootstrapping process than Blum and Mitchell’s “rank-by-confidence” method. 2 Noun Phr</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings ofAAAI, pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>R Hwa</author>
<author>S Clark</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Example selection for bootstrapping statistical parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2335" citStr="Steedman et al. (2003" startWordPosition="353" endWordPosition="356"> algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training.1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled d</context>
<context position="4241" citStr="Steedman et al. (2003" startWordPosition="649" endWordPosition="652">le. In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms — self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) — on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterioration due to the degradation in the quality of the labeled data as bootstrapping progresses (Pierce and Cardie, 2001; Riloff and Jones, 1999). In a set of baseline experiments, we first demonstrate that multi-view co-training fails to boost the performance of the coreference system under various parameter settings. We then show that our single-view weakly supervised algorithm successfully bootstraps the coreference classifiers, </context>
<context position="16735" citStr="Steedman et al. (2003" startWordPosition="2672" endWordPosition="2675">horizontal (dotted) line shows the performance of the baseline system, as described above. As co-training progresses, F-measure rises to 48.7 at iteration ten and gradually drops to and stabilizes at 42.9. We observe similar performance trends for the other classifier/data set combinations. The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). 5 Single-View Bootstrapping In this section, we describe and evaluate our singleview, multi-learner bootstrapping algorithm, which combines ideas from Goldman and Zhou (2000) and Steedman et al. (2003b). We will start by giving an overview of these two co-training algorithms. 5.1 Related Work The Goldman and Zhou (G&amp;Z) Algorithm. This single-view algorithm begins by training two classifiers on the initially labeled data using two different learning algorithms; it requires that each classifier partition the instance space into a set of equivalence classes (e.g. in a decision tree, each leaf node defines an equivalence class). Each classifier then considers each equivalence class and uses hypothesis testing to determine if adding all unlabeled instances within the equivalence class to the ot</context>
<context position="26407" citStr="Steedman et al. (2003" startWordPosition="4228" endWordPosition="4231">od used by B&amp;M (Preference 3). More formally, define a binary classifier as a function that maps an instance to a value that indicates the probability that it is labeled as positive. Now, let p be a function that rounds a number to its nearest integer. Given two binary classifiers C1 and C2 and instances i1 and i2, the ranking method shown in Figure 3 uses the three preferences described above to impose a partial ordering on the given instances for incorporation into C2’s labeled data. The method similarly ranks instances to be added to C1’s labeled data, with the roles of C1 and C2 reversed. Steedman et al. (2003a) also investigate instance selection methods for co-training, but their goal is primarily to use selection methods as a means to explore the trade-off between maximizing coverage and maximizing accuracy.6 In contrast, our focus Number of Co−Training Iterations Figure 4: F-measure curves for our single-view bootstrapping algorithm with different ranking methods (pool size = 5000, growth size = 50) for MUC-6. here is on examining whether a more conservative ranking method can alleviate the problem of performance deterioration. Nevertheless, Preference 2 is inspired by their Sint-n selection me</context>
</contexts>
<marker>Steedman, Hwa, Clark, Osborne, Sarkar, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003a. Example selection for bootstrapping statistical parsers. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>S Clark</author>
<author>R Hwa</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL.</booktitle>
<contexts>
<context position="2335" citStr="Steedman et al. (2003" startWordPosition="353" endWordPosition="356"> algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training.1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled d</context>
<context position="4241" citStr="Steedman et al. (2003" startWordPosition="649" endWordPosition="652">le. In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell cotraining algorithm with that of two existing singleview bootstrapping algorithms — self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) — on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. This paper instead focuses on developing a single-view algorithm that combines aspects of each of the Goldman and Zhou and Steedman et al. algorithms. Second, we investigate a new method that, inspired by Steedman et al. (2003a), ranks unlabeled instances to be added to the labeled data in an attempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterioration due to the degradation in the quality of the labeled data as bootstrapping progresses (Pierce and Cardie, 2001; Riloff and Jones, 1999). In a set of baseline experiments, we first demonstrate that multi-view co-training fails to boost the performance of the coreference system under various parameter settings. We then show that our single-view weakly supervised algorithm successfully bootstraps the coreference classifiers, </context>
<context position="16735" citStr="Steedman et al. (2003" startWordPosition="2672" endWordPosition="2675">horizontal (dotted) line shows the performance of the baseline system, as described above. As co-training progresses, F-measure rises to 48.7 at iteration ten and gradually drops to and stabilizes at 42.9. We observe similar performance trends for the other classifier/data set combinations. The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). 5 Single-View Bootstrapping In this section, we describe and evaluate our singleview, multi-learner bootstrapping algorithm, which combines ideas from Goldman and Zhou (2000) and Steedman et al. (2003b). We will start by giving an overview of these two co-training algorithms. 5.1 Related Work The Goldman and Zhou (G&amp;Z) Algorithm. This single-view algorithm begins by training two classifiers on the initially labeled data using two different learning algorithms; it requires that each classifier partition the instance space into a set of equivalence classes (e.g. in a decision tree, each leaf node defines an equivalence class). Each classifier then considers each equivalence class and uses hypothesis testing to determine if adding all unlabeled instances within the equivalence class to the ot</context>
<context position="26407" citStr="Steedman et al. (2003" startWordPosition="4228" endWordPosition="4231">od used by B&amp;M (Preference 3). More formally, define a binary classifier as a function that maps an instance to a value that indicates the probability that it is labeled as positive. Now, let p be a function that rounds a number to its nearest integer. Given two binary classifiers C1 and C2 and instances i1 and i2, the ranking method shown in Figure 3 uses the three preferences described above to impose a partial ordering on the given instances for incorporation into C2’s labeled data. The method similarly ranks instances to be added to C1’s labeled data, with the roles of C1 and C2 reversed. Steedman et al. (2003a) also investigate instance selection methods for co-training, but their goal is primarily to use selection methods as a means to explore the trade-off between maximizing coverage and maximizing accuracy.6 In contrast, our focus Number of Co−Training Iterations Figure 4: F-measure curves for our single-view bootstrapping algorithm with different ranking methods (pool size = 5000, growth size = 50) for MUC-6. here is on examining whether a more conservative ranking method can alleviate the problem of performance deterioration. Nevertheless, Preference 2 is inspired by their Sint-n selection me</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003b. Bootstrapping statistical parsers from small datasets. In Proceedings of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings ofthe Sixth MessageUnderstanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<contexts>
<context position="15040" citStr="Vilain et al., 1995" startWordPosition="2401" endWordPosition="2404">th size. We tested values of 10, 50, 100, 200. 4.3 Results and Discussion Results are shown in Table 2, where performance is reported in terms of recall, precision, and F-measure 4Space limitation precludes a detailed description of these methods. See Ng and Cardie (2003) for details. 100 Baseline Recall Precision F−measure 80 70 60 50 40 30 50 100 150 200 250 300 350 400 450 500 Number of Co−Training Iterations Figure 1: Learning curve for co-training (pool size = 500, growth size = 50, views formed by randomly splitting the features) for MUC-6. using the model-theoretic MUC scoring program (Vilain et al., 1995). The baseline coreference system, which is trained only on the initially labeled data using all of the features, achieves an F-measure of 51.6 (NB) and 28.7 (DL) on the MUC-6 data set and 40.1 (NB) and 45.8 (DL) on MUC-7. The results shown in row 2 of Table 2 correspond to the best F-measure scores achieved by co-training across all of the parameter combinations described in the previous subsection. In comparison to the baseline, co-training is able to improve system performance in only two of the four classifier/data set combinations: F-measure increases by 2% and 6% for MUC-6/DL and MUC-7/N</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings ofthe Sixth MessageUnderstanding Conference (MUC-6), pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedingsofthe ACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="10831" citStr="Yarowsky (1995)" startWordPosition="1713" endWordPosition="1714">daries. Table 1 provides the justifications for choosing these two learners as components in our single-view, multi-learner bootstrapping algorithm. Based on observations of the coreference task and the features employed by our coreference system, the justifications suggest that the two learners can potentially compensate for each other’s weaknesses. 4 Multi-View Co-Training In this section, we describe the Blum and Mitchell (B&amp;M) multi-view co-training algorithm and apply it to coreference resolution. 3This justifies the use of a decision list as a potential classifier for bootstrapping. See Yarowsky (1995) for details. 4.1 The Multi-View Co-Training Algorithm The intuition behind the B&amp;M co-training algorithm is to train two classifiers that can help augment each other’s labeled data by exploiting two separate but redundant views of the data. Specifically, each classifier is trained using one view of the labeled data and predicts labels for all instances in the data pool, which consists of a randomly chosen subset of the unlabeled data. Each then selects its most confident predictions, and adds the corresponding instances with their predicted labels to the labeled data while maintaining the cla</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedingsofthe ACL, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>