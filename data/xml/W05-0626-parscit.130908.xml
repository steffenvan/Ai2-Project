<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.98198">
Semantic Role Labeling via Consensus in Pattern-Matching
</title>
<author confidence="0.999037">
Chi-San (Althon) Lin Tony C. Smith
</author>
<affiliation confidence="0.974767333333333">
Department of Computer Science Department of Computer Science
Waikato University Waikato University
Hamilton, New Zealand Hamilton, New Zealand
</affiliation>
<email confidence="0.986121">
cl123@cs.waikato.ac.nz tcs@cs.waikato.ac.nz
</email>
<sectionHeader confidence="0.993547" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934615384615">
This paper describes a system for semantic
role labeling for the CoNLL2005 Shared
task. We divide the task into two sub-tasks:
boundary recognition by a general tree-
based predicate-argument recognition algo-
rithm to convert a parse tree into a flat rep-
resentation of all predicates and their
related boundaries, and role labeling by a
consensus model using a pattern-matching
framework to find suitable roles for core
constituents and adjuncts. We describe the
system architecture and report results for
the CoNLL2005 development dataset.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919925925926">
Semantic role labeling is to find all arguments for
all predicates in a sentence, and classify them by
semantic roles such as A0, A1, AM-TMP and so
on. The performance of semantic role labeling can
play a key role in Natural Language Processing
applications, such as Information Extraction, Ques-
tion Answering, and Summarization (Pradhan et al.,
2004).
Most existing systems separate semantic role la-
beling into two sub-problems, boundary recogni-
tion and role classification, and use feature-based
models to address both (Carreras et al., 2004). Our
strategy is to develop a boundary analyzer by a
general tree-based predicate-argument recognition
algorithm (GT-PARA) for boundary recognition,
and a pattern-matching model for role classifica-
tion. The only information used in our system is
Charniak’s annotation with words, which contains
all useful syntactic annotations. Five features,
which are Headword, Phrase type, Voice, Target
verb, and Preposition (of the first word), and a Pat-
tern set, which includes numbers and types of roles
in a pattern, are used for the pattern-matching ap-
proach. We develop a Pattern Database, trained by
Wall Street Journal section 02 to 21, as our knowl-
edge/Data base. The system outline is described in
the following section.
</bodyText>
<sectionHeader confidence="0.986117" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.992508928571429">
An overview of the system architecture is shown in
Figure 1. The input is a full parse tree for each
sentence. We convert a sentence with words, and
Charniak’s information into a parsed tree as the
input of GT-PARA. GT-PARA then converts the
parse tree into a flat representation with all predi-
cates and arguments expressed in [GPLVR] for-
mat; where
G: Grammatical function – 5 denotes subject, 3
object, and 2 others;
P: Phrase type of this boundary – 00 denotes ADJP,
01 ADVP, 02 NP, 03 PP, 04 S, 05 SBAR, 06
SBARQ, 07 SINV, 08 SQ, 09 VP, 10 WHADVP,
11 WHNP, 12 WHPP, and 13 Others
L: Distance (and position) of the argument with
respect to the predicate that follows
V: Voice of the predicate, 0: active 1: passive
R: Distance (and position) of the argument
with respect to the preceding predicate (n.b.
L and R are mutually exclusive).
An example of the output of GT-PARA is
shown in Figure 2. There is one predicate “take”
in the sample input sentence. There are 4 argu-
ments for that predicate, denoted as “302110”,
“AM-MOD”, “203011”, and “302012” respec-
tively. “302110” symbolizes the NP Object of
distance 1 prior to the passive predicate. “203011”
symbolizes an undefined PP argument (which
</bodyText>
<page confidence="0.984814">
185
</page>
<note confidence="0.546108">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 185–188, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998964">
Figure 1: System Architecture
</figureCaption>
<bodyText confidence="0.994750206896552">
means it can be a core argument or an adjunct)
with distance 1 after the passive predicate. And
“302012” symbolizes a NP Object with distance 2
after the passive predicate.
For all boundaries extracted by GT-PARA, we
simply denote all boundaries with noun phrases
(NP) or similar phrases, such as WHNP, SBAR,
and so on, as core pattern candidates and all
boundaries with prepositional phrases (PP), ADJP,
ADVP, or similar phrases, such as WHADJP,
WHADVP, and so on, as adjunct candidates. But
there is no exact rule for defining a core role or an
adjunct explicitly in a boundary span, for example,
given a sentence where
(1) P1 is done by P2. (P1 and P2 are two groups of
words or phrases)
We can guess P1 might be labeled with “A1”, and
P2 with ”A0” if there is no further feature informa-
tion. But if the ”head word” feature of P2 is
“hour”, for example, P2 can be labeled with “AM-
TMP” instead. Because there are some uncertain-
ties between core roles and adjuncts before label-
ing, we use the Member Generator (in Figure 1) to
create all possible combinations, called members,
from the output of GT-PARA by changing ANs
(Core Role Candidates) into AMs (Adjunct Candi-
dates), or AMs into ANs, except core candidates
before predicates. All possible combinations
(members) for the example in Figure 1 are
</bodyText>
<listItem confidence="0.561264555555556">
M1: [AN1, AM-MOD, V, AM1&lt;points&gt;(from), AN2]
(original)
M2: [AN1 AM-MOD V AN3 (from) AN2]
(change AM1 as AN3)
M3: [AN1 AM-MOD V AM1&lt;point&gt;(from)
AM2&lt;week&gt;] (change AN2 as AM2)
M4: [AN1 AM-MOD V AN3&lt;point&gt;(from)
AM2&lt;week&gt;]
(change AM1 as AN3 and one AN2 as AM2)
</listItem>
<bodyText confidence="0.999699">
The output from the Member Generator is
passed to the Role Classifier, which finds all pos-
sible roles for each member with suitable core
</bodyText>
<figure confidence="0.9780292">
Words POS Full Tree Syntax Predicate Boundaries
The DT (S1(S(NP(NP* - (302110*
economy NN * - *
&apos;s POS *) - *
temperature NN *) - *)
will MD (VP * - (AM-MOD*)
be AUX (VP* - *
taken VBN (VP* take (V*V)
from IN (PP* - (203011*
several JJ (NP* - *
vantage NN * - *
points NNS *)) - *)
this DT (NP* - (302012*
week NN *) - *)
. . *)) - *
</figure>
<figureCaption confidence="0.921122333333333">
Figure 2: Illustration of an output of GT-PARA of a sen-
tence, ”The economy ’s temperature will be taken from several
vantage points this week.”
</figureCaption>
<bodyText confidence="0.999929">
roles and adjuncts according to a Database built up
by training data, in which each predicate has dif-
ferent patterns associated with it, each pattern has
different semantic roles, and each role has the fol-
lowing format.
Role {Phrase type} &lt; Head Word&gt; (preposition)
There is an additional Boolean voice for a predi-
cate to show if the predicate is passive or active (0:
denotes active, 1: denotes passive). Each pattern
includes a count on the number of the same pat-
terns learned from the training data (denoted as
“[statistical figure]”). For example, eight patterns
for a predicate lemma “take” are
</bodyText>
<listItem confidence="0.428852315789474">
1. [30] A0{NP}&lt;buyers&gt; V{VP}&lt;take&gt;-0
A1{NP}&lt;stake&gt;
2. [1] A0{NP}&lt;U.S.&gt; V{VP}&lt;take&gt;-0 A1{NP}&lt;%&gt;
A2{PP}&lt;Canada&gt;(from) AM-
ADV{ADVP}&lt;up&gt;(up)
3. [2] A0{NP}&lt;Confidence&gt; V{VP}&lt;take&gt;-0
A1{NP}&lt;dive&gt; AM-ADV{SBAR}&lt; figures&gt;(if)
4. [1] A1{NP}&lt;it&gt; AM-MOD{VP}&lt;could&gt;
V{VP}&lt;take&gt;-0 A2{NP}&lt;place&gt; AM-
TMP{NP}&lt;today&gt; AM-LOC{PP}&lt;Express&gt;(at)
5. [1] AM-TMP{NP}&lt; week&gt; A0{NP}&lt;government&gt;
V{VP}&lt;take&gt;-0 A1{NP}&lt;bills&gt; AM-
DIR{PP}&lt;to&gt;(to)
6. [3] A1{NP}&lt;cells&gt; V{VP}&lt;take&gt;-1
A2{PP}&lt;tissue&gt;(from)
7. [6] A1{NP}&lt;action&gt; V{VP}&lt;take&gt;-1
8. [1] AM-TMP{ADVP}&lt;far&gt; A1{NP}&lt;festivities&gt;
V{VP}&lt;take&gt;-1 AM-EXT{PP}&lt;entirely&gt;
A0{NP}&lt;eating&gt;(by)
</listItem>
<bodyText confidence="0.43158">
Role Classifier consists of two parts, AN classi-
fier and AM classifier, which process core argu-
</bodyText>
<page confidence="0.996088">
186
</page>
<bodyText confidence="0.99984825">
ments and adjuncts respectively. AN classifier
finds a suitable core pattern for labeled core pattern
candidates in each member generated by Member
Generator according to
</bodyText>
<listItem confidence="0.901199272727273">
(1) the same numbers of core roles
(2) the same prepositions for each core role
(3) the same phrase types for each core role
(4) the same voice (active or passive)
AM classifier finds a suitable adjunct role for
any labeled adjunct candidate in each member
generated by Member Generator according to
(1) the same Head Word
(2) the same Phrase type
(3) the highest statistical probability learned from
the training data
</listItem>
<bodyText confidence="0.590967">
The followings are the results for each member
after Role Classification
</bodyText>
<listItem confidence="0.865765111111111">
M1: [AN1, AM-MOD, V, AM1&lt;points&gt;(from), AN2]
(no pattern applied)
M2: [AN1 AM-MOD V AN1 (from) AN2] (no pattern
applied)
M3: [A1 AM-MOD V AM1&lt;point&gt;(from) AM-
TMP&lt;week&gt;] ( ANs by pattern 7, AM-TMP by pattern
5) [stat: 6]
M4: [A1 AM-MOD V A2 (from) AM-TMP&lt;week&gt;]
( ANs by pattern 6, AM-TMP by pattern 5) [stat: 3]
</listItem>
<bodyText confidence="0.9401412">
Decision-making in the Consensus component
(see Figure 1) handles the final selection by select-
ing the highest score using the following formula.
Scorek = (α1* Rk + α2* Vk + α3* Sk ) for each Xk
(k=1 .. K, generated by Member Generator and
Role Classifier), where
Rk : numbers of all roles being labeled
Vk : votes of a pattern with the same roles
Sk : statistical figure learned from trained data
Xk : different pattern by Member General and
</bodyText>
<subsectionHeader confidence="0.38806">
Role Classifier
</subsectionHeader>
<bodyText confidence="0.990285571428572">
α1 ,α2 ,and α3 are weights (α1 &gt;&gt;α2 &gt;&gt;α3) used
to rank the relative contribution of Rk , Vk , and Sk.
Empirical studies led to the use of a so-called Max-
labeled-role Heuristic to derive suitable values for
these weights.
The final consensus decision for role classifica-
tion is determined by calculating
</bodyText>
<equation confidence="0.473701">
K
Consensus = Max Scorek
k=1
</equation>
<bodyText confidence="0.997688">
There are 3 roles labeled in M3, which are AN1
as A1, AM-MOD, AM2 as AM-TMP respectively.
And there are 4 roles labeled in M4, which are
AN1 as A1, AM-MOD, AN3 as A2, and AM2 as
AM-TMP respectively. Consensus scores for M3,
and M4 are
</bodyText>
<equation confidence="0.9968385">
(α1* 3 + α2* 1 + α3* 6 ) , and
(α1* 4 + α2* 1 + α3* 3 ).
</equation>
<bodyText confidence="0.999582333333333">
So the pattern [A1 AM-MOD V A2(from) AM-
TMP&lt;week&gt;] in M4 applied by Pattern 6 and Pat-
tern 5 is selected due to the most roles labeled.
</bodyText>
<sectionHeader confidence="0.986647" genericHeader="method">
3 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.996183285714286">
We extracted patterns from the training data (WSJ
Section 02 to 21) to build up a pattern database.
Table 1 reveals sparseness of the pattern database.
Twenty-six percent of predicates contain only one
pattern, and fifteen two patterns. Seventy-five per-
cents of predicates contain no more than 10 pat-
terns.
</bodyText>
<table confidence="0.9611">
No 1 2 3 4 5 5-10 11-50 51-100 &gt;100
% 26 15 10 7 5 13 20 4 2
A % 26 40 50 57 62 75 94 98 100
</table>
<tableCaption confidence="0.9973865">
Table 1: Statistical figures on the number of patterns
collected from training, WSJ Section 02-21
</tableCaption>
<bodyText confidence="0.998864333333333">
The evaluation software, srl-eval.pl, is available
from CoNLL2005 Shared Task1 , which is the offi-
cial script for evaluation of CoNLL-2005 Shared
Task systems. In order to test boundary perform-
ance of GT-PARA, we simply convert all correct
propositional arguments into A0s, except AM-
MOD and AM-NEG for both the training dataset
(WSJ Sections 15-18) and the development dataset
(WSJ Section 24).
</bodyText>
<sectionHeader confidence="0.997223" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999532272727273">
The results of classification on the development,
and test data of the CoNLL2005 shared task are
outlined in Table 2. The overall results on the De-
velopment, Test-WSJ, Test-Brown, and Test-
WSJ+Brown datasets for F-score are 65.78, 67.91,
58.58 and 66.72 respectively, which are moderate
compared to the best result reported in
CoNLL2004 Shared Task (Carreras et al., 2004)
using partial trees and the result in (Pradhan et al.,
2004). The results for boundary recognition via
GT-PARA are summarized in Table 3.
</bodyText>
<footnote confidence="0.94329">
1 http://www.lsi.upc.edu/~srlconll/soft.html
</footnote>
<page confidence="0.990313">
187
</page>
<table confidence="0.9990244">
Precision Recall Fβ=1
Development(WSJ24) 70.11% 61.96% 65.78
Test WSJ 71.49% 64.67% 67.91
Test Brown 65.75% 52.82% 58.58
Test WSJ + Brown 70.80% 63.09% 66.72
</table>
<bodyText confidence="0.9988379">
on the WSJ 21 is 73.48 shown in Table 4, which
increases about 7 points in the F1 score, compared
to WSJ 24 shown in Table 2. We found the label-
ing accuracy for WSJ 24 is 87.73, which is close to
89.30 for WSJ Section 21. But the results of
boundary recognition in Table 3 for the two data
are 9.14 points different, which leads to the better
performance in WSJ Section 21. Boundary recog-
nition as mentioned in CoNLL004 does play a very
important role in this system as well.
</bodyText>
<table confidence="0.99995025">
Precision Recall Fβ=1
WSJ 15-18 87.23% 83.98% 85.57
WSJ 21 86.89% 84.70% 85.78
WSJ 24 78.88% 74.12% 76.43
</table>
<tableCaption confidence="0.982173">
Table 3: Boundary Recognition results by GT-PARA
on WSJ 15-18, WSJ 21 and WSJ 24 sets
</tableCaption>
<table confidence="0.995909">
WSJ 21 Precision Recall Fβ=1
Overall 78.06% 69.41% 73.48
</table>
<tableCaption confidence="0.8577415">
Table 4: System results by the training data WSJ 15-18
on the WSJ Section 21
</tableCaption>
<sectionHeader confidence="0.985455" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999974125">
We have described a semantic role labeling archi-
tecture via consensus in a pattern-matching system.
The pattern-matching system is based on linear
pattern matching utilising statistical consensus for
decision-making. A General Tree-based Predicate-
Argument Boundary Recognition Algorithm (GT-
PARA) handles the conversion process, turning a
parse tree into a flat representation with all predi-
cates and their arguments labeled with some useful
features, such as phrase types. Label accuracy of
Consensus model for role classification is stable
but performance results of GT-PARA vary on dif-
ferent datasets, which is the key role for the overall
results. Although the results seem moderate on
test data, this system offers a decidedly different
approach to the problem of semantic role labeling.
</bodyText>
<sectionHeader confidence="0.993176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971901875">
Xavier Carreras, Lluís Màrquez and Grzegorz Chrupała.
2004. Hierarchical Recognition of Propositional Ar-
guments with Perceptrons. In Proceeding of
CoNLL’2004 Shared Task.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., Juraf-
sky, D. 2004. &amp;quot; Shallow Semantic Parsing using Sup-
port Vector Machines &amp;quot;, in Proceedings of
HLT/NAACL-2004, Boston, MA.
</reference>
<bodyText confidence="0.92649575">
Table 2: Overall results (top) and detailed results
on the WSJ test (bottom), obtained by the system.
The overall performance (F1: 76.43) on the WSJ
Section 24 is not as good as on the WSJ Section 21
(F1: 85.78). The poor performance for the devel-
opment was caused by more parser errors in the
WSJ Section 24. Most parser errors are brought on
by continuous phrases with commas and/or quota-
tion marks.
One interesting fact is that when we tested our
system using the data in CoNLL2004 shared task,
we found the result with the train data WSJ 15-18
</bodyText>
<figure confidence="0.999272864583334">
V 97.34% 95.25% 96.29
Test WSJ
Overall
A0
A1
A2
A3
A4
A5
AA
AM-ADV
AM-CAU
AM-DIR
AM-DIS
AM-EXT
AM-LOC
AM-MNR
AM-MOD
AM-NEG
AM-PNC
AM-PRD
AM-REC
AM-TMP
R-A0
R-A1
R-A2
R-A3
R-A4
R-AM-ADV
R-AM-CAU
R-AM-EXT
R-AM-LOC
R-AM-MNR
R-AM-TMP
Precision
Recall
Fβ=1
67.91
71.49% 64.67%
81.74% 81.53% 81.64
71.61% 69.54% 70.56
63.73% 40.36% 49.42
68.60% 34.10% 45.56
33.93% 18.63% 24.05
0.00% 0.00%
0.00% 0.00%
36.26% 31.82%
52.00% 35.62%
20.11% 42.35%
73.91% 63.75%
12.90% 12.50%
60.80% 33.33%
43.57% 30.52%
99.21% 90.93%
96.38% 92.61%
13.69% 31.30%
0.00% 0.00%
0.00% 0.00%
71.62% 54.55%
93.37% 69.20%
82.24% 56.41%
100.00% 25.00%
0.00% 0.00%
0.00% 0.00%
0.00% 0.00%
0.00% 0.00%
0.00% 0.00%
0.00% 0.00%
0.00% 0.00%
0.00% 0.00%
0.00
0.00
33.89
42.28
27.27
68.46
12.70
43.06
35.90
94.89
94.46
19.05
0.00
0.00
61.93
79.49
66.92
40.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
</figure>
<page confidence="0.93319">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516488">
<title confidence="0.999956">Semantic Role Labeling via Consensus in Pattern-Matching</title>
<author confidence="0.999652">Chi-San Lin Tony C Smith</author>
<affiliation confidence="0.999965">Department of Computer Science Department of Computer Science Waikato University Waikato University</affiliation>
<address confidence="0.992897">Hamilton, New Zealand Hamilton, New Zealand</address>
<note confidence="0.588836">cl123@cs.waikato.ac.nz tcs@cs.waikato.ac.nz</note>
<abstract confidence="0.991614714285714">This paper describes a system for semantic role labeling for the CoNLL2005 Shared task. We divide the task into two sub-tasks: boundary recognition by a general treebased predicate-argument recognition algorithm to convert a parse tree into a flat representation of all predicates and their related boundaries, and role labeling by a consensus model using a pattern-matching framework to find suitable roles for core constituents and adjuncts. We describe the system architecture and report results for the CoNLL2005 development dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Lluís Màrquez and Grzegorz Chrupała.</title>
<date>2004</date>
<booktitle>In Proceeding of CoNLL’2004 Shared Task.</booktitle>
<marker>Carreras, 2004</marker>
<rawString>Xavier Carreras, Lluís Màrquez and Grzegorz Chrupała. 2004. Hierarchical Recognition of Propositional Arguments with Perceptrons. In Proceeding of CoNLL’2004 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>K Hacioglu</author>
<author>J Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing using Support Vector Machines &amp;quot;,</title>
<date>2004</date>
<booktitle>in Proceedings of HLT/NAACL-2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1193" citStr="Pradhan et al., 2004" startWordPosition="171" endWordPosition="174">d their related boundaries, and role labeling by a consensus model using a pattern-matching framework to find suitable roles for core constituents and adjuncts. We describe the system architecture and report results for the CoNLL2005 development dataset. 1 Introduction Semantic role labeling is to find all arguments for all predicates in a sentence, and classify them by semantic roles such as A0, A1, AM-TMP and so on. The performance of semantic role labeling can play a key role in Natural Language Processing applications, such as Information Extraction, Question Answering, and Summarization (Pradhan et al., 2004). Most existing systems separate semantic role labeling into two sub-problems, boundary recognition and role classification, and use feature-based models to address both (Carreras et al., 2004). Our strategy is to develop a boundary analyzer by a general tree-based predicate-argument recognition algorithm (GT-PARA) for boundary recognition, and a pattern-matching model for role classification. The only information used in our system is Charniak’s annotation with words, which contains all useful syntactic annotations. Five features, which are Headword, Phrase type, Voice, Target verb, and Prepo</context>
<context position="10572" citStr="Pradhan et al., 2004" startWordPosition="1755" endWordPosition="1758">ll correct propositional arguments into A0s, except AMMOD and AM-NEG for both the training dataset (WSJ Sections 15-18) and the development dataset (WSJ Section 24). 4 Experimental Results The results of classification on the development, and test data of the CoNLL2005 shared task are outlined in Table 2. The overall results on the Development, Test-WSJ, Test-Brown, and TestWSJ+Brown datasets for F-score are 65.78, 67.91, 58.58 and 66.72 respectively, which are moderate compared to the best result reported in CoNLL2004 Shared Task (Carreras et al., 2004) using partial trees and the result in (Pradhan et al., 2004). The results for boundary recognition via GT-PARA are summarized in Table 3. 1 http://www.lsi.upc.edu/~srlconll/soft.html 187 Precision Recall Fβ=1 Development(WSJ24) 70.11% 61.96% 65.78 Test WSJ 71.49% 64.67% 67.91 Test Brown 65.75% 52.82% 58.58 Test WSJ + Brown 70.80% 63.09% 66.72 on the WSJ 21 is 73.48 shown in Table 4, which increases about 7 points in the F1 score, compared to WSJ 24 shown in Table 2. We found the labeling accuracy for WSJ 24 is 87.73, which is close to 89.30 for WSJ Section 21. But the results of boundary recognition in Table 3 for the two data are 9.14 points different</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Pradhan, S., Ward, W., Hacioglu, K., Martin, J., Jurafsky, D. 2004. &amp;quot; Shallow Semantic Parsing using Support Vector Machines &amp;quot;, in Proceedings of HLT/NAACL-2004, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>