<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.108317">
<title confidence="0.917635">
TAKELAB: Medical Information Extraction and Linking with MINERAL
</title>
<author confidence="0.991055">
Goran Glavaˇs
</author>
<affiliation confidence="0.946906666666667">
University of Zagreb
Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
</affiliation>
<address confidence="0.912746">
Unska 3, 10000 Zagreb, Croatia
</address>
<email confidence="0.997915">
goran.glavas@fer.hr
</email>
<sectionHeader confidence="0.995618" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999612266666666">
Medical texts are filled with mentions of dis-
eases, disorders, and other clinical conditions,
with many different surface forms relating to
the same condition. We describe MINERAL, a
system for extraction and normalization of dis-
ease mentions in clinical text, with which we
participated in the Task 14 of SemEval 2015
evaluation campaign. MINERAL relies on a
conditional random fields-based model with a
rich set of features for mention detection, and
a semantic textual similarity measure for entity
linking. MINERAL reaches joint extraction
and linking performance of 75.9% relaxed Fl-
score (strict score of 72.7%) and ranks fourth
among 16 participating teams.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999652630434783">
Clinical narratives contain numerous mentions of
diseases and disorders. Recognizing these mentions
in text and normalizing the different superficial forms
of a disorder to the same canonical form could enable
new types of analyses that would be beneficial for
both medical professionals and patients.
Detection and normalization of various concepts
such as named entities (McCallum and Li, 2003; Kr-
ishnan and Manning, 2006) or events (Bethard, 2013;
Glavaˇs and ˇSnajder, 2014) has long been in the focus
of the NLP community. Disorder mentions in clini-
cal text, however, have some peculiarities not typical
for traditional information extraction tasks such as
discontinuity or distributivity of a single token to
multiple disorder mentions. For example, the snippet
“Patient’s extremities were turned in and
clinched together as a consequence of... ”
contains two mentions of medical conditions, “ex-
tremities turned in” and “extremities clinched to-
gether”, which share the token “extremities”, with
the latter mention being discontinuous.
In this paper we present the MINERAL (Medi-
cal INformation ExtRAction and Linking) system
for recognizing and normalizing mentions of clinical
conditions, with which we participated in Task 14
of SemEval 2015 evaluation campaign. The system
recognizes disorder mentions via the supervised con-
ditional random fields (CRF) model with a rich set of
lexical, gazetteer-based, and informativeness-based
features. We apply a set of post-processing rules to
construct disorder mentions from token-level anno-
tations which follow the BEGIN-INSIDE-OUTSIDE
scheme. We utilize a measure of semantic textual
similarity to link recognized disorder mentions to
entries in the SNOMED-CT medical database. Our
approach is resource light in the sense that, except for
SNOMED-CT which is necessary for normalization,
it does not rely on medical NLP resources.
We ranked fourth (relaxed evaluation setting)
among 16 teams in the official evaluation, with 3%
lower performance than the best-performing system.
Such a result suggests that coupling sequence la-
belling for mention recognition with an STS measure
for concept normalization poses a viable solution for
entity recognition in the clinical domain. We make
the MINERAL system freely available.1
</bodyText>
<footnote confidence="0.987617">
1http://takelab.fer.hr/mineral
</footnote>
<page confidence="0.967375">
389
</page>
<note confidence="0.613955">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 389–393,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.973276" genericHeader="method">
2 Clinical Information Extraction
</sectionHeader>
<bodyText confidence="0.999965933333333">
Clinical concept extraction is an essential task in
medical natural language processing. While early
approaches heavily relied on domain-specific vocab-
ularies (Friedman et al., 1994; Aronson, 2001; Zeng
et al., 2006), more recent efforts leverage the human-
annotated corpora to develop machine learning mod-
els for the extraction of medical concepts (Tang et al.,
2013; Uzuner et al., 2010). The rise in the number
of data-driven efforts in the medical domain was par-
ticularly motivated by the shared tasks such as i2b2
challenges (Uzuner et al., 2010) and ShARe/CLEF
eHealth Evaluation Lab (Suominen et al., 2013).
The first subtask of the SemEval Task 14, in which
we participated, was essentially the same as the first
task in the ShARe/CLEF eHealth campaign. We did
not participate in the second subtask on extracting
arguments of disorder mentions. The best performing
system of the ShARe/CLEF eHealth task on disor-
der extraction and normalization (Tang et al., 2013)
employed CRF and structured SVM models for men-
tion extraction and the traditional vector-space model
from information retrieval (Salton et al., 1975) for
disorder normalization.
Similar to (Tang et al., 2013), we employ the CRF
model for extraction of disorder mentions, but we
leverage recent findings in word vector representa-
tions (Mikolov et al., 2013) for feature computation.
We make use of the state-of-the-art measure of se-
mantic similarity of short texts (ˇSari´c et al., 2012) for
concept normalization.
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="method">
3 MINERAL
</sectionHeader>
<bodyText confidence="0.9998604">
MINERAL consists of two subsystems: one for ex-
tracting disorder mentions and the other for normal-
izing extracted mentions by assigning them a Con-
cept Unique Identifier (CUI) from the SNOMED-CT
database (Stearns et al., 2001).
</bodyText>
<subsectionHeader confidence="0.979582">
3.1 Disorder Mention Extraction
</subsectionHeader>
<bodyText confidence="0.999458090909091">
At the core of the extraction subsystem is the
CRF model with lexical, gazetteer-based, and
informativeness-based features. We decided to use
the BEGIN-INSIDE-OUTSIDE annotation scheme for
the CRF model, although this scheme does not ac-
count for token-sharing disorder mentions. Thus,
we apply a set of postprocessing rules to derive dis-
order mentions from token-level outputs produced
by the CRF model and to handle most frequent
cases of token-sharing mentions (e.g., “abdomen non-
disturbed and non-distended”).
</bodyText>
<sectionHeader confidence="0.472763" genericHeader="method">
3.1.1 Features
</sectionHeader>
<bodyText confidence="0.999385447368421">
We feed the CRF model with a rich set of features
that can be divided into (1) token-based features, (2)
gazetteer-based features, and (3) information content-
based features. All of the features are templated on
the symmetric window of size two, i.e., computed
for two preceding tokens, current token, and two
subsequent tokens.
Token-based features (TK). Token-based fea-
tures group all features which can be computed just
from the token at hand. These include the surface
form, lemma, stem, POS-tag, and shape (encoding of
the capitalization of the word, e.g., “UL” for “Atrial”)
of the word. We also encode the first and the last char-
acter bigram and trigram of the word as features.
Gazetter-based features (GZ). Features in this
group rely on comparison of tokens in text with en-
tries in the SNOMED-CT database and with disease
annoations on the training set. For each token we
compute: the maximum similarity with any of the
words (1) starting a SNOMED-CT entry, (2) inside
a SNOMED-CT entry, and (3) ending a SNOMED-
CT entry. We compute the same three features only
considering gold annotations in the training set as
gazetteer entries. We compute the semantic similarity
between two words as the cosine between their cor-
responding word embedding vectors. We trained the
embedding vectors with the word2vec tool (Mikolov
et al., 2013) on the large unlabeled corpus of clini-
cal texts (with over 400K documents) provided by
the task organizers. We also counted the number of
gazetteer entries that start with, contain, and end with
the token at hand.
Information content-based features (IC). These
features compute the informativeness of ngrams
within the clinical domain and compare it their gen-
eral informativeness. We use information content
as a measure of the informativeness of the word w
within a corpus C:
</bodyText>
<equation confidence="0.999364">
freq(w) + 1
ic(w) = − log E
W&apos;∈C freq(w&apos;) + 1
</equation>
<page confidence="0.931953">
390
</page>
<bodyText confidence="0.999499176470588">
where freq(w) is the frequency of the word w in
corpus C. We compute three different information
content-based features. First, we compute the infor-
mation content of the word within a large corpus of
clinical narratives. Secondly, we compute the ratio
of the information content of the word computed on
the clinical corpus and the information content of the
same word computed on a large general corpus. We
used Google Books ngrams (Michel et al., 2011) as
the general corpus. The rationale here is that the clin-
ical concepts such as diseases and disorders will have
a higher relative frequency and, consequently, lower
information content in the clinical corpus than in the
general corpus. Finally, the third feature we com-
pute is the mutual information of the bigrams in the
clinical corpus, which we define via the information
content:
</bodyText>
<equation confidence="0.999633666666667">
ic(w1w2)
mi(w1, w2) =
ic(w1) · ic(w2)
</equation>
<bodyText confidence="0.998659714285714">
where ic(w1w2) is the information content of the bi-
gram w1w2. Mutual information score indicates pairs
of words that often appear together (e.g., “atrial di-
latation”). For each word wi we compute the mutual
information of the bigrams it constitues with the pre-
vious word (i.e., wi−1wi) and the subsequent word
(i.e., wiwi+1).
</bodyText>
<subsectionHeader confidence="0.852249">
3.1.2 Postprocessing
</subsectionHeader>
<bodyText confidence="0.9914675">
The only reasonable postprocessing strategy with
the B-I-O scheme is to join each INSIDE token with
the closest preceding BEGIN token. However, this
strategy requires rule-based fixes for common situa-
tions in which two disorder mentions share a token.
We designed postprocessing rules by observing the
most frequent mistakes our CRF model made on the
development set provided by the organizers. This led
to three particular fixes: (1) mentions of abdomen
condition typically correspond to two disorder men-
tions sharing the token “abdomen” (e.g., processing
“abdomen non-tender and non-distended” results with
two disorder mentions – “abdomen non-tender” and
“abdomen non-distended”); (2) mentions of allergies
typically share the token “allergies” (e.g., process-
ing “Allergies: Roxicet / Penicillins / Aspirin” pro-
duces three mentions – “Allergies Roxicet”, “Aller-
gies Penicillins”, and “Allergies Aspirin”); and (3)
the CRF model rather frequently fails to recognize
the type of the hepatitis. We associate the type of the
hepatitis (e.g., “B”) found in the proximity of the
token “hepatitis” when CRF fails to do so.
</bodyText>
<subsectionHeader confidence="0.999223">
3.2 Mention Normalization
</subsectionHeader>
<bodyText confidence="0.999934689655173">
The normalization subsystem assigns a CUI to each
extracted disorder mention by comparing the seman-
tic similarity of the mention with the SNOMED-CT
entries. Given that SNOMED-CT has over 650K
entries, it is infeasible to compute the similarity
of the disorder mentions with all database entries.
Therefore, we first filtered out only the entries which
contain at least one lemma from the extracted men-
tion. E.g., for the mention “melena due to gastroin-
testinal haemorrhage” we would consider only the
SNOMED-CT entries containing either “melena”,
“gastrointestinal”, or “haemorrhage”.
We compute the similarity as the modified variant
of the greedy weighted alignment overlap (GWAO)
measure from (ˇSari´c et al., 2012). To compute this
score, we iteratively pair the words – one from ex-
tracted mention and the other from the database entry
– according to their semantic similarity. In each iter-
ation we greedily select the pair of words with the
largest semantic similarity, and remove these words
from their corresponding text snippets. The similarity
between words is computed as the cosine between
their embedding vectors obtained with word2vec
(Mikolov et al., 2013) on the large unlabeled corpus
of clinical narratives. Let P(m, s) be the set of word
pairs obtained through the alignment between the
extracted mention m and the SNOMED-CT entry s
and let vec(w) be the embedding vector of the word
w. The GWAO score is then computed as follows:
</bodyText>
<equation confidence="0.999226">
�gwao(m, s) = α · cos (vec(wm), vec(ws))
(wm,ws)
∈P(m,s)
</equation>
<bodyText confidence="0.9998617">
where α is the larger of the information contents
of the two words, α = max (ic(wm), ic(ws)). The
gwao(m, s) score is normalized with the sum of in-
formation contents of words from m and s, respec-
tively, and the harmonic mean of the two normalized
scores is the final similarity score. We assign to
the extracted mention the CUI of the most similar
SNOMED-CT entry, assuming the similarity is above
some treshold A (otherwise, the label “CUI-less” is
assigned to the mention). The optimal value of A is
</bodyText>
<page confidence="0.996446">
391
</page>
<table confidence="0.999438428571429">
Model Strict Relaxed
P R Fl P R Fl
TK 75.6 65.6 70.2 90.0 80.4 84.9
TK + GZ 75.1 66.1 70.3 89.6 80.9 85.0
TK + IC 76.4 66.3 71.0 90.2 80.4 85.1
All feat. 76.3 66.9 71.3 90.1 81.1 85.4
All + PPR 77.4 69.1 73.0 90.1 82.2 86.0
</table>
<tableCaption confidence="0.971797">
Table 1: Model selection results.
</tableCaption>
<table confidence="0.99988825">
Team Strict Relaxed
P R Fl P R Fl
ezDI 78.3 73.2 75.7 81.5 76.1 78.7
ULisboa 77.9 70.5 74.0 80.6 72.9 76.5
UTH-CCB 77.8 69.6 73.5 79.7 71.4 75.3
UWM 77.3 69.9 73.4 80.9 73.1 76.8
TakeLab 76.1 69.6 72.7 79.4 72.7 75.9
Bioinf.-UA 69.0 73.6 71.2 71.9 76.6 74.2
</table>
<tableCaption confidence="0.998008">
Table 2: Official SemEval Task 14 (subtask 1) evaluation.
</tableCaption>
<bodyText confidence="0.999720285714286">
determined by maximizing the CUI prediction accu-
racy on the training and development set. A useful
add-on to the normalization step is the memorization
of CUIs for all disorder mentions observed in the
training set. In other words, a memorized mention
observed in the test set will be assigned the CUI it
had in the training set.
</bodyText>
<sectionHeader confidence="0.998207" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999748571428572">
Participants were provided with a training set consist-
ing of 298 clinical documents and a development set
with 133 documents. We used the training and devel-
opment set to optimize the model (features, postpro-
cessing rules, and the similarity treshold A). A test
set of 100 clinical documents was used for official
evaluation.
</bodyText>
<subsectionHeader confidence="0.998747">
4.1 Model Optimization
</subsectionHeader>
<bodyText confidence="0.999889047619048">
We trained the CRF model with different combina-
tions of feature groups (TK, GZ, and IC) and eval-
uated the performance of these models on the de-
velopment set. We also evaluated the contribution
of the postprocessing rules (PPR) on the develop-
ment set. The extraction performance of the differ-
ent models is shown in Table 4.1. The model using
only token-based features alone (model TK) achieves
solid performance. Information content-based fea-
tures (model TK + IC) seem to have a more positive
impact on the performance than the gazetteer-based
features (model TK + GZ). Still, the model with all
features displays the best performance. Applying
postprocessing rules further boosts the performance
on the development set, which is expected, because
the rules were designed precisely to fix the most fre-
quent errors on that dataset. We submitted the model
All + PPR for official evaluation. We also optimized
the similarity treshold A to maximize the normaliza-
tion accuracy on the development set, selecting the
optimal value of A = 0.83.
</bodyText>
<subsectionHeader confidence="0.988263">
4.2 Official Results
</subsectionHeader>
<bodyText confidence="0.999956454545455">
A subset of the official ranking on the test set is
shown in Table 4.2. MINERAL ranks fourth among
16 teams in relaxed evaluation and fifth in strict eval-
uation, with only 3% lower Fl performance than the
best performing system.
Like most other systems, MINERAL displays
higher precision than recall. This would suggest a
non-negligible amount of obdurate disorder mentions
which appear rarely in clinical documents and which
are not semantically similar with more frequent dis-
orders.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999951666666667">
We described MINERAL, a system for extraction
and normalization of disorder mentions in clinical
text, with which we participated in Task 14 of Se-
mEval 2015. At the core of the mention extraction
approach is the CRF model built on B-I-O annota-
tion scheme and a rich set of lexical, gazetteer-based,
and informativeness-based features. We link the dis-
ease mentions to the SNOMED-CT entries using a
measure of semantic textual similarity of short texts.
MINERAL achieved performance of almost 76%
Fl (relaxed evaluation setting), ranking us fourth
out of 16 teams participating in the task, with 3%
lower performance than the best-performing team.
Such a result suggests that a resource light approach
with sequence labeling (with semantic features) for
mention extraction and STS measures for concept
normalization offers competitive performance in the
clinical domain.
</bodyText>
<page confidence="0.996315">
392
</page>
<sectionHeader confidence="0.990145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999515958333333">
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
page 17.
Steven Bethard. 2013. ClearTK-TimeML: A minimal-
ist approach to TempEval 2013. In Second Joint
Conference on Lexical and Computational Semantics
(StarSEM), volume 2, pages 10–14.
Carol Friedman, Philip O. Alderson, John H.M. Austin,
James J. Cimino, and Stephen B. Johnson. 1994. A
general natural-language text processor for clinical ra-
diology. Journal of the American Medical Informatics
Association, 1(2):161–174.
Goran Glavaˇs and Jan ˇSnajder. 2014. Construction and
evaluation of event graphs. Natural Language Engi-
neering, pages 1–46.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 1121–1128.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons. In
Proceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL 2003, pages 188–191.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, Joseph P. Pick-
ett, Dale Hoiberg, Dan Clancy, Peter Norvig, and Jon
Orwant. 2011. Quantitative analysis of culture using
millions of digitized books. Science, 331(6014):176–
182.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector space model for automatic indexing. Commu-
nications of the ACM, 18(11):613–620.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems
for measuring semantic text similarity. In Proceed-
ings of the Sixth International Workshop on Semantic
Evaluation (SemEval), pages 441–448.
Michael Q. Stearns, Colin Price, Kent A. Spackman, and
Amy Y. Wang. 2001. SNOMED Clinical Terms:
Overview of the development process and project status.
In Proceedings of the AMIA Symposium, page 662.
Hanna Suominen, Sanna Salanter¨a, Sumithra Velupillai,
Wendy W. Chapman, Guergana Savova, Noemie El-
hadad, Sameer Pradhan, Brett R. South, Danielle L.
Mowery, and Gareth J.F. Jones. 2013. Overview of
the ShARe/CLEF eHealth Evaluation Lab 2013. In
Information Access Evaluation: Multilinguality, Multi-
modality, and Visualization, pages 212–231.
Buzhou Tang, Yonghui Wu, Min Jiang, Joshua C. Denny,
and Hua Xu. 2013. Recognizing and encoding dis-
corder concepts in clinical text using machine learning
and vector space model. In Workshop of ShARe/CLEF
eHealth Evaluation Lab 2013.
¨Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010.
Extracting medication information from clinical text.
Journal of the American Medical Informatics Associa-
tion, 17(5):514–518.
Qing T. Zeng, Sergey Goryachev, Scott Weiss, Margarita
Sordo, Shawn N. Murphy, and Ross Lazarus. 2006. Ex-
tracting principal diagnosis, co-morbidity and smoking
status for asthma research: Evaluation of a natural lan-
guage processing system. BMC Medical Informatics
and Decision Making, 6(1):30.
</reference>
<page confidence="0.99937">
393
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863243">
<title confidence="0.999255">TAKELAB: Medical Information Extraction and Linking with MINERAL</title>
<author confidence="0.997242">Goran</author>
<affiliation confidence="0.956617333333333">University of Faculty of Electrical Engineering and Text Analysis and Knowledge Engineering</affiliation>
<address confidence="0.956749">Unska 3, 10000 Zagreb,</address>
<email confidence="0.997293">goran.glavas@fer.hr</email>
<abstract confidence="0.999144">Medical texts are filled with mentions of diseases, disorders, and other clinical conditions, with many different surface forms relating to same condition. We describe a system for extraction and normalization of disease mentions in clinical text, with which we participated in the Task 14 of SemEval 2015 evaluation campaign. MINERAL relies on a conditional random fields-based model with a rich set of features for mention detection, and a semantic textual similarity measure for entity linking. MINERAL reaches joint extraction linking performance of (strict score of and ranks fourth among 16 participating teams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS metathesaurus: the MetaMap program.</title>
<date>2001</date>
<booktitle>In Proceedings of the AMIA Symposium,</booktitle>
<pages>17</pages>
<contexts>
<context position="3651" citStr="Aronson, 2001" startWordPosition="525" endWordPosition="526">n with an STS measure for concept normalization poses a viable solution for entity recognition in the clinical domain. We make the MINERAL system freely available.1 1http://takelab.fer.hr/mineral 389 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 389–393, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Clinical Information Extraction Clinical concept extraction is an essential task in medical natural language processing. While early approaches heavily relied on domain-specific vocabularies (Friedman et al., 1994; Aronson, 2001; Zeng et al., 2006), more recent efforts leverage the humanannotated corpora to develop machine learning models for the extraction of medical concepts (Tang et al., 2013; Uzuner et al., 2010). The rise in the number of data-driven efforts in the medical domain was particularly motivated by the shared tasks such as i2b2 challenges (Uzuner et al., 2010) and ShARe/CLEF eHealth Evaluation Lab (Suominen et al., 2013). The first subtask of the SemEval Task 14, in which we participated, was essentially the same as the first task in the ShARe/CLEF eHealth campaign. We did not participate in the secon</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Alan R. Aronson. 2001. Effective mapping of biomedical text to the UMLS metathesaurus: the MetaMap program. In Proceedings of the AMIA Symposium, page 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
</authors>
<title>ClearTK-TimeML: A minimalist approach to TempEval</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (StarSEM),</booktitle>
<volume>2</volume>
<pages>10--14</pages>
<contexts>
<context position="1380" citStr="Bethard, 2013" startWordPosition="201" endWordPosition="202">RAL reaches joint extraction and linking performance of 75.9% relaxed Flscore (strict score of 72.7%) and ranks fourth among 16 participating teams. 1 Introduction Clinical narratives contain numerous mentions of diseases and disorders. Recognizing these mentions in text and normalizing the different superficial forms of a disorder to the same canonical form could enable new types of analyses that would be beneficial for both medical professionals and patients. Detection and normalization of various concepts such as named entities (McCallum and Li, 2003; Krishnan and Manning, 2006) or events (Bethard, 2013; Glavaˇs and ˇSnajder, 2014) has long been in the focus of the NLP community. Disorder mentions in clinical text, however, have some peculiarities not typical for traditional information extraction tasks such as discontinuity or distributivity of a single token to multiple disorder mentions. For example, the snippet “Patient’s extremities were turned in and clinched together as a consequence of... ” contains two mentions of medical conditions, “extremities turned in” and “extremities clinched together”, which share the token “extremities”, with the latter mention being discontinuous. In this </context>
</contexts>
<marker>Bethard, 2013</marker>
<rawString>Steven Bethard. 2013. ClearTK-TimeML: A minimalist approach to TempEval 2013. In Second Joint Conference on Lexical and Computational Semantics (StarSEM), volume 2, pages 10–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Friedman</author>
<author>Philip O Alderson</author>
<author>John H M Austin</author>
<author>James J Cimino</author>
<author>Stephen B Johnson</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="3636" citStr="Friedman et al., 1994" startWordPosition="521" endWordPosition="524"> for mention recognition with an STS measure for concept normalization poses a viable solution for entity recognition in the clinical domain. We make the MINERAL system freely available.1 1http://takelab.fer.hr/mineral 389 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 389–393, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Clinical Information Extraction Clinical concept extraction is an essential task in medical natural language processing. While early approaches heavily relied on domain-specific vocabularies (Friedman et al., 1994; Aronson, 2001; Zeng et al., 2006), more recent efforts leverage the humanannotated corpora to develop machine learning models for the extraction of medical concepts (Tang et al., 2013; Uzuner et al., 2010). The rise in the number of data-driven efforts in the medical domain was particularly motivated by the shared tasks such as i2b2 challenges (Uzuner et al., 2010) and ShARe/CLEF eHealth Evaluation Lab (Suominen et al., 2013). The first subtask of the SemEval Task 14, in which we participated, was essentially the same as the first task in the ShARe/CLEF eHealth campaign. We did not participa</context>
</contexts>
<marker>Friedman, Alderson, Austin, Cimino, Johnson, 1994</marker>
<rawString>Carol Friedman, Philip O. Alderson, John H.M. Austin, James J. Cimino, and Stephen B. Johnson. 1994. A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2):161–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Goran Glavaˇs</author>
<author>Jan ˇSnajder</author>
</authors>
<title>Construction and evaluation of event graphs. Natural Language Engineering,</title>
<date>2014</date>
<pages>1--46</pages>
<marker>Glavaˇs, ˇSnajder, 2014</marker>
<rawString>Goran Glavaˇs and Jan ˇSnajder. 2014. Construction and evaluation of event graphs. Natural Language Engineering, pages 1–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An effective two-stage model for exploiting non-local dependencies in named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1121--1128</pages>
<contexts>
<context position="1355" citStr="Krishnan and Manning, 2006" startWordPosition="194" endWordPosition="198">larity measure for entity linking. MINERAL reaches joint extraction and linking performance of 75.9% relaxed Flscore (strict score of 72.7%) and ranks fourth among 16 participating teams. 1 Introduction Clinical narratives contain numerous mentions of diseases and disorders. Recognizing these mentions in text and normalizing the different superficial forms of a disorder to the same canonical form could enable new types of analyses that would be beneficial for both medical professionals and patients. Detection and normalization of various concepts such as named entities (McCallum and Li, 2003; Krishnan and Manning, 2006) or events (Bethard, 2013; Glavaˇs and ˇSnajder, 2014) has long been in the focus of the NLP community. Disorder mentions in clinical text, however, have some peculiarities not typical for traditional information extraction tasks such as discontinuity or distributivity of a single token to multiple disorder mentions. For example, the snippet “Patient’s extremities were turned in and clinched together as a consequence of... ” contains two mentions of medical conditions, “extremities turned in” and “extremities clinched together”, which share the token “extremities”, with the latter mention bein</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D. Manning. 2006. An effective two-stage model for exploiting non-local dependencies in named entity recognition. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 1121–1128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</booktitle>
<pages>188--191</pages>
<contexts>
<context position="1326" citStr="McCallum and Li, 2003" startWordPosition="190" endWordPosition="193">a semantic textual similarity measure for entity linking. MINERAL reaches joint extraction and linking performance of 75.9% relaxed Flscore (strict score of 72.7%) and ranks fourth among 16 participating teams. 1 Introduction Clinical narratives contain numerous mentions of diseases and disorders. Recognizing these mentions in text and normalizing the different superficial forms of a disorder to the same canonical form could enable new types of analyses that would be beneficial for both medical professionals and patients. Detection and normalization of various concepts such as named entities (McCallum and Li, 2003; Krishnan and Manning, 2006) or events (Bethard, 2013; Glavaˇs and ˇSnajder, 2014) has long been in the focus of the NLP community. Disorder mentions in clinical text, however, have some peculiarities not typical for traditional information extraction tasks such as discontinuity or distributivity of a single token to multiple disorder mentions. For example, the snippet “Patient’s extremities were turned in and clinched together as a consequence of... ” contains two mentions of medical conditions, “extremities turned in” and “extremities clinched together”, which share the token “extremities”,</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva Presser Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
<author>Joseph P Pickett</author>
<author>Dale Hoiberg</author>
<author>Dan Clancy</author>
<author>Peter Norvig</author>
<author>Jon Orwant</author>
</authors>
<title>Quantitative analysis of culture using millions of digitized books.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>331</volume>
<issue>6014</issue>
<pages>182</pages>
<contexts>
<context position="8055" citStr="Michel et al., 2011" startWordPosition="1236" endWordPosition="1239">ral informativeness. We use information content as a measure of the informativeness of the word w within a corpus C: freq(w) + 1 ic(w) = − log E W&apos;∈C freq(w&apos;) + 1 390 where freq(w) is the frequency of the word w in corpus C. We compute three different information content-based features. First, we compute the information content of the word within a large corpus of clinical narratives. Secondly, we compute the ratio of the information content of the word computed on the clinical corpus and the information content of the same word computed on a large general corpus. We used Google Books ngrams (Michel et al., 2011) as the general corpus. The rationale here is that the clinical concepts such as diseases and disorders will have a higher relative frequency and, consequently, lower information content in the clinical corpus than in the general corpus. Finally, the third feature we compute is the mutual information of the bigrams in the clinical corpus, which we define via the information content: ic(w1w2) mi(w1, w2) = ic(w1) · ic(w2) where ic(w1w2) is the information content of the bigram w1w2. Mutual information score indicates pairs of words that often appear together (e.g., “atrial dilatation”). For each</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, Pickett, Hoiberg, Clancy, Norvig, Orwant, 2011</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, and Jon Orwant. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="4782" citStr="Mikolov et al., 2013" startWordPosition="705" endWordPosition="708"> as the first task in the ShARe/CLEF eHealth campaign. We did not participate in the second subtask on extracting arguments of disorder mentions. The best performing system of the ShARe/CLEF eHealth task on disorder extraction and normalization (Tang et al., 2013) employed CRF and structured SVM models for mention extraction and the traditional vector-space model from information retrieval (Salton et al., 1975) for disorder normalization. Similar to (Tang et al., 2013), we employ the CRF model for extraction of disorder mentions, but we leverage recent findings in word vector representations (Mikolov et al., 2013) for feature computation. We make use of the state-of-the-art measure of semantic similarity of short texts (ˇSari´c et al., 2012) for concept normalization. 3 MINERAL MINERAL consists of two subsystems: one for extracting disorder mentions and the other for normalizing extracted mentions by assigning them a Concept Unique Identifier (CUI) from the SNOMED-CT database (Stearns et al., 2001). 3.1 Disorder Mention Extraction At the core of the extraction subsystem is the CRF model with lexical, gazetteer-based, and informativeness-based features. We decided to use the BEGIN-INSIDE-OUTSIDE annotat</context>
<context position="7074" citStr="Mikolov et al., 2013" startWordPosition="1070" endWordPosition="1073">atures in this group rely on comparison of tokens in text with entries in the SNOMED-CT database and with disease annoations on the training set. For each token we compute: the maximum similarity with any of the words (1) starting a SNOMED-CT entry, (2) inside a SNOMED-CT entry, and (3) ending a SNOMEDCT entry. We compute the same three features only considering gold annotations in the training set as gazetteer entries. We compute the semantic similarity between two words as the cosine between their corresponding word embedding vectors. We trained the embedding vectors with the word2vec tool (Mikolov et al., 2013) on the large unlabeled corpus of clinical texts (with over 400K documents) provided by the task organizers. We also counted the number of gazetteer entries that start with, contain, and end with the token at hand. Information content-based features (IC). These features compute the informativeness of ngrams within the clinical domain and compare it their general informativeness. We use information content as a measure of the informativeness of the word w within a corpus C: freq(w) + 1 ic(w) = − log E W&apos;∈C freq(w&apos;) + 1 390 where freq(w) is the frequency of the word w in corpus C. We compute thr</context>
<context position="11141" citStr="Mikolov et al., 2013" startWordPosition="1715" endWordPosition="1718">astrointestinal”, or “haemorrhage”. We compute the similarity as the modified variant of the greedy weighted alignment overlap (GWAO) measure from (ˇSari´c et al., 2012). To compute this score, we iteratively pair the words – one from extracted mention and the other from the database entry – according to their semantic similarity. In each iteration we greedily select the pair of words with the largest semantic similarity, and remove these words from their corresponding text snippets. The similarity between words is computed as the cosine between their embedding vectors obtained with word2vec (Mikolov et al., 2013) on the large unlabeled corpus of clinical narratives. Let P(m, s) be the set of word pairs obtained through the alignment between the extracted mention m and the SNOMED-CT entry s and let vec(w) be the embedding vector of the word w. The GWAO score is then computed as follows: �gwao(m, s) = α · cos (vec(wm), vec(ws)) (wm,ws) ∈P(m,s) where α is the larger of the information contents of the two words, α = max (ic(wm), ic(ws)). The gwao(m, s) score is normalized with the sum of information contents of words from m and s, respectively, and the harmonic mean of the two normalized scores is the fin</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="4575" citStr="Salton et al., 1975" startWordPosition="672" endWordPosition="675">asks such as i2b2 challenges (Uzuner et al., 2010) and ShARe/CLEF eHealth Evaluation Lab (Suominen et al., 2013). The first subtask of the SemEval Task 14, in which we participated, was essentially the same as the first task in the ShARe/CLEF eHealth campaign. We did not participate in the second subtask on extracting arguments of disorder mentions. The best performing system of the ShARe/CLEF eHealth task on disorder extraction and normalization (Tang et al., 2013) employed CRF and structured SVM models for mention extraction and the traditional vector-space model from information retrieval (Salton et al., 1975) for disorder normalization. Similar to (Tang et al., 2013), we employ the CRF model for extraction of disorder mentions, but we leverage recent findings in word vector representations (Mikolov et al., 2013) for feature computation. We make use of the state-of-the-art measure of semantic similarity of short texts (ˇSari´c et al., 2012) for concept normalization. 3 MINERAL MINERAL consists of two subsystems: one for extracting disorder mentions and the other for normalizing extracted mentions by assigning them a Concept Unique Identifier (CUI) from the SNOMED-CT database (Stearns et al., 2001).</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>TakeLab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval),</booktitle>
<pages>441--448</pages>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval), pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Q Stearns</author>
<author>Colin Price</author>
<author>Kent A Spackman</author>
<author>Amy Y Wang</author>
</authors>
<title>SNOMED Clinical Terms: Overview of the development process and project status.</title>
<date>2001</date>
<booktitle>In Proceedings of the AMIA Symposium,</booktitle>
<pages>662</pages>
<contexts>
<context position="5174" citStr="Stearns et al., 2001" startWordPosition="767" endWordPosition="770"> (Salton et al., 1975) for disorder normalization. Similar to (Tang et al., 2013), we employ the CRF model for extraction of disorder mentions, but we leverage recent findings in word vector representations (Mikolov et al., 2013) for feature computation. We make use of the state-of-the-art measure of semantic similarity of short texts (ˇSari´c et al., 2012) for concept normalization. 3 MINERAL MINERAL consists of two subsystems: one for extracting disorder mentions and the other for normalizing extracted mentions by assigning them a Concept Unique Identifier (CUI) from the SNOMED-CT database (Stearns et al., 2001). 3.1 Disorder Mention Extraction At the core of the extraction subsystem is the CRF model with lexical, gazetteer-based, and informativeness-based features. We decided to use the BEGIN-INSIDE-OUTSIDE annotation scheme for the CRF model, although this scheme does not account for token-sharing disorder mentions. Thus, we apply a set of postprocessing rules to derive disorder mentions from token-level outputs produced by the CRF model and to handle most frequent cases of token-sharing mentions (e.g., “abdomen nondisturbed and non-distended”). 3.1.1 Features We feed the CRF model with a rich set </context>
</contexts>
<marker>Stearns, Price, Spackman, Wang, 2001</marker>
<rawString>Michael Q. Stearns, Colin Price, Kent A. Spackman, and Amy Y. Wang. 2001. SNOMED Clinical Terms: Overview of the development process and project status. In Proceedings of the AMIA Symposium, page 662.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hanna Suominen</author>
<author>Sanna Salanter¨a</author>
<author>Sumithra Velupillai</author>
<author>Wendy W Chapman</author>
<author>Guergana Savova</author>
<author>Noemie Elhadad</author>
<author>Sameer Pradhan</author>
<author>Brett R South</author>
<author>L Danielle</author>
</authors>
<marker>Suominen, Salanter¨a, Velupillai, Chapman, Savova, Elhadad, Pradhan, South, Danielle, </marker>
<rawString>Hanna Suominen, Sanna Salanter¨a, Sumithra Velupillai, Wendy W. Chapman, Guergana Savova, Noemie Elhadad, Sameer Pradhan, Brett R. South, Danielle L.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mowery</author>
<author>Gareth J F Jones</author>
</authors>
<title>Overview of the ShARe/CLEF eHealth Evaluation Lab</title>
<date>2013</date>
<booktitle>In Information Access Evaluation: Multilinguality, Multimodality, and Visualization,</booktitle>
<pages>212--231</pages>
<marker>Mowery, Jones, 2013</marker>
<rawString>Mowery, and Gareth J.F. Jones. 2013. Overview of the ShARe/CLEF eHealth Evaluation Lab 2013. In Information Access Evaluation: Multilinguality, Multimodality, and Visualization, pages 212–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Buzhou Tang</author>
<author>Yonghui Wu</author>
<author>Min Jiang</author>
<author>Joshua C Denny</author>
<author>Hua Xu</author>
</authors>
<title>Recognizing and encoding discorder concepts in clinical text using machine learning and vector space model.</title>
<date>2013</date>
<booktitle>In Workshop of ShARe/CLEF eHealth Evaluation Lab</booktitle>
<contexts>
<context position="3821" citStr="Tang et al., 2013" startWordPosition="551" endWordPosition="554">http://takelab.fer.hr/mineral 389 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 389–393, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Clinical Information Extraction Clinical concept extraction is an essential task in medical natural language processing. While early approaches heavily relied on domain-specific vocabularies (Friedman et al., 1994; Aronson, 2001; Zeng et al., 2006), more recent efforts leverage the humanannotated corpora to develop machine learning models for the extraction of medical concepts (Tang et al., 2013; Uzuner et al., 2010). The rise in the number of data-driven efforts in the medical domain was particularly motivated by the shared tasks such as i2b2 challenges (Uzuner et al., 2010) and ShARe/CLEF eHealth Evaluation Lab (Suominen et al., 2013). The first subtask of the SemEval Task 14, in which we participated, was essentially the same as the first task in the ShARe/CLEF eHealth campaign. We did not participate in the second subtask on extracting arguments of disorder mentions. The best performing system of the ShARe/CLEF eHealth task on disorder extraction and normalization (Tang et al., 2</context>
</contexts>
<marker>Tang, Wu, Jiang, Denny, Xu, 2013</marker>
<rawString>Buzhou Tang, Yonghui Wu, Min Jiang, Joshua C. Denny, and Hua Xu. 2013. Recognizing and encoding discorder concepts in clinical text using machine learning and vector space model. In Workshop of ShARe/CLEF eHealth Evaluation Lab 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem Uzuner</author>
</authors>
<title>Imre Solti, and Eithon Cadag.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>5</issue>
<marker>Uzuner, 2010</marker>
<rawString>¨Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010. Extracting medication information from clinical text. Journal of the American Medical Informatics Association, 17(5):514–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing T Zeng</author>
<author>Sergey Goryachev</author>
<author>Scott Weiss</author>
<author>Margarita Sordo</author>
<author>Shawn N Murphy</author>
<author>Ross Lazarus</author>
</authors>
<title>Extracting principal diagnosis, co-morbidity and smoking status for asthma research: Evaluation of a natural language processing system.</title>
<date>2006</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="3671" citStr="Zeng et al., 2006" startWordPosition="527" endWordPosition="530">easure for concept normalization poses a viable solution for entity recognition in the clinical domain. We make the MINERAL system freely available.1 1http://takelab.fer.hr/mineral 389 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 389–393, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Clinical Information Extraction Clinical concept extraction is an essential task in medical natural language processing. While early approaches heavily relied on domain-specific vocabularies (Friedman et al., 1994; Aronson, 2001; Zeng et al., 2006), more recent efforts leverage the humanannotated corpora to develop machine learning models for the extraction of medical concepts (Tang et al., 2013; Uzuner et al., 2010). The rise in the number of data-driven efforts in the medical domain was particularly motivated by the shared tasks such as i2b2 challenges (Uzuner et al., 2010) and ShARe/CLEF eHealth Evaluation Lab (Suominen et al., 2013). The first subtask of the SemEval Task 14, in which we participated, was essentially the same as the first task in the ShARe/CLEF eHealth campaign. We did not participate in the second subtask on extract</context>
</contexts>
<marker>Zeng, Goryachev, Weiss, Sordo, Murphy, Lazarus, 2006</marker>
<rawString>Qing T. Zeng, Sergey Goryachev, Scott Weiss, Margarita Sordo, Shawn N. Murphy, and Ross Lazarus. 2006. Extracting principal diagnosis, co-morbidity and smoking status for asthma research: Evaluation of a natural language processing system. BMC Medical Informatics and Decision Making, 6(1):30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>