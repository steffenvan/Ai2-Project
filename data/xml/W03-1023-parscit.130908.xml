<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.977099">
Using the Web in Machine Learning for Other-Anaphora Resolution
</title>
<author confidence="0.985979">
Natalia N. Modjeska
</author>
<affiliation confidence="0.9992505">
School of Informatics
University of Edinburgh and
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.986082">
natalia@cs.utoronto.ca
</email>
<author confidence="0.993447">
Katja Markert
</author>
<affiliation confidence="0.99823725">
School of Computing
University of Leeds and
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.994743">
markert@inf.ed.ac.uk
</email>
<author confidence="0.989246">
Malvina Nissim
</author>
<affiliation confidence="0.9984375">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.987743">
mnissim@inf.ed.ac.uk
</email>
<bodyText confidence="0.980725361111111">
In (1), “eight other Soviet cities” refers to a set of So-
viet cities excluding Moscow, and can be rephrased
as “eight Soviet cities other than Moscow”. In (2),
“other schools” refers to a set of schools excluding
the mentioned Big Ten university. In (3), “other risk
factors for Mr. Cray’s company” refers to a set of
risk factors excluding the designer’s age.
In contrast, in list-contexts such as (4), the an-
tecedent is available both anaphorically and struc-
turally, as the left conjunct of the anaphor.2
(1) Research shows AZT can relieve dementia and
other symptoms in children [... ]
We focus on cases such as (1–3).
Section 2 describes a corpus of other-anaphors.
We present a machine learning approach to other-
anaphora, using a Naive Bayes (NB) classifier (Sec-
tion 3) with two different feature sets. In Section 4
we present the first feature set (F1) that includes
standard morpho-syntactic, recency, and string com-
parison features. However, there is evidence that,
e.g., syntactic features play a smaller role in resolv-
ing anaphors with full lexical heads than in pronom-
inal anaphora (Strube, 2002; Modjeska, 2002). In-
stead, a large and diverse amount of lexical or
world knowledge is necessary to understand exam-
ples such as (1–3), e.g., that Moscow is a (Soviet)
city, that universities are informally called schools
in American English and that age can be viewed as
a risk factor. Therefore we add lexical knowledge,
which is extracted from WordNet (Fellbaum, 1998)
and from a Named Entity (NE) Recognition algo-
rithm, to F1.
2Antecedents are also available structurally in constructions
“other than”, e.g., “few clients other than the state”. For a com-
putational treatment of “other” with structural antecedents see
(Bierner, 2001).
</bodyText>
<sectionHeader confidence="0.691871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944615384615">
We present a machine learning frame-
work for resolving other-anaphora. Be-
sides morpho-syntactic, recency, and se-
mantic features based on existing lexi-
cal knowledge resources, our algorithm
obtains additional semantic knowledge
from the Web. We search the Web via
lexico-syntactic patterns that are specific
to other-anaphors. Incorporating this in-
novative feature leads to an 11.4 percent-
age point improvement in the classifier’s
F-measure (25% improvement relative to
results without this feature).
</bodyText>
<sectionHeader confidence="0.975856" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.842777163265306">
Other-anaphors are referential NPs with the mod-
ifiers “other” or “another” and non-structural an-
tecedents:1
(1) An exhibition of American design and architec-
ture opened in September in Moscow and will
travel to eight other Soviet cities.
(2) [... ] the alumni director of a Big Ten university
“I’d love to see sports cut back and so would a
lot of my counterparts at other schools, [... ]”
(3) You either believe Seymour can do it again or
you don’t. Beside the designer’s age, other
risk factors for Mr. Cray’s company include
the Cray-3’s [... ] chip technology.
1All examples are from the Wall Street Journal; the correct
antecedents are in italics and the anaphors are in bold font.
The algorithm’s performance with this feature set
is encouraging. However, the semantic knowledge
the algorithm relies on is not sufficient for many
cases of other-anaphors (Section 4.2). Many expres-
sions, word senses and lexical relations are miss-
ing from WordNet. Whereas it includes Moscow
as a hyponym of city, so that the relation between
anaphor and antecedent in (1) can be retrieved, it
does not include the sense of school as university,
nor does it allow to infer that age is a risk factor.
There have been efforts to extract missing lexical
relations from corpora in order to build new knowl-
edge sources and enrich existing ones (Hearst, 1992;
Berland and Charniak, 1999; Poesio et al., 2002).3
However, the size of the used corpora still leads
to data sparseness (Berland and Charniak, 1999)
and the extraction procedure can therefore require
extensive smoothing. Moreover, some relations
should probably not be encoded in fixed context-
independent ontologies at all. Should, e.g., under-
specified and point-of-view dependent hyponymy
relations (Hearst, 1992) be included? Should age,
for example, be classified as a hyponym of risk fac-
tor independent of context?
Building on our previous work in (Markert et al.,
2003), we instead claim that the Web can be used
as a huge additional source of domain- and context-
independent, rich and up-to-date knowledge, with-
out having to build a fixed lexical knowledge base
(Section 5). We describe the benefit of integrating
Web frequency counts obtained for lexico-syntactic
patterns specific to other-anaphora as an additional
feature into our NB algorithm. This feature raises
the algorithm’s F-measure from 45.5% to 56.9%.
</bodyText>
<sectionHeader confidence="0.781708" genericHeader="introduction">
2 Data Collection and Preparation
</sectionHeader>
<bodyText confidence="0.999863555555556">
We collected 500 other-anaphors with NP an-
tecedents from the Wall Street Journal corpus (Penn
Treebank, release 2). This data sample excludes
several types of expressions containing “other”: (a)
list-contexts (Ex. 4) and other-than contexts (foot-
note 2), in which the antecedents are available struc-
turally and thus a relatively unsophisticated proce-
dure would suffice to find them; (b) idiomatic and
discourse connective “other”, e.g., “on the other
</bodyText>
<footnote confidence="0.7091735">
3In parallel, efforts have been made to enrich WordNet by
adding information in glosses (Harabagiu et al., 1999).
</footnote>
<bodyText confidence="0.999914789473684">
hand”, which are not anaphoric; and (c) reciprocal
“each other” and “one another”, elliptic phrases e.g.
“one X ... the other(s)” and one-anaphora, e.g., “the
other/another one”, which behave like pronouns and
thus would require a different search method. Also
excluded from the data set are samples of other-
anaphors with non-NP antecedents (e.g., adjectival
and nominal pre- and postmodifiers and clauses).
Each anaphor was extracted in a 5-sentence con-
text. The correct antecedents were manually an-
notated to create a training/test corpus. For each
anaphor, we automatically extracted a set of po-
tential NP antecedents as follows. First, we ex-
tracted all base NPs, i.e., NPs that contain no further
NPs within them. NPs containing a possessive NP
modifier, e.g., “Spain’s economy”, were split into a
possessor phrase, “Spain”, and a possessed entity,
“economy”. We then filtered out null elements and
lemmatised all antecedents and anaphors.
</bodyText>
<sectionHeader confidence="0.991463" genericHeader="method">
3 The Algorithm
</sectionHeader>
<bodyText confidence="0.999943863636364">
We use a Naive Bayes classifier, specifically the im-
plementation in the Weka ML library.4
The training data was generated following the
procedure employed by Soon et al. (2001) for
coreference resolution. Every pair of an anaphor
and its closest preceding antecedent created a pos-
itive training instance. To generate negative train-
ing instances, we paired anaphors with each of the
NPs that intervene between the anaphor and its an-
tecedent. This procedure produced a set of 3,084
antecedent-anaphor pairs, of which 500 (16%) were
positive training instances.
The classifier was trained and tested using 10-fold
cross-validation. We follow the general practice of
ML algorithms for coreference resolution and com-
pute precision (P), recall (R), and F-measure (F) on
all possible anaphor-antecedent pairs.
As a first approximation of the difficulty of our
task, we developed a simple rule-based baseline al-
gorithm which takes into account the fact that the
lemmatised head of an other-anaphor is sometimes
the same as that of its antecedent, as in (5).
</bodyText>
<footnote confidence="0.9651508">
4http://www.cs.waikato.ac.nz/—ml/weka/.
We also experimented with a decision tree classifier, with
Neural Networks and Support Vector Machines with Sequential
Minimal Optimization (SMO), all available from Weka. These
classifiers achieved worse results than NB on our data set.
</footnote>
<tableCaption confidence="0.994212">
Table 1: Feature set F1
</tableCaption>
<table confidence="0.990725736842105">
Type Feature Description Values
Gramm NP FORM Surface form (for all NPs) definite, indefinite, demonstrative, pronoun,
proper name, unknown
Match RESTR SUBSTR Does lemmatized antecedent string contain lemma- yes, no
tized anaphor string?
Syntactic GRAM FUNC Grammatical role (for all NPs) subject, predicative NP, dative object, direct
object, oblique, unknown
Syntactic SYN PAR Anaphor-antecedent agreement with respect to yes, no
grammatical function
Positional SDIST Distance between antecedent and anaphor in sen- 1, 2, 3, 4, 5
tences
Semantic SEMCLASS Semantic class (for all NPs) person, organization, location, date, money,
number, thing, abstract, unknown
Semantic SEMCLASS AGR Anaphor-antecedent agreement with respect to se- yes, no, unknown
mantic class
Semantic GENDER AGR Anaphor-antecedent agreement with respect to gen- same, compatible, incompatible, unknown
der
Semantic RELATION Type of relation between anaphor and antecedent same-predicate, hypernymy, meronymy,
compatible, incompatible, unknown
</table>
<bodyText confidence="0.986645714285714">
(5) These three countries aren’t completely off the
hook, though. They will remain on a lower-
priority list that includes other countries [... ]
For each anaphor, the baseline string-compares its
last (lemmatised) word with the last (lemmatised)
word of each of its possible antecedents. If the
words match, the corresponding antecedent is cho-
sen as the correct one. If several antecedents pro-
duce a match, the baseline chooses the most re-
cent one among them. If string-comparison returns
no antecedent, the baseline chooses the antecedent
closest to the anaphor among all antecedents. The
baseline assigns “yes” to exactly one antecedent per
anaphor. Its P, R and F-measure are 27.8%.
</bodyText>
<sectionHeader confidence="0.989541" genericHeader="method">
4 Naive Bayes without the Web
</sectionHeader>
<bodyText confidence="0.996787833333333">
First, we trained and tested the NB classifier with
a set of 9 features motivated by our own work on
other-anaphora (Modjeska, 2002) and previous ML
research on coreference resolution (Aone and Ben-
nett, 1995; McCarthy and Lehnert, 1995; Soon et
al., 2001; Ng and Cardie, 2002; Strube et al., 2002).
</bodyText>
<subsectionHeader confidence="0.88601">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.984892910714286">
A set of 9 features, F1, was automatically acquired
from the corpus and from additional external re-
sources (see summary in Table 1).
Non-semantic features. NP FORM is based on the
POS tags in the Wall Street Journal corpus and
heuristics. RESTR SUBSTR matches lemmatised
strings and checks whether the antecedent string
contains the anaphor string. This allows to resolve
examples such as “one woman ringer ... another
woman”. The values for GRAM FUNC were approxi-
mated from the parse trees and Penn Treebank anno-
tation. The feature SYN PAR captures syntactic par-
allelism between anaphor and antecedent. The fea-
ture SDIST measures the distance between anaphor
and antecedent in terms of sentences.5
Semantic features. GENDER AGR captures agree-
ment in gender between anaphor and antecedent,
gender having been determined using gazetteers,
kinship and occupational terms, titles, and Word-
Net. Four values are possible: “same”, if both NPs
have same gender; “compatible”, if antecedent and
anaphor have compatible gender, e.g., “lawyer ...
other women”; “incompatible”, e.g., “Mr. Johnson
... other women”; and “unknown”, if one of the
NPs is undifferentiated, i.e., the gender value is “un-
known”. SEMCLASS: Proper names were classified
using ANNIE, part of the GATE2 software package
(http://gate.ac.uk). Common nouns were
looked up in WordNet, considering only the most
frequent sense of each noun (the first sense in Word-
Net). In each case, the output was mapped onto one
of the values in Table 1. The SEMCLASS AGR fea-
5We also experimented with a feature MDIST that measures
intervening NP units. This feature worsened the overall perfor-
mance of the classifier.
ture compares the semantic class of the antecedent
with that of the anaphor NP and returns “yes” if
they belong to the same class; “no”, if they belong
to different classes; and “unknown” if the seman-
tic class of either the anaphor or antecedent has not
been determined. The RELATION between other-
anaphors and their antecedents can partially be de-
termined by string comparison (“same-predicate”)6
or WordNet (“hypernymy” and “meronymy”). As
other relations, e.g. “redescription” (Ex. (3), cannot
be readily determined on the basis of the information
in WordNet, the following values were used: “com-
patible”, for NPs with compatible semantic classes,
e.g., “woman ... other leaders”; and “incompati-
ble”, e.g., “woman ... other economic indicators”.
Compatibility can be defined along a variety of pa-
rameters. The notion we used roughly corresponds
to the root level of the WordNet hierarchy. Two
nouns are compatible if they have the same SEM-
CLASS value, e.g., “person”. “Unknown” was used
if the type of relation could not be determined.
</bodyText>
<sectionHeader confidence="0.776525" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.828383">
Table 2 shows the results for the Naive Bayes clas-
sifier using F1 in comparison to the baseline.
</bodyText>
<tableCaption confidence="0.998601">
Table 2: Results with F1
</tableCaption>
<table confidence="0.998757333333333">
Features P R F
baseline 27.8 27.8 27.8
F1 51.7 40.6 45.5
</table>
<bodyText confidence="0.996615">
Our algorithm performs significantly better than the
baseline.7 While these results are encouraging, there
were several classification errors.
Word sense ambiguity is one of the reasons for
misclassifications. Antecedents were looked up in
WordNet for their most frequent sense for a context-
independent assignment of the values of semantic
class and relations. However, in many cases either
the anaphor or antecedent or both are used in a sense
that is ranked as less frequent in Wordnet. This
might even be a quite frequent sense for a specific
corpus, e.g., the word “issue” in the sense of “shares,
stocks” in the WSJ. Therefore there is a strong inter-
</bodyText>
<footnote confidence="0.99568775">
6Same-predicate is not really a relation. We use it when the
head noun of the anaphor and antecedent are the same.
7We used a t-test with confidence level 0.05 for all signifi-
cance tests.
</footnote>
<bodyText confidence="0.999051166666666">
action between word sense disambiguation and ref-
erence resolution (see also (Preiss, 2002)).
Named Entity resolution is another weak link.
Several correct NE antecedents were classified as
“antecedent=no” (false negatives) because the NER
module assigned the wrong class to them.
The largest class of errors is however due to insuf-
ficient semantic knowledge. Problem examples can
roughly be classified into five partially overlapping
groups: (a) examples that suffer from gaps in Word-
Net, e.g., (2); (b) examples that require domain-,
situation-specific, or general world knowledge, e.g.,
(3); (c) examples involving bridging phenomena
(sometimes triggered by a metonymic or metaphoric
antecedent or anaphor), e.g., (6); (d) redescriptions
and paraphrases, often involving semantically vague
anaphors and/or antecedents, e.g., (7) and (3); and
(e) examples with ellipsis, e.g., (8).
</bodyText>
<listItem confidence="0.948599769230769">
(6) The Justice Department’s view is shared by
other lawyers [... ]
(7) While Mr. Dallara and Japanese officials say
the question of investors’ access to the U.S.
and Japanese markets may get a disproportion-
ate share of the public’s attention, a number of
other important economic issues will be on
the table at next week’s talks.
(8) He sees flashy sports as the only way the last-
place network can cut through the clutter of ca-
ble and VCRs, grab millions of new viewers
and tell them about other shows premiering a
few weeks later.
</listItem>
<bodyText confidence="0.9999362">
In (6), the antecedent is an organization-for-people
metonymy. In (7), the question of investors’ access
to the U.S. and Japanese markets is characterized as
an important economic issue. Also, the head “is-
sues” is lexically uninformative to sufficiently con-
strain the search space for the antecedent. In (8), the
antecedent is not the flashy sports, but rather flashy
sport shows, and thus an important piece of infor-
mation is omitted. Alternatively, the antecedent is a
content-for-container metonymy.
Overall, our approach misclassifies antecedents
whose relation to the other-anaphor is based on sim-
ilarity, property-sharing, causality, or is constrained
to a specific domain. These relation types are not —
and perhaps should not be — encoded in WordNet.
</bodyText>
<sectionHeader confidence="0.860596" genericHeader="method">
5 Naive Bayes with the Web
</sectionHeader>
<bodyText confidence="0.999986428571429">
With its approximately 3033M pages8 the Web is
the largest corpus available to the NLP community.
Building on our approach in (Markert et al., 2003),
we suggest using the Web as a knowledge source
for anaphora resolution. In this paper, we show how
to integrate Web counts for lexico-syntactic patterns
specific to other-anaphora into our ML approach.
</bodyText>
<subsectionHeader confidence="0.994392">
5.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.996871">
In the examples we consider, the relation between
anaphor and antecedent is implicitly expressed, i.e.,
anaphor and antecedent do not stand in a structural
relationship. However, they are linked by a strong
semantic relation that is likely to be structurally ex-
plicitly expressed in other texts. We exploit this in-
sight by adopting the following procedure:
</bodyText>
<listItem confidence="0.942589466666667">
1. In other-anaphora, a hyponymy/similarity rela-
tion between the lexical heads of anaphor and
antecedent is exploited or stipulated by the con-
text,9 e.g. that “schools“ is an alternative term
for universities in Ex. (2) or that age is viewed
as a risk factor in Ex. (3).
2. We select patterns that structurally explicitly
express the same lexical relations. E.g., the list-
context NP1 and other NP2 (as Ex. (4))
usually expresses hyponymy/similarity rela-
tions between the hyponym NP1 and its hyper-
nym NP2 (Hearst, 1992).
3. If the implicit lexical relationship between
anaphor and antecedent is strong, it is likely
that anaphor and antecedent also frequently
</listItem>
<bodyText confidence="0.920982285714286">
cooccur in the selected explicit patterns. We
instantiate the explicit pattern for all anaphor-
antecedent pairs. In (2) the pattern NP1
and other NP2 is instantiated with e.g.,
counterparts and other schools, sports
and other schools and universities and
other schools.10 These instantiations can be
</bodyText>
<footnote confidence="0.991051875">
8http://www.searchengineshowdown.com/
stats/sizeest.shtml, data from March 2003.
9In the Web feature context, we will often use
“anaphor/antecedent” instead of the more cumbersome
“lexical heads of the anaphor/antecedent”.
10These simplified instantiations serve as an example and are
neither exhaustive nor the final instantiations we use; see Sec-
tion 5.3.
</footnote>
<bodyText confidence="0.992714571428572">
searched in any corpus to determine their fre-
quencies. The rationale is that the most fre-
quent of these instantiated patterns is a good
clue for the correct antecedent.
4. As the patterns can be quite elaborate, most
corpora will be too small to determine the cor-
responding frequencies reliably. The instantia-
tion universities and other schools, e.g.,
does not occur at all in the British National Cor-
pus (BNC), a 100M words corpus of British
English.11 Therefore we use the largest corpus
available, the Web. We submit all instantiated
patterns as queries making use of the Google
API technology. Here, universities and
other schools yields over 700 hits, whereas
the other two instantiations yield under 10 hits
each. High frequencies do not only occur
for synonyms; the corresponding instantiation
for the correct antecedent in Ex. (3) age and
other risk factors yields over 400 hits on
the Web and again none in the BNC.
</bodyText>
<subsectionHeader confidence="0.998668">
5.2 Antecedent Preparation
</subsectionHeader>
<bodyText confidence="0.997477090909091">
In addition to the antecedent preparation described
in Section 2, further processing is necessary. First,
pronouns can be antecedents of other-anaphors but
they were not used as Web query input as they are
lexically empty. Second, all modification was elim-
inated and only the rightmost noun of compounds
was kept, to avoid data sparseness. Third, using pat-
terns containing NEs such as “Will Quinlan” in (9)
also leads to data sparseness (see also the use of NE
recognition for feature SEMCLASS).
(9) [... ] Will Quinlan had not inherited a damaged
retinoblastoma supressor gene and, therefore,
faced no more risk than other children [... ]
We resolved NEs in two steps. In addition
to GATE’s classification into ENAMEX and NU-
MEX categories, we used heuristics to automati-
cally obtain more fine-grained distinctions for the
categories LOCATION, ORGANIZATION, DATE and
MONEY, whenever possible. No further distinc-
tions were made for the category PERSON. We
classified LOCATIONS into COUNTRY, (US) STATE,
CITY, RIVER, LAKE and OCEAN, using mainly
</bodyText>
<page confidence="0.439376">
11http://info.ox.ac.uk/bnc
</page>
<tableCaption confidence="0.995003">
Table 3: Patterns and Instantiations for other-anaphora
</tableCaption>
<sectionHeader confidence="0.594363" genericHeader="method">
ANTECEDENT PATTERN INSTANTIATIONS
</sectionHeader>
<bodyText confidence="0.904252058823529">
common noun (O1): (N1fsgg OR N1fplg) and other N2fplg I1: “(university OR universities) and other schools”
proper name (O1): (N1fsgg OR N1fplg) and other N2fplg I1: “(person OR persons) and other children”
I�2: “(child OR children) and other persons”
(O2): N1 and other N2fplg I3: “Will Quinlan and other children”
gazetteers.12 If an entity classified by GATE as
ORGANIZATION contained an indication of the or-
ganization type, we used this as a subclassifica-
tion; therefore “Bank of America” is classified as
BANK. For DATE and MONEY entities we used
simple heuristics to classify them further into DAY,
MONTH, YEAR as well as DOLLAR.
From now on we call A the list of possible an-
tecedents and ana the anaphor. For (2), this list
is A2=fcounterpart, sport, universityg (the pronoun
“I” has been discarded) and ana2=school. For (9),
they are A9=frisk, gene, person [=Will Quinlanjg
and ana9 =child.
</bodyText>
<subsectionHeader confidence="0.931042">
5.3 Queries and Scoring Method
</subsectionHeader>
<bodyText confidence="0.991623">
We use the list-context pattern:13
</bodyText>
<equation confidence="0.856721">
(O1) (N1fsgg OR N1fplg) and other N2fplg
</equation>
<bodyText confidence="0.9706054">
For common noun antecedents, we instantiate the
pattern by substituting N1 with each possible an-
tecedent from set A, and N2 with ana, as normally
N1 is a hyponym of N2 in (O1), and the antecedent
is a hyponym of the anaphor. An instantiated pat-
tern for Ex. (2) is (university OR universities)
and other schools (Il in Table 3).14
For NE antecedents we instantiate (O1) by substi-
tuting N1 with the NE category of the antecedent,
and N2 with ana. An instantiated pattern for
Example (9) is (person OR persons) and other
children (Ip1 in Table 3). In this instantiation, N1
(“person”) is not a hyponym of N2 (“child”), instead
N2 is a hyponym of N1. This is a consequence of
the substitution of the antecedent (“Will Quinlan”)
</bodyText>
<footnote confidence="0.832024">
12They were extracted from the Web. Small gazetteers, con-
taining in all about 500 entries, are sufficient. This is the only
external knowledge collected for the Web feature.
13In all patterns in this paper, “OR” is the boolean operator,
“N1” and “N2” are variables, all other words are constants.
14Common noun instantiations are marked by a superscript
“c” and proper name instantiations by a superscript “p”.
</footnote>
<bodyText confidence="0.97533336">
with its NE category (“person”); such an instanti-
ation is not frequent, since it violates standard re-
lations within (O1). Therefore, we also instantiate
(O1) by substituting N1 with ana, and N2 with the
NE type of the antecedent (Ip2 in Table 3). Finally,
for NE antecedents, we use an additional pattern:
(O2) N1 and other N2fplg
which we instantiate by substituting N1 with the
original NE antecedent and N2 with ana (Ip3 in Ta-
ble 3).
Patterns and instantiations are summarised in Ta-
ble 3. We submit these instantiations as queries to
the Google search engine.
For each antecedent ant in A we obtain the raw
frequencies of all instantiations it occurs in (Il for
common nouns, or Ip1, Ip2, Ip3 for proper names) from
the Web, yielding freq(If), or freq(Ip1), freq(Ip2)
and freq(Ip3). We compute the maximum Mant
over these frequencies for proper names. For com-
mon nouns Mant corresponds to freq(If). The in-
stantiation yielding Mant is then called Imaxant.
Our scoring method takes into account the indi-
vidual frequencies of ant and ana by adapting mu-
tual information. We call the first part of Imaxant
(e.g. “university OR universities”, or “child OR chil-
</bodyText>
<listItem confidence="0.5604625">
dren”) Xant, and the second part (e.g. “schools”
or “persons”) Yant. We compute the probability of
Imaxant, Xant and Yant, using Google to determine
freq(Xant) and freq(Yant).
</listItem>
<equation confidence="0.966483285714286">
Mant
Pr(Imaxant) =
number of GOOGLE pages
freq(Xant)
Pr(Xant) = number of GOOGLE pages
freq(Yant)
Pr(Yant) = number of GOOGLE pages
</equation>
<bodyText confidence="0.996754">
We then compute the final score MIant.
</bodyText>
<subsectionHeader confidence="0.675327">
5.4 Integration into ML Framework and
Results
</subsectionHeader>
<bodyText confidence="0.99997746875">
For each anaphor, the antecedent in ,A with the
highest MIant gets feature value “webfirst”.15 All
other antecedents (including pronouns) get the fea-
ture value “webrest”. We chose this method instead
of e.g., giving score intervals for two reasons. First,
since score intervals are unique for each anaphor,
it is not straightforward to incorporate them into a
ML framework in a consistent manner. Second, this
method introduces an element of competition be-
tween several antecedents (see also (Connolly et al.,
1997)), which the individual scores do not reflect.
We trained and tested the NB classifier with the
feature set F1, plus the Web feature. The last row
in Table 4 shows the results. We obtained a 9.1 per-
centage point improvement in precision (an 18% im-
provement relative to the F1 feature set) and a 12.8
percentage point improvement in recall (32% im-
provement relative to F1), which amounts to an 11.4
percentage point improvement in F-measure (25%
improvement relative to F1 feature set). In particu-
lar, all the examples in this paper were resolved.
Our algorithm still misclassified several an-
tecedents. Sometimes even the Web is not large
enough to contain the instantiated pattern, espe-
cially when this is situation or speaker specific. An-
other problem is the high number of NE antecedents
(39.6%) in our corpus. While our NER module is
quite good, any errors in NE classification lead to
incorrect instantiations and thus to incorrect classi-
fications. In addition, the Web feature does not yet
take into account pronouns (7.43% of all correct and
potential antecedents in our corpus).
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
6 Related Work and Discussion
</sectionHeader>
<bodyText confidence="0.9996208">
Modjeska (2002) presented two hand-crafted algo-
rithms, SAL and LEX, which resolve the anaphoric
references of other-NPs on the basis of grammati-
cal salience and lexical information from WordNet,
respectively. In our own previous work (Markert et
</bodyText>
<footnote confidence="0.5993475">
15If several antecedents have the highest ML,,t they all get
value “webfirst”.
</footnote>
<tableCaption confidence="0.999035">
Table 4: Results with F1 and F1+Web
</tableCaption>
<table confidence="0.99870625">
Features P R F
baseline 27.8 27.8 27.8
F1 51.7 40.6 45.5
F1+Web 60.8 53.4 56.9
</table>
<bodyText confidence="0.999857725">
al., 2003) we presented a preliminary symbolic ap-
proach that uses Web counts and a recency-based
tie-breaker for resolution of other-anaphora and
bridging descriptions. (For another Web-based sym-
bolic approach to bridging see (Bunescu, 2003).)
The approach described in this paper is the first ma-
chine learning approach to other-anaphora. It is
not directly comparable to the symbolic approaches
above for two reasons. First, the approaches dif-
fer in the data and the evaluation metrics they used.
Second, our algorithm does not yet constitute a
full resolution procedure. As the classifier oper-
ates on the whole set of antecedent-anaphor pairs,
more than one potential antecedent for each anaphor
can be classified as “antecedent=yes”. This can
be amended by e.g. incremental processing. Also,
the classifier does not know that each other-NP is
anaphoric and therefore has an antecedent. (This
contrasts with e.g. definite NPs.) Thus, it can clas-
sify all antecedents as “antecedent=no”. This can be
remedied by using a back-off procedure, or a compe-
tition learning approach (Connolly et al., 1997). Fi-
nally, the full resolution procedure will have to take
into account other factors, e.g., syntactic constraints
on antecedent realization.
Our approach is the first ML approach to any kind
of anaphora that integrates the Web. Using the Web
as a knowledge source has considerable advantages.
First, the size of the Web almost eliminates the prob-
lem of data sparseness for our task. For this rea-
son, using the Web has proved successful in sev-
eral other fields of NLP, e.g., machine translation
(Grefenstette, 1999) and bigram frequency estima-
tion (Keller et al., 2002). In particular, (Keller et al.,
2002) have shown that using the Web handles data
sparseness better than smoothing. Second, we do
not process the returned Web pages in any way (tag-
ging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poe-
sio et al., 2002). Third, the linguistically motivated
patterns we use reduce long-distance dependencies
</bodyText>
<equation confidence="0.8630005">
MIant =log Pr(Xant)Pr(Yant)
Pr(Imaxant)
</equation>
<bodyText confidence="0.999823444444444">
between anaphor and antecedent to local dependen-
cies. By looking up these patterns on the Web we
obtain semantic information that is not and perhaps
should not be encoded in an ontology (redescrip-
tions, vague relations, etc.). Finally, these local de-
pendencies also reduce the need for prior word sense
disambiguation, as the anaphor and the antecedent
constrain each other’s sense within the context of the
pattern.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999991722222222">
We presented a machine learning approach to other-
anaphora, which uses a NB classifier and two sets
of features. The first set consists of standard
morpho-syntactic, recency, and semantic features
based on WordNet. The second set also incorpo-
rates semantic knowledge obtained from the Web via
lexico-semantic patterns specific to other-anaphora.
Adding this knowledge resulted in a dramatic im-
provement of 11.4% points in the classifier’s F-
measure, yielding a final F-measure of 56.9%.
To our knowledge, we are the first to integrate a
Web feature into a ML framework for anaphora reso-
lution. Adding this feature is inexpensive, solves the
data sparseness problem, and allows to handle ex-
amples with non-standard relations between anaphor
and antecedent. The approach is easily applicable to
other anaphoric phenomena by developing appropri-
ate lexico-syntactic patterns (Markert et al., 2003).
</bodyText>
<sectionHeader confidence="0.998362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999412">
Natalia N.Modjeska is supported by EPSRC grant
GR/M75129; Katja Markert by an Emmy Noether
Fellowship of the Deutsche Forschungsgemen-
schaft. We thank three anonymous reviewers for
helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947728813559">
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proc. ofACL’95, pages 122–129.
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. In Proc. ofACL’99, pages 57–64.
G. Bierner. 2001. Alternative phrases and natural lan-
guage information retrieval. In Proc. ofACL’01.
R. Bunescu. 2003. Associative anaphora resolution: A
Web-based approach. In R. Dale, K. van Deemter, and
R. Mitkov, editors, Proc. of the EACL Workshop on the
Computational Treatment ofAnaphora.
D. Connolly, J. D. Burger, and D. S. Day. 1997. A
machine learning approach to anaphoric reference. In
Daniel Jones and Harold Somers, editors, New Meth-
ods in Language Processing, pages 133–144. UCL
Press, London.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press.
G. Grefenstette. 1999. The WWW as a resource for
example-based MT tasks. In Proc. ofASLIB’99 Trans-
lating and the Computer 21, London.
S. Harabagiu, G. Miller, and D. Moldovan. 1999. Word-
net 2 - a morphologically and semantically enhanced
resource. In Proc. of SIGLEX-99, pages 1–8.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of COLING-92.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using the
Web to overcome data sparseness. In Proc. ofEMNLP
2002, pages 230–237.
K. Markert, M. Nissim, and N. N. Modjeska. 2003.
Using the Web for nominal anaphora resolution. In
R. Dale, K. van Deemter, and R. Mitkov, editors, Proc.
of the EACL Workshop on the Computational Treat-
ment ofAnaphora, pages 39–46.
J. F. McCarthy and W. G. Lehnert. 1995. Using decision
trees for coreference resolution. In Proc. ofIJCAI-95,
pages 1050–1055.
N. N. Modjeska. 2002. Lexical and grammatical role
constraints in resolving other-anaphora. In Proc. of
DAARC 2002, pages 129–134.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
ACL’02, pages 104–111.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Viera. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proc. of LREC 2002, pages
1220–1224.
J. Preiss. 2002. Anaphora resolution with word sense
disambiguation. In Proc. of SENSEVAL-2, pages 143–
146.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521–
544.
M. Strube, S. Rapp, and C. M¨uller. 2002. The influence
of minimum edit distance on reference resolution. In
Proc. ofEMNLP 2002, pages 312–319.
M. Strube. 2002. NLP approaches to reference resolu-
tion. Tutorial notes, ACL’02.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411217">
<title confidence="0.999922">Using the Web in Machine Learning for Other-Anaphora Resolution</title>
<author confidence="0.982536">N Natalia</author>
<affiliation confidence="0.99930675">School of University of Edinburgh Department of Computer University of</affiliation>
<email confidence="0.91003">natalia@cs.utoronto.ca</email>
<author confidence="0.988758">Katja Markert</author>
<affiliation confidence="0.9998525">School of University of Leeds and School of Informatics University of Edinburgh</affiliation>
<email confidence="0.989107">markert@inf.ed.ac.uk</email>
<author confidence="0.933013">Malvina</author>
<affiliation confidence="0.9948525">School of University of</affiliation>
<abstract confidence="0.987960352941177">mnissim@inf.ed.ac.uk In (1), “eight other Soviet cities” refers to a set of Socities and can be rephrased as “eight Soviet cities other than Moscow”. In (2), schools” refers to a set of schools the mentioned Big Ten university. In (3), “other risk factors for Mr. Cray’s company” refers to a set of factors designer’s age. In contrast, in list-contexts such as (4), the anis available both anaphorically and strucas the left conjunct of the Research shows AZT can relieve symptoms children [... ] We focus on cases such as (1–3). Section 2 describes a corpus of other-anaphors. We present a machine learning approach to otheranaphora, using a Naive Bayes (NB) classifier (Section 3) with two different feature sets. In Section 4 present the first feature set that includes standard morpho-syntactic, recency, and string comparison features. However, there is evidence that, e.g., syntactic features play a smaller role in resolving anaphors with full lexical heads than in pronominal anaphora (Strube, 2002; Modjeska, 2002). Instead, a large and diverse amount of lexical or world knowledge is necessary to understand examples such as (1–3), e.g., that Moscow is a (Soviet) city, that universities are informally called schools in American English and that age can be viewed as a risk factor. Therefore we add lexical knowledge, which is extracted from WordNet (Fellbaum, 1998) and from a Named Entity (NE) Recognition algoto are also available structurally in constructions “other than”, e.g., “few clients other than the state”. For a computational treatment of “other” with structural antecedents see (Bierner, 2001). Abstract We present a machine learning framework for resolving other-anaphora. Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web. We search the Web via lexico-syntactic patterns that are specific to other-anaphors. Incorporating this innovative feature leads to an 11.4 percentage point improvement in the classifier’s (25% improvement relative to results without this feature).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>S W Bennett</author>
</authors>
<title>Evaluating automated and manual acquisition of anaphora resolution strategies.</title>
<date>1995</date>
<booktitle>In Proc. ofACL’95,</booktitle>
<pages>122--129</pages>
<contexts>
<context position="9897" citStr="Aone and Bennett, 1995" startWordPosition="1526" endWordPosition="1530">match, the corresponding antecedent is chosen as the correct one. If several antecedents produce a match, the baseline chooses the most recent one among them. If string-comparison returns no antecedent, the baseline chooses the antecedent closest to the anaphor among all antecedents. The baseline assigns “yes” to exactly one antecedent per anaphor. Its P, R and F-measure are 27.8%. 4 Naive Bayes without the Web First, we trained and tested the NB classifier with a set of 9 features motivated by our own work on other-anaphora (Modjeska, 2002) and previous ML research on coreference resolution (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002). 4.1 Features A set of 9 features, F1, was automatically acquired from the corpus and from additional external resources (see summary in Table 1). Non-semantic features. NP FORM is based on the POS tags in the Wall Street Journal corpus and heuristics. RESTR SUBSTR matches lemmatised strings and checks whether the antecedent string contains the anaphor string. This allows to resolve examples such as “one woman ringer ... another woman”. The values for GRAM FUNC were approximated from the parse trees and </context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>C. Aone and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. In Proc. ofACL’95, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proc. ofACL’99,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="4053" citStr="Berland and Charniak, 1999" startWordPosition="640" endWordPosition="643">couraging. However, the semantic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that the Web can be used as a hug</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Proc. ofACL’99, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bierner</author>
</authors>
<title>Alternative phrases and natural language information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. ofACL’01.</booktitle>
<contexts>
<context position="2160" citStr="Bierner, 2001" startWordPosition="336" endWordPosition="337">ead, a large and diverse amount of lexical or world knowledge is necessary to understand examples such as (1–3), e.g., that Moscow is a (Soviet) city, that universities are informally called schools in American English and that age can be viewed as a risk factor. Therefore we add lexical knowledge, which is extracted from WordNet (Fellbaum, 1998) and from a Named Entity (NE) Recognition algorithm, to F1. 2Antecedents are also available structurally in constructions “other than”, e.g., “few clients other than the state”. For a computational treatment of “other” with structural antecedents see (Bierner, 2001). Abstract We present a machine learning framework for resolving other-anaphora. Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web. We search the Web via lexico-syntactic patterns that are specific to other-anaphors. Incorporating this innovative feature leads to an 11.4 percentage point improvement in the classifier’s F-measure (25% improvement relative to results without this feature). 1 Introduction Other-anaphors are referential NPs with the modifiers “other” or “another” </context>
</contexts>
<marker>Bierner, 2001</marker>
<rawString>G. Bierner. 2001. Alternative phrases and natural language information retrieval. In Proc. ofACL’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
</authors>
<title>Associative anaphora resolution: A Web-based approach.</title>
<date>2003</date>
<booktitle>Proc. of the EACL Workshop on the Computational Treatment ofAnaphora.</booktitle>
<editor>In R. Dale, K. van Deemter, and R. Mitkov, editors,</editor>
<contexts>
<context position="26093" citStr="Bunescu, 2003" startWordPosition="4141" endWordPosition="4142"> which resolve the anaphoric references of other-NPs on the basis of grammatical salience and lexical information from WordNet, respectively. In our own previous work (Markert et 15If several antecedents have the highest ML,,t they all get value “webfirst”. Table 4: Results with F1 and F1+Web Features P R F baseline 27.8 27.8 27.8 F1 51.7 40.6 45.5 F1+Web 60.8 53.4 56.9 al., 2003) we presented a preliminary symbolic approach that uses Web counts and a recency-based tie-breaker for resolution of other-anaphora and bridging descriptions. (For another Web-based symbolic approach to bridging see (Bunescu, 2003).) The approach described in this paper is the first machine learning approach to other-anaphora. It is not directly comparable to the symbolic approaches above for two reasons. First, the approaches differ in the data and the evaluation metrics they used. Second, our algorithm does not yet constitute a full resolution procedure. As the classifier operates on the whole set of antecedent-anaphor pairs, more than one potential antecedent for each anaphor can be classified as “antecedent=yes”. This can be amended by e.g. incremental processing. Also, the classifier does not know that each other-N</context>
</contexts>
<marker>Bunescu, 2003</marker>
<rawString>R. Bunescu. 2003. Associative anaphora resolution: A Web-based approach. In R. Dale, K. van Deemter, and R. Mitkov, editors, Proc. of the EACL Workshop on the Computational Treatment ofAnaphora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Connolly</author>
<author>J D Burger</author>
<author>D S Day</author>
</authors>
<title>A machine learning approach to anaphoric reference.</title>
<date>1997</date>
<booktitle>New Methods in Language Processing,</booktitle>
<pages>133--144</pages>
<editor>In Daniel Jones and Harold Somers, editors,</editor>
<publisher>UCL Press,</publisher>
<location>London.</location>
<contexts>
<context position="24296" citStr="Connolly et al., 1997" startWordPosition="3846" endWordPosition="3849">umber of GOOGLE pages We then compute the final score MIant. 5.4 Integration into ML Framework and Results For each anaphor, the antecedent in ,A with the highest MIant gets feature value “webfirst”.15 All other antecedents (including pronouns) get the feature value “webrest”. We chose this method instead of e.g., giving score intervals for two reasons. First, since score intervals are unique for each anaphor, it is not straightforward to incorporate them into a ML framework in a consistent manner. Second, this method introduces an element of competition between several antecedents (see also (Connolly et al., 1997)), which the individual scores do not reflect. We trained and tested the NB classifier with the feature set F1, plus the Web feature. The last row in Table 4 shows the results. We obtained a 9.1 percentage point improvement in precision (an 18% improvement relative to the F1 feature set) and a 12.8 percentage point improvement in recall (32% improvement relative to F1), which amounts to an 11.4 percentage point improvement in F-measure (25% improvement relative to F1 feature set). In particular, all the examples in this paper were resolved. Our algorithm still misclassified several antecedents</context>
<context position="26950" citStr="Connolly et al., 1997" startWordPosition="4275" endWordPosition="4278">on metrics they used. Second, our algorithm does not yet constitute a full resolution procedure. As the classifier operates on the whole set of antecedent-anaphor pairs, more than one potential antecedent for each anaphor can be classified as “antecedent=yes”. This can be amended by e.g. incremental processing. Also, the classifier does not know that each other-NP is anaphoric and therefore has an antecedent. (This contrasts with e.g. definite NPs.) Thus, it can classify all antecedents as “antecedent=no”. This can be remedied by using a back-off procedure, or a competition learning approach (Connolly et al., 1997). Finally, the full resolution procedure will have to take into account other factors, e.g., syntactic constraints on antecedent realization. Our approach is the first ML approach to any kind of anaphora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al.</context>
</contexts>
<marker>Connolly, Burger, Day, 1997</marker>
<rawString>D. Connolly, J. D. Burger, and D. S. Day. 1997. A machine learning approach to anaphoric reference. In Daniel Jones and Harold Somers, editors, New Methods in Language Processing, pages 133–144. UCL Press, London.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>The WWW as a resource for example-based MT tasks.</title>
<date>1999</date>
<booktitle>In Proc. ofASLIB’99 Translating and the Computer 21,</booktitle>
<location>London.</location>
<contexts>
<context position="27465" citStr="Grefenstette, 1999" startWordPosition="4362" endWordPosition="4363">an be remedied by using a back-off procedure, or a competition learning approach (Connolly et al., 1997). Finally, the full resolution procedure will have to take into account other factors, e.g., syntactic constraints on antecedent realization. Our approach is the first ML approach to any kind of anaphora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies MIant =log Pr(Xant)Pr(Yant) Pr(Imaxant) between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encode</context>
</contexts>
<marker>Grefenstette, 1999</marker>
<rawString>G. Grefenstette. 1999. The WWW as a resource for example-based MT tasks. In Proc. ofASLIB’99 Translating and the Computer 21, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>G Miller</author>
<author>D Moldovan</author>
</authors>
<title>Wordnet 2 - a morphologically and semantically enhanced resource.</title>
<date>1999</date>
<booktitle>In Proc. of SIGLEX-99,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5645" citStr="Harabagiu et al., 1999" startWordPosition="883" endWordPosition="886">sure from 45.5% to 56.9%. 2 Data Collection and Preparation We collected 500 other-anaphors with NP antecedents from the Wall Street Journal corpus (Penn Treebank, release 2). This data sample excludes several types of expressions containing “other”: (a) list-contexts (Ex. 4) and other-than contexts (footnote 2), in which the antecedents are available structurally and thus a relatively unsophisticated procedure would suffice to find them; (b) idiomatic and discourse connective “other”, e.g., “on the other 3In parallel, efforts have been made to enrich WordNet by adding information in glosses (Harabagiu et al., 1999). hand”, which are not anaphoric; and (c) reciprocal “each other” and “one another”, elliptic phrases e.g. “one X ... the other(s)” and one-anaphora, e.g., “the other/another one”, which behave like pronouns and thus would require a different search method. Also excluded from the data set are samples of otheranaphors with non-NP antecedents (e.g., adjectival and nominal pre- and postmodifiers and clauses). Each anaphor was extracted in a 5-sentence context. The correct antecedents were manually annotated to create a training/test corpus. For each anaphor, we automatically extracted a set of po</context>
</contexts>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>S. Harabagiu, G. Miller, and D. Moldovan. 1999. Wordnet 2 - a morphologically and semantically enhanced resource. In Proc. of SIGLEX-99, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of COLING-92.</booktitle>
<contexts>
<context position="4025" citStr="Hearst, 1992" startWordPosition="638" endWordPosition="639">ture set is encouraging. However, the semantic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that </context>
<context position="17180" citStr="Hearst, 1992" startWordPosition="2701" endWordPosition="2702">licitly expressed in other texts. We exploit this insight by adopting the following procedure: 1. In other-anaphora, a hyponymy/similarity relation between the lexical heads of anaphor and antecedent is exploited or stipulated by the context,9 e.g. that “schools“ is an alternative term for universities in Ex. (2) or that age is viewed as a risk factor in Ex. (3). 2. We select patterns that structurally explicitly express the same lexical relations. E.g., the listcontext NP1 and other NP2 (as Ex. (4)) usually expresses hyponymy/similarity relations between the hyponym NP1 and its hypernym NP2 (Hearst, 1992). 3. If the implicit lexical relationship between anaphor and antecedent is strong, it is likely that anaphor and antecedent also frequently cooccur in the selected explicit patterns. We instantiate the explicit pattern for all anaphorantecedent pairs. In (2) the pattern NP1 and other NP2 is instantiated with e.g., counterparts and other schools, sports and other schools and universities and other schools.10 These instantiations can be 8http://www.searchengineshowdown.com/ stats/sizeest.shtml, data from March 2003. 9In the Web feature context, we will often use “anaphor/antecedent” instead of </context>
<context position="27746" citStr="Hearst, 1992" startWordPosition="4409" endWordPosition="4410">ny kind of anaphora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies MIant =log Pr(Xant)Pr(Yant) Pr(Imaxant) between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encoded in an ontology (redescriptions, vague relations, etc.). Finally, these local dependencies also reduce the need for prior word sense disambiguation, as the anaphor and the antecedent constrain each other’s sense within the context of the pattern. 7 Conclusions We presented a mach</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of COLING-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
<author>O Ourioupina</author>
</authors>
<title>Using the Web to overcome data sparseness.</title>
<date>2002</date>
<booktitle>In Proc. ofEMNLP</booktitle>
<pages>230--237</pages>
<contexts>
<context position="27519" citStr="Keller et al., 2002" startWordPosition="4369" endWordPosition="4372">mpetition learning approach (Connolly et al., 1997). Finally, the full resolution procedure will have to take into account other factors, e.g., syntactic constraints on antecedent realization. Our approach is the first ML approach to any kind of anaphora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies MIant =log Pr(Xant)Pr(Yant) Pr(Imaxant) between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encoded in an ontology (redescriptions, vague relations, etc</context>
</contexts>
<marker>Keller, Lapata, Ourioupina, 2002</marker>
<rawString>F. Keller, M. Lapata, and O. Ourioupina. 2002. Using the Web to overcome data sparseness. In Proc. ofEMNLP 2002, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
<author>N N Modjeska</author>
</authors>
<title>Using the Web for nominal anaphora resolution. In</title>
<date>2003</date>
<booktitle>Proc. of the EACL Workshop on the Computational Treatment ofAnaphora,</booktitle>
<pages>39--46</pages>
<editor>R. Dale, K. van Deemter, and R. Mitkov, editors,</editor>
<contexts>
<context position="4601" citStr="Markert et al., 2003" startWordPosition="723" endWordPosition="726">ources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that the Web can be used as a huge additional source of domain- and contextindependent, rich and up-to-date knowledge, without having to build a fixed lexical knowledge base (Section 5). We describe the benefit of integrating Web frequency counts obtained for lexico-syntactic patterns specific to other-anaphora as an additional feature into our NB algorithm. This feature raises the algorithm’s F-measure from 45.5% to 56.9%. 2 Data Collection and Preparation We collected 500 other-anaphors with NP antecedents from the Wall Street Journal corpus (Penn Treebank, release 2). Thi</context>
<context position="16086" citStr="Markert et al., 2003" startWordPosition="2522" endWordPosition="2525">antecedent is not the flashy sports, but rather flashy sport shows, and thus an important piece of information is omitted. Alternatively, the antecedent is a content-for-container metonymy. Overall, our approach misclassifies antecedents whose relation to the other-anaphor is based on similarity, property-sharing, causality, or is constrained to a specific domain. These relation types are not — and perhaps should not be — encoded in WordNet. 5 Naive Bayes with the Web With its approximately 3033M pages8 the Web is the largest corpus available to the NLP community. Building on our approach in (Markert et al., 2003), we suggest using the Web as a knowledge source for anaphora resolution. In this paper, we show how to integrate Web counts for lexico-syntactic patterns specific to other-anaphora into our ML approach. 5.1 Basic Idea In the examples we consider, the relation between anaphor and antecedent is implicitly expressed, i.e., anaphor and antecedent do not stand in a structural relationship. However, they are linked by a strong semantic relation that is likely to be structurally explicitly expressed in other texts. We exploit this insight by adopting the following procedure: 1. In other-anaphora, a </context>
</contexts>
<marker>Markert, Nissim, Modjeska, 2003</marker>
<rawString>K. Markert, M. Nissim, and N. N. Modjeska. 2003. Using the Web for nominal anaphora resolution. In R. Dale, K. van Deemter, and R. Mitkov, editors, Proc. of the EACL Workshop on the Computational Treatment ofAnaphora, pages 39–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F McCarthy</author>
<author>W G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proc. ofIJCAI-95,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="9925" citStr="McCarthy and Lehnert, 1995" startWordPosition="1531" endWordPosition="1534"> antecedent is chosen as the correct one. If several antecedents produce a match, the baseline chooses the most recent one among them. If string-comparison returns no antecedent, the baseline chooses the antecedent closest to the anaphor among all antecedents. The baseline assigns “yes” to exactly one antecedent per anaphor. Its P, R and F-measure are 27.8%. 4 Naive Bayes without the Web First, we trained and tested the NB classifier with a set of 9 features motivated by our own work on other-anaphora (Modjeska, 2002) and previous ML research on coreference resolution (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002). 4.1 Features A set of 9 features, F1, was automatically acquired from the corpus and from additional external resources (see summary in Table 1). Non-semantic features. NP FORM is based on the POS tags in the Wall Street Journal corpus and heuristics. RESTR SUBSTR matches lemmatised strings and checks whether the antecedent string contains the anaphor string. This allows to resolve examples such as “one woman ringer ... another woman”. The values for GRAM FUNC were approximated from the parse trees and Penn Treebank annotation. Th</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. F. McCarthy and W. G. Lehnert. 1995. Using decision trees for coreference resolution. In Proc. ofIJCAI-95, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N N Modjeska</author>
</authors>
<title>Lexical and grammatical role constraints in resolving other-anaphora.</title>
<date>2002</date>
<booktitle>In Proc. of DAARC</booktitle>
<pages>129--134</pages>
<contexts>
<context position="1540" citStr="Modjeska, 2002" startWordPosition="237" endWordPosition="238">1) Research shows AZT can relieve dementia and other symptoms in children [... ] We focus on cases such as (1–3). Section 2 describes a corpus of other-anaphors. We present a machine learning approach to otheranaphora, using a Naive Bayes (NB) classifier (Section 3) with two different feature sets. In Section 4 we present the first feature set (F1) that includes standard morpho-syntactic, recency, and string comparison features. However, there is evidence that, e.g., syntactic features play a smaller role in resolving anaphors with full lexical heads than in pronominal anaphora (Strube, 2002; Modjeska, 2002). Instead, a large and diverse amount of lexical or world knowledge is necessary to understand examples such as (1–3), e.g., that Moscow is a (Soviet) city, that universities are informally called schools in American English and that age can be viewed as a risk factor. Therefore we add lexical knowledge, which is extracted from WordNet (Fellbaum, 1998) and from a Named Entity (NE) Recognition algorithm, to F1. 2Antecedents are also available structurally in constructions “other than”, e.g., “few clients other than the state”. For a computational treatment of “other” with structural antecedents</context>
<context position="9822" citStr="Modjeska, 2002" startWordPosition="1517" endWordPosition="1518">(lemmatised) word of each of its possible antecedents. If the words match, the corresponding antecedent is chosen as the correct one. If several antecedents produce a match, the baseline chooses the most recent one among them. If string-comparison returns no antecedent, the baseline chooses the antecedent closest to the anaphor among all antecedents. The baseline assigns “yes” to exactly one antecedent per anaphor. Its P, R and F-measure are 27.8%. 4 Naive Bayes without the Web First, we trained and tested the NB classifier with a set of 9 features motivated by our own work on other-anaphora (Modjeska, 2002) and previous ML research on coreference resolution (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002). 4.1 Features A set of 9 features, F1, was automatically acquired from the corpus and from additional external resources (see summary in Table 1). Non-semantic features. NP FORM is based on the POS tags in the Wall Street Journal corpus and heuristics. RESTR SUBSTR matches lemmatised strings and checks whether the antecedent string contains the anaphor string. This allows to resolve examples such as “one woman ringer ... another w</context>
<context position="25427" citStr="Modjeska (2002)" startWordPosition="4036" endWordPosition="4037">s in this paper were resolved. Our algorithm still misclassified several antecedents. Sometimes even the Web is not large enough to contain the instantiated pattern, especially when this is situation or speaker specific. Another problem is the high number of NE antecedents (39.6%) in our corpus. While our NER module is quite good, any errors in NE classification lead to incorrect instantiations and thus to incorrect classifications. In addition, the Web feature does not yet take into account pronouns (7.43% of all correct and potential antecedents in our corpus). 6 Related Work and Discussion Modjeska (2002) presented two hand-crafted algorithms, SAL and LEX, which resolve the anaphoric references of other-NPs on the basis of grammatical salience and lexical information from WordNet, respectively. In our own previous work (Markert et 15If several antecedents have the highest ML,,t they all get value “webfirst”. Table 4: Results with F1 and F1+Web Features P R F baseline 27.8 27.8 27.8 F1 51.7 40.6 45.5 F1+Web 60.8 53.4 56.9 al., 2003) we presented a preliminary symbolic approach that uses Web counts and a recency-based tie-breaker for resolution of other-anaphora and bridging descriptions. (For a</context>
</contexts>
<marker>Modjeska, 2002</marker>
<rawString>N. N. Modjeska. 2002. Lexical and grammatical role constraints in resolving other-anaphora. In Proc. of DAARC 2002, pages 129–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="9965" citStr="Ng and Cardie, 2002" startWordPosition="1539" endWordPosition="1542">veral antecedents produce a match, the baseline chooses the most recent one among them. If string-comparison returns no antecedent, the baseline chooses the antecedent closest to the anaphor among all antecedents. The baseline assigns “yes” to exactly one antecedent per anaphor. Its P, R and F-measure are 27.8%. 4 Naive Bayes without the Web First, we trained and tested the NB classifier with a set of 9 features motivated by our own work on other-anaphora (Modjeska, 2002) and previous ML research on coreference resolution (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002). 4.1 Features A set of 9 features, F1, was automatically acquired from the corpus and from additional external resources (see summary in Table 1). Non-semantic features. NP FORM is based on the POS tags in the Wall Street Journal corpus and heuristics. RESTR SUBSTR matches lemmatised strings and checks whether the antecedent string contains the anaphor string. This allows to resolve examples such as “one woman ringer ... another woman”. The values for GRAM FUNC were approximated from the parse trees and Penn Treebank annotation. The feature SYN PAR captures syntactic par</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. of ACL’02, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>T Ishikawa</author>
<author>S Schulte im Walde</author>
<author>R Viera</author>
</authors>
<title>Acquiring lexical knowledge for anaphora resolution.</title>
<date>2002</date>
<booktitle>In Proc. of LREC</booktitle>
<pages>1220--1224</pages>
<contexts>
<context position="4075" citStr="Poesio et al., 2002" startWordPosition="644" endWordPosition="647">ntic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that the Web can be used as a huge additional source of</context>
<context position="27768" citStr="Poesio et al., 2002" startWordPosition="4411" endWordPosition="4415">phora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies MIant =log Pr(Xant)Pr(Yant) Pr(Imaxant) between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encoded in an ontology (redescriptions, vague relations, etc.). Finally, these local dependencies also reduce the need for prior word sense disambiguation, as the anaphor and the antecedent constrain each other’s sense within the context of the pattern. 7 Conclusions We presented a machine learning approach </context>
</contexts>
<marker>Poesio, Ishikawa, Walde, Viera, 2002</marker>
<rawString>M. Poesio, T. Ishikawa, S. Schulte im Walde, and R. Viera. 2002. Acquiring lexical knowledge for anaphora resolution. In Proc. of LREC 2002, pages 1220–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
</authors>
<title>Anaphora resolution with word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of SENSEVAL-2,</booktitle>
<pages>143--146</pages>
<contexts>
<context position="13826" citStr="Preiss, 2002" startWordPosition="2168" endWordPosition="2169"> values of semantic class and relations. However, in many cases either the anaphor or antecedent or both are used in a sense that is ranked as less frequent in Wordnet. This might even be a quite frequent sense for a specific corpus, e.g., the word “issue” in the sense of “shares, stocks” in the WSJ. Therefore there is a strong inter6Same-predicate is not really a relation. We use it when the head noun of the anaphor and antecedent are the same. 7We used a t-test with confidence level 0.05 for all significance tests. action between word sense disambiguation and reference resolution (see also (Preiss, 2002)). Named Entity resolution is another weak link. Several correct NE antecedents were classified as “antecedent=no” (false negatives) because the NER module assigned the wrong class to them. The largest class of errors is however due to insufficient semantic knowledge. Problem examples can roughly be classified into five partially overlapping groups: (a) examples that suffer from gaps in WordNet, e.g., (2); (b) examples that require domain-, situation-specific, or general world knowledge, e.g., (3); (c) examples involving bridging phenomena (sometimes triggered by a metonymic or metaphoric ante</context>
</contexts>
<marker>Preiss, 2002</marker>
<rawString>J. Preiss. 2002. Anaphora resolution with word sense disambiguation. In Proc. of SENSEVAL-2, pages 143– 146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>544</pages>
<contexts>
<context position="6783" citStr="Soon et al. (2001)" startWordPosition="1062" endWordPosition="1065">a training/test corpus. For each anaphor, we automatically extracted a set of potential NP antecedents as follows. First, we extracted all base NPs, i.e., NPs that contain no further NPs within them. NPs containing a possessive NP modifier, e.g., “Spain’s economy”, were split into a possessor phrase, “Spain”, and a possessed entity, “economy”. We then filtered out null elements and lemmatised all antecedents and anaphors. 3 The Algorithm We use a Naive Bayes classifier, specifically the implementation in the Weka ML library.4 The training data was generated following the procedure employed by Soon et al. (2001) for coreference resolution. Every pair of an anaphor and its closest preceding antecedent created a positive training instance. To generate negative training instances, we paired anaphors with each of the NPs that intervene between the anaphor and its antecedent. This procedure produced a set of 3,084 antecedent-anaphor pairs, of which 500 (16%) were positive training instances. The classifier was trained and tested using 10-fold cross-validation. We follow the general practice of ML algorithms for coreference resolution and compute precision (P), recall (R), and F-measure (F) on all possible</context>
<context position="9944" citStr="Soon et al., 2001" startWordPosition="1535" endWordPosition="1538"> correct one. If several antecedents produce a match, the baseline chooses the most recent one among them. If string-comparison returns no antecedent, the baseline chooses the antecedent closest to the anaphor among all antecedents. The baseline assigns “yes” to exactly one antecedent per anaphor. Its P, R and F-measure are 27.8%. 4 Naive Bayes without the Web First, we trained and tested the NB classifier with a set of 9 features motivated by our own work on other-anaphora (Modjeska, 2002) and previous ML research on coreference resolution (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002). 4.1 Features A set of 9 features, F1, was automatically acquired from the corpus and from additional external resources (see summary in Table 1). Non-semantic features. NP FORM is based on the POS tags in the Wall Street Journal corpus and heuristics. RESTR SUBSTR matches lemmatised strings and checks whether the antecedent string contains the anaphor string. This allows to resolve examples such as “one woman ringer ... another woman”. The values for GRAM FUNC were approximated from the parse trees and Penn Treebank annotation. The feature SYN PAR c</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>S Rapp</author>
<author>C M¨uller</author>
</authors>
<title>The influence of minimum edit distance on reference resolution.</title>
<date>2002</date>
<booktitle>In Proc. ofEMNLP</booktitle>
<pages>312--319</pages>
<marker>Strube, Rapp, M¨uller, 2002</marker>
<rawString>M. Strube, S. Rapp, and C. M¨uller. 2002. The influence of minimum edit distance on reference resolution. In Proc. ofEMNLP 2002, pages 312–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
</authors>
<title>NLP approaches to reference resolution. Tutorial notes,</title>
<date>2002</date>
<pages>02</pages>
<contexts>
<context position="1523" citStr="Strube, 2002" startWordPosition="235" endWordPosition="236">he anaphor.2 (1) Research shows AZT can relieve dementia and other symptoms in children [... ] We focus on cases such as (1–3). Section 2 describes a corpus of other-anaphors. We present a machine learning approach to otheranaphora, using a Naive Bayes (NB) classifier (Section 3) with two different feature sets. In Section 4 we present the first feature set (F1) that includes standard morpho-syntactic, recency, and string comparison features. However, there is evidence that, e.g., syntactic features play a smaller role in resolving anaphors with full lexical heads than in pronominal anaphora (Strube, 2002; Modjeska, 2002). Instead, a large and diverse amount of lexical or world knowledge is necessary to understand examples such as (1–3), e.g., that Moscow is a (Soviet) city, that universities are informally called schools in American English and that age can be viewed as a risk factor. Therefore we add lexical knowledge, which is extracted from WordNet (Fellbaum, 1998) and from a Named Entity (NE) Recognition algorithm, to F1. 2Antecedents are also available structurally in constructions “other than”, e.g., “few clients other than the state”. For a computational treatment of “other” with struc</context>
</contexts>
<marker>Strube, 2002</marker>
<rawString>M. Strube. 2002. NLP approaches to reference resolution. Tutorial notes, ACL’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>