<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012083">
<title confidence="0.7027465">
Memory-Based Morphological Analysis
Antal van den Bosch and Walter Daelemans
</title>
<author confidence="0.747075">
ILK / Computational Linguistics
</author>
<affiliation confidence="0.861098">
Tilburg University
</affiliation>
<email confidence="0.860797">
fantalb,walterl@kub.n11
</email>
<sectionHeader confidence="0.995349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999731909090909">
We present a general architecture for efficient
and deterministic morphological analysis based
on memory-based learning, and apply it to
morphological analysis of Dutch. The system
makes direct mappings from letters in context
to rich categories that encode morphological
boundaries, syntactic class labels, and spelling
changes. Both precision and recall of labeled
morphemes are over 84% on held-out dictionary
test words and estimated to be over 93% in free
text.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905833333333">
Morphological analysis is an essential compo-
nent in language engineering applications rang-
ing from spelling error correction to machine
translation. Performing a full morphological
analysis of a wordform is usually regarded as a
segmentation of the word into morphemes, com-
bined with an analysis of the interaction of these
morphemes that determine the syntactic class
of the wordform as a whole. The complexity of
wordform morphology varies widely among the
world&apos;s languages, but is regarded quite high
even in the relatively simple cases, such as En-
glish. Many wordforms in English and other
western languages contain ambiguities in their
morphological composition that can be quite in-
tricate. General classes of linguistic knowledge
that are usually assumed to play a role in this
disambiguation process are knowledge of (i) the
morphemes of a language, (ii) the morphotac-
tics, i.e., constraints on how morphemes are al-
lowed to attach, and (iii) spelling changes that
can occur due to morpheme attachment.
State-of-the art systems for morphological
analysis of wordforms are usually based on
two-level finite-state transducers (FSTS, Kosken-
niemi (1983)). Even with the availability of
sophisticated development tools, the cost and
complexity of hand-crafting two-level rules is
high, and the representation of concatenative
compound morphology with continuation lexi-
cons is difficult. As in parsing, there is a trade-
off between coverage and spurious ambiguity in
these systems: the more sophisticated the rules
become, the more needless ambiguity they in-
troduce.
In this paper we present a learning approach
which models morphological analysis (includ-
ing compounding) of complex wordforms as se-
quences of classification tasks. Our model,
MBMA (Memory-Based Morphological Analy-
sis), is a memory-based learning system (Stan-
fill and Waltz, 1986; Daelemans et al., 1997).
Memory-based learning is a class of induc-
tive, supervised machine learning algorithms
that learn by storing examples of a task in
memory. Computational effort is invested on
a &amp;quot;call-by-need&amp;quot; basis for solving new exam-
ples (henceforth called instances) of the same
task. When new instances are presented to a
memory-based learner, it searches for the best-
matching instances in memory, according to a
task-dependent similarity metric. When it has
found the best matches (the nearest neighbors),
it transfers their solution (classification, label)
to the new instance. Memory-based learn-
ing has been shown to be quite adequate for
various natural-language processing tasks such
as stress assignment (Daelemans et al., 1994),
grapheme—phoneme conversion (Daelemans and
Van den Bosch, 1996; Van den Bosch, 1997),
and part-of-speech tagging (Daelemans et al.,
1996b).
The paper is structured as follows. First, we
give a brief overview of Dutch morphology in
Section 2. We then turn to a description of
MBMA in Section 3. In Section 4 we present
</bodyText>
<page confidence="0.996118">
285
</page>
<bodyText confidence="0.999921">
the experimental outcomes of our study with
MBMA. Section 5 summarizes our findings, re-
ports briefly on a partial study of English show-
ing that the approach is applicable to other lan-
guages, and lists our conclusions.
</bodyText>
<sectionHeader confidence="0.960752" genericHeader="method">
2 Dutch Morphology
</sectionHeader>
<bodyText confidence="0.999915046511628">
The processes of Dutch morphology include
inflection, derivation, and compounding. In-
flection of verbs, adjectives, and nouns is
mostly achieved by suffixation, but a circum-
fix also occurs in the Dutch past participle (e.g.
ge+werk+t as the past participle of verb werken,
to work). Irregular inflectional morphology is
due to relics of ablaut (vowel change) and to
suppletion (mixing of different roots in inflec-
tional paradigms). Processes of derivation in
Dutch morphology occur by means of prefixa-
tion and suffixation. Derivation can change the
syntactic class of wordforms. Compounding in
Dutch is concatenative (as in German and Scan-
dinavian languages): words can be strung to-
gether almost unlimitedly, with only a few mor-
photactic constraints, e.g., rechtsinformatica-
toepassingen (applications of computer science
in Law). In general, a complex wordform inher-
its its syntactic properties from its right-most
part (the head). Several spelling changes occur:
apart from the closed set of spelling changes due
to irregular morphology, a number of spelling
changes is predictably due to morphological
context. The spelling of long vowels varies be-
tween double and single (e.g. ik loop, I run,
versus wij lop+en, we run); the spelling of root-
final consonants can be doubled (e.g. ik stop,
I stop, versus wij stopp+en, we stop); there is
variation between s and z and f and v (e.g. huis,
house, versus huizen, houses). Finally, between
the parts of a compound, a linking morpheme
may appear (e.g. staat+s+loterij, state lottery).
For a detailed discussion of morphological phe-
nomena in Dutch, see De Haas and Trommelen
(1993). Previous approaches to Dutch morpho-
logical analysis have been based on finite-state
transducers (e.g., XEROX&apos;es morphological an-
alyzer), or on parsing with context-free word
grammars interleaved with exploration of pos-
sible spelling changes (e.g. Heemskerk and van
Heuven (1993); or see Heemskerk (1993) for a
probabilistic variant).
</bodyText>
<sectionHeader confidence="0.671956" genericHeader="method">
3 Applying memory-based learning
</sectionHeader>
<subsectionHeader confidence="0.900741">
to morphological analysis
</subsectionHeader>
<bodyText confidence="0.998037571428572">
Most linguistic problems can be seen ascontext-
sensitive mappings from one representation to
another (e.g., from text to speech; from a se-
quence of spelling words to a parse tree; from
a parse tree to logical form, from source lan-
guage to target language, etc.) (Daelemans,
1995). This is also the case for morphologi-
cal analysis. Memory-based learning algorithms
can learn mappings (classifications) if a suffi-
cient number of instances of these mappings is
presented to them.
We drew our instances from the CELEX lex-
ical data base (Baayen et al., 1993). CELEX
contains a large lexical data base of Dutch word-
forms, and features a full morphological analy-
sis for 247,415 of them. We took each wordform
and its associated analysis, and created task in-
stances using a windowing method (Sejnowski
and Rosenberg, 1987). Windowing transforms
each wordform into as many instances as it has
letters. Each example focuses on one letter,
and includes a fixed number of left and right
neighbor letters, chosen here to be five. Con-
sequently, each instance spans eleven letters,
which is also the average word length in the
CELEX data base. Moreover, we estimated
from exploratory data analysis that this con-
text would contain enough information to allow
for adequate disambiguation.
To illustrate the construction of instances,
Table 1 displays the 15 instances derived from
the Dutch example word abnormaliteiten (ab-
normalities) and their associated classes. The
class of the first instance is &amp;quot;A+Da&amp;quot;, which
says that (i) the morpheme starting in a is an
adjective (&amp;quot;A&amp;quot;)1, and (ii) an a was deleted at
the end (&amp;quot;+Da&amp;quot;). The coding thus tells that
the first morpheme is the adjective abnormaal.
The second morpheme, iteit, has class &amp;quot;N_A*&amp;quot;
This complex tag indicates that when iteit at-
taches right to an adjective (encoded by &amp;quot;A*&amp;quot;),
the new combination becomes a noun (&amp;quot;N_&amp;quot;).
Finally, the third morpheme is en, which is a
plural inflection (labeled &amp;quot;m&amp;quot; in CELEX). This
way we generated an instance base of 2,727,462
10ELEX features ten syntactic tags: noun (N), adjec-
tive (A), quantifier/numeral (Q), verb (V), article (D),
pronoun (0), adverb (B), preposition (P), conjunction
(C), interjection (J), and abbreviation (X).
</bodyText>
<page confidence="0.987092">
286
</page>
<bodyText confidence="0.999799129032258">
instances. Within these instances, 2422 differ-
ent class labels occur. The most frequently oc-
curring class label is &amp;quot;0&amp;quot;, occurring in 72.5% of
all instances. The three most frequent non-null
labels are &amp;quot;N&amp;quot; (6.9%), &amp;quot;V&amp;quot; (3.6%), and &amp;quot;m&amp;quot;
(1.6%). Most class labels combine a syntactic
or inflectional tag with a spelling change, and
generally have a low frequency.
When a wordform is listed in CELEX as hav-
ing more than one possible morphological la-
beling (e.g., a morpheme may be N or V, the
inflection -en may be plural for nouns or infini-
tive for verbs), these labels are joined into am-
biguous classes (&amp;quot;N/V&amp;quot;) and the first generated
example is labeled with this ambiguous class.
Ambiguity in syntactic and inflectional tags oc-
curs in 3.6% of all morphemes in our CELEX
data.
The memory-based learning algorithm used
within MBMA is n31-IG (Daelemans and Van
den Bosch, 1992; Daelemans et al., 1997), an
extension of IB 1 (Aha et al., 1991). IB 1- IG con-
structs a data base of instances in memory dur-
ing learning. New instances are classified by
ml-IG by matching them to all instances in
the instance base, and calculating with each
match the distance between the new instance
•X and the memory instance Y, A(X, Y) =
E7i1_1 W(fi)(5(xi, yi), where W( f i) is the weight
of the ith feature, and 8(x2, yi) is the distance
between the values of the ith feature in in-
stances X and Y. When the values of the in-
stance features are symbolic, as with our linguis-
tic tasks, the simple overlap distance function
is used: 6(x1, y2) 0 if x, = yi, else 1. The
(most frequently occurring) classification of the
memory instance Y with the smallest X, Y)
is then taken as the classification of X.
The weighting function W(f2) computes for
each feature, over the full instance base, its
information gain, a function from information
theory; cf. Quinlan (1986). In short, the infor-
mation gain of a feature expresses its relative
importance compared to the other features in
performing the mapping from input to classi-
fication. When information gain is used in the
similarity function, instances that match on im-
portant features are regarded as more alike than
instances that match on unimportant features.
In our experiments, we are primarily inter-
ested in the generalization accuracy of trained
models, i.e., the ability of these models to use
their accumulated knowledge to classify new
instances that were not in the training mate-
rial. A method that gives a good estimate
of the generalization performance of an algo-
rithm on a given instance base, is 10-fold cross-
validation (Weiss and Kulikowski, 1991). This
method generates on the basis of an instance
base 10 subsequent partitionings into a training
set (90%) and a test set (10%), resulting in 10
experiments.
</bodyText>
<sectionHeader confidence="0.981572" genericHeader="method">
4 Experiments: MBMA of Dutch
wordforms
</sectionHeader>
<bodyText confidence="0.999953294117647">
As described, we performed 10-fold cross vali-
dation experiments in an experimental matrix
in which MBMA is applied to the full instance
base, using a context width of five left and right
context letters. We structure the presentation
of the experimental outcomes as follows. First,
we give the generalization accuracies on test in-
stances and test words obtained in the exper-
iments, including measurements of generaliza-
tion accuracy when class labels are interpreted
at lower levels of granularity. While the latter
measures give a rough idea of system accuracy,
more insight is provided by two additional anal-
yses. First, precision and recall rates of mor-
phemes are given. We then provide prediction
accuracies of syntactic word classes. Finally, we
provide estimations on free-text accuracies.
</bodyText>
<subsectionHeader confidence="0.993924">
4.1 Generalization accuracies
</subsectionHeader>
<bodyText confidence="0.9999912">
The percentages of correctly classified test in-
stances are displayed in the top line of Table 2,
showing an error in test instances of about 4.1%
(which is markedly better than the baseline er-
ror of 27.5% when guessing the most frequent
class &amp;quot;0&amp;quot;), which translates in an error at the
word level of about 35%. The output of MBMA
can also be viewed at lower levels of granularity.
We have analyzed MBMA&apos;s output at the three
following lower granularity levels:
</bodyText>
<listItem confidence="0.638173375">
1. Only decide, per letter, whether a seg-
mentation occurs at that letter, and if so,
whether it marks the start of a derivational
stem or an inflection. This can be derived
straightforwardly from the full-task class
labeling.
2. Only decide, per letter, whether a segmen-
tation occurs at that letter. Again, this can
</listItem>
<page confidence="0.94915">
287
</page>
<figure confidence="0.757715294117647">
instance left focus right TASK
number context letter context
1 _ _ _ _ _ a n o rrmA+Da
2 _ _ _ _ a b n o r ma 0
3 _ _ _ a b n o r ma I 0
4 _ _ a bn o r ma I i 0
5 _ a bno r malit 0
6 a bno r m al I te 0
7 bnorm a I I tei 0
8 norma I i tei t 0
9 o rma I i tel teN_A*
10 r ma I I t ei t en 0
11 malit e i t e n _ 0
12 aIi te i t e n _ _ 0
13 I i tei t e n _ _ _ 0
14 i tei t e n _ _ _ _ m
15 tei te n _ _ _ _ _ 0
</figure>
<tableCaption confidence="0.8674485">
Table 1: Instances with morphological analysis classifications derived from abnormaliteiten, ana-
lyzed as [abnormaal]A[iteit]N_Ajen]m.
</tableCaption>
<bodyText confidence="0.997441571428571">
be derived straightforwardly. This task im-
plements segmentation of a complex word
form into morphemes.
3. Only check whether the desired spelling
change is predicted correctly. Because of
the irregularity of many spelling changes
this is a hard task.
The results from these analyses are displayed
in Table 2 under the top line. First, Ta-
ble 2 shows that performance on the lower-
granularity tasks that exclude detailed syntac-
tic labeling and spelling-change prediction is
about 1.1% on test instances, and roughly 10%
on test words. Second, making the distinction
between inflections and other morphemes is al-
most as easy as just determining whether there
is a boundary at all. Third, the relatively low
score on correctly predicted spelling changes,
80.95%, indicates that it is particularly hard
to generalize from stored instances of spelling
changes to new ones. This is in accordance with
the common linguistic view on spelling-change
exceptions. When, for instance, a past-tense
form of a verb involves a real exception (e.g.,
the past tense of Dutch brengen, to bring, is
bracht), it is often the case that this exception is
confined to generalize to only a few other exam-
ples of the same verb (brachten, gebracht) and
not to any other word that is not derived from
the same stem, while the memory-based learn-
ing approach is not aware of such constraints.
A post-processing step that checks whether the
proposed morphemes are also listed in a mor-
pheme lexicon would correct many of these er-
rors, but has not been included here.
</bodyText>
<subsectionHeader confidence="0.994047">
4.2 Precision and recall of morphemes
</subsectionHeader>
<bodyText confidence="0.999929111111111">
Precision is the percentage of morphemes pre-
dicted by MBMA that is actually a morpheme
in the target analysis; recall is the percentage
of morphemes in the target analysis that are
also predicted by MBMA. Precision and recall
of morphemes can again be computed at differ-
ent levels of granularity. Table 3 displays these
computed values. The results show that both
precision and recall of fully-labeled morphemes
within test words are relatively low. It comes
as no surprise that the level of 84% recalled
fully labeled morphemes, including spelling in-
formation, is not much higher than the level of
80% correctly recalled spelling changes (see Ta-
ble 2). When word-class information, type of
inflection, and spelling changes are discarded,
precision and recall of basic segment types be-
comes quite accurate: over 94%.
</bodyText>
<page confidence="0.994717">
288
</page>
<table confidence="0.997191833333333">
instances words
class labeling granularity labeling example % ± % ±
full morphological analysis [abnormaal]A[iteit]N_A*[en]ni 95.88 0.04 64.63 0.24
derivation/inflection [abnormai]deriv [iteit]dern [en]infl 98.83 0.02 89.62 0.17
segmentation [abnormal][iteit][en] 98.97 0.02 90.69 0.02
spelling changes +Da 80.95 0.40 — —
</table>
<tableCaption confidence="0.8351252">
Table 2: Generalization accuracies in terms of the percentage of correctly classified test instances
and words, with standard deviations (±) of MBMA applied to full Dutch morphological analysis and
three lower-granularity tasks derived from MBMA&apos;s full output. The example word abnormaliteiten
is shown according to the different labeling granularities, and only its single spelling change at the
bottom line).
</tableCaption>
<table confidence="0.9988004">
precision recall
task variation (%) (%)
full morphological analysis 84.33 83.76
derivation/inflection 94.72 94.07
segmentation 94.83 94.18
</table>
<tableCaption confidence="0.997535">
Table 3: Precision and recall of morphemes, de-
</tableCaption>
<bodyText confidence="0.9697626">
rived from the classification output of MBMA
applied to the full task and two lower-
granularity variations of Dutch morphological
analysis, using a context width of five left and
right letters.
</bodyText>
<subsectionHeader confidence="0.985207">
4.3 Predicting the syntactic class of
wordforms
</subsectionHeader>
<bodyText confidence="0.999051458333334">
Since MBMA predicts the syntactic label of
morphemes, and since complex Dutch word-
forms generally inherit their syntactic proper-
ties from their right-most morpheme, MBMA&apos;s
syntactic labeling can be used to predict the
syntactic class of the full wordform. When ac-
curate, this functionality can be an asset in han-
dling unknown words in part-of-speech tagging
systems. The results, displayed in Table 4, show
that about 91.2% of all test words are assigned
the exact tag they also have in CELEX (includ-
ing ambiguous tags such as &amp;quot;N/V&amp;quot; – 1.3% word-
forms in the CELEX dataset have an ambiguous
syntactic tag). When MBMA&apos;s output is also
considered correct if it predicts at least one out
of the possible tags listed in CELEX, the accu-
racy on test words is 91.6%. These accuracies
compare favorably with a related (yet strictly
incomparable) approach that predicts the word
class from the (ambiguous) part-of-speech tags
of the two surrounding words, the first letter,
and the final three letters of Dutch words, viz.
71.6% on unknown words in texts (Daelemans
et al., 1996a).
</bodyText>
<table confidence="0.87435275">
syntactic class correct test words
prediction words (%) ±
exact 91.24 0.21
exact or among alternatives 91.60 0.21
</table>
<tableCaption confidence="0.899264166666667">
Table 4: Average prediction accuracies (with
standard deviations) of MBMA on syntactic
classes of test words. The top line displays exact
matches with CELEX tags; the bottom line also
includes predictions that are among CELEX al-
ternatives.
</tableCaption>
<subsectionHeader confidence="0.998271">
4.4 Free text estimation
</subsectionHeader>
<bodyText confidence="0.99995265">
Although some of the above-mentioned accu-
racy results, especially the precision and recall
of fully-labeled morphemes, seem not very high,
they should be seen in the context of the test
they are derived from: they stem from held-out
portions of dictionary words. In texts sampled
from real-life usage, words are typically smaller
and morphologically less complex, and a rela-
tively small set of words re-occurs very often.
It is therefore relevant for our study to have
an estimate of the performance of MBMA on
real texts. We generate such an estimate fol-
lowing these considerations: New, unseen text
is bound to contain a lot of words that are in the
245,000 CELEX data base, but also some number
of unknown words. The morphological analy-
ses of known words are simply retrieved by the
memory-based learner from memory. Due to
some ambiguity in the class labeling in the data
base itself, retrieval accuracy will be somewhat
</bodyText>
<page confidence="0.995306">
289
</page>
<bodyText confidence="0.99968394">
below 100%. The morphological analyses of un-
known words are assumed to be as accurate as
was tested in the above-mentioned experiments:
they can be said to be of the type of dictionary
words in the 10% held-out test sets of 10-fold
cross validation experiments. CELEX bases its
wordform frequency information on word counts
made on the 42,380,000-words Dutch INL cor-
pus. 5.06% of these wordforms are wordform
tokens that occur only once. We assume that
this can be extrapolated to the estimate that
in real texts, 5% of the words do not occur
in the 245,000 words of the CELEX data base.
Therefore, a sensible estimate of the accura-
cies of memory-based learners on real text is a
weighted sum of accuracies comprised of 95% of
the reproduction accuracy (i.e, the error on the
training set itself), and 5% of the generalization
accuracy as reported earlier.
Table 5 summarizes the estimated generaliza-
tion accuracy results computed on the results
of MBMA. First, the percentages of correct in-
stances and words are estimated to be above
98% for the full task; in terms of words, it is es-
timated that 84% of all words are fully correctly
analyzed. When lower-granularity classification
tasks are discerned, accuracies on words are es-
timated to exceed 96% (on instances, less than
1% errors are estimated). Moreover, precision
and recall of morphemes on the full task are
estimated to be above 93%. A considerable sur-
plus is obtained by memory retrieval in the es-
timated percentage of correct spelling changes:
93%. Finally, the prediction of the syntactic
tags of wordforms would be about 97% accord-
ing to this estimate.
We briefly note that Heemskerk (1993) re-
ports a correct word score of 92% on free text
test material yielded by the probabilistic mor-
phological analyzer MORPA. MORPA segments
wordforms, decides whether a morpheme is a
stem, an affix or an inflection, detects spelling
changes, and assigns a syntactic tag to the word-
form. We have not made a conversion of our
output to Heemskerk&apos;s (1993). Moreover, a
proper comparison would demand the same test
data, but we believe that the 92% corresponds
roughly to our MBMA estimates of 97.2% correct
syntactic tags, 93.1% correct spelling changes,
and 96.7% correctly segmented words.
</bodyText>
<table confidence="0.999665133333333">
Estimate value
correct instances, full task 98.4%
correct words, full task 84.2%
correct instances, derivation/inflection 99.6%
correct words, derivation/inflection 96.7%
correct instances, segmentation 99.6%
correct words, segmentation 96.7%
precision of fully-labeled morphemes 93.6%
recall of fully-labeled morphemes 93.2%
precision of deriv./infl. morphemes 98.5%
recall of deriv./infl. morphemes 98.0%
precision of segments 98.5%
recall of segments 97.9%
correct spelling changes 93.1%
exactly correct syntactic wordform tags 97.2%
</table>
<tableCaption confidence="0.927339">
Table 5: Estimations of accuracies on real text,
derived from the generalization accuracies of
MBMA on full Dutch morphological analysis.
</tableCaption>
<sectionHeader confidence="0.985786" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999979851851852">
We have demonstrated the applicability of
memory-based learning to morphological anal-
ysis, by reformulating the problem as a classi-
fication task in which letter sequences are clas-
sified as marking different types of morpheme
boundaries. The generalization performance of
memory-based learning algorithms to the task
is encouraging, given that the tests are done
on held-out (dictionary) words. Estimates of
free-text performance give indications of high
accuracies: 84.6% correct fully-analyzed words
(64.6% on unseen words), and 96.7% correctly
segmented and coarsely-labeled words (about
90% for unseen words). Precision and recall
of fully-labeled morphemes is estimated in real
texts to be over 93% (about 84% for unseen
words). Finally, the prediction of (possibly am-
biguous) syntactic classes of unknown word-
forms in the test material was shown to be
91.2% correct; the corresponding free-text es-
timate is 97.2% correctly-tagged wordforms.
In comparison with the traditional approach,
which is not immune to costly hand-crafting and
spurious ambiguity, the memory-based learning
approach applied to a reformulation of the prob-
lem as a classification task of the segmentation
type, has a number of advantages:
</bodyText>
<page confidence="0.972678">
290
</page>
<listItem confidence="0.960523416666667">
• it presupposes no more linguistic knowl-
edge than explicitly present in the cor-
pus used for training, i.e., it avoids a
knowledge-acquisition bottleneck;
• it is language-independent, as it functions
on any morphologically analyzed corpus in
any language;
• learning is automatic and fast;
• processing is deterministic, non-recurrent
(i.e., it does not retry analysis generation)
and fast, and is only linearly related to the
length of the wordform being processed.
</listItem>
<bodyText confidence="0.999993533333333">
The language-independence of the approach
can be illustrated by means of the following par-
tial results on MBMA of English. We performed
experiments on 75,745 English wordforms from
CELEX and predicted the lower-granularity
tasks of predicting morpheme boundaries (Van
den Bosch et al., 1996). Experiments yielded
88.0% correctly segmented test words when de-
ciding only on the location of morpheme bound-
aries, and 85.6% correctly segmented test words
discerning between derivational and inflectional
morphemes. Both results are roughly compa-
rable to the 90% reported here (but note the
difference in training set size).
A possible limitation of the approach may
be the fact that it cannot return more than
one possible segmentation for a wordform. E.g.
the compound word Icwartslagen can be inter-
preted as either kwart+slagen (quarter turns)
or kvvarts+lagen (quartz layers). The memory-
based approach would select one segmentation.
However, true segmentation ambiguity of this
type is very rare in Dutch. Labeling ambigu-
ity occurs more often (3.6% of all morphemes),
and the current approach simply produces am-
biguous tags. However, it is possible for our
approach to return distributions of possible
classes, if desired, as well as it is possible to &amp;quot;un-
pack&amp;quot; ambiguous labeling into lists of possible
morphological analyses of a wordform. If, for
example, MBMA&apos;s output for the word bakken
(bake, an infinitive or plural verb form, or bins,
a plural noun) would be [bak]riAr[en]invon,
then this output could be expanded unambigu-
ously into the noun analysis [bak]N[en],, (plu-
ral) and the two verb readings [bak]v[en] (in-
finitive) and [bak]r[en]tin (present tense plu-
ral).
Points of future research are comparisons
with other morphological analyzers and lem-
matizers; applications of MBMA to other lan-
guages (particularly those with radically differ-
ent morphologies); and qualitative analyses of
MBMA&apos;s output in relation with linguistic pre-
dictions of errors and markedness of exceptions.
</bodyText>
<sectionHeader confidence="0.9947" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9967492">
This research was done in the context of
the &amp;quot;Induction of Linguistic Knowledge&amp;quot; (ILK)
research programme, supported partially by
the Netherlands Organization for Scientific Re-
search (NWO). The authors wish to thank Ton
Weijters and the members of the Tilburg ILK
group for stimulating discussions. A demonstra-
tion version of the morphological analysis sys-
tem for Dutch is available via ILK&apos;s homepage
http: //ilk .kub .nl.
</bodyText>
<sectionHeader confidence="0.996207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999491533333334">
D. W. Aha, D. Kibler, and M. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37-66.
R. H. Baayen, R. Piepenbrock, and H. van Rijn.
1993. The CELEX lexical data base on CD-
ROM. Linguistic Data Consortium, Philadel-
phia, PA.
W. Daelemans and A. Van den Bosch. 1992.
Generalisation performance of backpropaga-
tion learning on a syllabification task. In
M. F. J. Drossaers and A. Nijholt, editors,
Proc. of TWLT3: Connectionism and Nat-
ural Language Processing, pages 27-37, En-
schede. Twente University.
W. Daelemans and A. Van den Bosch.
1996. Language-independent data-oriented
grapheme-to-phoneme conversion. In J. P. H.
Van Santen, R. W. Sproat, J. P. Olive, and
J. Hirschberg, editors, Progress in Speech
Processing, pages 77-89. Springer-Verlag,
Berlin.
W. Daelemans, S. Gillis, and G. Durieux.
1994. The acquisition of stress: a data-
oriented approach. Computational Linguis-
tics, 20(3):421-451.
W. Daelemans, J. Zavrel, and P. Berck.
1996a. Part-of-speech tagging for Dutch with
MBT, a memory-based tagger generator. In
K. van der Meer, editor, Informatieweten-
schap 1996, Wetenschappelijke bijdrage aan
</reference>
<page confidence="0.962546">
291
</page>
<reference confidence="0.998845633333333">
de Vierde Interdisciplinaire Onderzoekscon-
ferentie Inform atiewetenchap, pages 33-40,
The Netherlands. TU Delft.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996b. MBT: A memory-based part of speech
tagger generator. In E. Ejerhed and I. Dagan,
editors, Proc. of Fourth Workshop on Very
Large Corpora, pages 14-27. ACL SIGDAT.
W. Daelemans, A. Van den Bosch, and A. Weij-
ters. 1997. mTree: using trees for com-
pression and classification in lazy learning
algorithms. Artificial Intelligence Review,
11:407-423:
W. Daelemans. 1995. Memory-based lexical ac-
quisition and processing. In P. Steffens, ed-
itor, Machine Translation and the Lexicon,
Lecture Notes in Artificial Intelligence, pages
85-98. Springer-Verlag, Berlin.
W. De Haas and M. Trommelen. 1993. Mor-
fologisch handboek van het Nederlands: Een
overzicht van de woordvorming. SDU, &apos;s
Gravenhage, The Netherlands.
J. Heemskerk and V. van Heuven. 1993.
MORPA: A morpheme lexicon-based mor-
phological parser. In V. van Heuven and
L. Pols, editors, Analysis and synthesis of
speech; Strategic research towards high-quality
speech generation, pages 67-85. Mouton de
Gruyter, Berlin.
J. Heemskerk. 1993. A probabilistic context-
free grammar for disambiguation in morpho-
logical parsing. In Proceedings of the 6th Con-
ference of the EACL, pages 183-192.
K. Koskenniemi. 1983. Two-level morphol-
ogy: a general computational model for word-
form recognition and production. Ph.D. the-
sis, University of Helsinki.
J.R. Quinlan. 1986. Induction of Decision
Trees. Machine Learning, 1:81-206.
T. J. Sejnowski and C. S. Rosenberg. 1987. Par-
allel networks that learn to pronounce English
text. Complex Systems, 1:145-168.
C. Stanfill and D. Waltz. 1986. Toward
memory-based reasoning. Communications
of the ACM, 29(12):1213-1228, December.
A. Van den Bosch, W. Daelemans, and A. Weij-
ters. 1996. Morphological analysis as classi-
fication: an inductive-learning approach. In
K. Oflazer and H. Somers, editors, Proceed-
ings of the Second International Conference
on New Methods in Natural Language Pro-
cessing, NeMLaP-2, Ankara, Turkey, pages
79-89.
A. Van den Bosch. 1997. Learning to pro-
nounce written words: A study in inductive
language learning. Ph.D. thesis, Universiteit
Maastricht.
S. Weiss and C. Kulikowski. 1991. Computer
systems that learn. San Mateo, CA: Morgan
Kaufmann.
</reference>
<page confidence="0.997457">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.303595">
<title confidence="0.998982">Memory-Based Morphological Analysis</title>
<author confidence="0.996858">van_den_Bosch Daelemans</author>
<affiliation confidence="0.995811">Linguistics Tilburg University</affiliation>
<address confidence="0.362638">fantalb,walterl@kub.n11</address>
<abstract confidence="0.985767666666667">We present a general architecture for efficient and deterministic morphological analysis based on memory-based learning, and apply it to morphological analysis of Dutch. The system makes direct mappings from letters in context to rich categories that encode morphological boundaries, syntactic class labels, and spelling changes. Both precision and recall of labeled morphemes are over 84% on held-out dictionary test words and estimated to be over 93% in free text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="8967" citStr="Aha et al., 1991" startWordPosition="1401" endWordPosition="1404">ge, and generally have a low frequency. When a wordform is listed in CELEX as having more than one possible morphological labeling (e.g., a morpheme may be N or V, the inflection -en may be plural for nouns or infinitive for verbs), these labels are joined into ambiguous classes (&amp;quot;N/V&amp;quot;) and the first generated example is labeled with this ambiguous class. Ambiguity in syntactic and inflectional tags occurs in 3.6% of all morphemes in our CELEX data. The memory-based learning algorithm used within MBMA is n31-IG (Daelemans and Van den Bosch, 1992; Daelemans et al., 1997), an extension of IB 1 (Aha et al., 1991). IB 1- IG constructs a data base of instances in memory during learning. New instances are classified by ml-IG by matching them to all instances in the instance base, and calculating with each match the distance between the new instance •X and the memory instance Y, A(X, Y) = E7i1_1 W(fi)(5(xi, yi), where W( f i) is the weight of the ith feature, and 8(x2, yi) is the distance between the values of the ith feature in instances X and Y. When the values of the instance features are symbolic, as with our linguistic tasks, the simple overlap distance function is used: 6(x1, y2) 0 if x, = yi, else </context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX lexical data base on CDROM. Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. The CELEX lexical data base on CDROM. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
</authors>
<title>Generalisation performance of backpropagation learning on a syllabification task.</title>
<date>1992</date>
<booktitle>Proc. of TWLT3: Connectionism and Natural Language Processing,</booktitle>
<pages>27--37</pages>
<editor>In M. F. J. Drossaers and A. Nijholt, editors,</editor>
<institution>Enschede. Twente University.</institution>
<marker>Daelemans, Van den Bosch, 1992</marker>
<rawString>W. Daelemans and A. Van den Bosch. 1992. Generalisation performance of backpropagation learning on a syllabification task. In M. F. J. Drossaers and A. Nijholt, editors, Proc. of TWLT3: Connectionism and Natural Language Processing, pages 27-37, Enschede. Twente University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
</authors>
<title>Language-independent data-oriented grapheme-to-phoneme conversion.</title>
<date>1996</date>
<booktitle>Progress in Speech Processing,</booktitle>
<pages>77--89</pages>
<editor>In J. P. H. Van Santen, R. W. Sproat, J. P. Olive, and J. Hirschberg, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Daelemans, Van den Bosch, 1996</marker>
<rawString>W. Daelemans and A. Van den Bosch. 1996. Language-independent data-oriented grapheme-to-phoneme conversion. In J. P. H. Van Santen, R. W. Sproat, J. P. Olive, and J. Hirschberg, editors, Progress in Speech Processing, pages 77-89. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>S Gillis</author>
<author>G Durieux</author>
</authors>
<title>The acquisition of stress: a dataoriented approach.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="3235" citStr="Daelemans et al., 1994" startWordPosition="476" endWordPosition="479">by storing examples of a task in memory. Computational effort is invested on a &amp;quot;call-by-need&amp;quot; basis for solving new examples (henceforth called instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme—phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology inclu</context>
</contexts>
<marker>Daelemans, Gillis, Durieux, 1994</marker>
<rawString>W. Daelemans, S. Gillis, and G. Durieux. 1994. The acquisition of stress: a dataoriented approach. Computational Linguistics, 20(3):421-451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
</authors>
<title>Part-of-speech tagging for Dutch with MBT, a memory-based tagger generator. In</title>
<date>1996</date>
<booktitle>Informatiewetenschap 1996, Wetenschappelijke bijdrage aan de Vierde Interdisciplinaire Onderzoeksconferentie Inform atiewetenchap,</booktitle>
<pages>33--40</pages>
<editor>K. van der Meer, editor,</editor>
<location>The Netherlands. TU Delft.</location>
<contexts>
<context position="3373" citStr="Daelemans et al., 1996" startWordPosition="495" endWordPosition="498">lled instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme—phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology include inflection, derivation, and compounding. Inflection of verbs, adjectives, and nouns is mostly achieved by suffixation, but a circumfix </context>
<context position="17585" citStr="Daelemans et al., 1996" startWordPosition="2881" endWordPosition="2884"> assigned the exact tag they also have in CELEX (including ambiguous tags such as &amp;quot;N/V&amp;quot; – 1.3% wordforms in the CELEX dataset have an ambiguous syntactic tag). When MBMA&apos;s output is also considered correct if it predicts at least one out of the possible tags listed in CELEX, the accuracy on test words is 91.6%. These accuracies compare favorably with a related (yet strictly incomparable) approach that predicts the word class from the (ambiguous) part-of-speech tags of the two surrounding words, the first letter, and the final three letters of Dutch words, viz. 71.6% on unknown words in texts (Daelemans et al., 1996a). syntactic class correct test words prediction words (%) ± exact 91.24 0.21 exact or among alternatives 91.60 0.21 Table 4: Average prediction accuracies (with standard deviations) of MBMA on syntactic classes of test words. The top line displays exact matches with CELEX tags; the bottom line also includes predictions that are among CELEX alternatives. 4.4 Free text estimation Although some of the above-mentioned accuracy results, especially the precision and recall of fully-labeled morphemes, seem not very high, they should be seen in the context of the test they are derived from: they ste</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, and P. Berck. 1996a. Part-of-speech tagging for Dutch with MBT, a memory-based tagger generator. In K. van der Meer, editor, Informatiewetenschap 1996, Wetenschappelijke bijdrage aan de Vierde Interdisciplinaire Onderzoeksconferentie Inform atiewetenchap, pages 33-40, The Netherlands. TU Delft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proc. of Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I. Dagan, editors,</editor>
<publisher>ACL SIGDAT.</publisher>
<contexts>
<context position="3373" citStr="Daelemans et al., 1996" startWordPosition="495" endWordPosition="498">lled instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme—phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology include inflection, derivation, and compounding. Inflection of verbs, adjectives, and nouns is mostly achieved by suffixation, but a circumfix </context>
<context position="17585" citStr="Daelemans et al., 1996" startWordPosition="2881" endWordPosition="2884"> assigned the exact tag they also have in CELEX (including ambiguous tags such as &amp;quot;N/V&amp;quot; – 1.3% wordforms in the CELEX dataset have an ambiguous syntactic tag). When MBMA&apos;s output is also considered correct if it predicts at least one out of the possible tags listed in CELEX, the accuracy on test words is 91.6%. These accuracies compare favorably with a related (yet strictly incomparable) approach that predicts the word class from the (ambiguous) part-of-speech tags of the two surrounding words, the first letter, and the final three letters of Dutch words, viz. 71.6% on unknown words in texts (Daelemans et al., 1996a). syntactic class correct test words prediction words (%) ± exact 91.24 0.21 exact or among alternatives 91.60 0.21 Table 4: Average prediction accuracies (with standard deviations) of MBMA on syntactic classes of test words. The top line displays exact matches with CELEX tags; the bottom line also includes predictions that are among CELEX alternatives. 4.4 Free text estimation Although some of the above-mentioned accuracy results, especially the precision and recall of fully-labeled morphemes, seem not very high, they should be seen in the context of the test they are derived from: they ste</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996b. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I. Dagan, editors, Proc. of Fourth Workshop on Very Large Corpora, pages 14-27. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>A Weijters</author>
</authors>
<title>mTree: using trees for compression and classification in lazy learning algorithms.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<pages>11--407</pages>
<marker>Daelemans, Van den Bosch, Weijters, 1997</marker>
<rawString>W. Daelemans, A. Van den Bosch, and A. Weijters. 1997. mTree: using trees for compression and classification in lazy learning algorithms. Artificial Intelligence Review, 11:407-423:</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
</authors>
<title>Memory-based lexical acquisition and processing.</title>
<date>1995</date>
<booktitle>Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence,</booktitle>
<pages>85--98</pages>
<editor>In P. Steffens, editor,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="6095" citStr="Daelemans, 1995" startWordPosition="929" endWordPosition="930">e been based on finite-state transducers (e.g., XEROX&apos;es morphological analyzer), or on parsing with context-free word grammars interleaved with exploration of possible spelling changes (e.g. Heemskerk and van Heuven (1993); or see Heemskerk (1993) for a probabilistic variant). 3 Applying memory-based learning to morphological analysis Most linguistic problems can be seen ascontextsensitive mappings from one representation to another (e.g., from text to speech; from a sequence of spelling words to a parse tree; from a parse tree to logical form, from source language to target language, etc.) (Daelemans, 1995). This is also the case for morphological analysis. Memory-based learning algorithms can learn mappings (classifications) if a sufficient number of instances of these mappings is presented to them. We drew our instances from the CELEX lexical data base (Baayen et al., 1993). CELEX contains a large lexical data base of Dutch wordforms, and features a full morphological analysis for 247,415 of them. We took each wordform and its associated analysis, and created task instances using a windowing method (Sejnowski and Rosenberg, 1987). Windowing transforms each wordform into as many instances as it</context>
</contexts>
<marker>Daelemans, 1995</marker>
<rawString>W. Daelemans. 1995. Memory-based lexical acquisition and processing. In P. Steffens, editor, Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence, pages 85-98. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W De Haas</author>
<author>M Trommelen</author>
</authors>
<title>Morfologisch handboek van het Nederlands: Een overzicht van de woordvorming. SDU, &apos;s Gravenhage, The Netherlands.</title>
<date>1993</date>
<marker>De Haas, Trommelen, 1993</marker>
<rawString>W. De Haas and M. Trommelen. 1993. Morfologisch handboek van het Nederlands: Een overzicht van de woordvorming. SDU, &apos;s Gravenhage, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Heemskerk</author>
<author>V van Heuven</author>
</authors>
<title>MORPA: A morpheme lexicon-based morphological parser.</title>
<date>1993</date>
<booktitle>Mouton de Gruyter,</booktitle>
<pages>67--85</pages>
<editor>In V. van Heuven and L. Pols, editors,</editor>
<location>Berlin.</location>
<marker>Heemskerk, van Heuven, 1993</marker>
<rawString>J. Heemskerk and V. van Heuven. 1993. MORPA: A morpheme lexicon-based morphological parser. In V. van Heuven and L. Pols, editors, Analysis and synthesis of speech; Strategic research towards high-quality speech generation, pages 67-85. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Heemskerk</author>
</authors>
<title>A probabilistic contextfree grammar for disambiguation in morphological parsing.</title>
<date>1993</date>
<booktitle>In Proceedings of the 6th Conference of the EACL,</booktitle>
<pages>183--192</pages>
<contexts>
<context position="5727" citStr="Heemskerk (1993)" startWordPosition="871" endWordPosition="872">+en, we stop); there is variation between s and z and f and v (e.g. huis, house, versus huizen, houses). Finally, between the parts of a compound, a linking morpheme may appear (e.g. staat+s+loterij, state lottery). For a detailed discussion of morphological phenomena in Dutch, see De Haas and Trommelen (1993). Previous approaches to Dutch morphological analysis have been based on finite-state transducers (e.g., XEROX&apos;es morphological analyzer), or on parsing with context-free word grammars interleaved with exploration of possible spelling changes (e.g. Heemskerk and van Heuven (1993); or see Heemskerk (1993) for a probabilistic variant). 3 Applying memory-based learning to morphological analysis Most linguistic problems can be seen ascontextsensitive mappings from one representation to another (e.g., from text to speech; from a sequence of spelling words to a parse tree; from a parse tree to logical form, from source language to target language, etc.) (Daelemans, 1995). This is also the case for morphological analysis. Memory-based learning algorithms can learn mappings (classifications) if a sufficient number of instances of these mappings is presented to them. We drew our instances from the CEL</context>
<context position="20552" citStr="Heemskerk (1993)" startWordPosition="3378" endWordPosition="3379">for the full task; in terms of words, it is estimated that 84% of all words are fully correctly analyzed. When lower-granularity classification tasks are discerned, accuracies on words are estimated to exceed 96% (on instances, less than 1% errors are estimated). Moreover, precision and recall of morphemes on the full task are estimated to be above 93%. A considerable surplus is obtained by memory retrieval in the estimated percentage of correct spelling changes: 93%. Finally, the prediction of the syntactic tags of wordforms would be about 97% according to this estimate. We briefly note that Heemskerk (1993) reports a correct word score of 92% on free text test material yielded by the probabilistic morphological analyzer MORPA. MORPA segments wordforms, decides whether a morpheme is a stem, an affix or an inflection, detects spelling changes, and assigns a syntactic tag to the wordform. We have not made a conversion of our output to Heemskerk&apos;s (1993). Moreover, a proper comparison would demand the same test data, but we believe that the 92% corresponds roughly to our MBMA estimates of 97.2% correct syntactic tags, 93.1% correct spelling changes, and 96.7% correctly segmented words. Estimate valu</context>
</contexts>
<marker>Heemskerk, 1993</marker>
<rawString>J. Heemskerk. 1993. A probabilistic contextfree grammar for disambiguation in morphological parsing. In Proceedings of the 6th Conference of the EACL, pages 183-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-level morphology: a general computational model for wordform recognition and production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="1799" citStr="Koskenniemi (1983)" startWordPosition="263" endWordPosition="265">es, such as English. Many wordforms in English and other western languages contain ambiguities in their morphological composition that can be quite intricate. General classes of linguistic knowledge that are usually assumed to play a role in this disambiguation process are knowledge of (i) the morphemes of a language, (ii) the morphotactics, i.e., constraints on how morphemes are allowed to attach, and (iii) spelling changes that can occur due to morpheme attachment. State-of-the art systems for morphological analysis of wordforms are usually based on two-level finite-state transducers (FSTS, Koskenniemi (1983)). Even with the availability of sophisticated development tools, the cost and complexity of hand-crafting two-level rules is high, and the representation of concatenative compound morphology with continuation lexicons is difficult. As in parsing, there is a tradeoff between coverage and spurious ambiguity in these systems: the more sophisticated the rules become, the more needless ambiguity they introduce. In this paper we present a learning approach which models morphological analysis (including compounding) of complex wordforms as sequences of classification tasks. Our model, MBMA (Memory-B</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>K. Koskenniemi. 1983. Two-level morphology: a general computational model for wordform recognition and production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of Decision Trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--81</pages>
<contexts>
<context position="9869" citStr="Quinlan (1986)" startWordPosition="1566" endWordPosition="1567">i)(5(xi, yi), where W( f i) is the weight of the ith feature, and 8(x2, yi) is the distance between the values of the ith feature in instances X and Y. When the values of the instance features are symbolic, as with our linguistic tasks, the simple overlap distance function is used: 6(x1, y2) 0 if x, = yi, else 1. The (most frequently occurring) classification of the memory instance Y with the smallest X, Y) is then taken as the classification of X. The weighting function W(f2) computes for each feature, over the full instance base, its information gain, a function from information theory; cf. Quinlan (1986). In short, the information gain of a feature expresses its relative importance compared to the other features in performing the mapping from input to classification. When information gain is used in the similarity function, instances that match on important features are regarded as more alike than instances that match on unimportant features. In our experiments, we are primarily interested in the generalization accuracy of trained models, i.e., the ability of these models to use their accumulated knowledge to classify new instances that were not in the training material. A method that gives a</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J.R. Quinlan. 1986. Induction of Decision Trees. Machine Learning, 1:81-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Sejnowski</author>
<author>C S Rosenberg</author>
</authors>
<title>Parallel networks that learn to pronounce English text.</title>
<date>1987</date>
<journal>Complex Systems,</journal>
<pages>1--145</pages>
<contexts>
<context position="6630" citStr="Sejnowski and Rosenberg, 1987" startWordPosition="1015" endWordPosition="1018"> a parse tree to logical form, from source language to target language, etc.) (Daelemans, 1995). This is also the case for morphological analysis. Memory-based learning algorithms can learn mappings (classifications) if a sufficient number of instances of these mappings is presented to them. We drew our instances from the CELEX lexical data base (Baayen et al., 1993). CELEX contains a large lexical data base of Dutch wordforms, and features a full morphological analysis for 247,415 of them. We took each wordform and its associated analysis, and created task instances using a windowing method (Sejnowski and Rosenberg, 1987). Windowing transforms each wordform into as many instances as it has letters. Each example focuses on one letter, and includes a fixed number of left and right neighbor letters, chosen here to be five. Consequently, each instance spans eleven letters, which is also the average word length in the CELEX data base. Moreover, we estimated from exploratory data analysis that this context would contain enough information to allow for adequate disambiguation. To illustrate the construction of instances, Table 1 displays the 15 instances derived from the Dutch example word abnormaliteiten (abnormalit</context>
</contexts>
<marker>Sejnowski, Rosenberg, 1987</marker>
<rawString>T. J. Sejnowski and C. S. Rosenberg. 1987. Parallel networks that learn to pronounce English text. Complex Systems, 1:145-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>D Waltz</author>
</authors>
<title>Toward memory-based reasoning.</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<pages>29--12</pages>
<contexts>
<context position="2488" citStr="Stanfill and Waltz, 1986" startWordPosition="362" endWordPosition="366"> the cost and complexity of hand-crafting two-level rules is high, and the representation of concatenative compound morphology with continuation lexicons is difficult. As in parsing, there is a tradeoff between coverage and spurious ambiguity in these systems: the more sophisticated the rules become, the more needless ambiguity they introduce. In this paper we present a learning approach which models morphological analysis (including compounding) of complex wordforms as sequences of classification tasks. Our model, MBMA (Memory-Based Morphological Analysis), is a memory-based learning system (Stanfill and Waltz, 1986; Daelemans et al., 1997). Memory-based learning is a class of inductive, supervised machine learning algorithms that learn by storing examples of a task in memory. Computational effort is invested on a &amp;quot;call-by-need&amp;quot; basis for solving new examples (henceforth called instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>C. Stanfill and D. Waltz. 1986. Toward memory-based reasoning. Communications of the ACM, 29(12):1213-1228, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>W Daelemans</author>
<author>A Weijters</author>
</authors>
<title>Morphological analysis as classification: an inductive-learning approach.</title>
<date>1996</date>
<booktitle>Proceedings of the Second International Conference on New Methods in Natural Language Processing, NeMLaP-2,</booktitle>
<pages>79--89</pages>
<editor>In K. Oflazer and H. Somers, editors,</editor>
<location>Ankara, Turkey,</location>
<marker>Van den Bosch, Daelemans, Weijters, 1996</marker>
<rawString>A. Van den Bosch, W. Daelemans, and A. Weijters. 1996. Morphological analysis as classification: an inductive-learning approach. In K. Oflazer and H. Somers, editors, Proceedings of the Second International Conference on New Methods in Natural Language Processing, NeMLaP-2, Ankara, Turkey, pages 79-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Learning to pronounce written words: A study in inductive language learning.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit Maastricht.</institution>
<marker>Van den Bosch, 1997</marker>
<rawString>A. Van den Bosch. 1997. Learning to pronounce written words: A study in inductive language learning. Ph.D. thesis, Universiteit Maastricht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weiss</author>
<author>C Kulikowski</author>
</authors>
<title>Computer systems that learn.</title>
<date>1991</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="10615" citStr="Weiss and Kulikowski, 1991" startWordPosition="1685" endWordPosition="1688">rming the mapping from input to classification. When information gain is used in the similarity function, instances that match on important features are regarded as more alike than instances that match on unimportant features. In our experiments, we are primarily interested in the generalization accuracy of trained models, i.e., the ability of these models to use their accumulated knowledge to classify new instances that were not in the training material. A method that gives a good estimate of the generalization performance of an algorithm on a given instance base, is 10-fold crossvalidation (Weiss and Kulikowski, 1991). This method generates on the basis of an instance base 10 subsequent partitionings into a training set (90%) and a test set (10%), resulting in 10 experiments. 4 Experiments: MBMA of Dutch wordforms As described, we performed 10-fold cross validation experiments in an experimental matrix in which MBMA is applied to the full instance base, using a context width of five left and right context letters. We structure the presentation of the experimental outcomes as follows. First, we give the generalization accuracies on test instances and test words obtained in the experiments, including measure</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>S. Weiss and C. Kulikowski. 1991. Computer systems that learn. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>